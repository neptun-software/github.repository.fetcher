{
  "metadata": {
    "timestamp": 1736567979248,
    "page": 116,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "slackhq/nebula",
      "stars": 14824,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3203125,
          "content": "/nebula\n/nebula-cert\n/nebula-arm\n/nebula-arm6\n/nebula-darwin\n/nebula.exe\n/nebula-cert.exe\n/coverage.out\n/cpu.pprof\n/build\n/*.tar.gz\n/e2e/mermaid/\n**.crt\n**.key\n**.pem\n**.pub\n!/examples/quickstart-vagrant/ansible/roles/nebula/files/vagrant-test-ca.key\n!/examples/quickstart-vagrant/ansible/roles/nebula/files/vagrant-test-ca.crt\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.2880859375,
          "content": "# This is the official list of Nebula authors for copyright purposes.\n\n# Names should be added to this file as:\n# Name or Organization <email address>\n# The email address is not required for organizations.\n\nSlack Technologies, Inc.\nNate Brown <nbrown.us@gmail.com>\nRyan Huber <rhuber@gmail.com>\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 23.8193359375,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [1.9.4] - 2024-09-09\n\n### Added\n\n- Support UDP dialing with gVisor. (#1181)\n\n### Changed\n\n- Make some Nebula state programmatically available via control object. (#1188)\n- Switch internal representation of IPs to netip, to prepare for IPv6 support\n  in the overlay. (#1173)\n- Minor build and cleanup changes. (#1171, #1164, #1162)\n- Various dependency updates. (#1195, #1190, #1174, #1168, #1167, #1161, #1147, #1146)\n\n### Fixed\n\n- Fix a bug on big endian hosts, like mips. (#1194)\n- Fix a rare panic if a local index collision happens. (#1191)\n- Fix integer wraparound in the calculation of handshake timeouts on 32-bit targets. (#1185)\n\n## [1.9.3] - 2024-06-06\n\n### Fixed\n\n- Initialize messageCounter to 2 instead of verifying later. (#1156)\n\n## [1.9.2] - 2024-06-03\n\n### Fixed\n\n- Ensure messageCounter is set before handshake is complete. (#1154)\n\n## [1.9.1] - 2024-05-29\n\n### Fixed\n\n- Fixed a potential deadlock in GetOrHandshake. (#1151)\n\n## [1.9.0] - 2024-05-07\n\n### Deprecated\n\n- This release adds a new setting `default_local_cidr_any` that defaults to\n  true to match previous behavior, but will default to false in the next\n  release (1.10). When set to false, `local_cidr` is matched correctly for\n  firewall rules on hosts acting as unsafe routers, and should be set for any\n  firewall rules you want to allow unsafe route hosts to access. See the issue\n  and example config for more details. (#1071, #1099)\n\n### Added\n\n- Nebula now has an official Docker image `nebulaoss/nebula` that is\n  distroless and contains just the `nebula` and `nebula-cert` binaries. You\n  can find it here: https://hub.docker.com/r/nebulaoss/nebula (#1037)\n\n- Experimental binaries for `loong64` are now provided. (#1003)\n\n- Added example service script for OpenRC. (#711)\n\n- The SSH daemon now supports inlined host keys. (#1054)\n\n- The SSH daemon now supports certificates with `sshd.trusted_cas`. (#1098)\n\n### Changed\n\n- Config setting `tun.unsafe_routes` is now reloadable. (#1083)\n\n- Small documentation and internal improvements. (#1065, #1067, #1069, #1108,\n  #1109, #1111, #1135)\n\n- Various dependency updates. (#1139, #1138, #1134, #1133, #1126, #1123, #1110,\n  #1094, #1092, #1087, #1086, #1085, #1072, #1063, #1059, #1055, #1053, #1047,\n  #1046, #1034, #1022)\n\n### Removed\n\n- Support for the deprecated `local_range` option has been removed. Please\n  change to `preferred_ranges` (which is also now reloadable). (#1043)\n\n- We are now building with go1.22, which means that for Windows you need at\n  least Windows 10 or Windows Server 2016. This is because support for earlier\n  versions was removed in Go 1.21. See https://go.dev/doc/go1.21#windows (#981)\n\n- Removed vagrant example, as it was unmaintained. (#1129)\n\n- Removed Fedora and Arch nebula.service files, as they are maintained in the\n  upstream repos. (#1128, #1132)\n\n- Remove the TCP round trip tracking metrics, as they never had correct data\n  and were an experiment to begin with. (#1114)\n\n### Fixed\n\n- Fixed a potential deadlock introduced in 1.8.1. (#1112)\n\n- Fixed support for Linux when IPv6 has been disabled at the OS level. (#787)\n\n- DNS will return NXDOMAIN now when there are no results. (#845)\n\n- Allow `::` in `lighthouse.dns.host`. (#1115)\n\n- Capitalization of `NotAfter` fixed in DNS TXT response. (#1127)\n\n- Don't log invalid certificates. It is untrusted data and can cause a large\n  volume of logs. (#1116)\n\n## [1.8.2] - 2024-01-08\n\n### Fixed\n\n- Fix multiple routines when listen.port is zero. This was a regression\n  introduced in v1.6.0. (#1057)\n\n### Changed\n\n- Small dependency update for Noise. (#1038)\n\n## [1.8.1] - 2023-12-19\n\n### Security\n\n- Update `golang.org/x/crypto`, which includes a fix for CVE-2023-48795. (#1048)\n\n### Fixed\n\n- Fix a deadlock introduced in v1.8.0 that could occur during handshakes. (#1044)\n\n- Fix mobile builds. (#1035)\n\n## [1.8.0] - 2023-12-06\n\n### Deprecated\n\n- The next minor release of Nebula, 1.9.0, will require at least Windows 10 or\n  Windows Server 2016. This is because support for earlier versions was removed\n  in Go 1.21. See https://go.dev/doc/go1.21#windows\n\n### Added\n\n- Linux: Notify systemd of service readiness. This should resolve timing issues\n  with services that depend on Nebula being active. For an example of how to\n  enable this, see: `examples/service_scripts/nebula.service`. (#929)\n\n- Windows: Use Registered IO (RIO) when possible. Testing on a Windows 11\n  machine shows ~50x improvement in throughput. (#905)\n\n- NetBSD, OpenBSD: Added rudimentary support. (#916, #812)\n\n- FreeBSD: Add support for naming tun devices. (#903)\n\n### Changed\n\n- `pki.disconnect_invalid` will now default to true. This means that once a\n  certificate expires, the tunnel will be disconnected. If you use SIGHUP to\n  reload certificates without restarting Nebula, you should ensure all of your\n  clients are on 1.7.0 or newer before you enable this feature. (#859)\n\n- Limit how often a busy tunnel can requery the lighthouse. The new config\n  option `timers.requery_wait_duration` defaults to `60s`. (#940)\n\n- The internal structures for hostmaps were refactored to reduce memory usage\n  and the potential for subtle bugs. (#843, #938, #953, #954, #955)\n\n- Lots of dependency updates.\n\n### Fixed\n\n- Windows: Retry wintun device creation if it fails the first time. (#985)\n\n- Fix issues with firewall reject packets that could cause panics. (#957)\n\n- Fix relay migration during re-handshakes. (#964)\n\n- Various other refactors and fixes. (#935, #952, #972, #961, #996, #1002,\n  #987, #1004, #1030, #1032, ...)\n\n## [1.7.2] - 2023-06-01\n\n### Fixed\n\n- Fix a freeze during config reload if the `static_host_map` config was changed. (#886)\n\n## [1.7.1] - 2023-05-18\n\n### Fixed\n\n- Fix IPv4 addresses returned by `static_host_map` DNS lookup queries being\n  treated as IPv6 addresses. (#877)\n\n## [1.7.0] - 2023-05-17\n\n### Added\n\n- `nebula-cert ca` now supports encrypting the CA's private key with a\n  passphrase. Pass `-encrypt` in order to be prompted for a passphrase.\n  Encryption is performed using AES-256-GCM and Argon2id for KDF. KDF\n  parameters default to RFC recommendations, but can be overridden via CLI\n  flags `-argon-memory`, `-argon-parallelism`, and `-argon-iterations`. (#386)\n\n- Support for curve P256 and BoringCrypto has been added. See README section\n  \"Curve P256 and BoringCrypto\" for more details. (#865, #861, #769, #856, #803)\n\n- New firewall rule `local_cidr`. This could be used to filter destinations\n  when using `unsafe_routes`. (#507)\n\n- Add `unsafe_route` option `install`. This controls whether the route is\n  installed in the systems routing table. (#831)\n\n- Add `tun.use_system_route_table` option. Set to true to manage unsafe routes\n  directly on the system route table with gateway routes instead of in Nebula\n  configuration files. This is only supported on Linux. (#839)\n\n- The metric `certificate.ttl_seconds` is now exposed via stats. (#782)\n\n- Add `punchy.respond_delay` option. This allows you to change the delay\n  before attempting punchy.respond. Default is 5 seconds. (#721)\n\n- Added SSH commands to allow the capture of a mutex profile. (#737)\n\n- You can now set `lighthouse.calculated_remotes` to make it possible to do\n  handshakes without a lighthouse in certain configurations. (#759)\n\n- The firewall can be configured to send REJECT replies instead of the default\n  DROP behavior. (#738)\n\n- For macOS, an example launchd configuration file is now provided. (#762)\n\n### Changed\n\n- Lighthouses and other `static_host_map` entries that use DNS names will now\n  be automatically refreshed to detect when the IP address changes. (#796)\n\n- Lighthouses send ACK replies back to clients so that they do not fall into\n  connection testing as often by clients. (#851, #408)\n\n- Allow the `listen.host` option to contain a hostname. (#825)\n\n- When Nebula switches to a new certificate (such as via SIGHUP), we now\n  rehandshake with all existing tunnels. This allows firewall groups to be\n  updated and `pki.disconnect_invalid` to know about the new certificate\n  expiration time. (#838, #857, #842, #840, #835, #828, #820, #807)\n\n### Fixed\n\n- Always disconnect blocklisted hosts, even if `pki.disconnect_invalid` is\n  not set. (#858)\n\n- Dependencies updated and go1.20 required. (#780, #824, #855, #854)\n\n- Fix possible race condition with relays. (#827)\n\n- FreeBSD: Fix connection to the localhost's own Nebula IP. (#808)\n\n- Normalize and document some common log field values. (#837, #811)\n\n- Fix crash if you set unlucky values for the firewall timeout configuration\n  options. (#802)\n\n- Make DNS queries case insensitive. (#793)\n\n- Update example systemd configurations to want `nss-lookup`. (#791)\n\n- Errors with SSH commands now go to the SSH tunnel instead of stderr. (#757)\n\n- Fix a hang when shutting down Android. (#772)\n\n## [1.6.1] - 2022-09-26\n\n### Fixed\n\n- Refuse to process underlay packets received from overlay IPs. This prevents\n  confusion on hosts that have unsafe routes configured. (#741)\n\n- The ssh `reload` command did not work on Windows, since it relied on sending\n  a SIGHUP signal internally. This has been fixed. (#725)\n\n- A regression in v1.5.2 that broke unsafe routes on Mobile clients has been\n  fixed. (#729)\n\n## [1.6.0] - 2022-06-30\n\n### Added\n\n- Experimental: nebula clients can be configured to act as relays for other nebula clients.\n  Primarily useful when stubborn NATs make a direct tunnel impossible. (#678)\n\n- Configuration option to report manually specified `ip:port`s to lighthouses. (#650)\n\n- Windows arm64 build. (#638)\n\n- `punchy` and most `lighthouse` config options now support hot reloading. (#649)\n\n### Changed\n\n- Build against go 1.18. (#656)\n\n- Promoted `routines` config from experimental to supported feature. (#702)\n\n- Dependencies updated. (#664)\n\n### Fixed\n\n- Packets destined for the same host that sent it will be returned on MacOS.\n  This matches the default behavior of other operating systems. (#501)\n\n- `unsafe_route` configuration will no longer crash on Windows. (#648)\n\n- A few panics that were introduced in 1.5.x. (#657, #658, #675)\n\n### Security\n\n- You can set `listen.send_recv_error` to control the conditions in which\n  `recv_error` messages are sent. Sending these messages can expose the fact\n  that Nebula is running on a host, but it speeds up re-handshaking. (#670)\n\n### Removed\n\n- `x509` config stanza support has been removed. (#685)\n\n## [1.5.2] - 2021-12-14\n\n### Added\n\n- Warn when a non lighthouse node does not have lighthouse hosts configured. (#587)\n\n### Changed\n\n- No longer fatals if expired CA certificates are present in `pki.ca`, as long as 1 valid CA is present. (#599)\n\n- `nebula-cert` will now enforce ipv4 addresses. (#604)\n\n- Warn on macOS if an unsafe route cannot be created due to a collision with an\n  existing route. (#610)\n\n- Warn if you set a route MTU on platforms where we don't support it. (#611)\n\n### Fixed\n\n- Rare race condition when tearing down a tunnel due to `recv_error` and sending packets on another thread. (#590)\n\n- Bug in `routes` and `unsafe_routes` handling that was introduced in 1.5.0. (#595)\n\n- `-test` mode no longer results in a crash. (#602)\n\n### Removed\n\n- `x509.ca` config alias for `pki.ca`. (#604)\n\n### Security\n\n- Upgraded `golang.org/x/crypto` to address an issue which allowed unauthenticated clients to cause a panic in SSH\n  servers. (#603)\n\n## 1.5.1 - 2021-12-13\n\n(This release was skipped due to discovering #610 and #611 after the tag was\ncreated.)\n\n## [1.5.0] - 2021-11-11\n\n### Added\n\n- SSH `print-cert` has a new `-raw` flag to get the PEM representation of a certificate. (#483)\n\n- New build architecture: Linux `riscv64`. (#542)\n\n- New experimental config option `remote_allow_ranges`. (#540)\n\n- New config option `pki.disconnect_invalid` that will tear down tunnels when they become invalid (through expiry or\n  removal of root trust). Default is `false`. Note, this will not currently recognize if a remote has changed\n  certificates since the last handshake. (#370)\n\n- New config option `unsafe_routes.<route>.metric` will set a metric for a specific unsafe route. It's useful if you have\n  more than one identical route and want to prefer one against the other. (#353)\n\n### Changed\n\n- Build against go 1.17. (#553)\n\n- Build with `CGO_ENABLED=0` set, to create more portable binaries. This could\n  have an effect on DNS resolution if you rely on anything non-standard. (#421)\n\n- Windows now uses the [wintun](https://www.wintun.net/) driver which does not require installation. This driver\n  is a large improvement over the TAP driver that was used in previous versions. If you had a previous version\n  of `nebula` running, you will want to disable the tap driver in Control Panel, or uninstall the `tap0901` driver\n  before running this version. (#289)\n\n- Darwin binaries are now universal (works on both amd64 and arm64), signed, and shipped in a notarized zip file.\n  `nebula-darwin.zip` will be the only darwin release artifact. (#571)\n\n- Darwin uses syscalls and AF_ROUTE to configure the routing table, instead of\n  using `/sbin/route`. Setting `tun.dev` is now allowed on Darwin as well, it\n  must be in the format `utun[0-9]+` or it will be ignored. (#163)\n\n### Deprecated\n\n- The `preferred_ranges` option has been supported as a replacement for\n  `local_range` since v1.0.0. It has now been documented and `local_range`\n  has been officially deprecated. (#541)\n\n### Fixed\n\n- Valid recv_error packets were incorrectly marked as \"spoofing\" and ignored. (#482)\n\n- SSH server handles single `exec` requests correctly. (#483)\n\n- Signing a certificate with `nebula-cert sign` now verifies that the supplied\n  ca-key matches the ca-crt. (#503)\n\n- If `preferred_ranges` (or the deprecated `local_range`) is configured, we\n  will immediately switch to a preferred remote address after the reception of\n  a handshake packet (instead of waiting until 1,000 packets have been sent).\n  (#532)\n\n- A race condition when `punchy.respond` is enabled and ensures the correct\n  vpn ip is sent a punch back response in highly queried node. (#566)\n\n- Fix a rare crash during handshake due to a race condition. (#535)\n\n## [1.4.0] - 2021-05-11\n\n### Added\n\n- Ability to output qr code images in `print`, `ca`, and `sign` modes for `nebula-cert`.\n  This is useful when configuring mobile clients. (#297)\n\n- Experimental: Nebula can now do work on more than 2 cpu cores in send and receive paths via\n  the new `routines` config option. (#382, #391, #395)\n\n- ICMP ping requests can be responded to when the `tun.disabled` is `true`.\n  This is useful so that you can \"ping\" a lighthouse running in this mode. (#342)\n\n- Run smoke tests via `make smoke-docker`. (#287)\n\n- More reported stats, udp memory use on linux, build version (when using Prometheus), firewall,\n  handshake, and cached packet stats. (#390, #405, #450, #453)\n\n- IPv6 support for the underlay network. (#369)\n\n- End to end testing, run with `make e2e`. (#425, #427, #428)\n\n### Changed\n\n- Darwin will now log stdout/stderr to a file when using `-service` mode. (#303)\n\n- Example systemd unit file now better arranged startup order when using `sshd`\n  and other fixes. (#317, #412, #438)\n\n- Reduced memory utilization/garbage collection. (#320, #323, #340)\n\n- Reduced CPU utilization. (#329)\n\n- Build against go 1.16. (#381)\n\n- Refactored handshakes to improve performance and correctness. (#401, #402, #404, #416, #451)\n\n- Improved roaming support for mobile clients. (#394, #457)\n\n- Lighthouse performance and correctness improvements. (#406, #418, #429, #433, #437, #442, #449)\n\n- Better ordered startup to enable `sshd`, `stats`, and `dns` subsystems to listen on\n  the nebula interface. (#375)\n\n### Fixed\n\n- No longer report handshake packets as `lost` in stats. (#331)\n\n- Error handling in the `cert` package. (#339, #373)\n\n- Orphaned pending hostmap entries are cleaned up. (#344)\n\n- Most known data races are now resolved. (#396, #400, #424)\n\n- Refuse to run a lighthouse on an ephemeral port. (#399)\n\n- Removed the global references. (#423, #426, #446)\n\n- Reloading via ssh command avoids a panic. (#447)\n\n- Shutdown is now performed in a cleaner way. (#448)\n\n- Logs will now find their way to Windows event viewer when running under `-service` mode\n  in Windows. (#443)\n\n## [1.3.0] - 2020-09-22\n\n### Added\n\n- You can emit statistics about non-message packets by setting the option\n  `stats.message_metrics`. You can similarly emit detailed statistics about\n  lighthouse packets by setting the option `stats.lighthouse_metrics`. See\n  the example config for more details. (#230)\n\n- We now support freebsd/amd64. This is experimental, please give us feedback.\n  (#103)\n\n- We now release a binary for `linux/mips-softfloat` which has also been\n  stripped to reduce filesize and hopefully have a better chance on running on\n  small mips devices. (#231)\n\n- You can set `tun.disabled` to true to run a standalone lighthouse without a\n  tun device (and thus, without root). (#269)\n\n- You can set `logging.disable_timestamp` to remove timestamps from log lines,\n  which is useful when output is redirected to a logging system that already\n  adds timestamps. (#288)\n\n### Changed\n\n- Handshakes should now trigger faster, as we try to be proactive with sending\n  them instead of waiting for the next timer tick in most cases. (#246, #265)\n\n- Previously, we would drop the conntrack table whenever firewall rules were\n  changed during a SIGHUP. Now, we will maintain the table and just validate\n  that an entry still matches with the new rule set. (#233)\n\n- Debug logs for firewall drops now include the reason. (#220, #239)\n\n- Logs for handshakes now include the fingerprint of the remote host. (#262)\n\n- Config item `pki.blacklist` is now `pki.blocklist`. (#272)\n\n- Better support for older Linux kernels. We now only set `SO_REUSEPORT` if\n  `tun.routines` is greater than 1 (default is 1). We also only use the\n  `recvmmsg` syscall if `listen.batch` is greater than 1 (default is 64).\n  (#275)\n\n- It is possible to run Nebula as a library inside of another process now.\n  Note that this is still experimental and the internal APIs around this might\n  change in minor version releases. (#279)\n\n### Deprecated\n\n- `pki.blacklist` is deprecated in favor of `pki.blocklist` with the same\n   functionality. Existing configs will continue to load for this release to\n   allow for migrations. (#272)\n\n### Fixed\n\n- `advmss` is now set correctly for each route table entry when `tun.routes`\n  is configured to have some routes with higher MTU. (#245)\n\n- Packets that arrive on the tun device with an unroutable destination IP are\n  now dropped correctly, instead of wasting time making queries to the\n  lighthouses for IP `0.0.0.0` (#267)\n\n## [1.2.0] - 2020-04-08\n\n### Added\n\n- Add `logging.timestamp_format` config option. The primary purpose of this\n  change is to allow logging timestamps with millisecond precision. (#187)\n\n- Support `unsafe_routes` on Windows. (#184)\n\n- Add `lighthouse.remote_allow_list` to filter which subnets we will use to\n  handshake with other hosts. See the example config for more details. (#217)\n\n- Add `lighthouse.local_allow_list` to filter which local IP addresses and/or\n  interfaces we advertise to the lighthouses. See the example config for more\n  details. (#217)\n\n- Wireshark dissector plugin. Add this file in `dist/wireshark` to your\n  Wireshark plugins folder to see Nebula packet headers decoded. (#216)\n\n- systemd unit for Arch, so it can be built entirely from this repo. (#216)\n\n### Changed\n\n- Added a delay to punching via lighthouse signal to deal with race conditions\n  in some linux conntrack implementations. (#210)\n\n  See deprecated, this also adds a new `punchy.delay` option that defaults to `1s`.\n\n- Validate all `lighthouse.hosts` and `static_host_map` VPN IPs are in the\n  subnet defined in our cert. Exit with a fatal error if they are not in our\n  subnet, as this is an invalid configuration (we will not have the proper\n  routes set up to communicate with these hosts). (#170)\n\n- Use absolute paths to system binaries on macOS and Windows. (#191)\n\n- Add configuration options for `handshakes`. This includes options to tweak\n  `try_interval`, `retries` and `wait_rotation`. See example config for\n  descriptions. (#179)\n\n- Allow `-config` file to not end in `.yaml` or `yml`. Useful when using\n  `-test` and automated tools like Ansible that create temporary files without\n  suffixes. (#189)\n\n- The config test mode, `-test`, is now more thorough and catches more parsing\n  issues. (#177)\n\n- Various documentation and example fixes. (#196)\n\n- Improved log messages. (#181, #200)\n\n- Dependencies updated. (#188)\n\n### Deprecated\n\n- `punchy`, `punch_back` configuration options have been collapsed under the\n  now top level `punchy` config directive. (#210)\n\n  `punchy.punch` - This is the old `punchy` option. Should we perform NAT hole\n  punching (default false)?\n\n  `punchy.respond` - This is the old `punch_back` option. Should we respond to\n  hole punching by hole punching back (default false)?\n\n### Fixed\n\n- Reduce memory allocations when not using `unsafe_routes`. (#198)\n\n- Ignore packets from self to self. (#192)\n\n- MTU fixed for `unsafe_routes`. (#209)\n\n## [1.1.0] - 2020-01-17\n\n### Added\n\n- For macOS and Windows, build a special version of the binary that can install\n  and manage its own service configuration. You can use this with `nebula\n  -service`.  If you are building from source, use `make service` to build this feature.\n- Support for `mips`, `mips64`, `386` and `ppc64le` processors on Linux.\n- You can now configure the DNS listen host and port with `lighthouse.dns.host`\n  and `lighthouse.dns.port`.\n- Subnet and routing support. You can now add a `unsafe_routes` section to your\n  config to allow hosts to act as gateways to other subnets. Read the example\n  config for more details. This is supported on Linux and macOS.\n\n### Changed\n\n- Certificates now have more verifications performed, including making sure\n  the certificate lifespan does not exceed the lifespan of the root CA. This\n  could cause issues if you have signed certificates with expirations beyond\n  the expiration of your CA, and you will need to reissue your certificates.\n- If lighthouse interval is set to `0`, never update the lighthouse (mobile\n  optimization).\n- Various documentation and example fixes.\n- Improved error messages.\n- Dependencies updated.\n\n### Fixed\n\n- If you have a firewall rule with `group: [\"one-group\"]`, this will\n  now be accepted, with a warning to use `group: \"one-group\"` instead.\n- The `listen.host` configuration option was previously ignored (the bind host\n  was always 0.0.0.0). This option will now be honored.\n- The `ca_sha` and `ca_name` firewall rule options should now work correctly.\n\n## [1.0.0] - 2019-11-19\n\n### Added\n\n- Initial public release.\n\n[Unreleased]: https://github.com/slackhq/nebula/compare/v1.9.4...HEAD\n[1.9.4]: https://github.com/slackhq/nebula/releases/tag/v1.9.4\n[1.9.3]: https://github.com/slackhq/nebula/releases/tag/v1.9.3\n[1.9.2]: https://github.com/slackhq/nebula/releases/tag/v1.9.2\n[1.9.1]: https://github.com/slackhq/nebula/releases/tag/v1.9.1\n[1.9.0]: https://github.com/slackhq/nebula/releases/tag/v1.9.0\n[1.8.2]: https://github.com/slackhq/nebula/releases/tag/v1.8.2\n[1.8.1]: https://github.com/slackhq/nebula/releases/tag/v1.8.1\n[1.8.0]: https://github.com/slackhq/nebula/releases/tag/v1.8.0\n[1.7.2]: https://github.com/slackhq/nebula/releases/tag/v1.7.2\n[1.7.1]: https://github.com/slackhq/nebula/releases/tag/v1.7.1\n[1.7.0]: https://github.com/slackhq/nebula/releases/tag/v1.7.0\n[1.6.1]: https://github.com/slackhq/nebula/releases/tag/v1.6.1\n[1.6.0]: https://github.com/slackhq/nebula/releases/tag/v1.6.0\n[1.5.2]: https://github.com/slackhq/nebula/releases/tag/v1.5.2\n[1.5.0]: https://github.com/slackhq/nebula/releases/tag/v1.5.0\n[1.4.0]: https://github.com/slackhq/nebula/releases/tag/v1.4.0\n[1.3.0]: https://github.com/slackhq/nebula/releases/tag/v1.3.0\n[1.2.0]: https://github.com/slackhq/nebula/releases/tag/v1.2.0\n[1.1.0]: https://github.com/slackhq/nebula/releases/tag/v1.1.0\n[1.0.0]: https://github.com/slackhq/nebula/releases/tag/v1.0.0\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0625,
          "content": "MIT License\n\nCopyright (c) 2018-2019 Slack Technologies, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n"
        },
        {
          "name": "LOGGING.md",
          "type": "blob",
          "size": 1.59765625,
          "content": "### Logging conventions\n\nA log message (the string/format passed to `Info`, `Error`, `Debug` etc, as well as their `Sprintf` counterparts) should\nbe a descriptive message about the event and may contain specific identifying characteristics. Regardless of the\nlevel of detail in the message identifying characteristics should always be included via `WithField`, `WithFields` or\n`WithError`\n\nIf an error is being logged use `l.WithError(err)` so that there is better discoverability about the event as well\nas the specific error condition.\n\n#### Common fields\n\n- `cert` - a `cert.NebulaCertificate` object, do not `.String()` this manually, `logrus` will marshal objects properly\n  for the formatter it is using.\n- `fingerprint` - a single `NebeulaCertificate` hex encoded fingerprint\n- `fingerprints` - an array of `NebulaCertificate` hex encoded fingerprints\n- `fwPacket` - a FirewallPacket object\n- `handshake` - an object containing:\n    - `stage` - the current stage counter\n    - `style` - noise handshake style `ix_psk0`, `xx`, etc\n- `header` - a nebula header object\n- `udpAddr` - a `net.UDPAddr` object\n- `udpIp` - a udp ip address\n- `vpnIp` - vpn ip of the host (remote or local)\n- `relay` - the vpnIp of the relay host that is or should be handling the relay packet\n- `relayFrom` - The vpnIp of the initial sender of the relayed packet \n- `relayTo` - The vpnIp of the final destination of a relayed packet\n\n#### Example:\n\n```\nl.WithError(err).\n    WithField(\"vpnIp\", IntIp(hostinfo.hostId)).\n    WithField(\"udpAddr\", addr).\n    WithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix\"}).\n    Info(\"Invalid certificate from host\")\n```"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 6.6435546875,
          "content": "NEBULA_CMD_PATH = \"./cmd/nebula\"\nCGO_ENABLED = 0\nexport CGO_ENABLED\n\n# Set up OS specific bits\nifeq ($(OS),Windows_NT)\n\tNEBULA_CMD_SUFFIX = .exe\n\tNULL_FILE = nul\n\t# RIO on windows does pointer stuff that makes go vet angry\n\tVET_FLAGS = -unsafeptr=false\nelse\n\tNEBULA_CMD_SUFFIX =\n\tNULL_FILE = /dev/null\nendif\n\n# Only defined the build number if we haven't already\nifndef BUILD_NUMBER\n\tifeq ($(shell git describe --exact-match 2>$(NULL_FILE)),)\n\t\tBUILD_NUMBER = $(shell git describe --abbrev=0 --match \"v*\" | cut -dv -f2)-$(shell git branch --show-current)-$(shell git describe --long --dirty | cut -d- -f2-)\n\telse\n\t\tBUILD_NUMBER = $(shell git describe --exact-match --dirty | cut -dv -f2)\n\tendif\nendif\n\nDOCKER_IMAGE_REPO ?= nebulaoss/nebula\nDOCKER_IMAGE_TAG ?= latest\n\nLDFLAGS = -X main.Build=$(BUILD_NUMBER)\n\nALL_LINUX = linux-amd64 \\\n\tlinux-386 \\\n\tlinux-ppc64le \\\n\tlinux-arm-5 \\\n\tlinux-arm-6 \\\n\tlinux-arm-7 \\\n\tlinux-arm64 \\\n\tlinux-mips \\\n\tlinux-mipsle \\\n\tlinux-mips64 \\\n\tlinux-mips64le \\\n\tlinux-mips-softfloat \\\n\tlinux-riscv64 \\\n\tlinux-loong64\n\nALL_FREEBSD = freebsd-amd64 \\\n\tfreebsd-arm64\n\nALL_OPENBSD = openbsd-amd64 \\\n\topenbsd-arm64\n\nALL_NETBSD = netbsd-amd64 \\\n \tnetbsd-arm64\n\nALL = $(ALL_LINUX) \\\n\t$(ALL_FREEBSD) \\\n\t$(ALL_OPENBSD) \\\n\t$(ALL_NETBSD) \\\n\tdarwin-amd64 \\\n\tdarwin-arm64 \\\n\twindows-amd64 \\\n\twindows-arm64\n\ne2e:\n\t$(TEST_ENV) go test -tags=e2e_testing -count=1 $(TEST_FLAGS) ./e2e\n\ne2ev: TEST_FLAGS += -v\ne2ev: e2e\n\ne2evv: TEST_ENV += TEST_LOGS=1\ne2evv: e2ev\n\ne2evvv: TEST_ENV += TEST_LOGS=2\ne2evvv: e2ev\n\ne2evvvv: TEST_ENV += TEST_LOGS=3\ne2evvvv: e2ev\n\ne2e-bench: TEST_FLAGS = -bench=. -benchmem -run=^$\ne2e-bench: e2e\n\nDOCKER_BIN = build/linux-amd64/nebula build/linux-amd64/nebula-cert\n\nall: $(ALL:%=build/%/nebula) $(ALL:%=build/%/nebula-cert)\n\ndocker: docker/linux-$(shell go env GOARCH)\n\nrelease: $(ALL:%=build/nebula-%.tar.gz)\n\nrelease-linux: $(ALL_LINUX:%=build/nebula-%.tar.gz)\n\nrelease-freebsd: $(ALL_FREEBSD:%=build/nebula-%.tar.gz)\n\nrelease-openbsd: $(ALL_OPENBSD:%=build/nebula-%.tar.gz)\n\nrelease-netbsd: $(ALL_NETBSD:%=build/nebula-%.tar.gz)\n\nrelease-boringcrypto: build/nebula-linux-$(shell go env GOARCH)-boringcrypto.tar.gz\n\nBUILD_ARGS += -trimpath\n\nbin-windows: build/windows-amd64/nebula.exe build/windows-amd64/nebula-cert.exe\n\tmv $? .\n\nbin-windows-arm64: build/windows-arm64/nebula.exe build/windows-arm64/nebula-cert.exe\n\tmv $? .\n\nbin-darwin: build/darwin-amd64/nebula build/darwin-amd64/nebula-cert\n\tmv $? .\n\nbin-freebsd: build/freebsd-amd64/nebula build/freebsd-amd64/nebula-cert\n\tmv $? .\n\nbin-freebsd-arm64: build/freebsd-arm64/nebula build/freebsd-arm64/nebula-cert\n\tmv $? .\n\nbin-boringcrypto: build/linux-$(shell go env GOARCH)-boringcrypto/nebula build/linux-$(shell go env GOARCH)-boringcrypto/nebula-cert\n\tmv $? .\n\nbin-pkcs11: BUILD_ARGS += -tags pkcs11\nbin-pkcs11: CGO_ENABLED = 1\nbin-pkcs11: bin\n\nbin:\n\tgo build $(BUILD_ARGS) -ldflags \"$(LDFLAGS)\" -o ./nebula${NEBULA_CMD_SUFFIX} ${NEBULA_CMD_PATH}\n\tgo build $(BUILD_ARGS) -ldflags \"$(LDFLAGS)\" -o ./nebula-cert${NEBULA_CMD_SUFFIX} ./cmd/nebula-cert\n\ninstall:\n\tgo install $(BUILD_ARGS) -ldflags \"$(LDFLAGS)\" ${NEBULA_CMD_PATH}\n\tgo install $(BUILD_ARGS) -ldflags \"$(LDFLAGS)\" ./cmd/nebula-cert\n\nbuild/linux-arm-%: GOENV += GOARM=$(word 3, $(subst -, ,$*))\nbuild/linux-mips-%: GOENV += GOMIPS=$(word 3, $(subst -, ,$*))\n\n# Build an extra small binary for mips-softfloat\nbuild/linux-mips-softfloat/%: LDFLAGS += -s -w\n\n# boringcrypto\nbuild/linux-amd64-boringcrypto/%: GOENV += GOEXPERIMENT=boringcrypto CGO_ENABLED=1\nbuild/linux-arm64-boringcrypto/%: GOENV += GOEXPERIMENT=boringcrypto CGO_ENABLED=1\n\nbuild/%/nebula: .FORCE\n\tGOOS=$(firstword $(subst -, , $*)) \\\n\t\tGOARCH=$(word 2, $(subst -, ,$*)) $(GOENV) \\\n\t\tgo build $(BUILD_ARGS) -o $@ -ldflags \"$(LDFLAGS)\" ${NEBULA_CMD_PATH}\n\nbuild/%/nebula-cert: .FORCE\n\tGOOS=$(firstword $(subst -, , $*)) \\\n\t\tGOARCH=$(word 2, $(subst -, ,$*)) $(GOENV) \\\n\t\tgo build $(BUILD_ARGS) -o $@ -ldflags \"$(LDFLAGS)\" ./cmd/nebula-cert\n\nbuild/%/nebula.exe: build/%/nebula\n\tmv $< $@\n\nbuild/%/nebula-cert.exe: build/%/nebula-cert\n\tmv $< $@\n\nbuild/nebula-%.tar.gz: build/%/nebula build/%/nebula-cert\n\ttar -zcv -C build/$* -f $@ nebula nebula-cert\n\nbuild/nebula-%.zip: build/%/nebula.exe build/%/nebula-cert.exe\n\tcd build/$* && zip ../nebula-$*.zip nebula.exe nebula-cert.exe\n\ndocker/%: build/%/nebula build/%/nebula-cert\n\tdocker build . $(DOCKER_BUILD_ARGS) -f docker/Dockerfile --platform \"$(subst -,/,$*)\" --tag \"${DOCKER_IMAGE_REPO}:${DOCKER_IMAGE_TAG}\" --tag \"${DOCKER_IMAGE_REPO}:$(BUILD_NUMBER)\"\n\nvet:\n\tgo vet $(VET_FLAGS) -v ./...\n\ntest:\n\tgo test -v ./...\n\ntest-boringcrypto:\n\tGOEXPERIMENT=boringcrypto CGO_ENABLED=1 go test -v ./...\n\ntest-pkcs11:\n\tCGO_ENABLED=1 go test -v -tags pkcs11 ./...\n\ntest-cov-html:\n\tgo test -coverprofile=coverage.out\n\tgo tool cover -html=coverage.out\n\nbuild-test-mobile:\n\tGOARCH=amd64 GOOS=ios go build $(shell go list ./... | grep -v '/cmd/\\|/examples/')\n\tGOARCH=arm64 GOOS=ios go build $(shell go list ./... | grep -v '/cmd/\\|/examples/')\n\tGOARCH=amd64 GOOS=android go build $(shell go list ./... | grep -v '/cmd/\\|/examples/')\n\tGOARCH=arm64 GOOS=android go build $(shell go list ./... | grep -v '/cmd/\\|/examples/')\n\nbench:\n\tgo test -bench=.\n\nbench-cpu:\n\tgo test -bench=. -benchtime=5s -cpuprofile=cpu.pprof\n\tgo tool pprof go-audit.test cpu.pprof\n\nbench-cpu-long:\n\tgo test -bench=. -benchtime=60s -cpuprofile=cpu.pprof\n\tgo tool pprof go-audit.test cpu.pprof\n\nproto: nebula.pb.go cert/cert.pb.go\n\nnebula.pb.go: nebula.proto .FORCE\n\tgo build github.com/gogo/protobuf/protoc-gen-gogofaster\n\tPATH=\"$(CURDIR):$(PATH)\" protoc --gogofaster_out=paths=source_relative:. $<\n\trm protoc-gen-gogofaster\n\ncert/cert.pb.go: cert/cert.proto .FORCE\n\t$(MAKE) -C cert cert.pb.go\n\nservice:\n\t@echo > $(NULL_FILE)\n\t$(eval NEBULA_CMD_PATH := \"./cmd/nebula-service\")\nifeq ($(words $(MAKECMDGOALS)),1)\n\t@$(MAKE) service ${.DEFAULT_GOAL} --no-print-directory\nendif\n\nbin-docker: bin build/linux-amd64/nebula build/linux-amd64/nebula-cert\n\nsmoke-docker: bin-docker\n\tcd .github/workflows/smoke/ && ./build.sh\n\tcd .github/workflows/smoke/ && ./smoke.sh\n\tcd .github/workflows/smoke/ && NAME=\"smoke-p256\" CURVE=\"P256\" ./build.sh\n\tcd .github/workflows/smoke/ && NAME=\"smoke-p256\" ./smoke.sh\n\nsmoke-relay-docker: bin-docker\n\tcd .github/workflows/smoke/ && ./build-relay.sh\n\tcd .github/workflows/smoke/ && ./smoke-relay.sh\n\nsmoke-docker-race: BUILD_ARGS = -race\nsmoke-docker-race: CGO_ENABLED = 1\nsmoke-docker-race: smoke-docker\n\nsmoke-vagrant/%: bin-docker build/%/nebula\n\tcd .github/workflows/smoke/ && ./build.sh $*\n\tcd .github/workflows/smoke/ && ./smoke-vagrant.sh $*\n\n.FORCE:\n.PHONY: bench bench-cpu bench-cpu-long bin build-test-mobile e2e e2ev e2evv e2evvv e2evvvv proto release service smoke-docker smoke-docker-race test test-cov-html smoke-vagrant/%\n.DEFAULT_GOAL := bin\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.5615234375,
          "content": "## What is Nebula?\nNebula is a scalable overlay networking tool with a focus on performance, simplicity and security.\nIt lets you seamlessly connect computers anywhere in the world. Nebula is portable, and runs on Linux, OSX, Windows, iOS, and Android.\nIt can be used to connect a small number of computers, but is also able to connect tens of thousands of computers.\n\nNebula incorporates a number of existing concepts like encryption, security groups, certificates,\nand tunneling, and each of those individual pieces existed before Nebula in various forms.\nWhat makes Nebula different to existing offerings is that it brings all of these ideas together,\nresulting in a sum that is greater than its individual parts.\n\nFurther documentation can be found [here](https://nebula.defined.net/docs/).\n\nYou can read more about Nebula [here](https://medium.com/p/884110a5579).\n\nYou can also join the NebulaOSS Slack group [here](https://join.slack.com/t/nebulaoss/shared_invite/enQtOTA5MDI4NDg3MTg4LTkwY2EwNTI4NzQyMzc0M2ZlODBjNWI3NTY1MzhiOThiMmZlZjVkMTI0NGY4YTMyNjUwMWEyNzNkZTJmYzQxOGU).\n\n## Supported Platforms\n\n#### Desktop and Server\n\nCheck the [releases](https://github.com/slackhq/nebula/releases/latest) page for downloads or see the [Distribution Packages](https://github.com/slackhq/nebula#distribution-packages) section.\n\n- Linux - 64 and 32 bit, arm, and others\n- Windows\n- MacOS\n- Freebsd\n\n#### Distribution Packages\n\n- [Arch Linux](https://archlinux.org/packages/extra/x86_64/nebula/)\n    ```\n    $ sudo pacman -S nebula\n    ```\n\n- [Fedora Linux](https://src.fedoraproject.org/rpms/nebula)\n    ```\n    $ sudo dnf install nebula\n    ```\n\n- [Debian Linux](https://packages.debian.org/source/stable/nebula)\n    ```\n    $ sudo apt install nebula\n    ```\n\n- [Alpine Linux](https://pkgs.alpinelinux.org/packages?name=nebula)\n    ```\n    $ sudo apk add nebula\n    ```\n\n- [macOS Homebrew](https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/nebula.rb)\n    ```\n    $ brew install nebula\n    ```\n\n- [Docker](https://hub.docker.com/r/nebulaoss/nebula)\n    ```\n    $ docker pull nebulaoss/nebula\n    ```\n\n#### Mobile\n\n- [iOS](https://apps.apple.com/us/app/mobile-nebula/id1509587936?itsct=apps_box&amp;itscg=30200)\n- [Android](https://play.google.com/store/apps/details?id=net.defined.mobile_nebula&pcampaignid=pcampaignidMKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1)\n\n## Technical Overview\n\nNebula is a mutually authenticated peer-to-peer software defined network based on the [Noise Protocol Framework](https://noiseprotocol.org/).\nNebula uses certificates to assert a node's IP address, name, and membership within user-defined groups.\nNebula's user-defined groups allow for provider agnostic traffic filtering between nodes.\nDiscovery nodes allow individual peers to find each other and optionally use UDP hole punching to establish connections from behind most firewalls or NATs.\nUsers can move data between nodes in any number of cloud service providers, datacenters, and endpoints, without needing to maintain a particular addressing scheme.\n\nNebula uses Elliptic-curve Diffie-Hellman (`ECDH`) key exchange and `AES-256-GCM` in its default configuration.\n\nNebula was created to provide a mechanism for groups of hosts to communicate securely, even across the internet, while enabling expressive firewall definitions similar in style to cloud security groups.\n\n## Getting started (quickly)\n\nTo set up a Nebula network, you'll need:\n\n#### 1. The [Nebula binaries](https://github.com/slackhq/nebula/releases) or [Distribution Packages](https://github.com/slackhq/nebula#distribution-packages) for your specific platform. Specifically you'll need `nebula-cert` and the specific nebula binary for each platform you use.\n\n#### 2. (Optional, but you really should..) At least one discovery node with a routable IP address, which we call a lighthouse.\n\nNebula lighthouses allow nodes to find each other, anywhere in the world. A lighthouse is the only node in a Nebula network whose IP should not change. Running a lighthouse requires very few compute resources, and you can easily use the least expensive option from a cloud hosting provider. If you're not sure which provider to use, a number of us have used $5/mo [DigitalOcean](https://digitalocean.com) droplets as lighthouses.\n\n  Once you have launched an instance, ensure that Nebula udp traffic (default port udp/4242) can reach it over the internet.\n\n\n#### 3. A Nebula certificate authority, which will be the root of trust for a particular Nebula network.\n\n  ```\n  ./nebula-cert ca -name \"Myorganization, Inc\"\n  ```\n  This will create files named `ca.key` and `ca.cert` in the current directory. The `ca.key` file is the most sensitive file you'll create, because it is the key used to sign the certificates for individual nebula nodes/hosts. Please store this file somewhere safe, preferably with strong encryption.\n\n#### 4. Nebula host keys and certificates generated from that certificate authority\nThis assumes you have four nodes, named lighthouse1, laptop, server1, host3. You can name the nodes any way you'd like, including FQDN. You'll also need to choose IP addresses and the associated subnet. In this example, we are creating a nebula network that will use 192.168.100.x/24 as its network range. This example also demonstrates nebula groups, which can later be used to define traffic rules in a nebula network.\n```\n./nebula-cert sign -name \"lighthouse1\" -ip \"192.168.100.1/24\"\n./nebula-cert sign -name \"laptop\" -ip \"192.168.100.2/24\" -groups \"laptop,home,ssh\"\n./nebula-cert sign -name \"server1\" -ip \"192.168.100.9/24\" -groups \"servers\"\n./nebula-cert sign -name \"host3\" -ip \"192.168.100.10/24\"\n```\n\n#### 5. Configuration files for each host\nDownload a copy of the nebula [example configuration](https://github.com/slackhq/nebula/blob/master/examples/config.yml).\n\n* On the lighthouse node, you'll need to ensure `am_lighthouse: true` is set.\n\n* On the individual hosts, ensure the lighthouse is defined properly in the `static_host_map` section, and is added to the lighthouse `hosts` section.\n\n\n#### 6. Copy nebula credentials, configuration, and binaries to each host\n\nFor each host, copy the nebula binary to the host, along with `config.yml` from step 5, and the files `ca.crt`, `{host}.crt`, and `{host}.key` from step 4.\n\n**DO NOT COPY `ca.key` TO INDIVIDUAL NODES.**\n\n#### 7. Run nebula on each host\n```\n./nebula -config /path/to/config.yml\n```\n\n## Building Nebula from source\n\nMake sure you have [go](https://go.dev/doc/install) installed and clone this repo. Change to the nebula directory.\n\nTo build nebula for all platforms:\n`make all`\n\nTo build nebula for a specific platform (ex, Windows):\n`make bin-windows`\n\nSee the [Makefile](Makefile) for more details on build targets\n\n## Curve P256 and BoringCrypto\n\nThe default curve used for cryptographic handshakes and signatures is Curve25519. This is the recommended setting for most users. If your deployment has certain compliance requirements, you have the option of creating your CA using `nebula-cert ca -curve P256` to use NIST Curve P256. The CA will then sign certificates using ECDSA P256, and any hosts using these certificates will use P256 for ECDH handshakes.\n\nIn addition, Nebula can be built using the [BoringCrypto GOEXPERIMENT](https://github.com/golang/go/blob/go1.20/src/crypto/internal/boring/README.md) by running either of the following make targets:\n\n    make bin-boringcrypto\n    make release-boringcrypto\n\nThis is not the recommended default deployment, but may be useful based on your compliance requirements.\n\n## Credits\n\nNebula was created at Slack Technologies, Inc by Nate Brown and Ryan Huber, with contributions from Oliver Fross, Alan Lam, Wade Simmons, and Lining Wang.\n\n\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.380859375,
          "content": "Security Policy\n===============\n\nReporting a Vulnerability\n-------------------------\n\nIf you believe you have found a security vulnerability with Nebula, please let\nus know right away. We will investigate all reports and do our best to quickly\nfix valid issues.\n\nYou can submit your report on [HackerOne](https://hackerone.com/slack) and our\nsecurity team will respond as soon as possible.\n"
        },
        {
          "name": "allow_list.go",
          "type": "blob",
          "size": 7.466796875,
          "content": "package nebula\n\nimport (\n\t\"fmt\"\n\t\"net/netip\"\n\t\"regexp\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\ntype AllowList struct {\n\t// The values of this cidrTree are `bool`, signifying allow/deny\n\tcidrTree *bart.Table[bool]\n}\n\ntype RemoteAllowList struct {\n\tAllowList *AllowList\n\n\t// Inside Range Specific, keys of this tree are inside CIDRs and values\n\t// are *AllowList\n\tinsideAllowLists *bart.Table[*AllowList]\n}\n\ntype LocalAllowList struct {\n\tAllowList *AllowList\n\n\t// To avoid ambiguity, all rules must be true, or all rules must be false.\n\tnameRules []AllowListNameRule\n}\n\ntype AllowListNameRule struct {\n\tName  *regexp.Regexp\n\tAllow bool\n}\n\nfunc NewLocalAllowListFromConfig(c *config.C, k string) (*LocalAllowList, error) {\n\tvar nameRules []AllowListNameRule\n\thandleKey := func(key string, value interface{}) (bool, error) {\n\t\tif key == \"interfaces\" {\n\t\t\tvar err error\n\t\t\tnameRules, err = getAllowListInterfaces(k, value)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\n\t\t\treturn true, nil\n\t\t}\n\t\treturn false, nil\n\t}\n\n\tal, err := newAllowListFromConfig(c, k, handleKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &LocalAllowList{AllowList: al, nameRules: nameRules}, nil\n}\n\nfunc NewRemoteAllowListFromConfig(c *config.C, k, rangesKey string) (*RemoteAllowList, error) {\n\tal, err := newAllowListFromConfig(c, k, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tremoteAllowRanges, err := getRemoteAllowRanges(c, rangesKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &RemoteAllowList{AllowList: al, insideAllowLists: remoteAllowRanges}, nil\n}\n\n// If the handleKey func returns true, the rest of the parsing is skipped\n// for this key. This allows parsing of special values like `interfaces`.\nfunc newAllowListFromConfig(c *config.C, k string, handleKey func(key string, value interface{}) (bool, error)) (*AllowList, error) {\n\tr := c.Get(k)\n\tif r == nil {\n\t\treturn nil, nil\n\t}\n\n\treturn newAllowList(k, r, handleKey)\n}\n\n// If the handleKey func returns true, the rest of the parsing is skipped\n// for this key. This allows parsing of special values like `interfaces`.\nfunc newAllowList(k string, raw interface{}, handleKey func(key string, value interface{}) (bool, error)) (*AllowList, error) {\n\trawMap, ok := raw.(map[interface{}]interface{})\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"config `%s` has invalid type: %T\", k, raw)\n\t}\n\n\ttree := new(bart.Table[bool])\n\n\t// Keep track of the rules we have added for both ipv4 and ipv6\n\ttype allowListRules struct {\n\t\tfirstValue     bool\n\t\tallValuesMatch bool\n\t\tdefaultSet     bool\n\t\tallValues      bool\n\t}\n\n\trules4 := allowListRules{firstValue: true, allValuesMatch: true, defaultSet: false}\n\trules6 := allowListRules{firstValue: true, allValuesMatch: true, defaultSet: false}\n\n\tfor rawKey, rawValue := range rawMap {\n\t\trawCIDR, ok := rawKey.(string)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid key (type %T): %v\", k, rawKey, rawKey)\n\t\t}\n\n\t\tif handleKey != nil {\n\t\t\thandled, err := handleKey(rawCIDR, rawValue)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif handled {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tvalue, ok := rawValue.(bool)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid value (type %T): %v\", k, rawValue, rawValue)\n\t\t}\n\n\t\tipNet, err := netip.ParsePrefix(rawCIDR)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid CIDR: %s. %w\", k, rawCIDR, err)\n\t\t}\n\n\t\tipNet = netip.PrefixFrom(ipNet.Addr().Unmap(), ipNet.Bits())\n\n\t\t// TODO: should we error on duplicate CIDRs in the config?\n\t\ttree.Insert(ipNet, value)\n\n\t\tmaskBits := ipNet.Bits()\n\n\t\tvar rules *allowListRules\n\t\tif ipNet.Addr().Is4() {\n\t\t\trules = &rules4\n\t\t} else {\n\t\t\trules = &rules6\n\t\t}\n\n\t\tif rules.firstValue {\n\t\t\trules.allValues = value\n\t\t\trules.firstValue = false\n\t\t} else {\n\t\t\tif value != rules.allValues {\n\t\t\t\trules.allValuesMatch = false\n\t\t\t}\n\t\t}\n\n\t\t// Check if this is 0.0.0.0/0 or ::/0\n\t\tif maskBits == 0 {\n\t\t\trules.defaultSet = true\n\t\t}\n\t}\n\n\tif !rules4.defaultSet {\n\t\tif rules4.allValuesMatch {\n\t\t\ttree.Insert(netip.PrefixFrom(netip.IPv4Unspecified(), 0), !rules4.allValues)\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` contains both true and false rules, but no default set for 0.0.0.0/0\", k)\n\t\t}\n\t}\n\n\tif !rules6.defaultSet {\n\t\tif rules6.allValuesMatch {\n\t\t\ttree.Insert(netip.PrefixFrom(netip.IPv6Unspecified(), 0), !rules6.allValues)\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` contains both true and false rules, but no default set for ::/0\", k)\n\t\t}\n\t}\n\n\treturn &AllowList{cidrTree: tree}, nil\n}\n\nfunc getAllowListInterfaces(k string, v interface{}) ([]AllowListNameRule, error) {\n\tvar nameRules []AllowListNameRule\n\n\trawRules, ok := v.(map[interface{}]interface{})\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"config `%s.interfaces` is invalid (type %T): %v\", k, v, v)\n\t}\n\n\tfirstEntry := true\n\tvar allValues bool\n\tfor rawName, rawAllow := range rawRules {\n\t\tname, ok := rawName.(string)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s.interfaces` has invalid key (type %T): %v\", k, rawName, rawName)\n\t\t}\n\t\tallow, ok := rawAllow.(bool)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s.interfaces` has invalid value (type %T): %v\", k, rawAllow, rawAllow)\n\t\t}\n\n\t\tnameRE, err := regexp.Compile(\"^\" + name + \"$\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"config `%s.interfaces` has invalid key: %s: %v\", k, name, err)\n\t\t}\n\n\t\tnameRules = append(nameRules, AllowListNameRule{\n\t\t\tName:  nameRE,\n\t\t\tAllow: allow,\n\t\t})\n\n\t\tif firstEntry {\n\t\t\tallValues = allow\n\t\t\tfirstEntry = false\n\t\t} else {\n\t\t\tif allow != allValues {\n\t\t\t\treturn nil, fmt.Errorf(\"config `%s.interfaces` values must all be the same true/false value\", k)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nameRules, nil\n}\n\nfunc getRemoteAllowRanges(c *config.C, k string) (*bart.Table[*AllowList], error) {\n\tvalue := c.Get(k)\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tremoteAllowRanges := new(bart.Table[*AllowList])\n\n\trawMap, ok := value.(map[interface{}]interface{})\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"config `%s` has invalid type: %T\", k, value)\n\t}\n\tfor rawKey, rawValue := range rawMap {\n\t\trawCIDR, ok := rawKey.(string)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid key (type %T): %v\", k, rawKey, rawKey)\n\t\t}\n\n\t\tallowList, err := newAllowList(fmt.Sprintf(\"%s.%s\", k, rawCIDR), rawValue, nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tipNet, err := netip.ParsePrefix(rawCIDR)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid CIDR: %s. %w\", k, rawCIDR, err)\n\t\t}\n\n\t\tremoteAllowRanges.Insert(netip.PrefixFrom(ipNet.Addr().Unmap(), ipNet.Bits()), allowList)\n\t}\n\n\treturn remoteAllowRanges, nil\n}\n\nfunc (al *AllowList) Allow(ip netip.Addr) bool {\n\tif al == nil {\n\t\treturn true\n\t}\n\n\tresult, _ := al.cidrTree.Lookup(ip)\n\treturn result\n}\n\nfunc (al *LocalAllowList) Allow(ip netip.Addr) bool {\n\tif al == nil {\n\t\treturn true\n\t}\n\treturn al.AllowList.Allow(ip)\n}\n\nfunc (al *LocalAllowList) AllowName(name string) bool {\n\tif al == nil || len(al.nameRules) == 0 {\n\t\treturn true\n\t}\n\n\tfor _, rule := range al.nameRules {\n\t\tif rule.Name.MatchString(name) {\n\t\t\treturn rule.Allow\n\t\t}\n\t}\n\n\t// If no rules match, return the default, which is the inverse of the rules\n\treturn !al.nameRules[0].Allow\n}\n\nfunc (al *RemoteAllowList) AllowUnknownVpnIp(ip netip.Addr) bool {\n\tif al == nil {\n\t\treturn true\n\t}\n\treturn al.AllowList.Allow(ip)\n}\n\nfunc (al *RemoteAllowList) Allow(vpnIp netip.Addr, ip netip.Addr) bool {\n\tif !al.getInsideAllowList(vpnIp).Allow(ip) {\n\t\treturn false\n\t}\n\treturn al.AllowList.Allow(ip)\n}\n\nfunc (al *RemoteAllowList) getInsideAllowList(vpnIp netip.Addr) *AllowList {\n\tif al.insideAllowLists != nil {\n\t\tinside, ok := al.insideAllowLists.Lookup(vpnIp)\n\t\tif ok {\n\t\t\treturn inside\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "allow_list_test.go",
          "type": "blob",
          "size": 4.65234375,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"regexp\"\n\t\"testing\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNewAllowListFromConfig(t *testing.T) {\n\tl := test.NewLogger()\n\tc := config.NewC(l)\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"192.168.0.0\": true,\n\t}\n\tr, err := newAllowListFromConfig(c, \"allowlist\", nil)\n\tassert.EqualError(t, err, \"config `allowlist` has invalid CIDR: 192.168.0.0. netip.ParsePrefix(\\\"192.168.0.0\\\"): no '/'\")\n\tassert.Nil(t, r)\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"192.168.0.0/16\": \"abc\",\n\t}\n\tr, err = newAllowListFromConfig(c, \"allowlist\", nil)\n\tassert.EqualError(t, err, \"config `allowlist` has invalid value (type string): abc\")\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"192.168.0.0/16\": true,\n\t\t\"10.0.0.0/8\":     false,\n\t}\n\tr, err = newAllowListFromConfig(c, \"allowlist\", nil)\n\tassert.EqualError(t, err, \"config `allowlist` contains both true and false rules, but no default set for 0.0.0.0/0\")\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"0.0.0.0/0\":      true,\n\t\t\"10.0.0.0/8\":     false,\n\t\t\"10.42.42.0/24\":  true,\n\t\t\"fd00::/8\":       true,\n\t\t\"fd00:fd00::/16\": false,\n\t}\n\tr, err = newAllowListFromConfig(c, \"allowlist\", nil)\n\tassert.EqualError(t, err, \"config `allowlist` contains both true and false rules, but no default set for ::/0\")\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"0.0.0.0/0\":     true,\n\t\t\"10.0.0.0/8\":    false,\n\t\t\"10.42.42.0/24\": true,\n\t}\n\tr, err = newAllowListFromConfig(c, \"allowlist\", nil)\n\tif assert.NoError(t, err) {\n\t\tassert.NotNil(t, r)\n\t}\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"0.0.0.0/0\":      true,\n\t\t\"10.0.0.0/8\":     false,\n\t\t\"10.42.42.0/24\":  true,\n\t\t\"::/0\":           false,\n\t\t\"fd00::/8\":       true,\n\t\t\"fd00:fd00::/16\": false,\n\t}\n\tr, err = newAllowListFromConfig(c, \"allowlist\", nil)\n\tif assert.NoError(t, err) {\n\t\tassert.NotNil(t, r)\n\t}\n\n\t// Test interface names\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"interfaces\": map[interface{}]interface{}{\n\t\t\t`docker.*`: \"foo\",\n\t\t},\n\t}\n\tlr, err := NewLocalAllowListFromConfig(c, \"allowlist\")\n\tassert.EqualError(t, err, \"config `allowlist.interfaces` has invalid value (type string): foo\")\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"interfaces\": map[interface{}]interface{}{\n\t\t\t`docker.*`: false,\n\t\t\t`eth.*`:    true,\n\t\t},\n\t}\n\tlr, err = NewLocalAllowListFromConfig(c, \"allowlist\")\n\tassert.EqualError(t, err, \"config `allowlist.interfaces` values must all be the same true/false value\")\n\n\tc.Settings[\"allowlist\"] = map[interface{}]interface{}{\n\t\t\"interfaces\": map[interface{}]interface{}{\n\t\t\t`docker.*`: false,\n\t\t},\n\t}\n\tlr, err = NewLocalAllowListFromConfig(c, \"allowlist\")\n\tif assert.NoError(t, err) {\n\t\tassert.NotNil(t, lr)\n\t}\n}\n\nfunc TestAllowList_Allow(t *testing.T) {\n\tassert.Equal(t, true, ((*AllowList)(nil)).Allow(netip.MustParseAddr(\"1.1.1.1\")))\n\n\ttree := new(bart.Table[bool])\n\ttree.Insert(netip.MustParsePrefix(\"0.0.0.0/0\"), true)\n\ttree.Insert(netip.MustParsePrefix(\"10.0.0.0/8\"), false)\n\ttree.Insert(netip.MustParsePrefix(\"10.42.42.42/32\"), true)\n\ttree.Insert(netip.MustParsePrefix(\"10.42.0.0/16\"), true)\n\ttree.Insert(netip.MustParsePrefix(\"10.42.42.0/24\"), true)\n\ttree.Insert(netip.MustParsePrefix(\"10.42.42.0/24\"), false)\n\ttree.Insert(netip.MustParsePrefix(\"::1/128\"), true)\n\ttree.Insert(netip.MustParsePrefix(\"::2/128\"), false)\n\tal := &AllowList{cidrTree: tree}\n\n\tassert.Equal(t, true, al.Allow(netip.MustParseAddr(\"1.1.1.1\")))\n\tassert.Equal(t, false, al.Allow(netip.MustParseAddr(\"10.0.0.4\")))\n\tassert.Equal(t, true, al.Allow(netip.MustParseAddr(\"10.42.42.42\")))\n\tassert.Equal(t, false, al.Allow(netip.MustParseAddr(\"10.42.42.41\")))\n\tassert.Equal(t, true, al.Allow(netip.MustParseAddr(\"10.42.0.1\")))\n\tassert.Equal(t, true, al.Allow(netip.MustParseAddr(\"::1\")))\n\tassert.Equal(t, false, al.Allow(netip.MustParseAddr(\"::2\")))\n}\n\nfunc TestLocalAllowList_AllowName(t *testing.T) {\n\tassert.Equal(t, true, ((*LocalAllowList)(nil)).AllowName(\"docker0\"))\n\n\trules := []AllowListNameRule{\n\t\t{Name: regexp.MustCompile(\"^docker.*$\"), Allow: false},\n\t\t{Name: regexp.MustCompile(\"^tun.*$\"), Allow: false},\n\t}\n\tal := &LocalAllowList{nameRules: rules}\n\n\tassert.Equal(t, false, al.AllowName(\"docker0\"))\n\tassert.Equal(t, false, al.AllowName(\"tun0\"))\n\tassert.Equal(t, true, al.AllowName(\"eth0\"))\n\n\trules = []AllowListNameRule{\n\t\t{Name: regexp.MustCompile(\"^eth.*$\"), Allow: true},\n\t\t{Name: regexp.MustCompile(\"^ens.*$\"), Allow: true},\n\t}\n\tal = &LocalAllowList{nameRules: rules}\n\n\tassert.Equal(t, false, al.AllowName(\"docker0\"))\n\tassert.Equal(t, true, al.AllowName(\"eth0\"))\n\tassert.Equal(t, true, al.AllowName(\"ens5\"))\n}\n"
        },
        {
          "name": "bits.go",
          "type": "blob",
          "size": 4.53515625,
          "content": "package nebula\n\nimport (\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n)\n\ntype Bits struct {\n\tlength             uint64\n\tcurrent            uint64\n\tbits               []bool\n\tfirstSeen          bool\n\tlostCounter        metrics.Counter\n\tdupeCounter        metrics.Counter\n\toutOfWindowCounter metrics.Counter\n}\n\nfunc NewBits(bits uint64) *Bits {\n\treturn &Bits{\n\t\tlength:             bits,\n\t\tbits:               make([]bool, bits, bits),\n\t\tcurrent:            0,\n\t\tlostCounter:        metrics.GetOrRegisterCounter(\"network.packets.lost\", nil),\n\t\tdupeCounter:        metrics.GetOrRegisterCounter(\"network.packets.duplicate\", nil),\n\t\toutOfWindowCounter: metrics.GetOrRegisterCounter(\"network.packets.out_of_window\", nil),\n\t}\n}\n\nfunc (b *Bits) Check(l logrus.FieldLogger, i uint64) bool {\n\t// If i is the next number, return true.\n\tif i > b.current || (i == 0 && b.firstSeen == false && b.current < b.length) {\n\t\treturn true\n\t}\n\n\t// If i is within the window, check if it's been set already. The first window will fail this check\n\tif i > b.current-b.length {\n\t\treturn !b.bits[i%b.length]\n\t}\n\n\t// If i is within the first window\n\tif i < b.length {\n\t\treturn !b.bits[i%b.length]\n\t}\n\n\t// Not within the window\n\tl.Debugf(\"rejected a packet (top) %d %d\\n\", b.current, i)\n\treturn false\n}\n\nfunc (b *Bits) Update(l *logrus.Logger, i uint64) bool {\n\t// If i is the next number, return true and update current.\n\tif i == b.current+1 {\n\t\t// Report missed packets, we can only understand what was missed after the first window has been gone through\n\t\tif i > b.length && b.bits[i%b.length] == false {\n\t\t\tb.lostCounter.Inc(1)\n\t\t}\n\t\tb.bits[i%b.length] = true\n\t\tb.current = i\n\t\treturn true\n\t}\n\n\t// If i packet is greater than current but less than the maximum length of our bitmap,\n\t// flip everything in between to false and move ahead.\n\tif i > b.current && i < b.current+b.length {\n\t\t// In between current and i need to be zero'd to allow those packets to come in later\n\t\tfor n := b.current + 1; n < i; n++ {\n\t\t\tb.bits[n%b.length] = false\n\t\t}\n\n\t\tb.bits[i%b.length] = true\n\t\tb.current = i\n\t\t//l.Debugf(\"missed %d packets between %d and %d\\n\", i-b.current, i, b.current)\n\t\treturn true\n\t}\n\n\t// If i is greater than the delta between current and the total length of our bitmap,\n\t// just flip everything in the map and move ahead.\n\tif i >= b.current+b.length {\n\t\t// The current window loss will be accounted for later, only record the jump as loss up until then\n\t\tlost := maxInt64(0, int64(i-b.current-b.length))\n\t\t//TODO: explain this\n\t\tif b.current == 0 {\n\t\t\tlost++\n\t\t}\n\n\t\tfor n := range b.bits {\n\t\t\t// Don't want to count the first window as a loss\n\t\t\t//TODO: this is likely wrong, we are wanting to track only the bit slots that we aren't going to track anymore and this is marking everything as missed\n\t\t\t//if b.bits[n] == false {\n\t\t\t//\tlost++\n\t\t\t//}\n\t\t\tb.bits[n] = false\n\t\t}\n\n\t\tb.lostCounter.Inc(lost)\n\n\t\tif l.Level >= logrus.DebugLevel {\n\t\t\tl.WithField(\"receiveWindow\", m{\"accepted\": true, \"currentCounter\": b.current, \"incomingCounter\": i, \"reason\": \"window shifting\"}).\n\t\t\t\tDebug(\"Receive window\")\n\t\t}\n\t\tb.bits[i%b.length] = true\n\t\tb.current = i\n\t\treturn true\n\t}\n\n\t// Allow for the 0 packet to come in within the first window\n\tif i == 0 && b.firstSeen == false && b.current < b.length {\n\t\tb.firstSeen = true\n\t\tb.bits[i%b.length] = true\n\t\treturn true\n\t}\n\n\t// If i is within the window of current minus length (the total pat window size),\n\t// allow it and flip to true but to NOT change current. We also have to account for the first window\n\tif ((b.current >= b.length && i > b.current-b.length) || (b.current < b.length && i < b.length)) && i <= b.current {\n\t\tif b.current == i {\n\t\t\tif l.Level >= logrus.DebugLevel {\n\t\t\t\tl.WithField(\"receiveWindow\", m{\"accepted\": false, \"currentCounter\": b.current, \"incomingCounter\": i, \"reason\": \"duplicate\"}).\n\t\t\t\t\tDebug(\"Receive window\")\n\t\t\t}\n\t\t\tb.dupeCounter.Inc(1)\n\t\t\treturn false\n\t\t}\n\n\t\tif b.bits[i%b.length] == true {\n\t\t\tif l.Level >= logrus.DebugLevel {\n\t\t\t\tl.WithField(\"receiveWindow\", m{\"accepted\": false, \"currentCounter\": b.current, \"incomingCounter\": i, \"reason\": \"old duplicate\"}).\n\t\t\t\t\tDebug(\"Receive window\")\n\t\t\t}\n\t\t\tb.dupeCounter.Inc(1)\n\t\t\treturn false\n\t\t}\n\n\t\tb.bits[i%b.length] = true\n\t\treturn true\n\n\t}\n\n\t// In all other cases, fail and don't change current.\n\tb.outOfWindowCounter.Inc(1)\n\tif l.Level >= logrus.DebugLevel {\n\t\tl.WithField(\"accepted\", false).\n\t\t\tWithField(\"currentCounter\", b.current).\n\t\t\tWithField(\"incomingCounter\", i).\n\t\t\tWithField(\"reason\", \"nonsense\").\n\t\t\tDebug(\"Receive window\")\n\t}\n\treturn false\n}\n\nfunc maxInt64(a, b int64) int64 {\n\tif a > b {\n\t\treturn a\n\t}\n\n\treturn b\n}\n"
        },
        {
          "name": "bits_test.go",
          "type": "blob",
          "size": 6.88671875,
          "content": "package nebula\n\nimport (\n\t\"testing\"\n\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestBits(t *testing.T) {\n\tl := test.NewLogger()\n\tb := NewBits(10)\n\n\t// make sure it is the right size\n\tassert.Len(t, b.bits, 10)\n\n\t// This is initialized to zero - receive one. This should work.\n\n\tassert.True(t, b.Check(l, 1))\n\tu := b.Update(l, 1)\n\tassert.True(t, u)\n\tassert.EqualValues(t, 1, b.current)\n\tg := []bool{false, true, false, false, false, false, false, false, false, false}\n\tassert.Equal(t, g, b.bits)\n\n\t// Receive two\n\tassert.True(t, b.Check(l, 2))\n\tu = b.Update(l, 2)\n\tassert.True(t, u)\n\tassert.EqualValues(t, 2, b.current)\n\tg = []bool{false, true, true, false, false, false, false, false, false, false}\n\tassert.Equal(t, g, b.bits)\n\n\t// Receive two again - it will fail\n\tassert.False(t, b.Check(l, 2))\n\tu = b.Update(l, 2)\n\tassert.False(t, u)\n\tassert.EqualValues(t, 2, b.current)\n\n\t// Jump ahead to 15, which should clear everything and set the 6th element\n\tassert.True(t, b.Check(l, 15))\n\tu = b.Update(l, 15)\n\tassert.True(t, u)\n\tassert.EqualValues(t, 15, b.current)\n\tg = []bool{false, false, false, false, false, true, false, false, false, false}\n\tassert.Equal(t, g, b.bits)\n\n\t// Mark 14, which is allowed because it is in the window\n\tassert.True(t, b.Check(l, 14))\n\tu = b.Update(l, 14)\n\tassert.True(t, u)\n\tassert.EqualValues(t, 15, b.current)\n\tg = []bool{false, false, false, false, true, true, false, false, false, false}\n\tassert.Equal(t, g, b.bits)\n\n\t// Mark 5, which is not allowed because it is not in the window\n\tassert.False(t, b.Check(l, 5))\n\tu = b.Update(l, 5)\n\tassert.False(t, u)\n\tassert.EqualValues(t, 15, b.current)\n\tg = []bool{false, false, false, false, true, true, false, false, false, false}\n\tassert.Equal(t, g, b.bits)\n\n\t// make sure we handle wrapping around once to the current position\n\tb = NewBits(10)\n\tassert.True(t, b.Update(l, 1))\n\tassert.True(t, b.Update(l, 11))\n\tassert.Equal(t, []bool{false, true, false, false, false, false, false, false, false, false}, b.bits)\n\n\t// Walk through a few windows in order\n\tb = NewBits(10)\n\tfor i := uint64(0); i <= 100; i++ {\n\t\tassert.True(t, b.Check(l, i), \"Error while checking %v\", i)\n\t\tassert.True(t, b.Update(l, i), \"Error while updating %v\", i)\n\t}\n}\n\nfunc TestBitsDupeCounter(t *testing.T) {\n\tl := test.NewLogger()\n\tb := NewBits(10)\n\tb.lostCounter.Clear()\n\tb.dupeCounter.Clear()\n\tb.outOfWindowCounter.Clear()\n\n\tassert.True(t, b.Update(l, 1))\n\tassert.Equal(t, int64(0), b.dupeCounter.Count())\n\n\tassert.False(t, b.Update(l, 1))\n\tassert.Equal(t, int64(1), b.dupeCounter.Count())\n\n\tassert.True(t, b.Update(l, 2))\n\tassert.Equal(t, int64(1), b.dupeCounter.Count())\n\n\tassert.True(t, b.Update(l, 3))\n\tassert.Equal(t, int64(1), b.dupeCounter.Count())\n\n\tassert.False(t, b.Update(l, 1))\n\tassert.Equal(t, int64(0), b.lostCounter.Count())\n\tassert.Equal(t, int64(2), b.dupeCounter.Count())\n\tassert.Equal(t, int64(0), b.outOfWindowCounter.Count())\n}\n\nfunc TestBitsOutOfWindowCounter(t *testing.T) {\n\tl := test.NewLogger()\n\tb := NewBits(10)\n\tb.lostCounter.Clear()\n\tb.dupeCounter.Clear()\n\tb.outOfWindowCounter.Clear()\n\n\tassert.True(t, b.Update(l, 20))\n\tassert.Equal(t, int64(0), b.outOfWindowCounter.Count())\n\n\tassert.True(t, b.Update(l, 21))\n\tassert.True(t, b.Update(l, 22))\n\tassert.True(t, b.Update(l, 23))\n\tassert.True(t, b.Update(l, 24))\n\tassert.True(t, b.Update(l, 25))\n\tassert.True(t, b.Update(l, 26))\n\tassert.True(t, b.Update(l, 27))\n\tassert.True(t, b.Update(l, 28))\n\tassert.True(t, b.Update(l, 29))\n\tassert.Equal(t, int64(0), b.outOfWindowCounter.Count())\n\n\tassert.False(t, b.Update(l, 0))\n\tassert.Equal(t, int64(1), b.outOfWindowCounter.Count())\n\n\t//tODO: make sure lostcounter doesn't increase in orderly increment\n\tassert.Equal(t, int64(20), b.lostCounter.Count())\n\tassert.Equal(t, int64(0), b.dupeCounter.Count())\n\tassert.Equal(t, int64(1), b.outOfWindowCounter.Count())\n}\n\nfunc TestBitsLostCounter(t *testing.T) {\n\tl := test.NewLogger()\n\tb := NewBits(10)\n\tb.lostCounter.Clear()\n\tb.dupeCounter.Clear()\n\tb.outOfWindowCounter.Clear()\n\n\t//assert.True(t, b.Update(0))\n\tassert.True(t, b.Update(l, 0))\n\tassert.True(t, b.Update(l, 20))\n\tassert.True(t, b.Update(l, 21))\n\tassert.True(t, b.Update(l, 22))\n\tassert.True(t, b.Update(l, 23))\n\tassert.True(t, b.Update(l, 24))\n\tassert.True(t, b.Update(l, 25))\n\tassert.True(t, b.Update(l, 26))\n\tassert.True(t, b.Update(l, 27))\n\tassert.True(t, b.Update(l, 28))\n\tassert.True(t, b.Update(l, 29))\n\tassert.Equal(t, int64(20), b.lostCounter.Count())\n\tassert.Equal(t, int64(0), b.dupeCounter.Count())\n\tassert.Equal(t, int64(0), b.outOfWindowCounter.Count())\n\n\tb = NewBits(10)\n\tb.lostCounter.Clear()\n\tb.dupeCounter.Clear()\n\tb.outOfWindowCounter.Clear()\n\n\tassert.True(t, b.Update(l, 0))\n\tassert.Equal(t, int64(0), b.lostCounter.Count())\n\tassert.True(t, b.Update(l, 9))\n\tassert.Equal(t, int64(0), b.lostCounter.Count())\n\t// 10 will set 0 index, 0 was already set, no lost packets\n\tassert.True(t, b.Update(l, 10))\n\tassert.Equal(t, int64(0), b.lostCounter.Count())\n\t// 11 will set 1 index, 1 was missed, we should see 1 packet lost\n\tassert.True(t, b.Update(l, 11))\n\tassert.Equal(t, int64(1), b.lostCounter.Count())\n\t// Now let's fill in the window, should end up with 8 lost packets\n\tassert.True(t, b.Update(l, 12))\n\tassert.True(t, b.Update(l, 13))\n\tassert.True(t, b.Update(l, 14))\n\tassert.True(t, b.Update(l, 15))\n\tassert.True(t, b.Update(l, 16))\n\tassert.True(t, b.Update(l, 17))\n\tassert.True(t, b.Update(l, 18))\n\tassert.True(t, b.Update(l, 19))\n\tassert.Equal(t, int64(8), b.lostCounter.Count())\n\n\t// Jump ahead by a window size\n\tassert.True(t, b.Update(l, 29))\n\tassert.Equal(t, int64(8), b.lostCounter.Count())\n\t// Now lets walk ahead normally through the window, the missed packets should fill in\n\tassert.True(t, b.Update(l, 30))\n\tassert.True(t, b.Update(l, 31))\n\tassert.True(t, b.Update(l, 32))\n\tassert.True(t, b.Update(l, 33))\n\tassert.True(t, b.Update(l, 34))\n\tassert.True(t, b.Update(l, 35))\n\tassert.True(t, b.Update(l, 36))\n\tassert.True(t, b.Update(l, 37))\n\tassert.True(t, b.Update(l, 38))\n\t// 39 packets tracked, 22 seen, 17 lost\n\tassert.Equal(t, int64(17), b.lostCounter.Count())\n\n\t// Jump ahead by 2 windows, should have recording 1 full window missing\n\tassert.True(t, b.Update(l, 58))\n\tassert.Equal(t, int64(27), b.lostCounter.Count())\n\t// Now lets walk ahead normally through the window, the missed packets should fill in from this window\n\tassert.True(t, b.Update(l, 59))\n\tassert.True(t, b.Update(l, 60))\n\tassert.True(t, b.Update(l, 61))\n\tassert.True(t, b.Update(l, 62))\n\tassert.True(t, b.Update(l, 63))\n\tassert.True(t, b.Update(l, 64))\n\tassert.True(t, b.Update(l, 65))\n\tassert.True(t, b.Update(l, 66))\n\tassert.True(t, b.Update(l, 67))\n\t// 68 packets tracked, 32 seen, 36 missed\n\tassert.Equal(t, int64(36), b.lostCounter.Count())\n\tassert.Equal(t, int64(0), b.dupeCounter.Count())\n\tassert.Equal(t, int64(0), b.outOfWindowCounter.Count())\n}\n\nfunc BenchmarkBits(b *testing.B) {\n\tz := NewBits(10)\n\tfor n := 0; n < b.N; n++ {\n\t\tfor i := range z.bits {\n\t\t\tz.bits[i] = true\n\t\t}\n\t\tfor i := range z.bits {\n\t\t\tz.bits[i] = false\n\t\t}\n\n\t}\n}\n"
        },
        {
          "name": "boring.go",
          "type": "blob",
          "size": 0.1201171875,
          "content": "//go:build boringcrypto\n// +build boringcrypto\n\npackage nebula\n\nimport \"crypto/boring\"\n\nvar boringEnabled = boring.Enabled\n"
        },
        {
          "name": "calculated_remote.go",
          "type": "blob",
          "size": 3.8935546875,
          "content": "package nebula\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/netip\"\n\t\"strconv\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\n// This allows us to \"guess\" what the remote might be for a host while we wait\n// for the lighthouse response. See \"lighthouse.calculated_remotes\" in the\n// example config file.\ntype calculatedRemote struct {\n\tipNet netip.Prefix\n\tmask  netip.Prefix\n\tport  uint32\n}\n\nfunc newCalculatedRemote(maskCidr netip.Prefix, port int) (*calculatedRemote, error) {\n\tmasked := maskCidr.Masked()\n\tif port < 0 || port > math.MaxUint16 {\n\t\treturn nil, fmt.Errorf(\"invalid port: %d\", port)\n\t}\n\n\treturn &calculatedRemote{\n\t\tipNet: maskCidr,\n\t\tmask:  masked,\n\t\tport:  uint32(port),\n\t}, nil\n}\n\nfunc (c *calculatedRemote) String() string {\n\treturn fmt.Sprintf(\"CalculatedRemote(mask=%v port=%d)\", c.ipNet, c.port)\n}\n\nfunc (c *calculatedRemote) Apply(ip netip.Addr) *Ip4AndPort {\n\t// Combine the masked bytes of the \"mask\" IP with the unmasked bytes\n\t// of the overlay IP\n\tif c.ipNet.Addr().Is4() {\n\t\treturn c.apply4(ip)\n\t}\n\treturn c.apply6(ip)\n}\n\nfunc (c *calculatedRemote) apply4(ip netip.Addr) *Ip4AndPort {\n\t//TODO: IPV6-WORK this can be less crappy\n\tmaskb := net.CIDRMask(c.mask.Bits(), c.mask.Addr().BitLen())\n\tmask := binary.BigEndian.Uint32(maskb[:])\n\n\tb := c.mask.Addr().As4()\n\tmaskIp := binary.BigEndian.Uint32(b[:])\n\n\tb = ip.As4()\n\tintIp := binary.BigEndian.Uint32(b[:])\n\n\treturn &Ip4AndPort{(maskIp & mask) | (intIp & ^mask), c.port}\n}\n\nfunc (c *calculatedRemote) apply6(ip netip.Addr) *Ip4AndPort {\n\t//TODO: IPV6-WORK\n\tpanic(\"Can not calculate ipv6 remote addresses\")\n}\n\nfunc NewCalculatedRemotesFromConfig(c *config.C, k string) (*bart.Table[[]*calculatedRemote], error) {\n\tvalue := c.Get(k)\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tcalculatedRemotes := new(bart.Table[[]*calculatedRemote])\n\n\trawMap, ok := value.(map[any]any)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"config `%s` has invalid type: %T\", k, value)\n\t}\n\tfor rawKey, rawValue := range rawMap {\n\t\trawCIDR, ok := rawKey.(string)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid key (type %T): %v\", k, rawKey, rawKey)\n\t\t}\n\n\t\tcidr, err := netip.ParsePrefix(rawCIDR)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"config `%s` has invalid CIDR: %s\", k, rawCIDR)\n\t\t}\n\n\t\t//TODO: IPV6-WORK this does not verify that rawValue contains the same bits as cidr here\n\t\tentry, err := newCalculatedRemotesListFromConfig(rawValue)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"config '%s.%s': %w\", k, rawCIDR, err)\n\t\t}\n\n\t\tcalculatedRemotes.Insert(cidr, entry)\n\t}\n\n\treturn calculatedRemotes, nil\n}\n\nfunc newCalculatedRemotesListFromConfig(raw any) ([]*calculatedRemote, error) {\n\trawList, ok := raw.([]any)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"calculated_remotes entry has invalid type: %T\", raw)\n\t}\n\n\tvar l []*calculatedRemote\n\tfor _, e := range rawList {\n\t\tc, err := newCalculatedRemotesEntryFromConfig(e)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"calculated_remotes entry: %w\", err)\n\t\t}\n\t\tl = append(l, c)\n\t}\n\n\treturn l, nil\n}\n\nfunc newCalculatedRemotesEntryFromConfig(raw any) (*calculatedRemote, error) {\n\trawMap, ok := raw.(map[any]any)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"invalid type: %T\", raw)\n\t}\n\n\trawValue := rawMap[\"mask\"]\n\tif rawValue == nil {\n\t\treturn nil, fmt.Errorf(\"missing mask: %v\", rawMap)\n\t}\n\trawMask, ok := rawValue.(string)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"invalid mask (type %T): %v\", rawValue, rawValue)\n\t}\n\tmaskCidr, err := netip.ParsePrefix(rawMask)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid mask: %s\", rawMask)\n\t}\n\n\tvar port int\n\trawValue = rawMap[\"port\"]\n\tif rawValue == nil {\n\t\treturn nil, fmt.Errorf(\"missing port: %v\", rawMap)\n\t}\n\tswitch v := rawValue.(type) {\n\tcase int:\n\t\tport = v\n\tcase string:\n\t\tport, err = strconv.Atoi(v)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid port: %s: %w\", v, err)\n\t\t}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"invalid port (type %T): %v\", rawValue, rawValue)\n\t}\n\n\treturn newCalculatedRemote(maskCidr, port)\n}\n"
        },
        {
          "name": "calculated_remote_test.go",
          "type": "blob",
          "size": 0.533203125,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCalculatedRemoteApply(t *testing.T) {\n\tipNet, err := netip.ParsePrefix(\"192.168.1.0/24\")\n\trequire.NoError(t, err)\n\n\tc, err := newCalculatedRemote(ipNet, 4242)\n\trequire.NoError(t, err)\n\n\tinput, err := netip.ParseAddr(\"10.0.10.182\")\n\tassert.NoError(t, err)\n\n\texpected, err := netip.ParseAddr(\"192.168.1.182\")\n\tassert.NoError(t, err)\n\n\tassert.Equal(t, NewIp4AndPortFromNetIP(expected, 4242), c.Apply(input))\n}\n"
        },
        {
          "name": "cert",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "connection_manager.go",
          "type": "blob",
          "size": 13.9228515625,
          "content": "package nebula\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"net/netip\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/header\"\n)\n\ntype trafficDecision int\n\nconst (\n\tdoNothing      trafficDecision = 0\n\tdeleteTunnel   trafficDecision = 1 // delete the hostinfo on our side, do not notify the remote\n\tcloseTunnel    trafficDecision = 2 // delete the hostinfo and notify the remote\n\tswapPrimary    trafficDecision = 3\n\tmigrateRelays  trafficDecision = 4\n\ttryRehandshake trafficDecision = 5\n\tsendTestPacket trafficDecision = 6\n)\n\ntype connectionManager struct {\n\tin     map[uint32]struct{}\n\tinLock *sync.RWMutex\n\n\tout     map[uint32]struct{}\n\toutLock *sync.RWMutex\n\n\t// relayUsed holds which relay localIndexs are in use\n\trelayUsed     map[uint32]struct{}\n\trelayUsedLock *sync.RWMutex\n\n\thostMap                 *HostMap\n\ttrafficTimer            *LockingTimerWheel[uint32]\n\tintf                    *Interface\n\tpendingDeletion         map[uint32]struct{}\n\tpunchy                  *Punchy\n\tcheckInterval           time.Duration\n\tpendingDeletionInterval time.Duration\n\tmetricsTxPunchy         metrics.Counter\n\n\tl *logrus.Logger\n}\n\nfunc newConnectionManager(ctx context.Context, l *logrus.Logger, intf *Interface, checkInterval, pendingDeletionInterval time.Duration, punchy *Punchy) *connectionManager {\n\tvar max time.Duration\n\tif checkInterval < pendingDeletionInterval {\n\t\tmax = pendingDeletionInterval\n\t} else {\n\t\tmax = checkInterval\n\t}\n\n\tnc := &connectionManager{\n\t\thostMap:                 intf.hostMap,\n\t\tin:                      make(map[uint32]struct{}),\n\t\tinLock:                  &sync.RWMutex{},\n\t\tout:                     make(map[uint32]struct{}),\n\t\toutLock:                 &sync.RWMutex{},\n\t\trelayUsed:               make(map[uint32]struct{}),\n\t\trelayUsedLock:           &sync.RWMutex{},\n\t\ttrafficTimer:            NewLockingTimerWheel[uint32](time.Millisecond*500, max),\n\t\tintf:                    intf,\n\t\tpendingDeletion:         make(map[uint32]struct{}),\n\t\tcheckInterval:           checkInterval,\n\t\tpendingDeletionInterval: pendingDeletionInterval,\n\t\tpunchy:                  punchy,\n\t\tmetricsTxPunchy:         metrics.GetOrRegisterCounter(\"messages.tx.punchy\", nil),\n\t\tl:                       l,\n\t}\n\n\tnc.Start(ctx)\n\treturn nc\n}\n\nfunc (n *connectionManager) In(localIndex uint32) {\n\tn.inLock.RLock()\n\t// If this already exists, return\n\tif _, ok := n.in[localIndex]; ok {\n\t\tn.inLock.RUnlock()\n\t\treturn\n\t}\n\tn.inLock.RUnlock()\n\tn.inLock.Lock()\n\tn.in[localIndex] = struct{}{}\n\tn.inLock.Unlock()\n}\n\nfunc (n *connectionManager) Out(localIndex uint32) {\n\tn.outLock.RLock()\n\t// If this already exists, return\n\tif _, ok := n.out[localIndex]; ok {\n\t\tn.outLock.RUnlock()\n\t\treturn\n\t}\n\tn.outLock.RUnlock()\n\tn.outLock.Lock()\n\tn.out[localIndex] = struct{}{}\n\tn.outLock.Unlock()\n}\n\nfunc (n *connectionManager) RelayUsed(localIndex uint32) {\n\tn.relayUsedLock.RLock()\n\t// If this already exists, return\n\tif _, ok := n.relayUsed[localIndex]; ok {\n\t\tn.relayUsedLock.RUnlock()\n\t\treturn\n\t}\n\tn.relayUsedLock.RUnlock()\n\tn.relayUsedLock.Lock()\n\tn.relayUsed[localIndex] = struct{}{}\n\tn.relayUsedLock.Unlock()\n}\n\n// getAndResetTrafficCheck returns if there was any inbound or outbound traffic within the last tick and\n// resets the state for this local index\nfunc (n *connectionManager) getAndResetTrafficCheck(localIndex uint32) (bool, bool) {\n\tn.inLock.Lock()\n\tn.outLock.Lock()\n\t_, in := n.in[localIndex]\n\t_, out := n.out[localIndex]\n\tdelete(n.in, localIndex)\n\tdelete(n.out, localIndex)\n\tn.inLock.Unlock()\n\tn.outLock.Unlock()\n\treturn in, out\n}\n\nfunc (n *connectionManager) AddTrafficWatch(localIndex uint32) {\n\t// Use a write lock directly because it should be incredibly rare that we are ever already tracking this index\n\tn.outLock.Lock()\n\tif _, ok := n.out[localIndex]; ok {\n\t\tn.outLock.Unlock()\n\t\treturn\n\t}\n\tn.out[localIndex] = struct{}{}\n\tn.trafficTimer.Add(localIndex, n.checkInterval)\n\tn.outLock.Unlock()\n}\n\nfunc (n *connectionManager) Start(ctx context.Context) {\n\tgo n.Run(ctx)\n}\n\nfunc (n *connectionManager) Run(ctx context.Context) {\n\t//TODO: this tick should be based on the min wheel tick? Check firewall\n\tclockSource := time.NewTicker(500 * time.Millisecond)\n\tdefer clockSource.Stop()\n\n\tp := []byte(\"\")\n\tnb := make([]byte, 12, 12)\n\tout := make([]byte, mtu)\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\n\t\tcase now := <-clockSource.C:\n\t\t\tn.trafficTimer.Advance(now)\n\t\t\tfor {\n\t\t\t\tlocalIndex, has := n.trafficTimer.Purge()\n\t\t\t\tif !has {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tn.doTrafficCheck(localIndex, p, nb, out, now)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (n *connectionManager) doTrafficCheck(localIndex uint32, p, nb, out []byte, now time.Time) {\n\tdecision, hostinfo, primary := n.makeTrafficDecision(localIndex, now)\n\n\tswitch decision {\n\tcase deleteTunnel:\n\t\tif n.hostMap.DeleteHostInfo(hostinfo) {\n\t\t\t// Only clearing the lighthouse cache if this is the last hostinfo for this vpn ip in the hostmap\n\t\t\tn.intf.lightHouse.DeleteVpnIp(hostinfo.vpnIp)\n\t\t}\n\n\tcase closeTunnel:\n\t\tn.intf.sendCloseTunnel(hostinfo)\n\t\tn.intf.closeTunnel(hostinfo)\n\n\tcase swapPrimary:\n\t\tn.swapPrimary(hostinfo, primary)\n\n\tcase migrateRelays:\n\t\tn.migrateRelayUsed(hostinfo, primary)\n\n\tcase tryRehandshake:\n\t\tn.tryRehandshake(hostinfo)\n\n\tcase sendTestPacket:\n\t\tn.intf.SendMessageToHostInfo(header.Test, header.TestRequest, hostinfo, p, nb, out)\n\t}\n\n\tn.resetRelayTrafficCheck(hostinfo)\n}\n\nfunc (n *connectionManager) resetRelayTrafficCheck(hostinfo *HostInfo) {\n\tif hostinfo != nil {\n\t\tn.relayUsedLock.Lock()\n\t\tdefer n.relayUsedLock.Unlock()\n\t\t// No need to migrate any relays, delete usage info now.\n\t\tfor _, idx := range hostinfo.relayState.CopyRelayForIdxs() {\n\t\t\tdelete(n.relayUsed, idx)\n\t\t}\n\t}\n}\n\nfunc (n *connectionManager) migrateRelayUsed(oldhostinfo, newhostinfo *HostInfo) {\n\trelayFor := oldhostinfo.relayState.CopyAllRelayFor()\n\n\tfor _, r := range relayFor {\n\t\texisting, ok := newhostinfo.relayState.QueryRelayForByIp(r.PeerIp)\n\n\t\tvar index uint32\n\t\tvar relayFrom netip.Addr\n\t\tvar relayTo netip.Addr\n\t\tswitch {\n\t\tcase ok && existing.State == Established:\n\t\t\t// This relay already exists in newhostinfo, then do nothing.\n\t\t\tcontinue\n\t\tcase ok && existing.State == Requested:\n\t\t\t// The relay exists in a Requested state; re-send the request\n\t\t\tindex = existing.LocalIndex\n\t\t\tswitch r.Type {\n\t\t\tcase TerminalType:\n\t\t\t\trelayFrom = n.intf.myVpnNet.Addr()\n\t\t\t\trelayTo = existing.PeerIp\n\t\t\tcase ForwardingType:\n\t\t\t\trelayFrom = existing.PeerIp\n\t\t\t\trelayTo = newhostinfo.vpnIp\n\t\t\tdefault:\n\t\t\t\t// should never happen\n\t\t\t}\n\t\tcase !ok:\n\t\t\tn.relayUsedLock.RLock()\n\t\t\tif _, relayUsed := n.relayUsed[r.LocalIndex]; !relayUsed {\n\t\t\t\t// The relay hasn't been used; don't migrate it.\n\t\t\t\tn.relayUsedLock.RUnlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tn.relayUsedLock.RUnlock()\n\t\t\t// The relay doesn't exist at all; create some relay state and send the request.\n\t\t\tvar err error\n\t\t\tindex, err = AddRelay(n.l, newhostinfo, n.hostMap, r.PeerIp, nil, r.Type, Requested)\n\t\t\tif err != nil {\n\t\t\t\tn.l.WithError(err).Error(\"failed to migrate relay to new hostinfo\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch r.Type {\n\t\t\tcase TerminalType:\n\t\t\t\trelayFrom = n.intf.myVpnNet.Addr()\n\t\t\t\trelayTo = r.PeerIp\n\t\t\tcase ForwardingType:\n\t\t\t\trelayFrom = r.PeerIp\n\t\t\t\trelayTo = newhostinfo.vpnIp\n\t\t\tdefault:\n\t\t\t\t// should never happen\n\t\t\t}\n\t\t}\n\n\t\t//TODO: IPV6-WORK\n\t\trelayFromB := relayFrom.As4()\n\t\trelayToB := relayTo.As4()\n\n\t\t// Send a CreateRelayRequest to the peer.\n\t\treq := NebulaControl{\n\t\t\tType:                NebulaControl_CreateRelayRequest,\n\t\t\tInitiatorRelayIndex: index,\n\t\t\tRelayFromIp:         binary.BigEndian.Uint32(relayFromB[:]),\n\t\t\tRelayToIp:           binary.BigEndian.Uint32(relayToB[:]),\n\t\t}\n\t\tmsg, err := req.Marshal()\n\t\tif err != nil {\n\t\t\tn.l.WithError(err).Error(\"failed to marshal Control message to migrate relay\")\n\t\t} else {\n\t\t\tn.intf.SendMessageToHostInfo(header.Control, 0, newhostinfo, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\tn.l.WithFields(logrus.Fields{\n\t\t\t\t\"relayFrom\":           req.RelayFromIp,\n\t\t\t\t\"relayTo\":             req.RelayToIp,\n\t\t\t\t\"initiatorRelayIndex\": req.InitiatorRelayIndex,\n\t\t\t\t\"responderRelayIndex\": req.ResponderRelayIndex,\n\t\t\t\t\"vpnIp\":               newhostinfo.vpnIp}).\n\t\t\t\tInfo(\"send CreateRelayRequest\")\n\t\t}\n\t}\n}\n\nfunc (n *connectionManager) makeTrafficDecision(localIndex uint32, now time.Time) (trafficDecision, *HostInfo, *HostInfo) {\n\tn.hostMap.RLock()\n\tdefer n.hostMap.RUnlock()\n\n\thostinfo := n.hostMap.Indexes[localIndex]\n\tif hostinfo == nil {\n\t\tn.l.WithField(\"localIndex\", localIndex).Debugf(\"Not found in hostmap\")\n\t\tdelete(n.pendingDeletion, localIndex)\n\t\treturn doNothing, nil, nil\n\t}\n\n\tif n.isInvalidCertificate(now, hostinfo) {\n\t\tdelete(n.pendingDeletion, hostinfo.localIndexId)\n\t\treturn closeTunnel, hostinfo, nil\n\t}\n\n\tprimary := n.hostMap.Hosts[hostinfo.vpnIp]\n\tmainHostInfo := true\n\tif primary != nil && primary != hostinfo {\n\t\tmainHostInfo = false\n\t}\n\n\t// Check for traffic on this hostinfo\n\tinTraffic, outTraffic := n.getAndResetTrafficCheck(localIndex)\n\n\t// A hostinfo is determined alive if there is incoming traffic\n\tif inTraffic {\n\t\tdecision := doNothing\n\t\tif n.l.Level >= logrus.DebugLevel {\n\t\t\thostinfo.logger(n.l).\n\t\t\t\tWithField(\"tunnelCheck\", m{\"state\": \"alive\", \"method\": \"passive\"}).\n\t\t\t\tDebug(\"Tunnel status\")\n\t\t}\n\t\tdelete(n.pendingDeletion, hostinfo.localIndexId)\n\n\t\tif mainHostInfo {\n\t\t\tdecision = tryRehandshake\n\n\t\t} else {\n\t\t\tif n.shouldSwapPrimary(hostinfo, primary) {\n\t\t\t\tdecision = swapPrimary\n\t\t\t} else {\n\t\t\t\t// migrate the relays to the primary, if in use.\n\t\t\t\tdecision = migrateRelays\n\t\t\t}\n\t\t}\n\n\t\tn.trafficTimer.Add(hostinfo.localIndexId, n.checkInterval)\n\n\t\tif !outTraffic {\n\t\t\t// Send a punch packet to keep the NAT state alive\n\t\t\tn.sendPunch(hostinfo)\n\t\t}\n\n\t\treturn decision, hostinfo, primary\n\t}\n\n\tif _, ok := n.pendingDeletion[hostinfo.localIndexId]; ok {\n\t\t// We have already sent a test packet and nothing was returned, this hostinfo is dead\n\t\thostinfo.logger(n.l).\n\t\t\tWithField(\"tunnelCheck\", m{\"state\": \"dead\", \"method\": \"active\"}).\n\t\t\tInfo(\"Tunnel status\")\n\n\t\tdelete(n.pendingDeletion, hostinfo.localIndexId)\n\t\treturn deleteTunnel, hostinfo, nil\n\t}\n\n\tdecision := doNothing\n\tif hostinfo != nil && hostinfo.ConnectionState != nil && mainHostInfo {\n\t\tif !outTraffic {\n\t\t\t// If we aren't sending or receiving traffic then its an unused tunnel and we don't to test the tunnel.\n\t\t\t// Just maintain NAT state if configured to do so.\n\t\t\tn.sendPunch(hostinfo)\n\t\t\tn.trafficTimer.Add(hostinfo.localIndexId, n.checkInterval)\n\t\t\treturn doNothing, nil, nil\n\n\t\t}\n\n\t\tif n.punchy.GetTargetEverything() {\n\t\t\t// This is similar to the old punchy behavior with a slight optimization.\n\t\t\t// We aren't receiving traffic but we are sending it, punch on all known\n\t\t\t// ips in case we need to re-prime NAT state\n\t\t\tn.sendPunch(hostinfo)\n\t\t}\n\n\t\tif n.l.Level >= logrus.DebugLevel {\n\t\t\thostinfo.logger(n.l).\n\t\t\t\tWithField(\"tunnelCheck\", m{\"state\": \"testing\", \"method\": \"active\"}).\n\t\t\t\tDebug(\"Tunnel status\")\n\t\t}\n\n\t\t// Send a test packet to trigger an authenticated tunnel test, this should suss out any lingering tunnel issues\n\t\tdecision = sendTestPacket\n\n\t} else {\n\t\tif n.l.Level >= logrus.DebugLevel {\n\t\t\thostinfo.logger(n.l).Debugf(\"Hostinfo sadness\")\n\t\t}\n\t}\n\n\tn.pendingDeletion[hostinfo.localIndexId] = struct{}{}\n\tn.trafficTimer.Add(hostinfo.localIndexId, n.pendingDeletionInterval)\n\treturn decision, hostinfo, nil\n}\n\nfunc (n *connectionManager) shouldSwapPrimary(current, primary *HostInfo) bool {\n\t// The primary tunnel is the most recent handshake to complete locally and should work entirely fine.\n\t// If we are here then we have multiple tunnels for a host pair and neither side believes the same tunnel is primary.\n\t// Let's sort this out.\n\n\tif current.vpnIp.Compare(n.intf.myVpnNet.Addr()) < 0 {\n\t\t// Only one side should flip primary because if both flip then we may never resolve to a single tunnel.\n\t\t// vpn ip is static across all tunnels for this host pair so lets use that to determine who is flipping.\n\t\t// The remotes vpn ip is lower than mine. I will not flip.\n\t\treturn false\n\t}\n\n\tcertState := n.intf.pki.GetCertState()\n\treturn bytes.Equal(current.ConnectionState.myCert.Signature(), certState.Certificate.Signature())\n}\n\nfunc (n *connectionManager) swapPrimary(current, primary *HostInfo) {\n\tn.hostMap.Lock()\n\t// Make sure the primary is still the same after the write lock. This avoids a race with a rehandshake.\n\tif n.hostMap.Hosts[current.vpnIp] == primary {\n\t\tn.hostMap.unlockedMakePrimary(current)\n\t}\n\tn.hostMap.Unlock()\n}\n\n// isInvalidCertificate will check if we should destroy a tunnel if pki.disconnect_invalid is true and\n// the certificate is no longer valid. Block listed certificates will skip the pki.disconnect_invalid\n// check and return true.\nfunc (n *connectionManager) isInvalidCertificate(now time.Time, hostinfo *HostInfo) bool {\n\tremoteCert := hostinfo.GetCert()\n\tif remoteCert == nil {\n\t\treturn false\n\t}\n\n\tcaPool := n.intf.pki.GetCAPool()\n\terr := caPool.VerifyCachedCertificate(now, remoteCert)\n\tif err == nil {\n\t\treturn false\n\t}\n\n\tif !n.intf.disconnectInvalid.Load() && err != cert.ErrBlockListed {\n\t\t// Block listed certificates should always be disconnected\n\t\treturn false\n\t}\n\n\thostinfo.logger(n.l).WithError(err).\n\t\tWithField(\"fingerprint\", remoteCert.Fingerprint).\n\t\tInfo(\"Remote certificate is no longer valid, tearing down the tunnel\")\n\n\treturn true\n}\n\nfunc (n *connectionManager) sendPunch(hostinfo *HostInfo) {\n\tif !n.punchy.GetPunch() {\n\t\t// Punching is disabled\n\t\treturn\n\t}\n\n\tif n.punchy.GetTargetEverything() {\n\t\thostinfo.remotes.ForEach(n.hostMap.GetPreferredRanges(), func(addr netip.AddrPort, preferred bool) {\n\t\t\tn.metricsTxPunchy.Inc(1)\n\t\t\tn.intf.outside.WriteTo([]byte{1}, addr)\n\t\t})\n\n\t} else if hostinfo.remote.IsValid() {\n\t\tn.metricsTxPunchy.Inc(1)\n\t\tn.intf.outside.WriteTo([]byte{1}, hostinfo.remote)\n\t}\n}\n\nfunc (n *connectionManager) tryRehandshake(hostinfo *HostInfo) {\n\tcertState := n.intf.pki.GetCertState()\n\tif bytes.Equal(hostinfo.ConnectionState.myCert.Signature(), certState.Certificate.Signature()) {\n\t\treturn\n\t}\n\n\tn.l.WithField(\"vpnIp\", hostinfo.vpnIp).\n\t\tWithField(\"reason\", \"local certificate is not current\").\n\t\tInfo(\"Re-handshaking with remote\")\n\n\tn.intf.handshakeManager.StartHandshake(hostinfo.vpnIp, nil)\n}\n"
        },
        {
          "name": "connection_manager_test.go",
          "type": "blob",
          "size": 11.31640625,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"crypto/ed25519\"\n\t\"crypto/rand\"\n\t\"net/netip\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/flynn/noise\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/slackhq/nebula/udp\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc newTestLighthouse() *LightHouse {\n\tlh := &LightHouse{\n\t\tl:         test.NewLogger(),\n\t\taddrMap:   map[netip.Addr]*RemoteList{},\n\t\tqueryChan: make(chan netip.Addr, 10),\n\t}\n\tlighthouses := map[netip.Addr]struct{}{}\n\tstaticList := map[netip.Addr]struct{}{}\n\n\tlh.lighthouses.Store(&lighthouses)\n\tlh.staticList.Store(&staticList)\n\n\treturn lh\n}\n\nfunc Test_NewConnectionManagerTest(t *testing.T) {\n\tl := test.NewLogger()\n\t//_, tuncidr, _ := net.ParseCIDR(\"1.1.1.1/24\")\n\tvpncidr := netip.MustParsePrefix(\"172.1.1.1/24\")\n\tlocalrange := netip.MustParsePrefix(\"10.1.1.1/24\")\n\tvpnIp := netip.MustParseAddr(\"172.1.1.2\")\n\tpreferredRanges := []netip.Prefix{localrange}\n\n\t// Very incomplete mock objects\n\thostMap := newHostMap(l, vpncidr)\n\thostMap.preferredRanges.Store(&preferredRanges)\n\n\tcs := &CertState{\n\t\tRawCertificate:      []byte{},\n\t\tPrivateKey:          []byte{},\n\t\tCertificate:         &dummyCert{},\n\t\tRawCertificateNoKey: []byte{},\n\t}\n\n\tlh := newTestLighthouse()\n\tifce := &Interface{\n\t\thostMap:          hostMap,\n\t\tinside:           &test.NoopTun{},\n\t\toutside:          &udp.NoopConn{},\n\t\tfirewall:         &Firewall{},\n\t\tlightHouse:       lh,\n\t\tpki:              &PKI{},\n\t\thandshakeManager: NewHandshakeManager(l, hostMap, lh, &udp.NoopConn{}, defaultHandshakeConfig),\n\t\tl:                l,\n\t}\n\tifce.pki.cs.Store(cs)\n\n\t// Create manager\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tpunchy := NewPunchyFromConfig(l, config.NewC(l))\n\tnc := newConnectionManager(ctx, l, ifce, 5, 10, punchy)\n\tp := []byte(\"\")\n\tnb := make([]byte, 12, 12)\n\tout := make([]byte, mtu)\n\n\t// Add an ip we have established a connection w/ to hostmap\n\thostinfo := &HostInfo{\n\t\tvpnIp:         vpnIp,\n\t\tlocalIndexId:  1099,\n\t\tremoteIndexId: 9901,\n\t}\n\thostinfo.ConnectionState = &ConnectionState{\n\t\tmyCert: &dummyCert{},\n\t\tH:      &noise.HandshakeState{},\n\t}\n\tnc.hostMap.unlockedAddHostInfo(hostinfo, ifce)\n\n\t// We saw traffic out to vpnIp\n\tnc.Out(hostinfo.localIndexId)\n\tnc.In(hostinfo.localIndexId)\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n\tassert.Contains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n\tassert.Contains(t, nc.out, hostinfo.localIndexId)\n\n\t// Do a traffic check tick, should not be pending deletion but should not have any in/out packets recorded\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.out, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.in, hostinfo.localIndexId)\n\n\t// Do another traffic check tick, this host should be pending deletion now\n\tnc.Out(hostinfo.localIndexId)\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.Contains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.out, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.in, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n\n\t// Do a final traffic check tick, the host should now be removed\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n\tassert.NotContains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n}\n\nfunc Test_NewConnectionManagerTest2(t *testing.T) {\n\tl := test.NewLogger()\n\t//_, tuncidr, _ := net.ParseCIDR(\"1.1.1.1/24\")\n\tvpncidr := netip.MustParsePrefix(\"172.1.1.1/24\")\n\tlocalrange := netip.MustParsePrefix(\"10.1.1.1/24\")\n\tvpnIp := netip.MustParseAddr(\"172.1.1.2\")\n\tpreferredRanges := []netip.Prefix{localrange}\n\n\t// Very incomplete mock objects\n\thostMap := newHostMap(l, vpncidr)\n\thostMap.preferredRanges.Store(&preferredRanges)\n\n\tcs := &CertState{\n\t\tRawCertificate:      []byte{},\n\t\tPrivateKey:          []byte{},\n\t\tCertificate:         &dummyCert{},\n\t\tRawCertificateNoKey: []byte{},\n\t}\n\n\tlh := newTestLighthouse()\n\tifce := &Interface{\n\t\thostMap:          hostMap,\n\t\tinside:           &test.NoopTun{},\n\t\toutside:          &udp.NoopConn{},\n\t\tfirewall:         &Firewall{},\n\t\tlightHouse:       lh,\n\t\tpki:              &PKI{},\n\t\thandshakeManager: NewHandshakeManager(l, hostMap, lh, &udp.NoopConn{}, defaultHandshakeConfig),\n\t\tl:                l,\n\t}\n\tifce.pki.cs.Store(cs)\n\n\t// Create manager\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tpunchy := NewPunchyFromConfig(l, config.NewC(l))\n\tnc := newConnectionManager(ctx, l, ifce, 5, 10, punchy)\n\tp := []byte(\"\")\n\tnb := make([]byte, 12, 12)\n\tout := make([]byte, mtu)\n\n\t// Add an ip we have established a connection w/ to hostmap\n\thostinfo := &HostInfo{\n\t\tvpnIp:         vpnIp,\n\t\tlocalIndexId:  1099,\n\t\tremoteIndexId: 9901,\n\t}\n\thostinfo.ConnectionState = &ConnectionState{\n\t\tmyCert: &dummyCert{},\n\t\tH:      &noise.HandshakeState{},\n\t}\n\tnc.hostMap.unlockedAddHostInfo(hostinfo, ifce)\n\n\t// We saw traffic out to vpnIp\n\tnc.Out(hostinfo.localIndexId)\n\tnc.In(hostinfo.localIndexId)\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.vpnIp)\n\tassert.Contains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n\tassert.Contains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n\n\t// Do a traffic check tick, should not be pending deletion but should not have any in/out packets recorded\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.out, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.in, hostinfo.localIndexId)\n\n\t// Do another traffic check tick, this host should be pending deletion now\n\tnc.Out(hostinfo.localIndexId)\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.Contains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.out, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.in, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n\n\t// We saw traffic, should no longer be pending deletion\n\tnc.In(hostinfo.localIndexId)\n\tnc.doTrafficCheck(hostinfo.localIndexId, p, nb, out, time.Now())\n\tassert.NotContains(t, nc.pendingDeletion, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.out, hostinfo.localIndexId)\n\tassert.NotContains(t, nc.in, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Indexes, hostinfo.localIndexId)\n\tassert.Contains(t, nc.hostMap.Hosts, hostinfo.vpnIp)\n}\n\n// Check if we can disconnect the peer.\n// Validate if the peer's certificate is invalid (expired, etc.)\n// Disconnect only if disconnectInvalid: true is set.\nfunc Test_NewConnectionManagerTest_DisconnectInvalid(t *testing.T) {\n\tnow := time.Now()\n\tl := test.NewLogger()\n\n\tvpncidr := netip.MustParsePrefix(\"172.1.1.1/24\")\n\tlocalrange := netip.MustParsePrefix(\"10.1.1.1/24\")\n\tvpnIp := netip.MustParseAddr(\"172.1.1.2\")\n\tpreferredRanges := []netip.Prefix{localrange}\n\thostMap := newHostMap(l, vpncidr)\n\thostMap.preferredRanges.Store(&preferredRanges)\n\n\t// Generate keys for CA and peer's cert.\n\tpubCA, privCA, _ := ed25519.GenerateKey(rand.Reader)\n\ttbs := &cert.TBSCertificate{\n\t\tVersion:   1,\n\t\tName:      \"ca\",\n\t\tIsCA:      true,\n\t\tNotBefore: now,\n\t\tNotAfter:  now.Add(1 * time.Hour),\n\t\tPublicKey: pubCA,\n\t}\n\n\tcaCert, err := tbs.Sign(nil, cert.Curve_CURVE25519, privCA)\n\tassert.NoError(t, err)\n\tncp := cert.NewCAPool()\n\tassert.NoError(t, ncp.AddCA(caCert))\n\n\tpubCrt, _, _ := ed25519.GenerateKey(rand.Reader)\n\ttbs = &cert.TBSCertificate{\n\t\tVersion:   1,\n\t\tName:      \"host\",\n\t\tNetworks:  []netip.Prefix{vpncidr},\n\t\tNotBefore: now,\n\t\tNotAfter:  now.Add(60 * time.Second),\n\t\tPublicKey: pubCrt,\n\t}\n\tpeerCert, err := tbs.Sign(caCert, cert.Curve_CURVE25519, privCA)\n\tassert.NoError(t, err)\n\n\tcachedPeerCert, err := ncp.VerifyCertificate(now.Add(time.Second), peerCert)\n\n\tcs := &CertState{\n\t\tRawCertificate:      []byte{},\n\t\tPrivateKey:          []byte{},\n\t\tCertificate:         &dummyCert{},\n\t\tRawCertificateNoKey: []byte{},\n\t}\n\n\tlh := newTestLighthouse()\n\tifce := &Interface{\n\t\thostMap:          hostMap,\n\t\tinside:           &test.NoopTun{},\n\t\toutside:          &udp.NoopConn{},\n\t\tfirewall:         &Firewall{},\n\t\tlightHouse:       lh,\n\t\thandshakeManager: NewHandshakeManager(l, hostMap, lh, &udp.NoopConn{}, defaultHandshakeConfig),\n\t\tl:                l,\n\t\tpki:              &PKI{},\n\t}\n\tifce.pki.cs.Store(cs)\n\tifce.pki.caPool.Store(ncp)\n\tifce.disconnectInvalid.Store(true)\n\n\t// Create manager\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tpunchy := NewPunchyFromConfig(l, config.NewC(l))\n\tnc := newConnectionManager(ctx, l, ifce, 5, 10, punchy)\n\tifce.connectionManager = nc\n\n\thostinfo := &HostInfo{\n\t\tvpnIp: vpnIp,\n\t\tConnectionState: &ConnectionState{\n\t\t\tmyCert:   &dummyCert{},\n\t\t\tpeerCert: cachedPeerCert,\n\t\t\tH:        &noise.HandshakeState{},\n\t\t},\n\t}\n\tnc.hostMap.unlockedAddHostInfo(hostinfo, ifce)\n\n\t// Move ahead 45s.\n\t// Check if to disconnect with invalid certificate.\n\t// Should be alive.\n\tnextTick := now.Add(45 * time.Second)\n\tinvalid := nc.isInvalidCertificate(nextTick, hostinfo)\n\tassert.False(t, invalid)\n\n\t// Move ahead 61s.\n\t// Check if to disconnect with invalid certificate.\n\t// Should be disconnected.\n\tnextTick = now.Add(61 * time.Second)\n\tinvalid = nc.isInvalidCertificate(nextTick, hostinfo)\n\tassert.True(t, invalid)\n}\n\ntype dummyCert struct {\n\tversion        cert.Version\n\tcurve          cert.Curve\n\tgroups         []string\n\tisCa           bool\n\tissuer         string\n\tname           string\n\tnetworks       []netip.Prefix\n\tnotAfter       time.Time\n\tnotBefore      time.Time\n\tpublicKey      []byte\n\tsignature      []byte\n\tunsafeNetworks []netip.Prefix\n}\n\nfunc (d *dummyCert) Version() cert.Version {\n\treturn d.version\n}\n\nfunc (d *dummyCert) Curve() cert.Curve {\n\treturn d.curve\n}\n\nfunc (d *dummyCert) Groups() []string {\n\treturn d.groups\n}\n\nfunc (d *dummyCert) IsCA() bool {\n\treturn d.isCa\n}\n\nfunc (d *dummyCert) Issuer() string {\n\treturn d.issuer\n}\n\nfunc (d *dummyCert) Name() string {\n\treturn d.name\n}\n\nfunc (d *dummyCert) Networks() []netip.Prefix {\n\treturn d.networks\n}\n\nfunc (d *dummyCert) NotAfter() time.Time {\n\treturn d.notAfter\n}\n\nfunc (d *dummyCert) NotBefore() time.Time {\n\treturn d.notBefore\n}\n\nfunc (d *dummyCert) PublicKey() []byte {\n\treturn d.publicKey\n}\n\nfunc (d *dummyCert) Signature() []byte {\n\treturn d.signature\n}\n\nfunc (d *dummyCert) UnsafeNetworks() []netip.Prefix {\n\treturn d.unsafeNetworks\n}\n\nfunc (d *dummyCert) MarshalForHandshakes() ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (d *dummyCert) Sign(curve cert.Curve, key []byte) error {\n\treturn nil\n}\n\nfunc (d *dummyCert) CheckSignature(key []byte) bool {\n\treturn true\n}\n\nfunc (d *dummyCert) Expired(t time.Time) bool {\n\treturn false\n}\n\nfunc (d *dummyCert) CheckRootConstraints(signer cert.Certificate) error {\n\treturn nil\n}\n\nfunc (d *dummyCert) VerifyPrivateKey(curve cert.Curve, key []byte) error {\n\treturn nil\n}\n\nfunc (d *dummyCert) String() string {\n\treturn \"\"\n}\n\nfunc (d *dummyCert) Marshal() ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (d *dummyCert) MarshalPEM() ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (d *dummyCert) Fingerprint() (string, error) {\n\treturn \"\", nil\n}\n\nfunc (d *dummyCert) MarshalJSON() ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (d *dummyCert) Copy() cert.Certificate {\n\treturn d\n}\n"
        },
        {
          "name": "connection_state.go",
          "type": "blob",
          "size": 2.37109375,
          "content": "package nebula\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/json\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/flynn/noise\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/noiseutil\"\n)\n\nconst ReplayWindow = 1024\n\ntype ConnectionState struct {\n\teKey           *NebulaCipherState\n\tdKey           *NebulaCipherState\n\tH              *noise.HandshakeState\n\tmyCert         cert.Certificate\n\tpeerCert       *cert.CachedCertificate\n\tinitiator      bool\n\tmessageCounter atomic.Uint64\n\twindow         *Bits\n\twriteLock      sync.Mutex\n}\n\nfunc NewConnectionState(l *logrus.Logger, cipher string, certState *CertState, initiator bool, pattern noise.HandshakePattern, psk []byte, pskStage int) *ConnectionState {\n\tvar dhFunc noise.DHFunc\n\tswitch certState.Certificate.Curve() {\n\tcase cert.Curve_CURVE25519:\n\t\tdhFunc = noise.DH25519\n\tcase cert.Curve_P256:\n\t\tif certState.pkcs11Backed {\n\t\t\tdhFunc = noiseutil.DHP256PKCS11\n\t\t} else {\n\t\t\tdhFunc = noiseutil.DHP256\n\t\t}\n\tdefault:\n\t\tl.Errorf(\"invalid curve: %s\", certState.Certificate.Curve())\n\t\treturn nil\n\t}\n\n\tvar cs noise.CipherSuite\n\tif cipher == \"chachapoly\" {\n\t\tcs = noise.NewCipherSuite(dhFunc, noise.CipherChaChaPoly, noise.HashSHA256)\n\t} else {\n\t\tcs = noise.NewCipherSuite(dhFunc, noiseutil.CipherAESGCM, noise.HashSHA256)\n\t}\n\n\tstatic := noise.DHKey{Private: certState.PrivateKey, Public: certState.PublicKey}\n\n\tb := NewBits(ReplayWindow)\n\t// Clear out bit 0, we never transmit it and we don't want it showing as packet loss\n\tb.Update(l, 0)\n\n\ths, err := noise.NewHandshakeState(noise.Config{\n\t\tCipherSuite:           cs,\n\t\tRandom:                rand.Reader,\n\t\tPattern:               pattern,\n\t\tInitiator:             initiator,\n\t\tStaticKeypair:         static,\n\t\tPresharedKey:          psk,\n\t\tPresharedKeyPlacement: pskStage,\n\t})\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\t// The queue and ready params prevent a counter race that would happen when\n\t// sending stored packets and simultaneously accepting new traffic.\n\tci := &ConnectionState{\n\t\tH:         hs,\n\t\tinitiator: initiator,\n\t\twindow:    b,\n\t\tmyCert:    certState.Certificate,\n\t}\n\t// always start the counter from 2, as packet 1 and packet 2 are handshake packets.\n\tci.messageCounter.Add(2)\n\n\treturn ci\n}\n\nfunc (cs *ConnectionState) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(m{\n\t\t\"certificate\":     cs.peerCert,\n\t\t\"initiator\":       cs.initiator,\n\t\t\"message_counter\": cs.messageCounter.Load(),\n\t})\n}\n"
        },
        {
          "name": "control.go",
          "type": "blob",
          "size": 8.826171875,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"net/netip\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/overlay\"\n)\n\n// Every interaction here needs to take extra care to copy memory and not return or use arguments \"as is\" when touching\n// core. This means copying IP objects, slices, de-referencing pointers and taking the actual value, etc\n\ntype controlEach func(h *HostInfo)\n\ntype controlHostLister interface {\n\tQueryVpnIp(vpnIp netip.Addr) *HostInfo\n\tForEachIndex(each controlEach)\n\tForEachVpnIp(each controlEach)\n\tGetPreferredRanges() []netip.Prefix\n}\n\ntype Control struct {\n\tf               *Interface\n\tl               *logrus.Logger\n\tctx             context.Context\n\tcancel          context.CancelFunc\n\tsshStart        func()\n\tstatsStart      func()\n\tdnsStart        func()\n\tlighthouseStart func()\n}\n\ntype ControlHostInfo struct {\n\tVpnIp                  netip.Addr       `json:\"vpnIp\"`\n\tLocalIndex             uint32           `json:\"localIndex\"`\n\tRemoteIndex            uint32           `json:\"remoteIndex\"`\n\tRemoteAddrs            []netip.AddrPort `json:\"remoteAddrs\"`\n\tCert                   cert.Certificate `json:\"cert\"`\n\tMessageCounter         uint64           `json:\"messageCounter\"`\n\tCurrentRemote          netip.AddrPort   `json:\"currentRemote\"`\n\tCurrentRelaysToMe      []netip.Addr     `json:\"currentRelaysToMe\"`\n\tCurrentRelaysThroughMe []netip.Addr     `json:\"currentRelaysThroughMe\"`\n}\n\n// Start actually runs nebula, this is a nonblocking call. To block use Control.ShutdownBlock()\nfunc (c *Control) Start() {\n\t// Activate the interface\n\tc.f.activate()\n\n\t// Call all the delayed funcs that waited patiently for the interface to be created.\n\tif c.sshStart != nil {\n\t\tgo c.sshStart()\n\t}\n\tif c.statsStart != nil {\n\t\tgo c.statsStart()\n\t}\n\tif c.dnsStart != nil {\n\t\tgo c.dnsStart()\n\t}\n\tif c.lighthouseStart != nil {\n\t\tc.lighthouseStart()\n\t}\n\n\t// Start reading packets.\n\tc.f.run()\n}\n\nfunc (c *Control) Context() context.Context {\n\treturn c.ctx\n}\n\n// Stop signals nebula to shutdown and close all tunnels, returns after the shutdown is complete\nfunc (c *Control) Stop() {\n\t// Stop the handshakeManager (and other services), to prevent new tunnels from\n\t// being created while we're shutting them all down.\n\tc.cancel()\n\n\tc.CloseAllTunnels(false)\n\tif err := c.f.Close(); err != nil {\n\t\tc.l.WithError(err).Error(\"Close interface failed\")\n\t}\n\tc.l.Info(\"Goodbye\")\n}\n\n// ShutdownBlock will listen for and block on term and interrupt signals, calling Control.Stop() once signalled\nfunc (c *Control) ShutdownBlock() {\n\tsigChan := make(chan os.Signal, 1)\n\tsignal.Notify(sigChan, syscall.SIGTERM)\n\tsignal.Notify(sigChan, syscall.SIGINT)\n\n\trawSig := <-sigChan\n\tsig := rawSig.String()\n\tc.l.WithField(\"signal\", sig).Info(\"Caught signal, shutting down\")\n\tc.Stop()\n}\n\n// RebindUDPServer asks the UDP listener to rebind it's listener. Mainly used on mobile clients when interfaces change\nfunc (c *Control) RebindUDPServer() {\n\t_ = c.f.outside.Rebind()\n\n\t// Trigger a lighthouse update, useful for mobile clients that should have an update interval of 0\n\tc.f.lightHouse.SendUpdate()\n\n\t// Let the main interface know that we rebound so that underlying tunnels know to trigger punches from their remotes\n\tc.f.rebindCount++\n}\n\n// ListHostmapHosts returns details about the actual or pending (handshaking) hostmap by vpn ip\nfunc (c *Control) ListHostmapHosts(pendingMap bool) []ControlHostInfo {\n\tif pendingMap {\n\t\treturn listHostMapHosts(c.f.handshakeManager)\n\t} else {\n\t\treturn listHostMapHosts(c.f.hostMap)\n\t}\n}\n\n// ListHostmapIndexes returns details about the actual or pending (handshaking) hostmap by local index id\nfunc (c *Control) ListHostmapIndexes(pendingMap bool) []ControlHostInfo {\n\tif pendingMap {\n\t\treturn listHostMapIndexes(c.f.handshakeManager)\n\t} else {\n\t\treturn listHostMapIndexes(c.f.hostMap)\n\t}\n}\n\n// GetCertByVpnIp returns the authenticated certificate of the given vpn IP, or nil if not found\nfunc (c *Control) GetCertByVpnIp(vpnIp netip.Addr) cert.Certificate {\n\tif c.f.myVpnNet.Addr() == vpnIp {\n\t\treturn c.f.pki.GetCertState().Certificate.Copy()\n\t}\n\thi := c.f.hostMap.QueryVpnIp(vpnIp)\n\tif hi == nil {\n\t\treturn nil\n\t}\n\treturn hi.GetCert().Certificate.Copy()\n}\n\n// CreateTunnel creates a new tunnel to the given vpn ip.\nfunc (c *Control) CreateTunnel(vpnIp netip.Addr) {\n\tc.f.handshakeManager.StartHandshake(vpnIp, nil)\n}\n\n// PrintTunnel creates a new tunnel to the given vpn ip.\nfunc (c *Control) PrintTunnel(vpnIp netip.Addr) *ControlHostInfo {\n\thi := c.f.hostMap.QueryVpnIp(vpnIp)\n\tif hi == nil {\n\t\treturn nil\n\t}\n\tchi := copyHostInfo(hi, c.f.hostMap.GetPreferredRanges())\n\treturn &chi\n}\n\n// QueryLighthouse queries the lighthouse.\nfunc (c *Control) QueryLighthouse(vpnIp netip.Addr) *CacheMap {\n\thi := c.f.lightHouse.Query(vpnIp)\n\tif hi == nil {\n\t\treturn nil\n\t}\n\treturn hi.CopyCache()\n}\n\n// GetHostInfoByVpnIp returns a single tunnels hostInfo, or nil if not found\n// Caller should take care to Unmap() any 4in6 addresses prior to calling.\nfunc (c *Control) GetHostInfoByVpnIp(vpnIp netip.Addr, pending bool) *ControlHostInfo {\n\tvar hl controlHostLister\n\tif pending {\n\t\thl = c.f.handshakeManager\n\t} else {\n\t\thl = c.f.hostMap\n\t}\n\n\th := hl.QueryVpnIp(vpnIp)\n\tif h == nil {\n\t\treturn nil\n\t}\n\n\tch := copyHostInfo(h, c.f.hostMap.GetPreferredRanges())\n\treturn &ch\n}\n\n// SetRemoteForTunnel forces a tunnel to use a specific remote\n// Caller should take care to Unmap() any 4in6 addresses prior to calling.\nfunc (c *Control) SetRemoteForTunnel(vpnIp netip.Addr, addr netip.AddrPort) *ControlHostInfo {\n\thostInfo := c.f.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo == nil {\n\t\treturn nil\n\t}\n\n\thostInfo.SetRemote(addr)\n\tch := copyHostInfo(hostInfo, c.f.hostMap.GetPreferredRanges())\n\treturn &ch\n}\n\n// CloseTunnel closes a fully established tunnel. If localOnly is false it will notify the remote end as well.\n// Caller should take care to Unmap() any 4in6 addresses prior to calling.\nfunc (c *Control) CloseTunnel(vpnIp netip.Addr, localOnly bool) bool {\n\thostInfo := c.f.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo == nil {\n\t\treturn false\n\t}\n\n\tif !localOnly {\n\t\tc.f.send(\n\t\t\theader.CloseTunnel,\n\t\t\t0,\n\t\t\thostInfo.ConnectionState,\n\t\t\thostInfo,\n\t\t\t[]byte{},\n\t\t\tmake([]byte, 12, 12),\n\t\t\tmake([]byte, mtu),\n\t\t)\n\t}\n\n\tc.f.closeTunnel(hostInfo)\n\treturn true\n}\n\n// CloseAllTunnels is just like CloseTunnel except it goes through and shuts them all down, optionally you can avoid shutting down lighthouse tunnels\n// the int returned is a count of tunnels closed\nfunc (c *Control) CloseAllTunnels(excludeLighthouses bool) (closed int) {\n\t//TODO: this is probably better as a function in ConnectionManager or HostMap directly\n\tlighthouses := c.f.lightHouse.GetLighthouses()\n\n\tshutdown := func(h *HostInfo) {\n\t\tif excludeLighthouses {\n\t\t\tif _, ok := lighthouses[h.vpnIp]; ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tc.f.send(header.CloseTunnel, 0, h.ConnectionState, h, []byte{}, make([]byte, 12, 12), make([]byte, mtu))\n\t\tc.f.closeTunnel(h)\n\n\t\tc.l.WithField(\"vpnIp\", h.vpnIp).WithField(\"udpAddr\", h.remote).\n\t\t\tDebug(\"Sending close tunnel message\")\n\t\tclosed++\n\t}\n\n\t// Learn which hosts are being used as relays, so we can shut them down last.\n\trelayingHosts := map[netip.Addr]*HostInfo{}\n\t// Grab the hostMap lock to access the Relays map\n\tc.f.hostMap.Lock()\n\tfor _, relayingHost := range c.f.hostMap.Relays {\n\t\trelayingHosts[relayingHost.vpnIp] = relayingHost\n\t}\n\tc.f.hostMap.Unlock()\n\n\thostInfos := []*HostInfo{}\n\t// Grab the hostMap lock to access the Hosts map\n\tc.f.hostMap.Lock()\n\tfor _, relayHost := range c.f.hostMap.Indexes {\n\t\tif _, ok := relayingHosts[relayHost.vpnIp]; !ok {\n\t\t\thostInfos = append(hostInfos, relayHost)\n\t\t}\n\t}\n\tc.f.hostMap.Unlock()\n\n\tfor _, h := range hostInfos {\n\t\tshutdown(h)\n\t}\n\tfor _, h := range relayingHosts {\n\t\tshutdown(h)\n\t}\n\treturn\n}\n\nfunc (c *Control) Device() overlay.Device {\n\treturn c.f.inside\n}\n\nfunc copyHostInfo(h *HostInfo, preferredRanges []netip.Prefix) ControlHostInfo {\n\n\tchi := ControlHostInfo{\n\t\tVpnIp:                  h.vpnIp,\n\t\tLocalIndex:             h.localIndexId,\n\t\tRemoteIndex:            h.remoteIndexId,\n\t\tRemoteAddrs:            h.remotes.CopyAddrs(preferredRanges),\n\t\tCurrentRelaysToMe:      h.relayState.CopyRelayIps(),\n\t\tCurrentRelaysThroughMe: h.relayState.CopyRelayForIps(),\n\t\tCurrentRemote:          h.remote,\n\t}\n\n\tif h.ConnectionState != nil {\n\t\tchi.MessageCounter = h.ConnectionState.messageCounter.Load()\n\t}\n\n\tif c := h.GetCert(); c != nil {\n\t\tchi.Cert = c.Certificate.Copy()\n\t}\n\n\treturn chi\n}\n\nfunc listHostMapHosts(hl controlHostLister) []ControlHostInfo {\n\thosts := make([]ControlHostInfo, 0)\n\tpr := hl.GetPreferredRanges()\n\thl.ForEachVpnIp(func(hostinfo *HostInfo) {\n\t\thosts = append(hosts, copyHostInfo(hostinfo, pr))\n\t})\n\treturn hosts\n}\n\nfunc listHostMapIndexes(hl controlHostLister) []ControlHostInfo {\n\thosts := make([]ControlHostInfo, 0)\n\tpr := hl.GetPreferredRanges()\n\thl.ForEachIndex(func(hostinfo *HostInfo) {\n\t\thosts = append(hosts, copyHostInfo(hostinfo, pr))\n\t})\n\treturn hosts\n}\n"
        },
        {
          "name": "control_test.go",
          "type": "blob",
          "size": 3.48828125,
          "content": "package nebula\n\nimport (\n\t\"net\"\n\t\"net/netip\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestControl_GetHostInfoByVpnIp(t *testing.T) {\n\t//TODO: with multiple certificate versions we have a problem with this test\n\t// Some certs versions have different characteristics and each version implements their own Copy() func\n\t// which means this is not a good place to test for exposing memory\n\tl := test.NewLogger()\n\t// Special care must be taken to re-use all objects provided to the hostmap and certificate in the expectedInfo object\n\t// To properly ensure we are not exposing core memory to the caller\n\thm := newHostMap(l, netip.Prefix{})\n\thm.preferredRanges.Store(&[]netip.Prefix{})\n\n\tremote1 := netip.MustParseAddrPort(\"0.0.0.100:4444\")\n\tremote2 := netip.MustParseAddrPort(\"[1:2:3:4:5:6:7:8]:4444\")\n\n\tipNet := net.IPNet{\n\t\tIP:   remote1.Addr().AsSlice(),\n\t\tMask: net.IPMask{255, 255, 255, 0},\n\t}\n\n\tipNet2 := net.IPNet{\n\t\tIP:   remote2.Addr().AsSlice(),\n\t\tMask: net.IPMask{255, 255, 255, 0},\n\t}\n\n\tremotes := NewRemoteList(nil)\n\tremotes.unlockedPrependV4(netip.IPv4Unspecified(), NewIp4AndPortFromNetIP(remote1.Addr(), remote1.Port()))\n\tremotes.unlockedPrependV6(netip.IPv4Unspecified(), NewIp6AndPortFromNetIP(remote2.Addr(), remote2.Port()))\n\n\tvpnIp, ok := netip.AddrFromSlice(ipNet.IP)\n\tassert.True(t, ok)\n\n\tcrt := &dummyCert{}\n\thm.unlockedAddHostInfo(&HostInfo{\n\t\tremote:  remote1,\n\t\tremotes: remotes,\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &cert.CachedCertificate{Certificate: crt},\n\t\t},\n\t\tremoteIndexId: 200,\n\t\tlocalIndexId:  201,\n\t\tvpnIp:         vpnIp,\n\t\trelayState: RelayState{\n\t\t\trelays:        map[netip.Addr]struct{}{},\n\t\t\trelayForByIp:  map[netip.Addr]*Relay{},\n\t\t\trelayForByIdx: map[uint32]*Relay{},\n\t\t},\n\t}, &Interface{})\n\n\tvpnIp2, ok := netip.AddrFromSlice(ipNet2.IP)\n\tassert.True(t, ok)\n\n\thm.unlockedAddHostInfo(&HostInfo{\n\t\tremote:  remote1,\n\t\tremotes: remotes,\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: nil,\n\t\t},\n\t\tremoteIndexId: 200,\n\t\tlocalIndexId:  201,\n\t\tvpnIp:         vpnIp2,\n\t\trelayState: RelayState{\n\t\t\trelays:        map[netip.Addr]struct{}{},\n\t\t\trelayForByIp:  map[netip.Addr]*Relay{},\n\t\t\trelayForByIdx: map[uint32]*Relay{},\n\t\t},\n\t}, &Interface{})\n\n\tc := Control{\n\t\tf: &Interface{\n\t\t\thostMap: hm,\n\t\t},\n\t\tl: logrus.New(),\n\t}\n\n\tthi := c.GetHostInfoByVpnIp(vpnIp, false)\n\n\texpectedInfo := ControlHostInfo{\n\t\tVpnIp:                  vpnIp,\n\t\tLocalIndex:             201,\n\t\tRemoteIndex:            200,\n\t\tRemoteAddrs:            []netip.AddrPort{remote2, remote1},\n\t\tCert:                   crt.Copy(),\n\t\tMessageCounter:         0,\n\t\tCurrentRemote:          remote1,\n\t\tCurrentRelaysToMe:      []netip.Addr{},\n\t\tCurrentRelaysThroughMe: []netip.Addr{},\n\t}\n\n\t// Make sure we don't have any unexpected fields\n\tassertFields(t, []string{\"VpnIp\", \"LocalIndex\", \"RemoteIndex\", \"RemoteAddrs\", \"Cert\", \"MessageCounter\", \"CurrentRemote\", \"CurrentRelaysToMe\", \"CurrentRelaysThroughMe\"}, thi)\n\tassert.EqualValues(t, &expectedInfo, thi)\n\ttest.AssertDeepCopyEqual(t, &expectedInfo, thi)\n\n\t// Make sure we don't panic if the host info doesn't have a cert yet\n\tassert.NotPanics(t, func() {\n\t\tthi = c.GetHostInfoByVpnIp(vpnIp2, false)\n\t})\n}\n\nfunc assertFields(t *testing.T, expected []string, actualStruct interface{}) {\n\tval := reflect.ValueOf(actualStruct).Elem()\n\tfields := make([]string, val.NumField())\n\tfor i := 0; i < val.NumField(); i++ {\n\t\tfields[i] = val.Type().Field(i).Name\n\t}\n\n\tassert.Equal(t, expected, fields)\n}\n"
        },
        {
          "name": "control_tester.go",
          "type": "blob",
          "size": 4.58203125,
          "content": "//go:build e2e_testing\n// +build e2e_testing\n\npackage nebula\n\nimport (\n\t\"net/netip\"\n\n\t\"github.com/slackhq/nebula/cert\"\n\n\t\"github.com/google/gopacket\"\n\t\"github.com/google/gopacket/layers\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/overlay\"\n\t\"github.com/slackhq/nebula/udp\"\n)\n\n// WaitForType will pipe all messages from this control device into the pipeTo control device\n// returning after a message matching the criteria has been piped\nfunc (c *Control) WaitForType(msgType header.MessageType, subType header.MessageSubType, pipeTo *Control) {\n\th := &header.H{}\n\tfor {\n\t\tp := c.f.outside.(*udp.TesterConn).Get(true)\n\t\tif err := h.Parse(p.Data); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tpipeTo.InjectUDPPacket(p)\n\t\tif h.Type == msgType && h.Subtype == subType {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// WaitForTypeByIndex is similar to WaitForType except it adds an index check\n// Useful if you have many nodes communicating and want to wait to find a specific nodes packet\nfunc (c *Control) WaitForTypeByIndex(toIndex uint32, msgType header.MessageType, subType header.MessageSubType, pipeTo *Control) {\n\th := &header.H{}\n\tfor {\n\t\tp := c.f.outside.(*udp.TesterConn).Get(true)\n\t\tif err := h.Parse(p.Data); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tpipeTo.InjectUDPPacket(p)\n\t\tif h.RemoteIndex == toIndex && h.Type == msgType && h.Subtype == subType {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// InjectLightHouseAddr will push toAddr into the local lighthouse cache for the vpnIp\n// This is necessary if you did not configure static hosts or are not running a lighthouse\nfunc (c *Control) InjectLightHouseAddr(vpnIp netip.Addr, toAddr netip.AddrPort) {\n\tc.f.lightHouse.Lock()\n\tremoteList := c.f.lightHouse.unlockedGetRemoteList(vpnIp)\n\tremoteList.Lock()\n\tdefer remoteList.Unlock()\n\tc.f.lightHouse.Unlock()\n\n\tif toAddr.Addr().Is4() {\n\t\tremoteList.unlockedPrependV4(vpnIp, NewIp4AndPortFromNetIP(toAddr.Addr(), toAddr.Port()))\n\t} else {\n\t\tremoteList.unlockedPrependV6(vpnIp, NewIp6AndPortFromNetIP(toAddr.Addr(), toAddr.Port()))\n\t}\n}\n\n// InjectRelays will push relayVpnIps into the local lighthouse cache for the vpnIp\n// This is necessary to inform an initiator of possible relays for communicating with a responder\nfunc (c *Control) InjectRelays(vpnIp netip.Addr, relayVpnIps []netip.Addr) {\n\tc.f.lightHouse.Lock()\n\tremoteList := c.f.lightHouse.unlockedGetRemoteList(vpnIp)\n\tremoteList.Lock()\n\tdefer remoteList.Unlock()\n\tc.f.lightHouse.Unlock()\n\n\tremoteList.unlockedSetRelay(vpnIp, vpnIp, relayVpnIps)\n}\n\n// GetFromTun will pull a packet off the tun side of nebula\nfunc (c *Control) GetFromTun(block bool) []byte {\n\treturn c.f.inside.(*overlay.TestTun).Get(block)\n}\n\n// GetFromUDP will pull a udp packet off the udp side of nebula\nfunc (c *Control) GetFromUDP(block bool) *udp.Packet {\n\treturn c.f.outside.(*udp.TesterConn).Get(block)\n}\n\nfunc (c *Control) GetUDPTxChan() <-chan *udp.Packet {\n\treturn c.f.outside.(*udp.TesterConn).TxPackets\n}\n\nfunc (c *Control) GetTunTxChan() <-chan []byte {\n\treturn c.f.inside.(*overlay.TestTun).TxPackets\n}\n\n// InjectUDPPacket will inject a packet into the udp side of nebula\nfunc (c *Control) InjectUDPPacket(p *udp.Packet) {\n\tc.f.outside.(*udp.TesterConn).Send(p)\n}\n\n// InjectTunUDPPacket puts a udp packet on the tun interface. Using UDP here because it's a simpler protocol\nfunc (c *Control) InjectTunUDPPacket(toIp netip.Addr, toPort uint16, fromPort uint16, data []byte) {\n\t//TODO: IPV6-WORK\n\tip := layers.IPv4{\n\t\tVersion:  4,\n\t\tTTL:      64,\n\t\tProtocol: layers.IPProtocolUDP,\n\t\tSrcIP:    c.f.inside.Cidr().Addr().Unmap().AsSlice(),\n\t\tDstIP:    toIp.Unmap().AsSlice(),\n\t}\n\n\tudp := layers.UDP{\n\t\tSrcPort: layers.UDPPort(fromPort),\n\t\tDstPort: layers.UDPPort(toPort),\n\t}\n\terr := udp.SetNetworkLayerForChecksum(&ip)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tbuffer := gopacket.NewSerializeBuffer()\n\topt := gopacket.SerializeOptions{\n\t\tComputeChecksums: true,\n\t\tFixLengths:       true,\n\t}\n\terr = gopacket.SerializeLayers(buffer, opt, &ip, &udp, gopacket.Payload(data))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tc.f.inside.(*overlay.TestTun).Send(buffer.Bytes())\n}\n\nfunc (c *Control) GetVpnIp() netip.Addr {\n\treturn c.f.myVpnNet.Addr()\n}\n\nfunc (c *Control) GetUDPAddr() netip.AddrPort {\n\treturn c.f.outside.(*udp.TesterConn).Addr\n}\n\nfunc (c *Control) KillPendingTunnel(vpnIp netip.Addr) bool {\n\thostinfo := c.f.handshakeManager.QueryVpnIp(vpnIp)\n\tif hostinfo == nil {\n\t\treturn false\n\t}\n\n\tc.f.handshakeManager.DeleteHostInfo(hostinfo)\n\treturn true\n}\n\nfunc (c *Control) GetHostmap() *HostMap {\n\treturn c.f.hostMap\n}\n\nfunc (c *Control) GetCert() cert.Certificate {\n\treturn c.f.pki.GetCertState().Certificate\n}\n\nfunc (c *Control) ReHandshake(vpnIp netip.Addr) {\n\tc.f.handshakeManager.StartHandshake(vpnIp, nil)\n}\n"
        },
        {
          "name": "dist",
          "type": "tree",
          "content": null
        },
        {
          "name": "dns_server.go",
          "type": "blob",
          "size": 3.5322265625,
          "content": "package nebula\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"net/netip\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/miekg/dns\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\n// This whole thing should be rewritten to use context\n\nvar dnsR *dnsRecords\nvar dnsServer *dns.Server\nvar dnsAddr string\n\ntype dnsRecords struct {\n\tsync.RWMutex\n\tdnsMap  map[string]string\n\thostMap *HostMap\n}\n\nfunc newDnsRecords(hostMap *HostMap) *dnsRecords {\n\treturn &dnsRecords{\n\t\tdnsMap:  make(map[string]string),\n\t\thostMap: hostMap,\n\t}\n}\n\nfunc (d *dnsRecords) Query(data string) string {\n\td.RLock()\n\tdefer d.RUnlock()\n\tif r, ok := d.dnsMap[strings.ToLower(data)]; ok {\n\t\treturn r\n\t}\n\treturn \"\"\n}\n\nfunc (d *dnsRecords) QueryCert(data string) string {\n\tip, err := netip.ParseAddr(data[:len(data)-1])\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\n\thostinfo := d.hostMap.QueryVpnIp(ip)\n\tif hostinfo == nil {\n\t\treturn \"\"\n\t}\n\n\tq := hostinfo.GetCert()\n\tif q == nil {\n\t\treturn \"\"\n\t}\n\n\tb, err := q.Certificate.MarshalJSON()\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\treturn string(b)\n}\n\nfunc (d *dnsRecords) Add(host, data string) {\n\td.Lock()\n\tdefer d.Unlock()\n\td.dnsMap[strings.ToLower(host)] = data\n}\n\nfunc parseQuery(l *logrus.Logger, m *dns.Msg, w dns.ResponseWriter) {\n\tfor _, q := range m.Question {\n\t\tswitch q.Qtype {\n\t\tcase dns.TypeA:\n\t\t\tl.Debugf(\"Query for A %s\", q.Name)\n\t\t\tip := dnsR.Query(q.Name)\n\t\t\tif ip != \"\" {\n\t\t\t\trr, err := dns.NewRR(fmt.Sprintf(\"%s A %s\", q.Name, ip))\n\t\t\t\tif err == nil {\n\t\t\t\t\tm.Answer = append(m.Answer, rr)\n\t\t\t\t}\n\t\t\t}\n\t\tcase dns.TypeTXT:\n\t\t\ta, _, _ := net.SplitHostPort(w.RemoteAddr().String())\n\t\t\tb, err := netip.ParseAddr(a)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We don't answer these queries from non nebula nodes or localhost\n\t\t\t//l.Debugf(\"Does %s contain %s\", b, dnsR.hostMap.vpnCIDR)\n\t\t\tif !dnsR.hostMap.vpnCIDR.Contains(b) && a != \"127.0.0.1\" {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl.Debugf(\"Query for TXT %s\", q.Name)\n\t\t\tip := dnsR.QueryCert(q.Name)\n\t\t\tif ip != \"\" {\n\t\t\t\trr, err := dns.NewRR(fmt.Sprintf(\"%s TXT %s\", q.Name, ip))\n\t\t\t\tif err == nil {\n\t\t\t\t\tm.Answer = append(m.Answer, rr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(m.Answer) == 0 {\n\t\tm.Rcode = dns.RcodeNameError\n\t}\n}\n\nfunc handleDnsRequest(l *logrus.Logger, w dns.ResponseWriter, r *dns.Msg) {\n\tm := new(dns.Msg)\n\tm.SetReply(r)\n\tm.Compress = false\n\n\tswitch r.Opcode {\n\tcase dns.OpcodeQuery:\n\t\tparseQuery(l, m, w)\n\t}\n\n\tw.WriteMsg(m)\n}\n\nfunc dnsMain(l *logrus.Logger, hostMap *HostMap, c *config.C) func() {\n\tdnsR = newDnsRecords(hostMap)\n\n\t// attach request handler func\n\tdns.HandleFunc(\".\", func(w dns.ResponseWriter, r *dns.Msg) {\n\t\thandleDnsRequest(l, w, r)\n\t})\n\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\treloadDns(l, c)\n\t})\n\n\treturn func() {\n\t\tstartDns(l, c)\n\t}\n}\n\nfunc getDnsServerAddr(c *config.C) string {\n\tdnsHost := strings.TrimSpace(c.GetString(\"lighthouse.dns.host\", \"\"))\n\t// Old guidance was to provide the literal `[::]` in `lighthouse.dns.host` but that won't resolve.\n\tif dnsHost == \"[::]\" {\n\t\tdnsHost = \"::\"\n\t}\n\treturn net.JoinHostPort(dnsHost, strconv.Itoa(c.GetInt(\"lighthouse.dns.port\", 53)))\n}\n\nfunc startDns(l *logrus.Logger, c *config.C) {\n\tdnsAddr = getDnsServerAddr(c)\n\tdnsServer = &dns.Server{Addr: dnsAddr, Net: \"udp\"}\n\tl.WithField(\"dnsListener\", dnsAddr).Info(\"Starting DNS responder\")\n\terr := dnsServer.ListenAndServe()\n\tdefer dnsServer.Shutdown()\n\tif err != nil {\n\t\tl.Errorf(\"Failed to start server: %s\\n \", err.Error())\n\t}\n}\n\nfunc reloadDns(l *logrus.Logger, c *config.C) {\n\tif dnsAddr == getDnsServerAddr(c) {\n\t\tl.Debug(\"No DNS server config change detected\")\n\t\treturn\n\t}\n\n\tl.Debug(\"Restarting DNS server\")\n\tdnsServer.Shutdown()\n\tgo startDns(l, c)\n}\n"
        },
        {
          "name": "dns_server_test.go",
          "type": "blob",
          "size": 1.22265625,
          "content": "package nebula\n\nimport (\n\t\"testing\"\n\n\t\"github.com/miekg/dns\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestParsequery(t *testing.T) {\n\t//TODO: This test is basically pointless\n\thostMap := &HostMap{}\n\tds := newDnsRecords(hostMap)\n\tds.Add(\"test.com.com\", \"1.2.3.4\")\n\n\tm := new(dns.Msg)\n\tm.SetQuestion(\"test.com.com\", dns.TypeA)\n\n\t//parseQuery(m)\n}\n\nfunc Test_getDnsServerAddr(t *testing.T) {\n\tc := config.NewC(nil)\n\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\n\t\t\"dns\": map[interface{}]interface{}{\n\t\t\t\"host\": \"0.0.0.0\",\n\t\t\t\"port\": \"1\",\n\t\t},\n\t}\n\tassert.Equal(t, \"0.0.0.0:1\", getDnsServerAddr(c))\n\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\n\t\t\"dns\": map[interface{}]interface{}{\n\t\t\t\"host\": \"::\",\n\t\t\t\"port\": \"1\",\n\t\t},\n\t}\n\tassert.Equal(t, \"[::]:1\", getDnsServerAddr(c))\n\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\n\t\t\"dns\": map[interface{}]interface{}{\n\t\t\t\"host\": \"[::]\",\n\t\t\t\"port\": \"1\",\n\t\t},\n\t}\n\tassert.Equal(t, \"[::]:1\", getDnsServerAddr(c))\n\n\t// Make sure whitespace doesn't mess us up\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\n\t\t\"dns\": map[interface{}]interface{}{\n\t\t\t\"host\": \"[::] \",\n\t\t\t\"port\": \"1\",\n\t\t},\n\t}\n\tassert.Equal(t, \"[::]:1\", getDnsServerAddr(c))\n}\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "e2e",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "firewall.go",
          "type": "blob",
          "size": 24.77734375,
          "content": "package nebula\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/fnv\"\n\t\"net/netip\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/firewall\"\n)\n\ntype FirewallInterface interface {\n\tAddRule(incoming bool, proto uint8, startPort int32, endPort int32, groups []string, host string, ip, localIp netip.Prefix, caName string, caSha string) error\n}\n\ntype conn struct {\n\tExpires time.Time // Time when this conntrack entry will expire\n\n\t// record why the original connection passed the firewall, so we can re-validate\n\t// after ruleset changes. Note, rulesVersion is a uint16 so that these two\n\t// fields pack for free after the uint32 above\n\tincoming     bool\n\trulesVersion uint16\n}\n\n// TODO: need conntrack max tracked connections handling\ntype Firewall struct {\n\tConntrack *FirewallConntrack\n\n\tInRules  *FirewallTable\n\tOutRules *FirewallTable\n\n\tInSendReject  bool\n\tOutSendReject bool\n\n\t//TODO: we should have many more options for TCP, an option for ICMP, and mimic the kernel a bit better\n\t// https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt\n\tTCPTimeout     time.Duration //linux: 5 days max\n\tUDPTimeout     time.Duration //linux: 180s max\n\tDefaultTimeout time.Duration //linux: 600s\n\n\t// Used to ensure we don't emit local packets for ips we don't own\n\tlocalIps          *bart.Table[struct{}]\n\tassignedCIDR      netip.Prefix\n\thasUnsafeNetworks bool\n\n\trules        string\n\trulesVersion uint16\n\n\tdefaultLocalCIDRAny bool\n\tincomingMetrics     firewallMetrics\n\toutgoingMetrics     firewallMetrics\n\n\tl *logrus.Logger\n}\n\ntype firewallMetrics struct {\n\tdroppedLocalIP  metrics.Counter\n\tdroppedRemoteIP metrics.Counter\n\tdroppedNoRule   metrics.Counter\n}\n\ntype FirewallConntrack struct {\n\tsync.Mutex\n\n\tConns      map[firewall.Packet]*conn\n\tTimerWheel *TimerWheel[firewall.Packet]\n}\n\n// FirewallTable is the entry point for a rule, the evaluation order is:\n// Proto AND port AND (CA SHA or CA name) AND local CIDR AND (group OR groups OR name OR remote CIDR)\ntype FirewallTable struct {\n\tTCP      firewallPort\n\tUDP      firewallPort\n\tICMP     firewallPort\n\tAnyProto firewallPort\n}\n\nfunc newFirewallTable() *FirewallTable {\n\treturn &FirewallTable{\n\t\tTCP:      firewallPort{},\n\t\tUDP:      firewallPort{},\n\t\tICMP:     firewallPort{},\n\t\tAnyProto: firewallPort{},\n\t}\n}\n\ntype FirewallCA struct {\n\tAny     *FirewallRule\n\tCANames map[string]*FirewallRule\n\tCAShas  map[string]*FirewallRule\n}\n\ntype FirewallRule struct {\n\t// Any makes Hosts, Groups, and CIDR irrelevant\n\tAny    *firewallLocalCIDR\n\tHosts  map[string]*firewallLocalCIDR\n\tGroups []*firewallGroups\n\tCIDR   *bart.Table[*firewallLocalCIDR]\n}\n\ntype firewallGroups struct {\n\tGroups    []string\n\tLocalCIDR *firewallLocalCIDR\n}\n\n// Even though ports are uint16, int32 maps are faster for lookup\n// Plus we can use `-1` for fragment rules\ntype firewallPort map[int32]*FirewallCA\n\ntype firewallLocalCIDR struct {\n\tAny       bool\n\tLocalCIDR *bart.Table[struct{}]\n}\n\n// NewFirewall creates a new Firewall object. A TimerWheel is created for you from the provided timeouts.\nfunc NewFirewall(l *logrus.Logger, tcpTimeout, UDPTimeout, defaultTimeout time.Duration, c cert.Certificate) *Firewall {\n\t//TODO: error on 0 duration\n\tvar min, max time.Duration\n\n\tif tcpTimeout < UDPTimeout {\n\t\tmin = tcpTimeout\n\t\tmax = UDPTimeout\n\t} else {\n\t\tmin = UDPTimeout\n\t\tmax = tcpTimeout\n\t}\n\n\tif defaultTimeout < min {\n\t\tmin = defaultTimeout\n\t} else if defaultTimeout > max {\n\t\tmax = defaultTimeout\n\t}\n\n\tlocalIps := new(bart.Table[struct{}])\n\tvar assignedCIDR netip.Prefix\n\tvar assignedSet bool\n\tfor _, network := range c.Networks() {\n\t\tnprefix := netip.PrefixFrom(network.Addr(), network.Addr().BitLen())\n\t\tlocalIps.Insert(nprefix, struct{}{})\n\n\t\tif !assignedSet {\n\t\t\t// Only grabbing the first one in the cert since any more than that currently has undefined behavior\n\t\t\tassignedCIDR = nprefix\n\t\t\tassignedSet = true\n\t\t}\n\t}\n\n\thasUnsafeNetworks := false\n\tfor _, n := range c.UnsafeNetworks() {\n\t\tlocalIps.Insert(n, struct{}{})\n\t\thasUnsafeNetworks = true\n\t}\n\n\treturn &Firewall{\n\t\tConntrack: &FirewallConntrack{\n\t\t\tConns:      make(map[firewall.Packet]*conn),\n\t\t\tTimerWheel: NewTimerWheel[firewall.Packet](min, max),\n\t\t},\n\t\tInRules:           newFirewallTable(),\n\t\tOutRules:          newFirewallTable(),\n\t\tTCPTimeout:        tcpTimeout,\n\t\tUDPTimeout:        UDPTimeout,\n\t\tDefaultTimeout:    defaultTimeout,\n\t\tlocalIps:          localIps,\n\t\tassignedCIDR:      assignedCIDR,\n\t\thasUnsafeNetworks: hasUnsafeNetworks,\n\t\tl:                 l,\n\n\t\tincomingMetrics: firewallMetrics{\n\t\t\tdroppedLocalIP:  metrics.GetOrRegisterCounter(\"firewall.incoming.dropped.local_ip\", nil),\n\t\t\tdroppedRemoteIP: metrics.GetOrRegisterCounter(\"firewall.incoming.dropped.remote_ip\", nil),\n\t\t\tdroppedNoRule:   metrics.GetOrRegisterCounter(\"firewall.incoming.dropped.no_rule\", nil),\n\t\t},\n\t\toutgoingMetrics: firewallMetrics{\n\t\t\tdroppedLocalIP:  metrics.GetOrRegisterCounter(\"firewall.outgoing.dropped.local_ip\", nil),\n\t\t\tdroppedRemoteIP: metrics.GetOrRegisterCounter(\"firewall.outgoing.dropped.remote_ip\", nil),\n\t\t\tdroppedNoRule:   metrics.GetOrRegisterCounter(\"firewall.outgoing.dropped.no_rule\", nil),\n\t\t},\n\t}\n}\n\nfunc NewFirewallFromConfig(l *logrus.Logger, nc cert.Certificate, c *config.C) (*Firewall, error) {\n\tfw := NewFirewall(\n\t\tl,\n\t\tc.GetDuration(\"firewall.conntrack.tcp_timeout\", time.Minute*12),\n\t\tc.GetDuration(\"firewall.conntrack.udp_timeout\", time.Minute*3),\n\t\tc.GetDuration(\"firewall.conntrack.default_timeout\", time.Minute*10),\n\t\tnc,\n\t\t//TODO: max_connections\n\t)\n\n\t//TODO: Flip to false after v1.9 release\n\tfw.defaultLocalCIDRAny = c.GetBool(\"firewall.default_local_cidr_any\", true)\n\n\tinboundAction := c.GetString(\"firewall.inbound_action\", \"drop\")\n\tswitch inboundAction {\n\tcase \"reject\":\n\t\tfw.InSendReject = true\n\tcase \"drop\":\n\t\tfw.InSendReject = false\n\tdefault:\n\t\tl.WithField(\"action\", inboundAction).Warn(\"invalid firewall.inbound_action, defaulting to `drop`\")\n\t\tfw.InSendReject = false\n\t}\n\n\toutboundAction := c.GetString(\"firewall.outbound_action\", \"drop\")\n\tswitch outboundAction {\n\tcase \"reject\":\n\t\tfw.OutSendReject = true\n\tcase \"drop\":\n\t\tfw.OutSendReject = false\n\tdefault:\n\t\tl.WithField(\"action\", inboundAction).Warn(\"invalid firewall.outbound_action, defaulting to `drop`\")\n\t\tfw.OutSendReject = false\n\t}\n\n\terr := AddFirewallRulesFromConfig(l, false, c, fw)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = AddFirewallRulesFromConfig(l, true, c, fw)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn fw, nil\n}\n\n// AddRule properly creates the in memory rule structure for a firewall table.\nfunc (f *Firewall) AddRule(incoming bool, proto uint8, startPort int32, endPort int32, groups []string, host string, ip, localIp netip.Prefix, caName string, caSha string) error {\n\t// Under gomobile, stringing a nil pointer with fmt causes an abort in debug mode for iOS\n\t// https://github.com/golang/go/issues/14131\n\tsIp := \"\"\n\tif ip.IsValid() {\n\t\tsIp = ip.String()\n\t}\n\tlIp := \"\"\n\tif localIp.IsValid() {\n\t\tlIp = localIp.String()\n\t}\n\n\t// We need this rule string because we generate a hash. Removing this will break firewall reload.\n\truleString := fmt.Sprintf(\n\t\t\"incoming: %v, proto: %v, startPort: %v, endPort: %v, groups: %v, host: %v, ip: %v, localIp: %v, caName: %v, caSha: %s\",\n\t\tincoming, proto, startPort, endPort, groups, host, sIp, lIp, caName, caSha,\n\t)\n\tf.rules += ruleString + \"\\n\"\n\n\tdirection := \"incoming\"\n\tif !incoming {\n\t\tdirection = \"outgoing\"\n\t}\n\tf.l.WithField(\"firewallRule\", m{\"direction\": direction, \"proto\": proto, \"startPort\": startPort, \"endPort\": endPort, \"groups\": groups, \"host\": host, \"ip\": sIp, \"localIp\": lIp, \"caName\": caName, \"caSha\": caSha}).\n\t\tInfo(\"Firewall rule added\")\n\n\tvar (\n\t\tft *FirewallTable\n\t\tfp firewallPort\n\t)\n\n\tif incoming {\n\t\tft = f.InRules\n\t} else {\n\t\tft = f.OutRules\n\t}\n\n\tswitch proto {\n\tcase firewall.ProtoTCP:\n\t\tfp = ft.TCP\n\tcase firewall.ProtoUDP:\n\t\tfp = ft.UDP\n\tcase firewall.ProtoICMP:\n\t\tfp = ft.ICMP\n\tcase firewall.ProtoAny:\n\t\tfp = ft.AnyProto\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown protocol %v\", proto)\n\t}\n\n\treturn fp.addRule(f, startPort, endPort, groups, host, ip, localIp, caName, caSha)\n}\n\n// GetRuleHash returns a hash representation of all inbound and outbound rules\nfunc (f *Firewall) GetRuleHash() string {\n\tsum := sha256.Sum256([]byte(f.rules))\n\treturn hex.EncodeToString(sum[:])\n}\n\n// GetRuleHashFNV returns a uint32 FNV-1 hash representation the rules, for use as a metric value\nfunc (f *Firewall) GetRuleHashFNV() uint32 {\n\th := fnv.New32a()\n\th.Write([]byte(f.rules))\n\treturn h.Sum32()\n}\n\n// GetRuleHashes returns both the sha256 and FNV-1 hashes, suitable for logging\nfunc (f *Firewall) GetRuleHashes() string {\n\treturn \"SHA:\" + f.GetRuleHash() + \",FNV:\" + strconv.FormatUint(uint64(f.GetRuleHashFNV()), 10)\n}\n\nfunc AddFirewallRulesFromConfig(l *logrus.Logger, inbound bool, c *config.C, fw FirewallInterface) error {\n\tvar table string\n\tif inbound {\n\t\ttable = \"firewall.inbound\"\n\t} else {\n\t\ttable = \"firewall.outbound\"\n\t}\n\n\tr := c.Get(table)\n\tif r == nil {\n\t\treturn nil\n\t}\n\n\trs, ok := r.([]interface{})\n\tif !ok {\n\t\treturn fmt.Errorf(\"%s failed to parse, should be an array of rules\", table)\n\t}\n\n\tfor i, t := range rs {\n\t\tvar groups []string\n\t\tr, err := convertRule(l, t, table, i)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s rule #%v; %s\", table, i, err)\n\t\t}\n\n\t\tif r.Code != \"\" && r.Port != \"\" {\n\t\t\treturn fmt.Errorf(\"%s rule #%v; only one of port or code should be provided\", table, i)\n\t\t}\n\n\t\tif r.Host == \"\" && len(r.Groups) == 0 && r.Group == \"\" && r.Cidr == \"\" && r.LocalCidr == \"\" && r.CAName == \"\" && r.CASha == \"\" {\n\t\t\treturn fmt.Errorf(\"%s rule #%v; at least one of host, group, cidr, local_cidr, ca_name, or ca_sha must be provided\", table, i)\n\t\t}\n\n\t\tif len(r.Groups) > 0 {\n\t\t\tgroups = r.Groups\n\t\t}\n\n\t\tif r.Group != \"\" {\n\t\t\t// Check if we have both groups and group provided in the rule config\n\t\t\tif len(groups) > 0 {\n\t\t\t\treturn fmt.Errorf(\"%s rule #%v; only one of group or groups should be defined, both provided\", table, i)\n\t\t\t}\n\n\t\t\tgroups = []string{r.Group}\n\t\t}\n\n\t\tvar sPort, errPort string\n\t\tif r.Code != \"\" {\n\t\t\terrPort = \"code\"\n\t\t\tsPort = r.Code\n\t\t} else {\n\t\t\terrPort = \"port\"\n\t\t\tsPort = r.Port\n\t\t}\n\n\t\tstartPort, endPort, err := parsePort(sPort)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s rule #%v; %s %s\", table, i, errPort, err)\n\t\t}\n\n\t\tvar proto uint8\n\t\tswitch r.Proto {\n\t\tcase \"any\":\n\t\t\tproto = firewall.ProtoAny\n\t\tcase \"tcp\":\n\t\t\tproto = firewall.ProtoTCP\n\t\tcase \"udp\":\n\t\t\tproto = firewall.ProtoUDP\n\t\tcase \"icmp\":\n\t\t\tproto = firewall.ProtoICMP\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"%s rule #%v; proto was not understood; `%s`\", table, i, r.Proto)\n\t\t}\n\n\t\tvar cidr netip.Prefix\n\t\tif r.Cidr != \"\" {\n\t\t\tcidr, err = netip.ParsePrefix(r.Cidr)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s rule #%v; cidr did not parse; %s\", table, i, err)\n\t\t\t}\n\t\t}\n\n\t\tvar localCidr netip.Prefix\n\t\tif r.LocalCidr != \"\" {\n\t\t\tlocalCidr, err = netip.ParsePrefix(r.LocalCidr)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s rule #%v; local_cidr did not parse; %s\", table, i, err)\n\t\t\t}\n\t\t}\n\n\t\terr = fw.AddRule(inbound, proto, startPort, endPort, groups, r.Host, cidr, localCidr, r.CAName, r.CASha)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s rule #%v; `%s`\", table, i, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nvar ErrInvalidRemoteIP = errors.New(\"remote IP is not in remote certificate subnets\")\nvar ErrInvalidLocalIP = errors.New(\"local IP is not in list of handled local IPs\")\nvar ErrNoMatchingRule = errors.New(\"no matching rule in firewall table\")\n\n// Drop returns an error if the packet should be dropped, explaining why. It\n// returns nil if the packet should not be dropped.\nfunc (f *Firewall) Drop(fp firewall.Packet, incoming bool, h *HostInfo, caPool *cert.CAPool, localCache firewall.ConntrackCache) error {\n\t// Check if we spoke to this tuple, if we did then allow this packet\n\tif f.inConns(fp, h, caPool, localCache) {\n\t\treturn nil\n\t}\n\n\t// Make sure remote address matches nebula certificate\n\tif remoteCidr := h.remoteCidr; remoteCidr != nil {\n\t\t//TODO: this would be better if we had a least specific match lookup, could waste time here, need to benchmark since the algo is different\n\t\t_, ok := remoteCidr.Lookup(fp.RemoteIP)\n\t\tif !ok {\n\t\t\tf.metrics(incoming).droppedRemoteIP.Inc(1)\n\t\t\treturn ErrInvalidRemoteIP\n\t\t}\n\t} else {\n\t\t// Simple case: Certificate has one IP and no subnets\n\t\tif fp.RemoteIP != h.vpnIp {\n\t\t\tf.metrics(incoming).droppedRemoteIP.Inc(1)\n\t\t\treturn ErrInvalidRemoteIP\n\t\t}\n\t}\n\n\t// Make sure we are supposed to be handling this local ip address\n\t//TODO: this would be better if we had a least specific match lookup, could waste time here, need to benchmark since the algo is different\n\t_, ok := f.localIps.Lookup(fp.LocalIP)\n\tif !ok {\n\t\tf.metrics(incoming).droppedLocalIP.Inc(1)\n\t\treturn ErrInvalidLocalIP\n\t}\n\n\ttable := f.OutRules\n\tif incoming {\n\t\ttable = f.InRules\n\t}\n\n\t// We now know which firewall table to check against\n\tif !table.match(fp, incoming, h.ConnectionState.peerCert, caPool) {\n\t\tf.metrics(incoming).droppedNoRule.Inc(1)\n\t\treturn ErrNoMatchingRule\n\t}\n\n\t// We always want to conntrack since it is a faster operation\n\tf.addConn(fp, incoming)\n\n\treturn nil\n}\n\nfunc (f *Firewall) metrics(incoming bool) firewallMetrics {\n\tif incoming {\n\t\treturn f.incomingMetrics\n\t} else {\n\t\treturn f.outgoingMetrics\n\t}\n}\n\n// Destroy cleans up any known cyclical references so the object can be free'd my GC. This should be called if a new\n// firewall object is created\nfunc (f *Firewall) Destroy() {\n\t//TODO: clean references if/when needed\n}\n\nfunc (f *Firewall) EmitStats() {\n\tconntrack := f.Conntrack\n\tconntrack.Lock()\n\tconntrackCount := len(conntrack.Conns)\n\tconntrack.Unlock()\n\tmetrics.GetOrRegisterGauge(\"firewall.conntrack.count\", nil).Update(int64(conntrackCount))\n\tmetrics.GetOrRegisterGauge(\"firewall.rules.version\", nil).Update(int64(f.rulesVersion))\n\tmetrics.GetOrRegisterGauge(\"firewall.rules.hash\", nil).Update(int64(f.GetRuleHashFNV()))\n}\n\nfunc (f *Firewall) inConns(fp firewall.Packet, h *HostInfo, caPool *cert.CAPool, localCache firewall.ConntrackCache) bool {\n\tif localCache != nil {\n\t\tif _, ok := localCache[fp]; ok {\n\t\t\treturn true\n\t\t}\n\t}\n\tconntrack := f.Conntrack\n\tconntrack.Lock()\n\n\t// Purge every time we test\n\tep, has := conntrack.TimerWheel.Purge()\n\tif has {\n\t\tf.evict(ep)\n\t}\n\n\tc, ok := conntrack.Conns[fp]\n\n\tif !ok {\n\t\tconntrack.Unlock()\n\t\treturn false\n\t}\n\n\tif c.rulesVersion != f.rulesVersion {\n\t\t// This conntrack entry was for an older rule set, validate\n\t\t// it still passes with the current rule set\n\t\ttable := f.OutRules\n\t\tif c.incoming {\n\t\t\ttable = f.InRules\n\t\t}\n\n\t\t// We now know which firewall table to check against\n\t\tif !table.match(fp, c.incoming, h.ConnectionState.peerCert, caPool) {\n\t\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\t\th.logger(f.l).\n\t\t\t\t\tWithField(\"fwPacket\", fp).\n\t\t\t\t\tWithField(\"incoming\", c.incoming).\n\t\t\t\t\tWithField(\"rulesVersion\", f.rulesVersion).\n\t\t\t\t\tWithField(\"oldRulesVersion\", c.rulesVersion).\n\t\t\t\t\tDebugln(\"dropping old conntrack entry, does not match new ruleset\")\n\t\t\t}\n\t\t\tdelete(conntrack.Conns, fp)\n\t\t\tconntrack.Unlock()\n\t\t\treturn false\n\t\t}\n\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\th.logger(f.l).\n\t\t\t\tWithField(\"fwPacket\", fp).\n\t\t\t\tWithField(\"incoming\", c.incoming).\n\t\t\t\tWithField(\"rulesVersion\", f.rulesVersion).\n\t\t\t\tWithField(\"oldRulesVersion\", c.rulesVersion).\n\t\t\t\tDebugln(\"keeping old conntrack entry, does match new ruleset\")\n\t\t}\n\n\t\tc.rulesVersion = f.rulesVersion\n\t}\n\n\tswitch fp.Protocol {\n\tcase firewall.ProtoTCP:\n\t\tc.Expires = time.Now().Add(f.TCPTimeout)\n\tcase firewall.ProtoUDP:\n\t\tc.Expires = time.Now().Add(f.UDPTimeout)\n\tdefault:\n\t\tc.Expires = time.Now().Add(f.DefaultTimeout)\n\t}\n\n\tconntrack.Unlock()\n\n\tif localCache != nil {\n\t\tlocalCache[fp] = struct{}{}\n\t}\n\n\treturn true\n}\n\nfunc (f *Firewall) addConn(fp firewall.Packet, incoming bool) {\n\tvar timeout time.Duration\n\tc := &conn{}\n\n\tswitch fp.Protocol {\n\tcase firewall.ProtoTCP:\n\t\ttimeout = f.TCPTimeout\n\tcase firewall.ProtoUDP:\n\t\ttimeout = f.UDPTimeout\n\tdefault:\n\t\ttimeout = f.DefaultTimeout\n\t}\n\n\tconntrack := f.Conntrack\n\tconntrack.Lock()\n\tif _, ok := conntrack.Conns[fp]; !ok {\n\t\tconntrack.TimerWheel.Advance(time.Now())\n\t\tconntrack.TimerWheel.Add(fp, timeout)\n\t}\n\n\t// Record which rulesVersion allowed this connection, so we can retest after\n\t// firewall reload\n\tc.incoming = incoming\n\tc.rulesVersion = f.rulesVersion\n\tc.Expires = time.Now().Add(timeout)\n\tconntrack.Conns[fp] = c\n\tconntrack.Unlock()\n}\n\n// Evict checks if a conntrack entry has expired, if so it is removed, if not it is re-added to the wheel\n// Caller must own the connMutex lock!\nfunc (f *Firewall) evict(p firewall.Packet) {\n\t// Are we still tracking this conn?\n\tconntrack := f.Conntrack\n\tt, ok := conntrack.Conns[p]\n\tif !ok {\n\t\treturn\n\t}\n\n\tnewT := t.Expires.Sub(time.Now())\n\n\t// Timeout is in the future, re-add the timer\n\tif newT > 0 {\n\t\tconntrack.TimerWheel.Advance(time.Now())\n\t\tconntrack.TimerWheel.Add(p, newT)\n\t\treturn\n\t}\n\n\t// This conn is done\n\tdelete(conntrack.Conns, p)\n}\n\nfunc (ft *FirewallTable) match(p firewall.Packet, incoming bool, c *cert.CachedCertificate, caPool *cert.CAPool) bool {\n\tif ft.AnyProto.match(p, incoming, c, caPool) {\n\t\treturn true\n\t}\n\n\tswitch p.Protocol {\n\tcase firewall.ProtoTCP:\n\t\tif ft.TCP.match(p, incoming, c, caPool) {\n\t\t\treturn true\n\t\t}\n\tcase firewall.ProtoUDP:\n\t\tif ft.UDP.match(p, incoming, c, caPool) {\n\t\t\treturn true\n\t\t}\n\tcase firewall.ProtoICMP:\n\t\tif ft.ICMP.match(p, incoming, c, caPool) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (fp firewallPort) addRule(f *Firewall, startPort int32, endPort int32, groups []string, host string, ip, localIp netip.Prefix, caName string, caSha string) error {\n\tif startPort > endPort {\n\t\treturn fmt.Errorf(\"start port was lower than end port\")\n\t}\n\n\tfor i := startPort; i <= endPort; i++ {\n\t\tif _, ok := fp[i]; !ok {\n\t\t\tfp[i] = &FirewallCA{\n\t\t\t\tCANames: make(map[string]*FirewallRule),\n\t\t\t\tCAShas:  make(map[string]*FirewallRule),\n\t\t\t}\n\t\t}\n\n\t\tif err := fp[i].addRule(f, groups, host, ip, localIp, caName, caSha); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (fp firewallPort) match(p firewall.Packet, incoming bool, c *cert.CachedCertificate, caPool *cert.CAPool) bool {\n\t// We don't have any allowed ports, bail\n\tif fp == nil {\n\t\treturn false\n\t}\n\n\tvar port int32\n\n\tif p.Fragment {\n\t\tport = firewall.PortFragment\n\t} else if incoming {\n\t\tport = int32(p.LocalPort)\n\t} else {\n\t\tport = int32(p.RemotePort)\n\t}\n\n\tif fp[port].match(p, c, caPool) {\n\t\treturn true\n\t}\n\n\treturn fp[firewall.PortAny].match(p, c, caPool)\n}\n\nfunc (fc *FirewallCA) addRule(f *Firewall, groups []string, host string, ip, localIp netip.Prefix, caName, caSha string) error {\n\tfr := func() *FirewallRule {\n\t\treturn &FirewallRule{\n\t\t\tHosts:  make(map[string]*firewallLocalCIDR),\n\t\t\tGroups: make([]*firewallGroups, 0),\n\t\t\tCIDR:   new(bart.Table[*firewallLocalCIDR]),\n\t\t}\n\t}\n\n\tif caSha == \"\" && caName == \"\" {\n\t\tif fc.Any == nil {\n\t\t\tfc.Any = fr()\n\t\t}\n\n\t\treturn fc.Any.addRule(f, groups, host, ip, localIp)\n\t}\n\n\tif caSha != \"\" {\n\t\tif _, ok := fc.CAShas[caSha]; !ok {\n\t\t\tfc.CAShas[caSha] = fr()\n\t\t}\n\t\terr := fc.CAShas[caSha].addRule(f, groups, host, ip, localIp)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif caName != \"\" {\n\t\tif _, ok := fc.CANames[caName]; !ok {\n\t\t\tfc.CANames[caName] = fr()\n\t\t}\n\t\terr := fc.CANames[caName].addRule(f, groups, host, ip, localIp)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (fc *FirewallCA) match(p firewall.Packet, c *cert.CachedCertificate, caPool *cert.CAPool) bool {\n\tif fc == nil {\n\t\treturn false\n\t}\n\n\tif fc.Any.match(p, c) {\n\t\treturn true\n\t}\n\n\tif t, ok := fc.CAShas[c.Certificate.Issuer()]; ok {\n\t\tif t.match(p, c) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\ts, err := caPool.GetCAForCert(c.Certificate)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\treturn fc.CANames[s.Certificate.Name()].match(p, c)\n}\n\nfunc (fr *FirewallRule) addRule(f *Firewall, groups []string, host string, ip, localCIDR netip.Prefix) error {\n\tflc := func() *firewallLocalCIDR {\n\t\treturn &firewallLocalCIDR{\n\t\t\tLocalCIDR: new(bart.Table[struct{}]),\n\t\t}\n\t}\n\n\tif fr.isAny(groups, host, ip) {\n\t\tif fr.Any == nil {\n\t\t\tfr.Any = flc()\n\t\t}\n\n\t\treturn fr.Any.addRule(f, localCIDR)\n\t}\n\n\tif len(groups) > 0 {\n\t\tnlc := flc()\n\t\terr := nlc.addRule(f, localCIDR)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfr.Groups = append(fr.Groups, &firewallGroups{\n\t\t\tGroups:    groups,\n\t\t\tLocalCIDR: nlc,\n\t\t})\n\t}\n\n\tif host != \"\" {\n\t\tnlc := fr.Hosts[host]\n\t\tif nlc == nil {\n\t\t\tnlc = flc()\n\t\t}\n\t\terr := nlc.addRule(f, localCIDR)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfr.Hosts[host] = nlc\n\t}\n\n\tif ip.IsValid() {\n\t\tnlc, _ := fr.CIDR.Get(ip)\n\t\tif nlc == nil {\n\t\t\tnlc = flc()\n\t\t}\n\t\terr := nlc.addRule(f, localCIDR)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfr.CIDR.Insert(ip, nlc)\n\t}\n\n\treturn nil\n}\n\nfunc (fr *FirewallRule) isAny(groups []string, host string, ip netip.Prefix) bool {\n\tif len(groups) == 0 && host == \"\" && !ip.IsValid() {\n\t\treturn true\n\t}\n\n\tfor _, group := range groups {\n\t\tif group == \"any\" {\n\t\t\treturn true\n\t\t}\n\t}\n\n\tif host == \"any\" {\n\t\treturn true\n\t}\n\n\tif ip.IsValid() && ip.Bits() == 0 {\n\t\treturn true\n\t}\n\n\treturn false\n}\n\nfunc (fr *FirewallRule) match(p firewall.Packet, c *cert.CachedCertificate) bool {\n\tif fr == nil {\n\t\treturn false\n\t}\n\n\t// Shortcut path for if groups, hosts, or cidr contained an `any`\n\tif fr.Any.match(p, c) {\n\t\treturn true\n\t}\n\n\t// Need any of group, host, or cidr to match\n\tfor _, sg := range fr.Groups {\n\t\tfound := false\n\n\t\tfor _, g := range sg.Groups {\n\t\t\tif _, ok := c.InvertedGroups[g]; !ok {\n\t\t\t\tfound = false\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tfound = true\n\t\t}\n\n\t\tif found && sg.LocalCIDR.match(p, c) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\tif fr.Hosts != nil {\n\t\tif flc, ok := fr.Hosts[c.Certificate.Name()]; ok {\n\t\t\tif flc.match(p, c) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\tmatched := false\n\tprefix := netip.PrefixFrom(p.RemoteIP, p.RemoteIP.BitLen())\n\tfr.CIDR.EachLookupPrefix(prefix, func(prefix netip.Prefix, val *firewallLocalCIDR) bool {\n\t\tif prefix.Contains(p.RemoteIP) && val.match(p, c) {\n\t\t\tmatched = true\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t})\n\treturn matched\n}\n\nfunc (flc *firewallLocalCIDR) addRule(f *Firewall, localIp netip.Prefix) error {\n\tif !localIp.IsValid() {\n\t\tif !f.hasUnsafeNetworks || f.defaultLocalCIDRAny {\n\t\t\tflc.Any = true\n\t\t\treturn nil\n\t\t}\n\n\t\tlocalIp = f.assignedCIDR\n\t} else if localIp.Bits() == 0 {\n\t\tflc.Any = true\n\t}\n\n\tflc.LocalCIDR.Insert(localIp, struct{}{})\n\treturn nil\n}\n\nfunc (flc *firewallLocalCIDR) match(p firewall.Packet, c *cert.CachedCertificate) bool {\n\tif flc == nil {\n\t\treturn false\n\t}\n\n\tif flc.Any {\n\t\treturn true\n\t}\n\n\t_, ok := flc.LocalCIDR.Lookup(p.LocalIP)\n\treturn ok\n}\n\ntype rule struct {\n\tPort      string\n\tCode      string\n\tProto     string\n\tHost      string\n\tGroup     string\n\tGroups    []string\n\tCidr      string\n\tLocalCidr string\n\tCAName    string\n\tCASha     string\n}\n\nfunc convertRule(l *logrus.Logger, p interface{}, table string, i int) (rule, error) {\n\tr := rule{}\n\n\tm, ok := p.(map[interface{}]interface{})\n\tif !ok {\n\t\treturn r, errors.New(\"could not parse rule\")\n\t}\n\n\ttoString := func(k string, m map[interface{}]interface{}) string {\n\t\tv, ok := m[k]\n\t\tif !ok {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn fmt.Sprintf(\"%v\", v)\n\t}\n\n\tr.Port = toString(\"port\", m)\n\tr.Code = toString(\"code\", m)\n\tr.Proto = toString(\"proto\", m)\n\tr.Host = toString(\"host\", m)\n\tr.Cidr = toString(\"cidr\", m)\n\tr.LocalCidr = toString(\"local_cidr\", m)\n\tr.CAName = toString(\"ca_name\", m)\n\tr.CASha = toString(\"ca_sha\", m)\n\n\t// Make sure group isn't an array\n\tif v, ok := m[\"group\"].([]interface{}); ok {\n\t\tif len(v) > 1 {\n\t\t\treturn r, errors.New(\"group should contain a single value, an array with more than one entry was provided\")\n\t\t}\n\n\t\tl.Warnf(\"%s rule #%v; group was an array with a single value, converting to simple value\", table, i)\n\t\tm[\"group\"] = v[0]\n\t}\n\tr.Group = toString(\"group\", m)\n\n\tif rg, ok := m[\"groups\"]; ok {\n\t\tswitch reflect.TypeOf(rg).Kind() {\n\t\tcase reflect.Slice:\n\t\t\tv := reflect.ValueOf(rg)\n\t\t\tr.Groups = make([]string, v.Len())\n\t\t\tfor i := 0; i < v.Len(); i++ {\n\t\t\t\tr.Groups[i] = v.Index(i).Interface().(string)\n\t\t\t}\n\t\tcase reflect.String:\n\t\t\tr.Groups = []string{rg.(string)}\n\t\tdefault:\n\t\t\tr.Groups = []string{fmt.Sprintf(\"%v\", rg)}\n\t\t}\n\t}\n\n\treturn r, nil\n}\n\nfunc parsePort(s string) (startPort, endPort int32, err error) {\n\tif s == \"any\" {\n\t\tstartPort = firewall.PortAny\n\t\tendPort = firewall.PortAny\n\n\t} else if s == \"fragment\" {\n\t\tstartPort = firewall.PortFragment\n\t\tendPort = firewall.PortFragment\n\n\t} else if strings.Contains(s, `-`) {\n\t\tsPorts := strings.SplitN(s, `-`, 2)\n\t\tsPorts[0] = strings.Trim(sPorts[0], \" \")\n\t\tsPorts[1] = strings.Trim(sPorts[1], \" \")\n\n\t\tif len(sPorts) != 2 || sPorts[0] == \"\" || sPorts[1] == \"\" {\n\t\t\treturn 0, 0, fmt.Errorf(\"appears to be a range but could not be parsed; `%s`\", s)\n\t\t}\n\n\t\trStartPort, err := strconv.Atoi(sPorts[0])\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"beginning range was not a number; `%s`\", sPorts[0])\n\t\t}\n\n\t\trEndPort, err := strconv.Atoi(sPorts[1])\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"ending range was not a number; `%s`\", sPorts[1])\n\t\t}\n\n\t\tstartPort = int32(rStartPort)\n\t\tendPort = int32(rEndPort)\n\n\t\tif startPort == firewall.PortAny {\n\t\t\tendPort = firewall.PortAny\n\t\t}\n\n\t} else {\n\t\trPort, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"was not a number; `%s`\", s)\n\t\t}\n\t\tstartPort = int32(rPort)\n\t\tendPort = startPort\n\t}\n\n\treturn\n}\n"
        },
        {
          "name": "firewall",
          "type": "tree",
          "content": null
        },
        {
          "name": "firewall_test.go",
          "type": "blob",
          "size": 30.05859375,
          "content": "package nebula\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"math\"\n\t\"net/netip\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNewFirewall(t *testing.T) {\n\tl := test.NewLogger()\n\tc := &dummyCert{}\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tconntrack := fw.Conntrack\n\tassert.NotNil(t, conntrack)\n\tassert.NotNil(t, conntrack.Conns)\n\tassert.NotNil(t, conntrack.TimerWheel)\n\tassert.NotNil(t, fw.InRules)\n\tassert.NotNil(t, fw.OutRules)\n\tassert.Equal(t, time.Second, fw.TCPTimeout)\n\tassert.Equal(t, time.Minute, fw.UDPTimeout)\n\tassert.Equal(t, time.Hour, fw.DefaultTimeout)\n\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n\n\tfw = NewFirewall(l, time.Second, time.Hour, time.Minute, c)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n\n\tfw = NewFirewall(l, time.Hour, time.Second, time.Minute, c)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n\n\tfw = NewFirewall(l, time.Hour, time.Minute, time.Second, c)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n\n\tfw = NewFirewall(l, time.Minute, time.Hour, time.Second, c)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n\n\tfw = NewFirewall(l, time.Minute, time.Second, time.Hour, c)\n\tassert.Equal(t, time.Hour, conntrack.TimerWheel.wheelDuration)\n\tassert.Equal(t, 3602, conntrack.TimerWheel.wheelLen)\n}\n\nfunc TestFirewall_AddRule(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\tc := &dummyCert{}\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.NotNil(t, fw.InRules)\n\tassert.NotNil(t, fw.OutRules)\n\n\tti, err := netip.ParsePrefix(\"1.2.3.4/32\")\n\tassert.NoError(t, err)\n\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoTCP, 1, 1, []string{}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\t// An empty rule is any\n\tassert.True(t, fw.InRules.TCP[1].Any.Any.Any)\n\tassert.Empty(t, fw.InRules.TCP[1].Any.Groups)\n\tassert.Empty(t, fw.InRules.TCP[1].Any.Hosts)\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoUDP, 1, 1, []string{\"g1\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tassert.Nil(t, fw.InRules.UDP[1].Any.Any)\n\tassert.Contains(t, fw.InRules.UDP[1].Any.Groups[0].Groups, \"g1\")\n\tassert.Empty(t, fw.InRules.UDP[1].Any.Hosts)\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoICMP, 1, 1, []string{}, \"h1\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tassert.Nil(t, fw.InRules.ICMP[1].Any.Any)\n\tassert.Empty(t, fw.InRules.ICMP[1].Any.Groups)\n\tassert.Contains(t, fw.InRules.ICMP[1].Any.Hosts, \"h1\")\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(false, firewall.ProtoAny, 1, 1, []string{}, \"\", ti, netip.Prefix{}, \"\", \"\"))\n\tassert.Nil(t, fw.OutRules.AnyProto[1].Any.Any)\n\t_, ok := fw.OutRules.AnyProto[1].Any.CIDR.Get(ti)\n\tassert.True(t, ok)\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(false, firewall.ProtoAny, 1, 1, []string{}, \"\", netip.Prefix{}, ti, \"\", \"\"))\n\tassert.NotNil(t, fw.OutRules.AnyProto[1].Any.Any)\n\t_, ok = fw.OutRules.AnyProto[1].Any.Any.LocalCIDR.Get(ti)\n\tassert.True(t, ok)\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoUDP, 1, 1, []string{\"g1\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"ca-name\", \"\"))\n\tassert.Contains(t, fw.InRules.UDP[1].CANames, \"ca-name\")\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoUDP, 1, 1, []string{\"g1\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"ca-sha\"))\n\tassert.Contains(t, fw.InRules.UDP[1].CAShas, \"ca-sha\")\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Nil(t, fw.AddRule(false, firewall.ProtoAny, 0, 0, []string{}, \"any\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tassert.True(t, fw.OutRules.AnyProto[0].Any.Any.Any)\n\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tanyIp, err := netip.ParsePrefix(\"0.0.0.0/0\")\n\tassert.NoError(t, err)\n\n\tassert.Nil(t, fw.AddRule(false, firewall.ProtoAny, 0, 0, []string{}, \"\", anyIp, netip.Prefix{}, \"\", \"\"))\n\tassert.True(t, fw.OutRules.AnyProto[0].Any.Any.Any)\n\n\t// Test error conditions\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c)\n\tassert.Error(t, fw.AddRule(true, math.MaxUint8, 0, 0, []string{}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tassert.Error(t, fw.AddRule(true, firewall.ProtoAny, 10, 0, []string{}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n}\n\nfunc TestFirewall_Drop(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\tp := firewall.Packet{\n\t\tLocalIP:    netip.MustParseAddr(\"1.2.3.4\"),\n\t\tRemoteIP:   netip.MustParseAddr(\"1.2.3.4\"),\n\t\tLocalPort:  10,\n\t\tRemotePort: 90,\n\t\tProtocol:   firewall.ProtoUDP,\n\t\tFragment:   false,\n\t}\n\n\tc := dummyCert{\n\t\tname:     \"host1\",\n\t\tnetworks: []netip.Prefix{netip.MustParsePrefix(\"1.2.3.4/24\")},\n\t\tgroups:   []string{\"default-group\"},\n\t\tissuer:   \"signer-shasum\",\n\t}\n\th := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &cert.CachedCertificate{\n\t\t\t\tCertificate:    &c,\n\t\t\t\tInvertedGroups: map[string]struct{}{\"default-group\": {}},\n\t\t\t},\n\t\t},\n\t\tvpnIp: netip.MustParseAddr(\"1.2.3.4\"),\n\t}\n\th.CreateRemoteCIDR(&c)\n\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, &c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"any\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tcp := cert.NewCAPool()\n\n\t// Drop outbound\n\tassert.Equal(t, ErrNoMatchingRule, fw.Drop(p, false, &h, cp, nil))\n\t// Allow inbound\n\tresetConntrack(fw)\n\tassert.NoError(t, fw.Drop(p, true, &h, cp, nil))\n\t// Allow outbound because conntrack\n\tassert.NoError(t, fw.Drop(p, false, &h, cp, nil))\n\n\t// test remote mismatch\n\toldRemote := p.RemoteIP\n\tp.RemoteIP = netip.MustParseAddr(\"1.2.3.10\")\n\tassert.Equal(t, fw.Drop(p, false, &h, cp, nil), ErrInvalidRemoteIP)\n\tp.RemoteIP = oldRemote\n\n\t// ensure signer doesn't get in the way of group checks\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, &c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"nope\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"signer-shasum\"))\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"default-group\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"signer-shasum-bad\"))\n\tassert.Equal(t, fw.Drop(p, true, &h, cp, nil), ErrNoMatchingRule)\n\n\t// test caSha doesn't drop on match\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, &c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"nope\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"signer-shasum-bad\"))\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"default-group\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"signer-shasum\"))\n\tassert.NoError(t, fw.Drop(p, true, &h, cp, nil))\n\n\t// ensure ca name doesn't get in the way of group checks\n\tcp.CAs[\"signer-shasum\"] = &cert.CachedCertificate{Certificate: &dummyCert{name: \"ca-good\"}}\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, &c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"nope\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"ca-good\", \"\"))\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"default-group\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"ca-good-bad\", \"\"))\n\tassert.Equal(t, fw.Drop(p, true, &h, cp, nil), ErrNoMatchingRule)\n\n\t// test caName doesn't drop on match\n\tcp.CAs[\"signer-shasum\"] = &cert.CachedCertificate{Certificate: &dummyCert{name: \"ca-good\"}}\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, &c)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"nope\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"ca-good-bad\", \"\"))\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"default-group\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"ca-good\", \"\"))\n\tassert.NoError(t, fw.Drop(p, true, &h, cp, nil))\n}\n\nfunc BenchmarkFirewallTable_match(b *testing.B) {\n\tf := &Firewall{}\n\tft := FirewallTable{\n\t\tTCP: firewallPort{},\n\t}\n\n\tpfix := netip.MustParsePrefix(\"172.1.1.1/32\")\n\t_ = ft.TCP.addRule(f, 10, 10, []string{\"good-group\"}, \"good-host\", pfix, netip.Prefix{}, \"\", \"\")\n\t_ = ft.TCP.addRule(f, 100, 100, []string{\"good-group\"}, \"good-host\", netip.Prefix{}, pfix, \"\", \"\")\n\tcp := cert.NewCAPool()\n\n\tb.Run(\"fail on proto\", func(b *testing.B) {\n\t\t// This benchmark is showing us the cost of failing to match the protocol\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.False(b, ft.match(firewall.Packet{Protocol: firewall.ProtoUDP}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass proto, fail on port\", func(b *testing.B) {\n\t\t// This benchmark is showing us the cost of matching a specific protocol but failing to match the port\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.False(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 1}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass proto, port, fail on local CIDR\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{},\n\t\t}\n\t\tip := netip.MustParsePrefix(\"9.254.254.254/32\")\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.False(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 100, LocalIP: ip.Addr()}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass proto, port, any local CIDR, fail all group, name, and cidr\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{\n\t\t\t\tname:     \"nope\",\n\t\t\t\tnetworks: []netip.Prefix{netip.MustParsePrefix(\"9.254.254.245/32\")},\n\t\t\t},\n\t\t\tInvertedGroups: map[string]struct{}{\"nope\": {}},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.False(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 10}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass proto, port, specific local CIDR, fail all group, name, and cidr\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{\n\t\t\t\tname:     \"nope\",\n\t\t\t\tnetworks: []netip.Prefix{netip.MustParsePrefix(\"9.254.254.245/32\")},\n\t\t\t},\n\t\t\tInvertedGroups: map[string]struct{}{\"nope\": {}},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.False(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 100, LocalIP: pfix.Addr()}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass on group on any local cidr\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{\n\t\t\t\tname: \"nope\",\n\t\t\t},\n\t\t\tInvertedGroups: map[string]struct{}{\"good-group\": {}},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.True(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 10}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass on group on specific local cidr\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{\n\t\t\t\tname: \"nope\",\n\t\t\t},\n\t\t\tInvertedGroups: map[string]struct{}{\"good-group\": {}},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tassert.True(b, ft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 100, LocalIP: pfix.Addr()}, true, c, cp))\n\t\t}\n\t})\n\n\tb.Run(\"pass on name\", func(b *testing.B) {\n\t\tc := &cert.CachedCertificate{\n\t\t\tCertificate: &dummyCert{\n\t\t\t\tname: \"good-host\",\n\t\t\t},\n\t\t\tInvertedGroups: map[string]struct{}{\"nope\": {}},\n\t\t}\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tft.match(firewall.Packet{Protocol: firewall.ProtoTCP, LocalPort: 10}, true, c, cp)\n\t\t}\n\t})\n}\n\nfunc TestFirewall_Drop2(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\tp := firewall.Packet{\n\t\tLocalIP:    netip.MustParseAddr(\"1.2.3.4\"),\n\t\tRemoteIP:   netip.MustParseAddr(\"1.2.3.4\"),\n\t\tLocalPort:  10,\n\t\tRemotePort: 90,\n\t\tProtocol:   firewall.ProtoUDP,\n\t\tFragment:   false,\n\t}\n\n\tnetwork := netip.MustParsePrefix(\"1.2.3.4/24\")\n\n\tc := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host1\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t},\n\t\tInvertedGroups: map[string]struct{}{\"default-group\": {}, \"test-group\": {}},\n\t}\n\th := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c,\n\t\t},\n\t\tvpnIp: network.Addr(),\n\t}\n\th.CreateRemoteCIDR(c.Certificate)\n\n\tc1 := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host1\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t},\n\t\tInvertedGroups: map[string]struct{}{\"default-group\": {}, \"test-group-not\": {}},\n\t}\n\th1 := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c1,\n\t\t},\n\t}\n\th1.CreateRemoteCIDR(c1.Certificate)\n\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, c.Certificate)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"default-group\", \"test-group\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tcp := cert.NewCAPool()\n\n\t// h1/c1 lacks the proper groups\n\tassert.Error(t, fw.Drop(p, true, &h1, cp, nil), ErrNoMatchingRule)\n\t// c has the proper groups\n\tresetConntrack(fw)\n\tassert.NoError(t, fw.Drop(p, true, &h, cp, nil))\n}\n\nfunc TestFirewall_Drop3(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\tp := firewall.Packet{\n\t\tLocalIP:    netip.MustParseAddr(\"1.2.3.4\"),\n\t\tRemoteIP:   netip.MustParseAddr(\"1.2.3.4\"),\n\t\tLocalPort:  1,\n\t\tRemotePort: 1,\n\t\tProtocol:   firewall.ProtoUDP,\n\t\tFragment:   false,\n\t}\n\n\tnetwork := netip.MustParsePrefix(\"1.2.3.4/24\")\n\tc := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host-owner\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t},\n\t}\n\n\tc1 := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host1\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t\tissuer:   \"signer-sha-bad\",\n\t\t},\n\t}\n\th1 := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c1,\n\t\t},\n\t\tvpnIp: network.Addr(),\n\t}\n\th1.CreateRemoteCIDR(c1.Certificate)\n\n\tc2 := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host2\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t\tissuer:   \"signer-sha\",\n\t\t},\n\t}\n\th2 := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c2,\n\t\t},\n\t\tvpnIp: network.Addr(),\n\t}\n\th2.CreateRemoteCIDR(c2.Certificate)\n\n\tc3 := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host3\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t\tissuer:   \"signer-sha-bad\",\n\t\t},\n\t}\n\th3 := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c3,\n\t\t},\n\t\tvpnIp: network.Addr(),\n\t}\n\th3.CreateRemoteCIDR(c3.Certificate)\n\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, c.Certificate)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 1, 1, []string{}, \"host1\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 1, 1, []string{}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"signer-sha\"))\n\tcp := cert.NewCAPool()\n\n\t// c1 should pass because host match\n\tassert.NoError(t, fw.Drop(p, true, &h1, cp, nil))\n\t// c2 should pass because ca sha match\n\tresetConntrack(fw)\n\tassert.NoError(t, fw.Drop(p, true, &h2, cp, nil))\n\t// c3 should fail because no match\n\tresetConntrack(fw)\n\tassert.Equal(t, fw.Drop(p, true, &h3, cp, nil), ErrNoMatchingRule)\n}\n\nfunc TestFirewall_DropConntrackReload(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\tp := firewall.Packet{\n\t\tLocalIP:    netip.MustParseAddr(\"1.2.3.4\"),\n\t\tRemoteIP:   netip.MustParseAddr(\"1.2.3.4\"),\n\t\tLocalPort:  10,\n\t\tRemotePort: 90,\n\t\tProtocol:   firewall.ProtoUDP,\n\t\tFragment:   false,\n\t}\n\tnetwork := netip.MustParsePrefix(\"1.2.3.4/24\")\n\n\tc := cert.CachedCertificate{\n\t\tCertificate: &dummyCert{\n\t\t\tname:     \"host1\",\n\t\t\tnetworks: []netip.Prefix{network},\n\t\t\tgroups:   []string{\"default-group\"},\n\t\t\tissuer:   \"signer-shasum\",\n\t\t},\n\t\tInvertedGroups: map[string]struct{}{\"default-group\": {}},\n\t}\n\th := HostInfo{\n\t\tConnectionState: &ConnectionState{\n\t\t\tpeerCert: &c,\n\t\t},\n\t\tvpnIp: network.Addr(),\n\t}\n\th.CreateRemoteCIDR(c.Certificate)\n\n\tfw := NewFirewall(l, time.Second, time.Minute, time.Hour, c.Certificate)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 0, 0, []string{\"any\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tcp := cert.NewCAPool()\n\n\t// Drop outbound\n\tassert.Equal(t, fw.Drop(p, false, &h, cp, nil), ErrNoMatchingRule)\n\t// Allow inbound\n\tresetConntrack(fw)\n\tassert.NoError(t, fw.Drop(p, true, &h, cp, nil))\n\t// Allow outbound because conntrack\n\tassert.NoError(t, fw.Drop(p, false, &h, cp, nil))\n\n\toldFw := fw\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c.Certificate)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 10, 10, []string{\"any\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tfw.Conntrack = oldFw.Conntrack\n\tfw.rulesVersion = oldFw.rulesVersion + 1\n\n\t// Allow outbound because conntrack and new rules allow port 10\n\tassert.NoError(t, fw.Drop(p, false, &h, cp, nil))\n\n\toldFw = fw\n\tfw = NewFirewall(l, time.Second, time.Minute, time.Hour, c.Certificate)\n\tassert.Nil(t, fw.AddRule(true, firewall.ProtoAny, 11, 11, []string{\"any\"}, \"\", netip.Prefix{}, netip.Prefix{}, \"\", \"\"))\n\tfw.Conntrack = oldFw.Conntrack\n\tfw.rulesVersion = oldFw.rulesVersion + 1\n\n\t// Drop outbound because conntrack doesn't match new ruleset\n\tassert.Equal(t, fw.Drop(p, false, &h, cp, nil), ErrNoMatchingRule)\n}\n\nfunc BenchmarkLookup(b *testing.B) {\n\tml := func(m map[string]struct{}, a [][]string) {\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tfor _, sg := range a {\n\t\t\t\tfound := false\n\n\t\t\t\tfor _, g := range sg {\n\t\t\t\t\tif _, ok := m[g]; !ok {\n\t\t\t\t\t\tfound = false\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\tfound = true\n\t\t\t\t}\n\n\t\t\t\tif found {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tb.Run(\"array to map best\", func(b *testing.B) {\n\t\tm := map[string]struct{}{\n\t\t\t\"1ne\": {},\n\t\t\t\"2wo\": {},\n\t\t\t\"3hr\": {},\n\t\t\t\"4ou\": {},\n\t\t\t\"5iv\": {},\n\t\t\t\"6ix\": {},\n\t\t}\n\n\t\ta := [][]string{\n\t\t\t{\"1ne\", \"2wo\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"2wo\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"fiv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"fiv\", \"six\"},\n\t\t}\n\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tml(m, a)\n\t\t}\n\t})\n\n\tb.Run(\"array to map worst\", func(b *testing.B) {\n\t\tm := map[string]struct{}{\n\t\t\t\"one\": {},\n\t\t\t\"two\": {},\n\t\t\t\"thr\": {},\n\t\t\t\"fou\": {},\n\t\t\t\"fiv\": {},\n\t\t\t\"six\": {},\n\t\t}\n\n\t\ta := [][]string{\n\t\t\t{\"1ne\", \"2wo\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"2wo\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"3hr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"4ou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"5iv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"fiv\", \"6ix\"},\n\t\t\t{\"one\", \"two\", \"thr\", \"fou\", \"fiv\", \"six\"},\n\t\t}\n\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tml(m, a)\n\t\t}\n\t})\n\n\t//TODO: only way array lookup in array will help is if both are sorted, then maybe it's faster\n}\n\nfunc Test_parsePort(t *testing.T) {\n\t_, _, err := parsePort(\"\")\n\tassert.EqualError(t, err, \"was not a number; ``\")\n\n\t_, _, err = parsePort(\"  \")\n\tassert.EqualError(t, err, \"was not a number; `  `\")\n\n\t_, _, err = parsePort(\"-\")\n\tassert.EqualError(t, err, \"appears to be a range but could not be parsed; `-`\")\n\n\t_, _, err = parsePort(\" - \")\n\tassert.EqualError(t, err, \"appears to be a range but could not be parsed; ` - `\")\n\n\t_, _, err = parsePort(\"a-b\")\n\tassert.EqualError(t, err, \"beginning range was not a number; `a`\")\n\n\t_, _, err = parsePort(\"1-b\")\n\tassert.EqualError(t, err, \"ending range was not a number; `b`\")\n\n\ts, e, err := parsePort(\" 1 - 2    \")\n\tassert.Equal(t, int32(1), s)\n\tassert.Equal(t, int32(2), e)\n\tassert.Nil(t, err)\n\n\ts, e, err = parsePort(\"0-1\")\n\tassert.Equal(t, int32(0), s)\n\tassert.Equal(t, int32(0), e)\n\tassert.Nil(t, err)\n\n\ts, e, err = parsePort(\"9919\")\n\tassert.Equal(t, int32(9919), s)\n\tassert.Equal(t, int32(9919), e)\n\tassert.Nil(t, err)\n\n\ts, e, err = parsePort(\"any\")\n\tassert.Equal(t, int32(0), s)\n\tassert.Equal(t, int32(0), e)\n\tassert.Nil(t, err)\n}\n\nfunc TestNewFirewallFromConfig(t *testing.T) {\n\tl := test.NewLogger()\n\t// Test a bad rule definition\n\tc := &dummyCert{}\n\tconf := config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": \"asdf\"}\n\t_, err := NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound failed to parse, should be an array of rules\")\n\n\t// Test both port and code\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"code\": \"2\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; only one of port or code should be provided\")\n\n\t// Test missing host, group, cidr, ca_name and ca_sha\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; at least one of host, group, cidr, local_cidr, ca_name, or ca_sha must be provided\")\n\n\t// Test code/port error\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"code\": \"a\", \"host\": \"testh\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; code was not a number; `a`\")\n\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"port\": \"a\", \"host\": \"testh\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; port was not a number; `a`\")\n\n\t// Test proto error\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"code\": \"1\", \"host\": \"testh\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; proto was not understood; ``\")\n\n\t// Test cidr parse error\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"code\": \"1\", \"cidr\": \"testh\", \"proto\": \"any\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; cidr did not parse; netip.ParsePrefix(\\\"testh\\\"): no '/'\")\n\n\t// Test local_cidr parse error\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"code\": \"1\", \"local_cidr\": \"testh\", \"proto\": \"any\"}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.outbound rule #0; local_cidr did not parse; netip.ParsePrefix(\\\"testh\\\"): no '/'\")\n\n\t// Test both group and groups\n\tconf = config.NewC(l)\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"group\": \"a\", \"groups\": []string{\"b\", \"c\"}}}}\n\t_, err = NewFirewallFromConfig(l, c, conf)\n\tassert.EqualError(t, err, \"firewall.inbound rule #0; only one of group or groups should be defined, both provided\")\n}\n\nfunc TestAddFirewallRulesFromConfig(t *testing.T) {\n\tl := test.NewLogger()\n\t// Test adding tcp rule\n\tconf := config.NewC(l)\n\tmf := &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"tcp\", \"host\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, false, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: false, proto: firewall.ProtoTCP, startPort: 1, endPort: 1, groups: nil, host: \"a\", ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test adding udp rule\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"udp\", \"host\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, false, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: false, proto: firewall.ProtoUDP, startPort: 1, endPort: 1, groups: nil, host: \"a\", ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test adding icmp rule\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"outbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"icmp\", \"host\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, false, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: false, proto: firewall.ProtoICMP, startPort: 1, endPort: 1, groups: nil, host: \"a\", ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test adding any rule\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"host\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: nil, host: \"a\", ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test adding rule with cidr\n\tcidr := netip.MustParsePrefix(\"10.0.0.0/8\")\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"cidr\": cidr.String()}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: nil, ip: cidr, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test adding rule with local_cidr\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"local_cidr\": cidr.String()}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: nil, ip: netip.Prefix{}, localIp: cidr}, mf.lastCall)\n\n\t// Test adding rule with ca_sha\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"ca_sha\": \"12312313123\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: nil, ip: netip.Prefix{}, localIp: netip.Prefix{}, caSha: \"12312313123\"}, mf.lastCall)\n\n\t// Test adding rule with ca_name\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"ca_name\": \"root01\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: nil, ip: netip.Prefix{}, localIp: netip.Prefix{}, caName: \"root01\"}, mf.lastCall)\n\n\t// Test single group\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"group\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: []string{\"a\"}, ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test single groups\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"groups\": \"a\"}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: []string{\"a\"}, ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test multiple AND groups\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"groups\": []string{\"a\", \"b\"}}}}\n\tassert.Nil(t, AddFirewallRulesFromConfig(l, true, conf, mf))\n\tassert.Equal(t, addRuleCall{incoming: true, proto: firewall.ProtoAny, startPort: 1, endPort: 1, groups: []string{\"a\", \"b\"}, ip: netip.Prefix{}, localIp: netip.Prefix{}}, mf.lastCall)\n\n\t// Test Add error\n\tconf = config.NewC(l)\n\tmf = &mockFirewall{}\n\tmf.nextCallReturn = errors.New(\"test error\")\n\tconf.Settings[\"firewall\"] = map[interface{}]interface{}{\"inbound\": []interface{}{map[interface{}]interface{}{\"port\": \"1\", \"proto\": \"any\", \"host\": \"a\"}}}\n\tassert.EqualError(t, AddFirewallRulesFromConfig(l, true, conf, mf), \"firewall.inbound rule #0; `test error`\")\n}\n\nfunc TestFirewall_convertRule(t *testing.T) {\n\tl := test.NewLogger()\n\tob := &bytes.Buffer{}\n\tl.SetOutput(ob)\n\n\t// Ensure group array of 1 is converted and a warning is printed\n\tc := map[interface{}]interface{}{\n\t\t\"group\": []interface{}{\"group1\"},\n\t}\n\n\tr, err := convertRule(l, c, \"test\", 1)\n\tassert.Contains(t, ob.String(), \"test rule #1; group was an array with a single value, converting to simple value\")\n\tassert.Nil(t, err)\n\tassert.Equal(t, \"group1\", r.Group)\n\n\t// Ensure group array of > 1 is errord\n\tob.Reset()\n\tc = map[interface{}]interface{}{\n\t\t\"group\": []interface{}{\"group1\", \"group2\"},\n\t}\n\n\tr, err = convertRule(l, c, \"test\", 1)\n\tassert.Equal(t, \"\", ob.String())\n\tassert.Error(t, err, \"group should contain a single value, an array with more than one entry was provided\")\n\n\t// Make sure a well formed group is alright\n\tob.Reset()\n\tc = map[interface{}]interface{}{\n\t\t\"group\": \"group1\",\n\t}\n\n\tr, err = convertRule(l, c, \"test\", 1)\n\tassert.Nil(t, err)\n\tassert.Equal(t, \"group1\", r.Group)\n}\n\ntype addRuleCall struct {\n\tincoming  bool\n\tproto     uint8\n\tstartPort int32\n\tendPort   int32\n\tgroups    []string\n\thost      string\n\tip        netip.Prefix\n\tlocalIp   netip.Prefix\n\tcaName    string\n\tcaSha     string\n}\n\ntype mockFirewall struct {\n\tlastCall       addRuleCall\n\tnextCallReturn error\n}\n\nfunc (mf *mockFirewall) AddRule(incoming bool, proto uint8, startPort int32, endPort int32, groups []string, host string, ip netip.Prefix, localIp netip.Prefix, caName string, caSha string) error {\n\tmf.lastCall = addRuleCall{\n\t\tincoming:  incoming,\n\t\tproto:     proto,\n\t\tstartPort: startPort,\n\t\tendPort:   endPort,\n\t\tgroups:    groups,\n\t\thost:      host,\n\t\tip:        ip,\n\t\tlocalIp:   localIp,\n\t\tcaName:    caName,\n\t\tcaSha:     caSha,\n\t}\n\n\terr := mf.nextCallReturn\n\tmf.nextCallReturn = nil\n\treturn err\n}\n\nfunc resetConntrack(fw *Firewall) {\n\tfw.Conntrack.Lock()\n\tfw.Conntrack.Conns = map[firewall.Packet]*conn{}\n\tfw.Conntrack.Unlock()\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 2.23828125,
          "content": "module github.com/slackhq/nebula\n\ngo 1.22.0\n\ntoolchain go1.22.2\n\nrequire (\n\tdario.cat/mergo v1.0.1\n\tgithub.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be\n\tgithub.com/armon/go-radix v1.0.0\n\tgithub.com/cyberdelia/go-metrics-graphite v0.0.0-20161219230853-39f87cc3b432\n\tgithub.com/flynn/noise v1.1.0\n\tgithub.com/gaissmai/bart v0.13.0\n\tgithub.com/gogo/protobuf v1.3.2\n\tgithub.com/google/gopacket v1.1.19\n\tgithub.com/kardianos/service v1.2.2\n\tgithub.com/miekg/dns v1.1.62\n\tgithub.com/miekg/pkcs11 v1.1.2-0.20231115102856-9078ad6b9d4b\n\tgithub.com/nbrownus/go-metrics-prometheus v0.0.0-20210712211119-974a6260965f\n\tgithub.com/prometheus/client_golang v1.20.4\n\tgithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475\n\tgithub.com/sirupsen/logrus v1.9.3\n\tgithub.com/skip2/go-qrcode v0.0.0-20200617195104-da1b6568686e\n\tgithub.com/songgao/water v0.0.0-20200317203138-2b4b6d7c09d8\n\tgithub.com/stefanberger/go-pkcs11uri v0.0.0-20230803200340-78284954bff6\n\tgithub.com/stretchr/testify v1.9.0\n\tgithub.com/vishvananda/netlink v1.3.0\n\tgolang.org/x/crypto v0.28.0\n\tgolang.org/x/exp v0.0.0-20230725093048-515e97ebf090\n\tgolang.org/x/net v0.30.0\n\tgolang.org/x/sync v0.8.0\n\tgolang.org/x/sys v0.26.0\n\tgolang.org/x/term v0.25.0\n\tgolang.zx2c4.com/wintun v0.0.0-20230126152724-0fa3db229ce2\n\tgolang.zx2c4.com/wireguard v0.0.0-20230325221338-052af4a8072b\n\tgolang.zx2c4.com/wireguard/windows v0.5.3\n\tgoogle.golang.org/protobuf v1.35.1\n\tgopkg.in/yaml.v2 v2.4.0\n\tgvisor.dev/gvisor v0.0.0-20240423190808-9d7a357edefe\n)\n\nrequire (\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/bits-and-blooms/bitset v1.14.3 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.3.0 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/google/btree v1.1.2 // indirect\n\tgithub.com/klauspost/compress v1.17.9 // indirect\n\tgithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/prometheus/client_model v0.6.1 // indirect\n\tgithub.com/prometheus/common v0.55.0 // indirect\n\tgithub.com/prometheus/procfs v0.15.1 // indirect\n\tgithub.com/vishvananda/netns v0.0.4 // indirect\n\tgolang.org/x/mod v0.18.0 // indirect\n\tgolang.org/x/time v0.5.0 // indirect\n\tgolang.org/x/tools v0.22.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 24.8359375,
          "content": "cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ndario.cat/mergo v1.0.1 h1:Ra4+bf83h2ztPIQYNP99R6m+Y7KfnARDfID+a+vLl4s=\ndario.cat/mergo v1.0.1/go.mod h1:uNxQE+84aUszobStD9th8a29P2fMDhsBdgRYvZOxGmk=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be h1:9AeTilPcZAjCFIImctFaOjnTIavg87rW78vTPkQqLI8=\ngithub.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be/go.mod h1:ySMOLuWl6zY27l47sB3qLNK6tF2fkHG55UZxx8oIVo4=\ngithub.com/armon/go-radix v1.0.0 h1:F4z6KzEeeQIMeLFa97iZU6vupzoecKdU5TX24SNppXI=\ngithub.com/armon/go-radix v1.0.0/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/bits-and-blooms/bitset v1.14.3 h1:Gd2c8lSNf9pKXom5JtD7AaKO8o7fGQ2LtFj1436qilA=\ngithub.com/bits-and-blooms/bitset v1.14.3/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=\ngithub.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cyberdelia/go-metrics-graphite v0.0.0-20161219230853-39f87cc3b432 h1:M5QgkYacWj0Xs8MhpIK/5uwU02icXpEoSo9sM2aRCps=\ngithub.com/cyberdelia/go-metrics-graphite v0.0.0-20161219230853-39f87cc3b432/go.mod h1:xwIwAxMvYnVrGJPe2FKx5prTrnAjGOD8zvDOnxnrrkM=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/flynn/noise v1.1.0 h1:KjPQoQCEFdZDiP03phOvGi11+SVVhBG2wOWAorLsstg=\ngithub.com/flynn/noise v1.1.0/go.mod h1:xbMo+0i6+IGbYdJhF31t2eR1BIU0CYc12+BNAKwUTag=\ngithub.com/gaissmai/bart v0.13.0 h1:pItEhXDVVebUa+i978FfQ7ye8xZc1FrMgs8nJPPWAgA=\ngithub.com/gaissmai/bart v0.13.0/go.mod h1:qSes2fnJ8hB410BW0ymHUN/eQkuGpTYyJcN8sKMYpJU=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/google/btree v1.1.2 h1:xf4v41cLI2Z6FxbKm+8Bu+m8ifhj15JuZ9sa0jZCMUU=\ngithub.com/google/btree v1.1.2/go.mod h1:qOPhT0dTNdNzV6Z/lhRX0YXUafgPLFUh+gZMl761Gm4=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/gopacket v1.1.19 h1:ves8RnFZPGiFnTS0uPQStjwru6uO6h+nlr9j6fL7kF8=\ngithub.com/google/gopacket v1.1.19/go.mod h1:iJ8V8n6KS+z2U1A8pUwu8bW5SyEMkXJB8Yo/Vo+TKTo=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/kardianos/service v1.2.2 h1:ZvePhAHfvo0A7Mftk/tEzqEZ7Q4lgnR8sGz4xu1YX60=\ngithub.com/kardianos/service v1.2.2/go.mod h1:CIMRFEJVL+0DS1a3Nx06NaMn4Dz63Ng6O7dl0qH0zVM=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=\ngithub.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\ngithub.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/kylelemons/godebug v1.1.0 h1:RPNrshWIDI6G2gRW9EHilWtl7Z6Sb1BR0xunSBf0SNc=\ngithub.com/kylelemons/godebug v1.1.0/go.mod h1:9/0rRGxNHcop5bhtWyNeEfOS8JIWk580+fNqagV/RAw=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/miekg/dns v1.1.62 h1:cN8OuEF1/x5Rq6Np+h1epln8OiyPWV+lROx9LxcGgIQ=\ngithub.com/miekg/dns v1.1.62/go.mod h1:mvDlcItzm+br7MToIKqkglaGhlFMHJ9DTNNWONWXbNQ=\ngithub.com/miekg/pkcs11 v1.1.2-0.20231115102856-9078ad6b9d4b h1:J/AzCvg5z0Hn1rqZUJjpbzALUmkKX0Zwbc/i4fw7Sfk=\ngithub.com/miekg/pkcs11 v1.1.2-0.20231115102856-9078ad6b9d4b/go.mod h1:XsNlhZGX73bx86s2hdc/FuaLm2CPZJemRLMA+WTFxgs=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=\ngithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/nbrownus/go-metrics-prometheus v0.0.0-20210712211119-974a6260965f h1:8dM0ilqKL0Uzl42GABzzC4Oqlc3kGRILz0vgoff7nwg=\ngithub.com/nbrownus/go-metrics-prometheus v0.0.0-20210712211119-974a6260965f/go.mod h1:nwPd6pDNId/Xi16qtKrFHrauSwMNuvk+zcjk89wrnlA=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.0/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_golang v1.20.4 h1:Tgh3Yr67PaOv/uTqloMsCEdeuFTatm5zIq5+qNN23vI=\ngithub.com/prometheus/client_golang v1.20.4/go.mod h1:PIEt8X02hGcP8JWbeHyeZ53Y/jReSnHgO035n//V5WE=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.6.1 h1:ZKSh/rekM+n3CeS952MLRAdFwIKqeY8b62p8ais2e9E=\ngithub.com/prometheus/client_model v0.6.1/go.mod h1:OrxVMOVHjw3lKMa8+x6HeMGkHMQyHDk9E3jmP2AmGiY=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/common v0.55.0 h1:KEi6DK7lXW/m7Ig5i47x0vRzuBsHuvJdi5ee6Y3G1dc=\ngithub.com/prometheus/common v0.55.0/go.mod h1:2SECS4xJG1kd8XF9IcM1gMX6510RAEL65zxzNImwdc8=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.15.1 h1:YagwOFzUgYfKKHX6Dr+sHT7km/hxC76UB0learggepc=\ngithub.com/prometheus/procfs v0.15.1/go.mod h1:fB45yRUv8NstnjriLhBQLuOUt+WW4BsoGhij/e3PBqk=\ngithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475 h1:N/ElC8H3+5XpJzTSTfLsJV/mx9Q9g7kxmchpfZyxgzM=\ngithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y+/ZMZ9VjZe4=\ngithub.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=\ngithub.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=\ngithub.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\ngithub.com/skip2/go-qrcode v0.0.0-20200617195104-da1b6568686e h1:MRM5ITcdelLK2j1vwZ3Je0FKVCfqOLp5zO6trqMLYs0=\ngithub.com/skip2/go-qrcode v0.0.0-20200617195104-da1b6568686e/go.mod h1:XV66xRDqSt+GTGFMVlhk3ULuV0y9ZmzeVGR4mloJI3M=\ngithub.com/songgao/water v0.0.0-20200317203138-2b4b6d7c09d8 h1:TG/diQgUe0pntT/2D9tmUCz4VNwm9MfrtPr0SU2qSX8=\ngithub.com/songgao/water v0.0.0-20200317203138-2b4b6d7c09d8/go.mod h1:P5HUIBuIWKbyjl083/loAegFkfbFNx5i2qEP4CNbm7E=\ngithub.com/stefanberger/go-pkcs11uri v0.0.0-20230803200340-78284954bff6 h1:pnnLyeX7o/5aX8qUQ69P/mLojDqwda8hFOCBTmP/6hw=\ngithub.com/stefanberger/go-pkcs11uri v0.0.0-20230803200340-78284954bff6/go.mod h1:39R/xuhNgVhi+K0/zst4TLrJrVmbm6LVgl4A0+ZFS5M=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=\ngithub.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/vishvananda/netlink v1.3.0 h1:X7l42GfcV4S6E4vHTsw48qbrV+9PVojNfIhZcwQdrZk=\ngithub.com/vishvananda/netlink v1.3.0/go.mod h1:i6NetklAujEcC6fK0JPjT8qSwWyO0HLn4UKG+hGqeJs=\ngithub.com/vishvananda/netns v0.0.4 h1:Oeaw1EM2JMxD51g9uhtC0D7erkIjgmj8+JZc26m1YX8=\ngithub.com/vishvananda/netns v0.0.4/go.mod h1:SpkAiCQRtJ6TvvxPnOSyH3BMl6unz3xZlaprSwhNNJM=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20210322153248-0c34fe9e7dc2/go.mod h1:T9bdIzuCu7OtxOm1hfPfRQxPLYneinmdGuTeoZ9dtd4=\ngolang.org/x/crypto v0.28.0 h1:GBDwsMXVQi34v5CCYUm2jkJvu4cbtru2U4TN2PSyQnw=\ngolang.org/x/crypto v0.28.0/go.mod h1:rmgy+3RHxRZMyY0jjAJShp2zgEdOqj2AO7U0pYmeQ7U=\ngolang.org/x/exp v0.0.0-20230725093048-515e97ebf090 h1:Di6/M8l0O2lCLc6VVRWhgCiApHV8MnQurBnFSHsQtNY=\ngolang.org/x/exp v0.0.0-20230725093048-515e97ebf090/go.mod h1:FXUEEKJgO7OQYeo8N01OfiKP8RXMtf6e8aTskBGqWdc=\ngolang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.18.0 h1:5+9lSbEzPSdWkH32vYPBwEpX8KwDbM52Ud9xBUvNlb0=\ngolang.org/x/mod v0.18.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.30.0 h1:AcW1SDZMkb8IpzCdQUaIq2sP4sZ4zw+55h6ynffypl4=\ngolang.org/x/net v0.30.0/go.mod h1:2wGyMJ5iFasEhkwi13ChkO/t1ECNC4X4eBKkVFyYFlU=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.8.0 h1:3NFvSEYkUoMifnESzZl15y791HH1qU2xm6eCJU5ZPXQ=\ngolang.org/x/sync v0.8.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201015000850-e3ed0017c211/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.2.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.10.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.26.0 h1:KHjCJyddX0LoSTb3J+vWpupP9p0oznkqVk/IfjymZbo=\ngolang.org/x/sys v0.26.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.25.0 h1:WtHI/ltw4NvSUig5KARz9h521QvRC8RmF/cuYqifU24=\ngolang.org/x/term v0.25.0/go.mod h1:RPyXicDX+6vLxogjjRxjgD2TKtmAO6NZBsBRfrOLu7M=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=\ngolang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.22.0 h1:gqSGLZqv+AI9lIQzniJ0nZDRG5GBPsSi+DRNHWNz6yA=\ngolang.org/x/tools v0.22.0/go.mod h1:aCwcsjqvq7Yqt6TNyX7QMU2enbQ/Gt0bo6krSeEri+c=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.zx2c4.com/wintun v0.0.0-20230126152724-0fa3db229ce2 h1:B82qJJgjvYKsXS9jeunTOisW56dUokqW/FOteYJJ/yg=\ngolang.zx2c4.com/wintun v0.0.0-20230126152724-0fa3db229ce2/go.mod h1:deeaetjYA+DHMHg+sMSMI58GrEteJUUzzw7en6TJQcI=\ngolang.zx2c4.com/wireguard v0.0.0-20230325221338-052af4a8072b h1:J1CaxgLerRR5lgx3wnr6L04cJFbWoceSK9JWBdglINo=\ngolang.zx2c4.com/wireguard v0.0.0-20230325221338-052af4a8072b/go.mod h1:tqur9LnfstdR9ep2LaJT4lFUl0EjlHtge+gAjmsHUG4=\ngolang.zx2c4.com/wireguard/windows v0.5.3 h1:On6j2Rpn3OEMXqBq00QEDC7bWSZrPIHKIus8eIuExIE=\ngolang.zx2c4.com/wireguard/windows v0.5.3/go.mod h1:9TEe8TJmtwyQebdFwAkEWOPr3prrtqm+REGFifP60hI=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.35.1 h1:m3LfL6/Ca+fqnjnlqQXNpFPABW1UD7mjh8KO2mKFytA=\ngoogle.golang.org/protobuf v1.35.1/go.mod h1:9fA7Ob0pmnwhb644+1+CVWFRbNajQ6iRojtC/QF5bRE=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngvisor.dev/gvisor v0.0.0-20240423190808-9d7a357edefe h1:fre4i6mv4iBuz5lCMOzHD1rH1ljqHWSICFmZRbbgp3g=\ngvisor.dev/gvisor v0.0.0-20240423190808-9d7a357edefe/go.mod h1:sxc3Uvk/vHcd3tj7/DHVBoR5wvWT/MmRq2pj7HRJnwU=\n"
        },
        {
          "name": "handshake_ix.go",
          "type": "blob",
          "size": 18.9599609375,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"time\"\n\n\t\"github.com/flynn/noise\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/header\"\n)\n\n// NOISE IX Handshakes\n\n// This function constructs a handshake packet, but does not actually send it\n// Sending is done by the handshake manager\nfunc ixHandshakeStage0(f *Interface, hh *HandshakeHostInfo) bool {\n\terr := f.handshakeManager.allocateIndex(hh)\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hh.hostinfo.vpnIp).\n\t\t\tWithField(\"handshake\", m{\"stage\": 0, \"style\": \"ix_psk0\"}).Error(\"Failed to generate index\")\n\t\treturn false\n\t}\n\n\tcertState := f.pki.GetCertState()\n\tci := NewConnectionState(f.l, f.cipher, certState, true, noise.HandshakeIX, []byte{}, 0)\n\thh.hostinfo.ConnectionState = ci\n\n\thsProto := &NebulaHandshakeDetails{\n\t\tInitiatorIndex: hh.hostinfo.localIndexId,\n\t\tTime:           uint64(time.Now().UnixNano()),\n\t\tCert:           certState.RawCertificateNoKey,\n\t}\n\n\thsBytes := []byte{}\n\n\ths := &NebulaHandshake{\n\t\tDetails: hsProto,\n\t}\n\thsBytes, err = hs.Marshal()\n\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hh.hostinfo.vpnIp).\n\t\t\tWithField(\"handshake\", m{\"stage\": 0, \"style\": \"ix_psk0\"}).Error(\"Failed to marshal handshake message\")\n\t\treturn false\n\t}\n\n\th := header.Encode(make([]byte, header.Len), header.Version, header.Handshake, header.HandshakeIXPSK0, 0, 1)\n\n\tmsg, _, _, err := ci.H.WriteMessage(h, hsBytes)\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hh.hostinfo.vpnIp).\n\t\t\tWithField(\"handshake\", m{\"stage\": 0, \"style\": \"ix_psk0\"}).Error(\"Failed to call noise.WriteMessage\")\n\t\treturn false\n\t}\n\n\t// We are sending handshake packet 1, so we don't expect to receive\n\t// handshake packet 1 from the responder\n\tci.window.Update(f.l, 1)\n\n\thh.hostinfo.HandshakePacket[0] = msg\n\thh.ready = true\n\treturn true\n}\n\nfunc ixHandshakeStage1(f *Interface, addr netip.AddrPort, via *ViaSender, packet []byte, h *header.H) {\n\tcertState := f.pki.GetCertState()\n\tci := NewConnectionState(f.l, f.cipher, certState, false, noise.HandshakeIX, []byte{}, 0)\n\t// Mark packet 1 as seen so it doesn't show up as missed\n\tci.window.Update(f.l, 1)\n\n\tmsg, _, _, err := ci.H.ReadMessage(nil, packet[header.Len:])\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Failed to call noise.ReadMessage\")\n\t\treturn\n\t}\n\n\ths := &NebulaHandshake{}\n\terr = hs.Unmarshal(msg)\n\t/*\n\t\tl.Debugln(\"GOT INDEX: \", hs.Details.InitiatorIndex)\n\t*/\n\tif err != nil || hs.Details == nil {\n\t\tf.l.WithError(err).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Failed unmarshal handshake message\")\n\t\treturn\n\t}\n\n\tremoteCert, err := RecombineCertAndValidate(ci.H, hs.Details.Cert, f.pki.GetCAPool())\n\tif err != nil {\n\t\te := f.l.WithError(err).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"})\n\n\t\tif f.l.Level > logrus.DebugLevel {\n\t\t\te = e.WithField(\"cert\", remoteCert)\n\t\t}\n\n\t\te.Info(\"Invalid certificate from host\")\n\t\treturn\n\t}\n\n\tif len(remoteCert.Certificate.Networks()) == 0 {\n\t\te := f.l.WithError(err).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"})\n\n\t\tif f.l.Level > logrus.DebugLevel {\n\t\t\te = e.WithField(\"cert\", remoteCert)\n\t\t}\n\n\t\te.Info(\"Invalid vpn ip from host\")\n\t\treturn\n\t}\n\n\tvpnIp := remoteCert.Certificate.Networks()[0].Addr().Unmap()\n\tcertName := remoteCert.Certificate.Name()\n\tfingerprint := remoteCert.Fingerprint\n\tissuer := remoteCert.Certificate.Issuer()\n\n\tif vpnIp == f.myVpnNet.Addr() {\n\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Refusing to handshake with myself\")\n\t\treturn\n\t}\n\n\tif addr.IsValid() {\n\t\tif !f.lightHouse.GetRemoteAllowList().Allow(vpnIp, addr.Addr()) {\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).Debug(\"lighthouse.remote_allow_list denied incoming handshake\")\n\t\t\treturn\n\t\t}\n\t}\n\n\tmyIndex, err := generateIndex(f.l)\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Failed to generate index\")\n\t\treturn\n\t}\n\n\thostinfo := &HostInfo{\n\t\tConnectionState:   ci,\n\t\tlocalIndexId:      myIndex,\n\t\tremoteIndexId:     hs.Details.InitiatorIndex,\n\t\tvpnIp:             vpnIp,\n\t\tHandshakePacket:   make(map[uint8][]byte, 0),\n\t\tlastHandshakeTime: hs.Details.Time,\n\t\trelayState: RelayState{\n\t\t\trelays:        map[netip.Addr]struct{}{},\n\t\t\trelayForByIp:  map[netip.Addr]*Relay{},\n\t\t\trelayForByIdx: map[uint32]*Relay{},\n\t\t},\n\t}\n\n\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\tWithField(\"certName\", certName).\n\t\tWithField(\"fingerprint\", fingerprint).\n\t\tWithField(\"issuer\", issuer).\n\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\tInfo(\"Handshake message received\")\n\n\ths.Details.ResponderIndex = myIndex\n\ths.Details.Cert = certState.RawCertificateNoKey\n\t// Update the time in case their clock is way off from ours\n\ths.Details.Time = uint64(time.Now().UnixNano())\n\n\thsBytes, err := hs.Marshal()\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Failed to marshal handshake message\")\n\t\treturn\n\t}\n\n\tnh := header.Encode(make([]byte, header.Len), header.Version, header.Handshake, header.HandshakeIXPSK0, hs.Details.InitiatorIndex, 2)\n\tmsg, dKey, eKey, err := ci.H.WriteMessage(nh, hsBytes)\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Failed to call noise.WriteMessage\")\n\t\treturn\n\t} else if dKey == nil || eKey == nil {\n\t\tf.l.WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).Error(\"Noise did not arrive at a key\")\n\t\treturn\n\t}\n\n\thostinfo.HandshakePacket[0] = make([]byte, len(packet[header.Len:]))\n\tcopy(hostinfo.HandshakePacket[0], packet[header.Len:])\n\n\t// Regardless of whether you are the sender or receiver, you should arrive here\n\t// and complete standing up the connection.\n\thostinfo.HandshakePacket[2] = make([]byte, len(msg))\n\tcopy(hostinfo.HandshakePacket[2], msg)\n\n\t// We are sending handshake packet 2, so we don't expect to receive\n\t// handshake packet 2 from the initiator.\n\tci.window.Update(f.l, 2)\n\n\tci.peerCert = remoteCert\n\tci.dKey = NewNebulaCipherState(dKey)\n\tci.eKey = NewNebulaCipherState(eKey)\n\n\thostinfo.remotes = f.lightHouse.QueryCache(vpnIp)\n\thostinfo.SetRemote(addr)\n\thostinfo.CreateRemoteCIDR(remoteCert.Certificate)\n\n\texisting, err := f.handshakeManager.CheckAndComplete(hostinfo, 0, f)\n\tif err != nil {\n\t\tswitch err {\n\t\tcase ErrAlreadySeen:\n\t\t\t// Update remote if preferred\n\t\t\tif existing.SetRemoteIfPreferred(f.hostMap, addr) {\n\t\t\t\t// Send a test packet to ensure the other side has also switched to\n\t\t\t\t// the preferred remote\n\t\t\t\tf.SendMessageToVpnIp(header.Test, header.TestRequest, vpnIp, []byte(\"\"), make([]byte, 12, 12), make([]byte, mtu))\n\t\t\t}\n\n\t\t\tmsg = existing.HandshakePacket[2]\n\t\t\tf.messageMetrics.Tx(header.Handshake, header.MessageSubType(msg[1]), 1)\n\t\t\tif addr.IsValid() {\n\t\t\t\terr := f.outside.WriteTo(msg, addr)\n\t\t\t\tif err != nil {\n\t\t\t\t\tf.l.WithField(\"vpnIp\", existing.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).WithField(\"cached\", true).\n\t\t\t\t\t\tWithError(err).Error(\"Failed to send handshake message\")\n\t\t\t\t} else {\n\t\t\t\t\tf.l.WithField(\"vpnIp\", existing.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).WithField(\"cached\", true).\n\t\t\t\t\t\tInfo(\"Handshake message sent\")\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t} else {\n\t\t\t\tif via == nil {\n\t\t\t\t\tf.l.Error(\"Handshake send failed: both addr and via are nil.\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\thostinfo.relayState.InsertRelayTo(via.relayHI.vpnIp)\n\t\t\t\tf.SendVia(via.relayHI, via.relay, msg, make([]byte, 12), make([]byte, mtu), false)\n\t\t\t\tf.l.WithField(\"vpnIp\", existing.vpnIp).WithField(\"relay\", via.relayHI.vpnIp).\n\t\t\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).WithField(\"cached\", true).\n\t\t\t\t\tInfo(\"Handshake message sent\")\n\t\t\t\treturn\n\t\t\t}\n\t\tcase ErrExistingHostInfo:\n\t\t\t// This means there was an existing tunnel and this handshake was older than the one we are currently based on\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"certName\", certName).\n\t\t\t\tWithField(\"oldHandshakeTime\", existing.lastHandshakeTime).\n\t\t\t\tWithField(\"newHandshakeTime\", hostinfo.lastHandshakeTime).\n\t\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\t\tWithField(\"issuer\", issuer).\n\t\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\t\tInfo(\"Handshake too old\")\n\n\t\t\t// Send a test packet to trigger an authenticated tunnel test, this should suss out any lingering tunnel issues\n\t\t\tf.SendMessageToVpnIp(header.Test, header.TestRequest, vpnIp, []byte(\"\"), make([]byte, 12, 12), make([]byte, mtu))\n\t\t\treturn\n\t\tcase ErrLocalIndexCollision:\n\t\t\t// This means we failed to insert because of collision on localIndexId. Just let the next handshake packet retry\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"certName\", certName).\n\t\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\t\tWithField(\"issuer\", issuer).\n\t\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\t\tWithField(\"localIndex\", hostinfo.localIndexId).WithField(\"collision\", existing.vpnIp).\n\t\t\t\tError(\"Failed to add HostInfo due to localIndex collision\")\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Shouldn't happen, but just in case someone adds a new error type to CheckAndComplete\n\t\t\t// And we forget to update it here\n\t\t\tf.l.WithError(err).WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"certName\", certName).\n\t\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\t\tWithField(\"issuer\", issuer).\n\t\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\t\tError(\"Failed to add HostInfo to HostMap\")\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Do the send\n\tf.messageMetrics.Tx(header.Handshake, header.MessageSubType(msg[1]), 1)\n\tif addr.IsValid() {\n\t\terr = f.outside.WriteTo(msg, addr)\n\t\tif err != nil {\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"certName\", certName).\n\t\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\t\tWithField(\"issuer\", issuer).\n\t\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\t\t\tWithError(err).Error(\"Failed to send handshake\")\n\t\t} else {\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"certName\", certName).\n\t\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\t\tWithField(\"issuer\", issuer).\n\t\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\t\t\tInfo(\"Handshake message sent\")\n\t\t}\n\t} else {\n\t\tif via == nil {\n\t\t\tf.l.Error(\"Handshake send failed: both addr and via are nil.\")\n\t\t\treturn\n\t\t}\n\t\thostinfo.relayState.InsertRelayTo(via.relayHI.vpnIp)\n\t\tf.SendVia(via.relayHI, via.relay, msg, make([]byte, 12), make([]byte, mtu), false)\n\t\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"relay\", via.relayHI.vpnIp).\n\t\t\tWithField(\"certName\", certName).\n\t\t\tWithField(\"fingerprint\", fingerprint).\n\t\t\tWithField(\"issuer\", issuer).\n\t\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\t\tInfo(\"Handshake message sent\")\n\t}\n\n\tf.connectionManager.AddTrafficWatch(hostinfo.localIndexId)\n\n\thostinfo.remotes.ResetBlockedRemotes()\n\n\treturn\n}\n\nfunc ixHandshakeStage2(f *Interface, addr netip.AddrPort, via *ViaSender, hh *HandshakeHostInfo, packet []byte, h *header.H) bool {\n\tif hh == nil {\n\t\t// Nothing here to tear down, got a bogus stage 2 packet\n\t\treturn true\n\t}\n\n\thh.Lock()\n\tdefer hh.Unlock()\n\n\thostinfo := hh.hostinfo\n\tif addr.IsValid() {\n\t\tif !f.lightHouse.GetRemoteAllowList().Allow(hostinfo.vpnIp, addr.Addr()) {\n\t\t\tf.l.WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).Debug(\"lighthouse.remote_allow_list denied incoming handshake\")\n\t\t\treturn false\n\t\t}\n\t}\n\n\tci := hostinfo.ConnectionState\n\tmsg, eKey, dKey, err := ci.H.ReadMessage(nil, packet[header.Len:])\n\tif err != nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).WithField(\"header\", h).\n\t\t\tError(\"Failed to call noise.ReadMessage\")\n\n\t\t// We don't want to tear down the connection on a bad ReadMessage because it could be an attacker trying\n\t\t// to DOS us. Every other error condition after should to allow a possible good handshake to complete in the\n\t\t// near future\n\t\treturn false\n\t} else if dKey == nil || eKey == nil {\n\t\tf.l.WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\t\tError(\"Noise did not arrive at a key\")\n\n\t\t// This should be impossible in IX but just in case, if we get here then there is no chance to recover\n\t\t// the handshake state machine. Tear it down\n\t\treturn true\n\t}\n\n\ths := &NebulaHandshake{}\n\terr = hs.Unmarshal(msg)\n\tif err != nil || hs.Details == nil {\n\t\tf.l.WithError(err).WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).Error(\"Failed unmarshal handshake message\")\n\n\t\t// The handshake state machine is complete, if things break now there is no chance to recover. Tear down and start again\n\t\treturn true\n\t}\n\n\tremoteCert, err := RecombineCertAndValidate(ci.H, hs.Details.Cert, f.pki.GetCAPool())\n\tif err != nil {\n\t\te := f.l.WithError(err).WithField(\"vpnIp\", hostinfo.vpnIp).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"})\n\n\t\tif f.l.Level > logrus.DebugLevel {\n\t\t\te = e.WithField(\"cert\", remoteCert)\n\t\t}\n\n\t\te.Error(\"Invalid certificate from host\")\n\n\t\t// The handshake state machine is complete, if things break now there is no chance to recover. Tear down and start again\n\t\treturn true\n\t}\n\n\tif len(remoteCert.Certificate.Networks()) == 0 {\n\t\te := f.l.WithError(err).WithField(\"udpAddr\", addr).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"})\n\n\t\tif f.l.Level > logrus.DebugLevel {\n\t\t\te = e.WithField(\"cert\", remoteCert)\n\t\t}\n\n\t\te.Info(\"Invalid vpn ip from host\")\n\t\treturn true\n\t}\n\n\tvpnIp := remoteCert.Certificate.Networks()[0].Addr().Unmap()\n\tcertName := remoteCert.Certificate.Name()\n\tfingerprint := remoteCert.Fingerprint\n\tissuer := remoteCert.Certificate.Issuer()\n\n\thostinfo.remoteIndexId = hs.Details.ResponderIndex\n\thostinfo.lastHandshakeTime = hs.Details.Time\n\n\t// Store their cert and our symmetric keys\n\tci.peerCert = remoteCert\n\tci.dKey = NewNebulaCipherState(dKey)\n\tci.eKey = NewNebulaCipherState(eKey)\n\n\t// Make sure the current udpAddr being used is set for responding\n\tif addr.IsValid() {\n\t\thostinfo.SetRemote(addr)\n\t} else {\n\t\thostinfo.relayState.InsertRelayTo(via.relayHI.vpnIp)\n\t}\n\n\t// Ensure the right host responded\n\tif vpnIp != hostinfo.vpnIp {\n\t\tf.l.WithField(\"intendedVpnIp\", hostinfo.vpnIp).WithField(\"haveVpnIp\", vpnIp).\n\t\t\tWithField(\"udpAddr\", addr).WithField(\"certName\", certName).\n\t\t\tWithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\t\tInfo(\"Incorrect host responded to handshake\")\n\n\t\t// Release our old handshake from pending, it should not continue\n\t\tf.handshakeManager.DeleteHostInfo(hostinfo)\n\n\t\t// Create a new hostinfo/handshake for the intended vpn ip\n\t\tf.handshakeManager.StartHandshake(hostinfo.vpnIp, func(newHH *HandshakeHostInfo) {\n\t\t\t//TODO: this doesnt know if its being added or is being used for caching a packet\n\t\t\t// Block the current used address\n\t\t\tnewHH.hostinfo.remotes = hostinfo.remotes\n\t\t\tnewHH.hostinfo.remotes.BlockRemote(addr)\n\n\t\t\tf.l.WithField(\"blockedUdpAddrs\", newHH.hostinfo.remotes.CopyBlockedRemotes()).\n\t\t\t\tWithField(\"vpnIp\", newHH.hostinfo.vpnIp).\n\t\t\t\tWithField(\"remotes\", newHH.hostinfo.remotes.CopyAddrs(f.hostMap.GetPreferredRanges())).\n\t\t\t\tInfo(\"Blocked addresses for handshakes\")\n\n\t\t\t// Swap the packet store to benefit the original intended recipient\n\t\t\tnewHH.packetStore = hh.packetStore\n\t\t\thh.packetStore = []*cachedPacket{}\n\n\t\t\t// Get the correct remote list for the host we did handshake with\n\t\t\thostinfo.SetRemote(addr)\n\t\t\thostinfo.remotes = f.lightHouse.QueryCache(vpnIp)\n\t\t\t// Finally, put the correct vpn ip in the host info, tell them to close the tunnel, and return true to tear down\n\t\t\thostinfo.vpnIp = vpnIp\n\t\t\tf.sendCloseTunnel(hostinfo)\n\t\t})\n\n\t\treturn true\n\t}\n\n\t// Mark packet 2 as seen so it doesn't show up as missed\n\tci.window.Update(f.l, 2)\n\n\tduration := time.Since(hh.startTime).Nanoseconds()\n\tf.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", addr).\n\t\tWithField(\"certName\", certName).\n\t\tWithField(\"fingerprint\", fingerprint).\n\t\tWithField(\"issuer\", issuer).\n\t\tWithField(\"initiatorIndex\", hs.Details.InitiatorIndex).WithField(\"responderIndex\", hs.Details.ResponderIndex).\n\t\tWithField(\"remoteIndex\", h.RemoteIndex).WithField(\"handshake\", m{\"stage\": 2, \"style\": \"ix_psk0\"}).\n\t\tWithField(\"durationNs\", duration).\n\t\tWithField(\"sentCachedPackets\", len(hh.packetStore)).\n\t\tInfo(\"Handshake message received\")\n\n\t// Build up the radix for the firewall if we have subnets in the cert\n\thostinfo.CreateRemoteCIDR(remoteCert.Certificate)\n\n\t// Complete our handshake and update metrics, this will replace any existing tunnels for this vpnIp\n\tf.handshakeManager.Complete(hostinfo, f)\n\tf.connectionManager.AddTrafficWatch(hostinfo.localIndexId)\n\n\tif f.l.Level >= logrus.DebugLevel {\n\t\thostinfo.logger(f.l).Debugf(\"Sending %d stored packets\", len(hh.packetStore))\n\t}\n\n\tif len(hh.packetStore) > 0 {\n\t\tnb := make([]byte, 12, 12)\n\t\tout := make([]byte, mtu)\n\t\tfor _, cp := range hh.packetStore {\n\t\t\tcp.callback(cp.messageType, cp.messageSubType, hostinfo, cp.packet, nb, out)\n\t\t}\n\t\tf.cachedPacketMetrics.sent.Inc(int64(len(hh.packetStore)))\n\t}\n\n\thostinfo.remotes.ResetBlockedRemotes()\n\tf.metricHandshakes.Update(duration)\n\n\treturn false\n}\n"
        },
        {
          "name": "handshake_manager.go",
          "type": "blob",
          "size": 21.259765625,
          "content": "package nebula\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/rand\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"net/netip\"\n\t\"slices\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/udp\"\n)\n\nconst (\n\tDefaultHandshakeTryInterval   = time.Millisecond * 100\n\tDefaultHandshakeRetries       = 10\n\tDefaultHandshakeTriggerBuffer = 64\n\tDefaultUseRelays              = true\n)\n\nvar (\n\tdefaultHandshakeConfig = HandshakeConfig{\n\t\ttryInterval:   DefaultHandshakeTryInterval,\n\t\tretries:       DefaultHandshakeRetries,\n\t\ttriggerBuffer: DefaultHandshakeTriggerBuffer,\n\t\tuseRelays:     DefaultUseRelays,\n\t}\n)\n\ntype HandshakeConfig struct {\n\ttryInterval   time.Duration\n\tretries       int64\n\ttriggerBuffer int\n\tuseRelays     bool\n\n\tmessageMetrics *MessageMetrics\n}\n\ntype HandshakeManager struct {\n\t// Mutex for interacting with the vpnIps and indexes maps\n\tsync.RWMutex\n\n\tvpnIps  map[netip.Addr]*HandshakeHostInfo\n\tindexes map[uint32]*HandshakeHostInfo\n\n\tmainHostMap            *HostMap\n\tlightHouse             *LightHouse\n\toutside                udp.Conn\n\tconfig                 HandshakeConfig\n\tOutboundHandshakeTimer *LockingTimerWheel[netip.Addr]\n\tmessageMetrics         *MessageMetrics\n\tmetricInitiated        metrics.Counter\n\tmetricTimedOut         metrics.Counter\n\tf                      *Interface\n\tl                      *logrus.Logger\n\n\t// can be used to trigger outbound handshake for the given vpnIp\n\ttrigger chan netip.Addr\n}\n\ntype HandshakeHostInfo struct {\n\tsync.Mutex\n\n\tstartTime   time.Time        // Time that we first started trying with this handshake\n\tready       bool             // Is the handshake ready\n\tcounter     int64            // How many attempts have we made so far\n\tlastRemotes []netip.AddrPort // Remotes that we sent to during the previous attempt\n\tpacketStore []*cachedPacket  // A set of packets to be transmitted once the handshake completes\n\n\thostinfo *HostInfo\n}\n\nfunc (hh *HandshakeHostInfo) cachePacket(l *logrus.Logger, t header.MessageType, st header.MessageSubType, packet []byte, f packetCallback, m *cachedPacketMetrics) {\n\tif len(hh.packetStore) < 100 {\n\t\ttempPacket := make([]byte, len(packet))\n\t\tcopy(tempPacket, packet)\n\n\t\thh.packetStore = append(hh.packetStore, &cachedPacket{t, st, f, tempPacket})\n\t\tif l.Level >= logrus.DebugLevel {\n\t\t\thh.hostinfo.logger(l).\n\t\t\t\tWithField(\"length\", len(hh.packetStore)).\n\t\t\t\tWithField(\"stored\", true).\n\t\t\t\tDebugf(\"Packet store\")\n\t\t}\n\n\t} else {\n\t\tm.dropped.Inc(1)\n\n\t\tif l.Level >= logrus.DebugLevel {\n\t\t\thh.hostinfo.logger(l).\n\t\t\t\tWithField(\"length\", len(hh.packetStore)).\n\t\t\t\tWithField(\"stored\", false).\n\t\t\t\tDebugf(\"Packet store\")\n\t\t}\n\t}\n}\n\nfunc NewHandshakeManager(l *logrus.Logger, mainHostMap *HostMap, lightHouse *LightHouse, outside udp.Conn, config HandshakeConfig) *HandshakeManager {\n\treturn &HandshakeManager{\n\t\tvpnIps:                 map[netip.Addr]*HandshakeHostInfo{},\n\t\tindexes:                map[uint32]*HandshakeHostInfo{},\n\t\tmainHostMap:            mainHostMap,\n\t\tlightHouse:             lightHouse,\n\t\toutside:                outside,\n\t\tconfig:                 config,\n\t\ttrigger:                make(chan netip.Addr, config.triggerBuffer),\n\t\tOutboundHandshakeTimer: NewLockingTimerWheel[netip.Addr](config.tryInterval, hsTimeout(config.retries, config.tryInterval)),\n\t\tmessageMetrics:         config.messageMetrics,\n\t\tmetricInitiated:        metrics.GetOrRegisterCounter(\"handshake_manager.initiated\", nil),\n\t\tmetricTimedOut:         metrics.GetOrRegisterCounter(\"handshake_manager.timed_out\", nil),\n\t\tl:                      l,\n\t}\n}\n\nfunc (c *HandshakeManager) Run(ctx context.Context) {\n\tclockSource := time.NewTicker(c.config.tryInterval)\n\tdefer clockSource.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase vpnIP := <-c.trigger:\n\t\t\tc.handleOutbound(vpnIP, true)\n\t\tcase now := <-clockSource.C:\n\t\t\tc.NextOutboundHandshakeTimerTick(now)\n\t\t}\n\t}\n}\n\nfunc (hm *HandshakeManager) HandleIncoming(addr netip.AddrPort, via *ViaSender, packet []byte, h *header.H) {\n\t// First remote allow list check before we know the vpnIp\n\tif addr.IsValid() {\n\t\tif !hm.lightHouse.GetRemoteAllowList().AllowUnknownVpnIp(addr.Addr()) {\n\t\t\thm.l.WithField(\"udpAddr\", addr).Debug(\"lighthouse.remote_allow_list denied incoming handshake\")\n\t\t\treturn\n\t\t}\n\t}\n\n\tswitch h.Subtype {\n\tcase header.HandshakeIXPSK0:\n\t\tswitch h.MessageCounter {\n\t\tcase 1:\n\t\t\tixHandshakeStage1(hm.f, addr, via, packet, h)\n\n\t\tcase 2:\n\t\t\tnewHostinfo := hm.queryIndex(h.RemoteIndex)\n\t\t\ttearDown := ixHandshakeStage2(hm.f, addr, via, newHostinfo, packet, h)\n\t\t\tif tearDown && newHostinfo != nil {\n\t\t\t\thm.DeleteHostInfo(newHostinfo.hostinfo)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (c *HandshakeManager) NextOutboundHandshakeTimerTick(now time.Time) {\n\tc.OutboundHandshakeTimer.Advance(now)\n\tfor {\n\t\tvpnIp, has := c.OutboundHandshakeTimer.Purge()\n\t\tif !has {\n\t\t\tbreak\n\t\t}\n\t\tc.handleOutbound(vpnIp, false)\n\t}\n}\n\nfunc (hm *HandshakeManager) handleOutbound(vpnIp netip.Addr, lighthouseTriggered bool) {\n\thh := hm.queryVpnIp(vpnIp)\n\tif hh == nil {\n\t\treturn\n\t}\n\thh.Lock()\n\tdefer hh.Unlock()\n\n\thostinfo := hh.hostinfo\n\t// If we are out of time, clean up\n\tif hh.counter >= hm.config.retries {\n\t\thh.hostinfo.logger(hm.l).WithField(\"udpAddrs\", hh.hostinfo.remotes.CopyAddrs(hm.mainHostMap.GetPreferredRanges())).\n\t\t\tWithField(\"initiatorIndex\", hh.hostinfo.localIndexId).\n\t\t\tWithField(\"remoteIndex\", hh.hostinfo.remoteIndexId).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\tWithField(\"durationNs\", time.Since(hh.startTime).Nanoseconds()).\n\t\t\tInfo(\"Handshake timed out\")\n\t\thm.metricTimedOut.Inc(1)\n\t\thm.DeleteHostInfo(hostinfo)\n\t\treturn\n\t}\n\n\t// Increment the counter to increase our delay, linear backoff\n\thh.counter++\n\n\t// Check if we have a handshake packet to transmit yet\n\tif !hh.ready {\n\t\tif !ixHandshakeStage0(hm.f, hh) {\n\t\t\thm.OutboundHandshakeTimer.Add(vpnIp, hm.config.tryInterval*time.Duration(hh.counter))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Get a remotes object if we don't already have one.\n\t// This is mainly to protect us as this should never be the case\n\t// NB ^ This comment doesn't jive. It's how the thing gets initialized.\n\t// It's the common path. Should it update every time, in case a future LH query/queries give us more info?\n\tif hostinfo.remotes == nil {\n\t\thostinfo.remotes = hm.lightHouse.QueryCache(vpnIp)\n\t}\n\n\tremotes := hostinfo.remotes.CopyAddrs(hm.mainHostMap.GetPreferredRanges())\n\tremotesHaveChanged := !slices.Equal(remotes, hh.lastRemotes)\n\n\t// We only care about a lighthouse trigger if we have new remotes to send to.\n\t// This is a very specific optimization for a fast lighthouse reply.\n\tif lighthouseTriggered && !remotesHaveChanged {\n\t\t// If we didn't return here a lighthouse could cause us to aggressively send handshakes\n\t\treturn\n\t}\n\n\thh.lastRemotes = remotes\n\n\t// TODO: this will generate a load of queries for hosts with only 1 ip\n\t// (such as ones registered to the lighthouse with only a private IP)\n\t// So we only do it one time after attempting 5 handshakes already.\n\tif len(remotes) <= 1 && hh.counter == 5 {\n\t\t// If we only have 1 remote it is highly likely our query raced with the other host registered within the lighthouse\n\t\t// Our vpnIp here has a tunnel with a lighthouse but has yet to send a host update packet there so we only know about\n\t\t// the learned public ip for them. Query again to short circuit the promotion counter\n\t\thm.lightHouse.QueryServer(vpnIp)\n\t}\n\n\t// Send the handshake to all known ips, stage 2 takes care of assigning the hostinfo.remote based on the first to reply\n\tvar sentTo []netip.AddrPort\n\thostinfo.remotes.ForEach(hm.mainHostMap.GetPreferredRanges(), func(addr netip.AddrPort, _ bool) {\n\t\thm.messageMetrics.Tx(header.Handshake, header.MessageSubType(hostinfo.HandshakePacket[0][1]), 1)\n\t\terr := hm.outside.WriteTo(hostinfo.HandshakePacket[0], addr)\n\t\tif err != nil {\n\t\t\thostinfo.logger(hm.l).WithField(\"udpAddr\", addr).\n\t\t\t\tWithField(\"initiatorIndex\", hostinfo.localIndexId).\n\t\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\t\tWithError(err).Error(\"Failed to send handshake message\")\n\n\t\t} else {\n\t\t\tsentTo = append(sentTo, addr)\n\t\t}\n\t})\n\n\t// Don't be too noisy or confusing if we fail to send a handshake - if we don't get through we'll eventually log a timeout,\n\t// so only log when the list of remotes has changed\n\tif remotesHaveChanged {\n\t\thostinfo.logger(hm.l).WithField(\"udpAddrs\", sentTo).\n\t\t\tWithField(\"initiatorIndex\", hostinfo.localIndexId).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\tInfo(\"Handshake message sent\")\n\t} else if hm.l.IsLevelEnabled(logrus.DebugLevel) {\n\t\thostinfo.logger(hm.l).WithField(\"udpAddrs\", sentTo).\n\t\t\tWithField(\"initiatorIndex\", hostinfo.localIndexId).\n\t\t\tWithField(\"handshake\", m{\"stage\": 1, \"style\": \"ix_psk0\"}).\n\t\t\tDebug(\"Handshake message sent\")\n\t}\n\n\tif hm.config.useRelays && len(hostinfo.remotes.relays) > 0 {\n\t\thostinfo.logger(hm.l).WithField(\"relays\", hostinfo.remotes.relays).Info(\"Attempt to relay through hosts\")\n\t\t// Send a RelayRequest to all known Relay IP's\n\t\tfor _, relay := range hostinfo.remotes.relays {\n\t\t\t// Don't relay to myself, and don't relay through the host I'm trying to connect to\n\t\t\tif relay == vpnIp || relay == hm.lightHouse.myVpnNet.Addr() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trelayHostInfo := hm.mainHostMap.QueryVpnIp(relay)\n\t\t\tif relayHostInfo == nil || !relayHostInfo.remote.IsValid() {\n\t\t\t\thostinfo.logger(hm.l).WithField(\"relay\", relay.String()).Info(\"Establish tunnel to relay target\")\n\t\t\t\thm.f.Handshake(relay)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Check the relay HostInfo to see if we already established a relay through it\n\t\t\tif existingRelay, ok := relayHostInfo.relayState.QueryRelayForByIp(vpnIp); ok {\n\t\t\t\tswitch existingRelay.State {\n\t\t\t\tcase Established:\n\t\t\t\t\thostinfo.logger(hm.l).WithField(\"relay\", relay.String()).Info(\"Send handshake via relay\")\n\t\t\t\t\thm.f.SendVia(relayHostInfo, existingRelay, hostinfo.HandshakePacket[0], make([]byte, 12), make([]byte, mtu), false)\n\t\t\t\tcase Requested:\n\t\t\t\t\thostinfo.logger(hm.l).WithField(\"relay\", relay.String()).Info(\"Re-send CreateRelay request\")\n\n\t\t\t\t\t//TODO: IPV6-WORK\n\t\t\t\t\tmyVpnIpB := hm.f.myVpnNet.Addr().As4()\n\t\t\t\t\ttheirVpnIpB := vpnIp.As4()\n\n\t\t\t\t\t// Re-send the CreateRelay request, in case the previous one was lost.\n\t\t\t\t\tm := NebulaControl{\n\t\t\t\t\t\tType:                NebulaControl_CreateRelayRequest,\n\t\t\t\t\t\tInitiatorRelayIndex: existingRelay.LocalIndex,\n\t\t\t\t\t\tRelayFromIp:         binary.BigEndian.Uint32(myVpnIpB[:]),\n\t\t\t\t\t\tRelayToIp:           binary.BigEndian.Uint32(theirVpnIpB[:]),\n\t\t\t\t\t}\n\t\t\t\t\tmsg, err := m.Marshal()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\thostinfo.logger(hm.l).\n\t\t\t\t\t\t\tWithError(err).\n\t\t\t\t\t\t\tError(\"Failed to marshal Control message to create relay\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// This must send over the hostinfo, not over hm.Hosts[ip]\n\t\t\t\t\t\thm.f.SendMessageToHostInfo(header.Control, 0, relayHostInfo, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\t\t\t\thm.l.WithFields(logrus.Fields{\n\t\t\t\t\t\t\t\"relayFrom\":           hm.f.myVpnNet.Addr(),\n\t\t\t\t\t\t\t\"relayTo\":             vpnIp,\n\t\t\t\t\t\t\t\"initiatorRelayIndex\": existingRelay.LocalIndex,\n\t\t\t\t\t\t\t\"relay\":               relay}).\n\t\t\t\t\t\t\tInfo(\"send CreateRelayRequest\")\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\thostinfo.logger(hm.l).\n\t\t\t\t\t\tWithField(\"vpnIp\", vpnIp).\n\t\t\t\t\t\tWithField(\"state\", existingRelay.State).\n\t\t\t\t\t\tWithField(\"relay\", relayHostInfo.vpnIp).\n\t\t\t\t\t\tErrorf(\"Relay unexpected state\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// No relays exist or requested yet.\n\t\t\t\tif relayHostInfo.remote.IsValid() {\n\t\t\t\t\tidx, err := AddRelay(hm.l, relayHostInfo, hm.mainHostMap, vpnIp, nil, TerminalType, Requested)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\thostinfo.logger(hm.l).WithField(\"relay\", relay.String()).WithError(err).Info(\"Failed to add relay to hostmap\")\n\t\t\t\t\t}\n\n\t\t\t\t\t//TODO: IPV6-WORK\n\t\t\t\t\tmyVpnIpB := hm.f.myVpnNet.Addr().As4()\n\t\t\t\t\ttheirVpnIpB := vpnIp.As4()\n\n\t\t\t\t\tm := NebulaControl{\n\t\t\t\t\t\tType:                NebulaControl_CreateRelayRequest,\n\t\t\t\t\t\tInitiatorRelayIndex: idx,\n\t\t\t\t\t\tRelayFromIp:         binary.BigEndian.Uint32(myVpnIpB[:]),\n\t\t\t\t\t\tRelayToIp:           binary.BigEndian.Uint32(theirVpnIpB[:]),\n\t\t\t\t\t}\n\t\t\t\t\tmsg, err := m.Marshal()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\thostinfo.logger(hm.l).\n\t\t\t\t\t\t\tWithError(err).\n\t\t\t\t\t\t\tError(\"Failed to marshal Control message to create relay\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\thm.f.SendMessageToHostInfo(header.Control, 0, relayHostInfo, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\t\t\t\thm.l.WithFields(logrus.Fields{\n\t\t\t\t\t\t\t\"relayFrom\":           hm.f.myVpnNet.Addr(),\n\t\t\t\t\t\t\t\"relayTo\":             vpnIp,\n\t\t\t\t\t\t\t\"initiatorRelayIndex\": idx,\n\t\t\t\t\t\t\t\"relay\":               relay}).\n\t\t\t\t\t\t\tInfo(\"send CreateRelayRequest\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// If a lighthouse triggered this attempt then we are still in the timer wheel and do not need to re-add\n\tif !lighthouseTriggered {\n\t\thm.OutboundHandshakeTimer.Add(vpnIp, hm.config.tryInterval*time.Duration(hh.counter))\n\t}\n}\n\n// GetOrHandshake will try to find a hostinfo with a fully formed tunnel or start a new handshake if one is not present\n// The 2nd argument will be true if the hostinfo is ready to transmit traffic\nfunc (hm *HandshakeManager) GetOrHandshake(vpnIp netip.Addr, cacheCb func(*HandshakeHostInfo)) (*HostInfo, bool) {\n\thm.mainHostMap.RLock()\n\th, ok := hm.mainHostMap.Hosts[vpnIp]\n\thm.mainHostMap.RUnlock()\n\n\tif ok {\n\t\t// Do not attempt promotion if you are a lighthouse\n\t\tif !hm.lightHouse.amLighthouse {\n\t\t\th.TryPromoteBest(hm.mainHostMap.GetPreferredRanges(), hm.f)\n\t\t}\n\t\treturn h, true\n\t}\n\n\treturn hm.StartHandshake(vpnIp, cacheCb), false\n}\n\n// StartHandshake will ensure a handshake is currently being attempted for the provided vpn ip\nfunc (hm *HandshakeManager) StartHandshake(vpnIp netip.Addr, cacheCb func(*HandshakeHostInfo)) *HostInfo {\n\thm.Lock()\n\n\tif hh, ok := hm.vpnIps[vpnIp]; ok {\n\t\t// We are already trying to handshake with this vpn ip\n\t\tif cacheCb != nil {\n\t\t\tcacheCb(hh)\n\t\t}\n\t\thm.Unlock()\n\t\treturn hh.hostinfo\n\t}\n\n\thostinfo := &HostInfo{\n\t\tvpnIp:           vpnIp,\n\t\tHandshakePacket: make(map[uint8][]byte, 0),\n\t\trelayState: RelayState{\n\t\t\trelays:        map[netip.Addr]struct{}{},\n\t\t\trelayForByIp:  map[netip.Addr]*Relay{},\n\t\t\trelayForByIdx: map[uint32]*Relay{},\n\t\t},\n\t}\n\n\thh := &HandshakeHostInfo{\n\t\thostinfo:  hostinfo,\n\t\tstartTime: time.Now(),\n\t}\n\thm.vpnIps[vpnIp] = hh\n\thm.metricInitiated.Inc(1)\n\thm.OutboundHandshakeTimer.Add(vpnIp, hm.config.tryInterval)\n\n\tif cacheCb != nil {\n\t\tcacheCb(hh)\n\t}\n\n\t// If this is a static host, we don't need to wait for the HostQueryReply\n\t// We can trigger the handshake right now\n\t_, doTrigger := hm.lightHouse.GetStaticHostList()[vpnIp]\n\tif !doTrigger {\n\t\t// Add any calculated remotes, and trigger early handshake if one found\n\t\tdoTrigger = hm.lightHouse.addCalculatedRemotes(vpnIp)\n\t}\n\n\tif doTrigger {\n\t\tselect {\n\t\tcase hm.trigger <- vpnIp:\n\t\tdefault:\n\t\t}\n\t}\n\n\thm.Unlock()\n\thm.lightHouse.QueryServer(vpnIp)\n\treturn hostinfo\n}\n\nvar (\n\tErrExistingHostInfo    = errors.New(\"existing hostinfo\")\n\tErrAlreadySeen         = errors.New(\"already seen\")\n\tErrLocalIndexCollision = errors.New(\"local index collision\")\n)\n\n// CheckAndComplete checks for any conflicts in the main and pending hostmap\n// before adding hostinfo to main. If err is nil, it was added. Otherwise err will be:\n//\n// ErrAlreadySeen if we already have an entry in the hostmap that has seen the\n// exact same handshake packet\n//\n// ErrExistingHostInfo if we already have an entry in the hostmap for this\n// VpnIp and the new handshake was older than the one we currently have\n//\n// ErrLocalIndexCollision if we already have an entry in the main or pending\n// hostmap for the hostinfo.localIndexId.\nfunc (c *HandshakeManager) CheckAndComplete(hostinfo *HostInfo, handshakePacket uint8, f *Interface) (*HostInfo, error) {\n\tc.mainHostMap.Lock()\n\tdefer c.mainHostMap.Unlock()\n\tc.Lock()\n\tdefer c.Unlock()\n\n\t// Check if we already have a tunnel with this vpn ip\n\texistingHostInfo, found := c.mainHostMap.Hosts[hostinfo.vpnIp]\n\tif found && existingHostInfo != nil {\n\t\ttestHostInfo := existingHostInfo\n\t\tfor testHostInfo != nil {\n\t\t\t// Is it just a delayed handshake packet?\n\t\t\tif bytes.Equal(hostinfo.HandshakePacket[handshakePacket], testHostInfo.HandshakePacket[handshakePacket]) {\n\t\t\t\treturn testHostInfo, ErrAlreadySeen\n\t\t\t}\n\n\t\t\ttestHostInfo = testHostInfo.next\n\t\t}\n\n\t\t// Is this a newer handshake?\n\t\tif existingHostInfo.lastHandshakeTime >= hostinfo.lastHandshakeTime && !existingHostInfo.ConnectionState.initiator {\n\t\t\treturn existingHostInfo, ErrExistingHostInfo\n\t\t}\n\n\t\texistingHostInfo.logger(c.l).Info(\"Taking new handshake\")\n\t}\n\n\texistingIndex, found := c.mainHostMap.Indexes[hostinfo.localIndexId]\n\tif found {\n\t\t// We have a collision, but for a different hostinfo\n\t\treturn existingIndex, ErrLocalIndexCollision\n\t}\n\n\texistingPendingIndex, found := c.indexes[hostinfo.localIndexId]\n\tif found && existingPendingIndex.hostinfo != hostinfo {\n\t\t// We have a collision, but for a different hostinfo\n\t\treturn existingPendingIndex.hostinfo, ErrLocalIndexCollision\n\t}\n\n\texistingRemoteIndex, found := c.mainHostMap.RemoteIndexes[hostinfo.remoteIndexId]\n\tif found && existingRemoteIndex != nil && existingRemoteIndex.vpnIp != hostinfo.vpnIp {\n\t\t// We have a collision, but this can happen since we can't control\n\t\t// the remote ID. Just log about the situation as a note.\n\t\thostinfo.logger(c.l).\n\t\t\tWithField(\"remoteIndex\", hostinfo.remoteIndexId).WithField(\"collision\", existingRemoteIndex.vpnIp).\n\t\t\tInfo(\"New host shadows existing host remoteIndex\")\n\t}\n\n\tc.mainHostMap.unlockedAddHostInfo(hostinfo, f)\n\treturn existingHostInfo, nil\n}\n\n// Complete is a simpler version of CheckAndComplete when we already know we\n// won't have a localIndexId collision because we already have an entry in the\n// pendingHostMap. An existing hostinfo is returned if there was one.\nfunc (hm *HandshakeManager) Complete(hostinfo *HostInfo, f *Interface) {\n\thm.mainHostMap.Lock()\n\tdefer hm.mainHostMap.Unlock()\n\thm.Lock()\n\tdefer hm.Unlock()\n\n\texistingRemoteIndex, found := hm.mainHostMap.RemoteIndexes[hostinfo.remoteIndexId]\n\tif found && existingRemoteIndex != nil {\n\t\t// We have a collision, but this can happen since we can't control\n\t\t// the remote ID. Just log about the situation as a note.\n\t\thostinfo.logger(hm.l).\n\t\t\tWithField(\"remoteIndex\", hostinfo.remoteIndexId).WithField(\"collision\", existingRemoteIndex.vpnIp).\n\t\t\tInfo(\"New host shadows existing host remoteIndex\")\n\t}\n\n\t// We need to remove from the pending hostmap first to avoid undoing work when after to the main hostmap.\n\thm.unlockedDeleteHostInfo(hostinfo)\n\thm.mainHostMap.unlockedAddHostInfo(hostinfo, f)\n}\n\n// allocateIndex generates a unique localIndexId for this HostInfo\n// and adds it to the pendingHostMap. Will error if we are unable to generate\n// a unique localIndexId\nfunc (hm *HandshakeManager) allocateIndex(hh *HandshakeHostInfo) error {\n\thm.mainHostMap.RLock()\n\tdefer hm.mainHostMap.RUnlock()\n\thm.Lock()\n\tdefer hm.Unlock()\n\n\tfor i := 0; i < 32; i++ {\n\t\tindex, err := generateIndex(hm.l)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t_, inPending := hm.indexes[index]\n\t\t_, inMain := hm.mainHostMap.Indexes[index]\n\n\t\tif !inMain && !inPending {\n\t\t\thh.hostinfo.localIndexId = index\n\t\t\thm.indexes[index] = hh\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn errors.New(\"failed to generate unique localIndexId\")\n}\n\nfunc (c *HandshakeManager) DeleteHostInfo(hostinfo *HostInfo) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tc.unlockedDeleteHostInfo(hostinfo)\n}\n\nfunc (c *HandshakeManager) unlockedDeleteHostInfo(hostinfo *HostInfo) {\n\tdelete(c.vpnIps, hostinfo.vpnIp)\n\tif len(c.vpnIps) == 0 {\n\t\tc.vpnIps = map[netip.Addr]*HandshakeHostInfo{}\n\t}\n\n\tdelete(c.indexes, hostinfo.localIndexId)\n\tif len(c.vpnIps) == 0 {\n\t\tc.indexes = map[uint32]*HandshakeHostInfo{}\n\t}\n\n\tif c.l.Level >= logrus.DebugLevel {\n\t\tc.l.WithField(\"hostMap\", m{\"mapTotalSize\": len(c.vpnIps),\n\t\t\t\"vpnIp\": hostinfo.vpnIp, \"indexNumber\": hostinfo.localIndexId, \"remoteIndexNumber\": hostinfo.remoteIndexId}).\n\t\t\tDebug(\"Pending hostmap hostInfo deleted\")\n\t}\n}\n\nfunc (hm *HandshakeManager) QueryVpnIp(vpnIp netip.Addr) *HostInfo {\n\thh := hm.queryVpnIp(vpnIp)\n\tif hh != nil {\n\t\treturn hh.hostinfo\n\t}\n\treturn nil\n\n}\n\nfunc (hm *HandshakeManager) queryVpnIp(vpnIp netip.Addr) *HandshakeHostInfo {\n\thm.RLock()\n\tdefer hm.RUnlock()\n\treturn hm.vpnIps[vpnIp]\n}\n\nfunc (hm *HandshakeManager) QueryIndex(index uint32) *HostInfo {\n\thh := hm.queryIndex(index)\n\tif hh != nil {\n\t\treturn hh.hostinfo\n\t}\n\treturn nil\n}\n\nfunc (hm *HandshakeManager) queryIndex(index uint32) *HandshakeHostInfo {\n\thm.RLock()\n\tdefer hm.RUnlock()\n\treturn hm.indexes[index]\n}\n\nfunc (c *HandshakeManager) GetPreferredRanges() []netip.Prefix {\n\treturn c.mainHostMap.GetPreferredRanges()\n}\n\nfunc (c *HandshakeManager) ForEachVpnIp(f controlEach) {\n\tc.RLock()\n\tdefer c.RUnlock()\n\n\tfor _, v := range c.vpnIps {\n\t\tf(v.hostinfo)\n\t}\n}\n\nfunc (c *HandshakeManager) ForEachIndex(f controlEach) {\n\tc.RLock()\n\tdefer c.RUnlock()\n\n\tfor _, v := range c.indexes {\n\t\tf(v.hostinfo)\n\t}\n}\n\nfunc (c *HandshakeManager) EmitStats() {\n\tc.RLock()\n\thostLen := len(c.vpnIps)\n\tindexLen := len(c.indexes)\n\tc.RUnlock()\n\n\tmetrics.GetOrRegisterGauge(\"hostmap.pending.hosts\", nil).Update(int64(hostLen))\n\tmetrics.GetOrRegisterGauge(\"hostmap.pending.indexes\", nil).Update(int64(indexLen))\n\tc.mainHostMap.EmitStats()\n}\n\n// Utility functions below\n\nfunc generateIndex(l *logrus.Logger) (uint32, error) {\n\tb := make([]byte, 4)\n\n\t// Let zero mean we don't know the ID, so don't generate zero\n\tvar index uint32\n\tfor index == 0 {\n\t\t_, err := rand.Read(b)\n\t\tif err != nil {\n\t\t\tl.Errorln(err)\n\t\t\treturn 0, err\n\t\t}\n\n\t\tindex = binary.BigEndian.Uint32(b)\n\t}\n\n\tif l.Level >= logrus.DebugLevel {\n\t\tl.WithField(\"index\", index).\n\t\t\tDebug(\"Generated index\")\n\t}\n\treturn index, nil\n}\n\nfunc hsTimeout(tries int64, interval time.Duration) time.Duration {\n\treturn time.Duration(tries / 2 * ((2 * int64(interval)) + (tries-1)*int64(interval)))\n}\n"
        },
        {
          "name": "handshake_manager_test.go",
          "type": "blob",
          "size": 2.4267578125,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/slackhq/nebula/udp\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc Test_NewHandshakeManagerVpnIp(t *testing.T) {\n\tl := test.NewLogger()\n\tvpncidr := netip.MustParsePrefix(\"172.1.1.1/24\")\n\tlocalrange := netip.MustParsePrefix(\"10.1.1.1/24\")\n\tip := netip.MustParseAddr(\"172.1.1.2\")\n\n\tpreferredRanges := []netip.Prefix{localrange}\n\tmainHM := newHostMap(l, vpncidr)\n\tmainHM.preferredRanges.Store(&preferredRanges)\n\n\tlh := newTestLighthouse()\n\n\tcs := &CertState{\n\t\tRawCertificate:      []byte{},\n\t\tPrivateKey:          []byte{},\n\t\tCertificate:         &dummyCert{},\n\t\tRawCertificateNoKey: []byte{},\n\t}\n\n\tblah := NewHandshakeManager(l, mainHM, lh, &udp.NoopConn{}, defaultHandshakeConfig)\n\tblah.f = &Interface{handshakeManager: blah, pki: &PKI{}, l: l}\n\tblah.f.pki.cs.Store(cs)\n\n\tnow := time.Now()\n\tblah.NextOutboundHandshakeTimerTick(now)\n\n\ti := blah.StartHandshake(ip, nil)\n\ti2 := blah.StartHandshake(ip, nil)\n\tassert.Same(t, i, i2)\n\n\ti.remotes = NewRemoteList(nil)\n\n\t// Adding something to pending should not affect the main hostmap\n\tassert.Len(t, mainHM.Hosts, 0)\n\n\t// Confirm they are in the pending index list\n\tassert.Contains(t, blah.vpnIps, ip)\n\n\t// Jump ahead `HandshakeRetries` ticks, offset by one to get the sleep logic right\n\tfor i := 1; i <= DefaultHandshakeRetries+1; i++ {\n\t\tnow = now.Add(time.Duration(i) * DefaultHandshakeTryInterval)\n\t\tblah.NextOutboundHandshakeTimerTick(now)\n\t}\n\n\t// Confirm they are still in the pending index list\n\tassert.Contains(t, blah.vpnIps, ip)\n\n\t// Tick 1 more time, a minute will certainly flush it out\n\tblah.NextOutboundHandshakeTimerTick(now.Add(time.Minute))\n\n\t// Confirm they have been removed\n\tassert.NotContains(t, blah.vpnIps, ip)\n}\n\nfunc testCountTimerWheelEntries(tw *LockingTimerWheel[netip.Addr]) (c int) {\n\tfor _, i := range tw.t.wheel {\n\t\tn := i.Head\n\t\tfor n != nil {\n\t\t\tc++\n\t\t\tn = n.Next\n\t\t}\n\t}\n\treturn c\n}\n\ntype mockEncWriter struct {\n}\n\nfunc (mw *mockEncWriter) SendMessageToVpnIp(t header.MessageType, st header.MessageSubType, vpnIp netip.Addr, p, nb, out []byte) {\n\treturn\n}\n\nfunc (mw *mockEncWriter) SendVia(via *HostInfo, relay *Relay, ad, nb, out []byte, nocopy bool) {\n\treturn\n}\n\nfunc (mw *mockEncWriter) SendMessageToHostInfo(t header.MessageType, st header.MessageSubType, hostinfo *HostInfo, p, nb, out []byte) {\n\treturn\n}\n\nfunc (mw *mockEncWriter) Handshake(vpnIP netip.Addr) {}\n"
        },
        {
          "name": "header",
          "type": "tree",
          "content": null
        },
        {
          "name": "hostmap.go",
          "type": "blob",
          "size": 19.4365234375,
          "content": "package nebula\n\nimport (\n\t\"errors\"\n\t\"net\"\n\t\"net/netip\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/header\"\n)\n\n// const ProbeLen = 100\nconst defaultPromoteEvery = 1000       // Count of packets sent before we try moving a tunnel to a preferred underlay ip address\nconst defaultReQueryEvery = 5000       // Count of packets sent before re-querying a hostinfo to the lighthouse\nconst defaultReQueryWait = time.Minute // Minimum amount of seconds to wait before re-querying a hostinfo the lighthouse. Evaluated every ReQueryEvery\nconst MaxRemotes = 10\nconst maxRecvError = 4\n\n// MaxHostInfosPerVpnIp is the max number of hostinfos we will track for a given vpn ip\n// 5 allows for an initial handshake and each host pair re-handshaking twice\nconst MaxHostInfosPerVpnIp = 5\n\n// How long we should prevent roaming back to the previous IP.\n// This helps prevent flapping due to packets already in flight\nconst RoamingSuppressSeconds = 2\n\nconst (\n\tRequested = iota\n\tPeerRequested\n\tEstablished\n)\n\nconst (\n\tUnknowntype = iota\n\tForwardingType\n\tTerminalType\n)\n\ntype Relay struct {\n\tType        int\n\tState       int\n\tLocalIndex  uint32\n\tRemoteIndex uint32\n\tPeerIp      netip.Addr\n}\n\ntype HostMap struct {\n\tsync.RWMutex    //Because we concurrently read and write to our maps\n\tIndexes         map[uint32]*HostInfo\n\tRelays          map[uint32]*HostInfo // Maps a Relay IDX to a Relay HostInfo object\n\tRemoteIndexes   map[uint32]*HostInfo\n\tHosts           map[netip.Addr]*HostInfo\n\tpreferredRanges atomic.Pointer[[]netip.Prefix]\n\tvpnCIDR         netip.Prefix\n\tl               *logrus.Logger\n}\n\n// For synchronization, treat the pointed-to Relay struct as immutable. To edit the Relay\n// struct, make a copy of an existing value, edit the fileds in the copy, and\n// then store a pointer to the new copy in both realyForBy* maps.\ntype RelayState struct {\n\tsync.RWMutex\n\n\trelays        map[netip.Addr]struct{} // Set of VpnIp's of Hosts to use as relays to access this peer\n\trelayForByIp  map[netip.Addr]*Relay   // Maps VpnIps of peers for which this HostInfo is a relay to some Relay info\n\trelayForByIdx map[uint32]*Relay       // Maps a local index to some Relay info\n}\n\nfunc (rs *RelayState) DeleteRelay(ip netip.Addr) {\n\trs.Lock()\n\tdefer rs.Unlock()\n\tdelete(rs.relays, ip)\n}\n\nfunc (rs *RelayState) CopyAllRelayFor() []*Relay {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tret := make([]*Relay, 0, len(rs.relayForByIdx))\n\tfor _, r := range rs.relayForByIdx {\n\t\tret = append(ret, r)\n\t}\n\treturn ret\n}\n\nfunc (rs *RelayState) GetRelayForByIp(ip netip.Addr) (*Relay, bool) {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tr, ok := rs.relayForByIp[ip]\n\treturn r, ok\n}\n\nfunc (rs *RelayState) InsertRelayTo(ip netip.Addr) {\n\trs.Lock()\n\tdefer rs.Unlock()\n\trs.relays[ip] = struct{}{}\n}\n\nfunc (rs *RelayState) CopyRelayIps() []netip.Addr {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tret := make([]netip.Addr, 0, len(rs.relays))\n\tfor ip := range rs.relays {\n\t\tret = append(ret, ip)\n\t}\n\treturn ret\n}\n\nfunc (rs *RelayState) CopyRelayForIps() []netip.Addr {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tcurrentRelays := make([]netip.Addr, 0, len(rs.relayForByIp))\n\tfor relayIp := range rs.relayForByIp {\n\t\tcurrentRelays = append(currentRelays, relayIp)\n\t}\n\treturn currentRelays\n}\n\nfunc (rs *RelayState) CopyRelayForIdxs() []uint32 {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tret := make([]uint32, 0, len(rs.relayForByIdx))\n\tfor i := range rs.relayForByIdx {\n\t\tret = append(ret, i)\n\t}\n\treturn ret\n}\n\nfunc (rs *RelayState) CompleteRelayByIP(vpnIp netip.Addr, remoteIdx uint32) bool {\n\trs.Lock()\n\tdefer rs.Unlock()\n\tr, ok := rs.relayForByIp[vpnIp]\n\tif !ok {\n\t\treturn false\n\t}\n\tnewRelay := *r\n\tnewRelay.State = Established\n\tnewRelay.RemoteIndex = remoteIdx\n\trs.relayForByIdx[r.LocalIndex] = &newRelay\n\trs.relayForByIp[r.PeerIp] = &newRelay\n\treturn true\n}\n\nfunc (rs *RelayState) CompleteRelayByIdx(localIdx uint32, remoteIdx uint32) (*Relay, bool) {\n\trs.Lock()\n\tdefer rs.Unlock()\n\tr, ok := rs.relayForByIdx[localIdx]\n\tif !ok {\n\t\treturn nil, false\n\t}\n\tnewRelay := *r\n\tnewRelay.State = Established\n\tnewRelay.RemoteIndex = remoteIdx\n\trs.relayForByIdx[r.LocalIndex] = &newRelay\n\trs.relayForByIp[r.PeerIp] = &newRelay\n\treturn &newRelay, true\n}\n\nfunc (rs *RelayState) QueryRelayForByIp(vpnIp netip.Addr) (*Relay, bool) {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tr, ok := rs.relayForByIp[vpnIp]\n\treturn r, ok\n}\n\nfunc (rs *RelayState) QueryRelayForByIdx(idx uint32) (*Relay, bool) {\n\trs.RLock()\n\tdefer rs.RUnlock()\n\tr, ok := rs.relayForByIdx[idx]\n\treturn r, ok\n}\n\nfunc (rs *RelayState) InsertRelay(ip netip.Addr, idx uint32, r *Relay) {\n\trs.Lock()\n\tdefer rs.Unlock()\n\trs.relayForByIp[ip] = r\n\trs.relayForByIdx[idx] = r\n}\n\ntype HostInfo struct {\n\tremote          netip.AddrPort\n\tremotes         *RemoteList\n\tpromoteCounter  atomic.Uint32\n\tConnectionState *ConnectionState\n\tremoteIndexId   uint32\n\tlocalIndexId    uint32\n\tvpnIp           netip.Addr\n\trecvError       atomic.Uint32\n\tremoteCidr      *bart.Table[struct{}]\n\trelayState      RelayState\n\n\t// HandshakePacket records the packets used to create this hostinfo\n\t// We need these to avoid replayed handshake packets creating new hostinfos which causes churn\n\tHandshakePacket map[uint8][]byte\n\n\t// nextLHQuery is the earliest we can ask the lighthouse for new information.\n\t// This is used to limit lighthouse re-queries in chatty clients\n\tnextLHQuery atomic.Int64\n\n\t// lastRebindCount is the other side of Interface.rebindCount, if these values don't match then we need to ask LH\n\t// for a punch from the remote end of this tunnel. The goal being to prime their conntrack for our traffic just like\n\t// with a handshake\n\tlastRebindCount int8\n\n\t// lastHandshakeTime records the time the remote side told us about at the stage when the handshake was completed locally\n\t// Stage 1 packet will contain it if I am a responder, stage 2 packet if I am an initiator\n\t// This is used to avoid an attack where a handshake packet is replayed after some time\n\tlastHandshakeTime uint64\n\n\tlastRoam       time.Time\n\tlastRoamRemote netip.AddrPort\n\n\t// Used to track other hostinfos for this vpn ip since only 1 can be primary\n\t// Synchronised via hostmap lock and not the hostinfo lock.\n\tnext, prev *HostInfo\n}\n\ntype ViaSender struct {\n\trelayHI   *HostInfo // relayHI is the host info object of the relay\n\tremoteIdx uint32    // remoteIdx is the index included in the header of the received packet\n\trelay     *Relay    // relay contains the rest of the relay information, including the PeerIP of the host trying to communicate with us.\n}\n\ntype cachedPacket struct {\n\tmessageType    header.MessageType\n\tmessageSubType header.MessageSubType\n\tcallback       packetCallback\n\tpacket         []byte\n}\n\ntype packetCallback func(t header.MessageType, st header.MessageSubType, h *HostInfo, p, nb, out []byte)\n\ntype cachedPacketMetrics struct {\n\tsent    metrics.Counter\n\tdropped metrics.Counter\n}\n\nfunc NewHostMapFromConfig(l *logrus.Logger, vpnCIDR netip.Prefix, c *config.C) *HostMap {\n\thm := newHostMap(l, vpnCIDR)\n\n\thm.reload(c, true)\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\thm.reload(c, false)\n\t})\n\n\tl.WithField(\"network\", hm.vpnCIDR.String()).\n\t\tWithField(\"preferredRanges\", hm.GetPreferredRanges()).\n\t\tInfo(\"Main HostMap created\")\n\n\treturn hm\n}\n\nfunc newHostMap(l *logrus.Logger, vpnCIDR netip.Prefix) *HostMap {\n\treturn &HostMap{\n\t\tIndexes:       map[uint32]*HostInfo{},\n\t\tRelays:        map[uint32]*HostInfo{},\n\t\tRemoteIndexes: map[uint32]*HostInfo{},\n\t\tHosts:         map[netip.Addr]*HostInfo{},\n\t\tvpnCIDR:       vpnCIDR,\n\t\tl:             l,\n\t}\n}\n\nfunc (hm *HostMap) reload(c *config.C, initial bool) {\n\tif initial || c.HasChanged(\"preferred_ranges\") {\n\t\tvar preferredRanges []netip.Prefix\n\t\trawPreferredRanges := c.GetStringSlice(\"preferred_ranges\", []string{})\n\n\t\tfor _, rawPreferredRange := range rawPreferredRanges {\n\t\t\tpreferredRange, err := netip.ParsePrefix(rawPreferredRange)\n\n\t\t\tif err != nil {\n\t\t\t\thm.l.WithError(err).WithField(\"range\", rawPreferredRanges).Warn(\"Failed to parse preferred ranges, ignoring\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tpreferredRanges = append(preferredRanges, preferredRange)\n\t\t}\n\n\t\toldRanges := hm.preferredRanges.Swap(&preferredRanges)\n\t\tif !initial {\n\t\t\thm.l.WithField(\"oldPreferredRanges\", *oldRanges).WithField(\"newPreferredRanges\", preferredRanges).Info(\"preferred_ranges changed\")\n\t\t}\n\t}\n}\n\n// EmitStats reports host, index, and relay counts to the stats collection system\nfunc (hm *HostMap) EmitStats() {\n\thm.RLock()\n\thostLen := len(hm.Hosts)\n\tindexLen := len(hm.Indexes)\n\tremoteIndexLen := len(hm.RemoteIndexes)\n\trelaysLen := len(hm.Relays)\n\thm.RUnlock()\n\n\tmetrics.GetOrRegisterGauge(\"hostmap.main.hosts\", nil).Update(int64(hostLen))\n\tmetrics.GetOrRegisterGauge(\"hostmap.main.indexes\", nil).Update(int64(indexLen))\n\tmetrics.GetOrRegisterGauge(\"hostmap.main.remoteIndexes\", nil).Update(int64(remoteIndexLen))\n\tmetrics.GetOrRegisterGauge(\"hostmap.main.relayIndexes\", nil).Update(int64(relaysLen))\n}\n\nfunc (hm *HostMap) RemoveRelay(localIdx uint32) {\n\thm.Lock()\n\t_, ok := hm.Relays[localIdx]\n\tif !ok {\n\t\thm.Unlock()\n\t\treturn\n\t}\n\tdelete(hm.Relays, localIdx)\n\thm.Unlock()\n}\n\n// DeleteHostInfo will fully unlink the hostinfo and return true if it was the final hostinfo for this vpn ip\nfunc (hm *HostMap) DeleteHostInfo(hostinfo *HostInfo) bool {\n\t// Delete the host itself, ensuring it's not modified anymore\n\thm.Lock()\n\t// If we have a previous or next hostinfo then we are not the last one for this vpn ip\n\tfinal := (hostinfo.next == nil && hostinfo.prev == nil)\n\thm.unlockedDeleteHostInfo(hostinfo)\n\thm.Unlock()\n\n\treturn final\n}\n\nfunc (hm *HostMap) MakePrimary(hostinfo *HostInfo) {\n\thm.Lock()\n\tdefer hm.Unlock()\n\thm.unlockedMakePrimary(hostinfo)\n}\n\nfunc (hm *HostMap) unlockedMakePrimary(hostinfo *HostInfo) {\n\toldHostinfo := hm.Hosts[hostinfo.vpnIp]\n\tif oldHostinfo == hostinfo {\n\t\treturn\n\t}\n\n\tif hostinfo.prev != nil {\n\t\thostinfo.prev.next = hostinfo.next\n\t}\n\n\tif hostinfo.next != nil {\n\t\thostinfo.next.prev = hostinfo.prev\n\t}\n\n\thm.Hosts[hostinfo.vpnIp] = hostinfo\n\n\tif oldHostinfo == nil {\n\t\treturn\n\t}\n\n\thostinfo.next = oldHostinfo\n\toldHostinfo.prev = hostinfo\n\thostinfo.prev = nil\n}\n\nfunc (hm *HostMap) unlockedDeleteHostInfo(hostinfo *HostInfo) {\n\tprimary, ok := hm.Hosts[hostinfo.vpnIp]\n\tif ok && primary == hostinfo {\n\t\t// The vpnIp pointer points to the same hostinfo as the local index id, we can remove it\n\t\tdelete(hm.Hosts, hostinfo.vpnIp)\n\t\tif len(hm.Hosts) == 0 {\n\t\t\thm.Hosts = map[netip.Addr]*HostInfo{}\n\t\t}\n\n\t\tif hostinfo.next != nil {\n\t\t\t// We had more than 1 hostinfo at this vpnip, promote the next in the list to primary\n\t\t\thm.Hosts[hostinfo.vpnIp] = hostinfo.next\n\t\t\t// It is primary, there is no previous hostinfo now\n\t\t\thostinfo.next.prev = nil\n\t\t}\n\n\t} else {\n\t\t// Relink if we were in the middle of multiple hostinfos for this vpn ip\n\t\tif hostinfo.prev != nil {\n\t\t\thostinfo.prev.next = hostinfo.next\n\t\t}\n\n\t\tif hostinfo.next != nil {\n\t\t\thostinfo.next.prev = hostinfo.prev\n\t\t}\n\t}\n\n\thostinfo.next = nil\n\thostinfo.prev = nil\n\n\t// The remote index uses index ids outside our control so lets make sure we are only removing\n\t// the remote index pointer here if it points to the hostinfo we are deleting\n\thostinfo2, ok := hm.RemoteIndexes[hostinfo.remoteIndexId]\n\tif ok && hostinfo2 == hostinfo {\n\t\tdelete(hm.RemoteIndexes, hostinfo.remoteIndexId)\n\t\tif len(hm.RemoteIndexes) == 0 {\n\t\t\thm.RemoteIndexes = map[uint32]*HostInfo{}\n\t\t}\n\t}\n\n\tdelete(hm.Indexes, hostinfo.localIndexId)\n\tif len(hm.Indexes) == 0 {\n\t\thm.Indexes = map[uint32]*HostInfo{}\n\t}\n\n\tif hm.l.Level >= logrus.DebugLevel {\n\t\thm.l.WithField(\"hostMap\", m{\"mapTotalSize\": len(hm.Hosts),\n\t\t\t\"vpnIp\": hostinfo.vpnIp, \"indexNumber\": hostinfo.localIndexId, \"remoteIndexNumber\": hostinfo.remoteIndexId}).\n\t\t\tDebug(\"Hostmap hostInfo deleted\")\n\t}\n\n\tfor _, localRelayIdx := range hostinfo.relayState.CopyRelayForIdxs() {\n\t\tdelete(hm.Relays, localRelayIdx)\n\t}\n}\n\nfunc (hm *HostMap) QueryIndex(index uint32) *HostInfo {\n\thm.RLock()\n\tif h, ok := hm.Indexes[index]; ok {\n\t\thm.RUnlock()\n\t\treturn h\n\t} else {\n\t\thm.RUnlock()\n\t\treturn nil\n\t}\n}\n\nfunc (hm *HostMap) QueryRelayIndex(index uint32) *HostInfo {\n\thm.RLock()\n\tif h, ok := hm.Relays[index]; ok {\n\t\thm.RUnlock()\n\t\treturn h\n\t} else {\n\t\thm.RUnlock()\n\t\treturn nil\n\t}\n}\n\nfunc (hm *HostMap) QueryReverseIndex(index uint32) *HostInfo {\n\thm.RLock()\n\tif h, ok := hm.RemoteIndexes[index]; ok {\n\t\thm.RUnlock()\n\t\treturn h\n\t} else {\n\t\thm.RUnlock()\n\t\treturn nil\n\t}\n}\n\nfunc (hm *HostMap) QueryVpnIp(vpnIp netip.Addr) *HostInfo {\n\treturn hm.queryVpnIp(vpnIp, nil)\n}\n\nfunc (hm *HostMap) QueryVpnIpRelayFor(targetIp, relayHostIp netip.Addr) (*HostInfo, *Relay, error) {\n\thm.RLock()\n\tdefer hm.RUnlock()\n\n\th, ok := hm.Hosts[relayHostIp]\n\tif !ok {\n\t\treturn nil, nil, errors.New(\"unable to find host\")\n\t}\n\tfor h != nil {\n\t\tr, ok := h.relayState.QueryRelayForByIp(targetIp)\n\t\tif ok && r.State == Established {\n\t\t\treturn h, r, nil\n\t\t}\n\t\th = h.next\n\t}\n\treturn nil, nil, errors.New(\"unable to find host with relay\")\n}\n\nfunc (hm *HostMap) queryVpnIp(vpnIp netip.Addr, promoteIfce *Interface) *HostInfo {\n\thm.RLock()\n\tif h, ok := hm.Hosts[vpnIp]; ok {\n\t\thm.RUnlock()\n\t\t// Do not attempt promotion if you are a lighthouse\n\t\tif promoteIfce != nil && !promoteIfce.lightHouse.amLighthouse {\n\t\t\th.TryPromoteBest(hm.GetPreferredRanges(), promoteIfce)\n\t\t}\n\t\treturn h\n\n\t}\n\n\thm.RUnlock()\n\treturn nil\n}\n\n// unlockedAddHostInfo assumes you have a write-lock and will add a hostinfo object to the hostmap Indexes and RemoteIndexes maps.\n// If an entry exists for the Hosts table (vpnIp -> hostinfo) then the provided hostinfo will be made primary\nfunc (hm *HostMap) unlockedAddHostInfo(hostinfo *HostInfo, f *Interface) {\n\tif f.serveDns {\n\t\tremoteCert := hostinfo.ConnectionState.peerCert\n\t\tdnsR.Add(remoteCert.Certificate.Name()+\".\", remoteCert.Certificate.Networks()[0].Addr().String())\n\t}\n\n\texisting := hm.Hosts[hostinfo.vpnIp]\n\thm.Hosts[hostinfo.vpnIp] = hostinfo\n\n\tif existing != nil {\n\t\thostinfo.next = existing\n\t\texisting.prev = hostinfo\n\t}\n\n\thm.Indexes[hostinfo.localIndexId] = hostinfo\n\thm.RemoteIndexes[hostinfo.remoteIndexId] = hostinfo\n\n\tif hm.l.Level >= logrus.DebugLevel {\n\t\thm.l.WithField(\"hostMap\", m{\"vpnIp\": hostinfo.vpnIp, \"mapTotalSize\": len(hm.Hosts),\n\t\t\t\"hostinfo\": m{\"existing\": true, \"localIndexId\": hostinfo.localIndexId, \"hostId\": hostinfo.vpnIp}}).\n\t\t\tDebug(\"Hostmap vpnIp added\")\n\t}\n\n\ti := 1\n\tcheck := hostinfo\n\tfor check != nil {\n\t\tif i > MaxHostInfosPerVpnIp {\n\t\t\thm.unlockedDeleteHostInfo(check)\n\t\t}\n\t\tcheck = check.next\n\t\ti++\n\t}\n}\n\nfunc (hm *HostMap) GetPreferredRanges() []netip.Prefix {\n\t//NOTE: if preferredRanges is ever not stored before a load this will fail to dereference a nil pointer\n\treturn *hm.preferredRanges.Load()\n}\n\nfunc (hm *HostMap) ForEachVpnIp(f controlEach) {\n\thm.RLock()\n\tdefer hm.RUnlock()\n\n\tfor _, v := range hm.Hosts {\n\t\tf(v)\n\t}\n}\n\nfunc (hm *HostMap) ForEachIndex(f controlEach) {\n\thm.RLock()\n\tdefer hm.RUnlock()\n\n\tfor _, v := range hm.Indexes {\n\t\tf(v)\n\t}\n}\n\n// TryPromoteBest handles re-querying lighthouses and probing for better paths\n// NOTE: It is an error to call this if you are a lighthouse since they should not roam clients!\nfunc (i *HostInfo) TryPromoteBest(preferredRanges []netip.Prefix, ifce *Interface) {\n\tc := i.promoteCounter.Add(1)\n\tif c%ifce.tryPromoteEvery.Load() == 0 {\n\t\tremote := i.remote\n\n\t\t// return early if we are already on a preferred remote\n\t\tif remote.IsValid() {\n\t\t\trIP := remote.Addr()\n\t\t\tfor _, l := range preferredRanges {\n\t\t\t\tif l.Contains(rIP) {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti.remotes.ForEach(preferredRanges, func(addr netip.AddrPort, preferred bool) {\n\t\t\tif remote.IsValid() && (!addr.IsValid() || !preferred) {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Try to send a test packet to that host, this should\n\t\t\t// cause it to detect a roaming event and switch remotes\n\t\t\tifce.sendTo(header.Test, header.TestRequest, i.ConnectionState, i, addr, []byte(\"\"), make([]byte, 12, 12), make([]byte, mtu))\n\t\t})\n\t}\n\n\t// Re query our lighthouses for new remotes occasionally\n\tif c%ifce.reQueryEvery.Load() == 0 && ifce.lightHouse != nil {\n\t\tnow := time.Now().UnixNano()\n\t\tif now < i.nextLHQuery.Load() {\n\t\t\treturn\n\t\t}\n\n\t\ti.nextLHQuery.Store(now + ifce.reQueryWait.Load())\n\t\tifce.lightHouse.QueryServer(i.vpnIp)\n\t}\n}\n\nfunc (i *HostInfo) GetCert() *cert.CachedCertificate {\n\tif i.ConnectionState != nil {\n\t\treturn i.ConnectionState.peerCert\n\t}\n\treturn nil\n}\n\nfunc (i *HostInfo) SetRemote(remote netip.AddrPort) {\n\t// We copy here because we likely got this remote from a source that reuses the object\n\tif i.remote != remote {\n\t\ti.remote = remote\n\t\ti.remotes.LearnRemote(i.vpnIp, remote)\n\t}\n}\n\n// SetRemoteIfPreferred returns true if the remote was changed. The lastRoam\n// time on the HostInfo will also be updated.\nfunc (i *HostInfo) SetRemoteIfPreferred(hm *HostMap, newRemote netip.AddrPort) bool {\n\tif !newRemote.IsValid() {\n\t\t// relays have nil udp Addrs\n\t\treturn false\n\t}\n\tcurrentRemote := i.remote\n\tif !currentRemote.IsValid() {\n\t\ti.SetRemote(newRemote)\n\t\treturn true\n\t}\n\n\t// NOTE: We do this loop here instead of calling `isPreferred` in\n\t// remote_list.go so that we only have to loop over preferredRanges once.\n\tnewIsPreferred := false\n\tfor _, l := range hm.GetPreferredRanges() {\n\t\t// return early if we are already on a preferred remote\n\t\tif l.Contains(currentRemote.Addr()) {\n\t\t\treturn false\n\t\t}\n\n\t\tif l.Contains(newRemote.Addr()) {\n\t\t\tnewIsPreferred = true\n\t\t}\n\t}\n\n\tif newIsPreferred {\n\t\t// Consider this a roaming event\n\t\ti.lastRoam = time.Now()\n\t\ti.lastRoamRemote = currentRemote\n\n\t\ti.SetRemote(newRemote)\n\n\t\treturn true\n\t}\n\n\treturn false\n}\n\nfunc (i *HostInfo) RecvErrorExceeded() bool {\n\tif i.recvError.Add(1) >= maxRecvError {\n\t\treturn true\n\t}\n\treturn true\n}\n\nfunc (i *HostInfo) CreateRemoteCIDR(c cert.Certificate) {\n\tif len(c.Networks()) == 1 && len(c.UnsafeNetworks()) == 0 {\n\t\t// Simple case, no CIDRTree needed\n\t\treturn\n\t}\n\n\tremoteCidr := new(bart.Table[struct{}])\n\tfor _, network := range c.Networks() {\n\t\tremoteCidr.Insert(network, struct{}{})\n\t}\n\n\tfor _, network := range c.UnsafeNetworks() {\n\t\tremoteCidr.Insert(network, struct{}{})\n\t}\n\ti.remoteCidr = remoteCidr\n}\n\nfunc (i *HostInfo) logger(l *logrus.Logger) *logrus.Entry {\n\tif i == nil {\n\t\treturn logrus.NewEntry(l)\n\t}\n\n\tli := l.WithField(\"vpnIp\", i.vpnIp).\n\t\tWithField(\"localIndex\", i.localIndexId).\n\t\tWithField(\"remoteIndex\", i.remoteIndexId)\n\n\tif connState := i.ConnectionState; connState != nil {\n\t\tif peerCert := connState.peerCert; peerCert != nil {\n\t\t\tli = li.WithField(\"certName\", peerCert.Certificate.Name())\n\t\t}\n\t}\n\n\treturn li\n}\n\n// Utility functions\n\nfunc localIps(l *logrus.Logger, allowList *LocalAllowList) []netip.Addr {\n\t//FIXME: This function is pretty garbage\n\tvar ips []netip.Addr\n\tifaces, _ := net.Interfaces()\n\tfor _, i := range ifaces {\n\t\tallow := allowList.AllowName(i.Name)\n\t\tif l.Level >= logrus.TraceLevel {\n\t\t\tl.WithField(\"interfaceName\", i.Name).WithField(\"allow\", allow).Trace(\"localAllowList.AllowName\")\n\t\t}\n\n\t\tif !allow {\n\t\t\tcontinue\n\t\t}\n\t\taddrs, _ := i.Addrs()\n\t\tfor _, addr := range addrs {\n\t\t\tvar ip net.IP\n\t\t\tswitch v := addr.(type) {\n\t\t\tcase *net.IPNet:\n\t\t\t\t//continue\n\t\t\t\tip = v.IP\n\t\t\tcase *net.IPAddr:\n\t\t\t\tip = v.IP\n\t\t\t}\n\n\t\t\tnip, ok := netip.AddrFromSlice(ip)\n\t\t\tif !ok {\n\t\t\t\tif l.Level >= logrus.DebugLevel {\n\t\t\t\t\tl.WithField(\"localIp\", ip).Debug(\"ip was invalid for netip\")\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnip = nip.Unmap()\n\n\t\t\t//TODO: Filtering out link local for now, this is probably the most correct thing\n\t\t\t//TODO: Would be nice to filter out SLAAC MAC based ips as well\n\t\t\tif nip.IsLoopback() == false && nip.IsLinkLocalUnicast() == false {\n\t\t\t\tallow := allowList.Allow(nip)\n\t\t\t\tif l.Level >= logrus.TraceLevel {\n\t\t\t\t\tl.WithField(\"localIp\", nip).WithField(\"allow\", allow).Trace(\"localAllowList.Allow\")\n\t\t\t\t}\n\t\t\t\tif !allow {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tips = append(ips, nip)\n\t\t\t}\n\t\t}\n\t}\n\treturn ips\n}\n"
        },
        {
          "name": "hostmap_test.go",
          "type": "blob",
          "size": 7.291015625,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"testing\"\n\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestHostMap_MakePrimary(t *testing.T) {\n\tl := test.NewLogger()\n\thm := newHostMap(\n\t\tl,\n\t\tnetip.MustParsePrefix(\"10.0.0.1/24\"),\n\t)\n\n\tf := &Interface{}\n\n\th1 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 1}\n\th2 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 2}\n\th3 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 3}\n\th4 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 4}\n\n\thm.unlockedAddHostInfo(h4, f)\n\thm.unlockedAddHostInfo(h3, f)\n\thm.unlockedAddHostInfo(h2, f)\n\thm.unlockedAddHostInfo(h1, f)\n\n\t// Make sure we go h1 -> h2 -> h3 -> h4\n\tprim := hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h1.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h2.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h1.localIndexId, h2.prev.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h3.prev.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h3.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h4.prev.localIndexId)\n\tassert.Nil(t, h4.next)\n\n\t// Swap h3/middle to primary\n\thm.MakePrimary(h3)\n\n\t// Make sure we go h3 -> h1 -> h2 -> h4\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h3.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h1.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h2.localIndexId, h1.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h1.prev.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h1.localIndexId, h2.prev.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h4.prev.localIndexId)\n\tassert.Nil(t, h4.next)\n\n\t// Swap h4/tail to primary\n\thm.MakePrimary(h4)\n\n\t// Make sure we go h4 -> h3 -> h1 -> h2\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h4.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h3.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h1.localIndexId, h3.next.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h3.prev.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h1.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h1.prev.localIndexId)\n\tassert.Equal(t, h1.localIndexId, h2.prev.localIndexId)\n\tassert.Nil(t, h2.next)\n\n\t// Swap h4 again should be no-op\n\thm.MakePrimary(h4)\n\n\t// Make sure we go h4 -> h3 -> h1 -> h2\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h4.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h3.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h1.localIndexId, h3.next.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h3.prev.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h1.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h1.prev.localIndexId)\n\tassert.Equal(t, h1.localIndexId, h2.prev.localIndexId)\n\tassert.Nil(t, h2.next)\n}\n\nfunc TestHostMap_DeleteHostInfo(t *testing.T) {\n\tl := test.NewLogger()\n\thm := newHostMap(\n\t\tl,\n\t\tnetip.MustParsePrefix(\"10.0.0.1/24\"),\n\t)\n\n\tf := &Interface{}\n\n\th1 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 1}\n\th2 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 2}\n\th3 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 3}\n\th4 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 4}\n\th5 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 5}\n\th6 := &HostInfo{vpnIp: netip.MustParseAddr(\"0.0.0.1\"), localIndexId: 6}\n\n\thm.unlockedAddHostInfo(h6, f)\n\thm.unlockedAddHostInfo(h5, f)\n\thm.unlockedAddHostInfo(h4, f)\n\thm.unlockedAddHostInfo(h3, f)\n\thm.unlockedAddHostInfo(h2, f)\n\thm.unlockedAddHostInfo(h1, f)\n\n\t// h6 should be deleted\n\tassert.Nil(t, h6.next)\n\tassert.Nil(t, h6.prev)\n\th := hm.QueryIndex(h6.localIndexId)\n\tassert.Nil(t, h)\n\n\t// Make sure we go h1 -> h2 -> h3 -> h4 -> h5\n\tprim := hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h1.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h2.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h1.localIndexId, h2.prev.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h3.prev.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h3.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h4.prev.localIndexId)\n\tassert.Equal(t, h5.localIndexId, h4.next.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h5.prev.localIndexId)\n\tassert.Nil(t, h5.next)\n\n\t// Delete primary\n\thm.DeleteHostInfo(h1)\n\tassert.Nil(t, h1.prev)\n\tassert.Nil(t, h1.next)\n\n\t// Make sure we go h2 -> h3 -> h4 -> h5\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h2.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h3.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h3.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h3.prev.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h3.next.localIndexId)\n\tassert.Equal(t, h3.localIndexId, h4.prev.localIndexId)\n\tassert.Equal(t, h5.localIndexId, h4.next.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h5.prev.localIndexId)\n\tassert.Nil(t, h5.next)\n\n\t// Delete in the middle\n\thm.DeleteHostInfo(h3)\n\tassert.Nil(t, h3.prev)\n\tassert.Nil(t, h3.next)\n\n\t// Make sure we go h2 -> h4 -> h5\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h2.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h4.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h4.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h4.prev.localIndexId)\n\tassert.Equal(t, h5.localIndexId, h4.next.localIndexId)\n\tassert.Equal(t, h4.localIndexId, h5.prev.localIndexId)\n\tassert.Nil(t, h5.next)\n\n\t// Delete the tail\n\thm.DeleteHostInfo(h5)\n\tassert.Nil(t, h5.prev)\n\tassert.Nil(t, h5.next)\n\n\t// Make sure we go h2 -> h4\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h2.localIndexId, prim.localIndexId)\n\tassert.Equal(t, h4.localIndexId, prim.next.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Equal(t, h4.localIndexId, h2.next.localIndexId)\n\tassert.Equal(t, h2.localIndexId, h4.prev.localIndexId)\n\tassert.Nil(t, h4.next)\n\n\t// Delete the head\n\thm.DeleteHostInfo(h2)\n\tassert.Nil(t, h2.prev)\n\tassert.Nil(t, h2.next)\n\n\t// Make sure we only have h4\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Equal(t, h4.localIndexId, prim.localIndexId)\n\tassert.Nil(t, prim.prev)\n\tassert.Nil(t, prim.next)\n\tassert.Nil(t, h4.next)\n\n\t// Delete the only item\n\thm.DeleteHostInfo(h4)\n\tassert.Nil(t, h4.prev)\n\tassert.Nil(t, h4.next)\n\n\t// Make sure we have nil\n\tprim = hm.QueryVpnIp(netip.MustParseAddr(\"0.0.0.1\"))\n\tassert.Nil(t, prim)\n}\n\nfunc TestHostMap_reload(t *testing.T) {\n\tl := test.NewLogger()\n\tc := config.NewC(l)\n\n\thm := NewHostMapFromConfig(\n\t\tl,\n\t\tnetip.MustParsePrefix(\"10.0.0.1/24\"),\n\t\tc,\n\t)\n\n\ttoS := func(ipn []netip.Prefix) []string {\n\t\tvar s []string\n\t\tfor _, n := range ipn {\n\t\t\ts = append(s, n.String())\n\t\t}\n\t\treturn s\n\t}\n\n\tassert.Empty(t, hm.GetPreferredRanges())\n\n\tc.ReloadConfigString(\"preferred_ranges: [1.1.1.0/24, 10.1.1.0/24]\")\n\tassert.EqualValues(t, []string{\"1.1.1.0/24\", \"10.1.1.0/24\"}, toS(hm.GetPreferredRanges()))\n\n\tc.ReloadConfigString(\"preferred_ranges: [1.1.1.1/32]\")\n\tassert.EqualValues(t, []string{\"1.1.1.1/32\"}, toS(hm.GetPreferredRanges()))\n}\n"
        },
        {
          "name": "hostmap_tester.go",
          "type": "blob",
          "size": 0.4423828125,
          "content": "//go:build e2e_testing\n// +build e2e_testing\n\npackage nebula\n\n// This file contains functions used to export information to the e2e testing framework\n\nimport (\n\t\"net/netip\"\n)\n\nfunc (i *HostInfo) GetVpnIp() netip.Addr {\n\treturn i.vpnIp\n}\n\nfunc (i *HostInfo) GetLocalIndex() uint32 {\n\treturn i.localIndexId\n}\n\nfunc (i *HostInfo) GetRemoteIndex() uint32 {\n\treturn i.remoteIndexId\n}\n\nfunc (i *HostInfo) GetRelayState() *RelayState {\n\treturn &i.relayState\n}\n"
        },
        {
          "name": "inside.go",
          "type": "blob",
          "size": 10.91796875,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/iputil\"\n\t\"github.com/slackhq/nebula/noiseutil\"\n)\n\nfunc (f *Interface) consumeInsidePacket(packet []byte, fwPacket *firewall.Packet, nb, out []byte, q int, localCache firewall.ConntrackCache) {\n\terr := newPacket(packet, false, fwPacket)\n\tif err != nil {\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\tf.l.WithField(\"packet\", packet).Debugf(\"Error while validating outbound packet: %s\", err)\n\t\t}\n\t\treturn\n\t}\n\n\t// Ignore local broadcast packets\n\tif f.dropLocalBroadcast && fwPacket.RemoteIP == f.myBroadcastAddr {\n\t\treturn\n\t}\n\n\tif fwPacket.RemoteIP == f.myVpnNet.Addr() {\n\t\t// Immediately forward packets from self to self.\n\t\t// This should only happen on Darwin-based and FreeBSD hosts, which\n\t\t// routes packets from the Nebula IP to the Nebula IP through the Nebula\n\t\t// TUN device.\n\t\tif immediatelyForwardToSelf {\n\t\t\t_, err := f.readers[q].Write(packet)\n\t\t\tif err != nil {\n\t\t\t\tf.l.WithError(err).Error(\"Failed to forward to tun\")\n\t\t\t}\n\t\t}\n\t\t// Otherwise, drop. On linux, we should never see these packets - Linux\n\t\t// routes packets from the nebula IP to the nebula IP through the loopback device.\n\t\treturn\n\t}\n\n\t// Ignore multicast packets\n\tif f.dropMulticast && fwPacket.RemoteIP.IsMulticast() {\n\t\treturn\n\t}\n\n\thostinfo, ready := f.getOrHandshake(fwPacket.RemoteIP, func(hh *HandshakeHostInfo) {\n\t\thh.cachePacket(f.l, header.Message, 0, packet, f.sendMessageNow, f.cachedPacketMetrics)\n\t})\n\n\tif hostinfo == nil {\n\t\tf.rejectInside(packet, out, q)\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\tf.l.WithField(\"vpnIp\", fwPacket.RemoteIP).\n\t\t\t\tWithField(\"fwPacket\", fwPacket).\n\t\t\t\tDebugln(\"dropping outbound packet, vpnIp not in our CIDR or in unsafe routes\")\n\t\t}\n\t\treturn\n\t}\n\n\tif !ready {\n\t\treturn\n\t}\n\n\tdropReason := f.firewall.Drop(*fwPacket, false, hostinfo, f.pki.GetCAPool(), localCache)\n\tif dropReason == nil {\n\t\tf.sendNoMetrics(header.Message, 0, hostinfo.ConnectionState, hostinfo, netip.AddrPort{}, packet, nb, out, q)\n\n\t} else {\n\t\tf.rejectInside(packet, out, q)\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\thostinfo.logger(f.l).\n\t\t\t\tWithField(\"fwPacket\", fwPacket).\n\t\t\t\tWithField(\"reason\", dropReason).\n\t\t\t\tDebugln(\"dropping outbound packet\")\n\t\t}\n\t}\n}\n\nfunc (f *Interface) rejectInside(packet []byte, out []byte, q int) {\n\tif !f.firewall.InSendReject {\n\t\treturn\n\t}\n\n\tout = iputil.CreateRejectPacket(packet, out)\n\tif len(out) == 0 {\n\t\treturn\n\t}\n\n\t_, err := f.readers[q].Write(out)\n\tif err != nil {\n\t\tf.l.WithError(err).Error(\"Failed to write to tun\")\n\t}\n}\n\nfunc (f *Interface) rejectOutside(packet []byte, ci *ConnectionState, hostinfo *HostInfo, nb, out []byte, q int) {\n\tif !f.firewall.OutSendReject {\n\t\treturn\n\t}\n\n\tout = iputil.CreateRejectPacket(packet, out)\n\tif len(out) == 0 {\n\t\treturn\n\t}\n\n\tif len(out) > iputil.MaxRejectPacketSize {\n\t\tif f.l.GetLevel() >= logrus.InfoLevel {\n\t\t\tf.l.\n\t\t\t\tWithField(\"packet\", packet).\n\t\t\t\tWithField(\"outPacket\", out).\n\t\t\t\tInfo(\"rejectOutside: packet too big, not sending\")\n\t\t}\n\t\treturn\n\t}\n\n\tf.sendNoMetrics(header.Message, 0, ci, hostinfo, netip.AddrPort{}, out, nb, packet, q)\n}\n\nfunc (f *Interface) Handshake(vpnIp netip.Addr) {\n\tf.getOrHandshake(vpnIp, nil)\n}\n\n// getOrHandshake returns nil if the vpnIp is not routable.\n// If the 2nd return var is false then the hostinfo is not ready to be used in a tunnel\nfunc (f *Interface) getOrHandshake(vpnIp netip.Addr, cacheCallback func(*HandshakeHostInfo)) (*HostInfo, bool) {\n\tif !f.myVpnNet.Contains(vpnIp) {\n\t\tvpnIp = f.inside.RouteFor(vpnIp)\n\t\tif !vpnIp.IsValid() {\n\t\t\treturn nil, false\n\t\t}\n\t}\n\n\treturn f.handshakeManager.GetOrHandshake(vpnIp, cacheCallback)\n}\n\nfunc (f *Interface) sendMessageNow(t header.MessageType, st header.MessageSubType, hostinfo *HostInfo, p, nb, out []byte) {\n\tfp := &firewall.Packet{}\n\terr := newPacket(p, false, fp)\n\tif err != nil {\n\t\tf.l.Warnf(\"error while parsing outgoing packet for firewall check; %v\", err)\n\t\treturn\n\t}\n\n\t// check if packet is in outbound fw rules\n\tdropReason := f.firewall.Drop(*fp, false, hostinfo, f.pki.GetCAPool(), nil)\n\tif dropReason != nil {\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\tf.l.WithField(\"fwPacket\", fp).\n\t\t\t\tWithField(\"reason\", dropReason).\n\t\t\t\tDebugln(\"dropping cached packet\")\n\t\t}\n\t\treturn\n\t}\n\n\tf.sendNoMetrics(header.Message, st, hostinfo.ConnectionState, hostinfo, netip.AddrPort{}, p, nb, out, 0)\n}\n\n// SendMessageToVpnIp handles real ip:port lookup and sends to the current best known address for vpnIp\nfunc (f *Interface) SendMessageToVpnIp(t header.MessageType, st header.MessageSubType, vpnIp netip.Addr, p, nb, out []byte) {\n\thostInfo, ready := f.getOrHandshake(vpnIp, func(hh *HandshakeHostInfo) {\n\t\thh.cachePacket(f.l, t, st, p, f.SendMessageToHostInfo, f.cachedPacketMetrics)\n\t})\n\n\tif hostInfo == nil {\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\tf.l.WithField(\"vpnIp\", vpnIp).\n\t\t\t\tDebugln(\"dropping SendMessageToVpnIp, vpnIp not in our CIDR or in unsafe routes\")\n\t\t}\n\t\treturn\n\t}\n\n\tif !ready {\n\t\treturn\n\t}\n\n\tf.SendMessageToHostInfo(t, st, hostInfo, p, nb, out)\n}\n\nfunc (f *Interface) SendMessageToHostInfo(t header.MessageType, st header.MessageSubType, hi *HostInfo, p, nb, out []byte) {\n\tf.send(t, st, hi.ConnectionState, hi, p, nb, out)\n}\n\nfunc (f *Interface) send(t header.MessageType, st header.MessageSubType, ci *ConnectionState, hostinfo *HostInfo, p, nb, out []byte) {\n\tf.messageMetrics.Tx(t, st, 1)\n\tf.sendNoMetrics(t, st, ci, hostinfo, netip.AddrPort{}, p, nb, out, 0)\n}\n\nfunc (f *Interface) sendTo(t header.MessageType, st header.MessageSubType, ci *ConnectionState, hostinfo *HostInfo, remote netip.AddrPort, p, nb, out []byte) {\n\tf.messageMetrics.Tx(t, st, 1)\n\tf.sendNoMetrics(t, st, ci, hostinfo, remote, p, nb, out, 0)\n}\n\n// SendVia sends a payload through a Relay tunnel. No authentication or encryption is done\n// to the payload for the ultimate target host, making this a useful method for sending\n// handshake messages to peers through relay tunnels.\n// via is the HostInfo through which the message is relayed.\n// ad is the plaintext data to authenticate, but not encrypt\n// nb is a buffer used to store the nonce value, re-used for performance reasons.\n// out is a buffer used to store the result of the Encrypt operation\n// q indicates which writer to use to send the packet.\nfunc (f *Interface) SendVia(via *HostInfo,\n\trelay *Relay,\n\tad,\n\tnb,\n\tout []byte,\n\tnocopy bool,\n) {\n\tif noiseutil.EncryptLockNeeded {\n\t\t// NOTE: for goboring AESGCMTLS we need to lock because of the nonce check\n\t\tvia.ConnectionState.writeLock.Lock()\n\t}\n\tc := via.ConnectionState.messageCounter.Add(1)\n\n\tout = header.Encode(out, header.Version, header.Message, header.MessageRelay, relay.RemoteIndex, c)\n\tf.connectionManager.Out(via.localIndexId)\n\n\t// Authenticate the header and payload, but do not encrypt for this message type.\n\t// The payload consists of the inner, unencrypted Nebula header, as well as the end-to-end encrypted payload.\n\tif len(out)+len(ad)+via.ConnectionState.eKey.Overhead() > cap(out) {\n\t\tif noiseutil.EncryptLockNeeded {\n\t\t\tvia.ConnectionState.writeLock.Unlock()\n\t\t}\n\t\tvia.logger(f.l).\n\t\t\tWithField(\"outCap\", cap(out)).\n\t\t\tWithField(\"payloadLen\", len(ad)).\n\t\t\tWithField(\"headerLen\", len(out)).\n\t\t\tWithField(\"cipherOverhead\", via.ConnectionState.eKey.Overhead()).\n\t\t\tError(\"SendVia out buffer not large enough for relay\")\n\t\treturn\n\t}\n\n\t// The header bytes are written to the 'out' slice; Grow the slice to hold the header and associated data payload.\n\toffset := len(out)\n\tout = out[:offset+len(ad)]\n\n\t// In one call path, the associated data _is_ already stored in out. In other call paths, the associated data must\n\t// be copied into 'out'.\n\tif !nocopy {\n\t\tcopy(out[offset:], ad)\n\t}\n\n\tvar err error\n\tout, err = via.ConnectionState.eKey.EncryptDanger(out, out, nil, c, nb)\n\tif noiseutil.EncryptLockNeeded {\n\t\tvia.ConnectionState.writeLock.Unlock()\n\t}\n\tif err != nil {\n\t\tvia.logger(f.l).WithError(err).Info(\"Failed to EncryptDanger in sendVia\")\n\t\treturn\n\t}\n\terr = f.writers[0].WriteTo(out, via.remote)\n\tif err != nil {\n\t\tvia.logger(f.l).WithError(err).Info(\"Failed to WriteTo in sendVia\")\n\t}\n\tf.connectionManager.RelayUsed(relay.LocalIndex)\n}\n\nfunc (f *Interface) sendNoMetrics(t header.MessageType, st header.MessageSubType, ci *ConnectionState, hostinfo *HostInfo, remote netip.AddrPort, p, nb, out []byte, q int) {\n\tif ci.eKey == nil {\n\t\t//TODO: log warning\n\t\treturn\n\t}\n\tuseRelay := !remote.IsValid() && !hostinfo.remote.IsValid()\n\tfullOut := out\n\n\tif useRelay {\n\t\tif len(out) < header.Len {\n\t\t\t// out always has a capacity of mtu, but not always a length greater than the header.Len.\n\t\t\t// Grow it to make sure the next operation works.\n\t\t\tout = out[:header.Len]\n\t\t}\n\t\t// Save a header's worth of data at the front of the 'out' buffer.\n\t\tout = out[header.Len:]\n\t}\n\n\tif noiseutil.EncryptLockNeeded {\n\t\t// NOTE: for goboring AESGCMTLS we need to lock because of the nonce check\n\t\tci.writeLock.Lock()\n\t}\n\tc := ci.messageCounter.Add(1)\n\n\t//l.WithField(\"trace\", string(debug.Stack())).Error(\"out Header \", &Header{Version, t, st, 0, hostinfo.remoteIndexId, c}, p)\n\tout = header.Encode(out, header.Version, t, st, hostinfo.remoteIndexId, c)\n\tf.connectionManager.Out(hostinfo.localIndexId)\n\n\t// Query our LH if we haven't since the last time we've been rebound, this will cause the remote to punch against\n\t// all our IPs and enable a faster roaming.\n\tif t != header.CloseTunnel && hostinfo.lastRebindCount != f.rebindCount {\n\t\t//NOTE: there is an update hole if a tunnel isn't used and exactly 256 rebinds occur before the tunnel is\n\t\t// finally used again. This tunnel would eventually be torn down and recreated if this action didn't help.\n\t\tf.lightHouse.QueryServer(hostinfo.vpnIp)\n\t\thostinfo.lastRebindCount = f.rebindCount\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\tf.l.WithField(\"vpnIp\", hostinfo.vpnIp).Debug(\"Lighthouse update triggered for punch due to rebind counter\")\n\t\t}\n\t}\n\n\tvar err error\n\tout, err = ci.eKey.EncryptDanger(out, out, p, c, nb)\n\tif noiseutil.EncryptLockNeeded {\n\t\tci.writeLock.Unlock()\n\t}\n\tif err != nil {\n\t\thostinfo.logger(f.l).WithError(err).\n\t\t\tWithField(\"udpAddr\", remote).WithField(\"counter\", c).\n\t\t\tWithField(\"attemptedCounter\", c).\n\t\t\tError(\"Failed to encrypt outgoing packet\")\n\t\treturn\n\t}\n\n\tif remote.IsValid() {\n\t\terr = f.writers[q].WriteTo(out, remote)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).\n\t\t\t\tWithField(\"udpAddr\", remote).Error(\"Failed to write outgoing packet\")\n\t\t}\n\t} else if hostinfo.remote.IsValid() {\n\t\terr = f.writers[q].WriteTo(out, hostinfo.remote)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).\n\t\t\t\tWithField(\"udpAddr\", remote).Error(\"Failed to write outgoing packet\")\n\t\t}\n\t} else {\n\t\t// Try to send via a relay\n\t\tfor _, relayIP := range hostinfo.relayState.CopyRelayIps() {\n\t\t\trelayHostInfo, relay, err := f.hostMap.QueryVpnIpRelayFor(hostinfo.vpnIp, relayIP)\n\t\t\tif err != nil {\n\t\t\t\thostinfo.relayState.DeleteRelay(relayIP)\n\t\t\t\thostinfo.logger(f.l).WithField(\"relay\", relayIP).WithError(err).Info(\"sendNoMetrics failed to find HostInfo\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tf.SendVia(relayHostInfo, relay, out, nb, fullOut[:header.Len+len(out)], true)\n\t\t\tbreak\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "inside_bsd.go",
          "type": "blob",
          "size": 0.1689453125,
          "content": "//go:build darwin || dragonfly || freebsd || netbsd || openbsd\n// +build darwin dragonfly freebsd netbsd openbsd\n\npackage nebula\n\nconst immediatelyForwardToSelf bool = true\n"
        },
        {
          "name": "inside_generic.go",
          "type": "blob",
          "size": 0.1796875,
          "content": "//go:build !darwin && !dragonfly && !freebsd && !netbsd && !openbsd\n// +build !darwin,!dragonfly,!freebsd,!netbsd,!openbsd\n\npackage nebula\n\nconst immediatelyForwardToSelf bool = false\n"
        },
        {
          "name": "interface.go",
          "type": "blob",
          "size": 11.67578125,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/netip\"\n\t\"os\"\n\t\"runtime\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/overlay\"\n\t\"github.com/slackhq/nebula/udp\"\n)\n\nconst mtu = 9001\n\ntype InterfaceConfig struct {\n\tHostMap                 *HostMap\n\tOutside                 udp.Conn\n\tInside                  overlay.Device\n\tpki                     *PKI\n\tCipher                  string\n\tFirewall                *Firewall\n\tServeDns                bool\n\tHandshakeManager        *HandshakeManager\n\tlightHouse              *LightHouse\n\tcheckInterval           time.Duration\n\tpendingDeletionInterval time.Duration\n\tDropLocalBroadcast      bool\n\tDropMulticast           bool\n\troutines                int\n\tMessageMetrics          *MessageMetrics\n\tversion                 string\n\trelayManager            *relayManager\n\tpunchy                  *Punchy\n\n\ttryPromoteEvery uint32\n\treQueryEvery    uint32\n\treQueryWait     time.Duration\n\n\tConntrackCacheTimeout time.Duration\n\tl                     *logrus.Logger\n}\n\ntype Interface struct {\n\thostMap            *HostMap\n\toutside            udp.Conn\n\tinside             overlay.Device\n\tpki                *PKI\n\tcipher             string\n\tfirewall           *Firewall\n\tconnectionManager  *connectionManager\n\thandshakeManager   *HandshakeManager\n\tserveDns           bool\n\tcreateTime         time.Time\n\tlightHouse         *LightHouse\n\tmyBroadcastAddr    netip.Addr\n\tmyVpnNet           netip.Prefix\n\tdropLocalBroadcast bool\n\tdropMulticast      bool\n\troutines           int\n\tdisconnectInvalid  atomic.Bool\n\tclosed             atomic.Bool\n\trelayManager       *relayManager\n\n\ttryPromoteEvery atomic.Uint32\n\treQueryEvery    atomic.Uint32\n\treQueryWait     atomic.Int64\n\n\tsendRecvErrorConfig sendRecvErrorConfig\n\n\t// rebindCount is used to decide if an active tunnel should trigger a punch notification through a lighthouse\n\trebindCount int8\n\tversion     string\n\n\tconntrackCacheTimeout time.Duration\n\n\twriters []udp.Conn\n\treaders []io.ReadWriteCloser\n\n\tmetricHandshakes    metrics.Histogram\n\tmessageMetrics      *MessageMetrics\n\tcachedPacketMetrics *cachedPacketMetrics\n\n\tl *logrus.Logger\n}\n\ntype EncWriter interface {\n\tSendVia(via *HostInfo,\n\t\trelay *Relay,\n\t\tad,\n\t\tnb,\n\t\tout []byte,\n\t\tnocopy bool,\n\t)\n\tSendMessageToVpnIp(t header.MessageType, st header.MessageSubType, vpnIp netip.Addr, p, nb, out []byte)\n\tSendMessageToHostInfo(t header.MessageType, st header.MessageSubType, hostinfo *HostInfo, p, nb, out []byte)\n\tHandshake(vpnIp netip.Addr)\n}\n\ntype sendRecvErrorConfig uint8\n\nconst (\n\tsendRecvErrorAlways sendRecvErrorConfig = iota\n\tsendRecvErrorNever\n\tsendRecvErrorPrivate\n)\n\nfunc (s sendRecvErrorConfig) ShouldSendRecvError(ip netip.AddrPort) bool {\n\tswitch s {\n\tcase sendRecvErrorPrivate:\n\t\treturn ip.Addr().IsPrivate()\n\tcase sendRecvErrorAlways:\n\t\treturn true\n\tcase sendRecvErrorNever:\n\t\treturn false\n\tdefault:\n\t\tpanic(fmt.Errorf(\"invalid sendRecvErrorConfig value: %d\", s))\n\t}\n}\n\nfunc (s sendRecvErrorConfig) String() string {\n\tswitch s {\n\tcase sendRecvErrorAlways:\n\t\treturn \"always\"\n\tcase sendRecvErrorNever:\n\t\treturn \"never\"\n\tcase sendRecvErrorPrivate:\n\t\treturn \"private\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"invalid(%d)\", s)\n\t}\n}\n\nfunc NewInterface(ctx context.Context, c *InterfaceConfig) (*Interface, error) {\n\tif c.Outside == nil {\n\t\treturn nil, errors.New(\"no outside connection\")\n\t}\n\tif c.Inside == nil {\n\t\treturn nil, errors.New(\"no inside interface (tun)\")\n\t}\n\tif c.pki == nil {\n\t\treturn nil, errors.New(\"no certificate state\")\n\t}\n\tif c.Firewall == nil {\n\t\treturn nil, errors.New(\"no firewall rules\")\n\t}\n\n\tcertificate := c.pki.GetCertState().Certificate\n\n\tifce := &Interface{\n\t\tpki:                c.pki,\n\t\thostMap:            c.HostMap,\n\t\toutside:            c.Outside,\n\t\tinside:             c.Inside,\n\t\tcipher:             c.Cipher,\n\t\tfirewall:           c.Firewall,\n\t\tserveDns:           c.ServeDns,\n\t\thandshakeManager:   c.HandshakeManager,\n\t\tcreateTime:         time.Now(),\n\t\tlightHouse:         c.lightHouse,\n\t\tdropLocalBroadcast: c.DropLocalBroadcast,\n\t\tdropMulticast:      c.DropMulticast,\n\t\troutines:           c.routines,\n\t\tversion:            c.version,\n\t\twriters:            make([]udp.Conn, c.routines),\n\t\treaders:            make([]io.ReadWriteCloser, c.routines),\n\t\tmyVpnNet:           certificate.Networks()[0],\n\t\trelayManager:       c.relayManager,\n\n\t\tconntrackCacheTimeout: c.ConntrackCacheTimeout,\n\n\t\tmetricHandshakes: metrics.GetOrRegisterHistogram(\"handshakes\", nil, metrics.NewExpDecaySample(1028, 0.015)),\n\t\tmessageMetrics:   c.MessageMetrics,\n\t\tcachedPacketMetrics: &cachedPacketMetrics{\n\t\t\tsent:    metrics.GetOrRegisterCounter(\"hostinfo.cached_packets.sent\", nil),\n\t\t\tdropped: metrics.GetOrRegisterCounter(\"hostinfo.cached_packets.dropped\", nil),\n\t\t},\n\n\t\tl: c.l,\n\t}\n\n\tif ifce.myVpnNet.Addr().Is4() {\n\t\tmaskedAddr := certificate.Networks()[0].Masked()\n\t\taddr := maskedAddr.Addr().As4()\n\t\tmask := net.CIDRMask(maskedAddr.Bits(), maskedAddr.Addr().BitLen())\n\t\tbinary.BigEndian.PutUint32(addr[:], binary.BigEndian.Uint32(addr[:])|^binary.BigEndian.Uint32(mask))\n\t\tifce.myBroadcastAddr = netip.AddrFrom4(addr)\n\t}\n\n\tifce.tryPromoteEvery.Store(c.tryPromoteEvery)\n\tifce.reQueryEvery.Store(c.reQueryEvery)\n\tifce.reQueryWait.Store(int64(c.reQueryWait))\n\n\tifce.connectionManager = newConnectionManager(ctx, c.l, ifce, c.checkInterval, c.pendingDeletionInterval, c.punchy)\n\n\treturn ifce, nil\n}\n\n// activate creates the interface on the host. After the interface is created, any\n// other services that want to bind listeners to its IP may do so successfully. However,\n// the interface isn't going to process anything until run() is called.\nfunc (f *Interface) activate() {\n\t// actually turn on tun dev\n\n\taddr, err := f.outside.LocalAddr()\n\tif err != nil {\n\t\tf.l.WithError(err).Error(\"Failed to get udp listen address\")\n\t}\n\n\tf.l.WithField(\"interface\", f.inside.Name()).WithField(\"network\", f.inside.Cidr().String()).\n\t\tWithField(\"build\", f.version).WithField(\"udpAddr\", addr).\n\t\tWithField(\"boringcrypto\", boringEnabled()).\n\t\tInfo(\"Nebula interface is active\")\n\n\tmetrics.GetOrRegisterGauge(\"routines\", nil).Update(int64(f.routines))\n\n\t// Prepare n tun queues\n\tvar reader io.ReadWriteCloser = f.inside\n\tfor i := 0; i < f.routines; i++ {\n\t\tif i > 0 {\n\t\t\treader, err = f.inside.NewMultiQueueReader()\n\t\t\tif err != nil {\n\t\t\t\tf.l.Fatal(err)\n\t\t\t}\n\t\t}\n\t\tf.readers[i] = reader\n\t}\n\n\tif err := f.inside.Activate(); err != nil {\n\t\tf.inside.Close()\n\t\tf.l.Fatal(err)\n\t}\n}\n\nfunc (f *Interface) run() {\n\t// Launch n queues to read packets from udp\n\tfor i := 0; i < f.routines; i++ {\n\t\tgo f.listenOut(i)\n\t}\n\n\t// Launch n queues to read packets from tun dev\n\tfor i := 0; i < f.routines; i++ {\n\t\tgo f.listenIn(f.readers[i], i)\n\t}\n}\n\nfunc (f *Interface) listenOut(i int) {\n\truntime.LockOSThread()\n\n\tvar li udp.Conn\n\t// TODO clean this up with a coherent interface for each outside connection\n\tif i > 0 {\n\t\tli = f.writers[i]\n\t} else {\n\t\tli = f.outside\n\t}\n\n\tlhh := f.lightHouse.NewRequestHandler()\n\tconntrackCache := firewall.NewConntrackCacheTicker(f.conntrackCacheTimeout)\n\tli.ListenOut(readOutsidePackets(f), lhHandleRequest(lhh, f), conntrackCache, i)\n}\n\nfunc (f *Interface) listenIn(reader io.ReadWriteCloser, i int) {\n\truntime.LockOSThread()\n\n\tpacket := make([]byte, mtu)\n\tout := make([]byte, mtu)\n\tfwPacket := &firewall.Packet{}\n\tnb := make([]byte, 12, 12)\n\n\tconntrackCache := firewall.NewConntrackCacheTicker(f.conntrackCacheTimeout)\n\n\tfor {\n\t\tn, err := reader.Read(packet)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, os.ErrClosed) && f.closed.Load() {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tf.l.WithError(err).Error(\"Error while reading outbound packet\")\n\t\t\t// This only seems to happen when something fatal happens to the fd, so exit.\n\t\t\tos.Exit(2)\n\t\t}\n\n\t\tf.consumeInsidePacket(packet[:n], fwPacket, nb, out, i, conntrackCache.Get(f.l))\n\t}\n}\n\nfunc (f *Interface) RegisterConfigChangeCallbacks(c *config.C) {\n\tc.RegisterReloadCallback(f.reloadFirewall)\n\tc.RegisterReloadCallback(f.reloadSendRecvError)\n\tc.RegisterReloadCallback(f.reloadDisconnectInvalid)\n\tc.RegisterReloadCallback(f.reloadMisc)\n\n\tfor _, udpConn := range f.writers {\n\t\tc.RegisterReloadCallback(udpConn.ReloadConfig)\n\t}\n}\n\nfunc (f *Interface) reloadDisconnectInvalid(c *config.C) {\n\tinitial := c.InitialLoad()\n\tif initial || c.HasChanged(\"pki.disconnect_invalid\") {\n\t\tf.disconnectInvalid.Store(c.GetBool(\"pki.disconnect_invalid\", true))\n\t\tif !initial {\n\t\t\tf.l.Infof(\"pki.disconnect_invalid changed to %v\", f.disconnectInvalid.Load())\n\t\t}\n\t}\n}\n\nfunc (f *Interface) reloadFirewall(c *config.C) {\n\t//TODO: need to trigger/detect if the certificate changed too\n\tif c.HasChanged(\"firewall\") == false {\n\t\tf.l.Debug(\"No firewall config change detected\")\n\t\treturn\n\t}\n\n\tfw, err := NewFirewallFromConfig(f.l, f.pki.GetCertState().Certificate, c)\n\tif err != nil {\n\t\tf.l.WithError(err).Error(\"Error while creating firewall during reload\")\n\t\treturn\n\t}\n\n\toldFw := f.firewall\n\tconntrack := oldFw.Conntrack\n\tconntrack.Lock()\n\tdefer conntrack.Unlock()\n\n\tfw.rulesVersion = oldFw.rulesVersion + 1\n\t// If rulesVersion is back to zero, we have wrapped all the way around. Be\n\t// safe and just reset conntrack in this case.\n\tif fw.rulesVersion == 0 {\n\t\tf.l.WithField(\"firewallHashes\", fw.GetRuleHashes()).\n\t\t\tWithField(\"oldFirewallHashes\", oldFw.GetRuleHashes()).\n\t\t\tWithField(\"rulesVersion\", fw.rulesVersion).\n\t\t\tWarn(\"firewall rulesVersion has overflowed, resetting conntrack\")\n\t} else {\n\t\tfw.Conntrack = conntrack\n\t}\n\n\tf.firewall = fw\n\n\toldFw.Destroy()\n\tf.l.WithField(\"firewallHashes\", fw.GetRuleHashes()).\n\t\tWithField(\"oldFirewallHashes\", oldFw.GetRuleHashes()).\n\t\tWithField(\"rulesVersion\", fw.rulesVersion).\n\t\tInfo(\"New firewall has been installed\")\n}\n\nfunc (f *Interface) reloadSendRecvError(c *config.C) {\n\tif c.InitialLoad() || c.HasChanged(\"listen.send_recv_error\") {\n\t\tstringValue := c.GetString(\"listen.send_recv_error\", \"always\")\n\n\t\tswitch stringValue {\n\t\tcase \"always\":\n\t\t\tf.sendRecvErrorConfig = sendRecvErrorAlways\n\t\tcase \"never\":\n\t\t\tf.sendRecvErrorConfig = sendRecvErrorNever\n\t\tcase \"private\":\n\t\t\tf.sendRecvErrorConfig = sendRecvErrorPrivate\n\t\tdefault:\n\t\t\tif c.GetBool(\"listen.send_recv_error\", true) {\n\t\t\t\tf.sendRecvErrorConfig = sendRecvErrorAlways\n\t\t\t} else {\n\t\t\t\tf.sendRecvErrorConfig = sendRecvErrorNever\n\t\t\t}\n\t\t}\n\n\t\tf.l.WithField(\"sendRecvError\", f.sendRecvErrorConfig.String()).\n\t\t\tInfo(\"Loaded send_recv_error config\")\n\t}\n}\n\nfunc (f *Interface) reloadMisc(c *config.C) {\n\tif c.HasChanged(\"counters.try_promote\") {\n\t\tn := c.GetUint32(\"counters.try_promote\", defaultPromoteEvery)\n\t\tf.tryPromoteEvery.Store(n)\n\t\tf.l.Info(\"counters.try_promote has changed\")\n\t}\n\n\tif c.HasChanged(\"counters.requery_every_packets\") {\n\t\tn := c.GetUint32(\"counters.requery_every_packets\", defaultReQueryEvery)\n\t\tf.reQueryEvery.Store(n)\n\t\tf.l.Info(\"counters.requery_every_packets has changed\")\n\t}\n\n\tif c.HasChanged(\"timers.requery_wait_duration\") {\n\t\tn := c.GetDuration(\"timers.requery_wait_duration\", defaultReQueryWait)\n\t\tf.reQueryWait.Store(int64(n))\n\t\tf.l.Info(\"timers.requery_wait_duration has changed\")\n\t}\n}\n\nfunc (f *Interface) emitStats(ctx context.Context, i time.Duration) {\n\tticker := time.NewTicker(i)\n\tdefer ticker.Stop()\n\n\tudpStats := udp.NewUDPStatsEmitter(f.writers)\n\n\tcertExpirationGauge := metrics.GetOrRegisterGauge(\"certificate.ttl_seconds\", nil)\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tf.firewall.EmitStats()\n\t\t\tf.handshakeManager.EmitStats()\n\t\t\tudpStats()\n\t\t\tcertExpirationGauge.Update(int64(f.pki.GetCertState().Certificate.NotAfter().Sub(time.Now()) / time.Second))\n\t\t}\n\t}\n}\n\nfunc (f *Interface) Close() error {\n\tf.closed.Store(true)\n\n\tfor _, u := range f.writers {\n\t\terr := u.Close()\n\t\tif err != nil {\n\t\t\tf.l.WithError(err).Error(\"Error while closing udp socket\")\n\t\t}\n\t}\n\n\t// Release the tun device\n\treturn f.inside.Close()\n}\n"
        },
        {
          "name": "iputil",
          "type": "tree",
          "content": null
        },
        {
          "name": "lighthouse.go",
          "type": "blob",
          "size": 32.669921875,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/netip\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/gaissmai/bart\"\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/udp\"\n\t\"github.com/slackhq/nebula/util\"\n)\n\n//TODO: if a lighthouse doesn't have an answer, clients AGGRESSIVELY REQUERY.. why? handshake manager and/or getOrHandshake?\n//TODO: nodes are roaming lighthouses, this is bad. How are they learning?\n\nvar ErrHostNotKnown = errors.New(\"host not known\")\n\ntype LightHouse struct {\n\t//TODO: We need a timer wheel to kick out vpnIps that haven't reported in a long time\n\tsync.RWMutex //Because we concurrently read and write to our maps\n\tctx          context.Context\n\tamLighthouse bool\n\tmyVpnNet     netip.Prefix\n\tpunchConn    udp.Conn\n\tpunchy       *Punchy\n\n\t// Local cache of answers from light houses\n\t// map of vpn Ip to answers\n\taddrMap map[netip.Addr]*RemoteList\n\n\t// filters remote addresses allowed for each host\n\t// - When we are a lighthouse, this filters what addresses we store and\n\t// respond with.\n\t// - When we are not a lighthouse, this filters which addresses we accept\n\t// from lighthouses.\n\tremoteAllowList atomic.Pointer[RemoteAllowList]\n\n\t// filters local addresses that we advertise to lighthouses\n\tlocalAllowList atomic.Pointer[LocalAllowList]\n\n\t// used to trigger the HandshakeManager when we receive HostQueryReply\n\thandshakeTrigger chan<- netip.Addr\n\n\t// staticList exists to avoid having a bool in each addrMap entry\n\t// since static should be rare\n\tstaticList  atomic.Pointer[map[netip.Addr]struct{}]\n\tlighthouses atomic.Pointer[map[netip.Addr]struct{}]\n\n\tinterval     atomic.Int64\n\tupdateCancel context.CancelFunc\n\tifce         EncWriter\n\tnebulaPort   uint32 // 32 bits because protobuf does not have a uint16\n\n\tadvertiseAddrs atomic.Pointer[[]netip.AddrPort]\n\n\t// IP's of relays that can be used by peers to access me\n\trelaysForMe atomic.Pointer[[]netip.Addr]\n\n\tqueryChan chan netip.Addr\n\n\tcalculatedRemotes atomic.Pointer[bart.Table[[]*calculatedRemote]] // Maps VpnIp to []*calculatedRemote\n\n\tmetrics           *MessageMetrics\n\tmetricHolepunchTx metrics.Counter\n\tl                 *logrus.Logger\n}\n\n// NewLightHouseFromConfig will build a Lighthouse struct from the values provided in the config object\n// addrMap should be nil unless this is during a config reload\nfunc NewLightHouseFromConfig(ctx context.Context, l *logrus.Logger, c *config.C, myVpnNet netip.Prefix, pc udp.Conn, p *Punchy) (*LightHouse, error) {\n\tamLighthouse := c.GetBool(\"lighthouse.am_lighthouse\", false)\n\tnebulaPort := uint32(c.GetInt(\"listen.port\", 0))\n\tif amLighthouse && nebulaPort == 0 {\n\t\treturn nil, util.NewContextualError(\"lighthouse.am_lighthouse enabled on node but no port number is set in config\", nil, nil)\n\t}\n\n\t// If port is dynamic, discover it\n\tif nebulaPort == 0 && pc != nil {\n\t\tuPort, err := pc.LocalAddr()\n\t\tif err != nil {\n\t\t\treturn nil, util.NewContextualError(\"Failed to get listening port\", nil, err)\n\t\t}\n\t\tnebulaPort = uint32(uPort.Port())\n\t}\n\n\th := LightHouse{\n\t\tctx:          ctx,\n\t\tamLighthouse: amLighthouse,\n\t\tmyVpnNet:     myVpnNet,\n\t\taddrMap:      make(map[netip.Addr]*RemoteList),\n\t\tnebulaPort:   nebulaPort,\n\t\tpunchConn:    pc,\n\t\tpunchy:       p,\n\t\tqueryChan:    make(chan netip.Addr, c.GetUint32(\"handshakes.query_buffer\", 64)),\n\t\tl:            l,\n\t}\n\tlighthouses := make(map[netip.Addr]struct{})\n\th.lighthouses.Store(&lighthouses)\n\tstaticList := make(map[netip.Addr]struct{})\n\th.staticList.Store(&staticList)\n\n\tif c.GetBool(\"stats.lighthouse_metrics\", false) {\n\t\th.metrics = newLighthouseMetrics()\n\t\th.metricHolepunchTx = metrics.GetOrRegisterCounter(\"messages.tx.holepunch\", nil)\n\t} else {\n\t\th.metricHolepunchTx = metrics.NilCounter{}\n\t}\n\n\terr := h.reload(c, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\terr := h.reload(c, false)\n\t\tswitch v := err.(type) {\n\t\tcase *util.ContextualError:\n\t\t\tv.Log(l)\n\t\tcase error:\n\t\t\tl.WithError(err).Error(\"failed to reload lighthouse\")\n\t\t}\n\t})\n\n\th.startQueryWorker()\n\n\treturn &h, nil\n}\n\nfunc (lh *LightHouse) GetStaticHostList() map[netip.Addr]struct{} {\n\treturn *lh.staticList.Load()\n}\n\nfunc (lh *LightHouse) GetLighthouses() map[netip.Addr]struct{} {\n\treturn *lh.lighthouses.Load()\n}\n\nfunc (lh *LightHouse) GetRemoteAllowList() *RemoteAllowList {\n\treturn lh.remoteAllowList.Load()\n}\n\nfunc (lh *LightHouse) GetLocalAllowList() *LocalAllowList {\n\treturn lh.localAllowList.Load()\n}\n\nfunc (lh *LightHouse) GetAdvertiseAddrs() []netip.AddrPort {\n\treturn *lh.advertiseAddrs.Load()\n}\n\nfunc (lh *LightHouse) GetRelaysForMe() []netip.Addr {\n\treturn *lh.relaysForMe.Load()\n}\n\nfunc (lh *LightHouse) getCalculatedRemotes() *bart.Table[[]*calculatedRemote] {\n\treturn lh.calculatedRemotes.Load()\n}\n\nfunc (lh *LightHouse) GetUpdateInterval() int64 {\n\treturn lh.interval.Load()\n}\n\nfunc (lh *LightHouse) reload(c *config.C, initial bool) error {\n\tif initial || c.HasChanged(\"lighthouse.advertise_addrs\") {\n\t\trawAdvAddrs := c.GetStringSlice(\"lighthouse.advertise_addrs\", []string{})\n\t\tadvAddrs := make([]netip.AddrPort, 0)\n\n\t\tfor i, rawAddr := range rawAdvAddrs {\n\t\t\thost, sport, err := net.SplitHostPort(rawAddr)\n\t\t\tif err != nil {\n\t\t\t\treturn util.NewContextualError(\"Unable to parse lighthouse.advertise_addrs entry\", m{\"addr\": rawAddr, \"entry\": i + 1}, err)\n\t\t\t}\n\n\t\t\tips, err := net.DefaultResolver.LookupNetIP(context.Background(), \"ip\", host)\n\t\t\tif err != nil {\n\t\t\t\treturn util.NewContextualError(\"Unable to lookup lighthouse.advertise_addrs entry\", m{\"addr\": rawAddr, \"entry\": i + 1}, err)\n\t\t\t}\n\t\t\tif len(ips) == 0 {\n\t\t\t\treturn util.NewContextualError(\"Unable to lookup lighthouse.advertise_addrs entry\", m{\"addr\": rawAddr, \"entry\": i + 1}, nil)\n\t\t\t}\n\n\t\t\tport, err := strconv.Atoi(sport)\n\t\t\tif err != nil {\n\t\t\t\treturn util.NewContextualError(\"Unable to parse port in lighthouse.advertise_addrs entry\", m{\"addr\": rawAddr, \"entry\": i + 1}, err)\n\t\t\t}\n\n\t\t\tif port == 0 {\n\t\t\t\tport = int(lh.nebulaPort)\n\t\t\t}\n\n\t\t\t//TODO: we could technically insert all returned ips instead of just the first one if a dns lookup was used\n\t\t\tip := ips[0].Unmap()\n\t\t\tif lh.myVpnNet.Contains(ip) {\n\t\t\t\tlh.l.WithField(\"addr\", rawAddr).WithField(\"entry\", i+1).\n\t\t\t\t\tWarn(\"Ignoring lighthouse.advertise_addrs report because it is within the nebula network range\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tadvAddrs = append(advAddrs, netip.AddrPortFrom(ip, uint16(port)))\n\t\t}\n\n\t\tlh.advertiseAddrs.Store(&advAddrs)\n\n\t\tif !initial {\n\t\t\tlh.l.Info(\"lighthouse.advertise_addrs has changed\")\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"lighthouse.interval\") {\n\t\tlh.interval.Store(int64(c.GetInt(\"lighthouse.interval\", 10)))\n\n\t\tif !initial {\n\t\t\tlh.l.Infof(\"lighthouse.interval changed to %v\", lh.interval.Load())\n\n\t\t\tif lh.updateCancel != nil {\n\t\t\t\t// May not always have a running routine\n\t\t\t\tlh.updateCancel()\n\t\t\t}\n\n\t\t\tlh.StartUpdateWorker()\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"lighthouse.remote_allow_list\") || c.HasChanged(\"lighthouse.remote_allow_ranges\") {\n\t\tral, err := NewRemoteAllowListFromConfig(c, \"lighthouse.remote_allow_list\", \"lighthouse.remote_allow_ranges\")\n\t\tif err != nil {\n\t\t\treturn util.NewContextualError(\"Invalid lighthouse.remote_allow_list\", nil, err)\n\t\t}\n\n\t\tlh.remoteAllowList.Store(ral)\n\t\tif !initial {\n\t\t\t//TODO: a diff will be annoyingly difficult\n\t\t\tlh.l.Info(\"lighthouse.remote_allow_list and/or lighthouse.remote_allow_ranges has changed\")\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"lighthouse.local_allow_list\") {\n\t\tlal, err := NewLocalAllowListFromConfig(c, \"lighthouse.local_allow_list\")\n\t\tif err != nil {\n\t\t\treturn util.NewContextualError(\"Invalid lighthouse.local_allow_list\", nil, err)\n\t\t}\n\n\t\tlh.localAllowList.Store(lal)\n\t\tif !initial {\n\t\t\t//TODO: a diff will be annoyingly difficult\n\t\t\tlh.l.Info(\"lighthouse.local_allow_list has changed\")\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"lighthouse.calculated_remotes\") {\n\t\tcr, err := NewCalculatedRemotesFromConfig(c, \"lighthouse.calculated_remotes\")\n\t\tif err != nil {\n\t\t\treturn util.NewContextualError(\"Invalid lighthouse.calculated_remotes\", nil, err)\n\t\t}\n\n\t\tlh.calculatedRemotes.Store(cr)\n\t\tif !initial {\n\t\t\t//TODO: a diff will be annoyingly difficult\n\t\t\tlh.l.Info(\"lighthouse.calculated_remotes has changed\")\n\t\t}\n\t}\n\n\t//NOTE: many things will get much simpler when we combine static_host_map and lighthouse.hosts in config\n\tif initial || c.HasChanged(\"static_host_map\") || c.HasChanged(\"static_map.cadence\") || c.HasChanged(\"static_map.network\") || c.HasChanged(\"static_map.lookup_timeout\") {\n\t\t// Clean up. Entries still in the static_host_map will be re-built.\n\t\t// Entries no longer present must have their (possible) background DNS goroutines stopped.\n\t\tif existingStaticList := lh.staticList.Load(); existingStaticList != nil {\n\t\t\tlh.RLock()\n\t\t\tfor staticVpnIp := range *existingStaticList {\n\t\t\t\tif am, ok := lh.addrMap[staticVpnIp]; ok && am != nil {\n\t\t\t\t\tam.hr.Cancel()\n\t\t\t\t}\n\t\t\t}\n\t\t\tlh.RUnlock()\n\t\t}\n\t\t// Build a new list based on current config.\n\t\tstaticList := make(map[netip.Addr]struct{})\n\t\terr := lh.loadStaticMap(c, staticList)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlh.staticList.Store(&staticList)\n\t\tif !initial {\n\t\t\t//TODO: we should remove any remote list entries for static hosts that were removed/modified?\n\t\t\tif c.HasChanged(\"static_host_map\") {\n\t\t\t\tlh.l.Info(\"static_host_map has changed\")\n\t\t\t}\n\t\t\tif c.HasChanged(\"static_map.cadence\") {\n\t\t\t\tlh.l.Info(\"static_map.cadence has changed\")\n\t\t\t}\n\t\t\tif c.HasChanged(\"static_map.network\") {\n\t\t\t\tlh.l.Info(\"static_map.network has changed\")\n\t\t\t}\n\t\t\tif c.HasChanged(\"static_map.lookup_timeout\") {\n\t\t\t\tlh.l.Info(\"static_map.lookup_timeout has changed\")\n\t\t\t}\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"lighthouse.hosts\") {\n\t\tlhMap := make(map[netip.Addr]struct{})\n\t\terr := lh.parseLighthouses(c, lhMap)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlh.lighthouses.Store(&lhMap)\n\t\tif !initial {\n\t\t\t//NOTE: we are not tearing down existing lighthouse connections because they might be used for non lighthouse traffic\n\t\t\tlh.l.Info(\"lighthouse.hosts has changed\")\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"relay.relays\") {\n\t\tswitch c.GetBool(\"relay.am_relay\", false) {\n\t\tcase true:\n\t\t\t// Relays aren't allowed to specify other relays\n\t\t\tif len(c.GetStringSlice(\"relay.relays\", nil)) > 0 {\n\t\t\t\tlh.l.Info(\"Ignoring relays from config because am_relay is true\")\n\t\t\t}\n\t\t\trelaysForMe := []netip.Addr{}\n\t\t\tlh.relaysForMe.Store(&relaysForMe)\n\t\tcase false:\n\t\t\trelaysForMe := []netip.Addr{}\n\t\t\tfor _, v := range c.GetStringSlice(\"relay.relays\", nil) {\n\t\t\t\tlh.l.WithField(\"relay\", v).Info(\"Read relay from config\")\n\n\t\t\t\tconfigRIP, err := netip.ParseAddr(v)\n\t\t\t\t//TODO: We could print the error here\n\t\t\t\tif err == nil {\n\t\t\t\t\trelaysForMe = append(relaysForMe, configRIP)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlh.relaysForMe.Store(&relaysForMe)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (lh *LightHouse) parseLighthouses(c *config.C, lhMap map[netip.Addr]struct{}) error {\n\tlhs := c.GetStringSlice(\"lighthouse.hosts\", []string{})\n\tif lh.amLighthouse && len(lhs) != 0 {\n\t\tlh.l.Warn(\"lighthouse.am_lighthouse enabled on node but upstream lighthouses exist in config\")\n\t}\n\n\tfor i, host := range lhs {\n\t\tip, err := netip.ParseAddr(host)\n\t\tif err != nil {\n\t\t\treturn util.NewContextualError(\"Unable to parse lighthouse host entry\", m{\"host\": host, \"entry\": i + 1}, err)\n\t\t}\n\t\tif !lh.myVpnNet.Contains(ip) {\n\t\t\treturn util.NewContextualError(\"lighthouse host is not in our subnet, invalid\", m{\"vpnIp\": ip, \"network\": lh.myVpnNet}, nil)\n\t\t}\n\t\tlhMap[ip] = struct{}{}\n\t}\n\n\tif !lh.amLighthouse && len(lhMap) == 0 {\n\t\tlh.l.Warn(\"No lighthouse.hosts configured, this host will only be able to initiate tunnels with static_host_map entries\")\n\t}\n\n\tstaticList := lh.GetStaticHostList()\n\tfor lhIP, _ := range lhMap {\n\t\tif _, ok := staticList[lhIP]; !ok {\n\t\t\treturn fmt.Errorf(\"lighthouse %s does not have a static_host_map entry\", lhIP)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc getStaticMapCadence(c *config.C) (time.Duration, error) {\n\tcadence := c.GetString(\"static_map.cadence\", \"30s\")\n\td, err := time.ParseDuration(cadence)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn d, nil\n}\n\nfunc getStaticMapLookupTimeout(c *config.C) (time.Duration, error) {\n\tlookupTimeout := c.GetString(\"static_map.lookup_timeout\", \"250ms\")\n\td, err := time.ParseDuration(lookupTimeout)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn d, nil\n}\n\nfunc getStaticMapNetwork(c *config.C) (string, error) {\n\tnetwork := c.GetString(\"static_map.network\", \"ip4\")\n\tif network != \"ip\" && network != \"ip4\" && network != \"ip6\" {\n\t\treturn \"\", fmt.Errorf(\"static_map.network must be one of ip, ip4, or ip6\")\n\t}\n\treturn network, nil\n}\n\nfunc (lh *LightHouse) loadStaticMap(c *config.C, staticList map[netip.Addr]struct{}) error {\n\td, err := getStaticMapCadence(c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tnetwork, err := getStaticMapNetwork(c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlookupTimeout, err := getStaticMapLookupTimeout(c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tshm := c.GetMap(\"static_host_map\", map[interface{}]interface{}{})\n\ti := 0\n\n\tfor k, v := range shm {\n\t\tvpnIp, err := netip.ParseAddr(fmt.Sprintf(\"%v\", k))\n\t\tif err != nil {\n\t\t\treturn util.NewContextualError(\"Unable to parse static_host_map entry\", m{\"host\": k, \"entry\": i + 1}, err)\n\t\t}\n\n\t\tif !lh.myVpnNet.Contains(vpnIp) {\n\t\t\treturn util.NewContextualError(\"static_host_map key is not in our subnet, invalid\", m{\"vpnIp\": vpnIp, \"network\": lh.myVpnNet, \"entry\": i + 1}, nil)\n\t\t}\n\n\t\tvals, ok := v.([]interface{})\n\t\tif !ok {\n\t\t\tvals = []interface{}{v}\n\t\t}\n\t\tremoteAddrs := []string{}\n\t\tfor _, v := range vals {\n\t\t\tremoteAddrs = append(remoteAddrs, fmt.Sprintf(\"%v\", v))\n\t\t}\n\n\t\terr = lh.addStaticRemotes(i, d, network, lookupTimeout, vpnIp, remoteAddrs, staticList)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ti++\n\t}\n\n\treturn nil\n}\n\nfunc (lh *LightHouse) Query(ip netip.Addr) *RemoteList {\n\tif !lh.IsLighthouseIP(ip) {\n\t\tlh.QueryServer(ip)\n\t}\n\tlh.RLock()\n\tif v, ok := lh.addrMap[ip]; ok {\n\t\tlh.RUnlock()\n\t\treturn v\n\t}\n\tlh.RUnlock()\n\treturn nil\n}\n\n// QueryServer is asynchronous so no reply should be expected\nfunc (lh *LightHouse) QueryServer(ip netip.Addr) {\n\t// Don't put lighthouse ips in the query channel because we can't query lighthouses about lighthouses\n\tif lh.amLighthouse || lh.IsLighthouseIP(ip) {\n\t\treturn\n\t}\n\n\tlh.queryChan <- ip\n}\n\nfunc (lh *LightHouse) QueryCache(ip netip.Addr) *RemoteList {\n\tlh.RLock()\n\tif v, ok := lh.addrMap[ip]; ok {\n\t\tlh.RUnlock()\n\t\treturn v\n\t}\n\tlh.RUnlock()\n\n\tlh.Lock()\n\tdefer lh.Unlock()\n\t// Add an entry if we don't already have one\n\treturn lh.unlockedGetRemoteList(ip)\n}\n\n// queryAndPrepMessage is a lock helper on RemoteList, assisting the caller to build a lighthouse message containing\n// details from the remote list. It looks for a hit in the addrMap and a hit in the RemoteList under the owner vpnIp\n// If one is found then f() is called with proper locking, f() must return result of n.MarshalTo()\nfunc (lh *LightHouse) queryAndPrepMessage(vpnIp netip.Addr, f func(*cache) (int, error)) (bool, int, error) {\n\tlh.RLock()\n\t// Do we have an entry in the main cache?\n\tif v, ok := lh.addrMap[vpnIp]; ok {\n\t\t// Swap lh lock for remote list lock\n\t\tv.RLock()\n\t\tdefer v.RUnlock()\n\n\t\tlh.RUnlock()\n\n\t\t// vpnIp should also be the owner here since we are a lighthouse.\n\t\tc := v.cache[vpnIp]\n\t\t// Make sure we have\n\t\tif c != nil {\n\t\t\tn, err := f(c)\n\t\t\treturn true, n, err\n\t\t}\n\t\treturn false, 0, nil\n\t}\n\tlh.RUnlock()\n\treturn false, 0, nil\n}\n\nfunc (lh *LightHouse) DeleteVpnIp(vpnIp netip.Addr) {\n\t// First we check the static mapping\n\t// and do nothing if it is there\n\tif _, ok := lh.GetStaticHostList()[vpnIp]; ok {\n\t\treturn\n\t}\n\tlh.Lock()\n\t//l.Debugln(lh.addrMap)\n\tdelete(lh.addrMap, vpnIp)\n\n\tif lh.l.Level >= logrus.DebugLevel {\n\t\tlh.l.Debugf(\"deleting %s from lighthouse.\", vpnIp)\n\t}\n\n\tlh.Unlock()\n}\n\n// AddStaticRemote adds a static host entry for vpnIp as ourselves as the owner\n// We are the owner because we don't want a lighthouse server to advertise for static hosts it was configured with\n// And we don't want a lighthouse query reply to interfere with our learned cache if we are a client\n// NOTE: this function should not interact with any hot path objects, like lh.staticList, the caller should handle it\nfunc (lh *LightHouse) addStaticRemotes(i int, d time.Duration, network string, timeout time.Duration, vpnIp netip.Addr, toAddrs []string, staticList map[netip.Addr]struct{}) error {\n\tlh.Lock()\n\tam := lh.unlockedGetRemoteList(vpnIp)\n\tam.Lock()\n\tdefer am.Unlock()\n\tctx := lh.ctx\n\tlh.Unlock()\n\n\thr, err := NewHostnameResults(ctx, lh.l, d, network, timeout, toAddrs, func() {\n\t\t// This callback runs whenever the DNS hostname resolver finds a different set of IP's\n\t\t// in its resolution for hostnames.\n\t\tam.Lock()\n\t\tdefer am.Unlock()\n\t\tam.shouldRebuild = true\n\t})\n\tif err != nil {\n\t\treturn util.NewContextualError(\"Static host address could not be parsed\", m{\"vpnIp\": vpnIp, \"entry\": i + 1}, err)\n\t}\n\tam.unlockedSetHostnamesResults(hr)\n\n\tfor _, addrPort := range hr.GetIPs() {\n\t\tif !lh.shouldAdd(vpnIp, addrPort.Addr()) {\n\t\t\tcontinue\n\t\t}\n\t\tswitch {\n\t\tcase addrPort.Addr().Is4():\n\t\t\tam.unlockedPrependV4(lh.myVpnNet.Addr(), NewIp4AndPortFromNetIP(addrPort.Addr(), addrPort.Port()))\n\t\tcase addrPort.Addr().Is6():\n\t\t\tam.unlockedPrependV6(lh.myVpnNet.Addr(), NewIp6AndPortFromNetIP(addrPort.Addr(), addrPort.Port()))\n\t\t}\n\t}\n\n\t// Mark it as static in the caller provided map\n\tstaticList[vpnIp] = struct{}{}\n\treturn nil\n}\n\n// addCalculatedRemotes adds any calculated remotes based on the\n// lighthouse.calculated_remotes configuration. It returns true if any\n// calculated remotes were added\nfunc (lh *LightHouse) addCalculatedRemotes(vpnIp netip.Addr) bool {\n\ttree := lh.getCalculatedRemotes()\n\tif tree == nil {\n\t\treturn false\n\t}\n\tcalculatedRemotes, ok := tree.Lookup(vpnIp)\n\tif !ok {\n\t\treturn false\n\t}\n\n\tvar calculated []*Ip4AndPort\n\tfor _, cr := range calculatedRemotes {\n\t\tc := cr.Apply(vpnIp)\n\t\tif c != nil {\n\t\t\tcalculated = append(calculated, c)\n\t\t}\n\t}\n\n\tlh.Lock()\n\tam := lh.unlockedGetRemoteList(vpnIp)\n\tam.Lock()\n\tdefer am.Unlock()\n\tlh.Unlock()\n\n\tam.unlockedSetV4(lh.myVpnNet.Addr(), vpnIp, calculated, lh.unlockedShouldAddV4)\n\n\treturn len(calculated) > 0\n}\n\n// unlockedGetRemoteList assumes you have the lh lock\nfunc (lh *LightHouse) unlockedGetRemoteList(vpnIp netip.Addr) *RemoteList {\n\tam, ok := lh.addrMap[vpnIp]\n\tif !ok {\n\t\tam = NewRemoteList(func(a netip.Addr) bool { return lh.shouldAdd(vpnIp, a) })\n\t\tlh.addrMap[vpnIp] = am\n\t}\n\treturn am\n}\n\nfunc (lh *LightHouse) shouldAdd(vpnIp netip.Addr, to netip.Addr) bool {\n\tallow := lh.GetRemoteAllowList().Allow(vpnIp, to)\n\tif lh.l.Level >= logrus.TraceLevel {\n\t\tlh.l.WithField(\"remoteIp\", vpnIp).WithField(\"allow\", allow).Trace(\"remoteAllowList.Allow\")\n\t}\n\tif !allow || lh.myVpnNet.Contains(to) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// unlockedShouldAddV4 checks if to is allowed by our allow list\nfunc (lh *LightHouse) unlockedShouldAddV4(vpnIp netip.Addr, to *Ip4AndPort) bool {\n\tip := AddrPortFromIp4AndPort(to)\n\tallow := lh.GetRemoteAllowList().Allow(vpnIp, ip.Addr())\n\tif lh.l.Level >= logrus.TraceLevel {\n\t\tlh.l.WithField(\"remoteIp\", vpnIp).WithField(\"allow\", allow).Trace(\"remoteAllowList.Allow\")\n\t}\n\n\tif !allow || lh.myVpnNet.Contains(ip.Addr()) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// unlockedShouldAddV6 checks if to is allowed by our allow list\nfunc (lh *LightHouse) unlockedShouldAddV6(vpnIp netip.Addr, to *Ip6AndPort) bool {\n\tip := AddrPortFromIp6AndPort(to)\n\tallow := lh.GetRemoteAllowList().Allow(vpnIp, ip.Addr())\n\tif lh.l.Level >= logrus.TraceLevel {\n\t\tlh.l.WithField(\"remoteIp\", lhIp6ToIp(to)).WithField(\"allow\", allow).Trace(\"remoteAllowList.Allow\")\n\t}\n\n\tif !allow || lh.myVpnNet.Contains(ip.Addr()) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc lhIp6ToIp(v *Ip6AndPort) net.IP {\n\tip := make(net.IP, 16)\n\tbinary.BigEndian.PutUint64(ip[:8], v.Hi)\n\tbinary.BigEndian.PutUint64(ip[8:], v.Lo)\n\treturn ip\n}\n\nfunc (lh *LightHouse) IsLighthouseIP(vpnIp netip.Addr) bool {\n\tif _, ok := lh.GetLighthouses()[vpnIp]; ok {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc NewLhQueryByInt(vpnIp netip.Addr) *NebulaMeta {\n\tif vpnIp.Is6() {\n\t\t//TODO: need to support ipv6\n\t\tpanic(\"ipv6 is not yet supported\")\n\t}\n\n\tb := vpnIp.As4()\n\treturn &NebulaMeta{\n\t\tType: NebulaMeta_HostQuery,\n\t\tDetails: &NebulaMetaDetails{\n\t\t\tVpnIp: binary.BigEndian.Uint32(b[:]),\n\t\t},\n\t}\n}\n\nfunc AddrPortFromIp4AndPort(ip *Ip4AndPort) netip.AddrPort {\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], ip.Ip)\n\treturn netip.AddrPortFrom(netip.AddrFrom4(b), uint16(ip.Port))\n}\n\nfunc AddrPortFromIp6AndPort(ip *Ip6AndPort) netip.AddrPort {\n\tb := [16]byte{}\n\tbinary.BigEndian.PutUint64(b[:8], ip.Hi)\n\tbinary.BigEndian.PutUint64(b[8:], ip.Lo)\n\treturn netip.AddrPortFrom(netip.AddrFrom16(b), uint16(ip.Port))\n}\n\nfunc NewIp4AndPortFromNetIP(ip netip.Addr, port uint16) *Ip4AndPort {\n\tv4Addr := ip.As4()\n\treturn &Ip4AndPort{\n\t\tIp:   binary.BigEndian.Uint32(v4Addr[:]),\n\t\tPort: uint32(port),\n\t}\n}\n\n// TODO: IPV6-WORK we can delete some more of these\nfunc NewIp6AndPortFromNetIP(ip netip.Addr, port uint16) *Ip6AndPort {\n\tip6Addr := ip.As16()\n\treturn &Ip6AndPort{\n\t\tHi:   binary.BigEndian.Uint64(ip6Addr[:8]),\n\t\tLo:   binary.BigEndian.Uint64(ip6Addr[8:]),\n\t\tPort: uint32(port),\n\t}\n}\n\nfunc (lh *LightHouse) startQueryWorker() {\n\tif lh.amLighthouse {\n\t\treturn\n\t}\n\n\tgo func() {\n\t\tnb := make([]byte, 12, 12)\n\t\tout := make([]byte, mtu)\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-lh.ctx.Done():\n\t\t\t\treturn\n\t\t\tcase ip := <-lh.queryChan:\n\t\t\t\tlh.innerQueryServer(ip, nb, out)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (lh *LightHouse) innerQueryServer(ip netip.Addr, nb, out []byte) {\n\tif lh.IsLighthouseIP(ip) {\n\t\treturn\n\t}\n\n\t// Send a query to the lighthouses and hope for the best next time\n\tquery, err := NewLhQueryByInt(ip).Marshal()\n\tif err != nil {\n\t\tlh.l.WithError(err).WithField(\"vpnIp\", ip).Error(\"Failed to marshal lighthouse query payload\")\n\t\treturn\n\t}\n\n\tlighthouses := lh.GetLighthouses()\n\tlh.metricTx(NebulaMeta_HostQuery, int64(len(lighthouses)))\n\n\tfor n := range lighthouses {\n\t\tlh.ifce.SendMessageToVpnIp(header.LightHouse, 0, n, query, nb, out)\n\t}\n}\n\nfunc (lh *LightHouse) StartUpdateWorker() {\n\tinterval := lh.GetUpdateInterval()\n\tif lh.amLighthouse || interval == 0 {\n\t\treturn\n\t}\n\n\tclockSource := time.NewTicker(time.Second * time.Duration(interval))\n\tupdateCtx, cancel := context.WithCancel(lh.ctx)\n\tlh.updateCancel = cancel\n\n\tgo func() {\n\t\tdefer clockSource.Stop()\n\n\t\tfor {\n\t\t\tlh.SendUpdate()\n\n\t\t\tselect {\n\t\t\tcase <-updateCtx.Done():\n\t\t\t\treturn\n\t\t\tcase <-clockSource.C:\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (lh *LightHouse) SendUpdate() {\n\tvar v4 []*Ip4AndPort\n\tvar v6 []*Ip6AndPort\n\n\tfor _, e := range lh.GetAdvertiseAddrs() {\n\t\tif e.Addr().Is4() {\n\t\t\tv4 = append(v4, NewIp4AndPortFromNetIP(e.Addr(), e.Port()))\n\t\t} else {\n\t\t\tv6 = append(v6, NewIp6AndPortFromNetIP(e.Addr(), e.Port()))\n\t\t}\n\t}\n\n\tlal := lh.GetLocalAllowList()\n\tfor _, e := range localIps(lh.l, lal) {\n\t\tif lh.myVpnNet.Contains(e) {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Only add IPs that aren't my VPN/tun IP\n\t\tif e.Is4() {\n\t\t\tv4 = append(v4, NewIp4AndPortFromNetIP(e, uint16(lh.nebulaPort)))\n\t\t} else {\n\t\t\tv6 = append(v6, NewIp6AndPortFromNetIP(e, uint16(lh.nebulaPort)))\n\t\t}\n\t}\n\n\tvar relays []uint32\n\tfor _, r := range lh.GetRelaysForMe() {\n\t\t//TODO: IPV6-WORK both relays and vpnip need ipv6 support\n\t\tb := r.As4()\n\t\trelays = append(relays, binary.BigEndian.Uint32(b[:]))\n\t}\n\n\t//TODO: IPV6-WORK both relays and vpnip need ipv6 support\n\tb := lh.myVpnNet.Addr().As4()\n\n\tm := &NebulaMeta{\n\t\tType: NebulaMeta_HostUpdateNotification,\n\t\tDetails: &NebulaMetaDetails{\n\t\t\tVpnIp:       binary.BigEndian.Uint32(b[:]),\n\t\t\tIp4AndPorts: v4,\n\t\t\tIp6AndPorts: v6,\n\t\t\tRelayVpnIp:  relays,\n\t\t},\n\t}\n\n\tlighthouses := lh.GetLighthouses()\n\tlh.metricTx(NebulaMeta_HostUpdateNotification, int64(len(lighthouses)))\n\tnb := make([]byte, 12, 12)\n\tout := make([]byte, mtu)\n\n\tmm, err := m.Marshal()\n\tif err != nil {\n\t\tlh.l.WithError(err).Error(\"Error while marshaling for lighthouse update\")\n\t\treturn\n\t}\n\n\tfor vpnIp := range lighthouses {\n\t\tlh.ifce.SendMessageToVpnIp(header.LightHouse, 0, vpnIp, mm, nb, out)\n\t}\n}\n\ntype LightHouseHandler struct {\n\tlh   *LightHouse\n\tnb   []byte\n\tout  []byte\n\tpb   []byte\n\tmeta *NebulaMeta\n\tl    *logrus.Logger\n}\n\nfunc (lh *LightHouse) NewRequestHandler() *LightHouseHandler {\n\tlhh := &LightHouseHandler{\n\t\tlh:  lh,\n\t\tnb:  make([]byte, 12, 12),\n\t\tout: make([]byte, mtu),\n\t\tl:   lh.l,\n\t\tpb:  make([]byte, mtu),\n\n\t\tmeta: &NebulaMeta{\n\t\t\tDetails: &NebulaMetaDetails{},\n\t\t},\n\t}\n\n\treturn lhh\n}\n\nfunc (lh *LightHouse) metricRx(t NebulaMeta_MessageType, i int64) {\n\tlh.metrics.Rx(header.MessageType(t), 0, i)\n}\n\nfunc (lh *LightHouse) metricTx(t NebulaMeta_MessageType, i int64) {\n\tlh.metrics.Tx(header.MessageType(t), 0, i)\n}\n\n// This method is similar to Reset(), but it re-uses the pointer structs\n// so that we don't have to re-allocate them\nfunc (lhh *LightHouseHandler) resetMeta() *NebulaMeta {\n\tdetails := lhh.meta.Details\n\tlhh.meta.Reset()\n\n\t// Keep the array memory around\n\tdetails.Ip4AndPorts = details.Ip4AndPorts[:0]\n\tdetails.Ip6AndPorts = details.Ip6AndPorts[:0]\n\tdetails.RelayVpnIp = details.RelayVpnIp[:0]\n\tlhh.meta.Details = details\n\n\treturn lhh.meta\n}\n\nfunc lhHandleRequest(lhh *LightHouseHandler, f *Interface) udp.LightHouseHandlerFunc {\n\treturn func(rAddr netip.AddrPort, vpnIp netip.Addr, p []byte) {\n\t\tlhh.HandleRequest(rAddr, vpnIp, p, f)\n\t}\n}\n\nfunc (lhh *LightHouseHandler) HandleRequest(rAddr netip.AddrPort, vpnIp netip.Addr, p []byte, w EncWriter) {\n\tn := lhh.resetMeta()\n\terr := n.Unmarshal(p)\n\tif err != nil {\n\t\tlhh.l.WithError(err).WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", rAddr).\n\t\t\tError(\"Failed to unmarshal lighthouse packet\")\n\t\t//TODO: send recv_error?\n\t\treturn\n\t}\n\n\tif n.Details == nil {\n\t\tlhh.l.WithField(\"vpnIp\", vpnIp).WithField(\"udpAddr\", rAddr).\n\t\t\tError(\"Invalid lighthouse update\")\n\t\t//TODO: send recv_error?\n\t\treturn\n\t}\n\n\tlhh.lh.metricRx(n.Type, 1)\n\n\tswitch n.Type {\n\tcase NebulaMeta_HostQuery:\n\t\tlhh.handleHostQuery(n, vpnIp, rAddr, w)\n\n\tcase NebulaMeta_HostQueryReply:\n\t\tlhh.handleHostQueryReply(n, vpnIp)\n\n\tcase NebulaMeta_HostUpdateNotification:\n\t\tlhh.handleHostUpdateNotification(n, vpnIp, w)\n\n\tcase NebulaMeta_HostMovedNotification:\n\tcase NebulaMeta_HostPunchNotification:\n\t\tlhh.handleHostPunchNotification(n, vpnIp, w)\n\n\tcase NebulaMeta_HostUpdateNotificationAck:\n\t\t// noop\n\t}\n}\n\nfunc (lhh *LightHouseHandler) handleHostQuery(n *NebulaMeta, vpnIp netip.Addr, addr netip.AddrPort, w EncWriter) {\n\t// Exit if we don't answer queries\n\tif !lhh.lh.amLighthouse {\n\t\tif lhh.l.Level >= logrus.DebugLevel {\n\t\t\tlhh.l.Debugln(\"I don't answer queries, but received from: \", addr)\n\t\t}\n\t\treturn\n\t}\n\n\t//TODO: we can DRY this further\n\treqVpnIp := n.Details.VpnIp\n\n\t//TODO: IPV6-WORK\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], n.Details.VpnIp)\n\tqueryVpnIp := netip.AddrFrom4(b)\n\n\t//TODO: Maybe instead of marshalling into n we marshal into a new `r` to not nuke our current request data\n\tfound, ln, err := lhh.lh.queryAndPrepMessage(queryVpnIp, func(c *cache) (int, error) {\n\t\tn = lhh.resetMeta()\n\t\tn.Type = NebulaMeta_HostQueryReply\n\t\tn.Details.VpnIp = reqVpnIp\n\n\t\tlhh.coalesceAnswers(c, n)\n\n\t\treturn n.MarshalTo(lhh.pb)\n\t})\n\n\tif !found {\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tlhh.l.WithError(err).WithField(\"vpnIp\", vpnIp).Error(\"Failed to marshal lighthouse host query reply\")\n\t\treturn\n\t}\n\n\tlhh.lh.metricTx(NebulaMeta_HostQueryReply, 1)\n\tw.SendMessageToVpnIp(header.LightHouse, 0, vpnIp, lhh.pb[:ln], lhh.nb, lhh.out[:0])\n\n\t// This signals the other side to punch some zero byte udp packets\n\tfound, ln, err = lhh.lh.queryAndPrepMessage(vpnIp, func(c *cache) (int, error) {\n\t\tn = lhh.resetMeta()\n\t\tn.Type = NebulaMeta_HostPunchNotification\n\t\t//TODO: IPV6-WORK\n\t\tb = vpnIp.As4()\n\t\tn.Details.VpnIp = binary.BigEndian.Uint32(b[:])\n\t\tlhh.coalesceAnswers(c, n)\n\n\t\treturn n.MarshalTo(lhh.pb)\n\t})\n\n\tif !found {\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tlhh.l.WithError(err).WithField(\"vpnIp\", vpnIp).Error(\"Failed to marshal lighthouse host was queried for\")\n\t\treturn\n\t}\n\n\tlhh.lh.metricTx(NebulaMeta_HostPunchNotification, 1)\n\n\t//TODO: IPV6-WORK\n\tbinary.BigEndian.PutUint32(b[:], reqVpnIp)\n\tsendTo := netip.AddrFrom4(b)\n\tw.SendMessageToVpnIp(header.LightHouse, 0, sendTo, lhh.pb[:ln], lhh.nb, lhh.out[:0])\n}\n\nfunc (lhh *LightHouseHandler) coalesceAnswers(c *cache, n *NebulaMeta) {\n\tif c.v4 != nil {\n\t\tif c.v4.learned != nil {\n\t\t\tn.Details.Ip4AndPorts = append(n.Details.Ip4AndPorts, c.v4.learned)\n\t\t}\n\t\tif c.v4.reported != nil && len(c.v4.reported) > 0 {\n\t\t\tn.Details.Ip4AndPorts = append(n.Details.Ip4AndPorts, c.v4.reported...)\n\t\t}\n\t}\n\n\tif c.v6 != nil {\n\t\tif c.v6.learned != nil {\n\t\t\tn.Details.Ip6AndPorts = append(n.Details.Ip6AndPorts, c.v6.learned)\n\t\t}\n\t\tif c.v6.reported != nil && len(c.v6.reported) > 0 {\n\t\t\tn.Details.Ip6AndPorts = append(n.Details.Ip6AndPorts, c.v6.reported...)\n\t\t}\n\t}\n\n\tif c.relay != nil {\n\t\t//TODO: IPV6-WORK\n\t\trelays := make([]uint32, len(c.relay.relay))\n\t\tb := [4]byte{}\n\t\tfor i, _ := range relays {\n\t\t\tb = c.relay.relay[i].As4()\n\t\t\trelays[i] = binary.BigEndian.Uint32(b[:])\n\t\t}\n\t\tn.Details.RelayVpnIp = append(n.Details.RelayVpnIp, relays...)\n\t}\n}\n\nfunc (lhh *LightHouseHandler) handleHostQueryReply(n *NebulaMeta, vpnIp netip.Addr) {\n\tif !lhh.lh.IsLighthouseIP(vpnIp) {\n\t\treturn\n\t}\n\n\tlhh.lh.Lock()\n\t//TODO: IPV6-WORK\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], n.Details.VpnIp)\n\tcertVpnIp := netip.AddrFrom4(b)\n\tam := lhh.lh.unlockedGetRemoteList(certVpnIp)\n\tam.Lock()\n\tlhh.lh.Unlock()\n\n\t//TODO: IPV6-WORK\n\tam.unlockedSetV4(vpnIp, certVpnIp, n.Details.Ip4AndPorts, lhh.lh.unlockedShouldAddV4)\n\tam.unlockedSetV6(vpnIp, certVpnIp, n.Details.Ip6AndPorts, lhh.lh.unlockedShouldAddV6)\n\n\t//TODO: IPV6-WORK\n\trelays := make([]netip.Addr, len(n.Details.RelayVpnIp))\n\tfor i, _ := range n.Details.RelayVpnIp {\n\t\tbinary.BigEndian.PutUint32(b[:], n.Details.RelayVpnIp[i])\n\t\trelays[i] = netip.AddrFrom4(b)\n\t}\n\tam.unlockedSetRelay(vpnIp, certVpnIp, relays)\n\tam.Unlock()\n\n\t// Non-blocking attempt to trigger, skip if it would block\n\tselect {\n\tcase lhh.lh.handshakeTrigger <- certVpnIp:\n\tdefault:\n\t}\n}\n\nfunc (lhh *LightHouseHandler) handleHostUpdateNotification(n *NebulaMeta, vpnIp netip.Addr, w EncWriter) {\n\tif !lhh.lh.amLighthouse {\n\t\tif lhh.l.Level >= logrus.DebugLevel {\n\t\t\tlhh.l.Debugln(\"I am not a lighthouse, do not take host updates: \", vpnIp)\n\t\t}\n\t\treturn\n\t}\n\n\t//Simple check that the host sent this not someone else\n\t//TODO: IPV6-WORK\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], n.Details.VpnIp)\n\tdetailsVpnIp := netip.AddrFrom4(b)\n\tif detailsVpnIp != vpnIp {\n\t\tif lhh.l.Level >= logrus.DebugLevel {\n\t\t\tlhh.l.WithField(\"vpnIp\", vpnIp).WithField(\"answer\", detailsVpnIp).Debugln(\"Host sent invalid update\")\n\t\t}\n\t\treturn\n\t}\n\n\tlhh.lh.Lock()\n\tam := lhh.lh.unlockedGetRemoteList(vpnIp)\n\tam.Lock()\n\tlhh.lh.Unlock()\n\n\tam.unlockedSetV4(vpnIp, detailsVpnIp, n.Details.Ip4AndPorts, lhh.lh.unlockedShouldAddV4)\n\tam.unlockedSetV6(vpnIp, detailsVpnIp, n.Details.Ip6AndPorts, lhh.lh.unlockedShouldAddV6)\n\n\t//TODO: IPV6-WORK\n\trelays := make([]netip.Addr, len(n.Details.RelayVpnIp))\n\tfor i, _ := range n.Details.RelayVpnIp {\n\t\tbinary.BigEndian.PutUint32(b[:], n.Details.RelayVpnIp[i])\n\t\trelays[i] = netip.AddrFrom4(b)\n\t}\n\tam.unlockedSetRelay(vpnIp, detailsVpnIp, relays)\n\tam.Unlock()\n\n\tn = lhh.resetMeta()\n\tn.Type = NebulaMeta_HostUpdateNotificationAck\n\n\t//TODO: IPV6-WORK\n\tvpnIpB := vpnIp.As4()\n\tn.Details.VpnIp = binary.BigEndian.Uint32(vpnIpB[:])\n\tln, err := n.MarshalTo(lhh.pb)\n\n\tif err != nil {\n\t\tlhh.l.WithError(err).WithField(\"vpnIp\", vpnIp).Error(\"Failed to marshal lighthouse host update ack\")\n\t\treturn\n\t}\n\n\tlhh.lh.metricTx(NebulaMeta_HostUpdateNotificationAck, 1)\n\tw.SendMessageToVpnIp(header.LightHouse, 0, vpnIp, lhh.pb[:ln], lhh.nb, lhh.out[:0])\n}\n\nfunc (lhh *LightHouseHandler) handleHostPunchNotification(n *NebulaMeta, vpnIp netip.Addr, w EncWriter) {\n\tif !lhh.lh.IsLighthouseIP(vpnIp) {\n\t\treturn\n\t}\n\n\tempty := []byte{0}\n\tpunch := func(vpnPeer netip.AddrPort) {\n\t\tif !vpnPeer.IsValid() {\n\t\t\treturn\n\t\t}\n\n\t\tgo func() {\n\t\t\ttime.Sleep(lhh.lh.punchy.GetDelay())\n\t\t\tlhh.lh.metricHolepunchTx.Inc(1)\n\t\t\tlhh.lh.punchConn.WriteTo(empty, vpnPeer)\n\t\t}()\n\n\t\tif lhh.l.Level >= logrus.DebugLevel {\n\t\t\t//TODO: lacking the ip we are actually punching on, old: l.Debugf(\"Punching %s on %d for %s\", IntIp(a.Ip), a.Port, IntIp(n.Details.VpnIp))\n\t\t\t//TODO: IPV6-WORK, make this debug line not suck\n\t\t\tb := [4]byte{}\n\t\t\tbinary.BigEndian.PutUint32(b[:], n.Details.VpnIp)\n\t\t\tlhh.l.Debugf(\"Punching on %d for %v\", vpnPeer.Port(), netip.AddrFrom4(b))\n\t\t}\n\t}\n\n\tfor _, a := range n.Details.Ip4AndPorts {\n\t\tpunch(AddrPortFromIp4AndPort(a))\n\t}\n\n\tfor _, a := range n.Details.Ip6AndPorts {\n\t\tpunch(AddrPortFromIp6AndPort(a))\n\t}\n\n\t// This sends a nebula test packet to the host trying to contact us. In the case\n\t// of a double nat or other difficult scenario, this may help establish\n\t// a tunnel.\n\tif lhh.lh.punchy.GetRespond() {\n\t\t//TODO: IPV6-WORK\n\t\tb := [4]byte{}\n\t\tbinary.BigEndian.PutUint32(b[:], n.Details.VpnIp)\n\t\tqueryVpnIp := netip.AddrFrom4(b)\n\t\tgo func() {\n\t\t\ttime.Sleep(lhh.lh.punchy.GetRespondDelay())\n\t\t\tif lhh.l.Level >= logrus.DebugLevel {\n\t\t\t\tlhh.l.Debugf(\"Sending a nebula test packet to vpn ip %s\", queryVpnIp)\n\t\t\t}\n\t\t\t//NOTE: we have to allocate a new output buffer here since we are spawning a new goroutine\n\t\t\t// for each punchBack packet. We should move this into a timerwheel or a single goroutine\n\t\t\t// managed by a channel.\n\t\t\tw.SendMessageToVpnIp(header.Test, header.TestRequest, queryVpnIp, []byte(\"\"), make([]byte, 12, 12), make([]byte, mtu))\n\t\t}()\n\t}\n}\n"
        },
        {
          "name": "lighthouse_test.go",
          "type": "blob",
          "size": 14.9296875,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"net/netip\"\n\t\"testing\"\n\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gopkg.in/yaml.v2\"\n)\n\n//TODO: Add a test to ensure udpAddr is copied and not reused\n\nfunc TestOldIPv4Only(t *testing.T) {\n\t// This test ensures our new ipv6 enabled LH protobuf IpAndPorts works with the old style to enable backwards compatibility\n\tb := []byte{8, 129, 130, 132, 80, 16, 10}\n\tvar m Ip4AndPort\n\terr := m.Unmarshal(b)\n\tassert.NoError(t, err)\n\tip := netip.MustParseAddr(\"10.1.1.1\")\n\tbp := ip.As4()\n\tassert.Equal(t, binary.BigEndian.Uint32(bp[:]), m.GetIp())\n}\n\nfunc TestNewLhQuery(t *testing.T) {\n\tmyIp, err := netip.ParseAddr(\"192.1.1.1\")\n\tassert.NoError(t, err)\n\n\t// Generating a new lh query should work\n\ta := NewLhQueryByInt(myIp)\n\n\t// The result should be a nebulameta protobuf\n\tassert.IsType(t, &NebulaMeta{}, a)\n\n\t// It should also Marshal fine\n\tb, err := a.Marshal()\n\tassert.Nil(t, err)\n\n\t// and then Unmarshal fine\n\tn := &NebulaMeta{}\n\terr = n.Unmarshal(b)\n\tassert.Nil(t, err)\n\n}\n\nfunc Test_lhStaticMapping(t *testing.T) {\n\tl := test.NewLogger()\n\tmyVpnNet := netip.MustParsePrefix(\"10.128.0.1/16\")\n\tlh1 := \"10.128.0.2\"\n\n\tc := config.NewC(l)\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\"hosts\": []interface{}{lh1}}\n\tc.Settings[\"static_host_map\"] = map[interface{}]interface{}{lh1: []interface{}{\"1.1.1.1:4242\"}}\n\t_, err := NewLightHouseFromConfig(context.Background(), l, c, myVpnNet, nil, nil)\n\tassert.Nil(t, err)\n\n\tlh2 := \"10.128.0.3\"\n\tc = config.NewC(l)\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\"hosts\": []interface{}{lh1, lh2}}\n\tc.Settings[\"static_host_map\"] = map[interface{}]interface{}{lh1: []interface{}{\"100.1.1.1:4242\"}}\n\t_, err = NewLightHouseFromConfig(context.Background(), l, c, myVpnNet, nil, nil)\n\tassert.EqualError(t, err, \"lighthouse 10.128.0.3 does not have a static_host_map entry\")\n}\n\nfunc TestReloadLighthouseInterval(t *testing.T) {\n\tl := test.NewLogger()\n\tmyVpnNet := netip.MustParsePrefix(\"10.128.0.1/16\")\n\tlh1 := \"10.128.0.2\"\n\n\tc := config.NewC(l)\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\n\t\t\"hosts\":    []interface{}{lh1},\n\t\t\"interval\": \"1s\",\n\t}\n\n\tc.Settings[\"static_host_map\"] = map[interface{}]interface{}{lh1: []interface{}{\"1.1.1.1:4242\"}}\n\tlh, err := NewLightHouseFromConfig(context.Background(), l, c, myVpnNet, nil, nil)\n\tassert.NoError(t, err)\n\tlh.ifce = &mockEncWriter{}\n\n\t// The first one routine is kicked off by main.go currently, lets make sure that one dies\n\tassert.NoError(t, c.ReloadConfigString(\"lighthouse:\\n  interval: 5\"))\n\tassert.Equal(t, int64(5), lh.interval.Load())\n\n\t// Subsequent calls are killed off by the LightHouse.Reload function\n\tassert.NoError(t, c.ReloadConfigString(\"lighthouse:\\n  interval: 10\"))\n\tassert.Equal(t, int64(10), lh.interval.Load())\n\n\t// If this completes then nothing is stealing our reload routine\n\tassert.NoError(t, c.ReloadConfigString(\"lighthouse:\\n  interval: 11\"))\n\tassert.Equal(t, int64(11), lh.interval.Load())\n}\n\nfunc BenchmarkLighthouseHandleRequest(b *testing.B) {\n\tl := test.NewLogger()\n\tmyVpnNet := netip.MustParsePrefix(\"10.128.0.1/0\")\n\n\tc := config.NewC(l)\n\tlh, err := NewLightHouseFromConfig(context.Background(), l, c, myVpnNet, nil, nil)\n\tif !assert.NoError(b, err) {\n\t\tb.Fatal()\n\t}\n\n\thAddr := netip.MustParseAddrPort(\"4.5.6.7:12345\")\n\thAddr2 := netip.MustParseAddrPort(\"4.5.6.7:12346\")\n\n\tvpnIp3 := netip.MustParseAddr(\"0.0.0.3\")\n\tlh.addrMap[vpnIp3] = NewRemoteList(nil)\n\tlh.addrMap[vpnIp3].unlockedSetV4(\n\t\tvpnIp3,\n\t\tvpnIp3,\n\t\t[]*Ip4AndPort{\n\t\t\tNewIp4AndPortFromNetIP(hAddr.Addr(), hAddr.Port()),\n\t\t\tNewIp4AndPortFromNetIP(hAddr2.Addr(), hAddr2.Port()),\n\t\t},\n\t\tfunc(netip.Addr, *Ip4AndPort) bool { return true },\n\t)\n\n\trAddr := netip.MustParseAddrPort(\"1.2.2.3:12345\")\n\trAddr2 := netip.MustParseAddrPort(\"1.2.2.3:12346\")\n\tvpnIp2 := netip.MustParseAddr(\"0.0.0.3\")\n\tlh.addrMap[vpnIp2] = NewRemoteList(nil)\n\tlh.addrMap[vpnIp2].unlockedSetV4(\n\t\tvpnIp3,\n\t\tvpnIp3,\n\t\t[]*Ip4AndPort{\n\t\t\tNewIp4AndPortFromNetIP(rAddr.Addr(), rAddr.Port()),\n\t\t\tNewIp4AndPortFromNetIP(rAddr2.Addr(), rAddr2.Port()),\n\t\t},\n\t\tfunc(netip.Addr, *Ip4AndPort) bool { return true },\n\t)\n\n\tmw := &mockEncWriter{}\n\n\tb.Run(\"notfound\", func(b *testing.B) {\n\t\tlhh := lh.NewRequestHandler()\n\t\treq := &NebulaMeta{\n\t\t\tType: NebulaMeta_HostQuery,\n\t\t\tDetails: &NebulaMetaDetails{\n\t\t\t\tVpnIp:       4,\n\t\t\t\tIp4AndPorts: nil,\n\t\t\t},\n\t\t}\n\t\tp, err := req.Marshal()\n\t\tassert.NoError(b, err)\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tlhh.HandleRequest(rAddr, vpnIp2, p, mw)\n\t\t}\n\t})\n\tb.Run(\"found\", func(b *testing.B) {\n\t\tlhh := lh.NewRequestHandler()\n\t\treq := &NebulaMeta{\n\t\t\tType: NebulaMeta_HostQuery,\n\t\t\tDetails: &NebulaMetaDetails{\n\t\t\t\tVpnIp:       3,\n\t\t\t\tIp4AndPorts: nil,\n\t\t\t},\n\t\t}\n\t\tp, err := req.Marshal()\n\t\tassert.NoError(b, err)\n\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\tlhh.HandleRequest(rAddr, vpnIp2, p, mw)\n\t\t}\n\t})\n}\n\nfunc TestLighthouse_Memory(t *testing.T) {\n\tl := test.NewLogger()\n\n\tmyUdpAddr0 := netip.MustParseAddrPort(\"10.0.0.2:4242\")\n\tmyUdpAddr1 := netip.MustParseAddrPort(\"192.168.0.2:4242\")\n\tmyUdpAddr2 := netip.MustParseAddrPort(\"172.16.0.2:4242\")\n\tmyUdpAddr3 := netip.MustParseAddrPort(\"100.152.0.2:4242\")\n\tmyUdpAddr4 := netip.MustParseAddrPort(\"24.15.0.2:4242\")\n\tmyUdpAddr5 := netip.MustParseAddrPort(\"192.168.0.2:4243\")\n\tmyUdpAddr6 := netip.MustParseAddrPort(\"192.168.0.2:4244\")\n\tmyUdpAddr7 := netip.MustParseAddrPort(\"192.168.0.2:4245\")\n\tmyUdpAddr8 := netip.MustParseAddrPort(\"192.168.0.2:4246\")\n\tmyUdpAddr9 := netip.MustParseAddrPort(\"192.168.0.2:4247\")\n\tmyUdpAddr10 := netip.MustParseAddrPort(\"192.168.0.2:4248\")\n\tmyUdpAddr11 := netip.MustParseAddrPort(\"192.168.0.2:4249\")\n\tmyVpnIp := netip.MustParseAddr(\"10.128.0.2\")\n\n\ttheirUdpAddr0 := netip.MustParseAddrPort(\"10.0.0.3:4242\")\n\ttheirUdpAddr1 := netip.MustParseAddrPort(\"192.168.0.3:4242\")\n\ttheirUdpAddr2 := netip.MustParseAddrPort(\"172.16.0.3:4242\")\n\ttheirUdpAddr3 := netip.MustParseAddrPort(\"100.152.0.3:4242\")\n\ttheirUdpAddr4 := netip.MustParseAddrPort(\"24.15.0.3:4242\")\n\ttheirVpnIp := netip.MustParseAddr(\"10.128.0.3\")\n\n\tc := config.NewC(l)\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\"am_lighthouse\": true}\n\tc.Settings[\"listen\"] = map[interface{}]interface{}{\"port\": 4242}\n\tlh, err := NewLightHouseFromConfig(context.Background(), l, c, netip.MustParsePrefix(\"10.128.0.1/24\"), nil, nil)\n\tassert.NoError(t, err)\n\tlhh := lh.NewRequestHandler()\n\n\t// Test that my first update responds with just that\n\tnewLHHostUpdate(myUdpAddr0, myVpnIp, []netip.AddrPort{myUdpAddr1, myUdpAddr2}, lhh)\n\tr := newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, myUdpAddr1, myUdpAddr2)\n\n\t// Ensure we don't accumulate addresses\n\tnewLHHostUpdate(myUdpAddr0, myVpnIp, []netip.AddrPort{myUdpAddr3}, lhh)\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, myUdpAddr3)\n\n\t// Grow it back to 2\n\tnewLHHostUpdate(myUdpAddr0, myVpnIp, []netip.AddrPort{myUdpAddr1, myUdpAddr4}, lhh)\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, myUdpAddr1, myUdpAddr4)\n\n\t// Update a different host and ask about it\n\tnewLHHostUpdate(theirUdpAddr0, theirVpnIp, []netip.AddrPort{theirUdpAddr1, theirUdpAddr2, theirUdpAddr3, theirUdpAddr4}, lhh)\n\tr = newLHHostRequest(theirUdpAddr0, theirVpnIp, theirVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, theirUdpAddr1, theirUdpAddr2, theirUdpAddr3, theirUdpAddr4)\n\n\t// Have both hosts ask about the other\n\tr = newLHHostRequest(theirUdpAddr0, theirVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, myUdpAddr1, myUdpAddr4)\n\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, theirVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, theirUdpAddr1, theirUdpAddr2, theirUdpAddr3, theirUdpAddr4)\n\n\t// Make sure we didn't get changed\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, myUdpAddr1, myUdpAddr4)\n\n\t// Ensure proper ordering and limiting\n\t// Send 12 addrs, get 10 back, the last 2 removed, allowing the duplicate to remain (clients dedupe)\n\tnewLHHostUpdate(\n\t\tmyUdpAddr0,\n\t\tmyVpnIp,\n\t\t[]netip.AddrPort{\n\t\t\tmyUdpAddr1,\n\t\t\tmyUdpAddr2,\n\t\t\tmyUdpAddr3,\n\t\t\tmyUdpAddr4,\n\t\t\tmyUdpAddr5,\n\t\t\tmyUdpAddr5, //Duplicated on purpose\n\t\t\tmyUdpAddr6,\n\t\t\tmyUdpAddr7,\n\t\t\tmyUdpAddr8,\n\t\t\tmyUdpAddr9,\n\t\t\tmyUdpAddr10,\n\t\t\tmyUdpAddr11, // This should get cut\n\t\t}, lhh)\n\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(\n\t\tt,\n\t\tr.msg.Details.Ip4AndPorts,\n\t\tmyUdpAddr1, myUdpAddr2, myUdpAddr3, myUdpAddr4, myUdpAddr5, myUdpAddr5, myUdpAddr6, myUdpAddr7, myUdpAddr8, myUdpAddr9,\n\t)\n\n\t// Make sure we won't add ips in our vpn network\n\tbad1 := netip.MustParseAddrPort(\"10.128.0.99:4242\")\n\tbad2 := netip.MustParseAddrPort(\"10.128.0.100:4242\")\n\tgood := netip.MustParseAddrPort(\"1.128.0.99:4242\")\n\tnewLHHostUpdate(myUdpAddr0, myVpnIp, []netip.AddrPort{bad1, bad2, good}, lhh)\n\tr = newLHHostRequest(myUdpAddr0, myVpnIp, myVpnIp, lhh)\n\tassertIp4InArray(t, r.msg.Details.Ip4AndPorts, good)\n}\n\nfunc TestLighthouse_reload(t *testing.T) {\n\tl := test.NewLogger()\n\tc := config.NewC(l)\n\tc.Settings[\"lighthouse\"] = map[interface{}]interface{}{\"am_lighthouse\": true}\n\tc.Settings[\"listen\"] = map[interface{}]interface{}{\"port\": 4242}\n\tlh, err := NewLightHouseFromConfig(context.Background(), l, c, netip.MustParsePrefix(\"10.128.0.1/24\"), nil, nil)\n\tassert.NoError(t, err)\n\n\tnc := map[interface{}]interface{}{\n\t\t\"static_host_map\": map[interface{}]interface{}{\n\t\t\t\"10.128.0.2\": []interface{}{\"1.1.1.1:4242\"},\n\t\t},\n\t}\n\trc, err := yaml.Marshal(nc)\n\tassert.NoError(t, err)\n\tc.ReloadConfigString(string(rc))\n\n\terr = lh.reload(c, false)\n\tassert.NoError(t, err)\n}\n\nfunc newLHHostRequest(fromAddr netip.AddrPort, myVpnIp, queryVpnIp netip.Addr, lhh *LightHouseHandler) testLhReply {\n\t//TODO: IPV6-WORK\n\tbip := queryVpnIp.As4()\n\treq := &NebulaMeta{\n\t\tType: NebulaMeta_HostQuery,\n\t\tDetails: &NebulaMetaDetails{\n\t\t\tVpnIp: binary.BigEndian.Uint32(bip[:]),\n\t\t},\n\t}\n\n\tb, err := req.Marshal()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfilter := NebulaMeta_HostQueryReply\n\tw := &testEncWriter{\n\t\tmetaFilter: &filter,\n\t}\n\tlhh.HandleRequest(fromAddr, myVpnIp, b, w)\n\treturn w.lastReply\n}\n\nfunc newLHHostUpdate(fromAddr netip.AddrPort, vpnIp netip.Addr, addrs []netip.AddrPort, lhh *LightHouseHandler) {\n\t//TODO: IPV6-WORK\n\tbip := vpnIp.As4()\n\treq := &NebulaMeta{\n\t\tType: NebulaMeta_HostUpdateNotification,\n\t\tDetails: &NebulaMetaDetails{\n\t\t\tVpnIp:       binary.BigEndian.Uint32(bip[:]),\n\t\t\tIp4AndPorts: make([]*Ip4AndPort, len(addrs)),\n\t\t},\n\t}\n\n\tfor k, v := range addrs {\n\t\treq.Details.Ip4AndPorts[k] = NewIp4AndPortFromNetIP(v.Addr(), v.Port())\n\t}\n\n\tb, err := req.Marshal()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tw := &testEncWriter{}\n\tlhh.HandleRequest(fromAddr, vpnIp, b, w)\n}\n\n//TODO: this is a RemoteList test\n//func Test_lhRemoteAllowList(t *testing.T) {\n//\tl := NewLogger()\n//\tc := NewConfig(l)\n//\tc.Settings[\"remoteallowlist\"] = map[interface{}]interface{}{\n//\t\t\"10.20.0.0/12\": false,\n//\t}\n//\tallowList, err := c.GetAllowList(\"remoteallowlist\", false)\n//\tassert.Nil(t, err)\n//\n//\tlh1 := \"10.128.0.2\"\n//\tlh1IP := net.ParseIP(lh1)\n//\n//\tudpServer, _ := NewListener(l, \"0.0.0.0\", 0, true)\n//\n//\tlh := NewLightHouse(l, true, &net.IPNet{IP: net.IP{0, 0, 0, 1}, Mask: net.IPMask{255, 255, 255, 0}}, []uint32{ip2int(lh1IP)}, 10, 10003, udpServer, false, 1, false)\n//\tlh.SetRemoteAllowList(allowList)\n//\n//\t// A disallowed ip should not enter the cache but we should end up with an empty entry in the addrMap\n//\tremote1IP := net.ParseIP(\"10.20.0.3\")\n//\tremotes := lh.unlockedGetRemoteList(ip2int(remote1IP))\n//\tremotes.unlockedPrependV4(ip2int(remote1IP), NewIp4AndPort(remote1IP, 4242))\n//\tassert.NotNil(t, lh.addrMap[ip2int(remote1IP)])\n//\tassert.Empty(t, lh.addrMap[ip2int(remote1IP)].CopyAddrs([]*net.IPNet{}))\n//\n//\t// Make sure a good ip enters the cache and addrMap\n//\tremote2IP := net.ParseIP(\"10.128.0.3\")\n//\tremote2UDPAddr := NewUDPAddr(remote2IP, uint16(4242))\n//\tlh.addRemoteV4(ip2int(remote2IP), ip2int(remote2IP), NewIp4AndPort(remote2UDPAddr.IP, uint32(remote2UDPAddr.Port)), false, false)\n//\tassertUdpAddrInArray(t, lh.addrMap[ip2int(remote2IP)].CopyAddrs([]*net.IPNet{}), remote2UDPAddr)\n//\n//\t// Another good ip gets into the cache, ordering is inverted\n//\tremote3IP := net.ParseIP(\"10.128.0.4\")\n//\tremote3UDPAddr := NewUDPAddr(remote3IP, uint16(4243))\n//\tlh.addRemoteV4(ip2int(remote2IP), ip2int(remote2IP), NewIp4AndPort(remote3UDPAddr.IP, uint32(remote3UDPAddr.Port)), false, false)\n//\tassertUdpAddrInArray(t, lh.addrMap[ip2int(remote2IP)].CopyAddrs([]*net.IPNet{}), remote2UDPAddr, remote3UDPAddr)\n//\n//\t// If we exceed the length limit we should only have the most recent addresses\n//\taddedAddrs := []*udpAddr{}\n//\tfor i := 0; i < 11; i++ {\n//\t\tremoteUDPAddr := NewUDPAddr(net.IP{10, 128, 0, 4}, uint16(4243+i))\n//\t\tlh.addRemoteV4(ip2int(remote2IP), ip2int(remote2IP), NewIp4AndPort(remoteUDPAddr.IP, uint32(remoteUDPAddr.Port)), false, false)\n//\t\t// The first entry here is a duplicate, don't add it to the assert list\n//\t\tif i != 0 {\n//\t\t\taddedAddrs = append(addedAddrs, remoteUDPAddr)\n//\t\t}\n//\t}\n//\n//\t// We should only have the last 10 of what we tried to add\n//\tassert.True(t, len(addedAddrs) >= 10, \"We should have tried to add at least 10 addresses\")\n//\tassertUdpAddrInArray(\n//\t\tt,\n//\t\tlh.addrMap[ip2int(remote2IP)].CopyAddrs([]*net.IPNet{}),\n//\t\taddedAddrs[0],\n//\t\taddedAddrs[1],\n//\t\taddedAddrs[2],\n//\t\taddedAddrs[3],\n//\t\taddedAddrs[4],\n//\t\taddedAddrs[5],\n//\t\taddedAddrs[6],\n//\t\taddedAddrs[7],\n//\t\taddedAddrs[8],\n//\t\taddedAddrs[9],\n//\t)\n//}\n\ntype testLhReply struct {\n\tnebType    header.MessageType\n\tnebSubType header.MessageSubType\n\tvpnIp      netip.Addr\n\tmsg        *NebulaMeta\n}\n\ntype testEncWriter struct {\n\tlastReply  testLhReply\n\tmetaFilter *NebulaMeta_MessageType\n}\n\nfunc (tw *testEncWriter) SendVia(via *HostInfo, relay *Relay, ad, nb, out []byte, nocopy bool) {\n}\nfunc (tw *testEncWriter) Handshake(vpnIp netip.Addr) {\n}\n\nfunc (tw *testEncWriter) SendMessageToHostInfo(t header.MessageType, st header.MessageSubType, hostinfo *HostInfo, p, _, _ []byte) {\n\tmsg := &NebulaMeta{}\n\terr := msg.Unmarshal(p)\n\tif tw.metaFilter == nil || msg.Type == *tw.metaFilter {\n\t\ttw.lastReply = testLhReply{\n\t\t\tnebType:    t,\n\t\t\tnebSubType: st,\n\t\t\tvpnIp:      hostinfo.vpnIp,\n\t\t\tmsg:        msg,\n\t\t}\n\t}\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (tw *testEncWriter) SendMessageToVpnIp(t header.MessageType, st header.MessageSubType, vpnIp netip.Addr, p, _, _ []byte) {\n\tmsg := &NebulaMeta{}\n\terr := msg.Unmarshal(p)\n\tif tw.metaFilter == nil || msg.Type == *tw.metaFilter {\n\t\ttw.lastReply = testLhReply{\n\t\t\tnebType:    t,\n\t\t\tnebSubType: st,\n\t\t\tvpnIp:      vpnIp,\n\t\t\tmsg:        msg,\n\t\t}\n\t}\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// assertIp4InArray asserts every address in want is at the same position in have and that the lengths match\nfunc assertIp4InArray(t *testing.T, have []*Ip4AndPort, want ...netip.AddrPort) {\n\tif !assert.Len(t, have, len(want)) {\n\t\treturn\n\t}\n\n\tfor k, w := range want {\n\t\t//TODO: IPV6-WORK\n\t\th := AddrPortFromIp4AndPort(have[k])\n\t\tif !(h == w) {\n\t\t\tassert.Fail(t, fmt.Sprintf(\"Response did not contain: %v at %v, found %v\", w, k, h))\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "logger.go",
          "type": "blob",
          "size": 1.1484375,
          "content": "package nebula\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\nfunc configLogger(l *logrus.Logger, c *config.C) error {\n\t// set up our logging level\n\tlogLevel, err := logrus.ParseLevel(strings.ToLower(c.GetString(\"logging.level\", \"info\")))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s; possible levels: %s\", err, logrus.AllLevels)\n\t}\n\tl.SetLevel(logLevel)\n\n\tdisableTimestamp := c.GetBool(\"logging.disable_timestamp\", false)\n\ttimestampFormat := c.GetString(\"logging.timestamp_format\", \"\")\n\tfullTimestamp := (timestampFormat != \"\")\n\tif timestampFormat == \"\" {\n\t\ttimestampFormat = time.RFC3339\n\t}\n\n\tlogFormat := strings.ToLower(c.GetString(\"logging.format\", \"text\"))\n\tswitch logFormat {\n\tcase \"text\":\n\t\tl.Formatter = &logrus.TextFormatter{\n\t\t\tTimestampFormat:  timestampFormat,\n\t\t\tFullTimestamp:    fullTimestamp,\n\t\t\tDisableTimestamp: disableTimestamp,\n\t\t}\n\tcase \"json\":\n\t\tl.Formatter = &logrus.JSONFormatter{\n\t\t\tTimestampFormat:  timestampFormat,\n\t\t\tDisableTimestamp: disableTimestamp,\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown log format `%s`. possible formats: %s\", logFormat, []string{\"text\", \"json\"})\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "main.go",
          "type": "blob",
          "size": 9.6533203125,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/netip\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/overlay\"\n\t\"github.com/slackhq/nebula/sshd\"\n\t\"github.com/slackhq/nebula/udp\"\n\t\"github.com/slackhq/nebula/util\"\n\t\"gopkg.in/yaml.v2\"\n)\n\ntype m map[string]interface{}\n\nfunc Main(c *config.C, configTest bool, buildVersion string, logger *logrus.Logger, deviceFactory overlay.DeviceFactory) (retcon *Control, reterr error) {\n\tctx, cancel := context.WithCancel(context.Background())\n\t// Automatically cancel the context if Main returns an error, to signal all created goroutines to quit.\n\tdefer func() {\n\t\tif reterr != nil {\n\t\t\tcancel()\n\t\t}\n\t}()\n\n\tl := logger\n\tl.Formatter = &logrus.TextFormatter{\n\t\tFullTimestamp: true,\n\t}\n\n\t// Print the config if in test, the exit comes later\n\tif configTest {\n\t\tb, err := yaml.Marshal(c.Settings)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Print the final config\n\t\tl.Println(string(b))\n\t}\n\n\terr := configLogger(l, c)\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to configure the logger\", err)\n\t}\n\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\terr := configLogger(l, c)\n\t\tif err != nil {\n\t\t\tl.WithError(err).Error(\"Failed to configure the logger\")\n\t\t}\n\t})\n\n\tpki, err := NewPKIFromConfig(l, c)\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to load PKI from config\", err)\n\t}\n\n\tcertificate := pki.GetCertState().Certificate\n\tfw, err := NewFirewallFromConfig(l, certificate, c)\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Error while loading firewall rules\", err)\n\t}\n\tl.WithField(\"firewallHashes\", fw.GetRuleHashes()).Info(\"Firewall started\")\n\n\ttunCidr := certificate.Networks()[0]\n\n\tssh, err := sshd.NewSSHServer(l.WithField(\"subsystem\", \"sshd\"))\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Error while creating SSH server\", err)\n\t}\n\twireSSHReload(l, ssh, c)\n\tvar sshStart func()\n\tif c.GetBool(\"sshd.enabled\", false) {\n\t\tsshStart, err = configSSH(l, ssh, c)\n\t\tif err != nil {\n\t\t\treturn nil, util.ContextualizeIfNeeded(\"Error while configuring the sshd\", err)\n\t\t}\n\t}\n\n\t////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\t// All non system modifying configuration consumption should live above this line\n\t// tun config, listeners, anything modifying the computer should be below\n\t////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n\tvar routines int\n\n\t// If `routines` is set, use that and ignore the specific values\n\tif routines = c.GetInt(\"routines\", 0); routines != 0 {\n\t\tif routines < 1 {\n\t\t\troutines = 1\n\t\t}\n\t\tif routines > 1 {\n\t\t\tl.WithField(\"routines\", routines).Info(\"Using multiple routines\")\n\t\t}\n\t} else {\n\t\t// deprecated and undocumented\n\t\ttunQueues := c.GetInt(\"tun.routines\", 1)\n\t\tudpQueues := c.GetInt(\"listen.routines\", 1)\n\t\tif tunQueues > udpQueues {\n\t\t\troutines = tunQueues\n\t\t} else {\n\t\t\troutines = udpQueues\n\t\t}\n\t\tif routines != 1 {\n\t\t\tl.WithField(\"routines\", routines).Warn(\"Setting tun.routines and listen.routines is deprecated. Use `routines` instead\")\n\t\t}\n\t}\n\n\t// EXPERIMENTAL\n\t// Intentionally not documented yet while we do more testing and determine\n\t// a good default value.\n\tconntrackCacheTimeout := c.GetDuration(\"firewall.conntrack.routine_cache_timeout\", 0)\n\tif routines > 1 && !c.IsSet(\"firewall.conntrack.routine_cache_timeout\") {\n\t\t// Use a different default if we are running with multiple routines\n\t\tconntrackCacheTimeout = 1 * time.Second\n\t}\n\tif conntrackCacheTimeout > 0 {\n\t\tl.WithField(\"duration\", conntrackCacheTimeout).Info(\"Using routine-local conntrack cache\")\n\t}\n\n\tvar tun overlay.Device\n\tif !configTest {\n\t\tc.CatchHUP(ctx)\n\n\t\tif deviceFactory == nil {\n\t\t\tdeviceFactory = overlay.NewDeviceFromConfig\n\t\t}\n\n\t\ttun, err = deviceFactory(c, l, tunCidr, routines)\n\t\tif err != nil {\n\t\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to get a tun/tap device\", err)\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif reterr != nil {\n\t\t\t\ttun.Close()\n\t\t\t}\n\t\t}()\n\t}\n\n\t// set up our UDP listener\n\tudpConns := make([]udp.Conn, routines)\n\tport := c.GetInt(\"listen.port\", 0)\n\n\tif !configTest {\n\t\trawListenHost := c.GetString(\"listen.host\", \"0.0.0.0\")\n\t\tvar listenHost netip.Addr\n\t\tif rawListenHost == \"[::]\" {\n\t\t\t// Old guidance was to provide the literal `[::]` in `listen.host` but that won't resolve.\n\t\t\tlistenHost = netip.IPv6Unspecified()\n\n\t\t} else {\n\t\t\tips, err := net.DefaultResolver.LookupNetIP(context.Background(), \"ip\", rawListenHost)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to resolve listen.host\", err)\n\t\t\t}\n\t\t\tif len(ips) == 0 {\n\t\t\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to resolve listen.host\", err)\n\t\t\t}\n\t\t\tlistenHost = ips[0].Unmap()\n\t\t}\n\n\t\tfor i := 0; i < routines; i++ {\n\t\t\tl.Infof(\"listening on %v\", netip.AddrPortFrom(listenHost, uint16(port)))\n\t\t\tudpServer, err := udp.NewListener(l, listenHost, port, routines > 1, c.GetInt(\"listen.batch\", 64))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, util.NewContextualError(\"Failed to open udp listener\", m{\"queue\": i}, err)\n\t\t\t}\n\t\t\tudpServer.ReloadConfig(c)\n\t\t\tudpConns[i] = udpServer\n\n\t\t\t// If port is dynamic, discover it before the next pass through the for loop\n\t\t\t// This way all routines will use the same port correctly\n\t\t\tif port == 0 {\n\t\t\t\tuPort, err := udpServer.LocalAddr()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, util.NewContextualError(\"Failed to get listening port\", nil, err)\n\t\t\t\t}\n\t\t\t\tport = int(uPort.Port())\n\t\t\t}\n\t\t}\n\t}\n\n\thostMap := NewHostMapFromConfig(l, tunCidr, c)\n\tpunchy := NewPunchyFromConfig(l, c)\n\tlightHouse, err := NewLightHouseFromConfig(ctx, l, c, tunCidr, udpConns[0], punchy)\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to initialize lighthouse handler\", err)\n\t}\n\n\tvar messageMetrics *MessageMetrics\n\tif c.GetBool(\"stats.message_metrics\", false) {\n\t\tmessageMetrics = newMessageMetrics()\n\t} else {\n\t\tmessageMetrics = newMessageMetricsOnlyRecvError()\n\t}\n\n\tuseRelays := c.GetBool(\"relay.use_relays\", DefaultUseRelays) && !c.GetBool(\"relay.am_relay\", false)\n\n\thandshakeConfig := HandshakeConfig{\n\t\ttryInterval:   c.GetDuration(\"handshakes.try_interval\", DefaultHandshakeTryInterval),\n\t\tretries:       int64(c.GetInt(\"handshakes.retries\", DefaultHandshakeRetries)),\n\t\ttriggerBuffer: c.GetInt(\"handshakes.trigger_buffer\", DefaultHandshakeTriggerBuffer),\n\t\tuseRelays:     useRelays,\n\n\t\tmessageMetrics: messageMetrics,\n\t}\n\n\thandshakeManager := NewHandshakeManager(l, hostMap, lightHouse, udpConns[0], handshakeConfig)\n\tlightHouse.handshakeTrigger = handshakeManager.trigger\n\n\tserveDns := false\n\tif c.GetBool(\"lighthouse.serve_dns\", false) {\n\t\tif c.GetBool(\"lighthouse.am_lighthouse\", false) {\n\t\t\tserveDns = true\n\t\t} else {\n\t\t\tl.Warn(\"DNS server refusing to run because this host is not a lighthouse.\")\n\t\t}\n\t}\n\n\tcheckInterval := c.GetInt(\"timers.connection_alive_interval\", 5)\n\tpendingDeletionInterval := c.GetInt(\"timers.pending_deletion_interval\", 10)\n\n\tifConfig := &InterfaceConfig{\n\t\tHostMap:                 hostMap,\n\t\tInside:                  tun,\n\t\tOutside:                 udpConns[0],\n\t\tpki:                     pki,\n\t\tCipher:                  c.GetString(\"cipher\", \"aes\"),\n\t\tFirewall:                fw,\n\t\tServeDns:                serveDns,\n\t\tHandshakeManager:        handshakeManager,\n\t\tlightHouse:              lightHouse,\n\t\tcheckInterval:           time.Second * time.Duration(checkInterval),\n\t\tpendingDeletionInterval: time.Second * time.Duration(pendingDeletionInterval),\n\t\ttryPromoteEvery:         c.GetUint32(\"counters.try_promote\", defaultPromoteEvery),\n\t\treQueryEvery:            c.GetUint32(\"counters.requery_every_packets\", defaultReQueryEvery),\n\t\treQueryWait:             c.GetDuration(\"timers.requery_wait_duration\", defaultReQueryWait),\n\t\tDropLocalBroadcast:      c.GetBool(\"tun.drop_local_broadcast\", false),\n\t\tDropMulticast:           c.GetBool(\"tun.drop_multicast\", false),\n\t\troutines:                routines,\n\t\tMessageMetrics:          messageMetrics,\n\t\tversion:                 buildVersion,\n\t\trelayManager:            NewRelayManager(ctx, l, hostMap, c),\n\t\tpunchy:                  punchy,\n\n\t\tConntrackCacheTimeout: conntrackCacheTimeout,\n\t\tl:                     l,\n\t}\n\n\tswitch ifConfig.Cipher {\n\tcase \"aes\":\n\t\tnoiseEndianness = binary.BigEndian\n\tcase \"chachapoly\":\n\t\tnoiseEndianness = binary.LittleEndian\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown cipher: %v\", ifConfig.Cipher)\n\t}\n\n\tvar ifce *Interface\n\tif !configTest {\n\t\tifce, err = NewInterface(ctx, ifConfig)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to initialize interface: %s\", err)\n\t\t}\n\n\t\t// TODO: Better way to attach these, probably want a new interface in InterfaceConfig\n\t\t// I don't want to make this initial commit too far-reaching though\n\t\tifce.writers = udpConns\n\t\tlightHouse.ifce = ifce\n\n\t\tifce.RegisterConfigChangeCallbacks(c)\n\t\tifce.reloadDisconnectInvalid(c)\n\t\tifce.reloadSendRecvError(c)\n\n\t\thandshakeManager.f = ifce\n\t\tgo handshakeManager.Run(ctx)\n\t}\n\n\t// TODO - stats third-party modules start uncancellable goroutines. Update those libs to accept\n\t// a context so that they can exit when the context is Done.\n\tstatsStart, err := startStats(l, c, buildVersion, configTest)\n\tif err != nil {\n\t\treturn nil, util.ContextualizeIfNeeded(\"Failed to start stats emitter\", err)\n\t}\n\n\tif configTest {\n\t\treturn nil, nil\n\t}\n\n\t//TODO: check if we _should_ be emitting stats\n\tgo ifce.emitStats(ctx, c.GetDuration(\"stats.interval\", time.Second*10))\n\n\tattachCommands(l, c, ssh, ifce)\n\n\t// Start DNS server last to allow using the nebula IP as lighthouse.dns.host\n\tvar dnsStart func()\n\tif lightHouse.amLighthouse && serveDns {\n\t\tl.Debugln(\"Starting dns server\")\n\t\tdnsStart = dnsMain(l, hostMap, c)\n\t}\n\n\treturn &Control{\n\t\tifce,\n\t\tl,\n\t\tctx,\n\t\tcancel,\n\t\tsshStart,\n\t\tstatsStart,\n\t\tdnsStart,\n\t\tlightHouse.StartUpdateWorker,\n\t}, nil\n}\n"
        },
        {
          "name": "message_metrics.go",
          "type": "blob",
          "size": 2.7138671875,
          "content": "package nebula\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/slackhq/nebula/header\"\n)\n\n//TODO: this can probably move into the header package\n\ntype MessageMetrics struct {\n\trx [][]metrics.Counter\n\ttx [][]metrics.Counter\n\n\trxUnknown metrics.Counter\n\ttxUnknown metrics.Counter\n}\n\nfunc (m *MessageMetrics) Rx(t header.MessageType, s header.MessageSubType, i int64) {\n\tif m != nil {\n\t\tif t >= 0 && int(t) < len(m.rx) && s >= 0 && int(s) < len(m.rx[t]) {\n\t\t\tm.rx[t][s].Inc(i)\n\t\t} else if m.rxUnknown != nil {\n\t\t\tm.rxUnknown.Inc(i)\n\t\t}\n\t}\n}\nfunc (m *MessageMetrics) Tx(t header.MessageType, s header.MessageSubType, i int64) {\n\tif m != nil {\n\t\tif t >= 0 && int(t) < len(m.tx) && s >= 0 && int(s) < len(m.tx[t]) {\n\t\t\tm.tx[t][s].Inc(i)\n\t\t} else if m.txUnknown != nil {\n\t\t\tm.txUnknown.Inc(i)\n\t\t}\n\t}\n}\n\nfunc newMessageMetrics() *MessageMetrics {\n\tgen := func(t string) [][]metrics.Counter {\n\t\treturn [][]metrics.Counter{\n\t\t\t{\n\t\t\t\tmetrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.handshake_ixpsk0\", t), nil),\n\t\t\t},\n\t\t\tnil,\n\t\t\t{metrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.recv_error\", t), nil)},\n\t\t\t{metrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.lighthouse\", t), nil)},\n\t\t\t{\n\t\t\t\tmetrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.test_request\", t), nil),\n\t\t\t\tmetrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.test_response\", t), nil),\n\t\t\t},\n\t\t\t{metrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.close_tunnel\", t), nil)},\n\t\t}\n\t}\n\treturn &MessageMetrics{\n\t\trx: gen(\"rx\"),\n\t\ttx: gen(\"tx\"),\n\n\t\trxUnknown: metrics.GetOrRegisterCounter(\"messages.rx.other\", nil),\n\t\ttxUnknown: metrics.GetOrRegisterCounter(\"messages.tx.other\", nil),\n\t}\n}\n\n// Historically we only recorded recv_error, so this is backwards compat\nfunc newMessageMetricsOnlyRecvError() *MessageMetrics {\n\tgen := func(t string) [][]metrics.Counter {\n\t\treturn [][]metrics.Counter{\n\t\t\tnil,\n\t\t\tnil,\n\t\t\t{metrics.GetOrRegisterCounter(fmt.Sprintf(\"messages.%s.recv_error\", t), nil)},\n\t\t}\n\t}\n\treturn &MessageMetrics{\n\t\trx: gen(\"rx\"),\n\t\ttx: gen(\"tx\"),\n\t}\n}\n\nfunc newLighthouseMetrics() *MessageMetrics {\n\tgen := func(t string) [][]metrics.Counter {\n\t\th := make([][]metrics.Counter, len(NebulaMeta_MessageType_name))\n\t\tused := []NebulaMeta_MessageType{\n\t\t\tNebulaMeta_HostQuery,\n\t\t\tNebulaMeta_HostQueryReply,\n\t\t\tNebulaMeta_HostUpdateNotification,\n\t\t\tNebulaMeta_HostPunchNotification,\n\t\t\tNebulaMeta_HostUpdateNotificationAck,\n\t\t}\n\t\tfor _, i := range used {\n\t\t\th[i] = []metrics.Counter{metrics.GetOrRegisterCounter(fmt.Sprintf(\"lighthouse.%s.%s\", t, i.String()), nil)}\n\t\t}\n\t\treturn h\n\t}\n\treturn &MessageMetrics{\n\t\trx: gen(\"rx\"),\n\t\ttx: gen(\"tx\"),\n\n\t\trxUnknown: metrics.GetOrRegisterCounter(\"lighthouse.rx.other\", nil),\n\t\ttxUnknown: metrics.GetOrRegisterCounter(\"lighthouse.tx.other\", nil),\n\t}\n}\n"
        },
        {
          "name": "metadata.go",
          "type": "blob",
          "size": 0.251953125,
          "content": "package nebula\n\n/*\n\nimport (\n\tproto \"google.golang.org/protobuf/proto\"\n)\n\nfunc HandleMetaProto(p []byte) {\n\tm := &NebulaMeta{}\n\terr := proto.Unmarshal(p, m)\n\tif err != nil {\n\t\tl.Debugf(\"problem unmarshaling meta message: %s\", err)\n\t}\n\t//fmt.Println(m)\n}\n\n*/\n"
        },
        {
          "name": "nebula.pb.go",
          "type": "blob",
          "size": 56.232421875,
          "content": "// Code generated by protoc-gen-gogo. DO NOT EDIT.\n// source: nebula.proto\n\npackage nebula\n\nimport (\n\tfmt \"fmt\"\n\tproto \"github.com/gogo/protobuf/proto\"\n\tio \"io\"\n\tmath \"math\"\n\tmath_bits \"math/bits\"\n)\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.GoGoProtoPackageIsVersion3 // please upgrade the proto package\n\ntype NebulaMeta_MessageType int32\n\nconst (\n\tNebulaMeta_None                      NebulaMeta_MessageType = 0\n\tNebulaMeta_HostQuery                 NebulaMeta_MessageType = 1\n\tNebulaMeta_HostQueryReply            NebulaMeta_MessageType = 2\n\tNebulaMeta_HostUpdateNotification    NebulaMeta_MessageType = 3\n\tNebulaMeta_HostMovedNotification     NebulaMeta_MessageType = 4\n\tNebulaMeta_HostPunchNotification     NebulaMeta_MessageType = 5\n\tNebulaMeta_HostWhoami                NebulaMeta_MessageType = 6\n\tNebulaMeta_HostWhoamiReply           NebulaMeta_MessageType = 7\n\tNebulaMeta_PathCheck                 NebulaMeta_MessageType = 8\n\tNebulaMeta_PathCheckReply            NebulaMeta_MessageType = 9\n\tNebulaMeta_HostUpdateNotificationAck NebulaMeta_MessageType = 10\n)\n\nvar NebulaMeta_MessageType_name = map[int32]string{\n\t0:  \"None\",\n\t1:  \"HostQuery\",\n\t2:  \"HostQueryReply\",\n\t3:  \"HostUpdateNotification\",\n\t4:  \"HostMovedNotification\",\n\t5:  \"HostPunchNotification\",\n\t6:  \"HostWhoami\",\n\t7:  \"HostWhoamiReply\",\n\t8:  \"PathCheck\",\n\t9:  \"PathCheckReply\",\n\t10: \"HostUpdateNotificationAck\",\n}\n\nvar NebulaMeta_MessageType_value = map[string]int32{\n\t\"None\":                      0,\n\t\"HostQuery\":                 1,\n\t\"HostQueryReply\":            2,\n\t\"HostUpdateNotification\":    3,\n\t\"HostMovedNotification\":     4,\n\t\"HostPunchNotification\":     5,\n\t\"HostWhoami\":                6,\n\t\"HostWhoamiReply\":           7,\n\t\"PathCheck\":                 8,\n\t\"PathCheckReply\":            9,\n\t\"HostUpdateNotificationAck\": 10,\n}\n\nfunc (x NebulaMeta_MessageType) String() string {\n\treturn proto.EnumName(NebulaMeta_MessageType_name, int32(x))\n}\n\nfunc (NebulaMeta_MessageType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{0, 0}\n}\n\ntype NebulaPing_MessageType int32\n\nconst (\n\tNebulaPing_Ping  NebulaPing_MessageType = 0\n\tNebulaPing_Reply NebulaPing_MessageType = 1\n)\n\nvar NebulaPing_MessageType_name = map[int32]string{\n\t0: \"Ping\",\n\t1: \"Reply\",\n}\n\nvar NebulaPing_MessageType_value = map[string]int32{\n\t\"Ping\":  0,\n\t\"Reply\": 1,\n}\n\nfunc (x NebulaPing_MessageType) String() string {\n\treturn proto.EnumName(NebulaPing_MessageType_name, int32(x))\n}\n\nfunc (NebulaPing_MessageType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{4, 0}\n}\n\ntype NebulaControl_MessageType int32\n\nconst (\n\tNebulaControl_None                NebulaControl_MessageType = 0\n\tNebulaControl_CreateRelayRequest  NebulaControl_MessageType = 1\n\tNebulaControl_CreateRelayResponse NebulaControl_MessageType = 2\n)\n\nvar NebulaControl_MessageType_name = map[int32]string{\n\t0: \"None\",\n\t1: \"CreateRelayRequest\",\n\t2: \"CreateRelayResponse\",\n}\n\nvar NebulaControl_MessageType_value = map[string]int32{\n\t\"None\":                0,\n\t\"CreateRelayRequest\":  1,\n\t\"CreateRelayResponse\": 2,\n}\n\nfunc (x NebulaControl_MessageType) String() string {\n\treturn proto.EnumName(NebulaControl_MessageType_name, int32(x))\n}\n\nfunc (NebulaControl_MessageType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{7, 0}\n}\n\ntype NebulaMeta struct {\n\tType    NebulaMeta_MessageType `protobuf:\"varint,1,opt,name=Type,proto3,enum=nebula.NebulaMeta_MessageType\" json:\"Type,omitempty\"`\n\tDetails *NebulaMetaDetails     `protobuf:\"bytes,2,opt,name=Details,proto3\" json:\"Details,omitempty\"`\n}\n\nfunc (m *NebulaMeta) Reset()         { *m = NebulaMeta{} }\nfunc (m *NebulaMeta) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaMeta) ProtoMessage()    {}\nfunc (*NebulaMeta) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{0}\n}\nfunc (m *NebulaMeta) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaMeta) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaMeta.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaMeta) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaMeta.Merge(m, src)\n}\nfunc (m *NebulaMeta) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaMeta) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaMeta.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaMeta proto.InternalMessageInfo\n\nfunc (m *NebulaMeta) GetType() NebulaMeta_MessageType {\n\tif m != nil {\n\t\treturn m.Type\n\t}\n\treturn NebulaMeta_None\n}\n\nfunc (m *NebulaMeta) GetDetails() *NebulaMetaDetails {\n\tif m != nil {\n\t\treturn m.Details\n\t}\n\treturn nil\n}\n\ntype NebulaMetaDetails struct {\n\tVpnIp       uint32        `protobuf:\"varint,1,opt,name=VpnIp,proto3\" json:\"VpnIp,omitempty\"`\n\tIp4AndPorts []*Ip4AndPort `protobuf:\"bytes,2,rep,name=Ip4AndPorts,proto3\" json:\"Ip4AndPorts,omitempty\"`\n\tIp6AndPorts []*Ip6AndPort `protobuf:\"bytes,4,rep,name=Ip6AndPorts,proto3\" json:\"Ip6AndPorts,omitempty\"`\n\tRelayVpnIp  []uint32      `protobuf:\"varint,5,rep,packed,name=RelayVpnIp,proto3\" json:\"RelayVpnIp,omitempty\"`\n\tCounter     uint32        `protobuf:\"varint,3,opt,name=counter,proto3\" json:\"counter,omitempty\"`\n}\n\nfunc (m *NebulaMetaDetails) Reset()         { *m = NebulaMetaDetails{} }\nfunc (m *NebulaMetaDetails) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaMetaDetails) ProtoMessage()    {}\nfunc (*NebulaMetaDetails) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{1}\n}\nfunc (m *NebulaMetaDetails) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaMetaDetails) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaMetaDetails.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaMetaDetails) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaMetaDetails.Merge(m, src)\n}\nfunc (m *NebulaMetaDetails) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaMetaDetails) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaMetaDetails.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaMetaDetails proto.InternalMessageInfo\n\nfunc (m *NebulaMetaDetails) GetVpnIp() uint32 {\n\tif m != nil {\n\t\treturn m.VpnIp\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaMetaDetails) GetIp4AndPorts() []*Ip4AndPort {\n\tif m != nil {\n\t\treturn m.Ip4AndPorts\n\t}\n\treturn nil\n}\n\nfunc (m *NebulaMetaDetails) GetIp6AndPorts() []*Ip6AndPort {\n\tif m != nil {\n\t\treturn m.Ip6AndPorts\n\t}\n\treturn nil\n}\n\nfunc (m *NebulaMetaDetails) GetRelayVpnIp() []uint32 {\n\tif m != nil {\n\t\treturn m.RelayVpnIp\n\t}\n\treturn nil\n}\n\nfunc (m *NebulaMetaDetails) GetCounter() uint32 {\n\tif m != nil {\n\t\treturn m.Counter\n\t}\n\treturn 0\n}\n\ntype Ip4AndPort struct {\n\tIp   uint32 `protobuf:\"varint,1,opt,name=Ip,proto3\" json:\"Ip,omitempty\"`\n\tPort uint32 `protobuf:\"varint,2,opt,name=Port,proto3\" json:\"Port,omitempty\"`\n}\n\nfunc (m *Ip4AndPort) Reset()         { *m = Ip4AndPort{} }\nfunc (m *Ip4AndPort) String() string { return proto.CompactTextString(m) }\nfunc (*Ip4AndPort) ProtoMessage()    {}\nfunc (*Ip4AndPort) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{2}\n}\nfunc (m *Ip4AndPort) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *Ip4AndPort) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_Ip4AndPort.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *Ip4AndPort) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Ip4AndPort.Merge(m, src)\n}\nfunc (m *Ip4AndPort) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *Ip4AndPort) XXX_DiscardUnknown() {\n\txxx_messageInfo_Ip4AndPort.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Ip4AndPort proto.InternalMessageInfo\n\nfunc (m *Ip4AndPort) GetIp() uint32 {\n\tif m != nil {\n\t\treturn m.Ip\n\t}\n\treturn 0\n}\n\nfunc (m *Ip4AndPort) GetPort() uint32 {\n\tif m != nil {\n\t\treturn m.Port\n\t}\n\treturn 0\n}\n\ntype Ip6AndPort struct {\n\tHi   uint64 `protobuf:\"varint,1,opt,name=Hi,proto3\" json:\"Hi,omitempty\"`\n\tLo   uint64 `protobuf:\"varint,2,opt,name=Lo,proto3\" json:\"Lo,omitempty\"`\n\tPort uint32 `protobuf:\"varint,3,opt,name=Port,proto3\" json:\"Port,omitempty\"`\n}\n\nfunc (m *Ip6AndPort) Reset()         { *m = Ip6AndPort{} }\nfunc (m *Ip6AndPort) String() string { return proto.CompactTextString(m) }\nfunc (*Ip6AndPort) ProtoMessage()    {}\nfunc (*Ip6AndPort) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{3}\n}\nfunc (m *Ip6AndPort) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *Ip6AndPort) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_Ip6AndPort.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *Ip6AndPort) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Ip6AndPort.Merge(m, src)\n}\nfunc (m *Ip6AndPort) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *Ip6AndPort) XXX_DiscardUnknown() {\n\txxx_messageInfo_Ip6AndPort.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Ip6AndPort proto.InternalMessageInfo\n\nfunc (m *Ip6AndPort) GetHi() uint64 {\n\tif m != nil {\n\t\treturn m.Hi\n\t}\n\treturn 0\n}\n\nfunc (m *Ip6AndPort) GetLo() uint64 {\n\tif m != nil {\n\t\treturn m.Lo\n\t}\n\treturn 0\n}\n\nfunc (m *Ip6AndPort) GetPort() uint32 {\n\tif m != nil {\n\t\treturn m.Port\n\t}\n\treturn 0\n}\n\ntype NebulaPing struct {\n\tType NebulaPing_MessageType `protobuf:\"varint,1,opt,name=Type,proto3,enum=nebula.NebulaPing_MessageType\" json:\"Type,omitempty\"`\n\tTime uint64                 `protobuf:\"varint,2,opt,name=Time,proto3\" json:\"Time,omitempty\"`\n}\n\nfunc (m *NebulaPing) Reset()         { *m = NebulaPing{} }\nfunc (m *NebulaPing) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaPing) ProtoMessage()    {}\nfunc (*NebulaPing) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{4}\n}\nfunc (m *NebulaPing) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaPing) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaPing.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaPing) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaPing.Merge(m, src)\n}\nfunc (m *NebulaPing) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaPing) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaPing.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaPing proto.InternalMessageInfo\n\nfunc (m *NebulaPing) GetType() NebulaPing_MessageType {\n\tif m != nil {\n\t\treturn m.Type\n\t}\n\treturn NebulaPing_Ping\n}\n\nfunc (m *NebulaPing) GetTime() uint64 {\n\tif m != nil {\n\t\treturn m.Time\n\t}\n\treturn 0\n}\n\ntype NebulaHandshake struct {\n\tDetails *NebulaHandshakeDetails `protobuf:\"bytes,1,opt,name=Details,proto3\" json:\"Details,omitempty\"`\n\tHmac    []byte                  `protobuf:\"bytes,2,opt,name=Hmac,proto3\" json:\"Hmac,omitempty\"`\n}\n\nfunc (m *NebulaHandshake) Reset()         { *m = NebulaHandshake{} }\nfunc (m *NebulaHandshake) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaHandshake) ProtoMessage()    {}\nfunc (*NebulaHandshake) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{5}\n}\nfunc (m *NebulaHandshake) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaHandshake) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaHandshake.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaHandshake) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaHandshake.Merge(m, src)\n}\nfunc (m *NebulaHandshake) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaHandshake) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaHandshake.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaHandshake proto.InternalMessageInfo\n\nfunc (m *NebulaHandshake) GetDetails() *NebulaHandshakeDetails {\n\tif m != nil {\n\t\treturn m.Details\n\t}\n\treturn nil\n}\n\nfunc (m *NebulaHandshake) GetHmac() []byte {\n\tif m != nil {\n\t\treturn m.Hmac\n\t}\n\treturn nil\n}\n\ntype NebulaHandshakeDetails struct {\n\tCert           []byte `protobuf:\"bytes,1,opt,name=Cert,proto3\" json:\"Cert,omitempty\"`\n\tInitiatorIndex uint32 `protobuf:\"varint,2,opt,name=InitiatorIndex,proto3\" json:\"InitiatorIndex,omitempty\"`\n\tResponderIndex uint32 `protobuf:\"varint,3,opt,name=ResponderIndex,proto3\" json:\"ResponderIndex,omitempty\"`\n\tCookie         uint64 `protobuf:\"varint,4,opt,name=Cookie,proto3\" json:\"Cookie,omitempty\"`\n\tTime           uint64 `protobuf:\"varint,5,opt,name=Time,proto3\" json:\"Time,omitempty\"`\n}\n\nfunc (m *NebulaHandshakeDetails) Reset()         { *m = NebulaHandshakeDetails{} }\nfunc (m *NebulaHandshakeDetails) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaHandshakeDetails) ProtoMessage()    {}\nfunc (*NebulaHandshakeDetails) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{6}\n}\nfunc (m *NebulaHandshakeDetails) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaHandshakeDetails) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaHandshakeDetails.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaHandshakeDetails) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaHandshakeDetails.Merge(m, src)\n}\nfunc (m *NebulaHandshakeDetails) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaHandshakeDetails) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaHandshakeDetails.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaHandshakeDetails proto.InternalMessageInfo\n\nfunc (m *NebulaHandshakeDetails) GetCert() []byte {\n\tif m != nil {\n\t\treturn m.Cert\n\t}\n\treturn nil\n}\n\nfunc (m *NebulaHandshakeDetails) GetInitiatorIndex() uint32 {\n\tif m != nil {\n\t\treturn m.InitiatorIndex\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaHandshakeDetails) GetResponderIndex() uint32 {\n\tif m != nil {\n\t\treturn m.ResponderIndex\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaHandshakeDetails) GetCookie() uint64 {\n\tif m != nil {\n\t\treturn m.Cookie\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaHandshakeDetails) GetTime() uint64 {\n\tif m != nil {\n\t\treturn m.Time\n\t}\n\treturn 0\n}\n\ntype NebulaControl struct {\n\tType                NebulaControl_MessageType `protobuf:\"varint,1,opt,name=Type,proto3,enum=nebula.NebulaControl_MessageType\" json:\"Type,omitempty\"`\n\tInitiatorRelayIndex uint32                    `protobuf:\"varint,2,opt,name=InitiatorRelayIndex,proto3\" json:\"InitiatorRelayIndex,omitempty\"`\n\tResponderRelayIndex uint32                    `protobuf:\"varint,3,opt,name=ResponderRelayIndex,proto3\" json:\"ResponderRelayIndex,omitempty\"`\n\tRelayToIp           uint32                    `protobuf:\"varint,4,opt,name=RelayToIp,proto3\" json:\"RelayToIp,omitempty\"`\n\tRelayFromIp         uint32                    `protobuf:\"varint,5,opt,name=RelayFromIp,proto3\" json:\"RelayFromIp,omitempty\"`\n}\n\nfunc (m *NebulaControl) Reset()         { *m = NebulaControl{} }\nfunc (m *NebulaControl) String() string { return proto.CompactTextString(m) }\nfunc (*NebulaControl) ProtoMessage()    {}\nfunc (*NebulaControl) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_2d65afa7693df5ef, []int{7}\n}\nfunc (m *NebulaControl) XXX_Unmarshal(b []byte) error {\n\treturn m.Unmarshal(b)\n}\nfunc (m *NebulaControl) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\tif deterministic {\n\t\treturn xxx_messageInfo_NebulaControl.Marshal(b, m, deterministic)\n\t} else {\n\t\tb = b[:cap(b)]\n\t\tn, err := m.MarshalToSizedBuffer(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:n], nil\n\t}\n}\nfunc (m *NebulaControl) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_NebulaControl.Merge(m, src)\n}\nfunc (m *NebulaControl) XXX_Size() int {\n\treturn m.Size()\n}\nfunc (m *NebulaControl) XXX_DiscardUnknown() {\n\txxx_messageInfo_NebulaControl.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_NebulaControl proto.InternalMessageInfo\n\nfunc (m *NebulaControl) GetType() NebulaControl_MessageType {\n\tif m != nil {\n\t\treturn m.Type\n\t}\n\treturn NebulaControl_None\n}\n\nfunc (m *NebulaControl) GetInitiatorRelayIndex() uint32 {\n\tif m != nil {\n\t\treturn m.InitiatorRelayIndex\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaControl) GetResponderRelayIndex() uint32 {\n\tif m != nil {\n\t\treturn m.ResponderRelayIndex\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaControl) GetRelayToIp() uint32 {\n\tif m != nil {\n\t\treturn m.RelayToIp\n\t}\n\treturn 0\n}\n\nfunc (m *NebulaControl) GetRelayFromIp() uint32 {\n\tif m != nil {\n\t\treturn m.RelayFromIp\n\t}\n\treturn 0\n}\n\nfunc init() {\n\tproto.RegisterEnum(\"nebula.NebulaMeta_MessageType\", NebulaMeta_MessageType_name, NebulaMeta_MessageType_value)\n\tproto.RegisterEnum(\"nebula.NebulaPing_MessageType\", NebulaPing_MessageType_name, NebulaPing_MessageType_value)\n\tproto.RegisterEnum(\"nebula.NebulaControl_MessageType\", NebulaControl_MessageType_name, NebulaControl_MessageType_value)\n\tproto.RegisterType((*NebulaMeta)(nil), \"nebula.NebulaMeta\")\n\tproto.RegisterType((*NebulaMetaDetails)(nil), \"nebula.NebulaMetaDetails\")\n\tproto.RegisterType((*Ip4AndPort)(nil), \"nebula.Ip4AndPort\")\n\tproto.RegisterType((*Ip6AndPort)(nil), \"nebula.Ip6AndPort\")\n\tproto.RegisterType((*NebulaPing)(nil), \"nebula.NebulaPing\")\n\tproto.RegisterType((*NebulaHandshake)(nil), \"nebula.NebulaHandshake\")\n\tproto.RegisterType((*NebulaHandshakeDetails)(nil), \"nebula.NebulaHandshakeDetails\")\n\tproto.RegisterType((*NebulaControl)(nil), \"nebula.NebulaControl\")\n}\n\nfunc init() { proto.RegisterFile(\"nebula.proto\", fileDescriptor_2d65afa7693df5ef) }\n\nvar fileDescriptor_2d65afa7693df5ef = []byte{\n\t// 707 bytes of a gzipped FileDescriptorProto\n\t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x7c, 0x54, 0x4d, 0x6f, 0xda, 0x4a,\n\t0x14, 0xc5, 0xc6, 0x7c, 0x5d, 0x02, 0xf1, 0xbb, 0x79, 0x8f, 0x07, 0x4f, 0xaf, 0x16, 0xf5, 0xa2,\n\t0x62, 0x45, 0x22, 0x92, 0x46, 0x5d, 0x36, 0xa5, 0xaa, 0x20, 0x4a, 0x22, 0x3a, 0x4a, 0x5b, 0xa9,\n\t0x9b, 0x6a, 0x62, 0xa6, 0xc1, 0x02, 0x3c, 0x8e, 0x3d, 0x54, 0xe1, 0x5f, 0xf4, 0xc7, 0xe4, 0x47,\n\t0x74, 0xd7, 0x2c, 0xbb, 0xac, 0x92, 0x65, 0x97, 0xfd, 0x03, 0xd5, 0x8c, 0xc1, 0x36, 0x84, 0x76,\n\t0x37, 0xe7, 0xde, 0x73, 0x66, 0xce, 0x9c, 0xb9, 0x36, 0x6c, 0x79, 0xec, 0x62, 0x36, 0xa1, 0x6d,\n\t0x3f, 0xe0, 0x82, 0x63, 0x3e, 0x42, 0xf6, 0x0f, 0x1d, 0xe0, 0x4c, 0x2d, 0x4f, 0x99, 0xa0, 0xd8,\n\t0x01, 0xe3, 0x7c, 0xee, 0xb3, 0xba, 0xd6, 0xd4, 0x5a, 0xd5, 0x8e, 0xd5, 0x5e, 0x68, 0x12, 0x46,\n\t0xfb, 0x94, 0x85, 0x21, 0xbd, 0x64, 0x92, 0x45, 0x14, 0x17, 0xf7, 0xa1, 0xf0, 0x92, 0x09, 0xea,\n\t0x4e, 0xc2, 0xba, 0xde, 0xd4, 0x5a, 0xe5, 0x4e, 0xe3, 0xa1, 0x6c, 0x41, 0x20, 0x4b, 0xa6, 0xfd,\n\t0x53, 0x83, 0x72, 0x6a, 0x2b, 0x2c, 0x82, 0x71, 0xc6, 0x3d, 0x66, 0x66, 0xb0, 0x02, 0xa5, 0x1e,\n\t0x0f, 0xc5, 0xeb, 0x19, 0x0b, 0xe6, 0xa6, 0x86, 0x08, 0xd5, 0x18, 0x12, 0xe6, 0x4f, 0xe6, 0xa6,\n\t0x8e, 0xff, 0x41, 0x4d, 0xd6, 0xde, 0xf8, 0x43, 0x2a, 0xd8, 0x19, 0x17, 0xee, 0x47, 0xd7, 0xa1,\n\t0xc2, 0xe5, 0x9e, 0x99, 0xc5, 0x06, 0xfc, 0x23, 0x7b, 0xa7, 0xfc, 0x13, 0x1b, 0xae, 0xb4, 0x8c,\n\t0x65, 0x6b, 0x30, 0xf3, 0x9c, 0xd1, 0x4a, 0x2b, 0x87, 0x55, 0x00, 0xd9, 0x7a, 0x37, 0xe2, 0x74,\n\t0xea, 0x9a, 0x79, 0xdc, 0x81, 0xed, 0x04, 0x47, 0xc7, 0x16, 0xa4, 0xb3, 0x01, 0x15, 0xa3, 0xee,\n\t0x88, 0x39, 0x63, 0xb3, 0x28, 0x9d, 0xc5, 0x30, 0xa2, 0x94, 0xf0, 0x11, 0x34, 0x36, 0x3b, 0x3b,\n\t0x72, 0xc6, 0x26, 0xd8, 0x5f, 0x35, 0xf8, 0xeb, 0x41, 0x28, 0xf8, 0x37, 0xe4, 0xde, 0xfa, 0x5e,\n\t0xdf, 0x57, 0xa9, 0x57, 0x48, 0x04, 0xf0, 0x00, 0xca, 0x7d, 0xff, 0xe0, 0xc8, 0x1b, 0x0e, 0x78,\n\t0x20, 0x64, 0xb4, 0xd9, 0x56, 0xb9, 0x83, 0xcb, 0x68, 0x93, 0x16, 0x49, 0xd3, 0x22, 0xd5, 0x61,\n\t0xac, 0x32, 0xd6, 0x55, 0x87, 0x29, 0x55, 0x4c, 0x43, 0x0b, 0x80, 0xb0, 0x09, 0x9d, 0x47, 0x36,\n\t0x72, 0xcd, 0x6c, 0xab, 0x42, 0x52, 0x15, 0xac, 0x43, 0xc1, 0xe1, 0x33, 0x4f, 0xb0, 0xa0, 0x9e,\n\t0x55, 0x1e, 0x97, 0xd0, 0xde, 0x03, 0x48, 0x8e, 0xc7, 0x2a, 0xe8, 0xf1, 0x35, 0xf4, 0xbe, 0x8f,\n\t0x08, 0x86, 0xac, 0xab, 0xb9, 0xa8, 0x10, 0xb5, 0xb6, 0x9f, 0x4b, 0xc5, 0x61, 0x4a, 0xd1, 0x73,\n\t0x95, 0xc2, 0x20, 0x7a, 0xcf, 0x95, 0xf8, 0x84, 0x2b, 0xbe, 0x41, 0xf4, 0x13, 0x1e, 0xef, 0x90,\n\t0x4d, 0xed, 0x70, 0xbd, 0x1c, 0xd9, 0x81, 0xeb, 0x5d, 0xfe, 0x79, 0x64, 0x25, 0x63, 0xc3, 0xc8,\n\t0x22, 0x18, 0xe7, 0xee, 0x94, 0x2d, 0xce, 0x51, 0x6b, 0xdb, 0x7e, 0x30, 0x90, 0x52, 0x6c, 0x66,\n\t0xb0, 0x04, 0xb9, 0xe8, 0x79, 0x35, 0xfb, 0x03, 0x6c, 0x47, 0xfb, 0xf6, 0xa8, 0x37, 0x0c, 0x47,\n\t0x74, 0xcc, 0xf0, 0x59, 0x32, 0xfd, 0x9a, 0x9a, 0xfe, 0x35, 0x07, 0x31, 0x73, 0xfd, 0x13, 0x90,\n\t0x26, 0x7a, 0x53, 0xea, 0x28, 0x13, 0x5b, 0x44, 0xad, 0xed, 0x1b, 0x0d, 0x6a, 0x9b, 0x75, 0x92,\n\t0xde, 0x65, 0x81, 0x50, 0xa7, 0x6c, 0x11, 0xb5, 0xc6, 0x27, 0x50, 0xed, 0x7b, 0xae, 0x70, 0xa9,\n\t0xe0, 0x41, 0xdf, 0x1b, 0xb2, 0xeb, 0x45, 0xd2, 0x6b, 0x55, 0xc9, 0x23, 0x2c, 0xf4, 0xb9, 0x37,\n\t0x64, 0x0b, 0x5e, 0x94, 0xe7, 0x5a, 0x15, 0x6b, 0x90, 0xef, 0x72, 0x3e, 0x76, 0x59, 0xdd, 0x50,\n\t0xc9, 0x2c, 0x50, 0x9c, 0x57, 0x2e, 0xc9, 0xeb, 0xd8, 0x28, 0xe6, 0xcd, 0xc2, 0xb1, 0x51, 0x2c,\n\t0x98, 0x45, 0xfb, 0x46, 0x87, 0x4a, 0x64, 0xbb, 0xcb, 0x3d, 0x11, 0xf0, 0x09, 0x3e, 0x5d, 0x79,\n\t0x95, 0xc7, 0xab, 0x99, 0x2c, 0x48, 0x1b, 0x1e, 0x66, 0x0f, 0x76, 0x62, 0xeb, 0x6a, 0xfe, 0xd2,\n\t0xb7, 0xda, 0xd4, 0x92, 0x8a, 0xf8, 0x12, 0x29, 0x45, 0x74, 0xbf, 0x4d, 0x2d, 0xfc, 0x1f, 0x4a,\n\t0x0a, 0x9d, 0xf3, 0xbe, 0xaf, 0xee, 0x59, 0x21, 0x49, 0x01, 0x9b, 0x50, 0x56, 0xe0, 0x55, 0xc0,\n\t0xa7, 0xea, 0x5b, 0x90, 0xfd, 0x74, 0xc9, 0xee, 0xfd, 0xee, 0xcf, 0x55, 0x03, 0xec, 0x06, 0x8c,\n\t0x0a, 0xa6, 0xd8, 0x84, 0x5d, 0xcd, 0x58, 0x28, 0x4c, 0x0d, 0xff, 0x85, 0x9d, 0x95, 0xba, 0xb4,\n\t0x14, 0x32, 0x53, 0x7f, 0xb1, 0xff, 0xe5, 0xce, 0xd2, 0x6e, 0xef, 0x2c, 0xed, 0xfb, 0x9d, 0xa5,\n\t0x7d, 0xbe, 0xb7, 0x32, 0xb7, 0xf7, 0x56, 0xe6, 0xdb, 0xbd, 0x95, 0x79, 0xdf, 0xb8, 0x74, 0xc5,\n\t0x68, 0x76, 0xd1, 0x76, 0xf8, 0x74, 0x37, 0x9c, 0x50, 0x67, 0x3c, 0xba, 0xda, 0x8d, 0x22, 0xbc,\n\t0xc8, 0xab, 0x1f, 0xf8, 0xfe, 0xaf, 0x00, 0x00, 0x00, 0xff, 0xff, 0x17, 0x56, 0x28, 0x74, 0xd0,\n\t0x05, 0x00, 0x00,\n}\n\nfunc (m *NebulaMeta) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaMeta) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaMeta) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.Details != nil {\n\t\t{\n\t\t\tsize, err := m.Details.MarshalToSizedBuffer(dAtA[:i])\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\ti -= size\n\t\t\ti = encodeVarintNebula(dAtA, i, uint64(size))\n\t\t}\n\t\ti--\n\t\tdAtA[i] = 0x12\n\t}\n\tif m.Type != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Type))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *NebulaMetaDetails) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaMetaDetails) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaMetaDetails) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif len(m.RelayVpnIp) > 0 {\n\t\tdAtA3 := make([]byte, len(m.RelayVpnIp)*10)\n\t\tvar j2 int\n\t\tfor _, num := range m.RelayVpnIp {\n\t\t\tfor num >= 1<<7 {\n\t\t\t\tdAtA3[j2] = uint8(uint64(num)&0x7f | 0x80)\n\t\t\t\tnum >>= 7\n\t\t\t\tj2++\n\t\t\t}\n\t\t\tdAtA3[j2] = uint8(num)\n\t\t\tj2++\n\t\t}\n\t\ti -= j2\n\t\tcopy(dAtA[i:], dAtA3[:j2])\n\t\ti = encodeVarintNebula(dAtA, i, uint64(j2))\n\t\ti--\n\t\tdAtA[i] = 0x2a\n\t}\n\tif len(m.Ip6AndPorts) > 0 {\n\t\tfor iNdEx := len(m.Ip6AndPorts) - 1; iNdEx >= 0; iNdEx-- {\n\t\t\t{\n\t\t\t\tsize, err := m.Ip6AndPorts[iNdEx].MarshalToSizedBuffer(dAtA[:i])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\t\ti -= size\n\t\t\t\ti = encodeVarintNebula(dAtA, i, uint64(size))\n\t\t\t}\n\t\t\ti--\n\t\t\tdAtA[i] = 0x22\n\t\t}\n\t}\n\tif m.Counter != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Counter))\n\t\ti--\n\t\tdAtA[i] = 0x18\n\t}\n\tif len(m.Ip4AndPorts) > 0 {\n\t\tfor iNdEx := len(m.Ip4AndPorts) - 1; iNdEx >= 0; iNdEx-- {\n\t\t\t{\n\t\t\t\tsize, err := m.Ip4AndPorts[iNdEx].MarshalToSizedBuffer(dAtA[:i])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\t\ti -= size\n\t\t\t\ti = encodeVarintNebula(dAtA, i, uint64(size))\n\t\t\t}\n\t\t\ti--\n\t\t\tdAtA[i] = 0x12\n\t\t}\n\t}\n\tif m.VpnIp != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.VpnIp))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *Ip4AndPort) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *Ip4AndPort) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *Ip4AndPort) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.Port != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Port))\n\t\ti--\n\t\tdAtA[i] = 0x10\n\t}\n\tif m.Ip != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Ip))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *Ip6AndPort) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *Ip6AndPort) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *Ip6AndPort) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.Port != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Port))\n\t\ti--\n\t\tdAtA[i] = 0x18\n\t}\n\tif m.Lo != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Lo))\n\t\ti--\n\t\tdAtA[i] = 0x10\n\t}\n\tif m.Hi != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Hi))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *NebulaPing) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaPing) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaPing) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.Time != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Time))\n\t\ti--\n\t\tdAtA[i] = 0x10\n\t}\n\tif m.Type != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Type))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *NebulaHandshake) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaHandshake) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaHandshake) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif len(m.Hmac) > 0 {\n\t\ti -= len(m.Hmac)\n\t\tcopy(dAtA[i:], m.Hmac)\n\t\ti = encodeVarintNebula(dAtA, i, uint64(len(m.Hmac)))\n\t\ti--\n\t\tdAtA[i] = 0x12\n\t}\n\tif m.Details != nil {\n\t\t{\n\t\t\tsize, err := m.Details.MarshalToSizedBuffer(dAtA[:i])\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\ti -= size\n\t\t\ti = encodeVarintNebula(dAtA, i, uint64(size))\n\t\t}\n\t\ti--\n\t\tdAtA[i] = 0xa\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *NebulaHandshakeDetails) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaHandshakeDetails) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaHandshakeDetails) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.Time != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Time))\n\t\ti--\n\t\tdAtA[i] = 0x28\n\t}\n\tif m.Cookie != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Cookie))\n\t\ti--\n\t\tdAtA[i] = 0x20\n\t}\n\tif m.ResponderIndex != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.ResponderIndex))\n\t\ti--\n\t\tdAtA[i] = 0x18\n\t}\n\tif m.InitiatorIndex != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.InitiatorIndex))\n\t\ti--\n\t\tdAtA[i] = 0x10\n\t}\n\tif len(m.Cert) > 0 {\n\t\ti -= len(m.Cert)\n\t\tcopy(dAtA[i:], m.Cert)\n\t\ti = encodeVarintNebula(dAtA, i, uint64(len(m.Cert)))\n\t\ti--\n\t\tdAtA[i] = 0xa\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc (m *NebulaControl) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalToSizedBuffer(dAtA[:size])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *NebulaControl) MarshalTo(dAtA []byte) (int, error) {\n\tsize := m.Size()\n\treturn m.MarshalToSizedBuffer(dAtA[:size])\n}\n\nfunc (m *NebulaControl) MarshalToSizedBuffer(dAtA []byte) (int, error) {\n\ti := len(dAtA)\n\t_ = i\n\tvar l int\n\t_ = l\n\tif m.RelayFromIp != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.RelayFromIp))\n\t\ti--\n\t\tdAtA[i] = 0x28\n\t}\n\tif m.RelayToIp != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.RelayToIp))\n\t\ti--\n\t\tdAtA[i] = 0x20\n\t}\n\tif m.ResponderRelayIndex != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.ResponderRelayIndex))\n\t\ti--\n\t\tdAtA[i] = 0x18\n\t}\n\tif m.InitiatorRelayIndex != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.InitiatorRelayIndex))\n\t\ti--\n\t\tdAtA[i] = 0x10\n\t}\n\tif m.Type != 0 {\n\t\ti = encodeVarintNebula(dAtA, i, uint64(m.Type))\n\t\ti--\n\t\tdAtA[i] = 0x8\n\t}\n\treturn len(dAtA) - i, nil\n}\n\nfunc encodeVarintNebula(dAtA []byte, offset int, v uint64) int {\n\toffset -= sovNebula(v)\n\tbase := offset\n\tfor v >= 1<<7 {\n\t\tdAtA[offset] = uint8(v&0x7f | 0x80)\n\t\tv >>= 7\n\t\toffset++\n\t}\n\tdAtA[offset] = uint8(v)\n\treturn base\n}\nfunc (m *NebulaMeta) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Type != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Type))\n\t}\n\tif m.Details != nil {\n\t\tl = m.Details.Size()\n\t\tn += 1 + l + sovNebula(uint64(l))\n\t}\n\treturn n\n}\n\nfunc (m *NebulaMetaDetails) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.VpnIp != 0 {\n\t\tn += 1 + sovNebula(uint64(m.VpnIp))\n\t}\n\tif len(m.Ip4AndPorts) > 0 {\n\t\tfor _, e := range m.Ip4AndPorts {\n\t\t\tl = e.Size()\n\t\t\tn += 1 + l + sovNebula(uint64(l))\n\t\t}\n\t}\n\tif m.Counter != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Counter))\n\t}\n\tif len(m.Ip6AndPorts) > 0 {\n\t\tfor _, e := range m.Ip6AndPorts {\n\t\t\tl = e.Size()\n\t\t\tn += 1 + l + sovNebula(uint64(l))\n\t\t}\n\t}\n\tif len(m.RelayVpnIp) > 0 {\n\t\tl = 0\n\t\tfor _, e := range m.RelayVpnIp {\n\t\t\tl += sovNebula(uint64(e))\n\t\t}\n\t\tn += 1 + sovNebula(uint64(l)) + l\n\t}\n\treturn n\n}\n\nfunc (m *Ip4AndPort) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Ip != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Ip))\n\t}\n\tif m.Port != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Port))\n\t}\n\treturn n\n}\n\nfunc (m *Ip6AndPort) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Hi != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Hi))\n\t}\n\tif m.Lo != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Lo))\n\t}\n\tif m.Port != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Port))\n\t}\n\treturn n\n}\n\nfunc (m *NebulaPing) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Type != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Type))\n\t}\n\tif m.Time != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Time))\n\t}\n\treturn n\n}\n\nfunc (m *NebulaHandshake) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Details != nil {\n\t\tl = m.Details.Size()\n\t\tn += 1 + l + sovNebula(uint64(l))\n\t}\n\tl = len(m.Hmac)\n\tif l > 0 {\n\t\tn += 1 + l + sovNebula(uint64(l))\n\t}\n\treturn n\n}\n\nfunc (m *NebulaHandshakeDetails) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tl = len(m.Cert)\n\tif l > 0 {\n\t\tn += 1 + l + sovNebula(uint64(l))\n\t}\n\tif m.InitiatorIndex != 0 {\n\t\tn += 1 + sovNebula(uint64(m.InitiatorIndex))\n\t}\n\tif m.ResponderIndex != 0 {\n\t\tn += 1 + sovNebula(uint64(m.ResponderIndex))\n\t}\n\tif m.Cookie != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Cookie))\n\t}\n\tif m.Time != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Time))\n\t}\n\treturn n\n}\n\nfunc (m *NebulaControl) Size() (n int) {\n\tif m == nil {\n\t\treturn 0\n\t}\n\tvar l int\n\t_ = l\n\tif m.Type != 0 {\n\t\tn += 1 + sovNebula(uint64(m.Type))\n\t}\n\tif m.InitiatorRelayIndex != 0 {\n\t\tn += 1 + sovNebula(uint64(m.InitiatorRelayIndex))\n\t}\n\tif m.ResponderRelayIndex != 0 {\n\t\tn += 1 + sovNebula(uint64(m.ResponderRelayIndex))\n\t}\n\tif m.RelayToIp != 0 {\n\t\tn += 1 + sovNebula(uint64(m.RelayToIp))\n\t}\n\tif m.RelayFromIp != 0 {\n\t\tn += 1 + sovNebula(uint64(m.RelayFromIp))\n\t}\n\treturn n\n}\n\nfunc sovNebula(x uint64) (n int) {\n\treturn (math_bits.Len64(x|1) + 6) / 7\n}\nfunc sozNebula(x uint64) (n int) {\n\treturn sovNebula(uint64((x << 1) ^ uint64((int64(x) >> 63))))\n}\nfunc (m *NebulaMeta) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaMeta: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaMeta: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Type\", wireType)\n\t\t\t}\n\t\t\tm.Type = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Type |= NebulaMeta_MessageType(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Details\", wireType)\n\t\t\t}\n\t\t\tvar msglen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tmsglen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif msglen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + msglen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tif m.Details == nil {\n\t\t\t\tm.Details = &NebulaMetaDetails{}\n\t\t\t}\n\t\t\tif err := m.Details.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *NebulaMetaDetails) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaMetaDetails: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaMetaDetails: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field VpnIp\", wireType)\n\t\t\t}\n\t\t\tm.VpnIp = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.VpnIp |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Ip4AndPorts\", wireType)\n\t\t\t}\n\t\t\tvar msglen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tmsglen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif msglen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + msglen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Ip4AndPorts = append(m.Ip4AndPorts, &Ip4AndPort{})\n\t\t\tif err := m.Ip4AndPorts[len(m.Ip4AndPorts)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tcase 3:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Counter\", wireType)\n\t\t\t}\n\t\t\tm.Counter = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Counter |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 4:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Ip6AndPorts\", wireType)\n\t\t\t}\n\t\t\tvar msglen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tmsglen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif msglen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + msglen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Ip6AndPorts = append(m.Ip6AndPorts, &Ip6AndPort{})\n\t\t\tif err := m.Ip6AndPorts[len(m.Ip6AndPorts)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tcase 5:\n\t\t\tif wireType == 0 {\n\t\t\t\tvar v uint32\n\t\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\t\tif shift >= 64 {\n\t\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t\t}\n\t\t\t\t\tif iNdEx >= l {\n\t\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t\t}\n\t\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\t\tiNdEx++\n\t\t\t\t\tv |= uint32(b&0x7F) << shift\n\t\t\t\t\tif b < 0x80 {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tm.RelayVpnIp = append(m.RelayVpnIp, v)\n\t\t\t} else if wireType == 2 {\n\t\t\t\tvar packedLen int\n\t\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\t\tif shift >= 64 {\n\t\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t\t}\n\t\t\t\t\tif iNdEx >= l {\n\t\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t\t}\n\t\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\t\tiNdEx++\n\t\t\t\t\tpackedLen |= int(b&0x7F) << shift\n\t\t\t\t\tif b < 0x80 {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif packedLen < 0 {\n\t\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t\t}\n\t\t\t\tpostIndex := iNdEx + packedLen\n\t\t\t\tif postIndex < 0 {\n\t\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t\t}\n\t\t\t\tif postIndex > l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tvar elementCount int\n\t\t\t\tvar count int\n\t\t\t\tfor _, integer := range dAtA[iNdEx:postIndex] {\n\t\t\t\t\tif integer < 128 {\n\t\t\t\t\t\tcount++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telementCount = count\n\t\t\t\tif elementCount != 0 && len(m.RelayVpnIp) == 0 {\n\t\t\t\t\tm.RelayVpnIp = make([]uint32, 0, elementCount)\n\t\t\t\t}\n\t\t\t\tfor iNdEx < postIndex {\n\t\t\t\t\tvar v uint32\n\t\t\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\t\t\tif shift >= 64 {\n\t\t\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif iNdEx >= l {\n\t\t\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\t\t\tiNdEx++\n\t\t\t\t\t\tv |= uint32(b&0x7F) << shift\n\t\t\t\t\t\tif b < 0x80 {\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tm.RelayVpnIp = append(m.RelayVpnIp, v)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field RelayVpnIp\", wireType)\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *Ip4AndPort) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: Ip4AndPort: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: Ip4AndPort: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Ip\", wireType)\n\t\t\t}\n\t\t\tm.Ip = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Ip |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Port\", wireType)\n\t\t\t}\n\t\t\tm.Port = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Port |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *Ip6AndPort) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: Ip6AndPort: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: Ip6AndPort: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Hi\", wireType)\n\t\t\t}\n\t\t\tm.Hi = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Hi |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Lo\", wireType)\n\t\t\t}\n\t\t\tm.Lo = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Lo |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 3:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Port\", wireType)\n\t\t\t}\n\t\t\tm.Port = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Port |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *NebulaPing) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaPing: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaPing: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Type\", wireType)\n\t\t\t}\n\t\t\tm.Type = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Type |= NebulaPing_MessageType(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Time\", wireType)\n\t\t\t}\n\t\t\tm.Time = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Time |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *NebulaHandshake) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaHandshake: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaHandshake: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Details\", wireType)\n\t\t\t}\n\t\t\tvar msglen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tmsglen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif msglen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + msglen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tif m.Details == nil {\n\t\t\t\tm.Details = &NebulaHandshakeDetails{}\n\t\t\t}\n\t\t\tif err := m.Details.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tcase 2:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Hmac\", wireType)\n\t\t\t}\n\t\t\tvar byteLen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tbyteLen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif byteLen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + byteLen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Hmac = append(m.Hmac[:0], dAtA[iNdEx:postIndex]...)\n\t\t\tif m.Hmac == nil {\n\t\t\t\tm.Hmac = []byte{}\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *NebulaHandshakeDetails) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaHandshakeDetails: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaHandshakeDetails: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Cert\", wireType)\n\t\t\t}\n\t\t\tvar byteLen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tbyteLen |= int(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif byteLen < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tpostIndex := iNdEx + byteLen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Cert = append(m.Cert[:0], dAtA[iNdEx:postIndex]...)\n\t\t\tif m.Cert == nil {\n\t\t\t\tm.Cert = []byte{}\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field InitiatorIndex\", wireType)\n\t\t\t}\n\t\t\tm.InitiatorIndex = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.InitiatorIndex |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 3:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field ResponderIndex\", wireType)\n\t\t\t}\n\t\t\tm.ResponderIndex = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.ResponderIndex |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 4:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Cookie\", wireType)\n\t\t\t}\n\t\t\tm.Cookie = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Cookie |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 5:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Time\", wireType)\n\t\t\t}\n\t\t\tm.Time = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Time |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *NebulaControl) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaControl: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: NebulaControl: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Type\", wireType)\n\t\t\t}\n\t\t\tm.Type = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Type |= NebulaControl_MessageType(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field InitiatorRelayIndex\", wireType)\n\t\t\t}\n\t\t\tm.InitiatorRelayIndex = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.InitiatorRelayIndex |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 3:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field ResponderRelayIndex\", wireType)\n\t\t\t}\n\t\t\tm.ResponderRelayIndex = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.ResponderRelayIndex |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 4:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field RelayToIp\", wireType)\n\t\t\t}\n\t\t\tm.RelayToIp = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.RelayToIp |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 5:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field RelayFromIp\", wireType)\n\t\t\t}\n\t\t\tm.RelayFromIp = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.RelayFromIp |= uint32(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipNebula(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc skipNebula(dAtA []byte) (n int, err error) {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tdepth := 0\n\tfor iNdEx < l {\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn 0, ErrIntOverflowNebula\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= (uint64(b) & 0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\twireType := int(wire & 0x7)\n\t\tswitch wireType {\n\t\tcase 0:\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn 0, ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tiNdEx++\n\t\t\t\tif dAtA[iNdEx-1] < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 1:\n\t\t\tiNdEx += 8\n\t\tcase 2:\n\t\t\tvar length int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn 0, ErrIntOverflowNebula\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tlength |= (int(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif length < 0 {\n\t\t\t\treturn 0, ErrInvalidLengthNebula\n\t\t\t}\n\t\t\tiNdEx += length\n\t\tcase 3:\n\t\t\tdepth++\n\t\tcase 4:\n\t\t\tif depth == 0 {\n\t\t\t\treturn 0, ErrUnexpectedEndOfGroupNebula\n\t\t\t}\n\t\t\tdepth--\n\t\tcase 5:\n\t\t\tiNdEx += 4\n\t\tdefault:\n\t\t\treturn 0, fmt.Errorf(\"proto: illegal wireType %d\", wireType)\n\t\t}\n\t\tif iNdEx < 0 {\n\t\t\treturn 0, ErrInvalidLengthNebula\n\t\t}\n\t\tif depth == 0 {\n\t\t\treturn iNdEx, nil\n\t\t}\n\t}\n\treturn 0, io.ErrUnexpectedEOF\n}\n\nvar (\n\tErrInvalidLengthNebula        = fmt.Errorf(\"proto: negative length found during unmarshaling\")\n\tErrIntOverflowNebula          = fmt.Errorf(\"proto: integer overflow\")\n\tErrUnexpectedEndOfGroupNebula = fmt.Errorf(\"proto: unexpected end of group\")\n)\n"
        },
        {
          "name": "nebula.proto",
          "type": "blob",
          "size": 1.421875,
          "content": "syntax = \"proto3\";\npackage nebula;\n\noption go_package = \"github.com/slackhq/nebula\";\n\nmessage NebulaMeta {\n  enum MessageType {\n    None = 0;\n    HostQuery = 1;\n    HostQueryReply = 2;\n    HostUpdateNotification = 3;\n    HostMovedNotification = 4;\n    HostPunchNotification = 5;\n    HostWhoami = 6;\n    HostWhoamiReply = 7;\n    PathCheck = 8;\n    PathCheckReply = 9;\n    HostUpdateNotificationAck = 10;\n  }\n\n  MessageType Type = 1;\n  NebulaMetaDetails Details = 2;\n}\n\nmessage NebulaMetaDetails {\n  uint32 VpnIp = 1;\n  repeated Ip4AndPort Ip4AndPorts = 2;\n  repeated Ip6AndPort Ip6AndPorts = 4;\n  repeated uint32 RelayVpnIp = 5;\n  uint32 counter = 3;\n}\n\nmessage Ip4AndPort {\n  uint32 Ip = 1;\n  uint32 Port = 2;\n}\n\nmessage Ip6AndPort {\n  uint64 Hi = 1;\n  uint64 Lo = 2;\n  uint32 Port = 3;\n}\n\nmessage NebulaPing {\n  enum MessageType {\n\t\tPing = 0;\n\t\tReply = 1;\n\t}\n\n\tMessageType Type = 1;\n\tuint64 Time = 2;\n}\n\nmessage NebulaHandshake {\n  NebulaHandshakeDetails Details = 1;\n  bytes Hmac = 2;\n}\n\nmessage NebulaHandshakeDetails {\n  bytes Cert = 1;\n  uint32 InitiatorIndex = 2;\n  uint32 ResponderIndex = 3;\n  uint64 Cookie = 4;\n  uint64 Time = 5;\n  // reserved for WIP multiport\n  reserved 6, 7;\n}\n\nmessage NebulaControl {\n  enum MessageType {\n    None = 0;\n    CreateRelayRequest = 1;\n    CreateRelayResponse = 2;\n  }\n  MessageType Type = 1;\n\n  uint32 InitiatorRelayIndex = 2;\n  uint32 ResponderRelayIndex = 3;\n  uint32 RelayToIp = 4;\n  uint32 RelayFromIp = 5;\n}\n"
        },
        {
          "name": "noise.go",
          "type": "blob",
          "size": 2.0263671875,
          "content": "package nebula\n\nimport (\n\t\"crypto/cipher\"\n\t\"encoding/binary\"\n\t\"errors\"\n\n\t\"github.com/flynn/noise\"\n)\n\ntype endianness interface {\n\tPutUint64(b []byte, v uint64)\n}\n\nvar noiseEndianness endianness = binary.BigEndian\n\ntype NebulaCipherState struct {\n\tc noise.Cipher\n\t//k [32]byte\n\t//n uint64\n}\n\nfunc NewNebulaCipherState(s *noise.CipherState) *NebulaCipherState {\n\treturn &NebulaCipherState{c: s.Cipher()}\n\n}\n\n// EncryptDanger encrypts and authenticates a given payload.\n//\n// out is a destination slice to hold the output of the EncryptDanger operation.\n//   - ad is additional data, which will be authenticated and appended to out, but not encrypted.\n//   - plaintext is encrypted, authenticated and appended to out.\n//   - n is a nonce value which must never be re-used with this key.\n//   - nb is a buffer used for temporary storage in the implementation of this call, which should\n//     be re-used by callers to minimize garbage collection.\nfunc (s *NebulaCipherState) EncryptDanger(out, ad, plaintext []byte, n uint64, nb []byte) ([]byte, error) {\n\tif s != nil {\n\t\t// TODO: Is this okay now that we have made messageCounter atomic?\n\t\t// Alternative may be to split the counter space into ranges\n\t\t//if n <= s.n {\n\t\t//\treturn nil, errors.New(\"CRITICAL: a duplicate counter value was used\")\n\t\t//}\n\t\t//s.n = n\n\t\tnb[0] = 0\n\t\tnb[1] = 0\n\t\tnb[2] = 0\n\t\tnb[3] = 0\n\t\tnoiseEndianness.PutUint64(nb[4:], n)\n\t\tout = s.c.(cipher.AEAD).Seal(out, nb, plaintext, ad)\n\t\t//l.Debugf(\"Encryption: outlen: %d, nonce: %d, ad: %s, plainlen %d\", len(out), n, ad, len(plaintext))\n\t\treturn out, nil\n\t} else {\n\t\treturn nil, errors.New(\"no cipher state available to encrypt\")\n\t}\n}\n\nfunc (s *NebulaCipherState) DecryptDanger(out, ad, ciphertext []byte, n uint64, nb []byte) ([]byte, error) {\n\tif s != nil {\n\t\tnb[0] = 0\n\t\tnb[1] = 0\n\t\tnb[2] = 0\n\t\tnb[3] = 0\n\t\tnoiseEndianness.PutUint64(nb[4:], n)\n\t\treturn s.c.(cipher.AEAD).Open(out, nb, ciphertext, ad)\n\t} else {\n\t\treturn []byte{}, nil\n\t}\n}\n\nfunc (s *NebulaCipherState) Overhead() int {\n\tif s != nil {\n\t\treturn s.c.(cipher.AEAD).Overhead()\n\t}\n\treturn 0\n}\n"
        },
        {
          "name": "noiseutil",
          "type": "tree",
          "content": null
        },
        {
          "name": "notboring.go",
          "type": "blob",
          "size": 0.1123046875,
          "content": "//go:build !boringcrypto\n// +build !boringcrypto\n\npackage nebula\n\nvar boringEnabled = func() bool { return false }\n"
        },
        {
          "name": "outside.go",
          "type": "blob",
          "size": 16.5712890625,
          "content": "package nebula\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/netip\"\n\t\"time\"\n\n\t\"github.com/flynn/noise\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/udp\"\n\t\"golang.org/x/net/ipv4\"\n)\n\nconst (\n\tminFwPacketLen = 4\n)\n\n// TODO: IPV6-WORK this can likely be removed now\nfunc readOutsidePackets(f *Interface) udp.EncReader {\n\treturn func(\n\t\taddr netip.AddrPort,\n\t\tout []byte,\n\t\tpacket []byte,\n\t\theader *header.H,\n\t\tfwPacket *firewall.Packet,\n\t\tlhh udp.LightHouseHandlerFunc,\n\t\tnb []byte,\n\t\tq int,\n\t\tlocalCache firewall.ConntrackCache,\n\t) {\n\t\tf.readOutsidePackets(addr, nil, out, packet, header, fwPacket, lhh, nb, q, localCache)\n\t}\n}\n\nfunc (f *Interface) readOutsidePackets(ip netip.AddrPort, via *ViaSender, out []byte, packet []byte, h *header.H, fwPacket *firewall.Packet, lhf udp.LightHouseHandlerFunc, nb []byte, q int, localCache firewall.ConntrackCache) {\n\terr := h.Parse(packet)\n\tif err != nil {\n\t\t// TODO: best if we return this and let caller log\n\t\t// TODO: Might be better to send the literal []byte(\"holepunch\") packet and ignore that?\n\t\t// Hole punch packets are 0 or 1 byte big, so lets ignore printing those errors\n\t\tif len(packet) > 1 {\n\t\t\tf.l.WithField(\"packet\", packet).Infof(\"Error while parsing inbound packet from %s: %s\", ip, err)\n\t\t}\n\t\treturn\n\t}\n\n\t//l.Error(\"in packet \", header, packet[HeaderLen:])\n\tif ip.IsValid() {\n\t\tif f.myVpnNet.Contains(ip.Addr()) {\n\t\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\t\tf.l.WithField(\"udpAddr\", ip).Debug(\"Refusing to process double encrypted packet\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tvar hostinfo *HostInfo\n\t// verify if we've seen this index before, otherwise respond to the handshake initiation\n\tif h.Type == header.Message && h.Subtype == header.MessageRelay {\n\t\thostinfo = f.hostMap.QueryRelayIndex(h.RemoteIndex)\n\t} else {\n\t\thostinfo = f.hostMap.QueryIndex(h.RemoteIndex)\n\t}\n\n\tvar ci *ConnectionState\n\tif hostinfo != nil {\n\t\tci = hostinfo.ConnectionState\n\t}\n\n\tswitch h.Type {\n\tcase header.Message:\n\t\t// TODO handleEncrypted sends directly to addr on error. Handle this in the tunneling case.\n\t\tif !f.handleEncrypted(ci, ip, h) {\n\t\t\treturn\n\t\t}\n\n\t\tswitch h.Subtype {\n\t\tcase header.MessageNone:\n\t\t\tif !f.decryptToTun(hostinfo, h.MessageCounter, out, packet, fwPacket, nb, q, localCache) {\n\t\t\t\treturn\n\t\t\t}\n\t\tcase header.MessageRelay:\n\t\t\t// The entire body is sent as AD, not encrypted.\n\t\t\t// The packet consists of a 16-byte parsed Nebula header, Associated Data-protected payload, and a trailing 16-byte AEAD signature value.\n\t\t\t// The packet is guaranteed to be at least 16 bytes at this point, b/c it got past the h.Parse() call above. If it's\n\t\t\t// otherwise malformed (meaning, there is no trailing 16 byte AEAD value), then this will result in at worst a 0-length slice\n\t\t\t// which will gracefully fail in the DecryptDanger call.\n\t\t\tsignedPayload := packet[:len(packet)-hostinfo.ConnectionState.dKey.Overhead()]\n\t\t\tsignatureValue := packet[len(packet)-hostinfo.ConnectionState.dKey.Overhead():]\n\t\t\tout, err = hostinfo.ConnectionState.dKey.DecryptDanger(out, signedPayload, signatureValue, h.MessageCounter, nb)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Successfully validated the thing. Get rid of the Relay header.\n\t\t\tsignedPayload = signedPayload[header.Len:]\n\t\t\t// Pull the Roaming parts up here, and return in all call paths.\n\t\t\tf.handleHostRoaming(hostinfo, ip)\n\t\t\t// Track usage of both the HostInfo and the Relay for the received & authenticated packet\n\t\t\tf.connectionManager.In(hostinfo.localIndexId)\n\t\t\tf.connectionManager.RelayUsed(h.RemoteIndex)\n\n\t\t\trelay, ok := hostinfo.relayState.QueryRelayForByIdx(h.RemoteIndex)\n\t\t\tif !ok {\n\t\t\t\t// The only way this happens is if hostmap has an index to the correct HostInfo, but the HostInfo is missing\n\t\t\t\t// its internal mapping. This should never happen.\n\t\t\t\thostinfo.logger(f.l).WithFields(logrus.Fields{\"vpnIp\": hostinfo.vpnIp, \"remoteIndex\": h.RemoteIndex}).Error(\"HostInfo missing remote relay index\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tswitch relay.Type {\n\t\t\tcase TerminalType:\n\t\t\t\t// If I am the target of this relay, process the unwrapped packet\n\t\t\t\t// From this recursive point, all these variables are 'burned'. We shouldn't rely on them again.\n\t\t\t\tf.readOutsidePackets(netip.AddrPort{}, &ViaSender{relayHI: hostinfo, remoteIdx: relay.RemoteIndex, relay: relay}, out[:0], signedPayload, h, fwPacket, lhf, nb, q, localCache)\n\t\t\t\treturn\n\t\t\tcase ForwardingType:\n\t\t\t\t// Find the target HostInfo relay object\n\t\t\t\ttargetHI, targetRelay, err := f.hostMap.QueryVpnIpRelayFor(hostinfo.vpnIp, relay.PeerIp)\n\t\t\t\tif err != nil {\n\t\t\t\t\thostinfo.logger(f.l).WithField(\"relayTo\", relay.PeerIp).WithError(err).Info(\"Failed to find target host info by ip\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// If that relay is Established, forward the payload through it\n\t\t\t\tif targetRelay.State == Established {\n\t\t\t\t\tswitch targetRelay.Type {\n\t\t\t\t\tcase ForwardingType:\n\t\t\t\t\t\t// Forward this packet through the relay tunnel\n\t\t\t\t\t\t// Find the target HostInfo\n\t\t\t\t\t\tf.SendVia(targetHI, targetRelay, signedPayload, nb, out, false)\n\t\t\t\t\t\treturn\n\t\t\t\t\tcase TerminalType:\n\t\t\t\t\t\thostinfo.logger(f.l).Error(\"Unexpected Relay Type of Terminal\")\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\thostinfo.logger(f.l).WithFields(logrus.Fields{\"relayTo\": relay.PeerIp, \"relayFrom\": hostinfo.vpnIp, \"targetRelayState\": targetRelay.State}).Info(\"Unexpected target relay state\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tcase header.LightHouse:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\tif !f.handleEncrypted(ci, ip, h) {\n\t\t\treturn\n\t\t}\n\n\t\td, err := f.decrypt(hostinfo, h.MessageCounter, out, packet, h, nb)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).WithField(\"udpAddr\", ip).\n\t\t\t\tWithField(\"packet\", packet).\n\t\t\t\tError(\"Failed to decrypt lighthouse packet\")\n\n\t\t\t//TODO: maybe after build 64 is out? 06/14/2018 - NB\n\t\t\t//f.sendRecvError(net.Addr(addr), header.RemoteIndex)\n\t\t\treturn\n\t\t}\n\n\t\tlhf(ip, hostinfo.vpnIp, d)\n\n\t\t// Fallthrough to the bottom to record incoming traffic\n\n\tcase header.Test:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\tif !f.handleEncrypted(ci, ip, h) {\n\t\t\treturn\n\t\t}\n\n\t\td, err := f.decrypt(hostinfo, h.MessageCounter, out, packet, h, nb)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).WithField(\"udpAddr\", ip).\n\t\t\t\tWithField(\"packet\", packet).\n\t\t\t\tError(\"Failed to decrypt test packet\")\n\n\t\t\t//TODO: maybe after build 64 is out? 06/14/2018 - NB\n\t\t\t//f.sendRecvError(net.Addr(addr), header.RemoteIndex)\n\t\t\treturn\n\t\t}\n\n\t\tif h.Subtype == header.TestRequest {\n\t\t\t// This testRequest might be from TryPromoteBest, so we should roam\n\t\t\t// to the new IP address before responding\n\t\t\tf.handleHostRoaming(hostinfo, ip)\n\t\t\tf.send(header.Test, header.TestReply, ci, hostinfo, d, nb, out)\n\t\t}\n\n\t\t// Fallthrough to the bottom to record incoming traffic\n\n\t\t// Non encrypted messages below here, they should not fall through to avoid tracking incoming traffic since they\n\t\t// are unauthenticated\n\n\tcase header.Handshake:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\tf.handshakeManager.HandleIncoming(ip, via, packet, h)\n\t\treturn\n\n\tcase header.RecvError:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\tf.handleRecvError(ip, h)\n\t\treturn\n\n\tcase header.CloseTunnel:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\tif !f.handleEncrypted(ci, ip, h) {\n\t\t\treturn\n\t\t}\n\n\t\thostinfo.logger(f.l).WithField(\"udpAddr\", ip).\n\t\t\tInfo(\"Close tunnel received, tearing down.\")\n\n\t\tf.closeTunnel(hostinfo)\n\t\treturn\n\n\tcase header.Control:\n\t\tif !f.handleEncrypted(ci, ip, h) {\n\t\t\treturn\n\t\t}\n\n\t\td, err := f.decrypt(hostinfo, h.MessageCounter, out, packet, h, nb)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).WithField(\"udpAddr\", ip).\n\t\t\t\tWithField(\"packet\", packet).\n\t\t\t\tError(\"Failed to decrypt Control packet\")\n\t\t\treturn\n\t\t}\n\t\tm := &NebulaControl{}\n\t\terr = m.Unmarshal(d)\n\t\tif err != nil {\n\t\t\thostinfo.logger(f.l).WithError(err).Error(\"Failed to unmarshal control message\")\n\t\t\tbreak\n\t\t}\n\n\t\tf.relayManager.HandleControlMsg(hostinfo, m, f)\n\n\tdefault:\n\t\tf.messageMetrics.Rx(h.Type, h.Subtype, 1)\n\t\thostinfo.logger(f.l).Debugf(\"Unexpected packet received from %s\", ip)\n\t\treturn\n\t}\n\n\tf.handleHostRoaming(hostinfo, ip)\n\n\tf.connectionManager.In(hostinfo.localIndexId)\n}\n\n// closeTunnel closes a tunnel locally, it does not send a closeTunnel packet to the remote\nfunc (f *Interface) closeTunnel(hostInfo *HostInfo) {\n\tfinal := f.hostMap.DeleteHostInfo(hostInfo)\n\tif final {\n\t\t// We no longer have any tunnels with this vpn ip, clear learned lighthouse state to lower memory usage\n\t\tf.lightHouse.DeleteVpnIp(hostInfo.vpnIp)\n\t}\n}\n\n// sendCloseTunnel is a helper function to send a proper close tunnel packet to a remote\nfunc (f *Interface) sendCloseTunnel(h *HostInfo) {\n\tf.send(header.CloseTunnel, 0, h.ConnectionState, h, []byte{}, make([]byte, 12, 12), make([]byte, mtu))\n}\n\nfunc (f *Interface) handleHostRoaming(hostinfo *HostInfo, ip netip.AddrPort) {\n\tif ip.IsValid() && hostinfo.remote != ip {\n\t\tif !f.lightHouse.GetRemoteAllowList().Allow(hostinfo.vpnIp, ip.Addr()) {\n\t\t\thostinfo.logger(f.l).WithField(\"newAddr\", ip).Debug(\"lighthouse.remote_allow_list denied roaming\")\n\t\t\treturn\n\t\t}\n\t\tif !hostinfo.lastRoam.IsZero() && ip == hostinfo.lastRoamRemote && time.Since(hostinfo.lastRoam) < RoamingSuppressSeconds*time.Second {\n\t\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\t\thostinfo.logger(f.l).WithField(\"udpAddr\", hostinfo.remote).WithField(\"newAddr\", ip).\n\t\t\t\t\tDebugf(\"Suppressing roam back to previous remote for %d seconds\", RoamingSuppressSeconds)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\thostinfo.logger(f.l).WithField(\"udpAddr\", hostinfo.remote).WithField(\"newAddr\", ip).\n\t\t\tInfo(\"Host roamed to new udp ip/port.\")\n\t\thostinfo.lastRoam = time.Now()\n\t\thostinfo.lastRoamRemote = hostinfo.remote\n\t\thostinfo.SetRemote(ip)\n\t}\n\n}\n\nfunc (f *Interface) handleEncrypted(ci *ConnectionState, addr netip.AddrPort, h *header.H) bool {\n\t// If connectionstate exists and the replay protector allows, process packet\n\t// Else, send recv errors for 300 seconds after a restart to allow fast reconnection.\n\tif ci == nil || !ci.window.Check(f.l, h.MessageCounter) {\n\t\tif addr.IsValid() {\n\t\t\tf.maybeSendRecvError(addr, h.RemoteIndex)\n\t\t\treturn false\n\t\t} else {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// newPacket validates and parses the interesting bits for the firewall out of the ip and sub protocol headers\nfunc newPacket(data []byte, incoming bool, fp *firewall.Packet) error {\n\t// Do we at least have an ipv4 header worth of data?\n\tif len(data) < ipv4.HeaderLen {\n\t\treturn fmt.Errorf(\"packet is less than %v bytes\", ipv4.HeaderLen)\n\t}\n\n\t// Is it an ipv4 packet?\n\tif int((data[0]>>4)&0x0f) != 4 {\n\t\treturn fmt.Errorf(\"packet is not ipv4, type: %v\", int((data[0]>>4)&0x0f))\n\t}\n\n\t// Adjust our start position based on the advertised ip header length\n\tihl := int(data[0]&0x0f) << 2\n\n\t// Well formed ip header length?\n\tif ihl < ipv4.HeaderLen {\n\t\treturn fmt.Errorf(\"packet had an invalid header length: %v\", ihl)\n\t}\n\n\t// Check if this is the second or further fragment of a fragmented packet.\n\tflagsfrags := binary.BigEndian.Uint16(data[6:8])\n\tfp.Fragment = (flagsfrags & 0x1FFF) != 0\n\n\t// Firewall handles protocol checks\n\tfp.Protocol = data[9]\n\n\t// Accounting for a variable header length, do we have enough data for our src/dst tuples?\n\tminLen := ihl\n\tif !fp.Fragment && fp.Protocol != firewall.ProtoICMP {\n\t\tminLen += minFwPacketLen\n\t}\n\tif len(data) < minLen {\n\t\treturn fmt.Errorf(\"packet is less than %v bytes, ip header len: %v\", minLen, ihl)\n\t}\n\n\t// Firewall packets are locally oriented\n\tif incoming {\n\t\t//TODO: IPV6-WORK\n\t\tfp.RemoteIP, _ = netip.AddrFromSlice(data[12:16])\n\t\tfp.LocalIP, _ = netip.AddrFromSlice(data[16:20])\n\t\tif fp.Fragment || fp.Protocol == firewall.ProtoICMP {\n\t\t\tfp.RemotePort = 0\n\t\t\tfp.LocalPort = 0\n\t\t} else {\n\t\t\tfp.RemotePort = binary.BigEndian.Uint16(data[ihl : ihl+2])\n\t\t\tfp.LocalPort = binary.BigEndian.Uint16(data[ihl+2 : ihl+4])\n\t\t}\n\t} else {\n\t\t//TODO: IPV6-WORK\n\t\tfp.LocalIP, _ = netip.AddrFromSlice(data[12:16])\n\t\tfp.RemoteIP, _ = netip.AddrFromSlice(data[16:20])\n\t\tif fp.Fragment || fp.Protocol == firewall.ProtoICMP {\n\t\t\tfp.RemotePort = 0\n\t\t\tfp.LocalPort = 0\n\t\t} else {\n\t\t\tfp.LocalPort = binary.BigEndian.Uint16(data[ihl : ihl+2])\n\t\t\tfp.RemotePort = binary.BigEndian.Uint16(data[ihl+2 : ihl+4])\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (f *Interface) decrypt(hostinfo *HostInfo, mc uint64, out []byte, packet []byte, h *header.H, nb []byte) ([]byte, error) {\n\tvar err error\n\tout, err = hostinfo.ConnectionState.dKey.DecryptDanger(out, packet[:header.Len], packet[header.Len:], mc, nb)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !hostinfo.ConnectionState.window.Update(f.l, mc) {\n\t\thostinfo.logger(f.l).WithField(\"header\", h).\n\t\t\tDebugln(\"dropping out of window packet\")\n\t\treturn nil, errors.New(\"out of window packet\")\n\t}\n\n\treturn out, nil\n}\n\nfunc (f *Interface) decryptToTun(hostinfo *HostInfo, messageCounter uint64, out []byte, packet []byte, fwPacket *firewall.Packet, nb []byte, q int, localCache firewall.ConntrackCache) bool {\n\tvar err error\n\n\tout, err = hostinfo.ConnectionState.dKey.DecryptDanger(out, packet[:header.Len], packet[header.Len:], messageCounter, nb)\n\tif err != nil {\n\t\thostinfo.logger(f.l).WithError(err).Error(\"Failed to decrypt packet\")\n\t\t//TODO: maybe after build 64 is out? 06/14/2018 - NB\n\t\t//f.sendRecvError(hostinfo.remote, header.RemoteIndex)\n\t\treturn false\n\t}\n\n\terr = newPacket(out, true, fwPacket)\n\tif err != nil {\n\t\thostinfo.logger(f.l).WithError(err).WithField(\"packet\", out).\n\t\t\tWarnf(\"Error while validating inbound packet\")\n\t\treturn false\n\t}\n\n\tif !hostinfo.ConnectionState.window.Update(f.l, messageCounter) {\n\t\thostinfo.logger(f.l).WithField(\"fwPacket\", fwPacket).\n\t\t\tDebugln(\"dropping out of window packet\")\n\t\treturn false\n\t}\n\n\tdropReason := f.firewall.Drop(*fwPacket, true, hostinfo, f.pki.GetCAPool(), localCache)\n\tif dropReason != nil {\n\t\t// NOTE: We give `packet` as the `out` here since we already decrypted from it and we don't need it anymore\n\t\t// This gives us a buffer to build the reject packet in\n\t\tf.rejectOutside(out, hostinfo.ConnectionState, hostinfo, nb, packet, q)\n\t\tif f.l.Level >= logrus.DebugLevel {\n\t\t\thostinfo.logger(f.l).WithField(\"fwPacket\", fwPacket).\n\t\t\t\tWithField(\"reason\", dropReason).\n\t\t\t\tDebugln(\"dropping inbound packet\")\n\t\t}\n\t\treturn false\n\t}\n\n\tf.connectionManager.In(hostinfo.localIndexId)\n\t_, err = f.readers[q].Write(out)\n\tif err != nil {\n\t\tf.l.WithError(err).Error(\"Failed to write to tun\")\n\t}\n\treturn true\n}\n\nfunc (f *Interface) maybeSendRecvError(endpoint netip.AddrPort, index uint32) {\n\tif f.sendRecvErrorConfig.ShouldSendRecvError(endpoint) {\n\t\tf.sendRecvError(endpoint, index)\n\t}\n}\n\nfunc (f *Interface) sendRecvError(endpoint netip.AddrPort, index uint32) {\n\tf.messageMetrics.Tx(header.RecvError, 0, 1)\n\n\t//TODO: this should be a signed message so we can trust that we should drop the index\n\tb := header.Encode(make([]byte, header.Len), header.Version, header.RecvError, 0, index, 0)\n\tf.outside.WriteTo(b, endpoint)\n\tif f.l.Level >= logrus.DebugLevel {\n\t\tf.l.WithField(\"index\", index).\n\t\t\tWithField(\"udpAddr\", endpoint).\n\t\t\tDebug(\"Recv error sent\")\n\t}\n}\n\nfunc (f *Interface) handleRecvError(addr netip.AddrPort, h *header.H) {\n\tif f.l.Level >= logrus.DebugLevel {\n\t\tf.l.WithField(\"index\", h.RemoteIndex).\n\t\t\tWithField(\"udpAddr\", addr).\n\t\t\tDebug(\"Recv error received\")\n\t}\n\n\thostinfo := f.hostMap.QueryReverseIndex(h.RemoteIndex)\n\tif hostinfo == nil {\n\t\tf.l.WithField(\"remoteIndex\", h.RemoteIndex).Debugln(\"Did not find remote index in main hostmap\")\n\t\treturn\n\t}\n\n\tif !hostinfo.RecvErrorExceeded() {\n\t\treturn\n\t}\n\n\tif hostinfo.remote.IsValid() && hostinfo.remote != addr {\n\t\tf.l.Infoln(\"Someone spoofing recv_errors? \", addr, hostinfo.remote)\n\t\treturn\n\t}\n\n\tf.closeTunnel(hostinfo)\n\t// We also delete it from pending hostmap to allow for fast reconnect.\n\tf.handshakeManager.DeleteHostInfo(hostinfo)\n}\n\n/*\nfunc (f *Interface) sendMeta(ci *ConnectionState, endpoint *net.UDPAddr, meta *NebulaMeta) {\n\tif ci.eKey != nil {\n\t\t//TODO: log error?\n\t\treturn\n\t}\n\n\tmsg, err := proto.Marshal(meta)\n\tif err != nil {\n\t\tl.Debugln(\"failed to encode header\")\n\t}\n\n\tc := ci.messageCounter\n\tb := HeaderEncode(nil, Version, uint8(metadata), 0, hostinfo.remoteIndexId, c)\n\tci.messageCounter++\n\n\tmsg := ci.eKey.EncryptDanger(b, nil, msg, c)\n\t//msg := ci.eKey.EncryptDanger(b, nil, []byte(fmt.Sprintf(\"%d\", counter)), c)\n\tf.outside.WriteTo(msg, endpoint)\n}\n*/\n\nfunc RecombineCertAndValidate(h *noise.HandshakeState, rawCertBytes []byte, caPool *cert.CAPool) (*cert.CachedCertificate, error) {\n\tpk := h.PeerStatic()\n\n\tif pk == nil {\n\t\treturn nil, errors.New(\"no peer static key was present\")\n\t}\n\n\tif rawCertBytes == nil {\n\t\treturn nil, errors.New(\"provided payload was empty\")\n\t}\n\n\tc, err := cert.UnmarshalCertificateFromHandshake(rawCertBytes, pk)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error unmarshaling cert: %w\", err)\n\t}\n\n\tcc, err := caPool.VerifyCertificate(time.Now(), c)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"certificate validation failed: %w\", err)\n\t}\n\n\treturn cc, nil\n}\n"
        },
        {
          "name": "outside_test.go",
          "type": "blob",
          "size": 2.1689453125,
          "content": "package nebula\n\nimport (\n\t\"net\"\n\t\"net/netip\"\n\t\"testing\"\n\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"golang.org/x/net/ipv4\"\n)\n\nfunc Test_newPacket(t *testing.T) {\n\tp := &firewall.Packet{}\n\n\t// length fail\n\terr := newPacket([]byte{0, 1}, true, p)\n\tassert.EqualError(t, err, \"packet is less than 20 bytes\")\n\n\t// length fail with ip options\n\th := ipv4.Header{\n\t\tVersion: 1,\n\t\tLen:     100,\n\t\tSrc:     net.IPv4(10, 0, 0, 1),\n\t\tDst:     net.IPv4(10, 0, 0, 2),\n\t\tOptions: []byte{0, 1, 0, 2},\n\t}\n\n\tb, _ := h.Marshal()\n\terr = newPacket(b, true, p)\n\n\tassert.EqualError(t, err, \"packet is less than 28 bytes, ip header len: 24\")\n\n\t// not an ipv4 packet\n\terr = newPacket([]byte{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, true, p)\n\tassert.EqualError(t, err, \"packet is not ipv4, type: 0\")\n\n\t// invalid ihl\n\terr = newPacket([]byte{4<<4 | (8 >> 2 & 0x0f), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, true, p)\n\tassert.EqualError(t, err, \"packet had an invalid header length: 8\")\n\n\t// account for variable ip header length - incoming\n\th = ipv4.Header{\n\t\tVersion:  1,\n\t\tLen:      100,\n\t\tSrc:      net.IPv4(10, 0, 0, 1),\n\t\tDst:      net.IPv4(10, 0, 0, 2),\n\t\tOptions:  []byte{0, 1, 0, 2},\n\t\tProtocol: firewall.ProtoTCP,\n\t}\n\n\tb, _ = h.Marshal()\n\tb = append(b, []byte{0, 3, 0, 4}...)\n\terr = newPacket(b, true, p)\n\n\tassert.Nil(t, err)\n\tassert.Equal(t, p.Protocol, uint8(firewall.ProtoTCP))\n\tassert.Equal(t, p.LocalIP, netip.MustParseAddr(\"10.0.0.2\"))\n\tassert.Equal(t, p.RemoteIP, netip.MustParseAddr(\"10.0.0.1\"))\n\tassert.Equal(t, p.RemotePort, uint16(3))\n\tassert.Equal(t, p.LocalPort, uint16(4))\n\n\t// account for variable ip header length - outgoing\n\th = ipv4.Header{\n\t\tVersion:  1,\n\t\tProtocol: 2,\n\t\tLen:      100,\n\t\tSrc:      net.IPv4(10, 0, 0, 1),\n\t\tDst:      net.IPv4(10, 0, 0, 2),\n\t\tOptions:  []byte{0, 1, 0, 2},\n\t}\n\n\tb, _ = h.Marshal()\n\tb = append(b, []byte{0, 5, 0, 6}...)\n\terr = newPacket(b, false, p)\n\n\tassert.Nil(t, err)\n\tassert.Equal(t, p.Protocol, uint8(2))\n\tassert.Equal(t, p.LocalIP, netip.MustParseAddr(\"10.0.0.1\"))\n\tassert.Equal(t, p.RemoteIP, netip.MustParseAddr(\"10.0.0.2\"))\n\tassert.Equal(t, p.RemotePort, uint16(6))\n\tassert.Equal(t, p.LocalPort, uint16(5))\n}\n"
        },
        {
          "name": "overlay",
          "type": "tree",
          "content": null
        },
        {
          "name": "pkclient",
          "type": "tree",
          "content": null
        },
        {
          "name": "pki.go",
          "type": "blob",
          "size": 6.6083984375,
          "content": "package nebula\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/cert\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/util\"\n)\n\ntype PKI struct {\n\tcs     atomic.Pointer[CertState]\n\tcaPool atomic.Pointer[cert.CAPool]\n\tl      *logrus.Logger\n}\n\ntype CertState struct {\n\tCertificate         cert.Certificate\n\tRawCertificate      []byte\n\tRawCertificateNoKey []byte\n\tPublicKey           []byte\n\tPrivateKey          []byte\n\tpkcs11Backed        bool\n}\n\nfunc NewPKIFromConfig(l *logrus.Logger, c *config.C) (*PKI, error) {\n\tpki := &PKI{l: l}\n\terr := pki.reload(c, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\trErr := pki.reload(c, false)\n\t\tif rErr != nil {\n\t\t\tutil.LogWithContextIfNeeded(\"Failed to reload PKI from config\", rErr, l)\n\t\t}\n\t})\n\n\treturn pki, nil\n}\n\nfunc (p *PKI) GetCertState() *CertState {\n\treturn p.cs.Load()\n}\n\nfunc (p *PKI) GetCAPool() *cert.CAPool {\n\treturn p.caPool.Load()\n}\n\nfunc (p *PKI) reload(c *config.C, initial bool) error {\n\terr := p.reloadCert(c, initial)\n\tif err != nil {\n\t\tif initial {\n\t\t\treturn err\n\t\t}\n\t\terr.Log(p.l)\n\t}\n\n\terr = p.reloadCAPool(c)\n\tif err != nil {\n\t\tif initial {\n\t\t\treturn err\n\t\t}\n\t\terr.Log(p.l)\n\t}\n\n\treturn nil\n}\n\nfunc (p *PKI) reloadCert(c *config.C, initial bool) *util.ContextualError {\n\tcs, err := newCertStateFromConfig(c)\n\tif err != nil {\n\t\treturn util.NewContextualError(\"Could not load client cert\", nil, err)\n\t}\n\n\tif !initial {\n\t\t//TODO: include check for mask equality as well\n\n\t\t// did IP in cert change? if so, don't set\n\t\tcurrentCert := p.cs.Load().Certificate\n\t\toldIPs := currentCert.Networks()\n\t\tnewIPs := cs.Certificate.Networks()\n\t\tif len(oldIPs) > 0 && len(newIPs) > 0 && oldIPs[0].String() != newIPs[0].String() {\n\t\t\treturn util.NewContextualError(\n\t\t\t\t\"Networks in new cert was different from old\",\n\t\t\t\tm{\"new_network\": newIPs[0], \"old_network\": oldIPs[0]},\n\t\t\t\tnil,\n\t\t\t)\n\t\t}\n\t}\n\n\tp.cs.Store(cs)\n\tif initial {\n\t\tp.l.WithField(\"cert\", cs.Certificate).Debug(\"Client nebula certificate\")\n\t} else {\n\t\tp.l.WithField(\"cert\", cs.Certificate).Info(\"Client cert refreshed from disk\")\n\t}\n\treturn nil\n}\n\nfunc (p *PKI) reloadCAPool(c *config.C) *util.ContextualError {\n\tcaPool, err := loadCAPoolFromConfig(p.l, c)\n\tif err != nil {\n\t\treturn util.NewContextualError(\"Failed to load ca from config\", nil, err)\n\t}\n\n\tp.caPool.Store(caPool)\n\tp.l.WithField(\"fingerprints\", caPool.GetFingerprints()).Debug(\"Trusted CA fingerprints\")\n\treturn nil\n}\n\nfunc newCertState(certificate cert.Certificate, pkcs11backed bool, privateKey []byte) (*CertState, error) {\n\t// Marshal the certificate to ensure it is valid\n\trawCertificate, err := certificate.Marshal()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid nebula certificate on interface: %s\", err)\n\t}\n\n\tpublicKey := certificate.PublicKey()\n\tcs := &CertState{\n\t\tRawCertificate: rawCertificate,\n\t\tCertificate:    certificate,\n\t\tPrivateKey:     privateKey,\n\t\tPublicKey:      publicKey,\n\t\tpkcs11Backed:   pkcs11backed,\n\t}\n\n\trawCertNoKey, err := cs.Certificate.MarshalForHandshakes()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error marshalling certificate no key: %s\", err)\n\t}\n\tcs.RawCertificateNoKey = rawCertNoKey\n\n\treturn cs, nil\n}\n\nfunc loadPrivateKey(privPathOrPEM string) (rawKey []byte, curve cert.Curve, isPkcs11 bool, err error) {\n\tvar pemPrivateKey []byte\n\tif strings.Contains(privPathOrPEM, \"-----BEGIN\") {\n\t\tpemPrivateKey = []byte(privPathOrPEM)\n\t\tprivPathOrPEM = \"<inline>\"\n\t\trawKey, _, curve, err = cert.UnmarshalPrivateKeyFromPEM(pemPrivateKey)\n\t\tif err != nil {\n\t\t\treturn nil, curve, false, fmt.Errorf(\"error while unmarshaling pki.key %s: %s\", privPathOrPEM, err)\n\t\t}\n\t} else if strings.HasPrefix(privPathOrPEM, \"pkcs11:\") {\n\t\trawKey = []byte(privPathOrPEM)\n\t\treturn rawKey, cert.Curve_P256, true, nil\n\t} else {\n\t\tpemPrivateKey, err = os.ReadFile(privPathOrPEM)\n\t\tif err != nil {\n\t\t\treturn nil, curve, false, fmt.Errorf(\"unable to read pki.key file %s: %s\", privPathOrPEM, err)\n\t\t}\n\t\trawKey, _, curve, err = cert.UnmarshalPrivateKeyFromPEM(pemPrivateKey)\n\t\tif err != nil {\n\t\t\treturn nil, curve, false, fmt.Errorf(\"error while unmarshaling pki.key %s: %s\", privPathOrPEM, err)\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc newCertStateFromConfig(c *config.C) (*CertState, error) {\n\tvar err error\n\n\tprivPathOrPEM := c.GetString(\"pki.key\", \"\")\n\tif privPathOrPEM == \"\" {\n\t\treturn nil, errors.New(\"no pki.key path or PEM data provided\")\n\t}\n\n\trawKey, curve, isPkcs11, err := loadPrivateKey(privPathOrPEM)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar rawCert []byte\n\n\tpubPathOrPEM := c.GetString(\"pki.cert\", \"\")\n\tif pubPathOrPEM == \"\" {\n\t\treturn nil, errors.New(\"no pki.cert path or PEM data provided\")\n\t}\n\n\tif strings.Contains(pubPathOrPEM, \"-----BEGIN\") {\n\t\trawCert = []byte(pubPathOrPEM)\n\t\tpubPathOrPEM = \"<inline>\"\n\n\t} else {\n\t\trawCert, err = os.ReadFile(pubPathOrPEM)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to read pki.cert file %s: %s\", pubPathOrPEM, err)\n\t\t}\n\t}\n\n\tnebulaCert, _, err := cert.UnmarshalCertificateFromPEM(rawCert)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error while unmarshaling pki.cert %s: %s\", pubPathOrPEM, err)\n\t}\n\n\tif nebulaCert.Expired(time.Now()) {\n\t\treturn nil, fmt.Errorf(\"nebula certificate for this host is expired\")\n\t}\n\n\tif len(nebulaCert.Networks()) == 0 {\n\t\treturn nil, fmt.Errorf(\"no networks encoded in certificate\")\n\t}\n\n\tif err = nebulaCert.VerifyPrivateKey(curve, rawKey); err != nil {\n\t\treturn nil, fmt.Errorf(\"private key is not a pair with public key in nebula cert\")\n\t}\n\n\treturn newCertState(nebulaCert, isPkcs11, rawKey)\n}\n\nfunc loadCAPoolFromConfig(l *logrus.Logger, c *config.C) (*cert.CAPool, error) {\n\tvar rawCA []byte\n\tvar err error\n\n\tcaPathOrPEM := c.GetString(\"pki.ca\", \"\")\n\tif caPathOrPEM == \"\" {\n\t\treturn nil, errors.New(\"no pki.ca path or PEM data provided\")\n\t}\n\n\tif strings.Contains(caPathOrPEM, \"-----BEGIN\") {\n\t\trawCA = []byte(caPathOrPEM)\n\n\t} else {\n\t\trawCA, err = os.ReadFile(caPathOrPEM)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to read pki.ca file %s: %s\", caPathOrPEM, err)\n\t\t}\n\t}\n\n\tcaPool, err := cert.NewCAPoolFromPEM(rawCA)\n\tif errors.Is(err, cert.ErrExpired) {\n\t\tvar expired int\n\t\tfor _, crt := range caPool.CAs {\n\t\t\tif crt.Certificate.Expired(time.Now()) {\n\t\t\t\texpired++\n\t\t\t\tl.WithField(\"cert\", crt).Warn(\"expired certificate present in CA pool\")\n\t\t\t}\n\t\t}\n\n\t\tif expired >= len(caPool.CAs) {\n\t\t\treturn nil, errors.New(\"no valid CA certificates present\")\n\t\t}\n\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"error while adding CA certificate to CA trust store: %s\", err)\n\t}\n\n\tfor _, fp := range c.GetStringSlice(\"pki.blocklist\", []string{}) {\n\t\tl.WithField(\"fingerprint\", fp).Info(\"Blocklisting cert\")\n\t\tcaPool.BlocklistFingerprint(fp)\n\t}\n\n\treturn caPool, nil\n}\n"
        },
        {
          "name": "punchy.go",
          "type": "blob",
          "size": 2.595703125,
          "content": "package nebula\n\nimport (\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\ntype Punchy struct {\n\tpunch           atomic.Bool\n\trespond         atomic.Bool\n\tdelay           atomic.Int64\n\trespondDelay    atomic.Int64\n\tpunchEverything atomic.Bool\n\tl               *logrus.Logger\n}\n\nfunc NewPunchyFromConfig(l *logrus.Logger, c *config.C) *Punchy {\n\tp := &Punchy{l: l}\n\n\tp.reload(c, true)\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\tp.reload(c, false)\n\t})\n\n\treturn p\n}\n\nfunc (p *Punchy) reload(c *config.C, initial bool) {\n\tif initial {\n\t\tvar yes bool\n\t\tif c.IsSet(\"punchy.punch\") {\n\t\t\tyes = c.GetBool(\"punchy.punch\", false)\n\t\t} else {\n\t\t\t// Deprecated fallback\n\t\t\tyes = c.GetBool(\"punchy\", false)\n\t\t}\n\n\t\tp.punch.Store(yes)\n\t\tif yes {\n\t\t\tp.l.Info(\"punchy enabled\")\n\t\t} else {\n\t\t\tp.l.Info(\"punchy disabled\")\n\t\t}\n\n\t} else if c.HasChanged(\"punchy.punch\") || c.HasChanged(\"punchy\") {\n\t\t//TODO: it should be relatively easy to support this, just need to be able to cancel the goroutine and boot it up from here\n\t\tp.l.Warn(\"Changing punchy.punch with reload is not supported, ignoring.\")\n\t}\n\n\tif initial || c.HasChanged(\"punchy.respond\") || c.HasChanged(\"punch_back\") {\n\t\tvar yes bool\n\t\tif c.IsSet(\"punchy.respond\") {\n\t\t\tyes = c.GetBool(\"punchy.respond\", false)\n\t\t} else {\n\t\t\t// Deprecated fallback\n\t\t\tyes = c.GetBool(\"punch_back\", false)\n\t\t}\n\n\t\tp.respond.Store(yes)\n\n\t\tif !initial {\n\t\t\tp.l.Infof(\"punchy.respond changed to %v\", p.GetRespond())\n\t\t}\n\t}\n\n\t//NOTE: this will not apply to any in progress operations, only the next one\n\tif initial || c.HasChanged(\"punchy.delay\") {\n\t\tp.delay.Store((int64)(c.GetDuration(\"punchy.delay\", time.Second)))\n\t\tif !initial {\n\t\t\tp.l.Infof(\"punchy.delay changed to %s\", p.GetDelay())\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"punchy.target_all_remotes\") {\n\t\tp.punchEverything.Store(c.GetBool(\"punchy.target_all_remotes\", false))\n\t\tif !initial {\n\t\t\tp.l.WithField(\"target_all_remotes\", p.GetTargetEverything()).Info(\"punchy.target_all_remotes changed\")\n\t\t}\n\t}\n\n\tif initial || c.HasChanged(\"punchy.respond_delay\") {\n\t\tp.respondDelay.Store((int64)(c.GetDuration(\"punchy.respond_delay\", 5*time.Second)))\n\t\tif !initial {\n\t\t\tp.l.Infof(\"punchy.respond_delay changed to %s\", p.GetRespondDelay())\n\t\t}\n\t}\n}\n\nfunc (p *Punchy) GetPunch() bool {\n\treturn p.punch.Load()\n}\n\nfunc (p *Punchy) GetRespond() bool {\n\treturn p.respond.Load()\n}\n\nfunc (p *Punchy) GetDelay() time.Duration {\n\treturn (time.Duration)(p.delay.Load())\n}\n\nfunc (p *Punchy) GetRespondDelay() time.Duration {\n\treturn (time.Duration)(p.respondDelay.Load())\n}\n\nfunc (p *Punchy) GetTargetEverything() bool {\n\treturn p.punchEverything.Load()\n}\n"
        },
        {
          "name": "punchy_test.go",
          "type": "blob",
          "size": 1.9033203125,
          "content": "package nebula\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNewPunchyFromConfig(t *testing.T) {\n\tl := test.NewLogger()\n\tc := config.NewC(l)\n\n\t// Test defaults\n\tp := NewPunchyFromConfig(l, c)\n\tassert.Equal(t, false, p.GetPunch())\n\tassert.Equal(t, false, p.GetRespond())\n\tassert.Equal(t, time.Second, p.GetDelay())\n\tassert.Equal(t, 5*time.Second, p.GetRespondDelay())\n\n\t// punchy deprecation\n\tc.Settings[\"punchy\"] = true\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, true, p.GetPunch())\n\n\t// punchy.punch\n\tc.Settings[\"punchy\"] = map[interface{}]interface{}{\"punch\": true}\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, true, p.GetPunch())\n\n\t// punch_back deprecation\n\tc.Settings[\"punch_back\"] = true\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, true, p.GetRespond())\n\n\t// punchy.respond\n\tc.Settings[\"punchy\"] = map[interface{}]interface{}{\"respond\": true}\n\tc.Settings[\"punch_back\"] = false\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, true, p.GetRespond())\n\n\t// punchy.delay\n\tc.Settings[\"punchy\"] = map[interface{}]interface{}{\"delay\": \"1m\"}\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, time.Minute, p.GetDelay())\n\n\t// punchy.respond_delay\n\tc.Settings[\"punchy\"] = map[interface{}]interface{}{\"respond_delay\": \"1m\"}\n\tp = NewPunchyFromConfig(l, c)\n\tassert.Equal(t, time.Minute, p.GetRespondDelay())\n}\n\nfunc TestPunchy_reload(t *testing.T) {\n\tl := test.NewLogger()\n\tc := config.NewC(l)\n\tdelay, _ := time.ParseDuration(\"1m\")\n\tassert.NoError(t, c.LoadString(`\npunchy:\n  delay: 1m\n  respond: false\n`))\n\tp := NewPunchyFromConfig(l, c)\n\tassert.Equal(t, delay, p.GetDelay())\n\tassert.Equal(t, false, p.GetRespond())\n\n\tnewDelay, _ := time.ParseDuration(\"10m\")\n\tassert.NoError(t, c.ReloadConfigString(`\npunchy:\n  delay: 10m\n  respond: true\n`))\n\tp.reload(c, false)\n\tassert.Equal(t, newDelay, p.GetDelay())\n\tassert.Equal(t, true, p.GetRespond())\n}\n"
        },
        {
          "name": "relay_manager.go",
          "type": "blob",
          "size": 11.84375,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/netip\"\n\t\"sync/atomic\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/header\"\n)\n\ntype relayManager struct {\n\tl       *logrus.Logger\n\thostmap *HostMap\n\tamRelay atomic.Bool\n}\n\nfunc NewRelayManager(ctx context.Context, l *logrus.Logger, hostmap *HostMap, c *config.C) *relayManager {\n\trm := &relayManager{\n\t\tl:       l,\n\t\thostmap: hostmap,\n\t}\n\trm.reload(c, true)\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\terr := rm.reload(c, false)\n\t\tif err != nil {\n\t\t\tl.WithError(err).Error(\"Failed to reload relay_manager\")\n\t\t}\n\t})\n\treturn rm\n}\n\nfunc (rm *relayManager) reload(c *config.C, initial bool) error {\n\tif initial || c.HasChanged(\"relay.am_relay\") {\n\t\trm.setAmRelay(c.GetBool(\"relay.am_relay\", false))\n\t}\n\treturn nil\n}\n\nfunc (rm *relayManager) GetAmRelay() bool {\n\treturn rm.amRelay.Load()\n}\n\nfunc (rm *relayManager) setAmRelay(v bool) {\n\trm.amRelay.Store(v)\n}\n\n// AddRelay finds an available relay index on the hostmap, and associates the relay info with it.\n// relayHostInfo is the Nebula peer which can be used as a relay to access the target vpnIp.\nfunc AddRelay(l *logrus.Logger, relayHostInfo *HostInfo, hm *HostMap, vpnIp netip.Addr, remoteIdx *uint32, relayType int, state int) (uint32, error) {\n\thm.Lock()\n\tdefer hm.Unlock()\n\tfor i := 0; i < 32; i++ {\n\t\tindex, err := generateIndex(l)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\t_, inRelays := hm.Relays[index]\n\t\tif !inRelays {\n\t\t\t// Avoid standing up a relay that can't be used since only the primary hostinfo\n\t\t\t// will be pointed to by the relay logic\n\t\t\t//TODO: if there was an existing primary and it had relay state, should we merge?\n\t\t\thm.unlockedMakePrimary(relayHostInfo)\n\n\t\t\thm.Relays[index] = relayHostInfo\n\t\t\tnewRelay := Relay{\n\t\t\t\tType:       relayType,\n\t\t\t\tState:      state,\n\t\t\t\tLocalIndex: index,\n\t\t\t\tPeerIp:     vpnIp,\n\t\t\t}\n\n\t\t\tif remoteIdx != nil {\n\t\t\t\tnewRelay.RemoteIndex = *remoteIdx\n\t\t\t}\n\t\t\trelayHostInfo.relayState.InsertRelay(vpnIp, index, &newRelay)\n\n\t\t\treturn index, nil\n\t\t}\n\t}\n\n\treturn 0, errors.New(\"failed to generate unique localIndexId\")\n}\n\n// EstablishRelay updates a Requested Relay to become an Established Relay, which can pass traffic.\nfunc (rm *relayManager) EstablishRelay(relayHostInfo *HostInfo, m *NebulaControl) (*Relay, error) {\n\trelay, ok := relayHostInfo.relayState.CompleteRelayByIdx(m.InitiatorRelayIndex, m.ResponderRelayIndex)\n\tif !ok {\n\t\trm.l.WithFields(logrus.Fields{\"relay\": relayHostInfo.vpnIp,\n\t\t\t\"initiatorRelayIndex\": m.InitiatorRelayIndex,\n\t\t\t\"relayFrom\":           m.RelayFromIp,\n\t\t\t\"relayTo\":             m.RelayToIp}).Info(\"relayManager failed to update relay\")\n\t\treturn nil, fmt.Errorf(\"unknown relay\")\n\t}\n\n\treturn relay, nil\n}\n\nfunc (rm *relayManager) HandleControlMsg(h *HostInfo, m *NebulaControl, f *Interface) {\n\n\tswitch m.Type {\n\tcase NebulaControl_CreateRelayRequest:\n\t\trm.handleCreateRelayRequest(h, f, m)\n\tcase NebulaControl_CreateRelayResponse:\n\t\trm.handleCreateRelayResponse(h, f, m)\n\t}\n\n}\n\nfunc (rm *relayManager) handleCreateRelayResponse(h *HostInfo, f *Interface, m *NebulaControl) {\n\trm.l.WithFields(logrus.Fields{\n\t\t\"relayFrom\":           m.RelayFromIp,\n\t\t\"relayTo\":             m.RelayToIp,\n\t\t\"initiatorRelayIndex\": m.InitiatorRelayIndex,\n\t\t\"responderRelayIndex\": m.ResponderRelayIndex,\n\t\t\"vpnIp\":               h.vpnIp}).\n\t\tInfo(\"handleCreateRelayResponse\")\n\ttarget := m.RelayToIp\n\t//TODO: IPV6-WORK\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], m.RelayToIp)\n\ttargetAddr := netip.AddrFrom4(b)\n\n\trelay, err := rm.EstablishRelay(h, m)\n\tif err != nil {\n\t\trm.l.WithError(err).Error(\"Failed to update relay for relayTo\")\n\t\treturn\n\t}\n\t// Do I need to complete the relays now?\n\tif relay.Type == TerminalType {\n\t\treturn\n\t}\n\t// I'm the middle man. Let the initiator know that the I've established the relay they requested.\n\tpeerHostInfo := rm.hostmap.QueryVpnIp(relay.PeerIp)\n\tif peerHostInfo == nil {\n\t\trm.l.WithField(\"relayTo\", relay.PeerIp).Error(\"Can't find a HostInfo for peer\")\n\t\treturn\n\t}\n\tpeerRelay, ok := peerHostInfo.relayState.QueryRelayForByIp(targetAddr)\n\tif !ok {\n\t\trm.l.WithField(\"relayTo\", peerHostInfo.vpnIp).Error(\"peerRelay does not have Relay state for relayTo\")\n\t\treturn\n\t}\n\tif peerRelay.State == PeerRequested {\n\t\t//TODO: IPV6-WORK\n\t\tb = peerHostInfo.vpnIp.As4()\n\t\tpeerRelay.State = Established\n\t\tresp := NebulaControl{\n\t\t\tType:                NebulaControl_CreateRelayResponse,\n\t\t\tResponderRelayIndex: peerRelay.LocalIndex,\n\t\t\tInitiatorRelayIndex: peerRelay.RemoteIndex,\n\t\t\tRelayFromIp:         binary.BigEndian.Uint32(b[:]),\n\t\t\tRelayToIp:           uint32(target),\n\t\t}\n\t\tmsg, err := resp.Marshal()\n\t\tif err != nil {\n\t\t\trm.l.\n\t\t\t\tWithError(err).Error(\"relayManager Failed to marshal Control CreateRelayResponse message to create relay\")\n\t\t} else {\n\t\t\tf.SendMessageToHostInfo(header.Control, 0, peerHostInfo, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\trm.l.WithFields(logrus.Fields{\n\t\t\t\t\"relayFrom\":           resp.RelayFromIp,\n\t\t\t\t\"relayTo\":             resp.RelayToIp,\n\t\t\t\t\"initiatorRelayIndex\": resp.InitiatorRelayIndex,\n\t\t\t\t\"responderRelayIndex\": resp.ResponderRelayIndex,\n\t\t\t\t\"vpnIp\":               peerHostInfo.vpnIp}).\n\t\t\t\tInfo(\"send CreateRelayResponse\")\n\t\t}\n\t}\n}\n\nfunc (rm *relayManager) handleCreateRelayRequest(h *HostInfo, f *Interface, m *NebulaControl) {\n\t//TODO: IPV6-WORK\n\tb := [4]byte{}\n\tbinary.BigEndian.PutUint32(b[:], m.RelayFromIp)\n\tfrom := netip.AddrFrom4(b)\n\n\tbinary.BigEndian.PutUint32(b[:], m.RelayToIp)\n\ttarget := netip.AddrFrom4(b)\n\n\tlogMsg := rm.l.WithFields(logrus.Fields{\n\t\t\"relayFrom\":           from,\n\t\t\"relayTo\":             target,\n\t\t\"initiatorRelayIndex\": m.InitiatorRelayIndex,\n\t\t\"vpnIp\":               h.vpnIp})\n\n\tlogMsg.Info(\"handleCreateRelayRequest\")\n\t// Is the source of the relay me? This should never happen, but did happen due to\n\t// an issue migrating relays over to newly re-handshaked host info objects.\n\tif from == f.myVpnNet.Addr() {\n\t\tlogMsg.WithField(\"myIP\", from).Error(\"Discarding relay request from myself\")\n\t\treturn\n\t}\n\t// Is the target of the relay me?\n\tif target == f.myVpnNet.Addr() {\n\t\texistingRelay, ok := h.relayState.QueryRelayForByIp(from)\n\t\tif ok {\n\t\t\tswitch existingRelay.State {\n\t\t\tcase Requested:\n\t\t\t\tok = h.relayState.CompleteRelayByIP(from, m.InitiatorRelayIndex)\n\t\t\t\tif !ok {\n\t\t\t\t\tlogMsg.Error(\"Relay State not found\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\tcase Established:\n\t\t\t\tif existingRelay.RemoteIndex != m.InitiatorRelayIndex {\n\t\t\t\t\t// We got a brand new Relay request, because its index is different than what we saw before.\n\t\t\t\t\t// This should never happen. The peer should never change an index, once created.\n\t\t\t\t\tlogMsg.WithFields(logrus.Fields{\n\t\t\t\t\t\t\"existingRemoteIndex\": existingRelay.RemoteIndex}).Error(\"Existing relay mismatch with CreateRelayRequest\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t_, err := AddRelay(rm.l, h, f.hostMap, from, &m.InitiatorRelayIndex, TerminalType, Established)\n\t\t\tif err != nil {\n\t\t\t\tlogMsg.WithError(err).Error(\"Failed to add relay\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\trelay, ok := h.relayState.QueryRelayForByIp(from)\n\t\tif !ok {\n\t\t\tlogMsg.Error(\"Relay State not found\")\n\t\t\treturn\n\t\t}\n\n\t\t//TODO: IPV6-WORK\n\t\tfromB := from.As4()\n\t\ttargetB := target.As4()\n\n\t\tresp := NebulaControl{\n\t\t\tType:                NebulaControl_CreateRelayResponse,\n\t\t\tResponderRelayIndex: relay.LocalIndex,\n\t\t\tInitiatorRelayIndex: relay.RemoteIndex,\n\t\t\tRelayFromIp:         binary.BigEndian.Uint32(fromB[:]),\n\t\t\tRelayToIp:           binary.BigEndian.Uint32(targetB[:]),\n\t\t}\n\t\tmsg, err := resp.Marshal()\n\t\tif err != nil {\n\t\t\tlogMsg.\n\t\t\t\tWithError(err).Error(\"relayManager Failed to marshal Control CreateRelayResponse message to create relay\")\n\t\t} else {\n\t\t\tf.SendMessageToHostInfo(header.Control, 0, h, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\trm.l.WithFields(logrus.Fields{\n\t\t\t\t//TODO: IPV6-WORK, this used to use the resp object but I am getting lazy now\n\t\t\t\t\"relayFrom\":           from,\n\t\t\t\t\"relayTo\":             target,\n\t\t\t\t\"initiatorRelayIndex\": resp.InitiatorRelayIndex,\n\t\t\t\t\"responderRelayIndex\": resp.ResponderRelayIndex,\n\t\t\t\t\"vpnIp\":               h.vpnIp}).\n\t\t\t\tInfo(\"send CreateRelayResponse\")\n\t\t}\n\t\treturn\n\t} else {\n\t\t// the target is not me. Create a relay to the target, from me.\n\t\tif !rm.GetAmRelay() {\n\t\t\treturn\n\t\t}\n\t\tpeer := rm.hostmap.QueryVpnIp(target)\n\t\tif peer == nil {\n\t\t\t// Try to establish a connection to this host. If we get a future relay request,\n\t\t\t// we'll be ready!\n\t\t\tf.Handshake(target)\n\t\t\treturn\n\t\t}\n\t\tif !peer.remote.IsValid() {\n\t\t\t// Only create relays to peers for whom I have a direct connection\n\t\t\treturn\n\t\t}\n\t\tsendCreateRequest := false\n\t\tvar index uint32\n\t\tvar err error\n\t\ttargetRelay, ok := peer.relayState.QueryRelayForByIp(from)\n\t\tif ok {\n\t\t\tindex = targetRelay.LocalIndex\n\t\t\tif targetRelay.State == Requested {\n\t\t\t\tsendCreateRequest = true\n\t\t\t}\n\t\t} else {\n\t\t\t// Allocate an index in the hostMap for this relay peer\n\t\t\tindex, err = AddRelay(rm.l, peer, f.hostMap, from, nil, ForwardingType, Requested)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tsendCreateRequest = true\n\t\t}\n\t\tif sendCreateRequest {\n\t\t\t//TODO: IPV6-WORK\n\t\t\tfromB := h.vpnIp.As4()\n\t\t\ttargetB := target.As4()\n\n\t\t\t// Send a CreateRelayRequest to the peer.\n\t\t\treq := NebulaControl{\n\t\t\t\tType:                NebulaControl_CreateRelayRequest,\n\t\t\t\tInitiatorRelayIndex: index,\n\t\t\t\tRelayFromIp:         binary.BigEndian.Uint32(fromB[:]),\n\t\t\t\tRelayToIp:           binary.BigEndian.Uint32(targetB[:]),\n\t\t\t}\n\t\t\tmsg, err := req.Marshal()\n\t\t\tif err != nil {\n\t\t\t\tlogMsg.\n\t\t\t\t\tWithError(err).Error(\"relayManager Failed to marshal Control message to create relay\")\n\t\t\t} else {\n\t\t\t\tf.SendMessageToHostInfo(header.Control, 0, peer, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\t\trm.l.WithFields(logrus.Fields{\n\t\t\t\t\t//TODO: IPV6-WORK another lazy used to use the req object\n\t\t\t\t\t\"relayFrom\":           h.vpnIp,\n\t\t\t\t\t\"relayTo\":             target,\n\t\t\t\t\t\"initiatorRelayIndex\": req.InitiatorRelayIndex,\n\t\t\t\t\t\"responderRelayIndex\": req.ResponderRelayIndex,\n\t\t\t\t\t\"vpnIp\":               target}).\n\t\t\t\t\tInfo(\"send CreateRelayRequest\")\n\t\t\t}\n\t\t}\n\t\t// Also track the half-created Relay state just received\n\t\trelay, ok := h.relayState.QueryRelayForByIp(target)\n\t\tif !ok {\n\t\t\t// Add the relay\n\t\t\tstate := PeerRequested\n\t\t\tif targetRelay != nil && targetRelay.State == Established {\n\t\t\t\tstate = Established\n\t\t\t}\n\t\t\t_, err := AddRelay(rm.l, h, f.hostMap, target, &m.InitiatorRelayIndex, ForwardingType, state)\n\t\t\tif err != nil {\n\t\t\t\tlogMsg.\n\t\t\t\t\tWithError(err).Error(\"relayManager Failed to allocate a local index for relay\")\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tswitch relay.State {\n\t\t\tcase Established:\n\t\t\t\tif relay.RemoteIndex != m.InitiatorRelayIndex {\n\t\t\t\t\t// We got a brand new Relay request, because its index is different than what we saw before.\n\t\t\t\t\t// This should never happen. The peer should never change an index, once created.\n\t\t\t\t\tlogMsg.WithFields(logrus.Fields{\n\t\t\t\t\t\t\"existingRemoteIndex\": relay.RemoteIndex}).Error(\"Existing relay mismatch with CreateRelayRequest\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t//TODO: IPV6-WORK\n\t\t\t\tfromB := h.vpnIp.As4()\n\t\t\t\ttargetB := target.As4()\n\t\t\t\tresp := NebulaControl{\n\t\t\t\t\tType:                NebulaControl_CreateRelayResponse,\n\t\t\t\t\tResponderRelayIndex: relay.LocalIndex,\n\t\t\t\t\tInitiatorRelayIndex: relay.RemoteIndex,\n\t\t\t\t\tRelayFromIp:         binary.BigEndian.Uint32(fromB[:]),\n\t\t\t\t\tRelayToIp:           binary.BigEndian.Uint32(targetB[:]),\n\t\t\t\t}\n\t\t\t\tmsg, err := resp.Marshal()\n\t\t\t\tif err != nil {\n\t\t\t\t\trm.l.\n\t\t\t\t\t\tWithError(err).Error(\"relayManager Failed to marshal Control CreateRelayResponse message to create relay\")\n\t\t\t\t} else {\n\t\t\t\t\tf.SendMessageToHostInfo(header.Control, 0, h, msg, make([]byte, 12), make([]byte, mtu))\n\t\t\t\t\trm.l.WithFields(logrus.Fields{\n\t\t\t\t\t\t//TODO: IPV6-WORK more lazy, used to use resp object\n\t\t\t\t\t\t\"relayFrom\":           h.vpnIp,\n\t\t\t\t\t\t\"relayTo\":             target,\n\t\t\t\t\t\t\"initiatorRelayIndex\": resp.InitiatorRelayIndex,\n\t\t\t\t\t\t\"responderRelayIndex\": resp.ResponderRelayIndex,\n\t\t\t\t\t\t\"vpnIp\":               h.vpnIp}).\n\t\t\t\t\t\tInfo(\"send CreateRelayResponse\")\n\t\t\t\t}\n\n\t\t\tcase Requested:\n\t\t\t\t// Keep waiting for the other relay to complete\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "remote_list.go",
          "type": "blob",
          "size": 18.896484375,
          "content": "package nebula\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"net/netip\"\n\t\"sort\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n)\n\n// forEachFunc is used to benefit folks that want to do work inside the lock\ntype forEachFunc func(addr netip.AddrPort, preferred bool)\n\n// The checkFuncs here are to simplify bulk importing LH query response logic into a single function (reset slice and iterate)\ntype checkFuncV4 func(vpnIp netip.Addr, to *Ip4AndPort) bool\ntype checkFuncV6 func(vpnIp netip.Addr, to *Ip6AndPort) bool\n\n// CacheMap is a struct that better represents the lighthouse cache for humans\n// The string key is the owners vpnIp\ntype CacheMap map[string]*Cache\n\n// Cache is the other part of CacheMap to better represent the lighthouse cache for humans\n// We don't reason about ipv4 vs ipv6 here\ntype Cache struct {\n\tLearned  []netip.AddrPort `json:\"learned,omitempty\"`\n\tReported []netip.AddrPort `json:\"reported,omitempty\"`\n\tRelay    []netip.Addr     `json:\"relay\"`\n}\n\n//TODO: Seems like we should plop static host entries in here too since the are protected by the lighthouse from deletion\n// We will never clean learned/reported information for them as it stands today\n\n// cache is an internal struct that splits v4 and v6 addresses inside the cache map\ntype cache struct {\n\tv4    *cacheV4\n\tv6    *cacheV6\n\trelay *cacheRelay\n}\n\ntype cacheRelay struct {\n\trelay []netip.Addr\n}\n\n// cacheV4 stores learned and reported ipv4 records under cache\ntype cacheV4 struct {\n\tlearned  *Ip4AndPort\n\treported []*Ip4AndPort\n}\n\n// cacheV4 stores learned and reported ipv6 records under cache\ntype cacheV6 struct {\n\tlearned  *Ip6AndPort\n\treported []*Ip6AndPort\n}\n\ntype hostnamePort struct {\n\tname string\n\tport uint16\n}\n\ntype hostnamesResults struct {\n\thostnames     []hostnamePort\n\tnetwork       string\n\tlookupTimeout time.Duration\n\tcancelFn      func()\n\tl             *logrus.Logger\n\tips           atomic.Pointer[map[netip.AddrPort]struct{}]\n}\n\nfunc NewHostnameResults(ctx context.Context, l *logrus.Logger, d time.Duration, network string, timeout time.Duration, hostPorts []string, onUpdate func()) (*hostnamesResults, error) {\n\tr := &hostnamesResults{\n\t\thostnames:     make([]hostnamePort, len(hostPorts)),\n\t\tnetwork:       network,\n\t\tlookupTimeout: timeout,\n\t\tl:             l,\n\t}\n\n\t// Fastrack IP addresses to ensure they're immediately available for use.\n\t// DNS lookups for hostnames that aren't hardcoded IP's will happen in a background goroutine.\n\tperformBackgroundLookup := false\n\tips := map[netip.AddrPort]struct{}{}\n\tfor idx, hostPort := range hostPorts {\n\n\t\trIp, sPort, err := net.SplitHostPort(hostPort)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tiPort, err := strconv.Atoi(sPort)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tr.hostnames[idx] = hostnamePort{name: rIp, port: uint16(iPort)}\n\t\taddr, err := netip.ParseAddr(rIp)\n\t\tif err != nil {\n\t\t\t// This address is a hostname, not an IP address\n\t\t\tperformBackgroundLookup = true\n\t\t\tcontinue\n\t\t}\n\n\t\t// Save the IP address immediately\n\t\tips[netip.AddrPortFrom(addr, uint16(iPort))] = struct{}{}\n\t}\n\tr.ips.Store(&ips)\n\n\t// Time for the DNS lookup goroutine\n\tif performBackgroundLookup {\n\t\tnewCtx, cancel := context.WithCancel(ctx)\n\t\tr.cancelFn = cancel\n\t\tticker := time.NewTicker(d)\n\t\tgo func() {\n\t\t\tdefer ticker.Stop()\n\t\t\tfor {\n\t\t\t\tnetipAddrs := map[netip.AddrPort]struct{}{}\n\t\t\t\tfor _, hostPort := range r.hostnames {\n\t\t\t\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, r.lookupTimeout)\n\t\t\t\t\taddrs, err := net.DefaultResolver.LookupNetIP(timeoutCtx, r.network, hostPort.name)\n\t\t\t\t\ttimeoutCancel()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tl.WithFields(logrus.Fields{\"hostname\": hostPort.name, \"network\": r.network}).WithError(err).Error(\"DNS resolution failed for static_map host\")\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tfor _, a := range addrs {\n\t\t\t\t\t\tnetipAddrs[netip.AddrPortFrom(a.Unmap(), hostPort.port)] = struct{}{}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\torigSet := r.ips.Load()\n\t\t\t\tdifferent := false\n\t\t\t\tfor a := range *origSet {\n\t\t\t\t\tif _, ok := netipAddrs[a]; !ok {\n\t\t\t\t\t\tdifferent = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !different {\n\t\t\t\t\tfor a := range netipAddrs {\n\t\t\t\t\t\tif _, ok := (*origSet)[a]; !ok {\n\t\t\t\t\t\t\tdifferent = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif different {\n\t\t\t\t\tl.WithFields(logrus.Fields{\"origSet\": origSet, \"newSet\": netipAddrs}).Info(\"DNS results changed for host list\")\n\t\t\t\t\tr.ips.Store(&netipAddrs)\n\t\t\t\t\tonUpdate()\n\t\t\t\t}\n\t\t\t\tselect {\n\t\t\t\tcase <-newCtx.Done():\n\t\t\t\t\treturn\n\t\t\t\tcase <-ticker.C:\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\n\treturn r, nil\n}\n\nfunc (hr *hostnamesResults) Cancel() {\n\tif hr != nil && hr.cancelFn != nil {\n\t\thr.cancelFn()\n\t}\n}\n\nfunc (hr *hostnamesResults) GetIPs() []netip.AddrPort {\n\tvar retSlice []netip.AddrPort\n\tif hr != nil {\n\t\tp := hr.ips.Load()\n\t\tif p != nil {\n\t\t\tfor k := range *p {\n\t\t\t\tretSlice = append(retSlice, k)\n\t\t\t}\n\t\t}\n\t}\n\treturn retSlice\n}\n\n// RemoteList is a unifying concept for lighthouse servers and clients as well as hostinfos.\n// It serves as a local cache of query replies, host update notifications, and locally learned addresses\ntype RemoteList struct {\n\t// Every interaction with internals requires a lock!\n\tsync.RWMutex\n\n\t// A deduplicated set of addresses. Any accessor should lock beforehand.\n\taddrs []netip.AddrPort\n\n\t// A set of relay addresses. VpnIp addresses that the remote identified as relays.\n\trelays []netip.Addr\n\n\t// These are maps to store v4 and v6 addresses per lighthouse\n\t// Map key is the vpnIp of the person that told us about this the cached entries underneath.\n\t// For learned addresses, this is the vpnIp that sent the packet\n\tcache map[netip.Addr]*cache\n\n\thr        *hostnamesResults\n\tshouldAdd func(netip.Addr) bool\n\n\t// This is a list of remotes that we have tried to handshake with and have returned from the wrong vpn ip.\n\t// They should not be tried again during a handshake\n\tbadRemotes []netip.AddrPort\n\n\t// A flag that the cache may have changed and addrs needs to be rebuilt\n\tshouldRebuild bool\n}\n\n// NewRemoteList creates a new empty RemoteList\nfunc NewRemoteList(shouldAdd func(netip.Addr) bool) *RemoteList {\n\treturn &RemoteList{\n\t\taddrs:     make([]netip.AddrPort, 0),\n\t\trelays:    make([]netip.Addr, 0),\n\t\tcache:     make(map[netip.Addr]*cache),\n\t\tshouldAdd: shouldAdd,\n\t}\n}\n\nfunc (r *RemoteList) unlockedSetHostnamesResults(hr *hostnamesResults) {\n\t// Cancel any existing hostnamesResults DNS goroutine to release resources\n\tr.hr.Cancel()\n\tr.hr = hr\n}\n\n// Len locks and reports the size of the deduplicated address list\n// The deduplication work may need to occur here, so you must pass preferredRanges\nfunc (r *RemoteList) Len(preferredRanges []netip.Prefix) int {\n\tr.Rebuild(preferredRanges)\n\tr.RLock()\n\tdefer r.RUnlock()\n\treturn len(r.addrs)\n}\n\n// ForEach locks and will call the forEachFunc for every deduplicated address in the list\n// The deduplication work may need to occur here, so you must pass preferredRanges\nfunc (r *RemoteList) ForEach(preferredRanges []netip.Prefix, forEach forEachFunc) {\n\tr.Rebuild(preferredRanges)\n\tr.RLock()\n\tfor _, v := range r.addrs {\n\t\tforEach(v, isPreferred(v.Addr(), preferredRanges))\n\t}\n\tr.RUnlock()\n}\n\n// CopyAddrs locks and makes a deep copy of the deduplicated address list\n// The deduplication work may need to occur here, so you must pass preferredRanges\nfunc (r *RemoteList) CopyAddrs(preferredRanges []netip.Prefix) []netip.AddrPort {\n\tif r == nil {\n\t\treturn nil\n\t}\n\n\tr.Rebuild(preferredRanges)\n\n\tr.RLock()\n\tdefer r.RUnlock()\n\tc := make([]netip.AddrPort, len(r.addrs))\n\tfor i, v := range r.addrs {\n\t\tc[i] = v\n\t}\n\treturn c\n}\n\n// LearnRemote locks and sets the learned slot for the owner vpn ip to the provided addr\n// Currently this is only needed when HostInfo.SetRemote is called as that should cover both handshaking and roaming.\n// It will mark the deduplicated address list as dirty, so do not call it unless new information is available\n// TODO: this needs to support the allow list list\nfunc (r *RemoteList) LearnRemote(ownerVpnIp netip.Addr, remote netip.AddrPort) {\n\tr.Lock()\n\tdefer r.Unlock()\n\tif remote.Addr().Is4() {\n\t\tr.unlockedSetLearnedV4(ownerVpnIp, NewIp4AndPortFromNetIP(remote.Addr(), remote.Port()))\n\t} else {\n\t\tr.unlockedSetLearnedV6(ownerVpnIp, NewIp6AndPortFromNetIP(remote.Addr(), remote.Port()))\n\t}\n}\n\n// CopyCache locks and creates a more human friendly form of the internal address cache.\n// This may contain duplicates and blocked addresses\nfunc (r *RemoteList) CopyCache() *CacheMap {\n\tr.RLock()\n\tdefer r.RUnlock()\n\n\tcm := make(CacheMap)\n\tgetOrMake := func(vpnIp string) *Cache {\n\t\tc := cm[vpnIp]\n\t\tif c == nil {\n\t\t\tc = &Cache{\n\t\t\t\tLearned:  make([]netip.AddrPort, 0),\n\t\t\t\tReported: make([]netip.AddrPort, 0),\n\t\t\t\tRelay:    make([]netip.Addr, 0),\n\t\t\t}\n\t\t\tcm[vpnIp] = c\n\t\t}\n\t\treturn c\n\t}\n\n\tfor owner, mc := range r.cache {\n\t\tc := getOrMake(owner.String())\n\n\t\tif mc.v4 != nil {\n\t\t\tif mc.v4.learned != nil {\n\t\t\t\tc.Learned = append(c.Learned, AddrPortFromIp4AndPort(mc.v4.learned))\n\t\t\t}\n\n\t\t\tfor _, a := range mc.v4.reported {\n\t\t\t\tc.Reported = append(c.Reported, AddrPortFromIp4AndPort(a))\n\t\t\t}\n\t\t}\n\n\t\tif mc.v6 != nil {\n\t\t\tif mc.v6.learned != nil {\n\t\t\t\tc.Learned = append(c.Learned, AddrPortFromIp6AndPort(mc.v6.learned))\n\t\t\t}\n\n\t\t\tfor _, a := range mc.v6.reported {\n\t\t\t\tc.Reported = append(c.Reported, AddrPortFromIp6AndPort(a))\n\t\t\t}\n\t\t}\n\n\t\tif mc.relay != nil {\n\t\t\tfor _, a := range mc.relay.relay {\n\t\t\t\tc.Relay = append(c.Relay, a)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn &cm\n}\n\n// BlockRemote locks and records the address as bad, it will be excluded from the deduplicated address list\nfunc (r *RemoteList) BlockRemote(bad netip.AddrPort) {\n\tif !bad.IsValid() {\n\t\t// relays can have nil udp Addrs\n\t\treturn\n\t}\n\tr.Lock()\n\tdefer r.Unlock()\n\n\t// Check if we already blocked this addr\n\tif r.unlockedIsBad(bad) {\n\t\treturn\n\t}\n\n\t// We copy here because we are taking something else's memory and we can't trust everything\n\tr.badRemotes = append(r.badRemotes, bad)\n\n\t// Mark the next interaction must recollect/dedupe\n\tr.shouldRebuild = true\n}\n\n// CopyBlockedRemotes locks and makes a deep copy of the blocked remotes list\nfunc (r *RemoteList) CopyBlockedRemotes() []netip.AddrPort {\n\tr.RLock()\n\tdefer r.RUnlock()\n\n\tc := make([]netip.AddrPort, len(r.badRemotes))\n\tfor i, v := range r.badRemotes {\n\t\tc[i] = v\n\t}\n\treturn c\n}\n\n// ResetBlockedRemotes locks and clears the blocked remotes list\nfunc (r *RemoteList) ResetBlockedRemotes() {\n\tr.Lock()\n\tr.badRemotes = nil\n\tr.Unlock()\n}\n\n// Rebuild locks and generates the deduplicated address list only if there is work to be done\n// There is generally no reason to call this directly but it is safe to do so\nfunc (r *RemoteList) Rebuild(preferredRanges []netip.Prefix) {\n\tr.Lock()\n\tdefer r.Unlock()\n\n\t// Only rebuild if the cache changed\n\t//TODO: shouldRebuild is probably pointless as we don't check for actual change when lighthouse updates come in\n\tif r.shouldRebuild {\n\t\tr.unlockedCollect()\n\t\tr.shouldRebuild = false\n\t}\n\n\t// Always re-sort, preferredRanges can change via HUP\n\tr.unlockedSort(preferredRanges)\n}\n\n// unlockedIsBad assumes you have the write lock and checks if the remote matches any entry in the blocked address list\nfunc (r *RemoteList) unlockedIsBad(remote netip.AddrPort) bool {\n\tfor _, v := range r.badRemotes {\n\t\tif v == remote {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// unlockedSetLearnedV4 assumes you have the write lock and sets the current learned address for this owner and marks the\n// deduplicated address list as dirty\nfunc (r *RemoteList) unlockedSetLearnedV4(ownerVpnIp netip.Addr, to *Ip4AndPort) {\n\tr.shouldRebuild = true\n\tr.unlockedGetOrMakeV4(ownerVpnIp).learned = to\n}\n\n// unlockedSetV4 assumes you have the write lock and resets the reported list of ips for this owner to the list provided\n// and marks the deduplicated address list as dirty\nfunc (r *RemoteList) unlockedSetV4(ownerVpnIp, vpnIp netip.Addr, to []*Ip4AndPort, check checkFuncV4) {\n\tr.shouldRebuild = true\n\tc := r.unlockedGetOrMakeV4(ownerVpnIp)\n\n\t// Reset the slice\n\tc.reported = c.reported[:0]\n\n\t// We can't take their array but we can take their pointers\n\tfor _, v := range to[:minInt(len(to), MaxRemotes)] {\n\t\tif check(vpnIp, v) {\n\t\t\tc.reported = append(c.reported, v)\n\t\t}\n\t}\n}\n\nfunc (r *RemoteList) unlockedSetRelay(ownerVpnIp, vpnIp netip.Addr, to []netip.Addr) {\n\tr.shouldRebuild = true\n\tc := r.unlockedGetOrMakeRelay(ownerVpnIp)\n\n\t// Reset the slice\n\tc.relay = c.relay[:0]\n\n\t// We can't take their array but we can take their pointers\n\tc.relay = append(c.relay, to[:minInt(len(to), MaxRemotes)]...)\n}\n\n// unlockedPrependV4 assumes you have the write lock and prepends the address in the reported list for this owner\n// This is only useful for establishing static hosts\nfunc (r *RemoteList) unlockedPrependV4(ownerVpnIp netip.Addr, to *Ip4AndPort) {\n\tr.shouldRebuild = true\n\tc := r.unlockedGetOrMakeV4(ownerVpnIp)\n\n\t// We are doing the easy append because this is rarely called\n\tc.reported = append([]*Ip4AndPort{to}, c.reported...)\n\tif len(c.reported) > MaxRemotes {\n\t\tc.reported = c.reported[:MaxRemotes]\n\t}\n}\n\n// unlockedSetLearnedV6 assumes you have the write lock and sets the current learned address for this owner and marks the\n// deduplicated address list as dirty\nfunc (r *RemoteList) unlockedSetLearnedV6(ownerVpnIp netip.Addr, to *Ip6AndPort) {\n\tr.shouldRebuild = true\n\tr.unlockedGetOrMakeV6(ownerVpnIp).learned = to\n}\n\n// unlockedSetV6 assumes you have the write lock and resets the reported list of ips for this owner to the list provided\n// and marks the deduplicated address list as dirty\nfunc (r *RemoteList) unlockedSetV6(ownerVpnIp, vpnIp netip.Addr, to []*Ip6AndPort, check checkFuncV6) {\n\tr.shouldRebuild = true\n\tc := r.unlockedGetOrMakeV6(ownerVpnIp)\n\n\t// Reset the slice\n\tc.reported = c.reported[:0]\n\n\t// We can't take their array but we can take their pointers\n\tfor _, v := range to[:minInt(len(to), MaxRemotes)] {\n\t\tif check(vpnIp, v) {\n\t\t\tc.reported = append(c.reported, v)\n\t\t}\n\t}\n}\n\n// unlockedPrependV6 assumes you have the write lock and prepends the address in the reported list for this owner\n// This is only useful for establishing static hosts\nfunc (r *RemoteList) unlockedPrependV6(ownerVpnIp netip.Addr, to *Ip6AndPort) {\n\tr.shouldRebuild = true\n\tc := r.unlockedGetOrMakeV6(ownerVpnIp)\n\n\t// We are doing the easy append because this is rarely called\n\tc.reported = append([]*Ip6AndPort{to}, c.reported...)\n\tif len(c.reported) > MaxRemotes {\n\t\tc.reported = c.reported[:MaxRemotes]\n\t}\n}\n\nfunc (r *RemoteList) unlockedGetOrMakeRelay(ownerVpnIp netip.Addr) *cacheRelay {\n\tam := r.cache[ownerVpnIp]\n\tif am == nil {\n\t\tam = &cache{}\n\t\tr.cache[ownerVpnIp] = am\n\t}\n\t// Avoid occupying memory for relay if we never have any\n\tif am.relay == nil {\n\t\tam.relay = &cacheRelay{}\n\t}\n\treturn am.relay\n}\n\n// unlockedGetOrMakeV4 assumes you have the write lock and builds the cache and owner entry. Only the v4 pointer is established.\n// The caller must dirty the learned address cache if required\nfunc (r *RemoteList) unlockedGetOrMakeV4(ownerVpnIp netip.Addr) *cacheV4 {\n\tam := r.cache[ownerVpnIp]\n\tif am == nil {\n\t\tam = &cache{}\n\t\tr.cache[ownerVpnIp] = am\n\t}\n\t// Avoid occupying memory for v6 addresses if we never have any\n\tif am.v4 == nil {\n\t\tam.v4 = &cacheV4{}\n\t}\n\treturn am.v4\n}\n\n// unlockedGetOrMakeV6 assumes you have the write lock and builds the cache and owner entry. Only the v6 pointer is established.\n// The caller must dirty the learned address cache if required\nfunc (r *RemoteList) unlockedGetOrMakeV6(ownerVpnIp netip.Addr) *cacheV6 {\n\tam := r.cache[ownerVpnIp]\n\tif am == nil {\n\t\tam = &cache{}\n\t\tr.cache[ownerVpnIp] = am\n\t}\n\t// Avoid occupying memory for v4 addresses if we never have any\n\tif am.v6 == nil {\n\t\tam.v6 = &cacheV6{}\n\t}\n\treturn am.v6\n}\n\n// unlockedCollect assumes you have the write lock and collects/transforms the cache into the deduped address list.\n// The result of this function can contain duplicates. unlockedSort handles cleaning it.\nfunc (r *RemoteList) unlockedCollect() {\n\taddrs := r.addrs[:0]\n\trelays := r.relays[:0]\n\n\tfor _, c := range r.cache {\n\t\tif c.v4 != nil {\n\t\t\tif c.v4.learned != nil {\n\t\t\t\tu := AddrPortFromIp4AndPort(c.v4.learned)\n\t\t\t\tif !r.unlockedIsBad(u) {\n\t\t\t\t\taddrs = append(addrs, u)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor _, v := range c.v4.reported {\n\t\t\t\tu := AddrPortFromIp4AndPort(v)\n\t\t\t\tif !r.unlockedIsBad(u) {\n\t\t\t\t\taddrs = append(addrs, u)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif c.v6 != nil {\n\t\t\tif c.v6.learned != nil {\n\t\t\t\tu := AddrPortFromIp6AndPort(c.v6.learned)\n\t\t\t\tif !r.unlockedIsBad(u) {\n\t\t\t\t\taddrs = append(addrs, u)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor _, v := range c.v6.reported {\n\t\t\t\tu := AddrPortFromIp6AndPort(v)\n\t\t\t\tif !r.unlockedIsBad(u) {\n\t\t\t\t\taddrs = append(addrs, u)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif c.relay != nil {\n\t\t\tfor _, v := range c.relay.relay {\n\t\t\t\trelays = append(relays, v)\n\t\t\t}\n\t\t}\n\t}\n\n\tdnsAddrs := r.hr.GetIPs()\n\tfor _, addr := range dnsAddrs {\n\t\tif r.shouldAdd == nil || r.shouldAdd(addr.Addr()) {\n\t\t\tif !r.unlockedIsBad(addr) {\n\t\t\t\taddrs = append(addrs, addr)\n\t\t\t}\n\t\t}\n\t}\n\n\tr.addrs = addrs\n\tr.relays = relays\n\n}\n\n// unlockedSort assumes you have the write lock and performs the deduping and sorting of the address list\nfunc (r *RemoteList) unlockedSort(preferredRanges []netip.Prefix) {\n\tn := len(r.addrs)\n\tif n < 2 {\n\t\treturn\n\t}\n\n\tlessFunc := func(i, j int) bool {\n\t\ta := r.addrs[i]\n\t\tb := r.addrs[j]\n\t\t// Preferred addresses first\n\n\t\taPref := isPreferred(a.Addr(), preferredRanges)\n\t\tbPref := isPreferred(b.Addr(), preferredRanges)\n\t\tswitch {\n\t\tcase aPref && !bPref:\n\t\t\t// If i is preferred and j is not, i is less than j\n\t\t\treturn true\n\n\t\tcase !aPref && bPref:\n\t\t\t// If j is preferred then i is not due to the else, i is not less than j\n\t\t\treturn false\n\n\t\tdefault:\n\t\t\t// Both i an j are either preferred or not, sort within that\n\t\t}\n\n\t\t// ipv6 addresses 2nd\n\t\ta4 := a.Addr().Is4()\n\t\tb4 := b.Addr().Is4()\n\t\tswitch {\n\t\tcase a4 == false && b4 == true:\n\t\t\t// If i is v6 and j is v4, i is less than j\n\t\t\treturn true\n\n\t\tcase a4 == true && b4 == false:\n\t\t\t// If j is v6 and i is v4, i is not less than j\n\t\t\treturn false\n\n\t\tcase a4 == true && b4 == true:\n\t\t\t// i and j are both ipv4\n\t\t\taPrivate := a.Addr().IsPrivate()\n\t\t\tbPrivate := b.Addr().IsPrivate()\n\t\t\tswitch {\n\t\t\tcase !aPrivate && bPrivate:\n\t\t\t\t// If i is a public ip (not private) and j is a private ip, i is less then j\n\t\t\t\treturn true\n\n\t\t\tcase aPrivate && !bPrivate:\n\t\t\t\t// If j is public (not private) then i is private due to the else, i is not less than j\n\t\t\t\treturn false\n\n\t\t\tdefault:\n\t\t\t\t// Both i an j are either public or private, sort within that\n\t\t\t}\n\n\t\tdefault:\n\t\t\t// Both i an j are either ipv4 or ipv6, sort within that\n\t\t}\n\n\t\t// lexical order of ips 3rd\n\t\tc := a.Addr().Compare(b.Addr())\n\t\tif c == 0 {\n\t\t\t// Ips are the same, Lexical order of ports 4th\n\t\t\treturn a.Port() < b.Port()\n\t\t}\n\n\t\t// Ip wasn't the same\n\t\treturn c < 0\n\t}\n\n\t// Sort it\n\tsort.Slice(r.addrs, lessFunc)\n\n\t// Deduplicate\n\ta, b := 0, 1\n\tfor b < n {\n\t\tif r.addrs[a] != r.addrs[b] {\n\t\t\ta++\n\t\t\tif a != b {\n\t\t\t\tr.addrs[a], r.addrs[b] = r.addrs[b], r.addrs[a]\n\t\t\t}\n\t\t}\n\t\tb++\n\t}\n\n\tr.addrs = r.addrs[:a+1]\n\treturn\n}\n\n// minInt returns the minimum integer of a or b\nfunc minInt(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}\n\n// isPreferred returns true of the ip is contained in the preferredRanges list\nfunc isPreferred(ip netip.Addr, preferredRanges []netip.Prefix) bool {\n\t//TODO: this would be better in a CIDR6Tree\n\tfor _, p := range preferredRanges {\n\t\tif p.Contains(ip) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n"
        },
        {
          "name": "remote_list_test.go",
          "type": "blob",
          "size": 7.892578125,
          "content": "package nebula\n\nimport (\n\t\"encoding/binary\"\n\t\"net/netip\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestRemoteList_Rebuild(t *testing.T) {\n\trl := NewRemoteList(nil)\n\trl.unlockedSetV4(\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\t[]*Ip4AndPort{\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1475\"), // this is duped\n\t\t\tnewIp4AndPortFromString(\"172.17.0.182:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"), // this is duped\n\t\t\tnewIp4AndPortFromString(\"172.18.0.1:10101\"), // this is duped\n\t\t\tnewIp4AndPortFromString(\"172.18.0.1:10101\"), // this is a dupe\n\t\t\tnewIp4AndPortFromString(\"172.19.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.31.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"),   // this is a dupe\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1476\"), // almost dupe of 0 with a diff port\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1475\"), // this is a dupe\n\t\t},\n\t\tfunc(netip.Addr, *Ip4AndPort) bool { return true },\n\t)\n\n\trl.unlockedSetV6(\n\t\tnetip.MustParseAddr(\"0.0.0.1\"),\n\t\tnetip.MustParseAddr(\"0.0.0.1\"),\n\t\t[]*Ip6AndPort{\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"), // this is duped\n\t\t\tnewIp6AndPortFromString(\"[1::1]:2\"), // almost dupe of 0 with a diff port, also gets duped\n\t\t\tnewIp6AndPortFromString(\"[1:100::1]:1\"),\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"), // this is a dupe\n\t\t\tnewIp6AndPortFromString(\"[1::1]:2\"), // this is a dupe\n\t\t},\n\t\tfunc(netip.Addr, *Ip6AndPort) bool { return true },\n\t)\n\n\trl.Rebuild([]netip.Prefix{})\n\tassert.Len(t, rl.addrs, 10, \"addrs contains too many entries\")\n\n\t// ipv6 first, sorted lexically within\n\tassert.Equal(t, \"[1::1]:1\", rl.addrs[0].String())\n\tassert.Equal(t, \"[1::1]:2\", rl.addrs[1].String())\n\tassert.Equal(t, \"[1:100::1]:1\", rl.addrs[2].String())\n\n\t// ipv4 last, sorted by public first, then private, lexically within them\n\tassert.Equal(t, \"70.199.182.92:1475\", rl.addrs[3].String())\n\tassert.Equal(t, \"70.199.182.92:1476\", rl.addrs[4].String())\n\tassert.Equal(t, \"172.17.0.182:10101\", rl.addrs[5].String())\n\tassert.Equal(t, \"172.17.1.1:10101\", rl.addrs[6].String())\n\tassert.Equal(t, \"172.18.0.1:10101\", rl.addrs[7].String())\n\tassert.Equal(t, \"172.19.0.1:10101\", rl.addrs[8].String())\n\tassert.Equal(t, \"172.31.0.1:10101\", rl.addrs[9].String())\n\n\t// Now ensure we can hoist ipv4 up\n\trl.Rebuild([]netip.Prefix{netip.MustParsePrefix(\"0.0.0.0/0\")})\n\tassert.Len(t, rl.addrs, 10, \"addrs contains too many entries\")\n\n\t// ipv4 first, public then private, lexically within them\n\tassert.Equal(t, \"70.199.182.92:1475\", rl.addrs[0].String())\n\tassert.Equal(t, \"70.199.182.92:1476\", rl.addrs[1].String())\n\tassert.Equal(t, \"172.17.0.182:10101\", rl.addrs[2].String())\n\tassert.Equal(t, \"172.17.1.1:10101\", rl.addrs[3].String())\n\tassert.Equal(t, \"172.18.0.1:10101\", rl.addrs[4].String())\n\tassert.Equal(t, \"172.19.0.1:10101\", rl.addrs[5].String())\n\tassert.Equal(t, \"172.31.0.1:10101\", rl.addrs[6].String())\n\n\t// ipv6 last, sorted by public first, then private, lexically within them\n\tassert.Equal(t, \"[1::1]:1\", rl.addrs[7].String())\n\tassert.Equal(t, \"[1::1]:2\", rl.addrs[8].String())\n\tassert.Equal(t, \"[1:100::1]:1\", rl.addrs[9].String())\n\n\t// Ensure we can hoist a specific ipv4 range over anything else\n\trl.Rebuild([]netip.Prefix{netip.MustParsePrefix(\"172.17.0.0/16\")})\n\tassert.Len(t, rl.addrs, 10, \"addrs contains too many entries\")\n\n\t// Preferred ipv4 first\n\tassert.Equal(t, \"172.17.0.182:10101\", rl.addrs[0].String())\n\tassert.Equal(t, \"172.17.1.1:10101\", rl.addrs[1].String())\n\n\t// ipv6 next\n\tassert.Equal(t, \"[1::1]:1\", rl.addrs[2].String())\n\tassert.Equal(t, \"[1::1]:2\", rl.addrs[3].String())\n\tassert.Equal(t, \"[1:100::1]:1\", rl.addrs[4].String())\n\n\t// the remaining ipv4 last\n\tassert.Equal(t, \"70.199.182.92:1475\", rl.addrs[5].String())\n\tassert.Equal(t, \"70.199.182.92:1476\", rl.addrs[6].String())\n\tassert.Equal(t, \"172.18.0.1:10101\", rl.addrs[7].String())\n\tassert.Equal(t, \"172.19.0.1:10101\", rl.addrs[8].String())\n\tassert.Equal(t, \"172.31.0.1:10101\", rl.addrs[9].String())\n}\n\nfunc BenchmarkFullRebuild(b *testing.B) {\n\trl := NewRemoteList(nil)\n\trl.unlockedSetV4(\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\t[]*Ip4AndPort{\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1475\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.0.182:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.18.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.19.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.31.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"),   // this is a dupe\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1476\"), // dupe of 0 with a diff port\n\t\t},\n\t\tfunc(netip.Addr, *Ip4AndPort) bool { return true },\n\t)\n\n\trl.unlockedSetV6(\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\t[]*Ip6AndPort{\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"),\n\t\t\tnewIp6AndPortFromString(\"[1::1]:2\"), // dupe of 0 with a diff port\n\t\t\tnewIp6AndPortFromString(\"[1:100::1]:1\"),\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"), // this is a dupe\n\t\t},\n\t\tfunc(netip.Addr, *Ip6AndPort) bool { return true },\n\t)\n\n\tb.Run(\"no preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.shouldRebuild = true\n\t\t\trl.Rebuild([]netip.Prefix{})\n\t\t}\n\t})\n\n\tipNet1 := netip.MustParsePrefix(\"172.17.0.0/16\")\n\tb.Run(\"1 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.shouldRebuild = true\n\t\t\trl.Rebuild([]netip.Prefix{ipNet1})\n\t\t}\n\t})\n\n\tipNet2 := netip.MustParsePrefix(\"70.0.0.0/8\")\n\tb.Run(\"2 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.shouldRebuild = true\n\t\t\trl.Rebuild([]netip.Prefix{ipNet2})\n\t\t}\n\t})\n\n\tipNet3 := netip.MustParsePrefix(\"0.0.0.0/0\")\n\tb.Run(\"3 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.shouldRebuild = true\n\t\t\trl.Rebuild([]netip.Prefix{ipNet1, ipNet2, ipNet3})\n\t\t}\n\t})\n}\n\nfunc BenchmarkSortRebuild(b *testing.B) {\n\trl := NewRemoteList(nil)\n\trl.unlockedSetV4(\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\t[]*Ip4AndPort{\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1475\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.0.182:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.18.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.19.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.31.0.1:10101\"),\n\t\t\tnewIp4AndPortFromString(\"172.17.1.1:10101\"),   // this is a dupe\n\t\t\tnewIp4AndPortFromString(\"70.199.182.92:1476\"), // dupe of 0 with a diff port\n\t\t},\n\t\tfunc(netip.Addr, *Ip4AndPort) bool { return true },\n\t)\n\n\trl.unlockedSetV6(\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\tnetip.MustParseAddr(\"0.0.0.0\"),\n\t\t[]*Ip6AndPort{\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"),\n\t\t\tnewIp6AndPortFromString(\"[1::1]:2\"), // dupe of 0 with a diff port\n\t\t\tnewIp6AndPortFromString(\"[1:100::1]:1\"),\n\t\t\tnewIp6AndPortFromString(\"[1::1]:1\"), // this is a dupe\n\t\t},\n\t\tfunc(netip.Addr, *Ip6AndPort) bool { return true },\n\t)\n\n\tb.Run(\"no preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.shouldRebuild = true\n\t\t\trl.Rebuild([]netip.Prefix{})\n\t\t}\n\t})\n\n\tipNet1 := netip.MustParsePrefix(\"172.17.0.0/16\")\n\trl.Rebuild([]netip.Prefix{ipNet1})\n\n\tb.Run(\"1 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.Rebuild([]netip.Prefix{ipNet1})\n\t\t}\n\t})\n\n\tipNet2 := netip.MustParsePrefix(\"70.0.0.0/8\")\n\trl.Rebuild([]netip.Prefix{ipNet1, ipNet2})\n\n\tb.Run(\"2 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.Rebuild([]netip.Prefix{ipNet1, ipNet2})\n\t\t}\n\t})\n\n\tipNet3 := netip.MustParsePrefix(\"0.0.0.0/0\")\n\trl.Rebuild([]netip.Prefix{ipNet1, ipNet2, ipNet3})\n\n\tb.Run(\"3 preferred\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\trl.Rebuild([]netip.Prefix{ipNet1, ipNet2, ipNet3})\n\t\t}\n\t})\n}\n\nfunc newIp4AndPortFromString(s string) *Ip4AndPort {\n\ta := netip.MustParseAddrPort(s)\n\tv4Addr := a.Addr().As4()\n\treturn &Ip4AndPort{\n\t\tIp:   binary.BigEndian.Uint32(v4Addr[:]),\n\t\tPort: uint32(a.Port()),\n\t}\n}\n\nfunc newIp6AndPortFromString(s string) *Ip6AndPort {\n\ta := netip.MustParseAddrPort(s)\n\tv6Addr := a.Addr().As16()\n\treturn &Ip6AndPort{\n\t\tHi:   binary.BigEndian.Uint64(v6Addr[:8]),\n\t\tLo:   binary.BigEndian.Uint64(v6Addr[8:]),\n\t\tPort: uint32(a.Port()),\n\t}\n}\n"
        },
        {
          "name": "service",
          "type": "tree",
          "content": null
        },
        {
          "name": "ssh.go",
          "type": "blob",
          "size": 25.7431640625,
          "content": "package nebula\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/netip\"\n\t\"os\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n\t\"github.com/slackhq/nebula/header\"\n\t\"github.com/slackhq/nebula/sshd\"\n)\n\ntype sshListHostMapFlags struct {\n\tJson    bool\n\tPretty  bool\n\tByIndex bool\n}\n\ntype sshPrintCertFlags struct {\n\tJson   bool\n\tPretty bool\n\tRaw    bool\n}\n\ntype sshPrintTunnelFlags struct {\n\tPretty bool\n}\n\ntype sshChangeRemoteFlags struct {\n\tAddress string\n}\n\ntype sshCloseTunnelFlags struct {\n\tLocalOnly bool\n}\n\ntype sshCreateTunnelFlags struct {\n\tAddress string\n}\n\ntype sshDeviceInfoFlags struct {\n\tJson   bool\n\tPretty bool\n}\n\nfunc wireSSHReload(l *logrus.Logger, ssh *sshd.SSHServer, c *config.C) {\n\tc.RegisterReloadCallback(func(c *config.C) {\n\t\tif c.GetBool(\"sshd.enabled\", false) {\n\t\t\tsshRun, err := configSSH(l, ssh, c)\n\t\t\tif err != nil {\n\t\t\t\tl.WithError(err).Error(\"Failed to reconfigure the sshd\")\n\t\t\t\tssh.Stop()\n\t\t\t}\n\t\t\tif sshRun != nil {\n\t\t\t\tgo sshRun()\n\t\t\t}\n\t\t} else {\n\t\t\tssh.Stop()\n\t\t}\n\t})\n}\n\n// configSSH reads the ssh info out of the passed-in Config and\n// updates the passed-in SSHServer. On success, it returns a function\n// that callers may invoke to run the configured ssh server. On\n// failure, it returns nil, error.\nfunc configSSH(l *logrus.Logger, ssh *sshd.SSHServer, c *config.C) (func(), error) {\n\t//TODO conntrack list\n\t//TODO print firewall rules or hash?\n\n\tlisten := c.GetString(\"sshd.listen\", \"\")\n\tif listen == \"\" {\n\t\treturn nil, fmt.Errorf(\"sshd.listen must be provided\")\n\t}\n\n\t_, port, err := net.SplitHostPort(listen)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid sshd.listen address: %s\", err)\n\t}\n\tif port == \"22\" {\n\t\treturn nil, fmt.Errorf(\"sshd.listen can not use port 22\")\n\t}\n\n\t//TODO: no good way to reload this right now\n\thostKeyPathOrKey := c.GetString(\"sshd.host_key\", \"\")\n\tif hostKeyPathOrKey == \"\" {\n\t\treturn nil, fmt.Errorf(\"sshd.host_key must be provided\")\n\t}\n\n\tvar hostKeyBytes []byte\n\tif strings.Contains(hostKeyPathOrKey, \"-----BEGIN\") {\n\t\thostKeyBytes = []byte(hostKeyPathOrKey)\n\t} else {\n\t\thostKeyBytes, err = os.ReadFile(hostKeyPathOrKey)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error while loading sshd.host_key file: %s\", err)\n\t\t}\n\t}\n\n\terr = ssh.SetHostKey(hostKeyBytes)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error while adding sshd.host_key: %s\", err)\n\t}\n\n\t// Clear existing trusted CAs and authorized keys\n\tssh.ClearTrustedCAs()\n\tssh.ClearAuthorizedKeys()\n\n\trawCAs := c.GetStringSlice(\"sshd.trusted_cas\", []string{})\n\tfor _, caAuthorizedKey := range rawCAs {\n\t\terr := ssh.AddTrustedCA(caAuthorizedKey)\n\t\tif err != nil {\n\t\t\tl.WithError(err).WithField(\"sshCA\", caAuthorizedKey).Warn(\"SSH CA had an error, ignoring\")\n\t\t\tcontinue\n\t\t}\n\t}\n\n\trawKeys := c.Get(\"sshd.authorized_users\")\n\tkeys, ok := rawKeys.([]interface{})\n\tif ok {\n\t\tfor _, rk := range keys {\n\t\t\tkDef, ok := rk.(map[interface{}]interface{})\n\t\t\tif !ok {\n\t\t\t\tl.WithField(\"sshKeyConfig\", rk).Warn(\"Authorized user had an error, ignoring\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tuser, ok := kDef[\"user\"].(string)\n\t\t\tif !ok {\n\t\t\t\tl.WithField(\"sshKeyConfig\", rk).Warn(\"Authorized user is missing the user field\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tk := kDef[\"keys\"]\n\t\t\tswitch v := k.(type) {\n\t\t\tcase string:\n\t\t\t\terr := ssh.AddAuthorizedKey(user, v)\n\t\t\t\tif err != nil {\n\t\t\t\t\tl.WithError(err).WithField(\"sshKeyConfig\", rk).WithField(\"sshKey\", v).Warn(\"Failed to authorize key\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\tcase []interface{}:\n\t\t\t\tfor _, subK := range v {\n\t\t\t\t\tsk, ok := subK.(string)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tl.WithField(\"sshKeyConfig\", rk).WithField(\"sshKey\", subK).Warn(\"Did not understand ssh key\")\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\terr := ssh.AddAuthorizedKey(user, sk)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tl.WithError(err).WithField(\"sshKeyConfig\", sk).Warn(\"Failed to authorize key\")\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\tl.WithField(\"sshKeyConfig\", rk).Warn(\"Authorized user is missing the keys field or was not understood\")\n\t\t\t}\n\t\t}\n\t} else {\n\t\tl.Info(\"no ssh users to authorize\")\n\t}\n\n\tvar runner func()\n\tif c.GetBool(\"sshd.enabled\", false) {\n\t\tssh.Stop()\n\t\trunner = func() {\n\t\t\tif err := ssh.Run(listen); err != nil {\n\t\t\t\tl.WithField(\"err\", err).Warn(\"Failed to run the SSH server\")\n\t\t\t}\n\t\t}\n\t} else {\n\t\tssh.Stop()\n\t}\n\n\treturn runner, nil\n}\n\nfunc attachCommands(l *logrus.Logger, c *config.C, ssh *sshd.SSHServer, f *Interface) {\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"list-hostmap\",\n\t\tShortDescription: \"List all known previously connected hosts\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshListHostMapFlags{}\n\t\t\tfl.BoolVar(&s.Json, \"json\", false, \"outputs as json with more information\")\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json, assumes -json\")\n\t\t\tfl.BoolVar(&s.ByIndex, \"by-index\", false, \"gets all hosts in the hostmap from the index table\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshListHostMap(f.hostMap, fs, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"list-pending-hostmap\",\n\t\tShortDescription: \"List all handshaking hosts\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshListHostMapFlags{}\n\t\t\tfl.BoolVar(&s.Json, \"json\", false, \"outputs as json with more information\")\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json, assumes -json\")\n\t\t\tfl.BoolVar(&s.ByIndex, \"by-index\", false, \"gets all hosts in the hostmap from the index table\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshListHostMap(f.handshakeManager, fs, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"list-lighthouse-addrmap\",\n\t\tShortDescription: \"List all lighthouse map entries\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshListHostMapFlags{}\n\t\t\tfl.BoolVar(&s.Json, \"json\", false, \"outputs as json with more information\")\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json, assumes -json\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshListLighthouseMap(f.lightHouse, fs, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"reload\",\n\t\tShortDescription: \"Reloads configuration from disk, same as sending HUP to the process\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshReload(c, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"start-cpu-profile\",\n\t\tShortDescription: \"Starts a cpu profile and write output to the provided file, ex: `cpu-profile.pb.gz`\",\n\t\tCallback:         sshStartCpuProfile,\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"stop-cpu-profile\",\n\t\tShortDescription: \"Stops a cpu profile and writes output to the previously provided file\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\tpprof.StopCPUProfile()\n\t\t\treturn w.WriteLine(\"If a CPU profile was running it is now stopped\")\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"save-heap-profile\",\n\t\tShortDescription: \"Saves a heap profile to the provided path, ex: `heap-profile.pb.gz`\",\n\t\tCallback:         sshGetHeapProfile,\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"mutex-profile-fraction\",\n\t\tShortDescription: \"Gets or sets runtime.SetMutexProfileFraction\",\n\t\tCallback:         sshMutexProfileFraction,\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"save-mutex-profile\",\n\t\tShortDescription: \"Saves a mutex profile to the provided path, ex: `mutex-profile.pb.gz`\",\n\t\tCallback:         sshGetMutexProfile,\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"log-level\",\n\t\tShortDescription: \"Gets or sets the current log level\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshLogLevel(l, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"log-format\",\n\t\tShortDescription: \"Gets or sets the current log format\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshLogFormat(l, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"version\",\n\t\tShortDescription: \"Prints the currently running version of nebula\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshVersion(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"device-info\",\n\t\tShortDescription: \"Prints information about the network device.\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshDeviceInfoFlags{}\n\t\t\tfl.BoolVar(&s.Json, \"json\", false, \"outputs as json with more information\")\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json, assumes -json\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshDeviceInfo(f, fs, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"print-cert\",\n\t\tShortDescription: \"Prints the current certificate being used or the certificate for the provided vpn ip\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshPrintCertFlags{}\n\t\t\tfl.BoolVar(&s.Json, \"json\", false, \"outputs as json\")\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json, assumes -json\")\n\t\t\tfl.BoolVar(&s.Raw, \"raw\", false, \"raw prints the PEM encoded certificate, not compatible with -json or -pretty\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshPrintCert(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"print-tunnel\",\n\t\tShortDescription: \"Prints json details about a tunnel for the provided vpn ip\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshPrintTunnelFlags{}\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshPrintTunnel(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"print-relays\",\n\t\tShortDescription: \"Prints json details about all relay info\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshPrintTunnelFlags{}\n\t\t\tfl.BoolVar(&s.Pretty, \"pretty\", false, \"pretty prints json\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshPrintRelays(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"change-remote\",\n\t\tShortDescription: \"Changes the remote address used in the tunnel for the provided vpn ip\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshChangeRemoteFlags{}\n\t\t\tfl.StringVar(&s.Address, \"address\", \"\", \"The new remote address, ip:port\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshChangeRemote(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"close-tunnel\",\n\t\tShortDescription: \"Closes a tunnel for the provided vpn ip\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshCloseTunnelFlags{}\n\t\t\tfl.BoolVar(&s.LocalOnly, \"local-only\", false, \"Disables notifying the remote that the tunnel is shutting down\")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshCloseTunnel(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"create-tunnel\",\n\t\tShortDescription: \"Creates a tunnel for the provided vpn ip and address\",\n\t\tHelp:             \"The lighthouses will be queried for real addresses but you can provide one as well.\",\n\t\tFlags: func() (*flag.FlagSet, interface{}) {\n\t\t\tfl := flag.NewFlagSet(\"\", flag.ContinueOnError)\n\t\t\ts := sshCreateTunnelFlags{}\n\t\t\tfl.StringVar(&s.Address, \"address\", \"\", \"Optionally provide a real remote address, ip:port \")\n\t\t\treturn fl, &s\n\t\t},\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshCreateTunnel(f, fs, a, w)\n\t\t},\n\t})\n\n\tssh.RegisterCommand(&sshd.Command{\n\t\tName:             \"query-lighthouse\",\n\t\tShortDescription: \"Query the lighthouses for the provided vpn ip\",\n\t\tHelp:             \"This command is asynchronous. Only currently known udp ips will be printed.\",\n\t\tCallback: func(fs interface{}, a []string, w sshd.StringWriter) error {\n\t\t\treturn sshQueryLighthouse(f, fs, a, w)\n\t\t},\n\t})\n}\n\nfunc sshListHostMap(hl controlHostLister, a interface{}, w sshd.StringWriter) error {\n\tfs, ok := a.(*sshListHostMapFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tvar hm []ControlHostInfo\n\tif fs.ByIndex {\n\t\thm = listHostMapIndexes(hl)\n\t} else {\n\t\thm = listHostMapHosts(hl)\n\t}\n\n\tsort.Slice(hm, func(i, j int) bool {\n\t\treturn hm[i].VpnIp.Compare(hm[j].VpnIp) < 0\n\t})\n\n\tif fs.Json || fs.Pretty {\n\t\tjs := json.NewEncoder(w.GetWriter())\n\t\tif fs.Pretty {\n\t\t\tjs.SetIndent(\"\", \"    \")\n\t\t}\n\n\t\terr := js.Encode(hm)\n\t\tif err != nil {\n\t\t\t//TODO\n\t\t\treturn nil\n\t\t}\n\n\t} else {\n\t\tfor _, v := range hm {\n\t\t\terr := w.WriteLine(fmt.Sprintf(\"%s: %s\", v.VpnIp, v.RemoteAddrs))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc sshListLighthouseMap(lightHouse *LightHouse, a interface{}, w sshd.StringWriter) error {\n\tfs, ok := a.(*sshListHostMapFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\ttype lighthouseInfo struct {\n\t\tVpnIp string    `json:\"vpnIp\"`\n\t\tAddrs *CacheMap `json:\"addrs\"`\n\t}\n\n\tlightHouse.RLock()\n\taddrMap := make([]lighthouseInfo, len(lightHouse.addrMap))\n\tx := 0\n\tfor k, v := range lightHouse.addrMap {\n\t\taddrMap[x] = lighthouseInfo{\n\t\t\tVpnIp: k.String(),\n\t\t\tAddrs: v.CopyCache(),\n\t\t}\n\t\tx++\n\t}\n\tlightHouse.RUnlock()\n\n\tsort.Slice(addrMap, func(i, j int) bool {\n\t\treturn strings.Compare(addrMap[i].VpnIp, addrMap[j].VpnIp) < 0\n\t})\n\n\tif fs.Json || fs.Pretty {\n\t\tjs := json.NewEncoder(w.GetWriter())\n\t\tif fs.Pretty {\n\t\t\tjs.SetIndent(\"\", \"    \")\n\t\t}\n\n\t\terr := js.Encode(addrMap)\n\t\tif err != nil {\n\t\t\t//TODO\n\t\t\treturn nil\n\t\t}\n\n\t} else {\n\t\tfor _, v := range addrMap {\n\t\t\tb, err := json.Marshal(v.Addrs)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = w.WriteLine(fmt.Sprintf(\"%s: %s\", v.VpnIp, string(b)))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc sshStartCpuProfile(fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\terr := w.WriteLine(\"No path to write profile provided\")\n\t\treturn err\n\t}\n\n\tfile, err := os.Create(a[0])\n\tif err != nil {\n\t\terr = w.WriteLine(fmt.Sprintf(\"Unable to create profile file: %s\", err))\n\t\treturn err\n\t}\n\n\terr = pprof.StartCPUProfile(file)\n\tif err != nil {\n\t\terr = w.WriteLine(fmt.Sprintf(\"Unable to start cpu profile: %s\", err))\n\t\treturn err\n\t}\n\n\terr = w.WriteLine(fmt.Sprintf(\"Started cpu profile, issue stop-cpu-profile to write the output to %s\", a))\n\treturn err\n}\n\nfunc sshVersion(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\treturn w.WriteLine(fmt.Sprintf(\"%s\", ifce.version))\n}\n\nfunc sshQueryLighthouse(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No vpn ip was provided\")\n\t}\n\n\tvpnIp, err := netip.ParseAddr(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tif !vpnIp.IsValid() {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tvar cm *CacheMap\n\trl := ifce.lightHouse.Query(vpnIp)\n\tif rl != nil {\n\t\tcm = rl.CopyCache()\n\t}\n\treturn json.NewEncoder(w.GetWriter()).Encode(cm)\n}\n\nfunc sshCloseTunnel(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\tflags, ok := fs.(*sshCloseTunnelFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No vpn ip was provided\")\n\t}\n\n\tvpnIp, err := netip.ParseAddr(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tif !vpnIp.IsValid() {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\thostInfo := ifce.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo == nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Could not find tunnel for vpn ip: %v\", a[0]))\n\t}\n\n\tif !flags.LocalOnly {\n\t\tifce.send(\n\t\t\theader.CloseTunnel,\n\t\t\t0,\n\t\t\thostInfo.ConnectionState,\n\t\t\thostInfo,\n\t\t\t[]byte{},\n\t\t\tmake([]byte, 12, 12),\n\t\t\tmake([]byte, mtu),\n\t\t)\n\t}\n\n\tifce.closeTunnel(hostInfo)\n\treturn w.WriteLine(\"Closed\")\n}\n\nfunc sshCreateTunnel(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\tflags, ok := fs.(*sshCreateTunnelFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No vpn ip was provided\")\n\t}\n\n\tvpnIp, err := netip.ParseAddr(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tif !vpnIp.IsValid() {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\thostInfo := ifce.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Tunnel already exists\"))\n\t}\n\n\thostInfo = ifce.handshakeManager.QueryVpnIp(vpnIp)\n\tif hostInfo != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Tunnel already handshaking\"))\n\t}\n\n\tvar addr netip.AddrPort\n\tif flags.Address != \"\" {\n\t\taddr, err = netip.ParseAddrPort(flags.Address)\n\t\tif err != nil {\n\t\t\treturn w.WriteLine(\"Address could not be parsed\")\n\t\t}\n\t}\n\n\thostInfo = ifce.handshakeManager.StartHandshake(vpnIp, nil)\n\tif addr.IsValid() {\n\t\thostInfo.SetRemote(addr)\n\t}\n\n\treturn w.WriteLine(\"Created\")\n}\n\nfunc sshChangeRemote(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\tflags, ok := fs.(*sshChangeRemoteFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No vpn ip was provided\")\n\t}\n\n\tif flags.Address == \"\" {\n\t\treturn w.WriteLine(\"No address was provided\")\n\t}\n\n\taddr, err := netip.ParseAddrPort(flags.Address)\n\tif err != nil {\n\t\treturn w.WriteLine(\"Address could not be parsed\")\n\t}\n\n\tvpnIp, err := netip.ParseAddr(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tif !vpnIp.IsValid() {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\thostInfo := ifce.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo == nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Could not find tunnel for vpn ip: %v\", a[0]))\n\t}\n\n\thostInfo.SetRemote(addr)\n\treturn w.WriteLine(\"Changed\")\n}\n\nfunc sshGetHeapProfile(fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No path to write profile provided\")\n\t}\n\n\tfile, err := os.Create(a[0])\n\tif err != nil {\n\t\terr = w.WriteLine(fmt.Sprintf(\"Unable to create profile file: %s\", err))\n\t\treturn err\n\t}\n\n\terr = pprof.WriteHeapProfile(file)\n\tif err != nil {\n\t\terr = w.WriteLine(fmt.Sprintf(\"Unable to write profile: %s\", err))\n\t\treturn err\n\t}\n\n\terr = w.WriteLine(fmt.Sprintf(\"Mem profile created at %s\", a))\n\treturn err\n}\n\nfunc sshMutexProfileFraction(fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\trate := runtime.SetMutexProfileFraction(-1)\n\t\treturn w.WriteLine(fmt.Sprintf(\"Current value: %d\", rate))\n\t}\n\n\tnewRate, err := strconv.Atoi(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Invalid argument: %s\", a[0]))\n\t}\n\n\toldRate := runtime.SetMutexProfileFraction(newRate)\n\treturn w.WriteLine(fmt.Sprintf(\"New value: %d. Old value: %d\", newRate, oldRate))\n}\n\nfunc sshGetMutexProfile(fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No path to write profile provided\")\n\t}\n\n\tfile, err := os.Create(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Unable to create profile file: %s\", err))\n\t}\n\tdefer file.Close()\n\n\tmutexProfile := pprof.Lookup(\"mutex\")\n\tif mutexProfile == nil {\n\t\treturn w.WriteLine(\"Unable to get pprof.Lookup(\\\"mutex\\\")\")\n\t}\n\n\terr = mutexProfile.WriteTo(file, 0)\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Unable to write profile: %s\", err))\n\t}\n\n\treturn w.WriteLine(fmt.Sprintf(\"Mutex profile created at %s\", a))\n}\n\nfunc sshLogLevel(l *logrus.Logger, fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Log level is: %s\", l.Level))\n\t}\n\n\tlevel, err := logrus.ParseLevel(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Unknown log level %s. Possible log levels: %s\", a, logrus.AllLevels))\n\t}\n\n\tl.SetLevel(level)\n\treturn w.WriteLine(fmt.Sprintf(\"Log level is: %s\", l.Level))\n}\n\nfunc sshLogFormat(l *logrus.Logger, fs interface{}, a []string, w sshd.StringWriter) error {\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Log format is: %s\", reflect.TypeOf(l.Formatter)))\n\t}\n\n\tlogFormat := strings.ToLower(a[0])\n\tswitch logFormat {\n\tcase \"text\":\n\t\tl.Formatter = &logrus.TextFormatter{}\n\tcase \"json\":\n\t\tl.Formatter = &logrus.JSONFormatter{}\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown log format `%s`. possible formats: %s\", logFormat, []string{\"text\", \"json\"})\n\t}\n\n\treturn w.WriteLine(fmt.Sprintf(\"Log format is: %s\", reflect.TypeOf(l.Formatter)))\n}\n\nfunc sshPrintCert(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\targs, ok := fs.(*sshPrintCertFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tcert := ifce.pki.GetCertState().Certificate\n\tif len(a) > 0 {\n\t\tvpnIp, err := netip.ParseAddr(a[0])\n\t\tif err != nil {\n\t\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t\t}\n\n\t\tif !vpnIp.IsValid() {\n\t\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t\t}\n\n\t\thostInfo := ifce.hostMap.QueryVpnIp(vpnIp)\n\t\tif hostInfo == nil {\n\t\t\treturn w.WriteLine(fmt.Sprintf(\"Could not find tunnel for vpn ip: %v\", a[0]))\n\t\t}\n\n\t\tcert = hostInfo.GetCert().Certificate\n\t}\n\n\tif args.Json || args.Pretty {\n\t\tb, err := cert.MarshalJSON()\n\t\tif err != nil {\n\t\t\t//TODO: handle it\n\t\t\treturn nil\n\t\t}\n\n\t\tif args.Pretty {\n\t\t\tbuf := new(bytes.Buffer)\n\t\t\terr := json.Indent(buf, b, \"\", \"    \")\n\t\t\tb = buf.Bytes()\n\t\t\tif err != nil {\n\t\t\t\t//TODO: handle it\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\treturn w.WriteBytes(b)\n\t}\n\n\tif args.Raw {\n\t\tb, err := cert.MarshalPEM()\n\t\tif err != nil {\n\t\t\t//TODO: handle it\n\t\t\treturn nil\n\t\t}\n\n\t\treturn w.WriteBytes(b)\n\t}\n\n\treturn w.WriteLine(cert.String())\n}\n\nfunc sshPrintRelays(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\targs, ok := fs.(*sshPrintTunnelFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\tw.WriteLine(fmt.Sprintf(\"sshPrintRelays failed to convert args type\"))\n\t\treturn nil\n\t}\n\n\trelays := map[uint32]*HostInfo{}\n\tifce.hostMap.Lock()\n\tfor k, v := range ifce.hostMap.Relays {\n\t\trelays[k] = v\n\t}\n\tifce.hostMap.Unlock()\n\n\ttype RelayFor struct {\n\t\tError          error\n\t\tType           string\n\t\tState          string\n\t\tPeerIp         netip.Addr\n\t\tLocalIndex     uint32\n\t\tRemoteIndex    uint32\n\t\tRelayedThrough []netip.Addr\n\t}\n\n\ttype RelayOutput struct {\n\t\tNebulaIp    netip.Addr\n\t\tRelayForIps []RelayFor\n\t}\n\n\ttype CmdOutput struct {\n\t\tRelays []*RelayOutput\n\t}\n\n\tco := CmdOutput{}\n\n\tenc := json.NewEncoder(w.GetWriter())\n\n\tif args.Pretty {\n\t\tenc.SetIndent(\"\", \"    \")\n\t}\n\n\tfor k, v := range relays {\n\t\tro := RelayOutput{NebulaIp: v.vpnIp}\n\t\tco.Relays = append(co.Relays, &ro)\n\t\trelayHI := ifce.hostMap.QueryVpnIp(v.vpnIp)\n\t\tif relayHI == nil {\n\t\t\tro.RelayForIps = append(ro.RelayForIps, RelayFor{Error: errors.New(\"could not find hostinfo\")})\n\t\t\tcontinue\n\t\t}\n\t\tfor _, vpnIp := range relayHI.relayState.CopyRelayForIps() {\n\t\t\trf := RelayFor{Error: nil}\n\t\t\tr, ok := relayHI.relayState.GetRelayForByIp(vpnIp)\n\t\t\tif ok {\n\t\t\t\tt := \"\"\n\t\t\t\tswitch r.Type {\n\t\t\t\tcase ForwardingType:\n\t\t\t\t\tt = \"forwarding\"\n\t\t\t\tcase TerminalType:\n\t\t\t\t\tt = \"terminal\"\n\t\t\t\tdefault:\n\t\t\t\t\tt = \"unknown\"\n\t\t\t\t}\n\n\t\t\t\ts := \"\"\n\t\t\t\tswitch r.State {\n\t\t\t\tcase Requested:\n\t\t\t\t\ts = \"requested\"\n\t\t\t\tcase Established:\n\t\t\t\t\ts = \"established\"\n\t\t\t\tdefault:\n\t\t\t\t\ts = \"unknown\"\n\t\t\t\t}\n\n\t\t\t\trf.LocalIndex = r.LocalIndex\n\t\t\t\trf.RemoteIndex = r.RemoteIndex\n\t\t\t\trf.PeerIp = r.PeerIp\n\t\t\t\trf.Type = t\n\t\t\t\trf.State = s\n\t\t\t\tif rf.LocalIndex != k {\n\t\t\t\t\trf.Error = fmt.Errorf(\"hostmap LocalIndex '%v' does not match RelayState LocalIndex\", k)\n\t\t\t\t}\n\t\t\t}\n\t\t\trelayedHI := ifce.hostMap.QueryVpnIp(vpnIp)\n\t\t\tif relayedHI != nil {\n\t\t\t\trf.RelayedThrough = append(rf.RelayedThrough, relayedHI.relayState.CopyRelayIps()...)\n\t\t\t}\n\n\t\t\tro.RelayForIps = append(ro.RelayForIps, rf)\n\t\t}\n\t}\n\terr := enc.Encode(co)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc sshPrintTunnel(ifce *Interface, fs interface{}, a []string, w sshd.StringWriter) error {\n\targs, ok := fs.(*sshPrintTunnelFlags)\n\tif !ok {\n\t\t//TODO: error\n\t\treturn nil\n\t}\n\n\tif len(a) == 0 {\n\t\treturn w.WriteLine(\"No vpn ip was provided\")\n\t}\n\n\tvpnIp, err := netip.ParseAddr(a[0])\n\tif err != nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\tif !vpnIp.IsValid() {\n\t\treturn w.WriteLine(fmt.Sprintf(\"The provided vpn ip could not be parsed: %s\", a[0]))\n\t}\n\n\thostInfo := ifce.hostMap.QueryVpnIp(vpnIp)\n\tif hostInfo == nil {\n\t\treturn w.WriteLine(fmt.Sprintf(\"Could not find tunnel for vpn ip: %v\", a[0]))\n\t}\n\n\tenc := json.NewEncoder(w.GetWriter())\n\tif args.Pretty {\n\t\tenc.SetIndent(\"\", \"    \")\n\t}\n\n\treturn enc.Encode(copyHostInfo(hostInfo, ifce.hostMap.GetPreferredRanges()))\n}\n\nfunc sshDeviceInfo(ifce *Interface, fs interface{}, w sshd.StringWriter) error {\n\n\tdata := struct {\n\t\tName string `json:\"name\"`\n\t\tCidr string `json:\"cidr\"`\n\t}{\n\t\tName: ifce.inside.Name(),\n\t\tCidr: ifce.inside.Cidr().String(),\n\t}\n\n\tflags, ok := fs.(*sshDeviceInfoFlags)\n\tif !ok {\n\t\treturn fmt.Errorf(\"internal error: expected flags to be sshDeviceInfoFlags but was %+v\", fs)\n\t}\n\n\tif flags.Json || flags.Pretty {\n\t\tjs := json.NewEncoder(w.GetWriter())\n\t\tif flags.Pretty {\n\t\t\tjs.SetIndent(\"\", \"    \")\n\t\t}\n\n\t\treturn js.Encode(data)\n\t} else {\n\t\treturn w.WriteLine(fmt.Sprintf(\"name=%v cidr=%v\", data.Name, data.Cidr))\n\t}\n}\n\nfunc sshReload(c *config.C, w sshd.StringWriter) error {\n\terr := w.WriteLine(\"Reloading config\")\n\tc.ReloadConfig()\n\treturn err\n}\n"
        },
        {
          "name": "sshd",
          "type": "tree",
          "content": null
        },
        {
          "name": "stats.go",
          "type": "blob",
          "size": 3.6044921875,
          "content": "package nebula\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"net/http\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"time\"\n\n\tgraphite \"github.com/cyberdelia/go-metrics-graphite\"\n\tmp \"github.com/nbrownus/go-metrics-prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rcrowley/go-metrics\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/slackhq/nebula/config\"\n)\n\n// startStats initializes stats from config. On success, if any further work\n// is needed to serve stats, it returns a func to handle that work. If no\n// work is needed, it'll return nil. On failure, it returns nil, error.\nfunc startStats(l *logrus.Logger, c *config.C, buildVersion string, configTest bool) (func(), error) {\n\tmType := c.GetString(\"stats.type\", \"\")\n\tif mType == \"\" || mType == \"none\" {\n\t\treturn nil, nil\n\t}\n\n\tinterval := c.GetDuration(\"stats.interval\", 0)\n\tif interval == 0 {\n\t\treturn nil, fmt.Errorf(\"stats.interval was an invalid duration: %s\", c.GetString(\"stats.interval\", \"\"))\n\t}\n\n\tvar startFn func()\n\tswitch mType {\n\tcase \"graphite\":\n\t\terr := startGraphiteStats(l, interval, c, configTest)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tcase \"prometheus\":\n\t\tvar err error\n\t\tstartFn, err = startPrometheusStats(l, interval, c, buildVersion, configTest)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"stats.type was not understood: %s\", mType)\n\t}\n\n\tmetrics.RegisterDebugGCStats(metrics.DefaultRegistry)\n\tmetrics.RegisterRuntimeMemStats(metrics.DefaultRegistry)\n\n\tgo metrics.CaptureDebugGCStats(metrics.DefaultRegistry, interval)\n\tgo metrics.CaptureRuntimeMemStats(metrics.DefaultRegistry, interval)\n\n\treturn startFn, nil\n}\n\nfunc startGraphiteStats(l *logrus.Logger, i time.Duration, c *config.C, configTest bool) error {\n\tproto := c.GetString(\"stats.protocol\", \"tcp\")\n\thost := c.GetString(\"stats.host\", \"\")\n\tif host == \"\" {\n\t\treturn errors.New(\"stats.host can not be empty\")\n\t}\n\n\tprefix := c.GetString(\"stats.prefix\", \"nebula\")\n\taddr, err := net.ResolveTCPAddr(proto, host)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error while setting up graphite sink: %s\", err)\n\t}\n\n\tif !configTest {\n\t\tl.Infof(\"Starting graphite. Interval: %s, prefix: %s, addr: %s\", i, prefix, addr)\n\t\tgo graphite.Graphite(metrics.DefaultRegistry, i, prefix, addr)\n\t}\n\treturn nil\n}\n\nfunc startPrometheusStats(l *logrus.Logger, i time.Duration, c *config.C, buildVersion string, configTest bool) (func(), error) {\n\tnamespace := c.GetString(\"stats.namespace\", \"\")\n\tsubsystem := c.GetString(\"stats.subsystem\", \"\")\n\n\tlisten := c.GetString(\"stats.listen\", \"\")\n\tif listen == \"\" {\n\t\treturn nil, fmt.Errorf(\"stats.listen should not be empty\")\n\t}\n\n\tpath := c.GetString(\"stats.path\", \"\")\n\tif path == \"\" {\n\t\treturn nil, fmt.Errorf(\"stats.path should not be empty\")\n\t}\n\n\tpr := prometheus.NewRegistry()\n\tpClient := mp.NewPrometheusProvider(metrics.DefaultRegistry, namespace, subsystem, pr, i)\n\tif !configTest {\n\t\tgo pClient.UpdatePrometheusMetrics()\n\t}\n\n\t// Export our version information as labels on a static gauge\n\tg := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tNamespace: namespace,\n\t\tSubsystem: subsystem,\n\t\tName:      \"info\",\n\t\tHelp:      \"Version information for the Nebula binary\",\n\t\tConstLabels: prometheus.Labels{\n\t\t\t\"version\":      buildVersion,\n\t\t\t\"goversion\":    runtime.Version(),\n\t\t\t\"boringcrypto\": strconv.FormatBool(boringEnabled()),\n\t\t},\n\t})\n\tpr.MustRegister(g)\n\tg.Set(1)\n\n\tvar startFn func()\n\tif !configTest {\n\t\tstartFn = func() {\n\t\t\tl.Infof(\"Prometheus stats listening on %s at %s\", listen, path)\n\t\t\thttp.Handle(path, promhttp.HandlerFor(pr, promhttp.HandlerOpts{ErrorLog: l}))\n\t\t\tlog.Fatal(http.ListenAndServe(listen, nil))\n\t\t}\n\t}\n\n\treturn startFn, nil\n}\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "timeout.go",
          "type": "blob",
          "size": 5.6025390625,
          "content": "package nebula\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\n// How many timer objects should be cached\nconst timerCacheMax = 50000\n\ntype TimerWheel[T any] struct {\n\t// Current tick\n\tcurrent int\n\n\t// Cheat on finding the length of the wheel\n\twheelLen int\n\n\t// Last time we ticked, since we are lazy ticking\n\tlastTick *time.Time\n\n\t// Durations of a tick and the entire wheel\n\ttickDuration  time.Duration\n\twheelDuration time.Duration\n\n\t// The actual wheel which is just a set of singly linked lists, head/tail pointers\n\twheel []*TimeoutList[T]\n\n\t// Singly linked list of items that have timed out of the wheel\n\texpired *TimeoutList[T]\n\n\t// Item cache to avoid garbage collect\n\titemCache   *TimeoutItem[T]\n\titemsCached int\n}\n\ntype LockingTimerWheel[T any] struct {\n\tm sync.Mutex\n\tt *TimerWheel[T]\n}\n\n// TimeoutList Represents a tick in the wheel\ntype TimeoutList[T any] struct {\n\tHead *TimeoutItem[T]\n\tTail *TimeoutItem[T]\n}\n\n// TimeoutItem Represents an item within a tick\ntype TimeoutItem[T any] struct {\n\tItem T\n\tNext *TimeoutItem[T]\n}\n\n// NewTimerWheel Builds a timer wheel and identifies the tick duration and wheel duration from the provided values\n// Purge must be called once per entry to actually remove anything\n// The TimerWheel does not handle concurrency on its own.\n// Locks around access to it must be used if multiple routines are manipulating it.\nfunc NewTimerWheel[T any](min, max time.Duration) *TimerWheel[T] {\n\t//TODO provide an error\n\t//if min >= max {\n\t//\treturn nil\n\t//}\n\n\t// Round down and add 2 so we can have the smallest # of ticks in the wheel and still account for a full\n\t// max duration, even if our current tick is at the maximum position and the next item to be added is at maximum\n\t// timeout\n\twLen := int((max / min) + 2)\n\n\ttw := TimerWheel[T]{\n\t\twheelLen:      wLen,\n\t\twheel:         make([]*TimeoutList[T], wLen),\n\t\ttickDuration:  min,\n\t\twheelDuration: max,\n\t\texpired:       &TimeoutList[T]{},\n\t}\n\n\tfor i := range tw.wheel {\n\t\ttw.wheel[i] = &TimeoutList[T]{}\n\t}\n\n\treturn &tw\n}\n\n// NewLockingTimerWheel is version of TimerWheel that is safe for concurrent use with a small performance penalty\nfunc NewLockingTimerWheel[T any](min, max time.Duration) *LockingTimerWheel[T] {\n\treturn &LockingTimerWheel[T]{\n\t\tt: NewTimerWheel[T](min, max),\n\t}\n}\n\n// Add will add an item to the wheel in its proper timeout.\n// Caller should Advance the wheel prior to ensure the proper slot is used.\nfunc (tw *TimerWheel[T]) Add(v T, timeout time.Duration) *TimeoutItem[T] {\n\ti := tw.findWheel(timeout)\n\n\t// Try to fetch off the cache\n\tti := tw.itemCache\n\tif ti != nil {\n\t\ttw.itemCache = ti.Next\n\t\ttw.itemsCached--\n\t\tti.Next = nil\n\t} else {\n\t\tti = &TimeoutItem[T]{}\n\t}\n\n\t// Relink and return\n\tti.Item = v\n\tif tw.wheel[i].Tail == nil {\n\t\ttw.wheel[i].Head = ti\n\t\ttw.wheel[i].Tail = ti\n\t} else {\n\t\ttw.wheel[i].Tail.Next = ti\n\t\ttw.wheel[i].Tail = ti\n\t}\n\n\treturn ti\n}\n\n// Purge removes and returns the first available expired item from the wheel and the 2nd argument is true.\n// If no item is available then an empty T is returned and the 2nd argument is false.\nfunc (tw *TimerWheel[T]) Purge() (T, bool) {\n\tif tw.expired.Head == nil {\n\t\tvar na T\n\t\treturn na, false\n\t}\n\n\tti := tw.expired.Head\n\ttw.expired.Head = ti.Next\n\n\tif tw.expired.Head == nil {\n\t\ttw.expired.Tail = nil\n\t}\n\n\t// Clear out the items references\n\tti.Next = nil\n\n\t// Maybe cache it for later\n\tif tw.itemsCached < timerCacheMax {\n\t\tti.Next = tw.itemCache\n\t\ttw.itemCache = ti\n\t\ttw.itemsCached++\n\t}\n\n\treturn ti.Item, true\n}\n\n// findWheel find the next position in the wheel for the provided timeout given the current tick\nfunc (tw *TimerWheel[T]) findWheel(timeout time.Duration) (i int) {\n\tif timeout < tw.tickDuration {\n\t\t// Can't track anything below the set resolution\n\t\ttimeout = tw.tickDuration\n\t} else if timeout > tw.wheelDuration {\n\t\t// We aren't handling timeouts greater than the wheels duration\n\t\ttimeout = tw.wheelDuration\n\t}\n\n\t// Find the next highest, rounding up\n\ttick := int(((timeout - 1) / tw.tickDuration) + 1)\n\n\t// Add another tick since the current tick may almost be over then map it to the wheel from our\n\t// current position\n\ttick += tw.current + 1\n\tif tick >= tw.wheelLen {\n\t\ttick -= tw.wheelLen\n\t}\n\n\treturn tick\n}\n\n// Advance will move the wheel forward by the appropriate number of ticks for the provided time and all items\n// passed over will be moved to the expired list. Calling Purge is necessary to remove them entirely.\nfunc (tw *TimerWheel[T]) Advance(now time.Time) {\n\tif tw.lastTick == nil {\n\t\ttw.lastTick = &now\n\t}\n\n\t// We want to round down\n\tticks := int(now.Sub(*tw.lastTick) / tw.tickDuration)\n\tadv := ticks\n\tif ticks > tw.wheelLen {\n\t\tticks = tw.wheelLen\n\t}\n\n\tfor i := 0; i < ticks; i++ {\n\t\ttw.current++\n\t\tif tw.current >= tw.wheelLen {\n\t\t\ttw.current = 0\n\t\t}\n\n\t\tif tw.wheel[tw.current].Head != nil {\n\t\t\t// We need to append the expired items as to not starve evicting the oldest ones\n\t\t\tif tw.expired.Tail == nil {\n\t\t\t\ttw.expired.Head = tw.wheel[tw.current].Head\n\t\t\t\ttw.expired.Tail = tw.wheel[tw.current].Tail\n\t\t\t} else {\n\t\t\t\ttw.expired.Tail.Next = tw.wheel[tw.current].Head\n\t\t\t\ttw.expired.Tail = tw.wheel[tw.current].Tail\n\t\t\t}\n\n\t\t\ttw.wheel[tw.current].Head = nil\n\t\t\ttw.wheel[tw.current].Tail = nil\n\t\t}\n\t}\n\n\t// Advance the tick based on duration to avoid losing some accuracy\n\tnewTick := tw.lastTick.Add(tw.tickDuration * time.Duration(adv))\n\ttw.lastTick = &newTick\n}\n\nfunc (lw *LockingTimerWheel[T]) Add(v T, timeout time.Duration) *TimeoutItem[T] {\n\tlw.m.Lock()\n\tdefer lw.m.Unlock()\n\treturn lw.t.Add(v, timeout)\n}\n\nfunc (lw *LockingTimerWheel[T]) Purge() (T, bool) {\n\tlw.m.Lock()\n\tdefer lw.m.Unlock()\n\treturn lw.t.Purge()\n}\n\nfunc (lw *LockingTimerWheel[T]) Advance(now time.Time) {\n\tlw.m.Lock()\n\tdefer lw.m.Unlock()\n\tlw.t.Advance(now)\n}\n"
        },
        {
          "name": "timeout_test.go",
          "type": "blob",
          "size": 4.7392578125,
          "content": "package nebula\n\nimport (\n\t\"net/netip\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/slackhq/nebula/firewall\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNewTimerWheel(t *testing.T) {\n\t// Make sure we get an object we expect\n\ttw := NewTimerWheel[firewall.Packet](time.Second, time.Second*10)\n\tassert.Equal(t, 12, tw.wheelLen)\n\tassert.Equal(t, 0, tw.current)\n\tassert.Nil(t, tw.lastTick)\n\tassert.Equal(t, time.Second*1, tw.tickDuration)\n\tassert.Equal(t, time.Second*10, tw.wheelDuration)\n\tassert.Len(t, tw.wheel, 12)\n\n\t// Assert the math is correct\n\ttw = NewTimerWheel[firewall.Packet](time.Second*3, time.Second*10)\n\tassert.Equal(t, 5, tw.wheelLen)\n\n\ttw = NewTimerWheel[firewall.Packet](time.Second*120, time.Minute*10)\n\tassert.Equal(t, 7, tw.wheelLen)\n\n\t// Test empty purge of non nil items\n\ti, ok := tw.Purge()\n\tassert.Equal(t, firewall.Packet{}, i)\n\tassert.False(t, ok)\n\n\t// Test empty purges of nil items\n\ttw2 := NewTimerWheel[*int](time.Second, time.Second*10)\n\ti2, ok := tw2.Purge()\n\tassert.Nil(t, i2)\n\tassert.False(t, ok)\n\n}\n\nfunc TestTimerWheel_findWheel(t *testing.T) {\n\ttw := NewTimerWheel[firewall.Packet](time.Second, time.Second*10)\n\tassert.Len(t, tw.wheel, 12)\n\n\t// Current + tick + 1 since we don't know how far into current we are\n\tassert.Equal(t, 2, tw.findWheel(time.Second*1))\n\n\t// Scale up to min duration\n\tassert.Equal(t, 2, tw.findWheel(time.Millisecond*1))\n\n\t// Make sure we hit that last index\n\tassert.Equal(t, 11, tw.findWheel(time.Second*10))\n\n\t// Scale down to max duration\n\tassert.Equal(t, 11, tw.findWheel(time.Second*11))\n\n\ttw.current = 1\n\t// Make sure we account for the current position properly\n\tassert.Equal(t, 3, tw.findWheel(time.Second*1))\n\tassert.Equal(t, 0, tw.findWheel(time.Second*10))\n}\n\nfunc TestTimerWheel_Add(t *testing.T) {\n\ttw := NewTimerWheel[firewall.Packet](time.Second, time.Second*10)\n\n\tfp1 := firewall.Packet{}\n\ttw.Add(fp1, time.Second*1)\n\n\t// Make sure we set head and tail properly\n\tassert.NotNil(t, tw.wheel[2])\n\tassert.Equal(t, fp1, tw.wheel[2].Head.Item)\n\tassert.Nil(t, tw.wheel[2].Head.Next)\n\tassert.Equal(t, fp1, tw.wheel[2].Tail.Item)\n\tassert.Nil(t, tw.wheel[2].Tail.Next)\n\n\t// Make sure we only modify head\n\tfp2 := firewall.Packet{}\n\ttw.Add(fp2, time.Second*1)\n\tassert.Equal(t, fp2, tw.wheel[2].Head.Item)\n\tassert.Equal(t, fp1, tw.wheel[2].Head.Next.Item)\n\tassert.Equal(t, fp1, tw.wheel[2].Tail.Item)\n\tassert.Nil(t, tw.wheel[2].Tail.Next)\n\n\t// Make sure we use free'd items first\n\ttw.itemCache = &TimeoutItem[firewall.Packet]{}\n\ttw.itemsCached = 1\n\ttw.Add(fp2, time.Second*1)\n\tassert.Nil(t, tw.itemCache)\n\tassert.Equal(t, 0, tw.itemsCached)\n\n\t// Ensure that all configurations of a wheel does not result in calculating an overflow of the wheel\n\tfor min := time.Duration(1); min < 100; min++ {\n\t\tfor max := min; max < 100; max++ {\n\t\t\ttw = NewTimerWheel[firewall.Packet](min, max)\n\n\t\t\tfor current := 0; current < tw.wheelLen; current++ {\n\t\t\t\ttw.current = current\n\t\t\t\tfor timeout := time.Duration(0); timeout <= tw.wheelDuration; timeout++ {\n\t\t\t\t\ttick := tw.findWheel(timeout)\n\t\t\t\t\tif tick >= tw.wheelLen {\n\t\t\t\t\t\tt.Errorf(\"Min: %v; Max: %v; Wheel len: %v; Current Tick: %v; Insert timeout: %v; Calc tick: %v\", min, max, tw.wheelLen, current, timeout, tick)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestTimerWheel_Purge(t *testing.T) {\n\t// First advance should set the lastTick and do nothing else\n\ttw := NewTimerWheel[firewall.Packet](time.Second, time.Second*10)\n\tassert.Nil(t, tw.lastTick)\n\ttw.Advance(time.Now())\n\tassert.NotNil(t, tw.lastTick)\n\tassert.Equal(t, 0, tw.current)\n\n\tfps := []firewall.Packet{\n\t\t{LocalIP: netip.MustParseAddr(\"0.0.0.1\")},\n\t\t{LocalIP: netip.MustParseAddr(\"0.0.0.2\")},\n\t\t{LocalIP: netip.MustParseAddr(\"0.0.0.3\")},\n\t\t{LocalIP: netip.MustParseAddr(\"0.0.0.4\")},\n\t}\n\n\ttw.Add(fps[0], time.Second*1)\n\ttw.Add(fps[1], time.Second*1)\n\ttw.Add(fps[2], time.Second*2)\n\ttw.Add(fps[3], time.Second*2)\n\n\tta := time.Now().Add(time.Second * 3)\n\tlastTick := *tw.lastTick\n\ttw.Advance(ta)\n\tassert.Equal(t, 3, tw.current)\n\tassert.True(t, tw.lastTick.After(lastTick))\n\n\t// Make sure we get all 4 packets back\n\tfor i := 0; i < 4; i++ {\n\t\tp, has := tw.Purge()\n\t\tassert.True(t, has)\n\t\tassert.Equal(t, fps[i], p)\n\t}\n\n\t// Make sure there aren't any leftover\n\t_, ok := tw.Purge()\n\tassert.False(t, ok)\n\tassert.Nil(t, tw.expired.Head)\n\tassert.Nil(t, tw.expired.Tail)\n\n\t// Make sure we cached the free'd items\n\tassert.Equal(t, 4, tw.itemsCached)\n\tci := tw.itemCache\n\tfor i := 0; i < 4; i++ {\n\t\tassert.NotNil(t, ci)\n\t\tci = ci.Next\n\t}\n\tassert.Nil(t, ci)\n\n\t// Let's make sure we roll over properly\n\tta = ta.Add(time.Second * 5)\n\ttw.Advance(ta)\n\tassert.Equal(t, 8, tw.current)\n\n\tta = ta.Add(time.Second * 2)\n\ttw.Advance(ta)\n\tassert.Equal(t, 10, tw.current)\n\n\tta = ta.Add(time.Second * 1)\n\ttw.Advance(ta)\n\tassert.Equal(t, 11, tw.current)\n\n\tta = ta.Add(time.Second * 1)\n\ttw.Advance(ta)\n\tassert.Equal(t, 0, tw.current)\n}\n"
        },
        {
          "name": "udp",
          "type": "tree",
          "content": null
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        },
        {
          "name": "wintun",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}