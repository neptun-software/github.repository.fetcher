{
  "metadata": {
    "timestamp": 1736567860385,
    "page": 16,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "gocolly/colly",
      "stars": 23568,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.0146484375,
          "content": "comment: false\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 1.2314453125,
          "content": "# 2.1.0 - 2020.06.09\n\n - HTTP tracing support\n - New callback: OnResponseHeader\n - Queue fixes\n - New collector option: Collector.CheckHead\n - Proxy fixes\n - Fixed POST revisit checking\n - Updated dependencies\n\n# 2.0.0 - 2019.11.28\n\n - Breaking change: Change Collector.RedirectHandler member to Collector.SetRedirectHandler function\n - Go module support\n - Collector.HasVisited method added to be able to check if an url has been visited\n - Collector.SetClient method introduced\n - HTMLElement.ChildTexts method added\n - New user agents\n - Multiple bugfixes\n\n# 1.2.0 - 2019.02.13\n\n - Compatibility with the latest htmlquery package\n - New request shortcut for HEAD requests\n - Check URL availibility before visiting\n - Fix proxy URL value\n - Request counter fix\n - Minor fixes in examples\n\n# 1.1.0 - 2018.08.13\n\n - Appengine integration takes context.Context instead of http.Request (API change)\n - Added \"Accept\" http header by default to every request\n - Support slices of pointers in unmarshal\n - Fixed a race condition in queues\n - ForEachWithBreak method added to HTMLElement\n - Added a local file example\n - Support gzip decompression of response bodies\n - Don't share waitgroup when cloning a collector\n - Fixed instagram example\n\n\n# 1.0.0 - 2018.05.13\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.390625,
          "content": "# Contribute\n\n## Introduction\n\nFirst, thank you for considering contributing to colly! It's people like you that make the open source community such a great community! üòä\n\nWe welcome any type of contribution, not only code. You can help with \n- **QA**: file bug reports, the more details you can give the better (e.g. screenshots with the console open)\n- **Marketing**: writing blog posts, howto's, printing stickers, ...\n- **Community**: presenting the project at meetups, organizing a dedicated meetup for the local community, ...\n- **Code**: take a look at the [open issues](https://github.com/gocolly/colly/issues). Even if you can't write code, commenting on them, showing that you care about a given issue matters. It helps us triage them.\n- **Money**: we welcome financial contributions in full transparency on our [open collective](https://opencollective.com/colly).\n\n## Your First Contribution\n\nWorking on your first Pull Request? You can learn how from this *free* series, [How to Contribute to an Open Source Project on GitHub](https://app.egghead.io/playlists/how-to-contribute-to-an-open-source-project-on-github).\n\n## Submitting code\n\nAny code change should be submitted as a pull request. The description should explain what the code does and give steps to execute it. The pull request should also contain tests.\n\n## Code review process\n\nThe bigger the pull request, the longer it will take to review and merge. Try to break down large pull requests in smaller chunks that are easier to review and merge.\nIt is also always helpful to have some context for your pull request. What was the purpose? Why does it matter to you?\n\n## Financial contributions\n\nWe also welcome financial contributions in full transparency on our [open collective](https://opencollective.com/colly).\nAnyone can file an expense. If the expense makes sense for the development of the community, it will be \"merged\" in the ledger of our open collective by the core contributors and the person who filed the expense will be reimbursed.\n\n## Questions\n\nIf you have any questions, create an [issue](https://github.com/gocolly/colly/issues/new) (protip: do a quick search first to see if someone else didn't ask the same question before!).\nYou can also reach us at hello@colly.opencollective.com.\n\n## Credits\n\n### Contributors\n\nThank you to all the people who have already contributed to colly!\n<a href=\"graphs/contributors\"><img src=\"https://opencollective.com/colly/contributors.svg?width=890\" /></a>\n\n\n### Backers\n\nThank you to all our backers! [[Become a backer](https://opencollective.com/colly#backer)]\n\n<a href=\"https://opencollective.com/colly#backers\" target=\"_blank\"><img src=\"https://opencollective.com/colly/backers.svg?width=890\"></a>\n\n\n### Sponsors\n\nThank you to all our sponsors! (please ask your company to also support this open source project by [becoming a sponsor](https://opencollective.com/colly#sponsor))\n\n<a href=\"https://opencollective.com/colly/sponsor/0/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/1/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/2/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/3/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/4/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/5/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/6/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/7/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/8/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/9/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/9/avatar.svg\"></a>\n\n<!-- This `CONTRIBUTING.md` is based on @nayafia's template https://github.com/nayafia/contributing-template -->\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.451171875,
          "content": "# Colly\n\nLightning Fast and Elegant Scraping Framework for Gophers\n\nColly provides a clean interface to write any kind of crawler/scraper/spider.\n\nWith Colly you can easily extract structured data from websites, which can be used for a wide range of applications, like data mining, data processing or archiving.\n\n[![GoDoc](https://godoc.org/github.com/gocolly/colly?status.svg)](https://pkg.go.dev/github.com/gocolly/colly/v2)\n[![Backers on Open Collective](https://opencollective.com/colly/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/colly/sponsors/badge.svg)](#sponsors) [![build status](https://github.com/gocolly/colly/actions/workflows/ci.yml/badge.svg)](https://github.com/gocolly/colly/actions/workflows/ci.yml)\n[![report card](https://img.shields.io/badge/report%20card-a%2B-ff3333.svg?style=flat-square)](http://goreportcard.com/report/gocolly/colly)\n[![view examples](https://img.shields.io/badge/learn%20by-examples-0077b3.svg?style=flat-square)](https://github.com/gocolly/colly/tree/master/_examples)\n[![Code Coverage](https://img.shields.io/codecov/c/github/gocolly/colly/master.svg)](https://codecov.io/github/gocolly/colly?branch=master)\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fgocolly%2Fcolly.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2Fgocolly%2Fcolly?ref=badge_shield)\n[![Twitter URL](https://img.shields.io/badge/twitter-follow-green.svg)](https://twitter.com/gocolly)\n\n\n------\n\n\n## Sponsors\n\n\n<a href=\"https://scrapfly.io/?utm_source=Github&utm_medium=repo&utm_campaign=colly\" target=\"_blank\"><img src=\"assets/scrapfly.png\" alt=\"Scrapfly.io\" width=\"149\"></a>\n\n[Scrapfly](https://scrapfly.io/?utm_source=Github&utm_medium=repo&utm_campaign=colly)\nis an enterprise-grade solution providing Web Scraping API that aims to simplify the\nscraping process by managing everything: real browser rendering, rotating proxies, and\nfingerprints (TLS, HTTP, browser) to bypass all major anti-bots. Scrapfly also unlocks the\nobservability by providing an analytical dashboard and measuring the success rate/block\nrate in detail.\n\n\n------\n\n\n\n## Features\n\n-   Clean API\n-   Fast (>1k request/sec on a single core)\n-   Manages request delays and maximum concurrency per domain\n-   Automatic cookie and session handling\n-   Sync/async/parallel scraping\n-   Caching\n-   Automatic encoding of non-unicode responses\n-   Robots.txt support\n-   Distributed scraping\n-   Configuration via environment variables\n-   Extensions\n\n## Example\n\n```go\nfunc main() {\n\tc := colly.NewCollector()\n\n\t// Find and visit all links\n\tc.OnHTML(\"a[href]\", func(e *colly.HTMLElement) {\n\t\te.Request.Visit(e.Attr(\"href\"))\n\t})\n\n\tc.OnRequest(func(r *colly.Request) {\n\t\tfmt.Println(\"Visiting\", r.URL)\n\t})\n\n\tc.Visit(\"http://go-colly.org/\")\n}\n```\n\nSee [examples folder](https://github.com/gocolly/colly/tree/master/_examples) for more detailed examples.\n\n## Installation\n\nAdd colly to your `go.mod` file:\n\n```\nmodule github.com/x/y\n\ngo 1.14\n\nrequire (\n        github.com/gocolly/colly/v2 latest\n)\n```\n\n## Bugs\n\nBugs or suggestions? Visit the [issue tracker](https://github.com/gocolly/colly/issues) or join `#colly` on freenode\n\n## Other Projects Using Colly\n\nBelow is a list of public, open source projects that use Colly:\n\n-   [greenpeace/check-my-pages](https://github.com/greenpeace/check-my-pages) Scraping script to test the Spanish Greenpeace web archive.\n-   [altsab/gowap](https://github.com/altsab/gowap) Wappalyzer implementation in Go.\n-   [jesuiscamille/goquotes](https://github.com/jesuiscamille/goquotes) A quotes scraper, making your day a little better!\n-   [jivesearch/jivesearch](https://github.com/jivesearch/jivesearch) A search engine that doesn't track you.\n-   [Leagify/colly-draft-prospects](https://github.com/Leagify/colly-draft-prospects) A scraper for future NFL Draft prospects.\n-   [lucasepe/go-ps4](https://github.com/lucasepe/go-ps4) Search playstation store for your favorite PS4 games using the command line.\n-   [yringler/inside-chassidus-scraper](https://github.com/yringler/inside-chassidus-scraper) Scrapes Rabbi Paltiel's web site for lesson metadata.\n-   [gamedb/gamedb](https://github.com/gamedb/gamedb) A database of Steam games.\n-   [lawzava/scrape](https://github.com/lawzava/scrape) CLI for email scraping from any website.\n-   [eureka101v/WeiboSpiderGo](https://github.com/eureka101v/WeiboSpiderGo) A sina weibo(chinese twitter) scraper\n-   [Go-phie/gophie](https://github.com/Go-phie/gophie) Search, Download and Stream movies from your terminal\n-   [imthaghost/goclone](https://github.com/imthaghost/goclone) Clone websites to your computer within seconds.\n-   [superiss/spidy](https://github.com/superiss/spidy) Crawl the web and collect expired domains.\n-   [docker-slim/docker-slim](https://github.com/docker-slim/docker-slim) Optimize your Docker containers to make them smaller and better.\n-   [seversky/gachifinder](https://github.com/seversky/gachifinder) an agent for asynchronous scraping, parsing and writing to some storages(elasticsearch for now)\n-   [eval-exec/goodreads](https://github.com/eval-exec/goodreads) crawl all tags and all pages of quotes from goodreads.\n\nIf you are using Colly in a project please send a pull request to add it to the list.\n\n## Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute]](CONTRIBUTING.md).\n<a href=\"https://github.com/gocolly/colly/graphs/contributors\"><img src=\"https://opencollective.com/colly/contributors.svg?width=890\" /></a>\n\n## Backers\n\nThank you to all our backers! üôè [[Become a backer](https://opencollective.com/colly#backer)]\n\n<a href=\"https://opencollective.com/colly#backers\" target=\"_blank\"><img src=\"https://opencollective.com/colly/backers.svg?width=890\"></a>\n\n## Sponsors\n\nSupport this project by becoming a sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/colly#sponsor)]\n\n<a href=\"https://opencollective.com/colly/sponsor/0/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/1/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/2/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/3/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/4/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/5/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/6/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/7/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/8/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/colly/sponsor/9/website\" target=\"_blank\"><img src=\"https://opencollective.com/colly/sponsor/9/avatar.svg\"></a>\n\n## License\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fgocolly%2Fcolly.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fgocolly%2Fcolly?ref=badge_large)\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.005859375,
          "content": "2.1.0\n"
        },
        {
          "name": "_examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "colly.go",
          "type": "blob",
          "size": 44.27734375,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package colly implements a HTTP scraping framework\npackage colly\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/rand\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/fnv\"\n\t\"io\"\n\t\"log\"\n\t\"net/http\"\n\t\"net/http/cookiejar\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/PuerkitoBio/goquery\"\n\t\"github.com/antchfx/htmlquery\"\n\t\"github.com/antchfx/xmlquery\"\n\t\"github.com/gocolly/colly/v2/debug\"\n\t\"github.com/gocolly/colly/v2/storage\"\n\t\"github.com/kennygrant/sanitize\"\n\twhatwgUrl \"github.com/nlnwa/whatwg-url/url\"\n\t\"github.com/temoto/robotstxt\"\n\t\"google.golang.org/appengine/urlfetch\"\n)\n\n// A CollectorOption sets an option on a Collector.\ntype CollectorOption func(*Collector)\n\n// Collector provides the scraper instance for a scraping job\ntype Collector struct {\n\t// UserAgent is the User-Agent string used by HTTP requests\n\tUserAgent string\n\t// Custom headers for the request\n\tHeaders *http.Header\n\t// MaxDepth limits the recursion depth of visited URLs.\n\t// Set it to 0 for infinite recursion (default).\n\tMaxDepth int\n\t// AllowedDomains is a domain whitelist.\n\t// Leave it blank to allow any domains to be visited\n\tAllowedDomains []string\n\t// DisallowedDomains is a domain blacklist.\n\tDisallowedDomains []string\n\t// DisallowedURLFilters is a list of regular expressions which restricts\n\t// visiting URLs. If any of the rules matches to a URL the\n\t// request will be stopped. DisallowedURLFilters will\n\t// be evaluated before URLFilters\n\t// Leave it blank to allow any URLs to be visited\n\tDisallowedURLFilters []*regexp.Regexp\n\t// URLFilters is a list of regular expressions which restricts\n\t// visiting URLs. If any of the rules matches to a URL the\n\t// request won't be stopped. DisallowedURLFilters will\n\t// be evaluated before URLFilters\n\n\t// Leave it blank to allow any URLs to be visited\n\tURLFilters []*regexp.Regexp\n\n\t// AllowURLRevisit allows multiple downloads of the same URL\n\tAllowURLRevisit bool\n\t// MaxBodySize is the limit of the retrieved response body in bytes.\n\t// 0 means unlimited.\n\t// The default value for MaxBodySize is 10MB (10 * 1024 * 1024 bytes).\n\tMaxBodySize int\n\t// CacheDir specifies a location where GET requests are cached as files.\n\t// When it's not defined, caching is disabled.\n\tCacheDir string\n\t// IgnoreRobotsTxt allows the Collector to ignore any restrictions set by\n\t// the target host's robots.txt file.  See http://www.robotstxt.org/ for more\n\t// information.\n\tIgnoreRobotsTxt bool\n\t// Async turns on asynchronous network communication. Use Collector.Wait() to\n\t// be sure all requests have been finished.\n\tAsync bool\n\t// ParseHTTPErrorResponse allows parsing HTTP responses with non 2xx status codes.\n\t// By default, Colly parses only successful HTTP responses. Set ParseHTTPErrorResponse\n\t// to true to enable it.\n\tParseHTTPErrorResponse bool\n\t// ID is the unique identifier of a collector\n\tID uint32\n\t// DetectCharset can enable character encoding detection for non-utf8 response bodies\n\t// without explicit charset declaration. This feature uses https://github.com/saintfish/chardet\n\tDetectCharset bool\n\t// RedirectHandler allows control on how a redirect will be managed\n\t// use c.SetRedirectHandler to set this value\n\tredirectHandler func(req *http.Request, via []*http.Request) error\n\t// CheckHead performs a HEAD request before every GET to pre-validate the response\n\tCheckHead bool\n\t// TraceHTTP enables capturing and reporting request performance for crawler tuning.\n\t// When set to true, the Response.Trace will be filled in with an HTTPTrace object.\n\tTraceHTTP bool\n\t// Context is the context that will be used for HTTP requests. You can set this\n\t// to support clean cancellation of scraping.\n\tContext context.Context\n\t// MaxRequests limit the number of requests done by the instance.\n\t// Set it to 0 for infinite requests (default).\n\tMaxRequests uint32\n\n\tstore                    storage.Storage\n\tdebugger                 debug.Debugger\n\trobotsMap                map[string]*robotstxt.RobotsData\n\thtmlCallbacks            []*htmlCallbackContainer\n\txmlCallbacks             []*xmlCallbackContainer\n\trequestCallbacks         []RequestCallback\n\tresponseCallbacks        []ResponseCallback\n\tresponseHeadersCallbacks []ResponseHeadersCallback\n\terrorCallbacks           []ErrorCallback\n\tscrapedCallbacks         []ScrapedCallback\n\trequestCount             uint32\n\tresponseCount            uint32\n\tbackend                  *httpBackend\n\twg                       *sync.WaitGroup\n\tlock                     *sync.RWMutex\n}\n\n// RequestCallback is a type alias for OnRequest callback functions\ntype RequestCallback func(*Request)\n\n// ResponseHeadersCallback is a type alias for OnResponseHeaders callback functions\ntype ResponseHeadersCallback func(*Response)\n\n// ResponseCallback is a type alias for OnResponse callback functions\ntype ResponseCallback func(*Response)\n\n// HTMLCallback is a type alias for OnHTML callback functions\ntype HTMLCallback func(*HTMLElement)\n\n// XMLCallback is a type alias for OnXML callback functions\ntype XMLCallback func(*XMLElement)\n\n// ErrorCallback is a type alias for OnError callback functions\ntype ErrorCallback func(*Response, error)\n\n// ScrapedCallback is a type alias for OnScraped callback functions\ntype ScrapedCallback func(*Response)\n\n// ProxyFunc is a type alias for proxy setter functions.\ntype ProxyFunc func(*http.Request) (*url.URL, error)\n\n// AlreadyVisitedError is the error type for already visited URLs.\n//\n// It's returned synchronously by Visit when the URL passed to Visit\n// is already visited.\n//\n// When already visited URL is encountered after following\n// redirects, this error appears in OnError callback, and if Async\n// mode is not enabled, is also returned by Visit.\ntype AlreadyVisitedError struct {\n\t// Destination is the URL that was attempted to be visited.\n\t// It might not match the URL passed to Visit if redirect\n\t// was followed.\n\tDestination *url.URL\n}\n\n// Error implements error interface.\nfunc (e *AlreadyVisitedError) Error() string {\n\treturn fmt.Sprintf(\"%q already visited\", e.Destination)\n}\n\ntype htmlCallbackContainer struct {\n\tSelector string\n\tFunction HTMLCallback\n}\n\ntype xmlCallbackContainer struct {\n\tQuery    string\n\tFunction XMLCallback\n}\n\ntype cookieJarSerializer struct {\n\tstore storage.Storage\n\tlock  *sync.RWMutex\n}\n\nvar collectorCounter uint32\n\n// The key type is unexported to prevent collisions with context keys defined in\n// other packages.\ntype key int\n\n// ProxyURLKey is the context key for the request proxy address.\nconst ProxyURLKey key = iota\n\nvar (\n\t// ErrForbiddenDomain is the error thrown if visiting\n\t// a domain which is not allowed in AllowedDomains\n\tErrForbiddenDomain = errors.New(\"Forbidden domain\")\n\t// ErrMissingURL is the error type for missing URL errors\n\tErrMissingURL = errors.New(\"Missing URL\")\n\t// ErrMaxDepth is the error type for exceeding max depth\n\tErrMaxDepth = errors.New(\"Max depth limit reached\")\n\t// ErrForbiddenURL is the error thrown if visiting\n\t// a URL which is not allowed by URLFilters\n\tErrForbiddenURL = errors.New(\"ForbiddenURL\")\n\n\t// ErrNoURLFiltersMatch is the error thrown if visiting\n\t// a URL which is not allowed by URLFilters\n\tErrNoURLFiltersMatch = errors.New(\"No URLFilters match\")\n\t// ErrRobotsTxtBlocked is the error type for robots.txt errors\n\tErrRobotsTxtBlocked = errors.New(\"URL blocked by robots.txt\")\n\t// ErrNoCookieJar is the error type for missing cookie jar\n\tErrNoCookieJar = errors.New(\"Cookie jar is not available\")\n\t// ErrNoPattern is the error type for LimitRules without patterns\n\tErrNoPattern = errors.New(\"No pattern defined in LimitRule\")\n\t// ErrEmptyProxyURL is the error type for empty Proxy URL list\n\tErrEmptyProxyURL = errors.New(\"Proxy URL list is empty\")\n\t// ErrAbortedAfterHeaders is the error returned when OnResponseHeaders aborts the transfer.\n\tErrAbortedAfterHeaders = errors.New(\"Aborted after receiving response headers\")\n\t// ErrQueueFull is the error returned when the queue is full\n\tErrQueueFull = errors.New(\"Queue MaxSize reached\")\n\t// ErrMaxRequests is the error returned when exceeding max requests\n\tErrMaxRequests = errors.New(\"Max Requests limit reached\")\n\t// ErrRetryBodyUnseekable is the error when retry with not seekable body\n\tErrRetryBodyUnseekable = errors.New(\"Retry Body Unseekable\")\n)\n\nvar envMap = map[string]func(*Collector, string){\n\t\"ALLOWED_DOMAINS\": func(c *Collector, val string) {\n\t\tc.AllowedDomains = strings.Split(val, \",\")\n\t},\n\t\"CACHE_DIR\": func(c *Collector, val string) {\n\t\tc.CacheDir = val\n\t},\n\t\"DETECT_CHARSET\": func(c *Collector, val string) {\n\t\tc.DetectCharset = isYesString(val)\n\t},\n\t\"DISABLE_COOKIES\": func(c *Collector, _ string) {\n\t\tc.backend.Client.Jar = nil\n\t},\n\t\"DISALLOWED_DOMAINS\": func(c *Collector, val string) {\n\t\tc.DisallowedDomains = strings.Split(val, \",\")\n\t},\n\t\"IGNORE_ROBOTSTXT\": func(c *Collector, val string) {\n\t\tc.IgnoreRobotsTxt = isYesString(val)\n\t},\n\t\"FOLLOW_REDIRECTS\": func(c *Collector, val string) {\n\t\tif !isYesString(val) {\n\t\t\tc.redirectHandler = func(req *http.Request, via []*http.Request) error {\n\t\t\t\treturn http.ErrUseLastResponse\n\t\t\t}\n\t\t}\n\t},\n\t\"MAX_BODY_SIZE\": func(c *Collector, val string) {\n\t\tsize, err := strconv.Atoi(val)\n\t\tif err == nil {\n\t\t\tc.MaxBodySize = size\n\t\t}\n\t},\n\t\"MAX_DEPTH\": func(c *Collector, val string) {\n\t\tmaxDepth, err := strconv.Atoi(val)\n\t\tif err == nil {\n\t\t\tc.MaxDepth = maxDepth\n\t\t}\n\t},\n\t\"MAX_REQUESTS\": func(c *Collector, val string) {\n\t\tmaxRequests, err := strconv.ParseUint(val, 0, 32)\n\t\tif err == nil {\n\t\t\tc.MaxRequests = uint32(maxRequests)\n\t\t}\n\t},\n\t\"PARSE_HTTP_ERROR_RESPONSE\": func(c *Collector, val string) {\n\t\tc.ParseHTTPErrorResponse = isYesString(val)\n\t},\n\t\"TRACE_HTTP\": func(c *Collector, val string) {\n\t\tc.TraceHTTP = isYesString(val)\n\t},\n\t\"USER_AGENT\": func(c *Collector, val string) {\n\t\tc.UserAgent = val\n\t},\n}\n\nvar urlParser = whatwgUrl.NewParser(whatwgUrl.WithPercentEncodeSinglePercentSign())\n\n// NewCollector creates a new Collector instance with default configuration\nfunc NewCollector(options ...CollectorOption) *Collector {\n\tc := &Collector{}\n\tc.Init()\n\n\tfor _, f := range options {\n\t\tf(c)\n\t}\n\n\tc.parseSettingsFromEnv()\n\n\treturn c\n}\n\n// UserAgent sets the user agent used by the Collector.\nfunc UserAgent(ua string) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.UserAgent = ua\n\t}\n}\n\n// Headers sets the custom headers used by the Collector.\nfunc Headers(headers map[string]string) CollectorOption {\n\treturn func(c *Collector) {\n\t\tcustomHeaders := make(http.Header)\n\t\tfor header, value := range headers {\n\t\t\tcustomHeaders.Add(header, value)\n\t\t}\n\t\tc.Headers = &customHeaders\n\t}\n}\n\n// MaxDepth limits the recursion depth of visited URLs.\nfunc MaxDepth(depth int) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.MaxDepth = depth\n\t}\n}\n\n// MaxRequests limit the number of requests done by the instance.\n// Set it to 0 for infinite requests (default).\nfunc MaxRequests(max uint32) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.MaxRequests = max\n\t}\n}\n\n// AllowedDomains sets the domain whitelist used by the Collector.\nfunc AllowedDomains(domains ...string) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.AllowedDomains = domains\n\t}\n}\n\n// ParseHTTPErrorResponse allows parsing responses with HTTP errors\nfunc ParseHTTPErrorResponse() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.ParseHTTPErrorResponse = true\n\t}\n}\n\n// DisallowedDomains sets the domain blacklist used by the Collector.\nfunc DisallowedDomains(domains ...string) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.DisallowedDomains = domains\n\t}\n}\n\n// DisallowedURLFilters sets the list of regular expressions which restricts\n// visiting URLs. If any of the rules matches to a URL the request will be stopped.\nfunc DisallowedURLFilters(filters ...*regexp.Regexp) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.DisallowedURLFilters = filters\n\t}\n}\n\n// URLFilters sets the list of regular expressions which restricts\n// visiting URLs. If any of the rules matches to a URL the request won't be stopped.\nfunc URLFilters(filters ...*regexp.Regexp) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.URLFilters = filters\n\t}\n}\n\n// AllowURLRevisit instructs the Collector to allow multiple downloads of the same URL\nfunc AllowURLRevisit() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.AllowURLRevisit = true\n\t}\n}\n\n// MaxBodySize sets the limit of the retrieved response body in bytes.\nfunc MaxBodySize(sizeInBytes int) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.MaxBodySize = sizeInBytes\n\t}\n}\n\n// CacheDir specifies the location where GET requests are cached as files.\nfunc CacheDir(path string) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.CacheDir = path\n\t}\n}\n\n// IgnoreRobotsTxt instructs the Collector to ignore any restrictions\n// set by the target host's robots.txt file.\nfunc IgnoreRobotsTxt() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.IgnoreRobotsTxt = true\n\t}\n}\n\n// TraceHTTP instructs the Collector to collect and report request trace data\n// on the Response.Trace.\nfunc TraceHTTP() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.TraceHTTP = true\n\t}\n}\n\n// StdlibContext sets the context that will be used for HTTP requests.\n// You can set this to support clean cancellation of scraping.\nfunc StdlibContext(ctx context.Context) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.Context = ctx\n\t}\n}\n\n// ID sets the unique identifier of the Collector.\nfunc ID(id uint32) CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.ID = id\n\t}\n}\n\n// Async turns on asynchronous network requests.\nfunc Async(a ...bool) CollectorOption {\n\treturn func(c *Collector) {\n\t\tif len(a) > 0 {\n\t\t\tc.Async = a[0]\n\t\t} else {\n\t\t\tc.Async = true\n\t\t}\n\t}\n}\n\n// DetectCharset enables character encoding detection for non-utf8 response bodies\n// without explicit charset declaration. This feature uses https://github.com/saintfish/chardet\nfunc DetectCharset() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.DetectCharset = true\n\t}\n}\n\n// Debugger sets the debugger used by the Collector.\nfunc Debugger(d debug.Debugger) CollectorOption {\n\treturn func(c *Collector) {\n\t\td.Init()\n\t\tc.debugger = d\n\t}\n}\n\n// CheckHead performs a HEAD request before every GET to pre-validate the response\nfunc CheckHead() CollectorOption {\n\treturn func(c *Collector) {\n\t\tc.CheckHead = true\n\t}\n}\n\n// Init initializes the Collector's private variables and sets default\n// configuration for the Collector\nfunc (c *Collector) Init() {\n\tc.UserAgent = \"colly - https://github.com/gocolly/colly/v2\"\n\tc.Headers = nil\n\tc.MaxDepth = 0\n\tc.MaxRequests = 0\n\tc.store = &storage.InMemoryStorage{}\n\tc.store.Init()\n\tc.MaxBodySize = 10 * 1024 * 1024\n\tc.backend = &httpBackend{}\n\tjar, _ := cookiejar.New(nil)\n\tc.backend.Init(jar)\n\tc.backend.Client.CheckRedirect = c.checkRedirectFunc()\n\tc.wg = &sync.WaitGroup{}\n\tc.lock = &sync.RWMutex{}\n\tc.robotsMap = make(map[string]*robotstxt.RobotsData)\n\tc.IgnoreRobotsTxt = true\n\tc.ID = atomic.AddUint32(&collectorCounter, 1)\n\tc.TraceHTTP = false\n\tc.Context = context.Background()\n}\n\n// Appengine will replace the Collector's backend http.Client\n// With an Http.Client that is provided by appengine/urlfetch\n// This function should be used when the scraper is run on\n// Google App Engine. Example:\n//\n//\tfunc startScraper(w http.ResponseWriter, r *http.Request) {\n//\t  ctx := appengine.NewContext(r)\n//\t  c := colly.NewCollector()\n//\t  c.Appengine(ctx)\n//\t   ...\n//\t  c.Visit(\"https://google.ca\")\n//\t}\nfunc (c *Collector) Appengine(ctx context.Context) {\n\tclient := urlfetch.Client(ctx)\n\tclient.Jar = c.backend.Client.Jar\n\tclient.CheckRedirect = c.backend.Client.CheckRedirect\n\tclient.Timeout = c.backend.Client.Timeout\n\n\tc.backend.Client = client\n}\n\n// Visit starts Collector's collecting job by creating a\n// request to the URL specified in parameter.\n// Visit also calls the previously provided callbacks\nfunc (c *Collector) Visit(URL string) error {\n\tif c.CheckHead {\n\t\tif check := c.scrape(URL, \"HEAD\", 1, nil, nil, nil, true); check != nil {\n\t\t\treturn check\n\t\t}\n\t}\n\treturn c.scrape(URL, \"GET\", 1, nil, nil, nil, true)\n}\n\n// HasVisited checks if the provided URL has been visited\nfunc (c *Collector) HasVisited(URL string) (bool, error) {\n\treturn c.checkHasVisited(URL, nil)\n}\n\n// HasPosted checks if the provided URL and requestData has been visited\n// This method is useful more likely to prevent re-visit same URL and POST body\nfunc (c *Collector) HasPosted(URL string, requestData map[string]string) (bool, error) {\n\treturn c.checkHasVisited(URL, requestData)\n}\n\n// Head starts a collector job by creating a HEAD request.\nfunc (c *Collector) Head(URL string) error {\n\treturn c.scrape(URL, \"HEAD\", 1, nil, nil, nil, false)\n}\n\n// Post starts a collector job by creating a POST request.\n// Post also calls the previously provided callbacks\nfunc (c *Collector) Post(URL string, requestData map[string]string) error {\n\treturn c.scrape(URL, \"POST\", 1, createFormReader(requestData), nil, nil, true)\n}\n\n// PostRaw starts a collector job by creating a POST request with raw binary data.\n// Post also calls the previously provided callbacks\nfunc (c *Collector) PostRaw(URL string, requestData []byte) error {\n\treturn c.scrape(URL, \"POST\", 1, bytes.NewReader(requestData), nil, nil, true)\n}\n\n// PostMultipart starts a collector job by creating a Multipart POST request\n// with raw binary data.  PostMultipart also calls the previously provided callbacks\nfunc (c *Collector) PostMultipart(URL string, requestData map[string][]byte) error {\n\tboundary := randomBoundary()\n\thdr := http.Header{}\n\thdr.Set(\"Content-Type\", \"multipart/form-data; boundary=\"+boundary)\n\thdr.Set(\"User-Agent\", c.UserAgent)\n\treturn c.scrape(URL, \"POST\", 1, createMultipartReader(boundary, requestData), nil, hdr, true)\n}\n\n// Request starts a collector job by creating a custom HTTP request\n// where method, context, headers and request data can be specified.\n// Set requestData, ctx, hdr parameters to nil if you don't want to use them.\n// Valid methods:\n//   - \"GET\"\n//   - \"HEAD\"\n//   - \"POST\"\n//   - \"PUT\"\n//   - \"DELETE\"\n//   - \"PATCH\"\n//   - \"OPTIONS\"\nfunc (c *Collector) Request(method, URL string, requestData io.Reader, ctx *Context, hdr http.Header) error {\n\treturn c.scrape(URL, method, 1, requestData, ctx, hdr, true)\n}\n\n// SetDebugger attaches a debugger to the collector\nfunc (c *Collector) SetDebugger(d debug.Debugger) {\n\td.Init()\n\tc.debugger = d\n}\n\n// UnmarshalRequest creates a Request from serialized data\nfunc (c *Collector) UnmarshalRequest(r []byte) (*Request, error) {\n\treq := &serializableRequest{}\n\terr := json.Unmarshal(r, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tu, err := url.Parse(req.URL)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tctx := NewContext()\n\tfor k, v := range req.Ctx {\n\t\tctx.Put(k, v)\n\t}\n\n\treturn &Request{\n\t\tMethod:    req.Method,\n\t\tURL:       u,\n\t\tDepth:     req.Depth,\n\t\tBody:      bytes.NewReader(req.Body),\n\t\tCtx:       ctx,\n\t\tID:        atomic.AddUint32(&c.requestCount, 1),\n\t\tHeaders:   &req.Headers,\n\t\tcollector: c,\n\t}, nil\n}\n\nfunc (c *Collector) scrape(u, method string, depth int, requestData io.Reader, ctx *Context, hdr http.Header, checkRevisit bool) error {\n\tparsedWhatwgURL, err := urlParser.Parse(u)\n\tif err != nil {\n\t\treturn err\n\t}\n\tparsedURL, err := url.Parse(parsedWhatwgURL.Href(false))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif hdr == nil {\n\t\thdr = http.Header{}\n\t\tif c.Headers != nil {\n\t\t\tfor k, v := range *c.Headers {\n\t\t\t\tfor _, value := range v {\n\t\t\t\t\thdr.Add(k, value)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif _, ok := hdr[\"User-Agent\"]; !ok {\n\t\thdr.Set(\"User-Agent\", c.UserAgent)\n\t}\n\tif seeker, ok := requestData.(io.ReadSeeker); ok {\n\t\t_, err := seeker.Seek(0, io.SeekStart)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treq, err := http.NewRequest(method, parsedURL.String(), requestData)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.Header = hdr\n\t// The Go HTTP API ignores \"Host\" in the headers, preferring the client\n\t// to use the Host field on Request.\n\tif hostHeader := hdr.Get(\"Host\"); hostHeader != \"\" {\n\t\treq.Host = hostHeader\n\t}\n\t// note: once 1.13 is minimum supported Go version,\n\t// replace this with http.NewRequestWithContext\n\treq = req.WithContext(c.Context)\n\tif err := c.requestCheck(parsedURL, method, req.GetBody, depth, checkRevisit); err != nil {\n\t\treturn err\n\t}\n\tu = parsedURL.String()\n\tc.wg.Add(1)\n\tif c.Async {\n\t\tgo c.fetch(u, method, depth, requestData, ctx, hdr, req)\n\t\treturn nil\n\t}\n\treturn c.fetch(u, method, depth, requestData, ctx, hdr, req)\n}\n\nfunc (c *Collector) fetch(u, method string, depth int, requestData io.Reader, ctx *Context, hdr http.Header, req *http.Request) error {\n\tdefer c.wg.Done()\n\tif ctx == nil {\n\t\tctx = NewContext()\n\t}\n\trequest := &Request{\n\t\tURL:       req.URL,\n\t\tHeaders:   &req.Header,\n\t\tHost:      req.Host,\n\t\tCtx:       ctx,\n\t\tDepth:     depth,\n\t\tMethod:    method,\n\t\tBody:      requestData,\n\t\tcollector: c,\n\t\tID:        atomic.AddUint32(&c.requestCount, 1),\n\t}\n\n\tif req.Header.Get(\"Accept\") == \"\" {\n\t\treq.Header.Set(\"Accept\", \"*/*\")\n\t}\n\n\tc.handleOnRequest(request)\n\n\tif request.abort {\n\t\treturn nil\n\t}\n\n\tif method == \"POST\" && req.Header.Get(\"Content-Type\") == \"\" {\n\t\treq.Header.Add(\"Content-Type\", \"application/x-www-form-urlencoded\")\n\t}\n\n\tvar hTrace *HTTPTrace\n\tif c.TraceHTTP {\n\t\thTrace = &HTTPTrace{}\n\t\treq = hTrace.WithTrace(req)\n\t}\n\torigURL := req.URL\n\tcheckHeadersFunc := func(req *http.Request, statusCode int, headers http.Header) bool {\n\t\tif req.URL != origURL {\n\t\t\trequest.URL = req.URL\n\t\t\trequest.Headers = &req.Header\n\t\t}\n\t\tc.handleOnResponseHeaders(&Response{Ctx: ctx, Request: request, StatusCode: statusCode, Headers: &headers})\n\t\treturn !request.abort\n\t}\n\tresponse, err := c.backend.Cache(req, c.MaxBodySize, checkHeadersFunc, c.CacheDir)\n\tif proxyURL, ok := req.Context().Value(ProxyURLKey).(string); ok {\n\t\trequest.ProxyURL = proxyURL\n\t}\n\tif err := c.handleOnError(response, err, request, ctx); err != nil {\n\t\treturn err\n\t}\n\tatomic.AddUint32(&c.responseCount, 1)\n\tresponse.Ctx = ctx\n\tresponse.Request = request\n\tresponse.Trace = hTrace\n\n\terr = response.fixCharset(c.DetectCharset, request.ResponseCharacterEncoding)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tc.handleOnResponse(response)\n\n\terr = c.handleOnHTML(response)\n\tif err != nil {\n\t\tc.handleOnError(response, err, request, ctx)\n\t}\n\n\terr = c.handleOnXML(response)\n\tif err != nil {\n\t\tc.handleOnError(response, err, request, ctx)\n\t}\n\n\tc.handleOnScraped(response)\n\n\treturn err\n}\n\nfunc (c *Collector) requestCheck(parsedURL *url.URL, method string, getBody func() (io.ReadCloser, error), depth int, checkRevisit bool) error {\n\tu := parsedURL.String()\n\tif c.MaxDepth > 0 && c.MaxDepth < depth {\n\t\treturn ErrMaxDepth\n\t}\n\tif c.MaxRequests > 0 && c.requestCount >= c.MaxRequests {\n\t\treturn ErrMaxRequests\n\t}\n\tif err := c.checkFilters(u, parsedURL.Hostname()); err != nil {\n\t\treturn err\n\t}\n\tif method != \"HEAD\" && !c.IgnoreRobotsTxt {\n\t\tif err := c.checkRobots(parsedURL); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif checkRevisit && !c.AllowURLRevisit {\n\t\t// TODO weird behaviour, it allows CheckHead to work correctly,\n\t\t// but it should probably better be solved with\n\t\t// \"check-but-not-save\" flag or something\n\t\tif method != \"GET\" && getBody == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tvar body io.ReadCloser\n\t\tif getBody != nil {\n\t\t\tvar err error\n\t\t\tbody, err = getBody()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdefer body.Close()\n\t\t}\n\t\tuHash := requestHash(u, body)\n\t\tvisited, err := c.store.IsVisited(uHash)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif visited {\n\t\t\treturn &AlreadyVisitedError{parsedURL}\n\t\t}\n\t\treturn c.store.Visited(uHash)\n\t}\n\treturn nil\n}\n\nfunc (c *Collector) checkFilters(URL, domain string) error {\n\tif len(c.DisallowedURLFilters) > 0 {\n\t\tif isMatchingFilter(c.DisallowedURLFilters, []byte(URL)) {\n\t\t\treturn ErrForbiddenURL\n\t\t}\n\t}\n\tif len(c.URLFilters) > 0 {\n\t\tif !isMatchingFilter(c.URLFilters, []byte(URL)) {\n\t\t\treturn ErrNoURLFiltersMatch\n\t\t}\n\t}\n\tif !c.isDomainAllowed(domain) {\n\t\treturn ErrForbiddenDomain\n\t}\n\treturn nil\n}\n\nfunc (c *Collector) isDomainAllowed(domain string) bool {\n\tfor _, d2 := range c.DisallowedDomains {\n\t\tif d2 == domain {\n\t\t\treturn false\n\t\t}\n\t}\n\tif c.AllowedDomains == nil || len(c.AllowedDomains) == 0 {\n\t\treturn true\n\t}\n\tfor _, d2 := range c.AllowedDomains {\n\t\tif d2 == domain {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (c *Collector) checkRobots(u *url.URL) error {\n\tc.lock.RLock()\n\trobot, ok := c.robotsMap[u.Host]\n\tc.lock.RUnlock()\n\n\tif !ok {\n\t\t// no robots file cached\n\t\tresp, err := c.backend.Client.Get(u.Scheme + \"://\" + u.Host + \"/robots.txt\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer resp.Body.Close()\n\n\t\trobot, err = robotstxt.FromResponse(resp)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.lock.Lock()\n\t\tc.robotsMap[u.Host] = robot\n\t\tc.lock.Unlock()\n\t}\n\n\tuaGroup := robot.FindGroup(c.UserAgent)\n\tif uaGroup == nil {\n\t\treturn nil\n\t}\n\n\teu := u.EscapedPath()\n\tif u.RawQuery != \"\" {\n\t\teu += \"?\" + u.Query().Encode()\n\t}\n\tif !uaGroup.Test(eu) {\n\t\treturn ErrRobotsTxtBlocked\n\t}\n\treturn nil\n}\n\n// String is the text representation of the collector.\n// It contains useful debug information about the collector's internals\nfunc (c *Collector) String() string {\n\treturn fmt.Sprintf(\n\t\t\"Requests made: %d (%d responses) | Callbacks: OnRequest: %d, OnHTML: %d, OnResponse: %d, OnError: %d\",\n\t\tatomic.LoadUint32(&c.requestCount),\n\t\tatomic.LoadUint32(&c.responseCount),\n\t\tlen(c.requestCallbacks),\n\t\tlen(c.htmlCallbacks),\n\t\tlen(c.responseCallbacks),\n\t\tlen(c.errorCallbacks),\n\t)\n}\n\n// Wait returns when the collector jobs are finished\nfunc (c *Collector) Wait() {\n\tc.wg.Wait()\n}\n\n// OnRequest registers a function. Function will be executed on every\n// request made by the Collector\nfunc (c *Collector) OnRequest(f RequestCallback) {\n\tc.lock.Lock()\n\tif c.requestCallbacks == nil {\n\t\tc.requestCallbacks = make([]RequestCallback, 0, 4)\n\t}\n\tc.requestCallbacks = append(c.requestCallbacks, f)\n\tc.lock.Unlock()\n}\n\n// OnResponseHeaders registers a function. Function will be executed on every response\n// when headers and status are already received, but body is not yet read.\n//\n// Like in OnRequest, you can call Request.Abort to abort the transfer. This might be\n// useful if, for example, you're following all hyperlinks, but want to avoid\n// downloading files.\n//\n// Be aware that using this will prevent HTTP/1.1 connection reuse, as\n// the only way to abort a download is to immediately close the connection.\n// HTTP/2 doesn't suffer from this problem, as it's possible to close\n// specific stream inside the connection.\nfunc (c *Collector) OnResponseHeaders(f ResponseHeadersCallback) {\n\tc.lock.Lock()\n\tc.responseHeadersCallbacks = append(c.responseHeadersCallbacks, f)\n\tc.lock.Unlock()\n}\n\n// OnResponse registers a function. Function will be executed on every response\nfunc (c *Collector) OnResponse(f ResponseCallback) {\n\tc.lock.Lock()\n\tif c.responseCallbacks == nil {\n\t\tc.responseCallbacks = make([]ResponseCallback, 0, 4)\n\t}\n\tc.responseCallbacks = append(c.responseCallbacks, f)\n\tc.lock.Unlock()\n}\n\n// OnHTML registers a function. Function will be executed on every HTML\n// element matched by the GoQuery Selector parameter.\n// GoQuery Selector is a selector used by https://github.com/PuerkitoBio/goquery\nfunc (c *Collector) OnHTML(goquerySelector string, f HTMLCallback) {\n\tc.lock.Lock()\n\tif c.htmlCallbacks == nil {\n\t\tc.htmlCallbacks = make([]*htmlCallbackContainer, 0, 4)\n\t}\n\tc.htmlCallbacks = append(c.htmlCallbacks, &htmlCallbackContainer{\n\t\tSelector: goquerySelector,\n\t\tFunction: f,\n\t})\n\tc.lock.Unlock()\n}\n\n// OnXML registers a function. Function will be executed on every XML\n// element matched by the xpath Query parameter.\n// xpath Query is used by https://github.com/antchfx/xmlquery\nfunc (c *Collector) OnXML(xpathQuery string, f XMLCallback) {\n\tc.lock.Lock()\n\tif c.xmlCallbacks == nil {\n\t\tc.xmlCallbacks = make([]*xmlCallbackContainer, 0, 4)\n\t}\n\tc.xmlCallbacks = append(c.xmlCallbacks, &xmlCallbackContainer{\n\t\tQuery:    xpathQuery,\n\t\tFunction: f,\n\t})\n\tc.lock.Unlock()\n}\n\n// OnHTMLDetach deregister a function. Function will not be execute after detached\nfunc (c *Collector) OnHTMLDetach(goquerySelector string) {\n\tc.lock.Lock()\n\tdeleteIdx := -1\n\tfor i, cc := range c.htmlCallbacks {\n\t\tif cc.Selector == goquerySelector {\n\t\t\tdeleteIdx = i\n\t\t\tbreak\n\t\t}\n\t}\n\tif deleteIdx != -1 {\n\t\tc.htmlCallbacks = append(c.htmlCallbacks[:deleteIdx], c.htmlCallbacks[deleteIdx+1:]...)\n\t}\n\tc.lock.Unlock()\n}\n\n// OnXMLDetach deregister a function. Function will not be execute after detached\nfunc (c *Collector) OnXMLDetach(xpathQuery string) {\n\tc.lock.Lock()\n\tdeleteIdx := -1\n\tfor i, cc := range c.xmlCallbacks {\n\t\tif cc.Query == xpathQuery {\n\t\t\tdeleteIdx = i\n\t\t\tbreak\n\t\t}\n\t}\n\tif deleteIdx != -1 {\n\t\tc.xmlCallbacks = append(c.xmlCallbacks[:deleteIdx], c.xmlCallbacks[deleteIdx+1:]...)\n\t}\n\tc.lock.Unlock()\n}\n\n// OnError registers a function. Function will be executed if an error\n// occurs during the HTTP request.\nfunc (c *Collector) OnError(f ErrorCallback) {\n\tc.lock.Lock()\n\tif c.errorCallbacks == nil {\n\t\tc.errorCallbacks = make([]ErrorCallback, 0, 4)\n\t}\n\tc.errorCallbacks = append(c.errorCallbacks, f)\n\tc.lock.Unlock()\n}\n\n// OnScraped registers a function. Function will be executed after\n// OnHTML, as a final part of the scraping.\nfunc (c *Collector) OnScraped(f ScrapedCallback) {\n\tc.lock.Lock()\n\tif c.scrapedCallbacks == nil {\n\t\tc.scrapedCallbacks = make([]ScrapedCallback, 0, 4)\n\t}\n\tc.scrapedCallbacks = append(c.scrapedCallbacks, f)\n\tc.lock.Unlock()\n}\n\n// SetClient will override the previously set http.Client\nfunc (c *Collector) SetClient(client *http.Client) {\n\tc.backend.Client = client\n}\n\n// WithTransport allows you to set a custom http.RoundTripper (transport)\nfunc (c *Collector) WithTransport(transport http.RoundTripper) {\n\tc.backend.Client.Transport = transport\n}\n\n// DisableCookies turns off cookie handling\nfunc (c *Collector) DisableCookies() {\n\tc.backend.Client.Jar = nil\n}\n\n// SetCookieJar overrides the previously set cookie jar\nfunc (c *Collector) SetCookieJar(j http.CookieJar) {\n\tc.backend.Client.Jar = j\n}\n\n// SetRequestTimeout overrides the default timeout (10 seconds) for this collector\nfunc (c *Collector) SetRequestTimeout(timeout time.Duration) {\n\tc.backend.Client.Timeout = timeout\n}\n\n// SetStorage overrides the default in-memory storage.\n// Storage stores scraping related data like cookies and visited urls\nfunc (c *Collector) SetStorage(s storage.Storage) error {\n\tif err := s.Init(); err != nil {\n\t\treturn err\n\t}\n\tc.store = s\n\tc.backend.Client.Jar = createJar(s)\n\treturn nil\n}\n\n// SetProxy sets a proxy for the collector. This method overrides the previously\n// used http.Transport if the type of the transport is not http.RoundTripper.\n// The proxy type is determined by the URL scheme. \"http\"\n// and \"socks5\" are supported. If the scheme is empty,\n// \"http\" is assumed.\nfunc (c *Collector) SetProxy(proxyURL string) error {\n\tproxyParsed, err := url.Parse(proxyURL)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tc.SetProxyFunc(http.ProxyURL(proxyParsed))\n\n\treturn nil\n}\n\n// SetProxyFunc sets a custom proxy setter/switcher function.\n// See built-in ProxyFuncs for more details.\n// This method overrides the previously used http.Transport\n// if the type of the transport is not http.RoundTripper.\n// The proxy type is determined by the URL scheme. \"http\"\n// and \"socks5\" are supported. If the scheme is empty,\n// \"http\" is assumed.\nfunc (c *Collector) SetProxyFunc(p ProxyFunc) {\n\tt, ok := c.backend.Client.Transport.(*http.Transport)\n\tif c.backend.Client.Transport != nil && ok {\n\t\tt.Proxy = p\n\t\tt.DisableKeepAlives = true\n\t} else {\n\t\tc.backend.Client.Transport = &http.Transport{\n\t\t\tProxy:             p,\n\t\t\tDisableKeepAlives: true,\n\t\t}\n\t}\n}\n\nfunc createEvent(eventType string, requestID, collectorID uint32, kvargs map[string]string) *debug.Event {\n\treturn &debug.Event{\n\t\tCollectorID: collectorID,\n\t\tRequestID:   requestID,\n\t\tType:        eventType,\n\t\tValues:      kvargs,\n\t}\n}\n\nfunc (c *Collector) handleOnRequest(r *Request) {\n\tif c.debugger != nil {\n\t\tc.debugger.Event(createEvent(\"request\", r.ID, c.ID, map[string]string{\n\t\t\t\"url\": r.URL.String(),\n\t\t}))\n\t}\n\tfor _, f := range c.requestCallbacks {\n\t\tf(r)\n\t}\n}\n\nfunc (c *Collector) handleOnResponse(r *Response) {\n\tif c.debugger != nil {\n\t\tc.debugger.Event(createEvent(\"response\", r.Request.ID, c.ID, map[string]string{\n\t\t\t\"url\":    r.Request.URL.String(),\n\t\t\t\"status\": http.StatusText(r.StatusCode),\n\t\t}))\n\t}\n\tfor _, f := range c.responseCallbacks {\n\t\tf(r)\n\t}\n}\n\nfunc (c *Collector) handleOnResponseHeaders(r *Response) {\n\tif c.debugger != nil {\n\t\tc.debugger.Event(createEvent(\"responseHeaders\", r.Request.ID, c.ID, map[string]string{\n\t\t\t\"url\":    r.Request.URL.String(),\n\t\t\t\"status\": http.StatusText(r.StatusCode),\n\t\t}))\n\t}\n\tfor _, f := range c.responseHeadersCallbacks {\n\t\tf(r)\n\t}\n}\n\nfunc (c *Collector) handleOnHTML(resp *Response) error {\n\tif len(c.htmlCallbacks) == 0 {\n\t\treturn nil\n\t}\n\n\tcontentType := resp.Headers.Get(\"Content-Type\")\n\tif contentType == \"\" {\n\t\tcontentType = http.DetectContentType(resp.Body)\n\t}\n\t// implementation of mime.ParseMediaType without parsing the params\n\t// part\n\tmediatype, _, _ := strings.Cut(contentType, \";\")\n\tmediatype = strings.TrimSpace(strings.ToLower(mediatype))\n\n\t// TODO we also want to parse application/xml as XHTML if it has\n\t// appropriate doctype\n\tswitch mediatype {\n\tcase \"text/html\", \"application/xhtml+xml\":\n\tdefault:\n\t\treturn nil\n\t}\n\n\tdoc, err := goquery.NewDocumentFromReader(bytes.NewBuffer(resp.Body))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif href, found := doc.Find(\"base[href]\").Attr(\"href\"); found {\n\t\tu, err := urlParser.ParseRef(resp.Request.URL.String(), href)\n\t\tif err == nil {\n\t\t\tbaseURL, err := url.Parse(u.Href(false))\n\t\t\tif err == nil {\n\t\t\t\tresp.Request.baseURL = baseURL\n\t\t\t}\n\t\t}\n\n\t}\n\tfor _, cc := range c.htmlCallbacks {\n\t\ti := 0\n\t\tdoc.Find(cc.Selector).Each(func(_ int, s *goquery.Selection) {\n\t\t\tfor _, n := range s.Nodes {\n\t\t\t\te := NewHTMLElementFromSelectionNode(resp, s, n, i)\n\t\t\t\ti++\n\t\t\t\tif c.debugger != nil {\n\t\t\t\t\tc.debugger.Event(createEvent(\"html\", resp.Request.ID, c.ID, map[string]string{\n\t\t\t\t\t\t\"selector\": cc.Selector,\n\t\t\t\t\t\t\"url\":      resp.Request.URL.String(),\n\t\t\t\t\t}))\n\t\t\t\t}\n\t\t\t\tcc.Function(e)\n\t\t\t}\n\t\t})\n\t}\n\treturn nil\n}\n\nfunc (c *Collector) handleOnXML(resp *Response) error {\n\tif len(c.xmlCallbacks) == 0 {\n\t\treturn nil\n\t}\n\tcontentType := strings.ToLower(resp.Headers.Get(\"Content-Type\"))\n\tisXMLFile := strings.HasSuffix(strings.ToLower(resp.Request.URL.Path), \".xml\") || strings.HasSuffix(strings.ToLower(resp.Request.URL.Path), \".xml.gz\")\n\tif !strings.Contains(contentType, \"html\") && (!strings.Contains(contentType, \"xml\") && !isXMLFile) {\n\t\treturn nil\n\t}\n\n\tif strings.Contains(contentType, \"html\") {\n\t\tdoc, err := htmlquery.Parse(bytes.NewBuffer(resp.Body))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif e := htmlquery.FindOne(doc, \"//base\"); e != nil {\n\t\t\tfor _, a := range e.Attr {\n\t\t\t\tif a.Key == \"href\" {\n\t\t\t\t\tbaseURL, err := resp.Request.URL.Parse(a.Val)\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tresp.Request.baseURL = baseURL\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor _, cc := range c.xmlCallbacks {\n\t\t\tfor _, n := range htmlquery.Find(doc, cc.Query) {\n\t\t\t\te := NewXMLElementFromHTMLNode(resp, n)\n\t\t\t\tif c.debugger != nil {\n\t\t\t\t\tc.debugger.Event(createEvent(\"xml\", resp.Request.ID, c.ID, map[string]string{\n\t\t\t\t\t\t\"selector\": cc.Query,\n\t\t\t\t\t\t\"url\":      resp.Request.URL.String(),\n\t\t\t\t\t}))\n\t\t\t\t}\n\t\t\t\tcc.Function(e)\n\t\t\t}\n\t\t}\n\t} else if strings.Contains(contentType, \"xml\") || isXMLFile {\n\t\tdoc, err := xmlquery.Parse(bytes.NewBuffer(resp.Body))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor _, cc := range c.xmlCallbacks {\n\t\t\txmlquery.FindEach(doc, cc.Query, func(i int, n *xmlquery.Node) {\n\t\t\t\te := NewXMLElementFromXMLNode(resp, n)\n\t\t\t\tif c.debugger != nil {\n\t\t\t\t\tc.debugger.Event(createEvent(\"xml\", resp.Request.ID, c.ID, map[string]string{\n\t\t\t\t\t\t\"selector\": cc.Query,\n\t\t\t\t\t\t\"url\":      resp.Request.URL.String(),\n\t\t\t\t\t}))\n\t\t\t\t}\n\t\t\t\tcc.Function(e)\n\t\t\t})\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (c *Collector) handleOnError(response *Response, err error, request *Request, ctx *Context) error {\n\tif err == nil && (c.ParseHTTPErrorResponse || response.StatusCode < 203) {\n\t\treturn nil\n\t}\n\tif err == nil && response.StatusCode >= 203 {\n\t\terr = errors.New(http.StatusText(response.StatusCode))\n\t}\n\tif response == nil {\n\t\tresponse = &Response{\n\t\t\tRequest: request,\n\t\t\tCtx:     ctx,\n\t\t}\n\t}\n\tif c.debugger != nil {\n\t\tc.debugger.Event(createEvent(\"error\", request.ID, c.ID, map[string]string{\n\t\t\t\"url\":    request.URL.String(),\n\t\t\t\"status\": http.StatusText(response.StatusCode),\n\t\t}))\n\t}\n\tif response.Request == nil {\n\t\tresponse.Request = request\n\t}\n\tif response.Ctx == nil {\n\t\tresponse.Ctx = request.Ctx\n\t}\n\tfor _, f := range c.errorCallbacks {\n\t\tf(response, err)\n\t}\n\treturn err\n}\n\nfunc (c *Collector) handleOnScraped(r *Response) {\n\tif c.debugger != nil {\n\t\tc.debugger.Event(createEvent(\"scraped\", r.Request.ID, c.ID, map[string]string{\n\t\t\t\"url\": r.Request.URL.String(),\n\t\t}))\n\t}\n\tfor _, f := range c.scrapedCallbacks {\n\t\tf(r)\n\t}\n}\n\n// Limit adds a new LimitRule to the collector\nfunc (c *Collector) Limit(rule *LimitRule) error {\n\treturn c.backend.Limit(rule)\n}\n\n// Limits adds new LimitRules to the collector\nfunc (c *Collector) Limits(rules []*LimitRule) error {\n\treturn c.backend.Limits(rules)\n}\n\n// SetRedirectHandler instructs the Collector to allow multiple downloads of the same URL\nfunc (c *Collector) SetRedirectHandler(f func(req *http.Request, via []*http.Request) error) {\n\tc.redirectHandler = f\n\tc.backend.Client.CheckRedirect = c.checkRedirectFunc()\n}\n\n// SetCookies handles the receipt of the cookies in a reply for the given URL\nfunc (c *Collector) SetCookies(URL string, cookies []*http.Cookie) error {\n\tif c.backend.Client.Jar == nil {\n\t\treturn ErrNoCookieJar\n\t}\n\tu, err := url.Parse(URL)\n\tif err != nil {\n\t\treturn err\n\t}\n\tc.backend.Client.Jar.SetCookies(u, cookies)\n\treturn nil\n}\n\n// Cookies returns the cookies to send in a request for the given URL.\nfunc (c *Collector) Cookies(URL string) []*http.Cookie {\n\tif c.backend.Client.Jar == nil {\n\t\treturn nil\n\t}\n\tu, err := url.Parse(URL)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn c.backend.Client.Jar.Cookies(u)\n}\n\n// Clone creates an exact copy of a Collector without callbacks.\n// HTTP backend, robots.txt cache and cookie jar are shared\n// between collectors.\nfunc (c *Collector) Clone() *Collector {\n\treturn &Collector{\n\t\tAllowedDomains:         c.AllowedDomains,\n\t\tAllowURLRevisit:        c.AllowURLRevisit,\n\t\tCacheDir:               c.CacheDir,\n\t\tDetectCharset:          c.DetectCharset,\n\t\tDisallowedDomains:      c.DisallowedDomains,\n\t\tID:                     atomic.AddUint32(&collectorCounter, 1),\n\t\tIgnoreRobotsTxt:        c.IgnoreRobotsTxt,\n\t\tMaxBodySize:            c.MaxBodySize,\n\t\tMaxDepth:               c.MaxDepth,\n\t\tMaxRequests:            c.MaxRequests,\n\t\tDisallowedURLFilters:   c.DisallowedURLFilters,\n\t\tURLFilters:             c.URLFilters,\n\t\tCheckHead:              c.CheckHead,\n\t\tParseHTTPErrorResponse: c.ParseHTTPErrorResponse,\n\t\tUserAgent:              c.UserAgent,\n\t\tHeaders:                c.Headers,\n\t\tTraceHTTP:              c.TraceHTTP,\n\t\tContext:                c.Context,\n\t\tstore:                  c.store,\n\t\tbackend:                c.backend,\n\t\tdebugger:               c.debugger,\n\t\tAsync:                  c.Async,\n\t\tredirectHandler:        c.redirectHandler,\n\t\terrorCallbacks:         make([]ErrorCallback, 0, 8),\n\t\thtmlCallbacks:          make([]*htmlCallbackContainer, 0, 8),\n\t\txmlCallbacks:           make([]*xmlCallbackContainer, 0, 8),\n\t\tscrapedCallbacks:       make([]ScrapedCallback, 0, 8),\n\t\tlock:                   c.lock,\n\t\trequestCallbacks:       make([]RequestCallback, 0, 8),\n\t\tresponseCallbacks:      make([]ResponseCallback, 0, 8),\n\t\trobotsMap:              c.robotsMap,\n\t\twg:                     &sync.WaitGroup{},\n\t}\n}\n\nfunc (c *Collector) checkRedirectFunc() func(req *http.Request, via []*http.Request) error {\n\treturn func(req *http.Request, via []*http.Request) error {\n\t\tif err := c.checkFilters(req.URL.String(), req.URL.Hostname()); err != nil {\n\t\t\treturn fmt.Errorf(\"Not following redirect to %q: %w\", req.URL, err)\n\t\t}\n\n\t\t// allow redirects to the original destination\n\t\t// to support websites redirecting to the same page while setting\n\t\t// session cookies\n\t\tsamePageRedirect := normalizeURL(req.URL.String()) == normalizeURL(via[0].URL.String())\n\n\t\tif !c.AllowURLRevisit && !samePageRedirect {\n\t\t\tvar body io.ReadCloser\n\t\t\tif req.GetBody != nil {\n\t\t\t\tvar err error\n\t\t\t\tbody, err = req.GetBody()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tdefer body.Close()\n\t\t\t}\n\t\t\tuHash := requestHash(req.URL.String(), body)\n\t\t\tvisited, err := c.store.IsVisited(uHash)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif visited {\n\t\t\t\treturn &AlreadyVisitedError{req.URL}\n\t\t\t}\n\t\t\terr = c.store.Visited(uHash)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tif c.redirectHandler != nil {\n\t\t\treturn c.redirectHandler(req, via)\n\t\t}\n\n\t\t// Honor golangs default of maximum of 10 redirects\n\t\tif len(via) >= 10 {\n\t\t\treturn http.ErrUseLastResponse\n\t\t}\n\n\t\tlastRequest := via[len(via)-1]\n\n\t\t// If domain has changed, remove the Authorization-header if it exists\n\t\tif req.URL.Host != lastRequest.URL.Host {\n\t\t\treq.Header.Del(\"Authorization\")\n\t\t}\n\n\t\treturn nil\n\t}\n}\n\nfunc (c *Collector) parseSettingsFromEnv() {\n\tfor _, e := range os.Environ() {\n\t\tif !strings.HasPrefix(e, \"COLLY_\") {\n\t\t\tcontinue\n\t\t}\n\t\tpair := strings.SplitN(e[6:], \"=\", 2)\n\t\tif f, ok := envMap[pair[0]]; ok {\n\t\t\tf(c, pair[1])\n\t\t} else {\n\t\t\tlog.Println(\"Unknown environment variable:\", pair[0])\n\t\t}\n\t}\n}\n\nfunc (c *Collector) checkHasVisited(URL string, requestData map[string]string) (bool, error) {\n\thash := requestHash(URL, createFormReader(requestData))\n\treturn c.store.IsVisited(hash)\n}\n\n// SanitizeFileName replaces dangerous characters in a string\n// so the return value can be used as a safe file name.\nfunc SanitizeFileName(fileName string) string {\n\text := filepath.Ext(fileName)\n\tcleanExt := sanitize.BaseName(ext)\n\tif cleanExt == \"\" {\n\t\tcleanExt = \".unknown\"\n\t}\n\treturn strings.Replace(fmt.Sprintf(\n\t\t\"%s.%s\",\n\t\tsanitize.BaseName(fileName[:len(fileName)-len(ext)]),\n\t\tcleanExt[1:],\n\t), \"-\", \"_\", -1)\n}\n\nfunc createFormReader(data map[string]string) io.Reader {\n\tform := url.Values{}\n\tfor k, v := range data {\n\t\tform.Add(k, v)\n\t}\n\treturn strings.NewReader(form.Encode())\n}\n\nfunc createMultipartReader(boundary string, data map[string][]byte) io.Reader {\n\tdashBoundary := \"--\" + boundary\n\n\tbody := []byte{}\n\tbuffer := bytes.NewBuffer(body)\n\n\tbuffer.WriteString(\"Content-type: multipart/form-data; boundary=\" + boundary + \"\\n\\n\")\n\tfor contentType, content := range data {\n\t\tbuffer.WriteString(dashBoundary + \"\\n\")\n\t\tbuffer.WriteString(\"Content-Disposition: form-data; name=\" + contentType + \"\\n\")\n\t\tbuffer.WriteString(fmt.Sprintf(\"Content-Length: %d \\n\\n\", len(content)))\n\t\tbuffer.Write(content)\n\t\tbuffer.WriteString(\"\\n\")\n\t}\n\tbuffer.WriteString(dashBoundary + \"--\\n\\n\")\n\treturn bytes.NewReader(buffer.Bytes())\n\n}\n\n// randomBoundary was borrowed from\n// github.com/golang/go/mime/multipart/writer.go#randomBoundary\nfunc randomBoundary() string {\n\tvar buf [30]byte\n\t_, err := io.ReadFull(rand.Reader, buf[:])\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn fmt.Sprintf(\"%x\", buf[:])\n}\n\nfunc isYesString(s string) bool {\n\tswitch strings.ToLower(s) {\n\tcase \"1\", \"yes\", \"true\", \"y\":\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc createJar(s storage.Storage) http.CookieJar {\n\treturn &cookieJarSerializer{store: s, lock: &sync.RWMutex{}}\n}\n\nfunc (j *cookieJarSerializer) SetCookies(u *url.URL, cookies []*http.Cookie) {\n\tj.lock.Lock()\n\tdefer j.lock.Unlock()\n\tcookieStr := j.store.Cookies(u)\n\n\t// Merge existing cookies, new cookies have precedence.\n\tcnew := make([]*http.Cookie, len(cookies))\n\tcopy(cnew, cookies)\n\texisting := storage.UnstringifyCookies(cookieStr)\n\tfor _, c := range existing {\n\t\tif !storage.ContainsCookie(cnew, c.Name) {\n\t\t\tcnew = append(cnew, c)\n\t\t}\n\t}\n\tj.store.SetCookies(u, storage.StringifyCookies(cnew))\n}\n\nfunc (j *cookieJarSerializer) Cookies(u *url.URL) []*http.Cookie {\n\tcookies := storage.UnstringifyCookies(j.store.Cookies(u))\n\t// Filter.\n\tnow := time.Now()\n\tcnew := make([]*http.Cookie, 0, len(cookies))\n\tfor _, c := range cookies {\n\t\t// Drop expired cookies.\n\t\tif c.RawExpires != \"\" && c.Expires.Before(now) {\n\t\t\tcontinue\n\t\t}\n\t\t// Drop secure cookies if not over https.\n\t\tif c.Secure && u.Scheme != \"https\" {\n\t\t\tcontinue\n\t\t}\n\t\tcnew = append(cnew, c)\n\t}\n\treturn cnew\n}\n\nfunc isMatchingFilter(fs []*regexp.Regexp, d []byte) bool {\n\tfor _, r := range fs {\n\t\tif r.Match(d) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc normalizeURL(u string) string {\n\tparsed, err := urlParser.Parse(u)\n\tif err != nil {\n\t\treturn u\n\t}\n\treturn parsed.String()\n}\n\nfunc requestHash(url string, body io.Reader) uint64 {\n\th := fnv.New64a()\n\t// reparse the url to fix ambiguities such as\n\t// \"http://example.com\" vs \"http://example.com/\"\n\tio.WriteString(h, normalizeURL(url))\n\tif body != nil {\n\t\tio.Copy(h, body)\n\t}\n\treturn h.Sum64()\n}\n"
        },
        {
          "name": "colly_test.go",
          "type": "blob",
          "size": 40.9619140625,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"net/url\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/PuerkitoBio/goquery\"\n\n\t\"github.com/gocolly/colly/v2/debug\"\n)\n\nvar serverIndexResponse = []byte(\"hello world\\n\")\nvar robotsFile = `\nUser-agent: *\nAllow: /allowed\nDisallow: /disallowed\nDisallow: /allowed*q=\n`\n\nfunc newUnstartedTestServer() *httptest.Server {\n\tmux := http.NewServeMux()\n\n\tmux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write(serverIndexResponse)\n\t})\n\n\tmux.HandleFunc(\"/html\", func(w http.ResponseWriter, r *http.Request) {\n\t\tif r.URL.Query().Get(\"no-content-type\") != \"\" {\n\t\t\tw.Header()[\"Content-Type\"] = nil\n\t\t} else {\n\t\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\t}\n\t\tw.Write([]byte(`<!DOCTYPE html>\n<html>\n<head>\n<title>Test Page</title>\n</head>\n<body>\n<h1>Hello World</h1>\n<p class=\"description\">This is a test page</p>\n<p class=\"description\">This is a test paragraph</p>\n</body>\n</html>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/xml\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/xml\")\n\t\tw.Write([]byte(`<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<page>\n\t<title>Test Page</title>\n\t<paragraph type=\"description\">This is a test page</paragraph>\n\t<paragraph type=\"description\">This is a test paragraph</paragraph>\n</page>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/login\", func(w http.ResponseWriter, r *http.Request) {\n\t\tif r.Method == \"POST\" {\n\t\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\t\tw.Write([]byte(r.FormValue(\"name\")))\n\t\t}\n\t})\n\n\tmux.HandleFunc(\"/robots.txt\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(robotsFile))\n\t})\n\n\tmux.HandleFunc(\"/allowed\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(\"allowed\"))\n\t})\n\n\tmux.HandleFunc(\"/disallowed\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(\"disallowed\"))\n\t})\n\n\tmux.Handle(\"/redirect\", http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tdestination := \"/redirected/\"\n\t\tif d := r.URL.Query().Get(\"d\"); d != \"\" {\n\t\t\tdestination = d\n\t\t}\n\t\thttp.Redirect(w, r, destination, http.StatusSeeOther)\n\n\t}))\n\n\tmux.Handle(\"/redirected/\", http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tfmt.Fprintf(w, `<a href=\"test\">test</a>`)\n\t}))\n\n\tmux.HandleFunc(\"/set_cookie\", func(w http.ResponseWriter, r *http.Request) {\n\t\tc := &http.Cookie{Name: \"test\", Value: \"testv\", HttpOnly: false}\n\t\thttp.SetCookie(w, c)\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(\"ok\"))\n\t})\n\n\tmux.HandleFunc(\"/check_cookie\", func(w http.ResponseWriter, r *http.Request) {\n\t\tcs := r.Cookies()\n\t\tif len(cs) != 1 || r.Cookies()[0].Value != \"testv\" {\n\t\t\tw.WriteHeader(500)\n\t\t\tw.Write([]byte(\"nok\"))\n\t\t\treturn\n\t\t}\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(\"ok\"))\n\t})\n\n\tmux.HandleFunc(\"/500\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\tw.WriteHeader(500)\n\t\tw.Write([]byte(\"<p>error</p>\"))\n\t})\n\n\tmux.HandleFunc(\"/user_agent\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(r.Header.Get(\"User-Agent\")))\n\t})\n\n\tmux.HandleFunc(\"/host_header\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(r.Host))\n\t})\n\n\tmux.HandleFunc(\"/accept_header\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(r.Header.Get(\"Accept\")))\n\t})\n\n\tmux.HandleFunc(\"/custom_header\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\t\tw.Write([]byte(r.Header.Get(\"Test\")))\n\t})\n\n\tmux.HandleFunc(\"/base\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\tw.Write([]byte(`<!DOCTYPE html>\n<html>\n<head>\n<title>Test Page</title>\n<base href=\"http://xy.com/\" />\n</head>\n<body>\n<a href=\"z\">link</a>\n</body>\n</html>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/base_relative\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\tw.Write([]byte(`<!DOCTYPE html>\n<html>\n<head>\n<title>Test Page</title>\n<base href=\"/foobar/\" />\n</head>\n<body>\n<a href=\"z\">link</a>\n</body>\n</html>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/tabs_and_newlines\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\tw.Write([]byte(`<!DOCTYPE html>\n<html>\n<head>\n<title>Test Page</title>\n<base href=\"/foo\tbar/\" />\n</head>\n<body>\n<a href=\"x\ny\">link</a>\n</body>\n</html>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/foobar/xy\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"text/html\")\n\t\tw.Write([]byte(`<!DOCTYPE html>\n<html>\n<head>\n<title>Test Page</title>\n</head>\n<body>\n<p>hello</p>\n</body>\n</html>\n\t\t`))\n\t})\n\n\tmux.HandleFunc(\"/100%25\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Write([]byte(\"100 percent\"))\n\t})\n\n\tmux.HandleFunc(\"/large_binary\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/octet-stream\")\n\t\tww := bufio.NewWriter(w)\n\t\tdefer ww.Flush()\n\t\tfor {\n\t\t\t// have to check error to detect client aborting download\n\t\t\tif _, err := ww.Write([]byte{0x41}); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\tmux.HandleFunc(\"/slow\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(200)\n\n\t\tticker := time.NewTicker(100 * time.Millisecond)\n\t\tdefer ticker.Stop()\n\n\t\ti := 0\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-r.Context().Done():\n\t\t\t\treturn\n\t\t\tcase t := <-ticker.C:\n\t\t\t\tfmt.Fprintf(w, \"%s\\n\", t)\n\t\t\t\tif flusher, ok := w.(http.Flusher); ok {\n\t\t\t\t\tflusher.Flush()\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tif i == 10 {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\treturn httptest.NewUnstartedServer(mux)\n}\n\nfunc newTestServer() *httptest.Server {\n\tsrv := newUnstartedTestServer()\n\tsrv.Start()\n\treturn srv\n}\n\nvar newCollectorTests = map[string]func(*testing.T){\n\t\"UserAgent\": func(t *testing.T) {\n\t\tfor _, ua := range []string{\n\t\t\t\"foo\",\n\t\t\t\"bar\",\n\t\t} {\n\t\t\tc := NewCollector(UserAgent(ua))\n\n\t\t\tif got, want := c.UserAgent, ua; got != want {\n\t\t\t\tt.Fatalf(\"c.UserAgent = %q, want %q\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"MaxDepth\": func(t *testing.T) {\n\t\tfor _, depth := range []int{\n\t\t\t12,\n\t\t\t34,\n\t\t\t0,\n\t\t} {\n\t\t\tc := NewCollector(MaxDepth(depth))\n\n\t\t\tif got, want := c.MaxDepth, depth; got != want {\n\t\t\t\tt.Fatalf(\"c.MaxDepth = %d, want %d\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"AllowedDomains\": func(t *testing.T) {\n\t\tfor _, domains := range [][]string{\n\t\t\t{\"example.com\", \"example.net\"},\n\t\t\t{\"example.net\"},\n\t\t\t{},\n\t\t\tnil,\n\t\t} {\n\t\t\tc := NewCollector(AllowedDomains(domains...))\n\n\t\t\tif got, want := c.AllowedDomains, domains; !reflect.DeepEqual(got, want) {\n\t\t\t\tt.Fatalf(\"c.AllowedDomains = %q, want %q\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"DisallowedDomains\": func(t *testing.T) {\n\t\tfor _, domains := range [][]string{\n\t\t\t{\"example.com\", \"example.net\"},\n\t\t\t{\"example.net\"},\n\t\t\t{},\n\t\t\tnil,\n\t\t} {\n\t\t\tc := NewCollector(DisallowedDomains(domains...))\n\n\t\t\tif got, want := c.DisallowedDomains, domains; !reflect.DeepEqual(got, want) {\n\t\t\t\tt.Fatalf(\"c.DisallowedDomains = %q, want %q\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"DisallowedURLFilters\": func(t *testing.T) {\n\t\tfor _, filters := range [][]*regexp.Regexp{\n\t\t\t{regexp.MustCompile(`.*not_allowed.*`)},\n\t\t} {\n\t\t\tc := NewCollector(DisallowedURLFilters(filters...))\n\n\t\t\tif got, want := c.DisallowedURLFilters, filters; !reflect.DeepEqual(got, want) {\n\t\t\t\tt.Fatalf(\"c.DisallowedURLFilters = %v, want %v\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"URLFilters\": func(t *testing.T) {\n\t\tfor _, filters := range [][]*regexp.Regexp{\n\t\t\t{regexp.MustCompile(`\\w+`)},\n\t\t\t{regexp.MustCompile(`\\d+`)},\n\t\t\t{},\n\t\t\tnil,\n\t\t} {\n\t\t\tc := NewCollector(URLFilters(filters...))\n\n\t\t\tif got, want := c.URLFilters, filters; !reflect.DeepEqual(got, want) {\n\t\t\t\tt.Fatalf(\"c.URLFilters = %v, want %v\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"AllowURLRevisit\": func(t *testing.T) {\n\t\tc := NewCollector(AllowURLRevisit())\n\n\t\tif !c.AllowURLRevisit {\n\t\t\tt.Fatal(\"c.AllowURLRevisit = false, want true\")\n\t\t}\n\t},\n\t\"MaxBodySize\": func(t *testing.T) {\n\t\tfor _, sizeInBytes := range []int{\n\t\t\t1024 * 1024,\n\t\t\t1024,\n\t\t\t0,\n\t\t} {\n\t\t\tc := NewCollector(MaxBodySize(sizeInBytes))\n\n\t\t\tif got, want := c.MaxBodySize, sizeInBytes; got != want {\n\t\t\t\tt.Fatalf(\"c.MaxBodySize = %d, want %d\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"CacheDir\": func(t *testing.T) {\n\t\tfor _, path := range []string{\n\t\t\t\"/tmp/\",\n\t\t\t\"/var/cache/\",\n\t\t} {\n\t\t\tc := NewCollector(CacheDir(path))\n\n\t\t\tif got, want := c.CacheDir, path; got != want {\n\t\t\t\tt.Fatalf(\"c.CacheDir = %q, want %q\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"IgnoreRobotsTxt\": func(t *testing.T) {\n\t\tc := NewCollector(IgnoreRobotsTxt())\n\n\t\tif !c.IgnoreRobotsTxt {\n\t\t\tt.Fatal(\"c.IgnoreRobotsTxt = false, want true\")\n\t\t}\n\t},\n\t\"ID\": func(t *testing.T) {\n\t\tfor _, id := range []uint32{\n\t\t\t0,\n\t\t\t1,\n\t\t\t2,\n\t\t} {\n\t\t\tc := NewCollector(ID(id))\n\n\t\t\tif got, want := c.ID, id; got != want {\n\t\t\t\tt.Fatalf(\"c.ID = %d, want %d\", got, want)\n\t\t\t}\n\t\t}\n\t},\n\t\"DetectCharset\": func(t *testing.T) {\n\t\tc := NewCollector(DetectCharset())\n\n\t\tif !c.DetectCharset {\n\t\t\tt.Fatal(\"c.DetectCharset = false, want true\")\n\t\t}\n\t},\n\t\"Debugger\": func(t *testing.T) {\n\t\td := &debug.LogDebugger{}\n\t\tc := NewCollector(Debugger(d))\n\n\t\tif got, want := c.debugger, d; got != want {\n\t\t\tt.Fatalf(\"c.debugger = %v, want %v\", got, want)\n\t\t}\n\t},\n\t\"CheckHead\": func(t *testing.T) {\n\t\tc := NewCollector(CheckHead())\n\n\t\tif !c.CheckHead {\n\t\t\tt.Fatal(\"c.CheckHead = false, want true\")\n\t\t}\n\t},\n\t\"Async\": func(t *testing.T) {\n\t\tc := NewCollector(Async())\n\n\t\tif !c.Async {\n\t\t\tt.Fatal(\"c.Async = false, want true\")\n\t\t}\n\t},\n}\n\nfunc TestNoAcceptHeader(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvar receivedHeader string\n\t// checks if Accept is enabled by default\n\tfunc() {\n\t\tc := NewCollector()\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedHeader = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/accept_header\")\n\t\tif receivedHeader != \"*/*\" {\n\t\t\tt.Errorf(\"default Accept header isn't */*. got: %v\", receivedHeader)\n\t\t}\n\t}()\n\n\t// checks if Accept can be disabled\n\tfunc() {\n\t\tc := NewCollector()\n\t\tc.OnRequest(func(r *Request) {\n\t\t\tr.Headers.Del(\"Accept\")\n\t\t})\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedHeader = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/accept_header\")\n\t\tif receivedHeader != \"\" {\n\t\t\tt.Errorf(\"failed to pass request with no Accept header. got: %v\", receivedHeader)\n\t\t}\n\t}()\n}\n\nfunc TestNewCollector(t *testing.T) {\n\tt.Run(\"Functional Options\", func(t *testing.T) {\n\t\tfor name, test := range newCollectorTests {\n\t\t\tt.Run(name, test)\n\t\t}\n\t})\n}\n\nfunc TestCollectorVisit(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\tonRequestCalled := false\n\tonResponseCalled := false\n\tonScrapedCalled := false\n\n\tc.OnRequest(func(r *Request) {\n\t\tonRequestCalled = true\n\t\tr.Ctx.Put(\"x\", \"y\")\n\t})\n\n\tc.OnResponse(func(r *Response) {\n\t\tonResponseCalled = true\n\n\t\tif r.Ctx.Get(\"x\") != \"y\" {\n\t\t\tt.Error(\"Failed to retrieve context value for key 'x'\")\n\t\t}\n\n\t\tif !bytes.Equal(r.Body, serverIndexResponse) {\n\t\t\tt.Error(\"Response body does not match with the original content\")\n\t\t}\n\t})\n\n\tc.OnScraped(func(r *Response) {\n\t\tif !onResponseCalled {\n\t\t\tt.Error(\"OnScraped called before OnResponse\")\n\t\t}\n\n\t\tif !onRequestCalled {\n\t\t\tt.Error(\"OnScraped called before OnRequest\")\n\t\t}\n\n\t\tonScrapedCalled = true\n\t})\n\n\tc.Visit(ts.URL)\n\n\tif !onRequestCalled {\n\t\tt.Error(\"Failed to call OnRequest callback\")\n\t}\n\n\tif !onResponseCalled {\n\t\tt.Error(\"Failed to call OnResponse callback\")\n\t}\n\n\tif !onScrapedCalled {\n\t\tt.Error(\"Failed to call OnScraped callback\")\n\t}\n}\n\nfunc TestCollectorVisitWithAllowedDomains(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector(AllowedDomains(\"localhost\", \"127.0.0.1\", \"::1\"))\n\terr := c.Visit(ts.URL)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to visit url %s\", ts.URL)\n\t}\n\n\terr = c.Visit(\"http://example.com\")\n\tif err != ErrForbiddenDomain {\n\t\tt.Errorf(\"c.Visit should return ErrForbiddenDomain, but got %v\", err)\n\t}\n}\n\nfunc TestCollectorVisitWithDisallowedDomains(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector(DisallowedDomains(\"localhost\", \"127.0.0.1\", \"::1\"))\n\terr := c.Visit(ts.URL)\n\tif err != ErrForbiddenDomain {\n\t\tt.Errorf(\"c.Visit should return ErrForbiddenDomain, but got %v\", err)\n\t}\n\n\tc2 := NewCollector(DisallowedDomains(\"example.com\"))\n\terr = c2.Visit(\"http://example.com:8080\")\n\tif err != ErrForbiddenDomain {\n\t\tt.Errorf(\"c.Visit should return ErrForbiddenDomain, but got %v\", err)\n\t}\n\terr = c2.Visit(ts.URL)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to visit url %s\", ts.URL)\n\t}\n}\n\nfunc TestCollectorVisitResponseHeaders(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvar onResponseHeadersCalled bool\n\n\tc := NewCollector()\n\tc.OnResponseHeaders(func(r *Response) {\n\t\tonResponseHeadersCalled = true\n\t\tif r.Headers.Get(\"Content-Type\") == \"application/octet-stream\" {\n\t\t\tr.Request.Abort()\n\t\t}\n\t})\n\tc.OnResponse(func(r *Response) {\n\t\tt.Error(\"OnResponse was called\")\n\t})\n\tc.Visit(ts.URL + \"/large_binary\")\n\tif !onResponseHeadersCalled {\n\t\tt.Error(\"OnResponseHeaders was not called\")\n\t}\n}\n\nfunc TestCollectorOnHTML(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\ttitleCallbackCalled := false\n\tparagraphCallbackCount := 0\n\n\tc.OnHTML(\"title\", func(e *HTMLElement) {\n\t\ttitleCallbackCalled = true\n\t\tif e.Text != \"Test Page\" {\n\t\t\tt.Error(\"Title element text does not match, got\", e.Text)\n\t\t}\n\t})\n\n\tc.OnHTML(\"p\", func(e *HTMLElement) {\n\t\tparagraphCallbackCount++\n\t\tif e.Attr(\"class\") != \"description\" {\n\t\t\tt.Error(\"Failed to get paragraph's class attribute\")\n\t\t}\n\t})\n\n\tc.OnHTML(\"body\", func(e *HTMLElement) {\n\t\tif e.ChildAttr(\"p\", \"class\") != \"description\" {\n\t\t\tt.Error(\"Invalid class value\")\n\t\t}\n\t\tclasses := e.ChildAttrs(\"p\", \"class\")\n\t\tif len(classes) != 2 {\n\t\t\tt.Error(\"Invalid class values\")\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/html\")\n\n\tif !titleCallbackCalled {\n\t\tt.Error(\"Failed to call OnHTML callback for <title> tag\")\n\t}\n\n\tif paragraphCallbackCount != 2 {\n\t\tt.Error(\"Failed to find all <p> tags\")\n\t}\n}\n\nfunc TestCollectorContentSniffing(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\thtmlCallbackCalled := false\n\n\tc.OnResponse(func(r *Response) {\n\t\tif (*r.Headers)[\"Content-Type\"] != nil {\n\t\t\tt.Error(\"Content-Type unexpectedly not nil\")\n\t\t}\n\t})\n\n\tc.OnHTML(\"html\", func(e *HTMLElement) {\n\t\thtmlCallbackCalled = true\n\t})\n\n\terr := c.Visit(ts.URL + \"/html?no-content-type=yes\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !htmlCallbackCalled {\n\t\tt.Error(\"OnHTML was not called\")\n\t}\n}\n\nfunc TestCollectorURLRevisit(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\tvisitCount := 0\n\n\tc.OnRequest(func(r *Request) {\n\t\tvisitCount++\n\t})\n\n\tc.Visit(ts.URL)\n\tc.Visit(ts.URL)\n\n\tif visitCount != 1 {\n\t\tt.Error(\"URL revisited\")\n\t}\n\n\tc.AllowURLRevisit = true\n\n\tc.Visit(ts.URL)\n\tc.Visit(ts.URL)\n\n\tif visitCount != 3 {\n\t\tt.Error(\"URL not revisited\")\n\t}\n}\n\nfunc TestCollectorPostRevisit(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tpostValue := \"hello\"\n\tpostData := map[string]string{\n\t\t\"name\": postValue,\n\t}\n\tvisitCount := 0\n\n\tc := NewCollector()\n\tc.OnResponse(func(r *Response) {\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST\")\n\t\t}\n\t\tvisitCount++\n\t})\n\n\tc.Post(ts.URL+\"/login\", postData)\n\tc.Post(ts.URL+\"/login\", postData)\n\tc.Post(ts.URL+\"/login\", map[string]string{\n\t\t\"name\":     postValue,\n\t\t\"lastname\": \"world\",\n\t})\n\n\tif visitCount != 2 {\n\t\tt.Error(\"URL POST revisited\")\n\t}\n\n\tc.AllowURLRevisit = true\n\n\tc.Post(ts.URL+\"/login\", postData)\n\tc.Post(ts.URL+\"/login\", postData)\n\n\tif visitCount != 4 {\n\t\tt.Error(\"URL POST not revisited\")\n\t}\n}\n\nfunc TestCollectorURLRevisitCheck(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\tvisited, err := c.HasVisited(ts.URL)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif visited != false {\n\t\tt.Error(\"Expected URL to NOT have been visited\")\n\t}\n\n\tc.Visit(ts.URL)\n\n\tvisited, err = c.HasVisited(ts.URL)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif visited != true {\n\t\tt.Error(\"Expected URL to have been visited\")\n\t}\n\n\terrorTestCases := []struct {\n\t\tPath             string\n\t\tDestinationError string\n\t}{\n\t\t{\"/\", \"/\"},\n\t\t{\"/redirect?d=/\", \"/\"},\n\t\t// now that /redirect?d=/ itself is recorded as visited,\n\t\t// it's now returned in error\n\t\t{\"/redirect?d=/\", \"/redirect?d=/\"},\n\t\t{\"/redirect?d=/redirect%3Fd%3D/\", \"/redirect?d=/\"},\n\t\t{\"/redirect?d=/redirect%3Fd%3D/\", \"/redirect?d=/redirect%3Fd%3D/\"},\n\t\t{\"/redirect?d=/redirect%3Fd%3D/&foo=bar\", \"/redirect?d=/\"},\n\t}\n\n\tfor i, testCase := range errorTestCases {\n\t\terr := c.Visit(ts.URL + testCase.Path)\n\t\tif testCase.DestinationError == \"\" {\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"got unexpected error in test %d: %q\", i, err)\n\t\t\t}\n\t\t} else {\n\t\t\tvar ave *AlreadyVisitedError\n\t\t\tif !errors.As(err, &ave) {\n\t\t\t\tt.Errorf(\"err=%q returned when trying to revisit, expected AlreadyVisitedError\", err)\n\t\t\t} else {\n\t\t\t\tif got, want := ave.Destination.String(), ts.URL+testCase.DestinationError; got != want {\n\t\t\t\t\tt.Errorf(\"wrong destination in AlreadyVisitedError in test %d, got=%q want=%q\", i, got, want)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestSetCookieRedirect(t *testing.T) {\n\ttype middleware = func(http.Handler) http.Handler\n\tfor _, m := range []middleware{\n\t\trequireSessionCookieSimple,\n\t\trequireSessionCookieAuthPage,\n\t} {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tts := newUnstartedTestServer()\n\t\t\tts.Config.Handler = m(ts.Config.Handler)\n\t\t\tts.Start()\n\t\t\tdefer ts.Close()\n\t\t\tc := NewCollector()\n\t\t\tc.OnResponse(func(r *Response) {\n\t\t\t\tif got, want := r.Body, serverIndexResponse; !bytes.Equal(got, want) {\n\t\t\t\t\tt.Errorf(\"bad response body got=%q want=%q\", got, want)\n\t\t\t\t}\n\t\t\t\tif got, want := r.StatusCode, http.StatusOK; got != want {\n\t\t\t\t\tt.Errorf(\"bad response code got=%d want=%d\", got, want)\n\t\t\t\t}\n\t\t\t})\n\t\t\tif err := c.Visit(ts.URL); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestCollectorPostURLRevisitCheck(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\tpostValue := \"hello\"\n\tpostData := map[string]string{\n\t\t\"name\": postValue,\n\t}\n\n\tposted, err := c.HasPosted(ts.URL+\"/login\", postData)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif posted != false {\n\t\tt.Error(\"Expected URL to NOT have been visited\")\n\t}\n\n\tc.Post(ts.URL+\"/login\", postData)\n\n\tposted, err = c.HasPosted(ts.URL+\"/login\", postData)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif posted != true {\n\t\tt.Error(\"Expected URL to have been visited\")\n\t}\n\n\tpostData[\"lastname\"] = \"world\"\n\tposted, err = c.HasPosted(ts.URL+\"/login\", postData)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif posted != false {\n\t\tt.Error(\"Expected URL to NOT have been visited\")\n\t}\n\n\tc.Post(ts.URL+\"/login\", postData)\n\n\tposted, err = c.HasPosted(ts.URL+\"/login\", postData)\n\n\tif err != nil {\n\t\tt.Error(err.Error())\n\t}\n\n\tif posted != true {\n\t\tt.Error(\"Expected URL to have been visited\")\n\t}\n}\n\n// TestCollectorURLRevisitDisallowed ensures that disallowed URL is not considered visited.\nfunc TestCollectorURLRevisitDomainDisallowed(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tparsedURL, err := url.Parse(ts.URL)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tc := NewCollector(DisallowedDomains(parsedURL.Hostname()))\n\terr = c.Visit(ts.URL)\n\tif got, want := err, ErrForbiddenDomain; got != want {\n\t\tt.Fatalf(\"wrong error on first visit: got=%v want=%v\", got, want)\n\t}\n\terr = c.Visit(ts.URL)\n\tif got, want := err, ErrForbiddenDomain; got != want {\n\t\tt.Fatalf(\"wrong error on second visit: got=%v want=%v\", got, want)\n\t}\n\n}\n\nfunc TestCollectorPost(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tpostValue := \"hello\"\n\tc := NewCollector()\n\n\tc.OnResponse(func(r *Response) {\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST\")\n\t\t}\n\t})\n\n\tc.Post(ts.URL+\"/login\", map[string]string{\n\t\t\"name\": postValue,\n\t})\n}\n\nfunc TestCollectorPostRaw(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tpostValue := \"hello\"\n\tc := NewCollector()\n\n\tc.OnResponse(func(r *Response) {\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST\")\n\t\t}\n\t})\n\n\tc.PostRaw(ts.URL+\"/login\", []byte(\"name=\"+postValue))\n}\n\nfunc TestCollectorPostRawRevisit(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tpostValue := \"hello\"\n\tpostData := \"name=\" + postValue\n\tvisitCount := 0\n\n\tc := NewCollector()\n\tc.OnResponse(func(r *Response) {\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST RAW\")\n\t\t}\n\t\tvisitCount++\n\t})\n\n\tc.PostRaw(ts.URL+\"/login\", []byte(postData))\n\tc.PostRaw(ts.URL+\"/login\", []byte(postData))\n\tc.PostRaw(ts.URL+\"/login\", []byte(postData+\"&lastname=world\"))\n\n\tif visitCount != 2 {\n\t\tt.Error(\"URL POST RAW revisited\")\n\t}\n\n\tc.AllowURLRevisit = true\n\n\tc.PostRaw(ts.URL+\"/login\", []byte(postData))\n\tc.PostRaw(ts.URL+\"/login\", []byte(postData))\n\n\tif visitCount != 4 {\n\t\tt.Error(\"URL POST RAW not revisited\")\n\t}\n}\n\nfunc TestRedirect(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.OnHTML(\"a[href]\", func(e *HTMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\tif !strings.HasSuffix(u, \"/redirected/test\") {\n\t\t\tt.Error(\"Invalid URL after redirect: \" + u)\n\t\t}\n\t})\n\n\tc.OnResponseHeaders(func(r *Response) {\n\t\tif !strings.HasSuffix(r.Request.URL.String(), \"/redirected/\") {\n\t\t\tt.Error(\"Invalid URL in Request after redirect (OnResponseHeaders): \" + r.Request.URL.String())\n\t\t}\n\t})\n\n\tc.OnResponse(func(r *Response) {\n\t\tif !strings.HasSuffix(r.Request.URL.String(), \"/redirected/\") {\n\t\t\tt.Error(\"Invalid URL in Request after redirect (OnResponse): \" + r.Request.URL.String())\n\t\t}\n\t})\n\tc.Visit(ts.URL + \"/redirect\")\n}\n\nfunc TestIssue594(t *testing.T) {\n\t// This is a regression test for a data race bug. There's no\n\t// assertions because it's meant to be used with race detector\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\t// if timeout is set, this bug is not triggered\n\tc.SetClient(&http.Client{Timeout: 0 * time.Second})\n\n\tc.Visit(ts.URL)\n}\n\nfunc TestRedirectWithDisallowedURLs(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.DisallowedURLFilters = []*regexp.Regexp{regexp.MustCompile(ts.URL + \"/redirected/test\")}\n\tc.OnHTML(\"a[href]\", func(e *HTMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\terr := c.Visit(u)\n\t\tif !errors.Is(err, ErrForbiddenURL) {\n\t\t\tt.Error(\"URL should have been forbidden: \" + u)\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/redirect\")\n}\n\nfunc TestBaseTag(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.OnHTML(\"a[href]\", func(e *HTMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\tif u != \"http://xy.com/z\" {\n\t\t\tt.Error(\"Invalid <base /> tag handling in OnHTML: expected https://xy.com/z, got \" + u)\n\t\t}\n\t})\n\tc.Visit(ts.URL + \"/base\")\n\n\tc2 := NewCollector()\n\tc2.OnXML(\"//a\", func(e *XMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\tif u != \"http://xy.com/z\" {\n\t\t\tt.Error(\"Invalid <base /> tag handling in OnXML: expected https://xy.com/z, got \" + u)\n\t\t}\n\t})\n\tc2.Visit(ts.URL + \"/base\")\n}\n\nfunc TestBaseTagRelative(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.OnHTML(\"a[href]\", func(e *HTMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\texpected := ts.URL + \"/foobar/z\"\n\t\tif u != expected {\n\t\t\tt.Errorf(\"Invalid <base /> tag handling in OnHTML: expected %q, got %q\", expected, u)\n\t\t}\n\t})\n\tc.Visit(ts.URL + \"/base_relative\")\n\n\tc2 := NewCollector()\n\tc2.OnXML(\"//a\", func(e *XMLElement) {\n\t\tu := e.Request.AbsoluteURL(e.Attr(\"href\"))\n\t\texpected := ts.URL + \"/foobar/z\"\n\t\tif u != expected {\n\t\t\tt.Errorf(\"Invalid <base /> tag handling in OnXML: expected %q, got %q\", expected, u)\n\t\t}\n\t})\n\tc2.Visit(ts.URL + \"/base_relative\")\n}\n\nfunc TestTabsAndNewlines(t *testing.T) {\n\t// this test might look odd, but see step 3 of\n\t// https://url.spec.whatwg.org/#concept-basic-url-parser\n\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvisited := map[string]struct{}{}\n\texpected := map[string]struct{}{\n\t\t\"/tabs_and_newlines\": {},\n\t\t\"/foobar/xy\":         {},\n\t}\n\n\tc := NewCollector()\n\tc.OnResponse(func(res *Response) {\n\t\tvisited[res.Request.URL.EscapedPath()] = struct{}{}\n\t})\n\tc.OnHTML(\"a[href]\", func(e *HTMLElement) {\n\t\tif err := e.Request.Visit(e.Attr(\"href\")); err != nil {\n\t\t\tt.Errorf(\"visit failed: %v\", err)\n\t\t}\n\t})\n\n\tif err := c.Visit(ts.URL + \"/tabs_and_newlines\"); err != nil {\n\t\tt.Errorf(\"visit failed: %v\", err)\n\t}\n\n\tif !reflect.DeepEqual(visited, expected) {\n\t\tt.Errorf(\"visited=%v expected=%v\", visited, expected)\n\t}\n}\n\nfunc TestLonePercent(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvar visitedPath string\n\n\tc := NewCollector()\n\tc.OnResponse(func(res *Response) {\n\t\tvisitedPath = res.Request.URL.RequestURI()\n\t})\n\tif err := c.Visit(ts.URL + \"/100%\"); err != nil {\n\t\tt.Errorf(\"visit failed: %v\", err)\n\t}\n\t// Automatic encoding is not really correct: browsers\n\t// would send bare percent here. However, Go net/http\n\t// cannot send such requests due to\n\t// https://github.com/golang/go/issues/29808. So we have two\n\t// alternatives really: return an error when attempting\n\t// to fetch such URLs, or at least try the encoded variant.\n\t// This test checks that the latter is attempted.\n\tif got, want := visitedPath, \"/100%25\"; got != want {\n\t\tt.Errorf(\"got=%q want=%q\", got, want)\n\t}\n\t// invalid URL escape in query component is not a problem,\n\t// but check it anyway\n\tif err := c.Visit(ts.URL + \"/?a=100%zz\"); err != nil {\n\t\tt.Errorf(\"visit failed: %v\", err)\n\t}\n\tif got, want := visitedPath, \"/?a=100%zz\"; got != want {\n\t\tt.Errorf(\"got=%q want=%q\", got, want)\n\t}\n}\n\nfunc TestCollectorCookies(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\tif err := c.Visit(ts.URL + \"/set_cookie\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := c.Visit(ts.URL + \"/check_cookie\"); err != nil {\n\t\tt.Fatalf(\"Failed to use previously set cookies: %s\", err)\n\t}\n}\n\nfunc TestRobotsWhenAllowed(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.IgnoreRobotsTxt = false\n\n\tc.OnResponse(func(resp *Response) {\n\t\tif resp.StatusCode != 200 {\n\t\t\tt.Fatalf(\"Wrong response code: %d\", resp.StatusCode)\n\t\t}\n\t})\n\n\terr := c.Visit(ts.URL + \"/allowed\")\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestRobotsWhenDisallowed(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.IgnoreRobotsTxt = false\n\n\tc.OnResponse(func(resp *Response) {\n\t\tt.Fatalf(\"Received response: %d\", resp.StatusCode)\n\t})\n\n\terr := c.Visit(ts.URL + \"/disallowed\")\n\tif err.Error() != \"URL blocked by robots.txt\" {\n\t\tt.Fatalf(\"wrong error message: %v\", err)\n\t}\n}\n\nfunc TestRobotsWhenDisallowedWithQueryParameter(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.IgnoreRobotsTxt = false\n\n\tc.OnResponse(func(resp *Response) {\n\t\tt.Fatalf(\"Received response: %d\", resp.StatusCode)\n\t})\n\n\terr := c.Visit(ts.URL + \"/allowed?q=1\")\n\tif err.Error() != \"URL blocked by robots.txt\" {\n\t\tt.Fatalf(\"wrong error message: %v\", err)\n\t}\n}\n\nfunc TestIgnoreRobotsWhenDisallowed(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.IgnoreRobotsTxt = true\n\n\tc.OnResponse(func(resp *Response) {\n\t\tif resp.StatusCode != 200 {\n\t\t\tt.Fatalf(\"Wrong response code: %d\", resp.StatusCode)\n\t\t}\n\t})\n\n\terr := c.Visit(ts.URL + \"/disallowed\")\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n}\n\nfunc TestConnectionErrorOnRobotsTxtResultsInError(t *testing.T) {\n\tts := newTestServer()\n\tts.Close() // immediately close the server to force a connection error\n\n\tc := NewCollector()\n\tc.IgnoreRobotsTxt = false\n\terr := c.Visit(ts.URL)\n\n\tif err == nil {\n\t\tt.Fatal(\"Error expected\")\n\t}\n}\n\nfunc TestEnvSettings(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tos.Setenv(\"COLLY_USER_AGENT\", \"test\")\n\tdefer os.Unsetenv(\"COLLY_USER_AGENT\")\n\n\tc := NewCollector()\n\n\tvalid := false\n\n\tc.OnResponse(func(resp *Response) {\n\t\tif string(resp.Body) == \"test\" {\n\t\t\tvalid = true\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/user_agent\")\n\n\tif !valid {\n\t\tt.Fatalf(\"Wrong user-agent from environment\")\n\t}\n}\n\nfunc TestUserAgent(t *testing.T) {\n\tconst exampleUserAgent1 = \"Example/1.0\"\n\tconst exampleUserAgent2 = \"Example/2.0\"\n\tconst defaultUserAgent = \"colly - https://github.com/gocolly/colly/v2\"\n\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvar receivedUserAgent string\n\n\tfunc() {\n\t\tc := NewCollector()\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/user_agent\")\n\t\tif got, want := receivedUserAgent, defaultUserAgent; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent: got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(UserAgent(exampleUserAgent1))\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/user_agent\")\n\t\tif got, want := receivedUserAgent, exampleUserAgent1; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent: got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(UserAgent(exampleUserAgent1))\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\n\t\tc.Request(\"GET\", ts.URL+\"/user_agent\", nil, nil, nil)\n\t\tif got, want := receivedUserAgent, exampleUserAgent1; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent (nil hdr): got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(UserAgent(exampleUserAgent1))\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\n\t\tc.Request(\"GET\", ts.URL+\"/user_agent\", nil, nil, http.Header{})\n\t\tif got, want := receivedUserAgent, exampleUserAgent1; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent (non-nil hdr): got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(UserAgent(exampleUserAgent1))\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\t\thdr := http.Header{}\n\t\thdr.Set(\"User-Agent\", \"\")\n\n\t\tc.Request(\"GET\", ts.URL+\"/user_agent\", nil, nil, hdr)\n\t\tif got, want := receivedUserAgent, \"\"; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent (hdr with empty UA): got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(UserAgent(exampleUserAgent1))\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedUserAgent = string(resp.Body)\n\t\t})\n\t\thdr := http.Header{}\n\t\thdr.Set(\"User-Agent\", exampleUserAgent2)\n\n\t\tc.Request(\"GET\", ts.URL+\"/user_agent\", nil, nil, hdr)\n\t\tif got, want := receivedUserAgent, exampleUserAgent2; got != want {\n\t\t\tt.Errorf(\"mismatched User-Agent (hdr with UA): got=%q want=%q\", got, want)\n\t\t}\n\t}()\n}\n\nfunc TestHeaders(t *testing.T) {\n\tconst exampleHostHeader = \"example.com\"\n\tconst exampleTestHeader = \"Testing\"\n\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tvar receivedHeader string\n\n\tfunc() {\n\t\tc := NewCollector(\n\t\t\tHeaders(map[string]string{\"Host\": exampleHostHeader}),\n\t\t)\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedHeader = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/host_header\")\n\t\tif got, want := receivedHeader, exampleHostHeader; got != want {\n\t\t\tt.Errorf(\"mismatched Host header: got=%q want=%q\", got, want)\n\t\t}\n\t}()\n\tfunc() {\n\t\tc := NewCollector(\n\t\t\tHeaders(map[string]string{\"Test\": exampleTestHeader}),\n\t\t)\n\t\tc.OnResponse(func(resp *Response) {\n\t\t\treceivedHeader = string(resp.Body)\n\t\t})\n\t\tc.Visit(ts.URL + \"/custom_header\")\n\t\tif got, want := receivedHeader, exampleTestHeader; got != want {\n\t\t\tt.Errorf(\"mismatched custom header: got=%q want=%q\", got, want)\n\t\t}\n\t}()\n}\n\nfunc TestParseHTTPErrorResponse(t *testing.T) {\n\tcontentCount := 0\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector(\n\t\tAllowURLRevisit(),\n\t)\n\n\tc.OnHTML(\"p\", func(e *HTMLElement) {\n\t\tif e.Text == \"error\" {\n\t\t\tcontentCount++\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/500\")\n\n\tif contentCount != 0 {\n\t\tt.Fatal(\"Content is parsed without ParseHTTPErrorResponse enabled\")\n\t}\n\n\tc.ParseHTTPErrorResponse = true\n\n\tc.Visit(ts.URL + \"/500\")\n\n\tif contentCount != 1 {\n\t\tt.Fatal(\"Content isn't parsed with ParseHTTPErrorResponse enabled\")\n\t}\n\n}\n\nfunc TestHTMLElement(t *testing.T) {\n\tctx := &Context{}\n\tresp := &Response{\n\t\tRequest: &Request{\n\t\t\tCtx: ctx,\n\t\t},\n\t\tCtx: ctx,\n\t}\n\n\tin := `<a href=\"http://go-colly.org\">Colly</a>`\n\tsel := \"a[href]\"\n\tdoc, err := goquery.NewDocumentFromReader(bytes.NewBuffer([]byte(in)))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\telements := []*HTMLElement{}\n\ti := 0\n\tdoc.Find(sel).Each(func(_ int, s *goquery.Selection) {\n\t\tfor _, n := range s.Nodes {\n\t\t\telements = append(elements, NewHTMLElementFromSelectionNode(resp, s, n, i))\n\t\t\ti++\n\t\t}\n\t})\n\telementsLen := len(elements)\n\tif elementsLen != 1 {\n\t\tt.Errorf(\"element length mismatch. got %d, expected %d.\\n\", elementsLen, 1)\n\t}\n\tv := elements[0]\n\tif v.Name != \"a\" {\n\t\tt.Errorf(\"element tag mismatch. got %s, expected %s.\\n\", v.Name, \"a\")\n\t}\n\tif v.Text != \"Colly\" {\n\t\tt.Errorf(\"element content mismatch. got %s, expected %s.\\n\", v.Text, \"Colly\")\n\t}\n\tif v.Attr(\"href\") != \"http://go-colly.org\" {\n\t\tt.Errorf(\"element href mismatch. got %s, expected %s.\\n\", v.Attr(\"href\"), \"http://go-colly.org\")\n\t}\n}\n\nfunc TestCollectorOnXMLWithHtml(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\ttitleCallbackCalled := false\n\tparagraphCallbackCount := 0\n\n\tc.OnXML(\"/html/head/title\", func(e *XMLElement) {\n\t\ttitleCallbackCalled = true\n\t\tif e.Text != \"Test Page\" {\n\t\t\tt.Error(\"Title element text does not match, got\", e.Text)\n\t\t}\n\t})\n\n\tc.OnXML(\"/html/body/p\", func(e *XMLElement) {\n\t\tparagraphCallbackCount++\n\t\tif e.Attr(\"class\") != \"description\" {\n\t\t\tt.Error(\"Failed to get paragraph's class attribute\")\n\t\t}\n\t})\n\n\tc.OnXML(\"/html/body\", func(e *XMLElement) {\n\t\tif e.ChildAttr(\"p\", \"class\") != \"description\" {\n\t\t\tt.Error(\"Invalid class value\")\n\t\t}\n\t\tclasses := e.ChildAttrs(\"p\", \"class\")\n\t\tif len(classes) != 2 {\n\t\t\tt.Error(\"Invalid class values\")\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/html\")\n\n\tif !titleCallbackCalled {\n\t\tt.Error(\"Failed to call OnXML callback for <title> tag\")\n\t}\n\n\tif paragraphCallbackCount != 2 {\n\t\tt.Error(\"Failed to find all <p> tags\")\n\t}\n}\n\nfunc TestCollectorOnXMLWithXML(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\n\ttitleCallbackCalled := false\n\tparagraphCallbackCount := 0\n\n\tc.OnXML(\"//page/title\", func(e *XMLElement) {\n\t\ttitleCallbackCalled = true\n\t\tif e.Text != \"Test Page\" {\n\t\t\tt.Error(\"Title element text does not match, got\", e.Text)\n\t\t}\n\t})\n\n\tc.OnXML(\"//page/paragraph\", func(e *XMLElement) {\n\t\tparagraphCallbackCount++\n\t\tif e.Attr(\"type\") != \"description\" {\n\t\t\tt.Error(\"Failed to get paragraph's type attribute\")\n\t\t}\n\t})\n\n\tc.OnXML(\"/page\", func(e *XMLElement) {\n\t\tif e.ChildAttr(\"paragraph\", \"type\") != \"description\" {\n\t\t\tt.Error(\"Invalid type value\")\n\t\t}\n\t\tclasses := e.ChildAttrs(\"paragraph\", \"type\")\n\t\tif len(classes) != 2 {\n\t\t\tt.Error(\"Invalid type values\")\n\t\t}\n\t})\n\n\tc.Visit(ts.URL + \"/xml\")\n\n\tif !titleCallbackCalled {\n\t\tt.Error(\"Failed to call OnXML callback for <title> tag\")\n\t}\n\n\tif paragraphCallbackCount != 2 {\n\t\tt.Error(\"Failed to find all <paragraph> tags\")\n\t}\n}\n\nfunc TestCollectorVisitWithTrace(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector(AllowedDomains(\"localhost\", \"127.0.0.1\", \"::1\"), TraceHTTP())\n\tc.OnResponse(func(resp *Response) {\n\t\tif resp.Trace == nil {\n\t\t\tt.Error(\"Failed to initialize trace\")\n\t\t}\n\t})\n\n\terr := c.Visit(ts.URL)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to visit url %s\", ts.URL)\n\t}\n}\n\nfunc TestCollectorVisitWithCheckHead(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector(CheckHead())\n\tvar requestMethodChain []string\n\tc.OnResponse(func(resp *Response) {\n\t\trequestMethodChain = append(requestMethodChain, resp.Request.Method)\n\t})\n\n\terr := c.Visit(ts.URL)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to visit url %s\", ts.URL)\n\t}\n\tif requestMethodChain[0] != \"HEAD\" && requestMethodChain[1] != \"GET\" {\n\t\tt.Errorf(\"Failed to perform a HEAD request before GET\")\n\t}\n}\n\nfunc TestCollectorDepth(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\tmaxDepth := 2\n\tc1 := NewCollector(\n\t\tMaxDepth(maxDepth),\n\t\tAllowURLRevisit(),\n\t)\n\trequestCount := 0\n\tc1.OnResponse(func(resp *Response) {\n\t\trequestCount++\n\t\tif requestCount >= 10 {\n\t\t\treturn\n\t\t}\n\t\tc1.Visit(ts.URL)\n\t})\n\tc1.Visit(ts.URL)\n\tif requestCount < 10 {\n\t\tt.Errorf(\"Invalid number of requests: %d (expected 10) without using MaxDepth\", requestCount)\n\t}\n\n\tc2 := c1.Clone()\n\trequestCount = 0\n\tc2.OnResponse(func(resp *Response) {\n\t\trequestCount++\n\t\tresp.Request.Visit(ts.URL)\n\t})\n\tc2.Visit(ts.URL)\n\tif requestCount != 2 {\n\t\tt.Errorf(\"Invalid number of requests: %d (expected 2) with using MaxDepth 2\", requestCount)\n\t}\n\n\tc1.Visit(ts.URL)\n\tif requestCount < 10 {\n\t\tt.Errorf(\"Invalid number of requests: %d (expected 10) without using MaxDepth again\", requestCount)\n\t}\n\n\trequestCount = 0\n\tc2.Visit(ts.URL)\n\tif requestCount != 2 {\n\t\tt.Errorf(\"Invalid number of requests: %d (expected 2) with using MaxDepth 2 again\", requestCount)\n\t}\n}\n\nfunc TestCollectorRequests(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\tmaxRequests := uint32(5)\n\tc1 := NewCollector(\n\t\tMaxRequests(maxRequests),\n\t\tAllowURLRevisit(),\n\t)\n\trequestCount := 0\n\tc1.OnResponse(func(resp *Response) {\n\t\trequestCount++\n\t\tc1.Visit(ts.URL)\n\t})\n\tc1.Visit(ts.URL)\n\tif requestCount != 5 {\n\t\tt.Errorf(\"Invalid number of requests: %d (expected 5) with MaxRequests\", requestCount)\n\t}\n}\n\nfunc TestCollectorContext(t *testing.T) {\n\t// \"/slow\" takes 1 second to return the response.\n\t// If context does abort the transfer after 0.5 seconds as it should,\n\t// OnError will be called, and the test is passed. Otherwise, test is failed.\n\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond)\n\tdefer cancel()\n\n\tc := NewCollector(StdlibContext(ctx))\n\n\tonErrorCalled := false\n\n\tc.OnResponse(func(resp *Response) {\n\t\tt.Error(\"OnResponse was called, expected OnError\")\n\t})\n\n\tc.OnError(func(resp *Response, err error) {\n\t\tonErrorCalled = true\n\t\tif err != context.DeadlineExceeded {\n\t\t\tt.Errorf(\"OnError got err=%#v, expected context.DeadlineExceeded\", err)\n\t\t}\n\t})\n\n\terr := c.Visit(ts.URL + \"/slow\")\n\tif err != context.DeadlineExceeded {\n\t\tt.Errorf(\"Visit return err=%#v, expected context.DeadlineExceeded\", err)\n\t}\n\n\tif !onErrorCalled {\n\t\tt.Error(\"OnError was not called\")\n\t}\n\n}\n\nfunc BenchmarkOnHTML(b *testing.B) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.OnHTML(\"p\", func(_ *HTMLElement) {})\n\n\tfor n := 0; n < b.N; n++ {\n\t\tc.Visit(fmt.Sprintf(\"%s/html?q=%d\", ts.URL, n))\n\t}\n}\n\nfunc BenchmarkOnXML(b *testing.B) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.OnXML(\"//p\", func(_ *XMLElement) {})\n\n\tfor n := 0; n < b.N; n++ {\n\t\tc.Visit(fmt.Sprintf(\"%s/html?q=%d\", ts.URL, n))\n\t}\n}\n\nfunc BenchmarkOnResponse(b *testing.B) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tc := NewCollector()\n\tc.AllowURLRevisit = true\n\tc.OnResponse(func(_ *Response) {})\n\n\tfor n := 0; n < b.N; n++ {\n\t\tc.Visit(ts.URL)\n\t}\n}\n\nfunc requireSessionCookieSimple(handler http.Handler) http.Handler {\n\tconst cookieName = \"session_id\"\n\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tif _, err := r.Cookie(cookieName); err == http.ErrNoCookie {\n\t\t\thttp.SetCookie(w, &http.Cookie{Name: cookieName, Value: \"1\"})\n\t\t\thttp.Redirect(w, r, r.RequestURI, http.StatusFound)\n\t\t\treturn\n\t\t}\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\nfunc requireSessionCookieAuthPage(handler http.Handler) http.Handler {\n\tconst setCookiePath = \"/auth\"\n\tconst cookieName = \"session_id\"\n\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tif r.URL.Path == setCookiePath {\n\t\t\tdestination := r.URL.Query().Get(\"return\")\n\t\t\thttp.Redirect(w, r, destination, http.StatusFound)\n\t\t\treturn\n\t\t}\n\t\tif _, err := r.Cookie(cookieName); err == http.ErrNoCookie {\n\t\t\thttp.SetCookie(w, &http.Cookie{Name: cookieName, Value: \"1\"})\n\t\t\thttp.Redirect(w, r, setCookiePath+\"?return=\"+url.QueryEscape(r.RequestURI), http.StatusFound)\n\t\t\treturn\n\t\t}\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\nfunc TestCollectorPostRetry(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\n\tpostValue := \"hello\"\n\tc := NewCollector()\n\ttry := false\n\tc.OnResponse(func(r *Response) {\n\t\tif r.Ctx.Get(\"notFirst\") == \"\" {\n\t\t\tr.Ctx.Put(\"notFirst\", \"first\")\n\t\t\t_ = r.Request.Retry()\n\t\t\treturn\n\t\t}\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST\")\n\t\t}\n\t\ttry = true\n\t})\n\n\tc.Post(ts.URL+\"/login\", map[string]string{\n\t\t\"name\": postValue,\n\t})\n\tif !try {\n\t\tt.Error(\"OnResponse Retry was not called\")\n\t}\n}\nfunc TestCollectorGetRetry(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\ttry := false\n\n\tc := NewCollector()\n\n\tc.OnResponse(func(r *Response) {\n\t\tif r.Ctx.Get(\"notFirst\") == \"\" {\n\t\t\tr.Ctx.Put(\"notFirst\", \"first\")\n\t\t\t_ = r.Request.Retry()\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(r.Body, serverIndexResponse) {\n\t\t\tt.Error(\"Response body does not match with the original content\")\n\t\t}\n\t\ttry = true\n\t})\n\n\tc.Visit(ts.URL)\n\tif !try {\n\t\tt.Error(\"OnResponse Retry was not called\")\n\t}\n}\n\nfunc TestCollectorPostRetryUnseekable(t *testing.T) {\n\tts := newTestServer()\n\tdefer ts.Close()\n\ttry := false\n\tpostValue := \"hello\"\n\tc := NewCollector()\n\n\tc.OnResponse(func(r *Response) {\n\t\tif postValue != string(r.Body) {\n\t\t\tt.Error(\"Failed to send data with POST\")\n\t\t}\n\n\t\tif r.Ctx.Get(\"notFirst\") == \"\" {\n\t\t\tr.Ctx.Put(\"notFirst\", \"first\")\n\t\t\terr := r.Request.Retry()\n\t\t\tif !errors.Is(err, ErrRetryBodyUnseekable) {\n\t\t\t\tt.Errorf(\"Unexpected error Type ErrRetryBodyUnseekable : %v\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\ttry = true\n\t})\n\tc.Request(\"POST\", ts.URL+\"/login\", bytes.NewBuffer([]byte(\"name=\"+postValue)), nil, nil)\n\tif try {\n\t\tt.Error(\"OnResponse Retry was called but BodyUnseekable\")\n\t}\n}\n"
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 2.1611328125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"sync\"\n)\n\n// Context provides a tiny layer for passing data between callbacks\ntype Context struct {\n\tcontextMap map[string]interface{}\n\tlock       *sync.RWMutex\n}\n\n// NewContext initializes a new Context instance\nfunc NewContext() *Context {\n\treturn &Context{\n\t\tcontextMap: make(map[string]interface{}),\n\t\tlock:       &sync.RWMutex{},\n\t}\n}\n\n// UnmarshalBinary decodes Context value to nil\n// This function is used by request caching\nfunc (c *Context) UnmarshalBinary(_ []byte) error {\n\treturn nil\n}\n\n// MarshalBinary encodes Context value\n// This function is used by request caching\nfunc (c *Context) MarshalBinary() (_ []byte, _ error) {\n\treturn nil, nil\n}\n\n// Put stores a value of any type in Context\nfunc (c *Context) Put(key string, value interface{}) {\n\tc.lock.Lock()\n\tc.contextMap[key] = value\n\tc.lock.Unlock()\n}\n\n// Get retrieves a string value from Context.\n// Get returns an empty string if key not found\nfunc (c *Context) Get(key string) string {\n\tc.lock.RLock()\n\tdefer c.lock.RUnlock()\n\tif v, ok := c.contextMap[key]; ok {\n\t\treturn v.(string)\n\t}\n\treturn \"\"\n}\n\n// GetAny retrieves a value from Context.\n// GetAny returns nil if key not found\nfunc (c *Context) GetAny(key string) interface{} {\n\tc.lock.RLock()\n\tdefer c.lock.RUnlock()\n\tif v, ok := c.contextMap[key]; ok {\n\t\treturn v\n\t}\n\treturn nil\n}\n\n// ForEach iterate context\nfunc (c *Context) ForEach(fn func(k string, v interface{}) interface{}) []interface{} {\n\tc.lock.RLock()\n\tdefer c.lock.RUnlock()\n\n\tret := make([]interface{}, 0, len(c.contextMap))\n\tfor k, v := range c.contextMap {\n\t\tret = append(ret, fn(k, v))\n\t}\n\n\treturn ret\n}\n"
        },
        {
          "name": "context_test.go",
          "type": "blob",
          "size": 1.017578125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"strconv\"\n\t\"testing\"\n)\n\nfunc TestContextIteration(t *testing.T) {\n\tctx := NewContext()\n\tfor i := 0; i < 10; i++ {\n\t\tctx.Put(strconv.Itoa(i), i)\n\t}\n\tvalues := ctx.ForEach(func(k string, v interface{}) interface{} {\n\t\treturn v.(int)\n\t})\n\tif len(values) != 10 {\n\t\tt.Fatal(\"fail to iterate context\")\n\t}\n\tfor _, i := range values {\n\t\tv := i.(int)\n\t\tif v != ctx.GetAny(strconv.Itoa(v)).(int) {\n\t\t\tt.Fatal(\"value not equal\")\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "debug",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.55859375,
          "content": "module github.com/gocolly/colly/v2\n\ngo 1.12\n\nrequire (\n\tgithub.com/PuerkitoBio/goquery v1.5.1\n\tgithub.com/andybalholm/cascadia v1.3.1 // indirect\n\tgithub.com/antchfx/htmlquery v1.2.3\n\tgithub.com/antchfx/xmlquery v1.3.4\n\tgithub.com/gobwas/glob v0.2.3\n\tgithub.com/jawher/mow.cli v1.1.0\n\tgithub.com/kennygrant/sanitize v1.2.4\n\tgithub.com/nlnwa/whatwg-url v0.1.2\n\tgithub.com/saintfish/chardet v0.0.0-20120816061221-3af4cd4741ca\n\tgithub.com/temoto/robotstxt v1.1.1\n\tgolang.org/x/net v0.17.0\n\tgoogle.golang.org/appengine v1.6.6\n\tgoogle.golang.org/protobuf v1.33.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 9.4970703125,
          "content": "github.com/PuerkitoBio/goquery v1.5.1 h1:PSPBGne8NIUWw+/7vFBV+kG2J/5MOjbzc7154OaKCSE=\ngithub.com/PuerkitoBio/goquery v1.5.1/go.mod h1:GsLWisAFVj4WgDibEWF4pvYnkVQBpKBKeU+7zCJoLcc=\ngithub.com/andybalholm/cascadia v1.1.0/go.mod h1:GsXiBklL0woXo1j/WYWtSYYC4ouU9PqHO0sqidkEA4Y=\ngithub.com/andybalholm/cascadia v1.3.1 h1:nhxRkql1kdYCc8Snf7D5/D3spOX+dBgjA6u8x004T2c=\ngithub.com/andybalholm/cascadia v1.3.1/go.mod h1:R4bJ1UQfqADjvDa4P6HZHLh/3OxWWEqc0Sk8XGwHqvA=\ngithub.com/antchfx/htmlquery v1.2.3 h1:sP3NFDneHx2stfNXCKbhHFo8XgNjCACnU/4AO5gWz6M=\ngithub.com/antchfx/htmlquery v1.2.3/go.mod h1:B0ABL+F5irhhMWg54ymEZinzMSi0Kt3I2if0BLYa3V0=\ngithub.com/antchfx/xmlquery v1.3.4 h1:RuhsI4AA5Ma4XoXhaAr2VjJxU0Xp0W2zy/f9ZIpsF4s=\ngithub.com/antchfx/xmlquery v1.3.4/go.mod h1:64w0Xesg2sTaawIdNqMB+7qaW/bSqkQm+ssPaCMWNnc=\ngithub.com/antchfx/xpath v1.1.6/go.mod h1:Yee4kTMuNiPYJ7nSNorELQMr1J33uOpXDMByNYhvtNk=\ngithub.com/antchfx/xpath v1.1.10 h1:cJ0pOvEdN/WvYXxvRrzQH9x5QWKpzHacYO8qzCcDYAg=\ngithub.com/antchfx/xpath v1.1.10/go.mod h1:Yee4kTMuNiPYJ7nSNorELQMr1J33uOpXDMByNYhvtNk=\ngithub.com/bits-and-blooms/bitset v1.2.2-0.20220111210104-dfa3e347c392 h1:9d7ak0NpT8/bhFM5ZkQuLpeS8Ey9zDY9OJJcOYqYV4c=\ngithub.com/bits-and-blooms/bitset v1.2.2-0.20220111210104-dfa3e347c392/go.mod h1:gIdJ4wp64HaoK2YrL1Q5/N7Y16edYb8uY+O0FJTyyDA=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/gobwas/glob v0.2.3 h1:A4xDbljILXROh+kObIiy5kIaPYD8e96x1tgBhUI5J+Y=\ngithub.com/gobwas/glob v0.2.3/go.mod h1:d3Ez4x06l9bZtSvzIay5+Yzi0fmZzPgnTbPcKjJAkT8=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e h1:1r7pUrabqp18hOBcwBwiTsbnFeTZHV9eER/QT5JVZxY=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.5.0 h1:LUVKkCeviFUMKqHa4tXIIij/lbhnMbP7Fn5wKdKkRh4=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/google/go-cmp v0.5.5 h1:Khx7svrCpmxxtHBq5j2mp/xVjsi8hQMfNLvJFAlrGgU=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/jawher/mow.cli v1.1.0 h1:NdtHXRc0CwZQ507wMvQ/IS+Q3W3x2fycn973/b8Zuk8=\ngithub.com/jawher/mow.cli v1.1.0/go.mod h1:aNaQlc7ozF3vw6IJ2dHjp2ZFiA4ozMIYY6PyuRJwlUg=\ngithub.com/kennygrant/sanitize v1.2.4 h1:gN25/otpP5vAsO2djbMhF/LQX6R7+O1TB4yv8NzpJ3o=\ngithub.com/kennygrant/sanitize v1.2.4/go.mod h1:LGsjYYtgxbetdg5owWB2mpgUL6e2nfw2eObZ0u0qvak=\ngithub.com/nlnwa/whatwg-url v0.1.2 h1:BqqsIVG6xv71wOoMAoFDmV6OK6/2sXn7BJdOsTkBl88=\ngithub.com/nlnwa/whatwg-url v0.1.2/go.mod h1:b0r+dEyM/KztLMDSVY6ApcO9Fmzgq+e9+Ugq20UBYck=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/saintfish/chardet v0.0.0-20120816061221-3af4cd4741ca h1:NugYot0LIVPxTvN8n+Kvkn6TrbMyxQiuvKdEwFdR9vI=\ngithub.com/saintfish/chardet v0.0.0-20120816061221-3af4cd4741ca/go.mod h1:uugorj2VCxiV1x+LzaIdVa9b4S4qGAcH6cbhh4qVxOU=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=\ngithub.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/temoto/robotstxt v1.1.1 h1:Gh8RCs8ouX3hRSxxK7B1mO5RFByQ4CmJZDwgom++JaA=\ngithub.com/temoto/robotstxt v1.1.1/go.mod h1:+1AmkuG3IYkh1kv0d2qEB9Le88ehNO0zwOr3ujewlOo=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.14.0/go.mod h1:MVFd36DqK4CsrnJYDkBA3VC4m2GkXAM0PvzMCn4JQf4=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\ngolang.org/x/net v0.0.0-20180218175443-cbe0f9307d01/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200421231249-e086a090c8fd/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200813134508-3edf25e44fcc/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210916014120-12bc252f5db8/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220114011407-0dd24b26b47d/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=\ngolang.org/x/net v0.17.0 h1:pVaXccu2ozPjCXewfr1S7xza/zcXTity9cCdXQYSjIM=\ngolang.org/x/net v0.17.0/go.mod h1:NxSsAGuq816PNPmqtQdLE42eU2Fs7NoRIZrHJAlaCOE=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\ngolang.org/x/term v0.8.0/go.mod h1:xPskH00ivmX89bAKVGSKKtLOWNx2+17Eiy94tnKShWo=\ngolang.org/x/term v0.13.0/go.mod h1:LTmsnFJwVN6bCy1rVCoS+qHT1HhALEFxKncY3WNNh4U=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\ngolang.org/x/text v0.13.0 h1:ablQoSUd0tRdKxZewP80B+BaqeKJuVhuRxj/dkrun3k=\ngolang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/appengine v1.6.6 h1:lMO5rYAqUxkmaj76jAkRUvt5JZgFymx/+Q5Mzfivuhc=\ngoogle.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=\ngoogle.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=\n"
        },
        {
          "name": "htmlelement.go",
          "type": "blob",
          "size": 4.0888671875,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"strings\"\n\n\t\"github.com/PuerkitoBio/goquery\"\n\t\"golang.org/x/net/html\"\n)\n\n// HTMLElement is the representation of a HTML tag.\ntype HTMLElement struct {\n\t// Name is the name of the tag\n\tName       string\n\tText       string\n\tattributes []html.Attribute\n\t// Request is the request object of the element's HTML document\n\tRequest *Request\n\t// Response is the Response object of the element's HTML document\n\tResponse *Response\n\t// DOM is the goquery parsed DOM object of the page. DOM is relative\n\t// to the current HTMLElement\n\tDOM *goquery.Selection\n\t// Index stores the position of the current element within all the elements matched by an OnHTML callback\n\tIndex int\n}\n\n// NewHTMLElementFromSelectionNode creates a HTMLElement from a goquery.Selection Node.\nfunc NewHTMLElementFromSelectionNode(resp *Response, s *goquery.Selection, n *html.Node, idx int) *HTMLElement {\n\treturn &HTMLElement{\n\t\tName:       n.Data,\n\t\tRequest:    resp.Request,\n\t\tResponse:   resp,\n\t\tText:       goquery.NewDocumentFromNode(n).Text(),\n\t\tDOM:        s,\n\t\tIndex:      idx,\n\t\tattributes: n.Attr,\n\t}\n}\n\n// Attr returns the selected attribute of a HTMLElement or empty string\n// if no attribute found\nfunc (h *HTMLElement) Attr(k string) string {\n\tfor _, a := range h.attributes {\n\t\tif a.Key == k {\n\t\t\treturn a.Val\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// ChildText returns the concatenated and stripped text content of the matching\n// elements.\nfunc (h *HTMLElement) ChildText(goquerySelector string) string {\n\treturn strings.TrimSpace(h.DOM.Find(goquerySelector).Text())\n}\n\n// ChildTexts returns the stripped text content of all the matching\n// elements.\nfunc (h *HTMLElement) ChildTexts(goquerySelector string) []string {\n\tvar res []string\n\th.DOM.Find(goquerySelector).Each(func(_ int, s *goquery.Selection) {\n\n\t\tres = append(res, strings.TrimSpace(s.Text()))\n\t})\n\treturn res\n}\n\n// ChildAttr returns the stripped text content of the first matching\n// element's attribute.\nfunc (h *HTMLElement) ChildAttr(goquerySelector, attrName string) string {\n\tif attr, ok := h.DOM.Find(goquerySelector).Attr(attrName); ok {\n\t\treturn strings.TrimSpace(attr)\n\t}\n\treturn \"\"\n}\n\n// ChildAttrs returns the stripped text content of all the matching\n// element's attributes.\nfunc (h *HTMLElement) ChildAttrs(goquerySelector, attrName string) []string {\n\tvar res []string\n\th.DOM.Find(goquerySelector).Each(func(_ int, s *goquery.Selection) {\n\t\tif attr, ok := s.Attr(attrName); ok {\n\t\t\tres = append(res, strings.TrimSpace(attr))\n\t\t}\n\t})\n\treturn res\n}\n\n// ForEach iterates over the elements matched by the first argument\n// and calls the callback function on every HTMLElement match.\nfunc (h *HTMLElement) ForEach(goquerySelector string, callback func(int, *HTMLElement)) {\n\ti := 0\n\th.DOM.Find(goquerySelector).Each(func(_ int, s *goquery.Selection) {\n\t\tfor _, n := range s.Nodes {\n\t\t\tcallback(i, NewHTMLElementFromSelectionNode(h.Response, s, n, i))\n\t\t\ti++\n\t\t}\n\t})\n}\n\n// ForEachWithBreak iterates over the elements matched by the first argument\n// and calls the callback function on every HTMLElement match.\n// It is identical to ForEach except that it is possible to break\n// out of the loop by returning false in the callback function. It returns the\n// current Selection object.\nfunc (h *HTMLElement) ForEachWithBreak(goquerySelector string, callback func(int, *HTMLElement) bool) {\n\ti := 0\n\th.DOM.Find(goquerySelector).EachWithBreak(func(_ int, s *goquery.Selection) bool {\n\t\tfor _, n := range s.Nodes {\n\t\t\tif callback(i, NewHTMLElementFromSelectionNode(h.Response, s, n, i)) {\n\t\t\t\ti++\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t})\n}\n"
        },
        {
          "name": "http_backend.go",
          "type": "blob",
          "size": 6.2783203125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"crypto/sha1\"\n\t\"encoding/gob\"\n\t\"encoding/hex\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"regexp\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"compress/gzip\"\n\n\t\"github.com/gobwas/glob\"\n)\n\ntype httpBackend struct {\n\tLimitRules []*LimitRule\n\tClient     *http.Client\n\tlock       *sync.RWMutex\n}\n\ntype checkHeadersFunc func(req *http.Request, statusCode int, header http.Header) bool\n\n// LimitRule provides connection restrictions for domains.\n// Both DomainRegexp and DomainGlob can be used to specify\n// the included domains patterns, but at least one is required.\n// There can be two kind of limitations:\n//   - Parallelism: Set limit for the number of concurrent requests to matching domains\n//   - Delay: Wait specified amount of time between requests (parallelism is 1 in this case)\ntype LimitRule struct {\n\t// DomainRegexp is a regular expression to match against domains\n\tDomainRegexp string\n\t// DomainGlob is a glob pattern to match against domains\n\tDomainGlob string\n\t// Delay is the duration to wait before creating a new request to the matching domains\n\tDelay time.Duration\n\t// RandomDelay is the extra randomized duration to wait added to Delay before creating a new request\n\tRandomDelay time.Duration\n\t// Parallelism is the number of the maximum allowed concurrent requests of the matching domains\n\tParallelism    int\n\twaitChan       chan bool\n\tcompiledRegexp *regexp.Regexp\n\tcompiledGlob   glob.Glob\n}\n\n// Init initializes the private members of LimitRule\nfunc (r *LimitRule) Init() error {\n\twaitChanSize := 1\n\tif r.Parallelism > 1 {\n\t\twaitChanSize = r.Parallelism\n\t}\n\tr.waitChan = make(chan bool, waitChanSize)\n\thasPattern := false\n\tif r.DomainRegexp != \"\" {\n\t\tc, err := regexp.Compile(r.DomainRegexp)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr.compiledRegexp = c\n\t\thasPattern = true\n\t}\n\tif r.DomainGlob != \"\" {\n\t\tc, err := glob.Compile(r.DomainGlob)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr.compiledGlob = c\n\t\thasPattern = true\n\t}\n\tif !hasPattern {\n\t\treturn ErrNoPattern\n\t}\n\treturn nil\n}\n\nfunc (h *httpBackend) Init(jar http.CookieJar) {\n\trand.Seed(time.Now().UnixNano())\n\th.Client = &http.Client{\n\t\tJar:     jar,\n\t\tTimeout: 10 * time.Second,\n\t}\n\th.lock = &sync.RWMutex{}\n}\n\n// Match checks that the domain parameter triggers the rule\nfunc (r *LimitRule) Match(domain string) bool {\n\tmatch := false\n\tif r.compiledRegexp != nil && r.compiledRegexp.MatchString(domain) {\n\t\tmatch = true\n\t}\n\tif r.compiledGlob != nil && r.compiledGlob.Match(domain) {\n\t\tmatch = true\n\t}\n\treturn match\n}\n\nfunc (h *httpBackend) GetMatchingRule(domain string) *LimitRule {\n\tif h.LimitRules == nil {\n\t\treturn nil\n\t}\n\th.lock.RLock()\n\tdefer h.lock.RUnlock()\n\tfor _, r := range h.LimitRules {\n\t\tif r.Match(domain) {\n\t\t\treturn r\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (h *httpBackend) Cache(request *http.Request, bodySize int, checkHeadersFunc checkHeadersFunc, cacheDir string) (*Response, error) {\n\tif cacheDir == \"\" || request.Method != \"GET\" || request.Header.Get(\"Cache-Control\") == \"no-cache\" {\n\t\treturn h.Do(request, bodySize, checkHeadersFunc)\n\t}\n\tsum := sha1.Sum([]byte(request.URL.String()))\n\thash := hex.EncodeToString(sum[:])\n\tdir := path.Join(cacheDir, hash[:2])\n\tfilename := path.Join(dir, hash)\n\tif file, err := os.Open(filename); err == nil {\n\t\tresp := new(Response)\n\t\terr := gob.NewDecoder(file).Decode(resp)\n\t\tfile.Close()\n\t\tcheckHeadersFunc(request, resp.StatusCode, *resp.Headers)\n\t\tif resp.StatusCode < 500 {\n\t\t\treturn resp, err\n\t\t}\n\t}\n\tresp, err := h.Do(request, bodySize, checkHeadersFunc)\n\tif err != nil || resp.StatusCode >= 500 {\n\t\treturn resp, err\n\t}\n\tif _, err := os.Stat(dir); err != nil {\n\t\tif err := os.MkdirAll(dir, 0750); err != nil {\n\t\t\treturn resp, err\n\t\t}\n\t}\n\tfile, err := os.Create(filename + \"~\")\n\tif err != nil {\n\t\treturn resp, err\n\t}\n\tif err := gob.NewEncoder(file).Encode(resp); err != nil {\n\t\tfile.Close()\n\t\treturn resp, err\n\t}\n\tfile.Close()\n\treturn resp, os.Rename(filename+\"~\", filename)\n}\n\nfunc (h *httpBackend) Do(request *http.Request, bodySize int, checkHeadersFunc checkHeadersFunc) (*Response, error) {\n\tr := h.GetMatchingRule(request.URL.Host)\n\tif r != nil {\n\t\tr.waitChan <- true\n\t\tdefer func(r *LimitRule) {\n\t\t\trandomDelay := time.Duration(0)\n\t\t\tif r.RandomDelay != 0 {\n\t\t\t\trandomDelay = time.Duration(rand.Int63n(int64(r.RandomDelay)))\n\t\t\t}\n\t\t\ttime.Sleep(r.Delay + randomDelay)\n\t\t\t<-r.waitChan\n\t\t}(r)\n\t}\n\n\tres, err := h.Client.Do(request)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer res.Body.Close()\n\n\tfinalRequest := request\n\tif res.Request != nil {\n\t\tfinalRequest = res.Request\n\t}\n\tif !checkHeadersFunc(finalRequest, res.StatusCode, res.Header) {\n\t\t// closing res.Body (see defer above) without reading it aborts\n\t\t// the download\n\t\treturn nil, ErrAbortedAfterHeaders\n\t}\n\n\tvar bodyReader io.Reader = res.Body\n\tif bodySize > 0 {\n\t\tbodyReader = io.LimitReader(bodyReader, int64(bodySize))\n\t}\n\tcontentEncoding := strings.ToLower(res.Header.Get(\"Content-Encoding\"))\n\tif !res.Uncompressed && (strings.Contains(contentEncoding, \"gzip\") || (contentEncoding == \"\" && strings.Contains(strings.ToLower(res.Header.Get(\"Content-Type\")), \"gzip\")) || strings.HasSuffix(strings.ToLower(finalRequest.URL.Path), \".xml.gz\")) {\n\t\tbodyReader, err = gzip.NewReader(bodyReader)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer bodyReader.(*gzip.Reader).Close()\n\t}\n\tbody, err := io.ReadAll(bodyReader)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Response{\n\t\tStatusCode: res.StatusCode,\n\t\tBody:       body,\n\t\tHeaders:    &res.Header,\n\t}, nil\n}\n\nfunc (h *httpBackend) Limit(rule *LimitRule) error {\n\th.lock.Lock()\n\tif h.LimitRules == nil {\n\t\th.LimitRules = make([]*LimitRule, 0, 8)\n\t}\n\th.LimitRules = append(h.LimitRules, rule)\n\th.lock.Unlock()\n\treturn rule.Init()\n}\n\nfunc (h *httpBackend) Limits(rules []*LimitRule) error {\n\tfor _, r := range rules {\n\t\tif err := h.Limit(r); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "http_trace.go",
          "type": "blob",
          "size": 1.04296875,
          "content": "package colly\n\nimport (\n\t\"net/http\"\n\t\"net/http/httptrace\"\n\t\"time\"\n)\n\n// HTTPTrace provides a datastructure for storing an http trace.\ntype HTTPTrace struct {\n\tstart, connect    time.Time\n\tConnectDuration   time.Duration\n\tFirstByteDuration time.Duration\n}\n\n// trace returns a httptrace.ClientTrace object to be used with an http\n// request via httptrace.WithClientTrace() that fills in the HttpTrace.\nfunc (ht *HTTPTrace) trace() *httptrace.ClientTrace {\n\ttrace := &httptrace.ClientTrace{\n\t\tConnectStart: func(network, addr string) { ht.connect = time.Now() },\n\t\tConnectDone: func(network, addr string, err error) {\n\t\t\tht.ConnectDuration = time.Since(ht.connect)\n\t\t},\n\n\t\tGetConn: func(hostPort string) { ht.start = time.Now() },\n\t\tGotFirstResponseByte: func() {\n\t\t\tht.FirstByteDuration = time.Since(ht.start)\n\t\t},\n\t}\n\treturn trace\n}\n\n// WithTrace returns the given HTTP Request with this HTTPTrace added to its\n// context.\nfunc (ht *HTTPTrace) WithTrace(req *http.Request) *http.Request {\n\treturn req.WithContext(httptrace.WithClientTrace(req.Context(), ht.trace()))\n}\n"
        },
        {
          "name": "http_trace_test.go",
          "type": "blob",
          "size": 1.7392578125,
          "content": "package colly\n\nimport (\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"testing\"\n\t\"time\"\n)\n\nconst testDelay = 200 * time.Millisecond\n\nfunc newTraceTestServer(delay time.Duration) *httptest.Server {\n\tmux := http.NewServeMux()\n\n\tmux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\ttime.Sleep(delay)\n\t\tw.WriteHeader(200)\n\t})\n\tmux.HandleFunc(\"/error\", func(w http.ResponseWriter, r *http.Request) {\n\t\ttime.Sleep(delay)\n\t\tw.WriteHeader(500)\n\t})\n\n\treturn httptest.NewServer(mux)\n}\n\nfunc TestTraceWithNoDelay(t *testing.T) {\n\tts := newTraceTestServer(0)\n\tdefer ts.Close()\n\n\tclient := ts.Client()\n\treq, err := http.NewRequest(\"GET\", ts.URL, nil)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to construct request %v\", err)\n\t}\n\ttrace := &HTTPTrace{}\n\treq = trace.WithTrace(req)\n\n\tif _, err = client.Do(req); err != nil {\n\t\tt.Errorf(\"Failed to make request %v\", err)\n\t}\n\n\tif trace.ConnectDuration > testDelay {\n\t\tt.Errorf(\"trace ConnectDuration should be (almost) 0, got %v\", trace.ConnectDuration)\n\t}\n\tif trace.FirstByteDuration > testDelay {\n\t\tt.Errorf(\"trace FirstByteDuration should be (almost) 0, got %v\", trace.FirstByteDuration)\n\t}\n}\n\nfunc TestTraceWithDelay(t *testing.T) {\n\tts := newTraceTestServer(testDelay)\n\tdefer ts.Close()\n\n\tclient := ts.Client()\n\treq, err := http.NewRequest(\"GET\", ts.URL, nil)\n\tif err != nil {\n\t\tt.Errorf(\"Failed to construct request %v\", err)\n\t}\n\ttrace := &HTTPTrace{}\n\treq = trace.WithTrace(req)\n\n\tif _, err = client.Do(req); err != nil {\n\t\tt.Errorf(\"Failed to make request %v\", err)\n\t}\n\n\tif trace.ConnectDuration > testDelay {\n\t\tt.Errorf(\"trace ConnectDuration should be (almost) 0, got %v\", trace.ConnectDuration)\n\t}\n\tif trace.FirstByteDuration < testDelay {\n\t\tt.Errorf(\"trace FirstByteDuration should be at least 200ms, got %v\", trace.FirstByteDuration)\n\t}\n}\n"
        },
        {
          "name": "proxy",
          "type": "tree",
          "content": null
        },
        {
          "name": "queue",
          "type": "tree",
          "content": null
        },
        {
          "name": "request.go",
          "type": "blob",
          "size": 5.69140625,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strings\"\n\t\"sync/atomic\"\n)\n\n// Request is the representation of a HTTP request made by a Collector\ntype Request struct {\n\t// URL is the parsed URL of the HTTP request\n\tURL *url.URL\n\t// Headers contains the Request's HTTP headers\n\tHeaders *http.Header\n\t// the Host header\n\tHost string\n\t// Ctx is a context between a Request and a Response\n\tCtx *Context\n\t// Depth is the number of the parents of the request\n\tDepth int\n\t// Method is the HTTP method of the request\n\tMethod string\n\t// Body is the request body which is used on POST/PUT requests\n\tBody io.Reader\n\t// ResponseCharacterencoding is the character encoding of the response body.\n\t// Leave it blank to allow automatic character encoding of the response body.\n\t// It is empty by default and it can be set in OnRequest callback.\n\tResponseCharacterEncoding string\n\t// ID is the Unique identifier of the request\n\tID        uint32\n\tcollector *Collector\n\tabort     bool\n\tbaseURL   *url.URL\n\t// ProxyURL is the proxy address that handles the request\n\tProxyURL string\n}\n\ntype serializableRequest struct {\n\tURL     string\n\tMethod  string\n\tDepth   int\n\tBody    []byte\n\tID      uint32\n\tCtx     map[string]interface{}\n\tHeaders http.Header\n\tHost    string\n}\n\n// New creates a new request with the context of the original request\nfunc (r *Request) New(method, URL string, body io.Reader) (*Request, error) {\n\tu, err := urlParser.Parse(URL)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tu2, err := url.Parse(u.Href(false))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Request{\n\t\tMethod:    method,\n\t\tURL:       u2,\n\t\tBody:      body,\n\t\tCtx:       r.Ctx,\n\t\tHeaders:   &http.Header{},\n\t\tHost:      r.Host,\n\t\tID:        atomic.AddUint32(&r.collector.requestCount, 1),\n\t\tcollector: r.collector,\n\t}, nil\n}\n\n// Abort cancels the HTTP request when called in an OnRequest callback\nfunc (r *Request) Abort() {\n\tr.abort = true\n}\n\n// AbsoluteURL returns with the resolved absolute URL of an URL chunk.\n// AbsoluteURL returns empty string if the URL chunk is a fragment or\n// could not be parsed\nfunc (r *Request) AbsoluteURL(u string) string {\n\tif strings.HasPrefix(u, \"#\") {\n\t\treturn \"\"\n\t}\n\tvar base *url.URL\n\tif r.baseURL != nil {\n\t\tbase = r.baseURL\n\t} else {\n\t\tbase = r.URL\n\t}\n\n\tabsURL, err := urlParser.ParseRef(base.String(), u)\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\treturn absURL.Href(false)\n}\n\n// Visit continues Collector's collecting job by creating a\n// request and preserves the Context of the previous request.\n// Visit also calls the previously provided callbacks\nfunc (r *Request) Visit(URL string) error {\n\treturn r.collector.scrape(r.AbsoluteURL(URL), \"GET\", r.Depth+1, nil, r.Ctx, nil, true)\n}\n\n// HasVisited checks if the provided URL has been visited\nfunc (r *Request) HasVisited(URL string) (bool, error) {\n\treturn r.collector.HasVisited(URL)\n}\n\n// Post continues a collector job by creating a POST request and preserves the Context\n// of the previous request.\n// Post also calls the previously provided callbacks\nfunc (r *Request) Post(URL string, requestData map[string]string) error {\n\treturn r.collector.scrape(r.AbsoluteURL(URL), \"POST\", r.Depth+1, createFormReader(requestData), r.Ctx, nil, true)\n}\n\n// PostRaw starts a collector job by creating a POST request with raw binary data.\n// PostRaw preserves the Context of the previous request\n// and calls the previously provided callbacks\nfunc (r *Request) PostRaw(URL string, requestData []byte) error {\n\treturn r.collector.scrape(r.AbsoluteURL(URL), \"POST\", r.Depth+1, bytes.NewReader(requestData), r.Ctx, nil, true)\n}\n\n// PostMultipart starts a collector job by creating a Multipart POST request\n// with raw binary data.  PostMultipart also calls the previously provided.\n// callbacks\nfunc (r *Request) PostMultipart(URL string, requestData map[string][]byte) error {\n\tboundary := randomBoundary()\n\thdr := http.Header{}\n\thdr.Set(\"Content-Type\", \"multipart/form-data; boundary=\"+boundary)\n\thdr.Set(\"User-Agent\", r.collector.UserAgent)\n\treturn r.collector.scrape(r.AbsoluteURL(URL), \"POST\", r.Depth+1, createMultipartReader(boundary, requestData), r.Ctx, hdr, true)\n}\n\n// Retry submits HTTP request again with the same parameters\nfunc (r *Request) Retry() error {\n\tr.Headers.Del(\"Cookie\")\n\tif _, ok := r.Body.(io.ReadSeeker); r.Body != nil && !ok {\n\t\treturn ErrRetryBodyUnseekable\n\t}\n\treturn r.collector.scrape(r.URL.String(), r.Method, r.Depth, r.Body, r.Ctx, *r.Headers, false)\n}\n\n// Do submits the request\nfunc (r *Request) Do() error {\n\treturn r.collector.scrape(r.URL.String(), r.Method, r.Depth, r.Body, r.Ctx, *r.Headers, !r.collector.AllowURLRevisit)\n}\n\n// Marshal serializes the Request\nfunc (r *Request) Marshal() ([]byte, error) {\n\tctx := make(map[string]interface{})\n\tif r.Ctx != nil {\n\t\tr.Ctx.ForEach(func(k string, v interface{}) interface{} {\n\t\t\tctx[k] = v\n\t\t\treturn nil\n\t\t})\n\t}\n\tvar err error\n\tvar body []byte\n\tif r.Body != nil {\n\t\tbody, err = io.ReadAll(r.Body)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tsr := &serializableRequest{\n\t\tURL:    r.URL.String(),\n\t\tHost:   r.Host,\n\t\tMethod: r.Method,\n\t\tDepth:  r.Depth,\n\t\tBody:   body,\n\t\tID:     r.ID,\n\t\tCtx:    ctx,\n\t}\n\tif r.Headers != nil {\n\t\tsr.Headers = *r.Headers\n\t}\n\treturn json.Marshal(sr)\n}\n"
        },
        {
          "name": "response.go",
          "type": "blob",
          "size": 3.14453125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"mime\"\n\t\"net/http\"\n\t\"os\"\n\t\"strings\"\n\n\t\"github.com/saintfish/chardet\"\n\t\"golang.org/x/net/html/charset\"\n)\n\n// Response is the representation of a HTTP response made by a Collector\ntype Response struct {\n\t// StatusCode is the status code of the Response\n\tStatusCode int\n\t// Body is the content of the Response\n\tBody []byte\n\t// Ctx is a context between a Request and a Response\n\tCtx *Context\n\t// Request is the Request object of the response\n\tRequest *Request\n\t// Headers contains the Response's HTTP headers\n\tHeaders *http.Header\n\t// Trace contains the HTTPTrace for the request. Will only be set by the\n\t// collector if Collector.TraceHTTP is set to true.\n\tTrace *HTTPTrace\n}\n\n// Save writes response body to disk\nfunc (r *Response) Save(fileName string) error {\n\treturn os.WriteFile(fileName, r.Body, 0644)\n}\n\n// FileName returns the sanitized file name parsed from \"Content-Disposition\"\n// header or from URL\nfunc (r *Response) FileName() string {\n\t_, params, err := mime.ParseMediaType(r.Headers.Get(\"Content-Disposition\"))\n\tif fName, ok := params[\"filename\"]; ok && err == nil {\n\t\treturn SanitizeFileName(fName)\n\t}\n\tif r.Request.URL.RawQuery != \"\" {\n\t\treturn SanitizeFileName(fmt.Sprintf(\"%s_%s\", r.Request.URL.Path, r.Request.URL.RawQuery))\n\t}\n\treturn SanitizeFileName(strings.TrimPrefix(r.Request.URL.Path, \"/\"))\n}\n\nfunc (r *Response) fixCharset(detectCharset bool, defaultEncoding string) error {\n\tif len(r.Body) == 0 {\n\t\treturn nil\n\t}\n\tif defaultEncoding != \"\" {\n\t\ttmpBody, err := encodeBytes(r.Body, \"text/plain; charset=\"+defaultEncoding)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr.Body = tmpBody\n\t\treturn nil\n\t}\n\tcontentType := strings.ToLower(r.Headers.Get(\"Content-Type\"))\n\n\tif strings.Contains(contentType, \"image/\") ||\n\t\tstrings.Contains(contentType, \"video/\") ||\n\t\tstrings.Contains(contentType, \"audio/\") ||\n\t\tstrings.Contains(contentType, \"font/\") {\n\t\t// These MIME types should not have textual data.\n\n\t\treturn nil\n\t}\n\n\tif !strings.Contains(contentType, \"charset\") {\n\t\tif !detectCharset {\n\t\t\treturn nil\n\t\t}\n\t\td := chardet.NewTextDetector()\n\t\tr, err := d.DetectBest(r.Body)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcontentType = \"text/plain; charset=\" + r.Charset\n\t}\n\tif strings.Contains(contentType, \"utf-8\") || strings.Contains(contentType, \"utf8\") {\n\t\treturn nil\n\t}\n\ttmpBody, err := encodeBytes(r.Body, contentType)\n\tif err != nil {\n\t\treturn err\n\t}\n\tr.Body = tmpBody\n\treturn nil\n}\n\nfunc encodeBytes(b []byte, contentType string) ([]byte, error) {\n\tr, err := charset.NewReader(bytes.NewReader(b), contentType)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn io.ReadAll(r)\n}\n"
        },
        {
          "name": "storage",
          "type": "tree",
          "content": null
        },
        {
          "name": "unmarshal.go",
          "type": "blob",
          "size": 5.83203125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"errors\"\n\t\"reflect\"\n\t\"strings\"\n\n\t\"github.com/PuerkitoBio/goquery\"\n)\n\n// Unmarshal is a shorthand for colly.UnmarshalHTML\nfunc (h *HTMLElement) Unmarshal(v interface{}) error {\n\treturn UnmarshalHTML(v, h.DOM, nil)\n}\n\n// UnmarshalWithMap is a shorthand for colly.UnmarshalHTML, extended to allow maps to be passed in.\nfunc (h *HTMLElement) UnmarshalWithMap(v interface{}, structMap map[string]string) error {\n\treturn UnmarshalHTML(v, h.DOM, structMap)\n}\n\n// UnmarshalHTML declaratively extracts text or attributes to a struct from\n// HTML response using struct tags composed of css selectors.\n// Allowed struct tags:\n//   - \"selector\" (required): CSS (goquery) selector of the desired data\n//   - \"attr\" (optional): Selects the matching element's attribute's value.\n//     Leave it blank or omit to get the text of the element.\n//\n// Example struct declaration:\n//\n//\ttype Nested struct {\n//\t\tString  string   `selector:\"div > p\"`\n//\t   Classes []string `selector:\"li\" attr:\"class\"`\n//\t\tStruct  *Nested  `selector:\"div > div\"`\n//\t}\n//\n// Supported types: struct, *struct, string, []string\nfunc UnmarshalHTML(v interface{}, s *goquery.Selection, structMap map[string]string) error {\n\trv := reflect.ValueOf(v)\n\n\tif rv.Kind() != reflect.Ptr || rv.IsNil() {\n\t\treturn errors.New(\"Invalid type or nil-pointer\")\n\t}\n\n\tsv := rv.Elem()\n\tst := reflect.TypeOf(v).Elem()\n\tif structMap != nil {\n\t\tfor k, v := range structMap {\n\t\t\tattrV := sv.FieldByName(k)\n\t\t\tif !attrV.CanAddr() || !attrV.CanSet() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := unmarshalSelector(s, attrV, v); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := 0; i < sv.NumField(); i++ {\n\t\t\tattrV := sv.Field(i)\n\t\t\tif !attrV.CanAddr() || !attrV.CanSet() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := unmarshalAttr(s, attrV, st.Field(i)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc unmarshalSelector(s *goquery.Selection, attrV reflect.Value, selector string) error {\n\t//selector is \"-\" specify that field should ignore.\n\tif selector == \"-\" {\n\t\treturn nil\n\t}\n\thtmlAttr := \"\"\n\t// TODO support more types\n\tswitch attrV.Kind() {\n\tcase reflect.Slice:\n\t\tif err := unmarshalSlice(s, selector, htmlAttr, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase reflect.String:\n\t\tval := getDOMValue(s.Find(selector), htmlAttr)\n\t\tattrV.Set(reflect.Indirect(reflect.ValueOf(val)))\n\tcase reflect.Struct:\n\t\tif err := unmarshalStruct(s, selector, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase reflect.Ptr:\n\t\tif err := unmarshalPtr(s, selector, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn errors.New(\"Invalid type: \" + attrV.String())\n\t}\n\treturn nil\n}\n\nfunc unmarshalAttr(s *goquery.Selection, attrV reflect.Value, attrT reflect.StructField) error {\n\tselector := attrT.Tag.Get(\"selector\")\n\t//selector is \"-\" specify that field should ignore.\n\tif selector == \"-\" {\n\t\treturn nil\n\t}\n\thtmlAttr := attrT.Tag.Get(\"attr\")\n\t// TODO support more types\n\tswitch attrV.Kind() {\n\tcase reflect.Slice:\n\t\tif err := unmarshalSlice(s, selector, htmlAttr, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase reflect.String:\n\t\tval := getDOMValue(s.Find(selector), htmlAttr)\n\t\tattrV.Set(reflect.Indirect(reflect.ValueOf(val)))\n\tcase reflect.Struct:\n\t\tif err := unmarshalStruct(s, selector, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase reflect.Ptr:\n\t\tif err := unmarshalPtr(s, selector, attrV); err != nil {\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn errors.New(\"Invalid type: \" + attrV.String())\n\t}\n\treturn nil\n}\n\nfunc unmarshalStruct(s *goquery.Selection, selector string, attrV reflect.Value) error {\n\tnewS := s\n\tif selector != \"\" {\n\t\tnewS = newS.Find(selector)\n\t}\n\tif newS.Nodes == nil {\n\t\treturn nil\n\t}\n\tv := reflect.New(attrV.Type())\n\terr := UnmarshalHTML(v.Interface(), newS, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tattrV.Set(reflect.Indirect(v))\n\treturn nil\n}\n\nfunc unmarshalPtr(s *goquery.Selection, selector string, attrV reflect.Value) error {\n\tnewS := s\n\tif selector != \"\" {\n\t\tnewS = newS.Find(selector)\n\t}\n\tif newS.Nodes == nil {\n\t\treturn nil\n\t}\n\te := attrV.Type().Elem()\n\tif e.Kind() != reflect.Struct {\n\t\treturn errors.New(\"Invalid slice type\")\n\t}\n\tv := reflect.New(e)\n\terr := UnmarshalHTML(v.Interface(), newS, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tattrV.Set(v)\n\treturn nil\n}\n\nfunc unmarshalSlice(s *goquery.Selection, selector, htmlAttr string, attrV reflect.Value) error {\n\tif attrV.Pointer() == 0 {\n\t\tv := reflect.MakeSlice(attrV.Type(), 0, 0)\n\t\tattrV.Set(v)\n\t}\n\tswitch attrV.Type().Elem().Kind() {\n\tcase reflect.String:\n\t\ts.Find(selector).Each(func(_ int, s *goquery.Selection) {\n\t\t\tval := getDOMValue(s, htmlAttr)\n\t\t\tattrV.Set(reflect.Append(attrV, reflect.Indirect(reflect.ValueOf(val))))\n\t\t})\n\tcase reflect.Ptr:\n\t\ts.Find(selector).Each(func(_ int, innerSel *goquery.Selection) {\n\t\t\tsomeVal := reflect.New(attrV.Type().Elem().Elem())\n\t\t\tUnmarshalHTML(someVal.Interface(), innerSel, nil)\n\t\t\tattrV.Set(reflect.Append(attrV, someVal))\n\t\t})\n\tcase reflect.Struct:\n\t\ts.Find(selector).Each(func(_ int, innerSel *goquery.Selection) {\n\t\t\tsomeVal := reflect.New(attrV.Type().Elem())\n\t\t\tUnmarshalHTML(someVal.Interface(), innerSel, nil)\n\t\t\tattrV.Set(reflect.Append(attrV, reflect.Indirect(someVal)))\n\t\t})\n\tdefault:\n\t\treturn errors.New(\"Invalid slice type\")\n\t}\n\treturn nil\n}\n\nfunc getDOMValue(s *goquery.Selection, attr string) string {\n\tif attr == \"\" {\n\t\treturn strings.TrimSpace(s.First().Text())\n\t}\n\tattrV, _ := s.Attr(attr)\n\treturn attrV\n}\n"
        },
        {
          "name": "unmarshal_test.go",
          "type": "blob",
          "size": 4.6728515625,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n\n\t\"github.com/PuerkitoBio/goquery\"\n)\n\nvar basicTestData = []byte(`<ul><li class=\"x\">list <span>item</span> 1</li><li>list item 2</li><li>3</li></ul>`)\nvar nestedTestData = []byte(`<div><p>a</p><div><p>b</p><div><p>c</p></div></div></div>`)\nvar pointerSliceTestData = []byte(`<ul class=\"object\"><li class=\"info\">Information: <span>Info 1</span></li><li class=\"info\">Information: <span>Info 2</span></li></ul>`)\n\nfunc TestBasicUnmarshal(t *testing.T) {\n\tdoc, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(basicTestData))\n\te := &HTMLElement{\n\t\tDOM: doc.First(),\n\t}\n\ts := struct {\n\t\tString string   `selector:\"li:first-child\" attr:\"class\"`\n\t\tItems  []string `selector:\"li\"`\n\t\tStruct struct {\n\t\t\tString string `selector:\"li:last-child\"`\n\t\t}\n\t}{}\n\tif err := e.Unmarshal(&s); err != nil {\n\t\tt.Error(\"Cannot unmarshal struct: \" + err.Error())\n\t}\n\tif s.String != \"x\" {\n\t\tt.Errorf(`Invalid data for String: %q, expected \"x\"`, s.String)\n\t}\n\tif s.Struct.String != \"3\" {\n\t\tt.Errorf(`Invalid data for Struct.String: %q, expected \"3\"`, s.Struct.String)\n\t}\n}\n\nfunc TestNestedUnmarshalMap(t *testing.T) {\n\tdoc, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(nestedTestData))\n\te := &HTMLElement{\n\t\tDOM: doc.First(),\n\t}\n\tdoc2, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(basicTestData))\n\te2 := &HTMLElement{\n\t\tDOM: doc2.First(),\n\t}\n\ttype nested struct {\n\t\tString string\n\t}\n\tmapSelector := make(map[string]string)\n\tmapSelector[\"String\"] = \"div > p\"\n\n\tmapSelector2 := make(map[string]string)\n\tmapSelector2[\"String\"] = \"span\"\n\n\ts := nested{}\n\ts2 := nested{}\n\tif err := e.UnmarshalWithMap(&s, mapSelector); err != nil {\n\t\tt.Error(\"Cannot unmarshal struct: \" + err.Error())\n\t}\n\tif err := e2.UnmarshalWithMap(&s2, mapSelector2); err != nil {\n\t\tt.Error(\"Cannot unmarshal struct: \" + err.Error())\n\t}\n\tif s.String != \"a\" {\n\t\tt.Errorf(`Invalid data for String: %q, expected \"a\"`, s.String)\n\t}\n\tif s2.String != \"item\" {\n\t\tt.Errorf(`Invalid data for String: %q, expected \"a\"`, s.String)\n\t}\n}\n\nfunc TestNestedUnmarshal(t *testing.T) {\n\tdoc, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(nestedTestData))\n\te := &HTMLElement{\n\t\tDOM: doc.First(),\n\t}\n\ttype nested struct {\n\t\tString string  `selector:\"div > p\"`\n\t\tStruct *nested `selector:\"div > div\"`\n\t}\n\ts := nested{}\n\tif err := e.Unmarshal(&s); err != nil {\n\t\tt.Error(\"Cannot unmarshal struct: \" + err.Error())\n\t}\n\tif s.String != \"a\" {\n\t\tt.Errorf(`Invalid data for String: %q, expected \"a\"`, s.String)\n\t}\n\tif s.Struct.String != \"b\" {\n\t\tt.Errorf(`Invalid data for Struct.String: %q, expected \"b\"`, s.Struct.String)\n\t}\n\tif s.Struct.Struct.String != \"c\" {\n\t\tt.Errorf(`Invalid data for Struct.Struct.String: %q, expected \"c\"`, s.Struct.Struct.String)\n\t}\n}\n\nfunc TestPointerSliceUnmarshall(t *testing.T) {\n\ttype info struct {\n\t\tText string `selector:\"span\"`\n\t}\n\ttype object struct {\n\t\tInfo []*info `selector:\"li.info\"`\n\t}\n\n\tdoc, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(pointerSliceTestData))\n\te := HTMLElement{DOM: doc.First()}\n\to := object{}\n\terr := e.Unmarshal(&o)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to unmarshal page: %s\\n\", err.Error())\n\t}\n\n\tif len(o.Info) != 2 {\n\t\tt.Errorf(\"Invalid length for Info: %d, expected 2\", len(o.Info))\n\t}\n\tif o.Info[0].Text != \"Info 1\" {\n\t\tt.Errorf(\"Invalid data for Info.[0].Text: %s, expected Info 1\", o.Info[0].Text)\n\t}\n\tif o.Info[1].Text != \"Info 2\" {\n\t\tt.Errorf(\"Invalid data for Info.[1].Text: %s, expected Info 2\", o.Info[1].Text)\n\t}\n\n}\n\nfunc TestStructSliceUnmarshall(t *testing.T) {\n\ttype info struct {\n\t\tText string `selector:\"span\"`\n\t}\n\ttype object struct {\n\t\tInfo []info `selector:\"li.info\"`\n\t}\n\n\tdoc, _ := goquery.NewDocumentFromReader(bytes.NewBuffer(pointerSliceTestData))\n\te := HTMLElement{DOM: doc.First()}\n\to := object{}\n\terr := e.Unmarshal(&o)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to unmarshal page: %s\\n\", err.Error())\n\t}\n\n\tif len(o.Info) != 2 {\n\t\tt.Errorf(\"Invalid length for Info: %d, expected 2\", len(o.Info))\n\t}\n\tif o.Info[0].Text != \"Info 1\" {\n\t\tt.Errorf(\"Invalid data for Info.[0].Text: %s, expected Info 1\", o.Info[0].Text)\n\t}\n\tif o.Info[1].Text != \"Info 2\" {\n\t\tt.Errorf(\"Invalid data for Info.[1].Text: %s, expected Info 2\", o.Info[1].Text)\n\t}\n\n}\n"
        },
        {
          "name": "xmlelement.go",
          "type": "blob",
          "size": 4.6220703125,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly\n\nimport (\n\t\"strings\"\n\n\t\"github.com/antchfx/htmlquery\"\n\t\"github.com/antchfx/xmlquery\"\n\t\"golang.org/x/net/html\"\n)\n\n// XMLElement is the representation of a XML tag.\ntype XMLElement struct {\n\t// Name is the name of the tag\n\tName       string\n\tText       string\n\tattributes interface{}\n\t// Request is the request object of the element's HTML document\n\tRequest *Request\n\t// Response is the Response object of the element's HTML document\n\tResponse *Response\n\t// DOM is the DOM object of the page. DOM is relative\n\t// to the current XMLElement and is either a html.Node or xmlquery.Node\n\t// based on how the XMLElement was created.\n\tDOM    interface{}\n\tisHTML bool\n}\n\n// NewXMLElementFromHTMLNode creates a XMLElement from a html.Node.\nfunc NewXMLElementFromHTMLNode(resp *Response, s *html.Node) *XMLElement {\n\treturn &XMLElement{\n\t\tName:       s.Data,\n\t\tRequest:    resp.Request,\n\t\tResponse:   resp,\n\t\tText:       htmlquery.InnerText(s),\n\t\tDOM:        s,\n\t\tattributes: s.Attr,\n\t\tisHTML:     true,\n\t}\n}\n\n// NewXMLElementFromXMLNode creates a XMLElement from a xmlquery.Node.\nfunc NewXMLElementFromXMLNode(resp *Response, s *xmlquery.Node) *XMLElement {\n\treturn &XMLElement{\n\t\tName:       s.Data,\n\t\tRequest:    resp.Request,\n\t\tResponse:   resp,\n\t\tText:       s.InnerText(),\n\t\tDOM:        s,\n\t\tattributes: s.Attr,\n\t\tisHTML:     false,\n\t}\n}\n\n// Attr returns the selected attribute of a HTMLElement or empty string\n// if no attribute found\nfunc (h *XMLElement) Attr(k string) string {\n\tif h.isHTML {\n\t\tfor _, a := range h.attributes.([]html.Attribute) {\n\t\t\tif a.Key == k {\n\t\t\t\treturn a.Val\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor _, a := range h.attributes.([]xmlquery.Attr) {\n\t\t\tif a.Name.Local == k {\n\t\t\t\treturn a.Value\n\t\t\t}\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// ChildText returns the concatenated and stripped text content of the matching\n// elements.\nfunc (h *XMLElement) ChildText(xpathQuery string) string {\n\tif h.isHTML {\n\t\tchild := htmlquery.FindOne(h.DOM.(*html.Node), xpathQuery)\n\t\tif child == nil {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn strings.TrimSpace(htmlquery.InnerText(child))\n\t}\n\tchild := xmlquery.FindOne(h.DOM.(*xmlquery.Node), xpathQuery)\n\tif child == nil {\n\t\treturn \"\"\n\t}\n\treturn strings.TrimSpace(child.InnerText())\n\n}\n\n// ChildAttr returns the stripped text content of the first matching\n// element's attribute.\nfunc (h *XMLElement) ChildAttr(xpathQuery, attrName string) string {\n\tif h.isHTML {\n\t\tchild := htmlquery.FindOne(h.DOM.(*html.Node), xpathQuery)\n\t\tif child != nil {\n\t\t\tfor _, attr := range child.Attr {\n\t\t\t\tif attr.Key == attrName {\n\t\t\t\t\treturn strings.TrimSpace(attr.Val)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tchild := xmlquery.FindOne(h.DOM.(*xmlquery.Node), xpathQuery)\n\t\tif child != nil {\n\t\t\tfor _, attr := range child.Attr {\n\t\t\t\tif attr.Name.Local == attrName {\n\t\t\t\t\treturn strings.TrimSpace(attr.Value)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn \"\"\n}\n\n// ChildAttrs returns the stripped text content of all the matching\n// element's attributes.\nfunc (h *XMLElement) ChildAttrs(xpathQuery, attrName string) []string {\n\tvar res []string\n\tif h.isHTML {\n\t\tfor _, child := range htmlquery.Find(h.DOM.(*html.Node), xpathQuery) {\n\t\t\tfor _, attr := range child.Attr {\n\t\t\t\tif attr.Key == attrName {\n\t\t\t\t\tres = append(res, strings.TrimSpace(attr.Val))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\txmlquery.FindEach(h.DOM.(*xmlquery.Node), xpathQuery, func(i int, child *xmlquery.Node) {\n\t\t\tfor _, attr := range child.Attr {\n\t\t\t\tif attr.Name.Local == attrName {\n\t\t\t\t\tres = append(res, strings.TrimSpace(attr.Value))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\treturn res\n}\n\n// ChildTexts returns an array of strings corresponding to child elements that match the xpath query.\n// Each item in the array is the stripped text content of the corresponding matching child element.\nfunc (h *XMLElement) ChildTexts(xpathQuery string) []string {\n\ttexts := make([]string, 0)\n\tif h.isHTML {\n\t\tfor _, child := range htmlquery.Find(h.DOM.(*html.Node), xpathQuery) {\n\t\t\ttexts = append(texts, strings.TrimSpace(htmlquery.InnerText(child)))\n\t\t}\n\t} else {\n\t\txmlquery.FindEach(h.DOM.(*xmlquery.Node), xpathQuery, func(i int, child *xmlquery.Node) {\n\t\t\ttexts = append(texts, strings.TrimSpace(child.InnerText()))\n\t\t})\n\t}\n\treturn texts\n}\n"
        },
        {
          "name": "xmlelement_test.go",
          "type": "blob",
          "size": 4.1982421875,
          "content": "// Copyright 2018 Adam Tauber\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colly_test\n\nimport (\n\t\"github.com/antchfx/htmlquery\"\n\t\"github.com/gocolly/colly/v2\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n)\n\n// Borrowed from http://infohost.nmt.edu/tcc/help/pubs/xhtml/example.html\n// Added attributes to the `<li>` tags for testing purposes\nconst htmlPage = `\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\"\n \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\">\n  <head>\n    <title>Your page title here</title>\n  </head>\n  <body>\n    <h1>Your major heading here</h1>\n    <p>\n      This is a regular text paragraph.\n    </p>\n    <ul>\n      <li class=\"list-item-1\">\n        First bullet of a bullet list.\n      </li>\n      <li class=\"list-item-2\">\n        This is the <em>second</em> bullet.\n      </li>\n    </ul>\n  </body>\n</html>\n`\n\nfunc TestAttr(t *testing.T) {\n\tresp := &colly.Response{StatusCode: 200, Body: []byte(htmlPage)}\n\tdoc, _ := htmlquery.Parse(strings.NewReader(htmlPage))\n\txmlNode := htmlquery.FindOne(doc, \"/html\")\n\txmlElem := colly.NewXMLElementFromHTMLNode(resp, xmlNode)\n\n\tif xmlElem.Attr(\"xmlns\") != \"http://www.w3.org/1999/xhtml\" {\n\t\tt.Fatalf(\"failed xmlns attribute test: %v != http://www.w3.org/1999/xhtml\", xmlElem.Attr(\"xmlns\"))\n\t}\n\n\tif xmlElem.Attr(\"xml:lang\") != \"en\" {\n\t\tt.Fatalf(\"failed lang attribute test: %v != en\", xmlElem.Attr(\"lang\"))\n\t}\n}\n\nfunc TestChildText(t *testing.T) {\n\tresp := &colly.Response{StatusCode: 200, Body: []byte(htmlPage)}\n\tdoc, _ := htmlquery.Parse(strings.NewReader(htmlPage))\n\txmlNode := htmlquery.FindOne(doc, \"/html\")\n\txmlElem := colly.NewXMLElementFromHTMLNode(resp, xmlNode)\n\n\tif text := xmlElem.ChildText(\"//p\"); text != \"This is a regular text paragraph.\" {\n\t\tt.Fatalf(\"failed child tag test: %v != This is a regular text paragraph.\", text)\n\t}\n\tif text := xmlElem.ChildText(\"//dl\"); text != \"\" {\n\t\tt.Fatalf(\"failed child tag test: %v != \\\"\\\"\", text)\n\t}\n}\n\nfunc TestChildTexts(t *testing.T) {\n\tresp := &colly.Response{StatusCode: 200, Body: []byte(htmlPage)}\n\tdoc, _ := htmlquery.Parse(strings.NewReader(htmlPage))\n\txmlNode := htmlquery.FindOne(doc, \"/html\")\n\txmlElem := colly.NewXMLElementFromHTMLNode(resp, xmlNode)\n\texpected := []string{\"First bullet of a bullet list.\", \"This is the second bullet.\"}\n\tif texts := xmlElem.ChildTexts(\"//li\"); reflect.DeepEqual(texts, expected) == false {\n\t\tt.Fatalf(\"failed child tags test: %v != %v\", texts, expected)\n\t}\n\tif texts := xmlElem.ChildTexts(\"//dl\"); reflect.DeepEqual(texts, make([]string, 0)) == false {\n\t\tt.Fatalf(\"failed child tag test: %v != \\\"\\\"\", texts)\n\t}\n}\nfunc TestChildAttr(t *testing.T) {\n\tresp := &colly.Response{StatusCode: 200, Body: []byte(htmlPage)}\n\tdoc, _ := htmlquery.Parse(strings.NewReader(htmlPage))\n\txmlNode := htmlquery.FindOne(doc, \"/html\")\n\txmlElem := colly.NewXMLElementFromHTMLNode(resp, xmlNode)\n\n\tif attr := xmlElem.ChildAttr(\"/body/ul/li[1]\", \"class\"); attr != \"list-item-1\" {\n\t\tt.Fatalf(\"failed child attribute test: %v != list-item-1\", attr)\n\t}\n\tif attr := xmlElem.ChildAttr(\"/body/ul/li[2]\", \"class\"); attr != \"list-item-2\" {\n\t\tt.Fatalf(\"failed child attribute test: %v != list-item-2\", attr)\n\t}\n}\n\nfunc TestChildAttrs(t *testing.T) {\n\tresp := &colly.Response{StatusCode: 200, Body: []byte(htmlPage)}\n\tdoc, _ := htmlquery.Parse(strings.NewReader(htmlPage))\n\txmlNode := htmlquery.FindOne(doc, \"/html\")\n\txmlElem := colly.NewXMLElementFromHTMLNode(resp, xmlNode)\n\n\tattrs := xmlElem.ChildAttrs(\"/body/ul/li\", \"class\")\n\tif len(attrs) != 2 {\n\t\tt.Fatalf(\"failed child attributes length test: %d != 2\", len(attrs))\n\t}\n\n\tfor _, attr := range attrs {\n\t\tif !(attr == \"list-item-1\" || attr == \"list-item-2\") {\n\t\t\tt.Fatalf(\"failed child attributes values test: %s != list-item-(1 or 2)\", attr)\n\t\t}\n\t}\n}\n"
        }
      ]
    }
  ]
}