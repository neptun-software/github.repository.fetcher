{
  "metadata": {
    "timestamp": 1736568103279,
    "page": 241,
    "hasNextPage": false,
    "endCursor": "Y3Vyc29yOjI0OA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "blevesearch/bleve",
      "stars": 10183,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.26953125,
          "content": "#*\n*.sublime-*\n*~\n.#*\n.project\n.settings\n**/.idea/\n**/*.iml\n.DS_Store\nquery_string.y.go.tmp\n/analysis/token_filters/cld2/cld2-read-only\n/analysis/token_filters/cld2/libcld2_full.a\n/cmd/bleve/bleve\nvendor/**\n!vendor/manifest\n/y.output\n/search/query/y.output\n*.test\ntags\ngo.sum\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.5576171875,
          "content": "sudo: false\n\nlanguage: go\n\ngo:\n - \"1.12.x\"\n - \"1.13.x\"\n - \"1.14.x\"\n\nscript:\n  - go get golang.org/x/tools/cmd/cover\n  - go get github.com/mattn/goveralls\n  - go get github.com/kisielk/errcheck\n  - go get -u github.com/FiloSottile/gvt\n  - gvt restore\n  - go test -race -v $(go list ./... | grep -v vendor/)\n  - go vet $(go list ./... | grep -v vendor/)\n  - go test ./test -v -indexType scorch\n  - errcheck -ignorepkg fmt $(go list ./... | grep -v vendor/);\n  - docs/project-code-coverage.sh\n  - docs/build_children.sh\n\nnotifications:\n  email:\n    - marty.schoch@gmail.com\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.9912109375,
          "content": "# Contributing to Bleve\n\nWe look forward to your contributions, but ask that you first review these guidelines.\n\n### Sign the CLA\n\nAs Bleve is a Couchbase project we require contributors accept the [Couchbase Contributor License Agreement](http://review.couchbase.org/static/individual_agreement.html). To sign this agreement log into the Couchbase [code review tool](http://review.couchbase.org/). The Bleve project does not use this code review tool but it is still used to track acceptance of the contributor license agreements.\n\n### Submitting a Pull Request\n\nAll types of contributions are welcome, but please keep the following in mind:\n\n- If you're planning a large change, you should really discuss it in a github issue or on the google group first. This helps avoid duplicate effort and spending time on something that may not be merged.\n- Existing tests should continue to pass, new tests for the contribution are nice to have.\n- All code should have gone through `go fmt`\n- All code should pass `go vet`\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.8427734375,
          "content": "# ![bleve](docs/bleve.png) bleve\n\n[![Tests](https://github.com/blevesearch/bleve/workflows/Tests/badge.svg?branch=master&event=push)](https://github.com/blevesearch/bleve/actions?query=workflow%3ATests+event%3Apush+branch%3Amaster)\n[![Coverage Status](https://coveralls.io/repos/github/blevesearch/bleve/badge.svg?branch=master)](https://coveralls.io/github/blevesearch/bleve?branch=master)\n[![GoDoc](https://godoc.org/github.com/blevesearch/bleve?status.svg)](https://godoc.org/github.com/blevesearch/bleve)\n[![Join the chat at https://gitter.im/blevesearch/bleve](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/blevesearch/bleve?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![codebeat](https://codebeat.co/badges/38a7cbc9-9cf5-41c0-a315-0746178230f4)](https://codebeat.co/projects/github-com-blevesearch-bleve)\n[![Go Report Card](https://goreportcard.com/badge/blevesearch/bleve)](https://goreportcard.com/report/blevesearch/bleve)\n[![Sourcegraph](https://sourcegraph.com/github.com/blevesearch/bleve/-/badge.svg)](https://sourcegraph.com/github.com/blevesearch/bleve?badge)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nA modern indexing + search library in GO\n\n## Features\n\n* Index any GO data structure or JSON\n* Intelligent defaults backed up by powerful configuration ([scorch](https://github.com/blevesearch/bleve/blob/master/index/scorch/README.md))\n* Supported field types:\n    * `text`, `number`, `datetime`, `boolean`, `geopoint`, `geoshape`, `IP`, `vector`\n* Supported query types:\n    * `term`, `phrase`, `match`, `match_phrase`, `prefix`, `regexp`, `wildcard`, `fuzzy`\n    * term range, numeric range, date range, boolean field\n    * compound queries: `conjuncts`, `disjuncts`, boolean (`must`/`should`/`must_not`)\n    * [query string syntax](http://www.blevesearch.com/docs/Query-String-Query/)\n    * [geo spatial search](https://github.com/blevesearch/bleve/blob/master/geo/README.md)\n    * approximate k-nearest neighbors via [vector search](https://github.com/blevesearch/bleve/blob/master/docs/vectors.md)\n* [tf-idf](https://en.wikipedia.org/wiki/Tf-idf) scoring\n* Hybrid search: exact + semantic\n* Query time boosting\n* Search result match highlighting with document fragments\n* Aggregations/faceting support:\n    * terms facet\n    * numeric range facet\n    * date range facet\n\n## Indexing\n\n```go\nmessage := struct{\n\tId   string\n\tFrom string\n\tBody string\n}{\n\tId:   \"example\",\n\tFrom: \"marty.schoch@gmail.com\",\n\tBody: \"bleve indexing is easy\",\n}\n\nmapping := bleve.NewIndexMapping()\nindex, err := bleve.New(\"example.bleve\", mapping)\nif err != nil {\n\tpanic(err)\n}\nindex.Index(message.Id, message)\n```\n\n## Querying\n\n```go\nindex, _ := bleve.Open(\"example.bleve\")\nquery := bleve.NewQueryStringQuery(\"bleve\")\nsearchRequest := bleve.NewSearchRequest(query)\nsearchResult, _ := index.Search(searchRequest)\n```\n\n## Command Line Interface\n\nTo install the CLI for the latest release of bleve, run:\n\n```bash\n$ go install github.com/blevesearch/bleve/v2/cmd/bleve@latest\n```\n\n```\n$ bleve --help\nBleve is a command-line tool to interact with a bleve index.\n\nUsage:\n  bleve [command]\n\nAvailable Commands:\n  bulk        bulk loads from newline delimited JSON files\n  check       checks the contents of the index\n  count       counts the number documents in the index\n  create      creates a new index\n  dictionary  prints the term dictionary for the specified field in the index\n  dump        dumps the contents of the index\n  fields      lists the fields in this index\n  help        Help about any command\n  index       adds the files to the index\n  mapping     prints the mapping used for this index\n  query       queries the index\n  registry    registry lists the bleve components compiled into this executable\n  scorch      command-line tool to interact with a scorch index\n\nFlags:\n  -h, --help   help for bleve\n\nUse \"bleve [command] --help\" for more information about a command.\n```\n\n## Text Analysis\n\nBleve includes general-purpose analyzers (customizable) as well as pre-built text analyzers for the following languages:\n\nArabic (ar), Bulgarian (bg), Catalan (ca), Chinese-Japanese-Korean (cjk), Kurdish (ckb), Danish (da), German (de), Greek (el), English (en), Spanish - Castilian (es), Basque (eu), Persian (fa), Finnish (fi), French (fr), Gaelic (ga), Spanish - Galician (gl), Hindi (hi), Croatian (hr), Hungarian (hu), Armenian (hy), Indonesian (id, in), Italian (it), Dutch (nl), Norwegian (no), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Swedish (sv), Turkish (tr)\n\n## Text Analysis Wizard\n\n[bleveanalysis.couchbase.com](https://bleveanalysis.couchbase.com)\n\n## Discussion/Issues\n\nDiscuss usage/development of bleve and/or report issues here:\n* [Github issues](https://github.com/blevesearch/bleve/issues)\n* [Google group](https://groups.google.com/forum/#!forum/bleve)\n\n## License\n\nApache License Version 2.0\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.6513671875,
          "content": "# Security Policy\n\n## Supported Versions\n\nWe support the latest release (for example, bleve v2.3.x).\n\n## Reporting a Vulnerability\n\nAll security issues for this project should be reported by email to security@couchbase.com and fts-team@couchbase.com. \nThis mail will be delivered to the owners of this project.\n\n- To ensure your report is NOT marked as spam, please include the word \"security/vulnerability\" along with the project name (blevesearch/bleve) in the subject of the email.\n- Please be as descriptive as possible while explaining the issue, and a testcase highlighting the issue is always welcome.\n\nYour email will be acknowledged at the soonest possible.\n"
        },
        {
          "name": "analysis",
          "type": "tree",
          "content": null
        },
        {
          "name": "builder.go",
          "type": "blob",
          "size": 2.1962890625,
          "content": "//  Copyright (c) 2019 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/util\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\ntype builderImpl struct {\n\tb index.IndexBuilder\n\tm mapping.IndexMapping\n}\n\nfunc (b *builderImpl) Index(id string, data interface{}) error {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\n\tdoc := document.NewDocument(id)\n\terr := b.m.MapDocument(doc, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = b.b.Index(doc)\n\treturn err\n}\n\nfunc (b *builderImpl) Close() error {\n\treturn b.b.Close()\n}\n\nfunc newBuilder(path string, mapping mapping.IndexMapping, config map[string]interface{}) (Builder, error) {\n\tif path == \"\" {\n\t\treturn nil, fmt.Errorf(\"builder requires path\")\n\t}\n\n\terr := mapping.Validate()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif config == nil {\n\t\tconfig = map[string]interface{}{}\n\t}\n\n\t// the builder does not have an API to interact with internal storage\n\t// however we can pass k/v pairs through the config\n\tmappingBytes, err := util.MarshalJSON(mapping)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tconfig[\"internal\"] = map[string][]byte{\n\t\tstring(mappingInternalKey): mappingBytes,\n\t}\n\n\t// do not use real config, as these are options for the builder,\n\t// not the resulting index\n\tmeta := newIndexMeta(scorch.Name, scorch.Name, map[string]interface{}{})\n\terr = meta.Save(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconfig[\"path\"] = indexStorePath(path)\n\n\tb, err := scorch.NewBuilder(config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trv := &builderImpl{\n\t\tb: b,\n\t\tm: mapping,\n\t}\n\n\treturn rv, nil\n}\n"
        },
        {
          "name": "builder_test.go",
          "type": "blob",
          "size": 1.87109375,
          "content": "//  Copyright (c) 2019 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n)\n\nfunc TestBuilder(t *testing.T) {\n\ttmpDir, err := os.MkdirTemp(\"\", \"bleve-scorch-builder-test\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = os.RemoveAll(tmpDir)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error cleaning up test index\")\n\t\t}\n\t}()\n\n\tconf := map[string]interface{}{\n\t\t\"batchSize\": 2,\n\t\t\"mergeMax\":  2,\n\t}\n\tb, err := NewBuilder(tmpDir, NewIndexMapping(), conf)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor i := 0; i < 10; i++ {\n\t\tdoc := map[string]interface{}{\n\t\t\t\"name\": \"hello\",\n\t\t}\n\t\terr = b.Index(fmt.Sprintf(\"%d\", i), doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\terr = b.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidx, err := Open(tmpDir)\n\tif err != nil {\n\t\tt.Fatalf(\"error opening index: %v\", err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error closing index: %v\", err)\n\t\t}\n\t}()\n\n\tdocCount, err := idx.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"error checking doc count: %v\", err)\n\t}\n\tif docCount != 10 {\n\t\tt.Errorf(\"expected doc count to be 10, got %d\", docCount)\n\t}\n\n\tq := NewTermQuery(\"hello\")\n\tq.SetField(\"name\")\n\treq := NewSearchRequest(q)\n\tres, err := idx.Search(req)\n\tif err != nil {\n\t\tt.Errorf(\"error searching index: %v\", err)\n\t}\n\tif res.Total != 10 {\n\t\tt.Errorf(\"expected 10 search hits, got %d\", res.Total)\n\t}\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 2.3759765625,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"expvar\"\n\t\"io\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown/store/gtreap\"\n\t\"github.com/blevesearch/bleve/v2/registry\"\n\t\"github.com/blevesearch/bleve/v2/search/highlight/highlighter/html\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nvar bleveExpVar = expvar.NewMap(\"bleve\")\n\ntype configuration struct {\n\tCache                  *registry.Cache\n\tDefaultHighlighter     string\n\tDefaultKVStore         string\n\tDefaultMemKVStore      string\n\tDefaultIndexType       string\n\tSlowSearchLogThreshold time.Duration\n\tanalysisQueue          *index.AnalysisQueue\n}\n\nfunc (c *configuration) SetAnalysisQueueSize(n int) {\n\tif c.analysisQueue != nil {\n\t\tc.analysisQueue.Close()\n\t}\n\tc.analysisQueue = index.NewAnalysisQueue(n)\n}\n\nfunc (c *configuration) Shutdown() {\n\tc.SetAnalysisQueueSize(0)\n}\n\nfunc newConfiguration() *configuration {\n\treturn &configuration{\n\t\tCache:         registry.NewCache(),\n\t\tanalysisQueue: index.NewAnalysisQueue(4),\n\t}\n}\n\n// Config contains library level configuration\nvar Config *configuration\n\nfunc init() {\n\tbootStart := time.Now()\n\n\t// build the default configuration\n\tConfig = newConfiguration()\n\n\t// set the default highlighter\n\tConfig.DefaultHighlighter = html.Name\n\n\t// default kv store\n\tConfig.DefaultKVStore = \"\"\n\n\t// default mem only kv store\n\tConfig.DefaultMemKVStore = gtreap.Name\n\n\t// default index\n\tConfig.DefaultIndexType = scorch.Name\n\n\tbootDuration := time.Since(bootStart)\n\tbleveExpVar.Add(\"bootDuration\", int64(bootDuration))\n\tindexStats = NewIndexStats()\n\tbleveExpVar.Set(\"indexes\", indexStats)\n\n\tinitDisk()\n}\n\nvar logger = log.New(io.Discard, \"bleve\", log.LstdFlags)\n\n// SetLog sets the logger used for logging\n// by default log messages are sent to io.Discard\nfunc SetLog(l *log.Logger) {\n\tlogger = l\n}\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "config_app.go",
          "type": "blob",
          "size": 0.80078125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build appengine || appenginevm\n// +build appengine appenginevm\n\npackage bleve\n\n// in the appengine environment we cannot support disk based indexes\n// so we do no extra configuration in this method\nfunc initDisk() {\n\n}\n"
        },
        {
          "name": "config_disk.go",
          "type": "blob",
          "size": 0.8818359375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !appengine && !appenginevm\n// +build !appengine,!appenginevm\n\npackage bleve\n\nimport \"github.com/blevesearch/bleve/v2/index/upsidedown/store/boltdb\"\n\n// in normal environments we configure boltdb as the default storage\nfunc initDisk() {\n\t// default kv store\n\tConfig.DefaultKVStore = boltdb.Name\n}\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 1.16015625,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/*\nPackage bleve is a library for indexing and searching text.\n\nExample Opening New Index, Indexing Data\n\n\tmessage := struct{\n\t    Id:   \"example\"\n\t    From: \"marty.schoch@gmail.com\",\n\t    Body: \"bleve indexing is easy\",\n\t}\n\n\tmapping := bleve.NewIndexMapping()\n\tindex, _ := bleve.New(\"example.bleve\", mapping)\n\tindex.Index(message.Id, message)\n\nExample Opening Existing Index, Searching Data\n\n\tindex, _ := bleve.Open(\"example.bleve\")\n\tquery := bleve.NewQueryStringQuery(\"bleve\")\n\tsearchRequest := bleve.NewSearchRequest(query)\n\tsearchResult, _ := index.Search(searchRequest)\n*/\npackage bleve\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "document",
          "type": "tree",
          "content": null
        },
        {
          "name": "error.go",
          "type": "blob",
          "size": 2.0771484375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\n// Constant Error values which can be compared to determine the type of error\nconst (\n\tErrorIndexPathExists Error = iota\n\tErrorIndexPathDoesNotExist\n\tErrorIndexMetaMissing\n\tErrorIndexMetaCorrupt\n\tErrorIndexClosed\n\tErrorAliasMulti\n\tErrorAliasEmpty\n\tErrorUnknownIndexType\n\tErrorEmptyID\n\tErrorIndexReadInconsistency\n\tErrorTwoPhaseSearchInconsistency\n\tErrorSynonymSearchNotSupported\n)\n\n// Error represents a more strongly typed bleve error for detecting\n// and handling specific types of errors.\ntype Error int\n\nfunc (e Error) Error() string {\n\treturn errorMessages[e]\n}\n\nvar errorMessages = map[Error]string{\n\tErrorIndexPathExists:             \"cannot create new index, path already exists\",\n\tErrorIndexPathDoesNotExist:       \"cannot open index, path does not exist\",\n\tErrorIndexMetaMissing:            \"cannot open index, metadata missing\",\n\tErrorIndexMetaCorrupt:            \"cannot open index, metadata corrupt\",\n\tErrorIndexClosed:                 \"index is closed\",\n\tErrorAliasMulti:                  \"cannot perform single index operation on multiple index alias\",\n\tErrorAliasEmpty:                  \"cannot perform operation on empty alias\",\n\tErrorUnknownIndexType:            \"unknown index type\",\n\tErrorEmptyID:                     \"document ID cannot be empty\",\n\tErrorIndexReadInconsistency:      \"index read inconsistency detected\",\n\tErrorTwoPhaseSearchInconsistency: \"2-phase search failed, likely due to an overlapping topology change\",\n\tErrorSynonymSearchNotSupported:   \"synonym search not supported\",\n}\n"
        },
        {
          "name": "examples_test.go",
          "type": "blob",
          "size": 10.958984375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/highlight/highlighter/ansi\"\n)\n\nvar indexMapping mapping.IndexMapping\nvar exampleIndex Index\nvar err error\n\nfunc TestMain(m *testing.M) {\n\terr = os.RemoveAll(\"path_to_index\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\ttoRun := m.Run()\n\tif exampleIndex != nil {\n\t\terr = exampleIndex.Close()\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\terr = os.RemoveAll(\"path_to_index\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tos.Exit(toRun)\n}\n\nfunc ExampleNew() {\n\tindexMapping = NewIndexMapping()\n\texampleIndex, err = New(\"path_to_index\", indexMapping)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tcount, err := exampleIndex.DocCount()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(count)\n\t// Output:\n\t// 0\n}\n\nfunc ExampleIndex_indexing() {\n\tdata := struct {\n\t\tName    string\n\t\tCreated time.Time\n\t\tAge     int\n\t}{Name: \"named one\", Created: time.Now(), Age: 50}\n\tdata2 := struct {\n\t\tName    string\n\t\tCreated time.Time\n\t\tAge     int\n\t}{Name: \"great nameless one\", Created: time.Now(), Age: 25}\n\n\t// index some data\n\terr = exampleIndex.Index(\"document id 1\", data)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\terr = exampleIndex.Index(\"document id 2\", data2)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// 2 documents have been indexed\n\tcount, err := exampleIndex.DocCount()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(count)\n\t// Output:\n\t// 2\n}\n\n// Examples for query related functions\n\nfunc ExampleNewMatchQuery() {\n\t// finds documents with fields fully matching the given query text\n\tquery := NewMatchQuery(\"named one\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 1\n}\n\nfunc ExampleNewMatchAllQuery() {\n\t// finds all documents in the index\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(len(searchResults.Hits))\n\t// Output:\n\t// 2\n}\n\nfunc ExampleNewMatchNoneQuery() {\n\t// matches no documents in the index\n\tquery := NewMatchNoneQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(len(searchResults.Hits))\n\t// Output:\n\t// 0\n}\n\nfunc ExampleNewMatchPhraseQuery() {\n\t// finds all documents with the given phrase in the index\n\tquery := NewMatchPhraseQuery(\"nameless one\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 2\n}\n\nfunc ExampleNewNumericRangeQuery() {\n\tvalue1 := float64(11)\n\tvalue2 := float64(100)\n\tdata := struct{ Priority float64 }{Priority: float64(15)}\n\tdata2 := struct{ Priority float64 }{Priority: float64(10)}\n\n\terr = exampleIndex.Index(\"document id 3\", data)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\terr = exampleIndex.Index(\"document id 4\", data2)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tquery := NewNumericRangeQuery(&value1, &value2)\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 3\n}\n\nfunc ExampleNewNumericRangeInclusiveQuery() {\n\tvalue1 := float64(10)\n\tvalue2 := float64(100)\n\tv1incl := false\n\tv2incl := false\n\n\tquery := NewNumericRangeInclusiveQuery(&value1, &value2, &v1incl, &v2incl)\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 3\n}\n\nfunc ExampleNewPhraseQuery() {\n\t// finds all documents with the given phrases in the given field in the index\n\tquery := NewPhraseQuery([]string{\"nameless\", \"one\"}, \"Name\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 2\n}\n\nfunc ExampleNewPrefixQuery() {\n\t// finds all documents with terms having the given prefix in the index\n\tquery := NewPrefixQuery(\"name\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(len(searchResults.Hits))\n\t// Output:\n\t// 2\n}\n\nfunc ExampleNewQueryStringQuery() {\n\tquery := NewQueryStringQuery(\"+one -great\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 1\n}\n\nfunc ExampleNewTermQuery() {\n\tquery := NewTermQuery(\"great\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 2\n}\n\nfunc ExampleNewFacetRequest() {\n\tfacet := NewFacetRequest(\"Name\", 1)\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.AddFacet(\"facet name\", facet)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// total number of terms\n\tfmt.Println(searchResults.Facets[\"facet name\"].Total)\n\t// numer of docs with no value for this field\n\tfmt.Println(searchResults.Facets[\"facet name\"].Missing)\n\t// term with highest occurrences in field name\n\tfmt.Println(searchResults.Facets[\"facet name\"].Terms.Terms()[0].Term)\n\t// Output:\n\t// 5\n\t// 2\n\t// one\n}\n\nfunc ExampleFacetRequest_AddDateTimeRange() {\n\tfacet := NewFacetRequest(\"Created\", 1)\n\tfacet.AddDateTimeRange(\"range name\", time.Unix(0, 0), time.Now())\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.AddFacet(\"facet name\", facet)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// dates in field Created since starting of unix time till now\n\tfmt.Println(searchResults.Facets[\"facet name\"].DateRanges[0].Count)\n\t// Output:\n\t// 2\n}\n\nfunc ExampleFacetRequest_AddNumericRange() {\n\tvalue1 := float64(11)\n\n\tfacet := NewFacetRequest(\"Priority\", 1)\n\tfacet.AddNumericRange(\"range name\", &value1, nil)\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.AddFacet(\"facet name\", facet)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// number documents with field Priority in the given range\n\tfmt.Println(searchResults.Facets[\"facet name\"].NumericRanges[0].Count)\n\t// Output:\n\t// 1\n}\n\nfunc ExampleNewHighlight() {\n\tquery := NewMatchQuery(\"nameless\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.Highlight = NewHighlight()\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].Fragments[\"Name\"][0])\n\t// Output:\n\t// great <mark>nameless</mark> one\n}\n\nfunc ExampleNewHighlightWithStyle() {\n\tquery := NewMatchQuery(\"nameless\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.Highlight = NewHighlightWithStyle(ansi.Name)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].Fragments[\"Name\"][0])\n\t// Output:\n\t// great \u001b[43mnameless\u001b[0m one\n}\n\nfunc ExampleSearchRequest_AddFacet() {\n\tfacet := NewFacetRequest(\"Name\", 1)\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.AddFacet(\"facet name\", facet)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// total number of terms\n\tfmt.Println(searchResults.Facets[\"facet name\"].Total)\n\t// numer of docs with no value for this field\n\tfmt.Println(searchResults.Facets[\"facet name\"].Missing)\n\t// term with highest occurrences in field name\n\tfmt.Println(searchResults.Facets[\"facet name\"].Terms.Terms()[0].Term)\n\t// Output:\n\t// 5\n\t// 2\n\t// one\n}\n\nfunc ExampleNewSearchRequest() {\n\t// finds documents with fields fully matching the given query text\n\tquery := NewMatchQuery(\"named one\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 1\n}\n\nfunc ExampleNewBooleanQuery() {\n\tmust := NewMatchQuery(\"one\")\n\tmustNot := NewMatchQuery(\"great\")\n\tquery := NewBooleanQuery()\n\tquery.AddMust(must)\n\tquery.AddMustNot(mustNot)\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 1\n}\n\nfunc ExampleNewConjunctionQuery() {\n\tconjunct1 := NewMatchQuery(\"great\")\n\tconjunct2 := NewMatchQuery(\"one\")\n\tquery := NewConjunctionQuery(conjunct1, conjunct2)\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\t// Output:\n\t// document id 2\n}\n\nfunc ExampleNewDisjunctionQuery() {\n\tdisjunct1 := NewMatchQuery(\"great\")\n\tdisjunct2 := NewMatchQuery(\"named\")\n\tquery := NewDisjunctionQuery(disjunct1, disjunct2)\n\tsearchRequest := NewSearchRequest(query)\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(len(searchResults.Hits))\n\t// Output:\n\t// 2\n}\n\nfunc ExampleSearchRequest_SortBy() {\n\t// find docs containing \"one\", order by Age instead of score\n\tquery := NewMatchQuery(\"one\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.SortBy([]string{\"Age\"})\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\tfmt.Println(searchResults.Hits[1].ID)\n\t// Output:\n\t// document id 2\n\t// document id 1\n}\n\nfunc ExampleSearchRequest_SortByCustom() {\n\t// find all docs, order by Age, with docs missing Age field first\n\tquery := NewMatchAllQuery()\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.SortByCustom(search.SortOrder{\n\t\t&search.SortField{\n\t\t\tField:   \"Age\",\n\t\t\tMissing: search.SortFieldMissingFirst,\n\t\t},\n\t\t&search.SortDocID{},\n\t})\n\tsearchResults, err := exampleIndex.Search(searchRequest)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(searchResults.Hits[0].ID)\n\tfmt.Println(searchResults.Hits[1].ID)\n\tfmt.Println(searchResults.Hits[2].ID)\n\tfmt.Println(searchResults.Hits[3].ID)\n\t// Output:\n\t// document id 3\n\t// document id 4\n\t// document id 2\n\t// document id 1\n}\n"
        },
        {
          "name": "geo",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.6533203125,
          "content": "module github.com/blevesearch/bleve/v2\n\ngo 1.21\n\nrequire (\n\tgithub.com/RoaringBitmap/roaring v1.9.3\n\tgithub.com/bits-and-blooms/bitset v1.12.0\n\tgithub.com/blevesearch/bleve_index_api v1.2.0\n\tgithub.com/blevesearch/geo v0.1.20\n\tgithub.com/blevesearch/go-faiss v1.0.24\n\tgithub.com/blevesearch/go-metrics v0.0.0-20201227073835-cf1acfcdf475\n\tgithub.com/blevesearch/go-porterstemmer v1.0.3\n\tgithub.com/blevesearch/goleveldb v1.0.1\n\tgithub.com/blevesearch/gtreap v0.1.1\n\tgithub.com/blevesearch/scorch_segment_api/v2 v2.3.0\n\tgithub.com/blevesearch/segment v0.9.1\n\tgithub.com/blevesearch/snowball v0.6.1\n\tgithub.com/blevesearch/snowballstem v0.9.0\n\tgithub.com/blevesearch/stempel v0.2.0\n\tgithub.com/blevesearch/upsidedown_store_api v1.0.2\n\tgithub.com/blevesearch/vellum v1.1.0\n\tgithub.com/blevesearch/zapx/v11 v11.3.10\n\tgithub.com/blevesearch/zapx/v12 v12.3.10\n\tgithub.com/blevesearch/zapx/v13 v13.3.10\n\tgithub.com/blevesearch/zapx/v14 v14.3.10\n\tgithub.com/blevesearch/zapx/v15 v15.3.17\n\tgithub.com/blevesearch/zapx/v16 v16.1.11-0.20241219160422-82553cdd4b38\n\tgithub.com/couchbase/moss v0.2.0\n\tgithub.com/golang/protobuf v1.3.2\n\tgithub.com/spf13/cobra v1.7.0\n\tgo.etcd.io/bbolt v1.3.7\n\tgolang.org/x/text v0.8.0\n)\n\nrequire (\n\tgithub.com/blevesearch/mmap-go v1.0.4 // indirect\n\tgithub.com/couchbase/ghistogram v0.1.0 // indirect\n\tgithub.com/golang/geo v0.0.0-20210211234256-740aa86cb551 // indirect\n\tgithub.com/golang/snappy v0.0.4 // indirect\n\tgithub.com/inconshreveable/mousetrap v1.1.0 // indirect\n\tgithub.com/json-iterator/go v0.0.0-20171115153421-f7279a603ede // indirect\n\tgithub.com/mschoch/smat v0.2.0 // indirect\n\tgithub.com/spf13/pflag v1.0.5 // indirect\n\tgolang.org/x/sys v0.13.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 10.30859375,
          "content": "github.com/RoaringBitmap/roaring v1.9.3 h1:t4EbC5qQwnisr5PrP9nt0IRhRTb9gMUgQF4t4S2OByM=\ngithub.com/RoaringBitmap/roaring v1.9.3/go.mod h1:6AXUsoIEzDTFFQCe1RbGA6uFONMhvejWj5rqITANK90=\ngithub.com/bits-and-blooms/bitset v1.12.0 h1:U/q1fAF7xXRhFCrhROzIfffYnu+dlS38vCZtmFVPHmA=\ngithub.com/bits-and-blooms/bitset v1.12.0/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=\ngithub.com/blevesearch/bleve_index_api v1.2.0 h1:/DXMMWBwx/UmGKM1xDhTwDoJI5yQrG6rqRWPFcOgUVo=\ngithub.com/blevesearch/bleve_index_api v1.2.0/go.mod h1:PbcwjIcRmjhGbkS/lJCpfgVSMROV6TRubGGAODaK1W8=\ngithub.com/blevesearch/geo v0.1.20 h1:paaSpu2Ewh/tn5DKn/FB5SzvH0EWupxHEIwbCk/QPqM=\ngithub.com/blevesearch/geo v0.1.20/go.mod h1:DVG2QjwHNMFmjo+ZgzrIq2sfCh6rIHzy9d9d0B59I6w=\ngithub.com/blevesearch/go-faiss v1.0.24 h1:K79IvKjoKHdi7FdiXEsAhxpMuns0x4fM0BO93bW5jLI=\ngithub.com/blevesearch/go-faiss v1.0.24/go.mod h1:OMGQwOaRRYxrmeNdMrXJPvVx8gBnvE5RYrr0BahNnkk=\ngithub.com/blevesearch/go-metrics v0.0.0-20201227073835-cf1acfcdf475 h1:kDy+zgJFJJoJYBvdfBSiZYBbdsUL0XcjHYWezpQBGPA=\ngithub.com/blevesearch/go-metrics v0.0.0-20201227073835-cf1acfcdf475/go.mod h1:9eJDeqxJ3E7WnLebQUlPD7ZjSce7AnDb9vjGmMCbD0A=\ngithub.com/blevesearch/go-porterstemmer v1.0.3 h1:GtmsqID0aZdCSNiY8SkuPJ12pD4jI+DdXTAn4YRcHCo=\ngithub.com/blevesearch/go-porterstemmer v1.0.3/go.mod h1:angGc5Ht+k2xhJdZi511LtmxuEf0OVpvUUNrwmM1P7M=\ngithub.com/blevesearch/goleveldb v1.0.1 h1:iAtV2Cu5s0GD1lwUiekkFHe2gTMCCNVj2foPclDLIFI=\ngithub.com/blevesearch/goleveldb v1.0.1/go.mod h1:WrU8ltZbIp0wAoig/MHbrPCXSOLpe79nz5lv5nqfYrQ=\ngithub.com/blevesearch/gtreap v0.1.1 h1:2JWigFrzDMR+42WGIN/V2p0cUvn4UP3C4Q5nmaZGW8Y=\ngithub.com/blevesearch/gtreap v0.1.1/go.mod h1:QaQyDRAT51sotthUWAH4Sj08awFSSWzgYICSZ3w0tYk=\ngithub.com/blevesearch/mmap-go v1.0.2/go.mod h1:ol2qBqYaOUsGdm7aRMRrYGgPvnwLe6Y+7LMvAB5IbSA=\ngithub.com/blevesearch/mmap-go v1.0.4 h1:OVhDhT5B/M1HNPpYPBKIEJaD0F3Si+CrEKULGCDPWmc=\ngithub.com/blevesearch/mmap-go v1.0.4/go.mod h1:EWmEAOmdAS9z/pi/+Toxu99DnsbhG1TIxUoRmJw/pSs=\ngithub.com/blevesearch/scorch_segment_api/v2 v2.3.0 h1:vxCjbXAkkEBSb4AB3Iqgr/EJcPyYRsiGxpcvsS8E1Dw=\ngithub.com/blevesearch/scorch_segment_api/v2 v2.3.0/go.mod h1:5y+TgXYSx+xJGaCwSlvy9G/UJBIY5wzvIkhvhBm2ATc=\ngithub.com/blevesearch/segment v0.9.1 h1:+dThDy+Lvgj5JMxhmOVlgFfkUtZV2kw49xax4+jTfSU=\ngithub.com/blevesearch/segment v0.9.1/go.mod h1:zN21iLm7+GnBHWTao9I+Au/7MBiL8pPFtJBJTsk6kQw=\ngithub.com/blevesearch/snowball v0.6.1 h1:cDYjn/NCH+wwt2UdehaLpr2e4BwLIjN4V/TdLsL+B5A=\ngithub.com/blevesearch/snowball v0.6.1/go.mod h1:ZF0IBg5vgpeoUhnMza2v0A/z8m1cWPlwhke08LpNusg=\ngithub.com/blevesearch/snowballstem v0.9.0 h1:lMQ189YspGP6sXvZQ4WZ+MLawfV8wOmPoD/iWeNXm8s=\ngithub.com/blevesearch/snowballstem v0.9.0/go.mod h1:PivSj3JMc8WuaFkTSRDW2SlrulNWPl4ABg1tC/hlgLs=\ngithub.com/blevesearch/stempel v0.2.0 h1:CYzVPaScODMvgE9o+kf6D4RJ/VRomyi9uHF+PtB+Afc=\ngithub.com/blevesearch/stempel v0.2.0/go.mod h1:wjeTHqQv+nQdbPuJ/YcvOjTInA2EIc6Ks1FoSUzSLvc=\ngithub.com/blevesearch/upsidedown_store_api v1.0.2 h1:U53Q6YoWEARVLd1OYNc9kvhBMGZzVrdmaozG2MfoB+A=\ngithub.com/blevesearch/upsidedown_store_api v1.0.2/go.mod h1:M01mh3Gpfy56Ps/UXHjEO/knbqyQ1Oamg8If49gRwrQ=\ngithub.com/blevesearch/vellum v1.1.0 h1:CinkGyIsgVlYf8Y2LUQHvdelgXr6PYuvoDIajq6yR9w=\ngithub.com/blevesearch/vellum v1.1.0/go.mod h1:QgwWryE8ThtNPxtgWJof5ndPfx0/YMBh+W2weHKPw8Y=\ngithub.com/blevesearch/zapx/v11 v11.3.10 h1:hvjgj9tZ9DeIqBCxKhi70TtSZYMdcFn7gDb71Xo/fvk=\ngithub.com/blevesearch/zapx/v11 v11.3.10/go.mod h1:0+gW+FaE48fNxoVtMY5ugtNHHof/PxCqh7CnhYdnMzQ=\ngithub.com/blevesearch/zapx/v12 v12.3.10 h1:yHfj3vXLSYmmsBleJFROXuO08mS3L1qDCdDK81jDl8s=\ngithub.com/blevesearch/zapx/v12 v12.3.10/go.mod h1:0yeZg6JhaGxITlsS5co73aqPtM04+ycnI6D1v0mhbCs=\ngithub.com/blevesearch/zapx/v13 v13.3.10 h1:0KY9tuxg06rXxOZHg3DwPJBjniSlqEgVpxIqMGahDE8=\ngithub.com/blevesearch/zapx/v13 v13.3.10/go.mod h1:w2wjSDQ/WBVeEIvP0fvMJZAzDwqwIEzVPnCPrz93yAk=\ngithub.com/blevesearch/zapx/v14 v14.3.10 h1:SG6xlsL+W6YjhX5N3aEiL/2tcWh3DO75Bnz77pSwwKU=\ngithub.com/blevesearch/zapx/v14 v14.3.10/go.mod h1:qqyuR0u230jN1yMmE4FIAuCxmahRQEOehF78m6oTgns=\ngithub.com/blevesearch/zapx/v15 v15.3.17 h1:NkkMI98pYLq/uHnB6YWcITrrLpCVyvZ9iP+AyfpW1Ys=\ngithub.com/blevesearch/zapx/v15 v15.3.17/go.mod h1:vXRQzJJvlGVCdmOD5hg7t7JdjUT5DmDPhsAfjvtzIq8=\ngithub.com/blevesearch/zapx/v16 v16.1.11-0.20241219160422-82553cdd4b38 h1:iJ3Q3sbyo2d0bjfb720RmGjj7cqzh/EdP3528ggDIMY=\ngithub.com/blevesearch/zapx/v16 v16.1.11-0.20241219160422-82553cdd4b38/go.mod h1:JTZseJiEpogtkepKSubIKAmfgbQiOReJXfmjxB1qta4=\ngithub.com/couchbase/ghistogram v0.1.0 h1:b95QcQTCzjTUocDXp/uMgSNQi8oj1tGwnJ4bODWZnps=\ngithub.com/couchbase/ghistogram v0.1.0/go.mod h1:s1Jhy76zqfEecpNWJfWUiKZookAFaiGOEoyzgHt9i7k=\ngithub.com/couchbase/moss v0.2.0 h1:VCYrMzFwEryyhRSeI+/b3tRBSeTpi/8gn5Kf6dxqn+o=\ngithub.com/couchbase/moss v0.2.0/go.mod h1:9MaHIaRuy9pvLPUJxB8sh8OrLfyDczECVL37grCIubs=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.2/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/golang/geo v0.0.0-20210211234256-740aa86cb551 h1:gtexQ/VGyN+VVFRXSFiguSNcXmS6rkKT+X7FdIrTtfo=\ngithub.com/golang/geo v0.0.0-20210211234256-740aa86cb551/go.mod h1:QZ0nwyI2jOfgRAoBvP+ab5aRr7c9x7lhGEJrKvBwjWI=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2 h1:6nsPYzhq5kReh6QImI3k5qWzO4PEbvbIW2cwSfR/6xs=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/snappy v0.0.0-20180518054509-2e65f85255db/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/golang/snappy v0.0.4 h1:yAGX7huGHXlcLOEtBnF4w7FQwA26wojNCwOYAEhLjQM=\ngithub.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=\ngithub.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/hpcloud/tail v1.0.0 h1:nfCOvKYfkgYP8hkirhJocXT2+zOD8yUNjXaWfTlyFKI=\ngithub.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\ngithub.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=\ngithub.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=\ngithub.com/json-iterator/go v0.0.0-20171115153421-f7279a603ede h1:YrgBGwxMRK0Vq0WSCWFaZUnTsrA/PZE/xs1QZh+/edg=\ngithub.com/json-iterator/go v0.0.0-20171115153421-f7279a603ede/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/mschoch/smat v0.2.0 h1:8imxQsjDm8yFEAVBe7azKmKSgzSkZXDuKkSq9374khM=\ngithub.com/mschoch/smat v0.2.0/go.mod h1:kc9mz7DoBKqDyiRL7VZN8KvXQMWeTaVnttLRXOlotKw=\ngithub.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.7.0 h1:WSHQ+IS43OoUrWtD1/bbclrwK8TTH5hzp+umCiuxHgs=\ngithub.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/gomega v1.4.3 h1:RE1xgDvH7imwFD45h+u2SgIfERHlS2yNG4DObb5BSKU=\ngithub.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/spf13/cobra v1.7.0 h1:hyqWnYt1ZQShIddO5kBpj3vu05/++x6tJ6dg8EC572I=\ngithub.com/spf13/cobra v1.7.0/go.mod h1:uLxZILRyS/50WlhOIKD7W6V5bgeIt+4sICxh6uRMrb0=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngo.etcd.io/bbolt v1.3.7 h1:j+zJOnnEjF/kyHlDDgGnVL/AIqIJPq8UoB2GSNfkUfQ=\ngo.etcd.io/bbolt v1.3.7/go.mod h1:N9Mkw9X8x5fupy0IKsmuqVtoGDyxsaDlbk4Rd05IAQw=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd h1:nTDtHvHSdCn1m6ITfMRqtOd/9+7a3s8RBNOZ3eYZzJA=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181221143128-b4a75ba826a6/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.13.0 h1:Af8nKPmuFypiUBjVoU9V20FiaFXOcuZI21p0ycVYYGE=\ngolang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.8.0 h1:57P1ETyNKtuIjB4SRd15iJxuhj8Gc416Y78H3qgMh68=\ngolang.org/x/text v0.8.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/fsnotify.v1 v1.4.7 h1:xOHLXZwVvI9hhs+cLKq5+I5onOuwQLhQwiu63xxlHs4=\ngopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\ngopkg.in/yaml.v2 v2.2.1 h1:mUhvW9EsL+naU5Q3cakzfE91YhliOondGd6ZrsDBHQE=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "http",
          "type": "tree",
          "content": null
        },
        {
          "name": "index.go",
          "type": "blob",
          "size": 12.9375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown\"\n\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/size\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\n// A Batch groups together multiple Index and Delete\n// operations you would like performed at the same\n// time.  The Batch structure is NOT thread-safe.\n// You should only perform operations on a batch\n// from a single thread at a time.  Once batch\n// execution has started, you may not modify it.\ntype Batch struct {\n\tindex    Index\n\tinternal *index.Batch\n\n\tlastDocSize uint64\n\ttotalSize   uint64\n}\n\n// Index adds the specified index operation to the\n// batch.  NOTE: the bleve Index is not updated\n// until the batch is executed.\nfunc (b *Batch) Index(id string, data interface{}) error {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\tif eventIndex, ok := b.index.(index.EventIndex); ok {\n\t\teventIndex.FireIndexEvent()\n\t}\n\tdoc := document.NewDocument(id)\n\terr := b.index.Mapping().MapDocument(doc, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\tb.internal.Update(doc)\n\n\tb.lastDocSize = uint64(doc.Size() +\n\t\tlen(id) + size.SizeOfString) // overhead from internal\n\tb.totalSize += b.lastDocSize\n\n\treturn nil\n}\n\nfunc (b *Batch) IndexSynonym(id string, collection string, definition *SynonymDefinition) error {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\tif eventIndex, ok := b.index.(index.EventIndex); ok {\n\t\teventIndex.FireIndexEvent()\n\t}\n\tsynMap, ok := b.index.Mapping().(mapping.SynonymMapping)\n\tif !ok {\n\t\treturn ErrorSynonymSearchNotSupported\n\t}\n\n\tif err := definition.Validate(); err != nil {\n\t\treturn err\n\t}\n\n\tdoc := document.NewSynonymDocument(id)\n\terr := synMap.MapSynonymDocument(doc, collection, definition.Input, definition.Synonyms)\n\tif err != nil {\n\t\treturn err\n\t}\n\tb.internal.Update(doc)\n\n\tb.lastDocSize = uint64(doc.Size() +\n\t\tlen(id) + size.SizeOfString) // overhead from internal\n\tb.totalSize += b.lastDocSize\n\n\treturn nil\n}\n\nfunc (b *Batch) LastDocSize() uint64 {\n\treturn b.lastDocSize\n}\n\nfunc (b *Batch) TotalDocsSize() uint64 {\n\treturn b.totalSize\n}\n\n// IndexAdvanced adds the specified index operation to the\n// batch which skips the mapping.  NOTE: the bleve Index is not updated\n// until the batch is executed.\nfunc (b *Batch) IndexAdvanced(doc *document.Document) (err error) {\n\tif doc.ID() == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\tb.internal.Update(doc)\n\treturn nil\n}\n\n// Delete adds the specified delete operation to the\n// batch.  NOTE: the bleve Index is not updated until\n// the batch is executed.\nfunc (b *Batch) Delete(id string) {\n\tif id != \"\" {\n\t\tb.internal.Delete(id)\n\t}\n}\n\n// SetInternal adds the specified set internal\n// operation to the batch. NOTE: the bleve Index is\n// not updated until the batch is executed.\nfunc (b *Batch) SetInternal(key, val []byte) {\n\tb.internal.SetInternal(key, val)\n}\n\n// DeleteInternal adds the specified delete internal\n// operation to the batch. NOTE: the bleve Index is\n// not updated until the batch is executed.\nfunc (b *Batch) DeleteInternal(key []byte) {\n\tb.internal.DeleteInternal(key)\n}\n\n// Size returns the total number of operations inside the batch\n// including normal index operations and internal operations.\nfunc (b *Batch) Size() int {\n\treturn len(b.internal.IndexOps) + len(b.internal.InternalOps)\n}\n\n// String prints a user friendly string representation of what\n// is inside this batch.\nfunc (b *Batch) String() string {\n\treturn b.internal.String()\n}\n\n// Reset returns a Batch to the empty state so that it can\n// be re-used in the future.\nfunc (b *Batch) Reset() {\n\tb.internal.Reset()\n\tb.lastDocSize = 0\n\tb.totalSize = 0\n}\n\nfunc (b *Batch) Merge(o *Batch) {\n\tif o != nil && o.internal != nil {\n\t\tb.internal.Merge(o.internal)\n\t\tif o.LastDocSize() > 0 {\n\t\t\tb.lastDocSize = o.LastDocSize()\n\t\t}\n\t\tb.totalSize = uint64(b.internal.TotalDocSize())\n\t}\n}\n\nfunc (b *Batch) SetPersistedCallback(f index.BatchCallback) {\n\tb.internal.SetPersistedCallback(f)\n}\n\nfunc (b *Batch) PersistedCallback() index.BatchCallback {\n\treturn b.internal.PersistedCallback()\n}\n\n// An Index implements all the indexing and searching\n// capabilities of bleve.  An Index can be created\n// using the New() and Open() methods.\n//\n// Index() takes an input value, deduces a DocumentMapping for its type,\n// assigns string paths to its fields or values then applies field mappings on\n// them.\n//\n// The DocumentMapping used to index a value is deduced by the following rules:\n//  1. If value implements mapping.bleveClassifier interface, resolve the mapping\n//     from BleveType().\n//  2. If value implements mapping.Classifier interface, resolve the mapping\n//     from Type().\n//  3. If value has a string field or value at IndexMapping.TypeField.\n//\n// (defaulting to \"_type\"), use it to resolve the mapping. Fields addressing\n// is described below.\n// 4) If IndexMapping.DefaultType is registered, return it.\n// 5) Return IndexMapping.DefaultMapping.\n//\n// Each field or nested field of the value is identified by a string path, then\n// mapped to one or several FieldMappings which extract the result for analysis.\n//\n// Struct values fields are identified by their \"json:\" tag, or by their name.\n// Nested fields are identified by prefixing with their parent identifier,\n// separated by a dot.\n//\n// Map values entries are identified by their string key. Entries not indexed\n// by strings are ignored. Entry values are identified recursively like struct\n// fields.\n//\n// Slice and array values are identified by their field name. Their elements\n// are processed sequentially with the same FieldMapping.\n//\n// String, float64 and time.Time values are identified by their field name.\n// Other types are ignored.\n//\n// Each value identifier is decomposed in its parts and recursively address\n// SubDocumentMappings in the tree starting at the root DocumentMapping.  If a\n// mapping is found, all its FieldMappings are applied to the value. If no\n// mapping is found and the root DocumentMapping is dynamic, default mappings\n// are used based on value type and IndexMapping default configurations.\n//\n// Finally, mapped values are analyzed, indexed or stored. See\n// FieldMapping.Analyzer to know how an analyzer is resolved for a given field.\n//\n// Examples:\n//\n//\ttype Date struct {\n//\t  Day string `json:\"day\"`\n//\t  Month string\n//\t  Year string\n//\t}\n//\n//\ttype Person struct {\n//\t  FirstName string `json:\"first_name\"`\n//\t  LastName string\n//\t  BirthDate Date `json:\"birth_date\"`\n//\t}\n//\n// A Person value FirstName is mapped by the SubDocumentMapping at\n// \"first_name\". Its LastName is mapped by the one at \"LastName\". The day of\n// BirthDate is mapped to the SubDocumentMapping \"day\" of the root\n// SubDocumentMapping \"birth_date\". It will appear as the \"birth_date.day\"\n// field in the index. The month is mapped to \"birth_date.Month\".\ntype Index interface {\n\t// Index analyzes, indexes or stores mapped data fields. Supplied\n\t// identifier is bound to analyzed data and will be retrieved by search\n\t// requests. See Index interface documentation for details about mapping\n\t// rules.\n\tIndex(id string, data interface{}) error\n\tDelete(id string) error\n\n\tNewBatch() *Batch\n\tBatch(b *Batch) error\n\n\t// Document returns specified document or nil if the document is not\n\t// indexed or stored.\n\tDocument(id string) (index.Document, error)\n\t// DocCount returns the number of documents in the index.\n\tDocCount() (uint64, error)\n\n\tSearch(req *SearchRequest) (*SearchResult, error)\n\tSearchInContext(ctx context.Context, req *SearchRequest) (*SearchResult, error)\n\n\tFields() ([]string, error)\n\n\tFieldDict(field string) (index.FieldDict, error)\n\tFieldDictRange(field string, startTerm []byte, endTerm []byte) (index.FieldDict, error)\n\tFieldDictPrefix(field string, termPrefix []byte) (index.FieldDict, error)\n\n\tClose() error\n\n\tMapping() mapping.IndexMapping\n\n\tStats() *IndexStat\n\tStatsMap() map[string]interface{}\n\n\tGetInternal(key []byte) ([]byte, error)\n\tSetInternal(key, val []byte) error\n\tDeleteInternal(key []byte) error\n\n\t// Name returns the name of the index (by default this is the path)\n\tName() string\n\t// SetName lets you assign your own logical name to this index\n\tSetName(string)\n\n\t// Advanced returns the internal index implementation\n\tAdvanced() (index.Index, error)\n}\n\n// New index at the specified path, must not exist.\n// The provided mapping will be used for all\n// Index/Search operations.\nfunc New(path string, mapping mapping.IndexMapping) (Index, error) {\n\treturn newIndexUsing(path, mapping, Config.DefaultIndexType, Config.DefaultKVStore, nil)\n}\n\n// NewMemOnly creates a memory-only index.\n// The contents of the index is NOT persisted,\n// and will be lost once closed.\n// The provided mapping will be used for all\n// Index/Search operations.\nfunc NewMemOnly(mapping mapping.IndexMapping) (Index, error) {\n\treturn newIndexUsing(\"\", mapping, upsidedown.Name, Config.DefaultMemKVStore, nil)\n}\n\n// NewUsing creates index at the specified path,\n// which must not already exist.\n// The provided mapping will be used for all\n// Index/Search operations.\n// The specified index type will be used.\n// The specified kvstore implementation will be used\n// and the provided kvconfig will be passed to its\n// constructor. Note that currently the values of kvconfig must\n// be able to be marshaled and unmarshaled using the encoding/json library (used\n// when reading/writing the index metadata file).\nfunc NewUsing(path string, mapping mapping.IndexMapping, indexType string, kvstore string, kvconfig map[string]interface{}) (Index, error) {\n\treturn newIndexUsing(path, mapping, indexType, kvstore, kvconfig)\n}\n\n// Open index at the specified path, must exist.\n// The mapping used when it was created will be used for all Index/Search operations.\nfunc Open(path string) (Index, error) {\n\treturn openIndexUsing(path, nil)\n}\n\n// OpenUsing opens index at the specified path, must exist.\n// The mapping used when it was created will be used for all Index/Search operations.\n// The provided runtimeConfig can override settings\n// persisted when the kvstore was created.\nfunc OpenUsing(path string, runtimeConfig map[string]interface{}) (Index, error) {\n\treturn openIndexUsing(path, runtimeConfig)\n}\n\n// Builder is a limited interface, used to build indexes in an offline mode.\n// Items cannot be updated or deleted, and the caller MUST ensure a document is\n// indexed only once.\ntype Builder interface {\n\tIndex(id string, data interface{}) error\n\tClose() error\n}\n\n// NewBuilder creates a builder, which will build an index at the specified path,\n// using the specified mapping and options.\nfunc NewBuilder(path string, mapping mapping.IndexMapping, config map[string]interface{}) (Builder, error) {\n\treturn newBuilder(path, mapping, config)\n}\n\n// IndexCopyable is an index which supports an online copy operation\n// of the index.\ntype IndexCopyable interface {\n\t// CopyTo creates a fully functional copy of the index at the\n\t// specified destination directory implementation.\n\tCopyTo(d index.Directory) error\n}\n\n// FileSystemDirectory is the default implementation for the\n// index.Directory interface.\ntype FileSystemDirectory string\n\n// SynonymDefinition represents a synonym mapping in Bleve.\n// Each instance associates one or more input terms with a list of synonyms,\n// defining how terms are treated as equivalent in searches.\ntype SynonymDefinition struct {\n\t// Input is an optional list of terms for unidirectional synonym mapping.\n\t// When terms are specified in Input, they will map to the terms in Synonyms,\n\t// making the relationship unidirectional (each Input maps to all Synonyms).\n\t// If Input is omitted, the relationship is bidirectional among all Synonyms.\n\tInput []string `json:\"input,omitempty\"`\n\n\t// Synonyms is a list of terms that are considered equivalent.\n\t// If Input is specified, each term in Input will map to each term in Synonyms.\n\t// If Input is not specified, the Synonyms list will be treated bidirectionally,\n\t// meaning each term in Synonyms is treated as synonymous with all others.\n\tSynonyms []string `json:\"synonyms\"`\n}\n\nfunc (sd *SynonymDefinition) Validate() error {\n\tif len(sd.Synonyms) == 0 {\n\t\treturn fmt.Errorf(\"synonym definition must have at least one synonym\")\n\t}\n\treturn nil\n}\n\n// SynonymIndex supports indexing synonym definitions alongside regular documents.\n// Synonyms, grouped by collection name, define term relationships for query expansion in searches.\ntype SynonymIndex interface {\n\tIndex\n\t// IndexSynonym indexes a synonym definition, with the specified id and belonging to the specified collection.\n\tIndexSynonym(id string, collection string, definition *SynonymDefinition) error\n}\n"
        },
        {
          "name": "index",
          "type": "tree",
          "content": null
        },
        {
          "name": "index_alias.go",
          "type": "blob",
          "size": 1.287109375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\n// An IndexAlias is a wrapper around one or more\n// Index objects.  It has two distinct modes of\n// operation.\n// 1.  When it points to a single index, ALL index\n// operations are valid and will be passed through\n// to the underlying index.\n// 2.  When it points to more than one index, the only\n// valid operation is Search.  In this case the\n// search will be performed across all the\n// underlying indexes and the results merged.\n// Calls to Add/Remove/Swap the underlying indexes\n// are atomic, so you can safely change the\n// underlying Index objects while other components\n// are performing operations.\ntype IndexAlias interface {\n\tIndex\n\n\tAdd(i ...Index)\n\tRemove(i ...Index)\n\tSwap(in, out []Index)\n}\n"
        },
        {
          "name": "index_alias_impl.go",
          "type": "blob",
          "size": 24.6015625,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/collector\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\ntype indexAliasImpl struct {\n\tname    string\n\tindexes []Index\n\tmutex   sync.RWMutex\n\topen    bool\n\t// if all the indexes in tha alias have the same mapping\n\t// then the user can set the mapping here to avoid\n\t// checking the mapping of each index in the alias\n\tmapping mapping.IndexMapping\n}\n\n// NewIndexAlias creates a new IndexAlias over the provided\n// Index objects.\nfunc NewIndexAlias(indexes ...Index) *indexAliasImpl {\n\treturn &indexAliasImpl{\n\t\tname:    \"alias\",\n\t\tindexes: indexes,\n\t\topen:    true,\n\t}\n}\n\n// VisitIndexes invokes the visit callback on every\n// indexes included in the index alias.\nfunc (i *indexAliasImpl) VisitIndexes(visit func(Index)) {\n\ti.mutex.RLock()\n\tfor _, idx := range i.indexes {\n\t\tvisit(idx)\n\t}\n\ti.mutex.RUnlock()\n}\n\nfunc (i *indexAliasImpl) isAliasToSingleIndex() error {\n\tif len(i.indexes) < 1 {\n\t\treturn ErrorAliasEmpty\n\t} else if len(i.indexes) > 1 {\n\t\treturn ErrorAliasMulti\n\t}\n\treturn nil\n}\n\nfunc (i *indexAliasImpl) Index(id string, data interface{}) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn i.indexes[0].Index(id, data)\n}\n\nfunc (i *indexAliasImpl) IndexSynonym(id string, collection string, definition *SynonymDefinition) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif si, ok := i.indexes[0].(SynonymIndex); ok {\n\t\treturn si.IndexSynonym(id, collection, definition)\n\t}\n\treturn ErrorSynonymSearchNotSupported\n}\n\nfunc (i *indexAliasImpl) Delete(id string) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn i.indexes[0].Delete(id)\n}\n\nfunc (i *indexAliasImpl) Batch(b *Batch) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn i.indexes[0].Batch(b)\n}\n\nfunc (i *indexAliasImpl) Document(id string) (index.Document, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn i.indexes[0].Document(id)\n}\n\nfunc (i *indexAliasImpl) DocCount() (uint64, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\trv := uint64(0)\n\n\tif !i.open {\n\t\treturn 0, ErrorIndexClosed\n\t}\n\n\tfor _, index := range i.indexes {\n\t\totherCount, err := index.DocCount()\n\t\tif err == nil {\n\t\t\trv += otherCount\n\t\t}\n\t\t// tolerate errors to produce partial counts\n\t}\n\n\treturn rv, nil\n}\n\nfunc (i *indexAliasImpl) Search(req *SearchRequest) (*SearchResult, error) {\n\treturn i.SearchInContext(context.Background(), req)\n}\n\nfunc (i *indexAliasImpl) SearchInContext(ctx context.Context, req *SearchRequest) (*SearchResult, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\tif len(i.indexes) < 1 {\n\t\treturn nil, ErrorAliasEmpty\n\t}\n\tif _, ok := ctx.Value(search.PreSearchKey).(bool); ok {\n\t\t// since preSearchKey is set, it means that the request\n\t\t// is being executed as part of a preSearch, which\n\t\t// indicates that this index alias is set as an Index\n\t\t// in another alias, so we need to do a preSearch search\n\t\t// and NOT a real search\n\t\tflags := &preSearchFlags{\n\t\t\tknn:      requestHasKNN(req),\n\t\t\tsynonyms: !isMatchNoneQuery(req.Query),\n\t\t}\n\t\treturn preSearchDataSearch(ctx, req, flags, i.indexes...)\n\t}\n\n\t// at this point we know we are doing a real search\n\t// either after a preSearch is done, or directly\n\t// on the alias\n\n\t// check if request has preSearchData which would indicate that the\n\t// request has already been preSearched and we can skip the\n\t// preSearch step now, we call an optional function to\n\t// redistribute the preSearchData to the individual indexes\n\t// if necessary\n\tvar preSearchData map[string]map[string]interface{}\n\tif req.PreSearchData != nil {\n\t\tvar err error\n\t\tpreSearchData, err = redistributePreSearchData(req, i.indexes)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// short circuit the simple case\n\tif len(i.indexes) == 1 {\n\t\tif preSearchData != nil {\n\t\t\treq.PreSearchData = preSearchData[i.indexes[0].Name()]\n\t\t}\n\t\treturn i.indexes[0].SearchInContext(ctx, req)\n\t}\n\n\t// at this stage we know we have multiple indexes\n\t// check if preSearchData needs to be gathered from all indexes\n\t// before executing the query\n\tvar err error\n\t// only perform preSearch if\n\t//  - the request does not already have preSearchData\n\t//  - the request requires preSearch\n\tvar preSearchDuration time.Duration\n\tvar sr *SearchResult\n\tflags, err := preSearchRequired(req, i.mapping)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif req.PreSearchData == nil && flags != nil {\n\t\tsearchStart := time.Now()\n\t\tpreSearchResult, err := preSearch(ctx, req, flags, i.indexes...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// check if the preSearch result has any errors and if so\n\t\t// return the search result as is without executing the query\n\t\t// so that the errors are not lost\n\t\tif preSearchResult.Status.Failed > 0 || len(preSearchResult.Status.Errors) > 0 {\n\t\t\treturn preSearchResult, nil\n\t\t}\n\t\t// finalize the preSearch result now\n\t\tfinalizePreSearchResult(req, flags, preSearchResult)\n\n\t\t// if there are no errors, then merge the data in the preSearch result\n\t\t// and construct the preSearchData to be used in the actual search\n\t\t// if the request is satisfied by the preSearch result, then we can\n\t\t// directly return the preSearch result as the final result\n\t\tif requestSatisfiedByPreSearch(req, flags) {\n\t\t\tsr = finalizeSearchResult(req, preSearchResult)\n\t\t\t// no need to run the 2nd phase MultiSearch(..)\n\t\t} else {\n\t\t\tpreSearchData, err = constructPreSearchData(req, flags, preSearchResult, i.indexes)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tpreSearchDuration = time.Since(searchStart)\n\t}\n\n\t// check if search result was generated as part of preSearch itself\n\tif sr == nil {\n\t\tsr, err = MultiSearch(ctx, req, preSearchData, i.indexes...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tsr.Took += preSearchDuration\n\treturn sr, nil\n}\n\nfunc (i *indexAliasImpl) Fields() ([]string, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn i.indexes[0].Fields()\n}\n\nfunc (i *indexAliasImpl) FieldDict(field string) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := i.indexes[0].FieldDict(field)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexAliasImplFieldDict{\n\t\tindex:     i,\n\t\tfieldDict: fieldDict,\n\t}, nil\n}\n\nfunc (i *indexAliasImpl) FieldDictRange(field string, startTerm []byte, endTerm []byte) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := i.indexes[0].FieldDictRange(field, startTerm, endTerm)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexAliasImplFieldDict{\n\t\tindex:     i,\n\t\tfieldDict: fieldDict,\n\t}, nil\n}\n\nfunc (i *indexAliasImpl) FieldDictPrefix(field string, termPrefix []byte) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := i.indexes[0].FieldDictPrefix(field, termPrefix)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexAliasImplFieldDict{\n\t\tindex:     i,\n\t\tfieldDict: fieldDict,\n\t}, nil\n}\n\nfunc (i *indexAliasImpl) Close() error {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\n\ti.open = false\n\treturn nil\n}\n\n// SetIndexMapping sets the mapping for the alias and must be used\n// ONLY when all the indexes in the alias have the same mapping.\n// This is to avoid checking the mapping of each index in the alias\n// when executing a search request.\nfunc (i *indexAliasImpl) SetIndexMapping(m mapping.IndexMapping) error {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\ti.mapping = m\n\treturn nil\n}\n\nfunc (i *indexAliasImpl) Mapping() mapping.IndexMapping {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil\n\t}\n\n\t// if the mapping is already set, return it\n\tif i.mapping != nil {\n\t\treturn i.mapping\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\treturn i.indexes[0].Mapping()\n}\n\nfunc (i *indexAliasImpl) Stats() *IndexStat {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\treturn i.indexes[0].Stats()\n}\n\nfunc (i *indexAliasImpl) StatsMap() map[string]interface{} {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\treturn i.indexes[0].StatsMap()\n}\n\nfunc (i *indexAliasImpl) GetInternal(key []byte) ([]byte, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn i.indexes[0].GetInternal(key)\n}\n\nfunc (i *indexAliasImpl) SetInternal(key, val []byte) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn i.indexes[0].SetInternal(key, val)\n}\n\nfunc (i *indexAliasImpl) DeleteInternal(key []byte) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn i.indexes[0].DeleteInternal(key)\n}\n\nfunc (i *indexAliasImpl) Advanced() (index.Index, error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn i.indexes[0].Advanced()\n}\n\nfunc (i *indexAliasImpl) Add(indexes ...Index) {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\n\ti.indexes = append(i.indexes, indexes...)\n}\n\nfunc (i *indexAliasImpl) removeSingle(index Index) {\n\tfor pos, in := range i.indexes {\n\t\tif in == index {\n\t\t\ti.indexes = append(i.indexes[:pos], i.indexes[pos+1:]...)\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc (i *indexAliasImpl) Remove(indexes ...Index) {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\n\tfor _, in := range indexes {\n\t\ti.removeSingle(in)\n\t}\n}\n\nfunc (i *indexAliasImpl) Swap(in, out []Index) {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\n\t// add\n\ti.indexes = append(i.indexes, in...)\n\n\t// delete\n\tfor _, ind := range out {\n\t\ti.removeSingle(ind)\n\t}\n}\n\n// createChildSearchRequest creates a separate\n// request from the original\n// For now, avoid data race on req structure.\n// TODO disable highlight/field load on child\n// requests, and add code to do this only on\n// the actual final results.\n// Perhaps that part needs to be optional,\n// could be slower in remote usages.\nfunc createChildSearchRequest(req *SearchRequest, preSearchData map[string]interface{}) *SearchRequest {\n\treturn copySearchRequest(req, preSearchData)\n}\n\ntype asyncSearchResult struct {\n\tName   string\n\tResult *SearchResult\n\tErr    error\n}\n\n// preSearchFlags is a struct to hold flags indicating why preSearch is required\ntype preSearchFlags struct {\n\tknn      bool\n\tsynonyms bool\n}\n\n// preSearchRequired checks if preSearch is required and returns a boolean flag\n// It only allocates the preSearchFlags struct if necessary\nfunc preSearchRequired(req *SearchRequest, m mapping.IndexMapping) (*preSearchFlags, error) {\n\t// Check for KNN query\n\tknn := requestHasKNN(req)\n\tvar synonyms bool\n\tif !isMatchNoneQuery(req.Query) {\n\t\t// Check if synonyms are defined in the mapping\n\t\tif sm, ok := m.(mapping.SynonymMapping); ok && sm.SynonymCount() > 0 {\n\t\t\t// check if any of the fields queried have a synonym source\n\t\t\t// in the index mapping, to prevent unnecessary preSearch\n\t\t\tfs, err := query.ExtractFields(req.Query, m, nil)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tfor field := range fs {\n\t\t\t\tif sm.SynonymSourceForPath(field) != \"\" {\n\t\t\t\t\tsynonyms = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif knn || synonyms {\n\t\treturn &preSearchFlags{\n\t\t\tknn:      knn,\n\t\t\tsynonyms: synonyms,\n\t\t}, nil\n\t}\n\treturn nil, nil\n}\n\nfunc preSearch(ctx context.Context, req *SearchRequest, flags *preSearchFlags, indexes ...Index) (*SearchResult, error) {\n\tvar dummyQuery = req.Query\n\tif !flags.synonyms {\n\t\t// create a dummy request with a match none query\n\t\t// since we only care about the preSearchData in PreSearch\n\t\tdummyQuery = query.NewMatchNoneQuery()\n\t}\n\tdummyRequest := &SearchRequest{\n\t\tQuery: dummyQuery,\n\t}\n\tnewCtx := context.WithValue(ctx, search.PreSearchKey, true)\n\tif flags.knn {\n\t\taddKnnToDummyRequest(dummyRequest, req)\n\t}\n\treturn preSearchDataSearch(newCtx, dummyRequest, flags, indexes...)\n}\n\n// if the request is satisfied by just the preSearch result,\n// finalize the result and return it directly without\n// performing multi search\nfunc finalizeSearchResult(req *SearchRequest, preSearchResult *SearchResult) *SearchResult {\n\tif preSearchResult == nil {\n\t\treturn nil\n\t}\n\n\t// global values across all hits irrespective of pagination settings\n\tpreSearchResult.Total = uint64(preSearchResult.Hits.Len())\n\tmaxScore := float64(0)\n\tfor i, hit := range preSearchResult.Hits {\n\t\t// since we are now using the preSearch result as the final result\n\t\t// we can discard the indexNames from the hits as they are no longer\n\t\t// relevant.\n\t\thit.IndexNames = nil\n\t\tif hit.Score > maxScore {\n\t\t\tmaxScore = hit.Score\n\t\t}\n\t\thit.HitNumber = uint64(i)\n\t}\n\tpreSearchResult.MaxScore = maxScore\n\t// now apply pagination settings\n\tvar reverseQueryExecution bool\n\tif req.SearchBefore != nil {\n\t\treverseQueryExecution = true\n\t\treq.Sort.Reverse()\n\t\treq.SearchAfter = req.SearchBefore\n\t}\n\tif req.SearchAfter != nil {\n\t\tpreSearchResult.Hits = collector.FilterHitsBySearchAfter(preSearchResult.Hits, req.Sort, req.SearchAfter)\n\t}\n\tpreSearchResult.Hits = hitsInCurrentPage(req, preSearchResult.Hits)\n\tif reverseQueryExecution {\n\t\t// reverse the sort back to the original\n\t\treq.Sort.Reverse()\n\t\t// resort using the original order\n\t\tmhs := newSearchHitSorter(req.Sort, preSearchResult.Hits)\n\t\treq.SortFunc()(mhs)\n\t\treq.SearchAfter = nil\n\t}\n\n\tif req.Explain {\n\t\tpreSearchResult.Request = req\n\t}\n\treturn preSearchResult\n}\n\nfunc requestSatisfiedByPreSearch(req *SearchRequest, flags *preSearchFlags) bool {\n\tif flags == nil {\n\t\treturn false\n\t}\n\t// if the synonyms presearch flag is set the request can never be satisfied by\n\t// the preSearch result as synonyms are not part of the preSearch result\n\tif flags.synonyms {\n\t\treturn false\n\t}\n\tif flags.knn && isKNNrequestSatisfiedByPreSearch(req) {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc constructSynonymPreSearchData(rv map[string]map[string]interface{}, sr *SearchResult, indexes []Index) map[string]map[string]interface{} {\n\tfor _, index := range indexes {\n\t\trv[index.Name()][search.SynonymPreSearchDataKey] = sr.SynonymResult\n\t}\n\treturn rv\n}\n\nfunc constructPreSearchData(req *SearchRequest, flags *preSearchFlags,\n\tpreSearchResult *SearchResult, indexes []Index) (map[string]map[string]interface{}, error) {\n\tif flags == nil || preSearchResult == nil {\n\t\treturn nil, fmt.Errorf(\"invalid input, flags: %v, preSearchResult: %v\", flags, preSearchResult)\n\t}\n\tmergedOut := make(map[string]map[string]interface{}, len(indexes))\n\tfor _, index := range indexes {\n\t\tmergedOut[index.Name()] = make(map[string]interface{})\n\t}\n\tvar err error\n\tif flags.knn {\n\t\tmergedOut, err = constructKnnPreSearchData(mergedOut, preSearchResult, indexes)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif flags.synonyms {\n\t\tmergedOut = constructSynonymPreSearchData(mergedOut, preSearchResult, indexes)\n\t}\n\treturn mergedOut, nil\n}\n\nfunc preSearchDataSearch(ctx context.Context, req *SearchRequest, flags *preSearchFlags, indexes ...Index) (*SearchResult, error) {\n\tasyncResults := make(chan *asyncSearchResult, len(indexes))\n\t// run search on each index in separate go routine\n\tvar waitGroup sync.WaitGroup\n\tvar searchChildIndex = func(in Index, childReq *SearchRequest) {\n\t\trv := asyncSearchResult{Name: in.Name()}\n\t\trv.Result, rv.Err = in.SearchInContext(ctx, childReq)\n\t\tasyncResults <- &rv\n\t\twaitGroup.Done()\n\t}\n\twaitGroup.Add(len(indexes))\n\tfor _, in := range indexes {\n\t\tgo searchChildIndex(in, createChildSearchRequest(req, nil))\n\t}\n\t// on another go routine, close after finished\n\tgo func() {\n\t\twaitGroup.Wait()\n\t\tclose(asyncResults)\n\t}()\n\t// the final search result to be returned after combining the preSearch results\n\tvar sr *SearchResult\n\t// the preSearch result processor\n\tvar prp preSearchResultProcessor\n\t// error map\n\tindexErrors := make(map[string]error)\n\tfor asr := range asyncResults {\n\t\tif asr.Err == nil {\n\t\t\t// a valid preSearch result\n\t\t\tif prp == nil {\n\t\t\t\t// first valid preSearch result\n\t\t\t\t// create a new preSearch result processor\n\t\t\t\tprp = createPreSearchResultProcessor(req, flags)\n\t\t\t}\n\t\t\tprp.add(asr.Result, asr.Name)\n\t\t\tif sr == nil {\n\t\t\t\t// first result\n\t\t\t\tsr = &SearchResult{\n\t\t\t\t\tStatus: asr.Result.Status,\n\t\t\t\t\tCost:   asr.Result.Cost,\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// merge with previous\n\t\t\t\tsr.Status.Merge(asr.Result.Status)\n\t\t\t\tsr.Cost += asr.Result.Cost\n\t\t\t}\n\t\t} else {\n\t\t\tindexErrors[asr.Name] = asr.Err\n\t\t}\n\t}\n\t// handle case where no results were successful\n\tif sr == nil {\n\t\tsr = &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tErrors: make(map[string]error),\n\t\t\t},\n\t\t}\n\t}\n\t// in preSearch, partial results are not allowed as it can lead to\n\t// the real search giving incorrect results, and hence the search\n\t// result is not populated with any of the processed data from\n\t// the preSearch result processor if there are any errors\n\t// or the preSearch result status has any failures\n\tif len(indexErrors) > 0 || sr.Status.Failed > 0 {\n\t\tif sr.Status.Errors == nil {\n\t\t\tsr.Status.Errors = make(map[string]error)\n\t\t}\n\t\tfor indexName, indexErr := range indexErrors {\n\t\t\tsr.Status.Errors[indexName] = indexErr\n\t\t\tsr.Status.Total++\n\t\t\tsr.Status.Failed++\n\t\t}\n\t} else {\n\t\tprp.finalize(sr)\n\t}\n\treturn sr, nil\n}\n\n// redistributePreSearchData redistributes the preSearchData sent in the search request to an index alias\n// which would happen in the case of an alias tree and depending on the level of the tree, the preSearchData\n// needs to be redistributed to the indexes at that level\nfunc redistributePreSearchData(req *SearchRequest, indexes []Index) (map[string]map[string]interface{}, error) {\n\trv := make(map[string]map[string]interface{})\n\tfor _, index := range indexes {\n\t\trv[index.Name()] = make(map[string]interface{})\n\t}\n\tif knnHits, ok := req.PreSearchData[search.KnnPreSearchDataKey].([]*search.DocumentMatch); ok {\n\t\t// the preSearchData for KNN is a list of DocumentMatch objects\n\t\t// that need to be redistributed to the right index.\n\t\t// This is used only in the case of an alias tree, where the indexes\n\t\t// are at the leaves of the tree, and the master alias is at the root.\n\t\t// At each level of the tree, the preSearchData needs to be redistributed\n\t\t// to the indexes/aliases at that level. Because the preSearchData is\n\t\t// specific to each final index at the leaf.\n\t\tsegregatedKnnHits, err := validateAndDistributeKNNHits(knnHits, indexes)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfor _, index := range indexes {\n\t\t\trv[index.Name()][search.KnnPreSearchDataKey] = segregatedKnnHits[index.Name()]\n\t\t}\n\t}\n\tif fts, ok := req.PreSearchData[search.SynonymPreSearchDataKey].(search.FieldTermSynonymMap); ok {\n\t\tfor _, index := range indexes {\n\t\t\trv[index.Name()][search.SynonymPreSearchDataKey] = fts\n\t\t}\n\t}\n\treturn rv, nil\n}\n\n// finalizePreSearchResult finalizes the preSearch result by applying the finalization steps\n// specific to the preSearch flags\nfunc finalizePreSearchResult(req *SearchRequest, flags *preSearchFlags, preSearchResult *SearchResult) {\n\t// if flags is nil then return\n\tif flags == nil {\n\t\treturn\n\t}\n\tif flags.knn {\n\t\tpreSearchResult.Hits = finalizeKNNResults(req, preSearchResult.Hits)\n\t}\n}\n\n// hitsInCurrentPage returns the hits in the current page\n// using the From and Size parameters in the request\nfunc hitsInCurrentPage(req *SearchRequest, hits []*search.DocumentMatch) []*search.DocumentMatch {\n\tsortFunc := req.SortFunc()\n\t// sort all hits with the requested order\n\tif len(req.Sort) > 0 {\n\t\tsorter := newSearchHitSorter(req.Sort, hits)\n\t\tsortFunc(sorter)\n\t}\n\t// now skip over the correct From\n\tif req.From > 0 && len(hits) > req.From {\n\t\thits = hits[req.From:]\n\t} else if req.From > 0 {\n\t\thits = search.DocumentMatchCollection{}\n\t}\n\t// now trim to the correct size\n\tif req.Size > 0 && len(hits) > req.Size {\n\t\thits = hits[0:req.Size]\n\t}\n\treturn hits\n}\n\n// MultiSearch executes a SearchRequest across multiple Index objects,\n// then merges the results.  The indexes must honor any ctx deadline.\nfunc MultiSearch(ctx context.Context, req *SearchRequest, preSearchData map[string]map[string]interface{}, indexes ...Index) (*SearchResult, error) {\n\n\tsearchStart := time.Now()\n\tasyncResults := make(chan *asyncSearchResult, len(indexes))\n\n\tvar reverseQueryExecution bool\n\tif req.SearchBefore != nil {\n\t\treverseQueryExecution = true\n\t\treq.Sort.Reverse()\n\t\treq.SearchAfter = req.SearchBefore\n\t\treq.SearchBefore = nil\n\t}\n\n\t// run search on each index in separate go routine\n\tvar waitGroup sync.WaitGroup\n\n\tvar searchChildIndex = func(in Index, childReq *SearchRequest) {\n\t\trv := asyncSearchResult{Name: in.Name()}\n\t\trv.Result, rv.Err = in.SearchInContext(ctx, childReq)\n\t\tasyncResults <- &rv\n\t\twaitGroup.Done()\n\t}\n\n\twaitGroup.Add(len(indexes))\n\tfor _, in := range indexes {\n\t\tvar payload map[string]interface{}\n\t\tif preSearchData != nil {\n\t\t\tpayload = preSearchData[in.Name()]\n\t\t}\n\t\tgo searchChildIndex(in, createChildSearchRequest(req, payload))\n\t}\n\n\t// on another go routine, close after finished\n\tgo func() {\n\t\twaitGroup.Wait()\n\t\tclose(asyncResults)\n\t}()\n\n\tvar sr *SearchResult\n\tindexErrors := make(map[string]error)\n\n\tfor asr := range asyncResults {\n\t\tif asr.Err == nil {\n\t\t\tif sr == nil {\n\t\t\t\t// first result\n\t\t\t\tsr = asr.Result\n\t\t\t} else {\n\t\t\t\t// merge with previous\n\t\t\t\tsr.Merge(asr.Result)\n\t\t\t}\n\t\t} else {\n\t\t\tindexErrors[asr.Name] = asr.Err\n\t\t}\n\t}\n\n\t// merge just concatenated all the hits\n\t// now lets clean it up\n\n\t// handle case where no results were successful\n\tif sr == nil {\n\t\tsr = &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tErrors: make(map[string]error),\n\t\t\t},\n\t\t}\n\t}\n\n\tsr.Hits = hitsInCurrentPage(req, sr.Hits)\n\n\t// fix up facets\n\tfor name, fr := range req.Facets {\n\t\tsr.Facets.Fixup(name, fr.Size)\n\t}\n\n\tif reverseQueryExecution {\n\t\t// reverse the sort back to the original\n\t\treq.Sort.Reverse()\n\t\t// resort using the original order\n\t\tmhs := newSearchHitSorter(req.Sort, sr.Hits)\n\t\treq.SortFunc()(mhs)\n\t\t// reset request\n\t\treq.SearchBefore = req.SearchAfter\n\t\treq.SearchAfter = nil\n\t}\n\n\t// fix up original request\n\tif req.Explain {\n\t\tsr.Request = req\n\t}\n\tsearchDuration := time.Since(searchStart)\n\tsr.Took = searchDuration\n\n\t// fix up errors\n\tif len(indexErrors) > 0 {\n\t\tif sr.Status.Errors == nil {\n\t\t\tsr.Status.Errors = make(map[string]error)\n\t\t}\n\t\tfor indexName, indexErr := range indexErrors {\n\t\t\tsr.Status.Errors[indexName] = indexErr\n\t\t\tsr.Status.Total++\n\t\t\tsr.Status.Failed++\n\t\t}\n\t}\n\n\treturn sr, nil\n}\n\nfunc (i *indexAliasImpl) NewBatch() *Batch {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil\n\t}\n\n\terr := i.isAliasToSingleIndex()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\treturn i.indexes[0].NewBatch()\n}\n\nfunc (i *indexAliasImpl) Name() string {\n\treturn i.name\n}\n\nfunc (i *indexAliasImpl) SetName(name string) {\n\ti.name = name\n}\n\ntype indexAliasImplFieldDict struct {\n\tindex     *indexAliasImpl\n\tfieldDict index.FieldDict\n}\n\nfunc (f *indexAliasImplFieldDict) BytesRead() uint64 {\n\treturn f.fieldDict.BytesRead()\n}\n\nfunc (f *indexAliasImplFieldDict) Next() (*index.DictEntry, error) {\n\treturn f.fieldDict.Next()\n}\n\nfunc (f *indexAliasImplFieldDict) Close() error {\n\tdefer f.index.mutex.RUnlock()\n\treturn f.fieldDict.Close()\n}\n"
        },
        {
          "name": "index_alias_impl_test.go",
          "type": "blob",
          "size": 31.2685546875,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/numeric\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nfunc TestIndexAliasSingle(t *testing.T) {\n\texpectedError := fmt.Errorf(\"expected\")\n\tei1 := &stubIndex{\n\t\terr: expectedError,\n\t}\n\n\talias := NewIndexAlias(ei1)\n\n\terr := alias.Index(\"a\", \"a\")\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\tbatch := alias.NewBatch()\n\terr = alias.Batch(batch)\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\tmapping := alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat := alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\t_, err = alias.Search(sr)\n\tif err != expectedError {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError, err)\n\t}\n\n\tcount, err := alias.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif count != 0 {\n\t\tt.Errorf(\"expected count 0, got %d\", count)\n\t}\n\n\t// now change the def using add/remove\n\texpectedError2 := fmt.Errorf(\"expected2\")\n\tei2 := &stubIndex{\n\t\terr: expectedError2,\n\t}\n\n\talias.Add(ei2)\n\talias.Remove(ei1)\n\n\terr = alias.Index(\"a\", \"a\")\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\terr = alias.Batch(batch)\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\tmapping = alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat = alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\t_, err = alias.Search(sr)\n\tif err != expectedError2 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError2, err)\n\t}\n\n\tcount, err = alias.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif count != 0 {\n\t\tt.Errorf(\"expected count 0, got %d\", count)\n\t}\n\n\t// now change the def using swap\n\texpectedError3 := fmt.Errorf(\"expected3\")\n\tei3 := &stubIndex{\n\t\terr: expectedError3,\n\t}\n\n\talias.Swap([]Index{ei3}, []Index{ei2})\n\n\terr = alias.Index(\"a\", \"a\")\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\terr = alias.Batch(batch)\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\tmapping = alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat = alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\t_, err = alias.Search(sr)\n\tif err != expectedError3 {\n\t\tt.Errorf(\"expected %v, got %v\", expectedError3, err)\n\t}\n\n\tcount, err = alias.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif count != 0 {\n\t\tt.Errorf(\"expected count 0, got %d\", count)\n\t}\n}\n\nfunc TestIndexAliasClosed(t *testing.T) {\n\talias := NewIndexAlias()\n\terr := alias.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = alias.Index(\"a\", \"a\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\tbatch := alias.NewBatch()\n\terr = alias.Batch(batch)\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\tmapping := alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat := alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\t_, err = alias.Search(sr)\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n\n\t_, err = alias.DocCount()\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorIndexClosed, err)\n\t}\n}\n\nfunc TestIndexAliasEmpty(t *testing.T) {\n\talias := NewIndexAlias()\n\n\terr := alias.Index(\"a\", \"a\")\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\tbatch := alias.NewBatch()\n\terr = alias.Batch(batch)\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\tmapping := alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat := alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\t_, err = alias.Search(sr)\n\tif err != ErrorAliasEmpty {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasEmpty, err)\n\t}\n\n\tcount, err := alias.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"error getting alias doc count: %v\", err)\n\t}\n\tif count != 0 {\n\t\tt.Errorf(\"expected %d, got %d\", 0, count)\n\t}\n}\n\nfunc TestIndexAliasMulti(t *testing.T) {\n\tscore1, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(1.0), 0)\n\tscore2, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(2.0), 0)\n\tei1Count := uint64(7)\n\tei1 := &stubIndex{\n\t\terr:            nil,\n\t\tdocCountResult: &ei1Count,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: search.DocumentMatchCollection{\n\t\t\t\t{\n\t\t\t\t\tID:    \"a\",\n\t\t\t\t\tScore: 1.0,\n\t\t\t\t\tSort:  []string{string(score1)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 1.0,\n\t\t}}\n\tei2Count := uint64(8)\n\tei2 := &stubIndex{\n\t\terr:            nil,\n\t\tdocCountResult: &ei2Count,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: search.DocumentMatchCollection{\n\t\t\t\t{\n\t\t\t\t\tID:    \"b\",\n\t\t\t\t\tScore: 2.0,\n\t\t\t\t\tSort:  []string{string(score2)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 2.0,\n\t\t}}\n\n\talias := NewIndexAlias(ei1, ei2)\n\n\terr := alias.Index(\"a\", \"a\")\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\terr = alias.Delete(\"a\")\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\tbatch := alias.NewBatch()\n\terr = alias.Batch(batch)\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\t_, err = alias.Document(\"a\")\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\t_, err = alias.Fields()\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\t_, err = alias.GetInternal([]byte(\"a\"))\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\terr = alias.SetInternal([]byte(\"a\"), []byte(\"a\"))\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\terr = alias.DeleteInternal([]byte(\"a\"))\n\tif err != ErrorAliasMulti {\n\t\tt.Errorf(\"expected %v, got %v\", ErrorAliasMulti, err)\n\t}\n\n\tmapping := alias.Mapping()\n\tif mapping != nil {\n\t\tt.Errorf(\"expected nil, got %v\", mapping)\n\t}\n\n\tindexStat := alias.Stats()\n\tif indexStat != nil {\n\t\tt.Errorf(\"expected nil, got %v\", indexStat)\n\t}\n\n\t// now a few things that should work\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      2,\n\t\t\tSuccessful: 2,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{string(score2)},\n\t\t\t},\n\t\t\t{\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{string(score1)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 2.0,\n\t}\n\tresults, err := alias.Search(sr)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\t// cheat and ensure that Took field matches since it involves time\n\texpected.Took = results.Took\n\tif !reflect.DeepEqual(results, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, results)\n\t}\n\n\tcount, err := alias.DocCount()\n\tif err != nil {\n\t\tt.Errorf(\"error getting alias doc count: %v\", err)\n\t}\n\tif count != (*ei1.docCountResult + *ei2.docCountResult) {\n\t\tt.Errorf(\"expected %d, got %d\", (*ei1.docCountResult + *ei2.docCountResult), count)\n\t}\n}\n\n// TestMultiSearchNoError\nfunc TestMultiSearchNoError(t *testing.T) {\n\tscore1, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(1.0), 0)\n\tscore2, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(2.0), 0)\n\tei1 := &stubIndex{err: nil, searchResult: &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 1,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{string(score1)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 1.0,\n\t}}\n\tei2 := &stubIndex{err: nil, searchResult: &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 1,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{string(score2)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 2.0,\n\t}}\n\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      2,\n\t\t\tSuccessful: 2,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{string(score2)},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{string(score1)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 2.0,\n\t}\n\n\tresults, err := MultiSearch(context.Background(), sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\t// cheat and ensure that Took field matches since it involves time\n\texpected.Took = results.Took\n\tif !reflect.DeepEqual(results, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, results)\n\t}\n}\n\n// TestMultiSearchSomeError\nfunc TestMultiSearchSomeError(t *testing.T) {\n\tei1 := &stubIndex{name: \"ei1\", err: nil, searchResult: &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 1,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t},\n\t\t},\n\t\tTook:     1 * time.Second,\n\t\tMaxScore: 1.0,\n\t}}\n\tei2 := &stubIndex{name: \"ei2\", err: fmt.Errorf(\"deliberate error\")}\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\tres, err := MultiSearch(context.Background(), sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif res.Status.Total != 2 {\n\t\tt.Errorf(\"expected 2 indexes to be queried, got %d\", res.Status.Total)\n\t}\n\tif res.Status.Failed != 1 {\n\t\tt.Errorf(\"expected 1 index to fail, got %d\", res.Status.Failed)\n\t}\n\tif res.Status.Successful != 1 {\n\t\tt.Errorf(\"expected 1 index to be successful, got %d\", res.Status.Successful)\n\t}\n\tif len(res.Status.Errors) != 1 {\n\t\tt.Fatalf(\"expected 1 status error message, got %d\", len(res.Status.Errors))\n\t}\n\tif res.Status.Errors[\"ei2\"].Error() != \"deliberate error\" {\n\t\tt.Errorf(\"expected ei2 index error message 'deliberate error', got '%s'\", res.Status.Errors[\"ei2\"])\n\t}\n}\n\n// TestMultiSearchAllError\n// reproduces https://github.com/blevesearch/bleve/issues/126\nfunc TestMultiSearchAllError(t *testing.T) {\n\tei1 := &stubIndex{name: \"ei1\", err: fmt.Errorf(\"deliberate error\")}\n\tei2 := &stubIndex{name: \"ei2\", err: fmt.Errorf(\"deliberate error\")}\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\tres, err := MultiSearch(context.Background(), sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif res.Status.Total != 2 {\n\t\tt.Errorf(\"expected 2 indexes to be queried, got %d\", res.Status.Total)\n\t}\n\tif res.Status.Failed != 2 {\n\t\tt.Errorf(\"expected 2 indexes to fail, got %d\", res.Status.Failed)\n\t}\n\tif res.Status.Successful != 0 {\n\t\tt.Errorf(\"expected 0 indexes to be successful, got %d\", res.Status.Successful)\n\t}\n\tif len(res.Status.Errors) != 2 {\n\t\tt.Fatalf(\"expected 2 status error messages, got %d\", len(res.Status.Errors))\n\t}\n\tif res.Status.Errors[\"ei1\"].Error() != \"deliberate error\" {\n\t\tt.Errorf(\"expected ei1 index error message 'deliberate error', got '%s'\", res.Status.Errors[\"ei1\"])\n\t}\n\tif res.Status.Errors[\"ei2\"].Error() != \"deliberate error\" {\n\t\tt.Errorf(\"expected ei2 index error message 'deliberate error', got '%s'\", res.Status.Errors[\"ei2\"])\n\t}\n}\n\nfunc TestMultiSearchSecondPage(t *testing.T) {\n\tcheckRequest := func(sr *SearchRequest) error {\n\t\tif sr.From != 0 {\n\t\t\treturn fmt.Errorf(\"child request from should be 0\")\n\t\t}\n\t\tif sr.Size != 20 {\n\t\t\treturn fmt.Errorf(\"child request size should be 20\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tei1 := &stubIndex{\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t},\n\t\tcheckRequest: checkRequest,\n\t}\n\tei2 := &stubIndex{\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t},\n\t\tcheckRequest: checkRequest,\n\t}\n\tsr := NewSearchRequestOptions(NewTermQuery(\"test\"), 10, 10, false)\n\t_, err := MultiSearch(context.Background(), sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"unexpected error %v\", err)\n\t}\n\n}\n\n// TestMultiSearchTimeout tests simple timeout cases\n// 1. all searches finish successfully before timeout\n// 2. no searchers finish before the timeout\n// 3. no searches finish before cancellation\nfunc TestMultiSearchTimeout(t *testing.T) {\n\tscore1, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(1.0), 0)\n\tscore2, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(2.0), 0)\n\tvar ctx context.Context\n\tei1 := &stubIndex{\n\t\tname: \"ei1\",\n\t\tcheckRequest: func(req *SearchRequest) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\t\treturn nil\n\t\t\t}\n\t\t},\n\t\terr: nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"1\",\n\t\t\t\t\tID:    \"a\",\n\t\t\t\t\tScore: 1.0,\n\t\t\t\t\tSort:  []string{string(score1)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 1.0,\n\t\t}}\n\tei2 := &stubIndex{\n\t\tname: \"ei2\",\n\t\tcheckRequest: func(req *SearchRequest) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\t\treturn nil\n\t\t\t}\n\t\t},\n\t\terr: nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"2\",\n\t\t\t\t\tID:    \"b\",\n\t\t\t\t\tScore: 2.0,\n\t\t\t\t\tSort:  []string{string(score2)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 2.0,\n\t\t}}\n\n\t// first run with absurdly long time out, should succeed\n\tvar cancel context.CancelFunc\n\tctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tquery := NewTermQuery(\"test\")\n\tsr := NewSearchRequest(query)\n\tres, err := MultiSearch(ctx, sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif res.Status.Total != 2 {\n\t\tt.Errorf(\"expected 2 total, got %d\", res.Status.Failed)\n\t}\n\tif res.Status.Successful != 2 {\n\t\tt.Errorf(\"expected 0 success, got %d\", res.Status.Successful)\n\t}\n\tif res.Status.Failed != 0 {\n\t\tt.Errorf(\"expected 2 failed, got %d\", res.Status.Failed)\n\t}\n\tif len(res.Status.Errors) != 0 {\n\t\tt.Errorf(\"expected 0 errors, got %v\", res.Status.Errors)\n\t}\n\n\t// now run a search again with an absurdly low timeout (should timeout)\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Microsecond)\n\tdefer cancel()\n\tres, err = MultiSearch(ctx, sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif res.Status.Total != 2 {\n\t\tt.Errorf(\"expected 2 failed, got %d\", res.Status.Failed)\n\t}\n\tif res.Status.Successful != 0 {\n\t\tt.Errorf(\"expected 0 success, got %d\", res.Status.Successful)\n\t}\n\tif res.Status.Failed != 2 {\n\t\tt.Errorf(\"expected 2 failed, got %d\", res.Status.Failed)\n\t}\n\tif len(res.Status.Errors) != 2 {\n\t\tt.Errorf(\"expected 2 errors, got %v\", res.Status.Errors)\n\t} else {\n\t\tif res.Status.Errors[\"ei1\"].Error() != context.DeadlineExceeded.Error() {\n\t\t\tt.Errorf(\"expected err for 'ei1' to be '%s' got '%s'\", context.DeadlineExceeded.Error(), res.Status.Errors[\"ei1\"])\n\t\t}\n\t\tif res.Status.Errors[\"ei2\"].Error() != context.DeadlineExceeded.Error() {\n\t\t\tt.Errorf(\"expected err for 'ei2' to be '%s' got '%s'\", context.DeadlineExceeded.Error(), res.Status.Errors[\"ei2\"])\n\t\t}\n\t}\n\n\t// now run a search again with a normal timeout, but cancel it first\n\tctx, cancel = context.WithTimeout(context.Background(), 5*time.Second)\n\tcancel()\n\tres, err = MultiSearch(ctx, sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Errorf(\"expected no error, got %v\", err)\n\t}\n\tif res.Status.Total != 2 {\n\t\tt.Errorf(\"expected 2 failed, got %d\", res.Status.Failed)\n\t}\n\tif res.Status.Successful != 0 {\n\t\tt.Errorf(\"expected 0 success, got %d\", res.Status.Successful)\n\t}\n\tif res.Status.Failed != 2 {\n\t\tt.Errorf(\"expected 2 failed, got %d\", res.Status.Failed)\n\t}\n\tif len(res.Status.Errors) != 2 {\n\t\tt.Errorf(\"expected 2 errors, got %v\", res.Status.Errors)\n\t} else {\n\t\tif res.Status.Errors[\"ei1\"].Error() != context.Canceled.Error() {\n\t\t\tt.Errorf(\"expected err for 'ei1' to be '%s' got '%s'\", context.Canceled.Error(), res.Status.Errors[\"ei1\"])\n\t\t}\n\t\tif res.Status.Errors[\"ei2\"].Error() != context.Canceled.Error() {\n\t\t\tt.Errorf(\"expected err for 'ei2' to be '%s' got '%s'\", context.Canceled.Error(), res.Status.Errors[\"ei2\"])\n\t\t}\n\t}\n}\n\n// TestMultiSearchTimeoutPartial tests the case where some indexes exceed\n// the timeout, while others complete successfully\nfunc TestMultiSearchTimeoutPartial(t *testing.T) {\n\tscore1, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(1.0), 0)\n\tscore2, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(2.0), 0)\n\tscore3, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(3.0), 0)\n\tvar ctx context.Context\n\tei1 := &stubIndex{\n\t\tname: \"ei1\",\n\t\terr:  nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"1\",\n\t\t\t\t\tID:    \"a\",\n\t\t\t\t\tScore: 1.0,\n\t\t\t\t\tSort:  []string{string(score1)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 1.0,\n\t\t}}\n\tei2 := &stubIndex{\n\t\tname: \"ei2\",\n\t\terr:  nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"2\",\n\t\t\t\t\tID:    \"b\",\n\t\t\t\t\tScore: 2.0,\n\t\t\t\t\tSort:  []string{string(score2)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 2.0,\n\t\t}}\n\n\tei3 := &stubIndex{\n\t\tname: \"ei3\",\n\t\tcheckRequest: func(req *SearchRequest) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\t}\n\t\t},\n\t\terr: nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"3\",\n\t\t\t\t\tID:    \"c\",\n\t\t\t\t\tScore: 3.0,\n\t\t\t\t\tSort:  []string{string(score3)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 3.0,\n\t\t}}\n\n\t// ei3 is set to take >50ms, so run search with timeout less than\n\t// this, this should return partial results\n\tvar cancel context.CancelFunc\n\tctx, cancel = context.WithTimeout(context.Background(), 25*time.Millisecond)\n\tdefer cancel()\n\tquery := NewTermQuery(\"test\")\n\tsr := NewSearchRequest(query)\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      3,\n\t\t\tSuccessful: 2,\n\t\t\tFailed:     1,\n\t\t\tErrors: map[string]error{\n\t\t\t\t\"ei3\": context.DeadlineExceeded,\n\t\t\t},\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{string(score2)},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{string(score1)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 2.0,\n\t}\n\n\tres, err := MultiSearch(ctx, sr, nil, ei1, ei2, ei3)\n\tif err != nil {\n\t\tt.Fatalf(\"expected no err, got %v\", err)\n\t}\n\texpected.Took = res.Took\n\tif !reflect.DeepEqual(res, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, res)\n\t}\n}\n\nfunc TestIndexAliasMultipleLayer(t *testing.T) {\n\tscore1, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(1.0), 0)\n\tscore2, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(2.0), 0)\n\tscore3, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(3.0), 0)\n\tscore4, _ := numeric.NewPrefixCodedInt64(numeric.Float64ToInt64(4.0), 0)\n\tvar ctx context.Context\n\tei1 := &stubIndex{\n\t\tname: \"ei1\",\n\t\terr:  nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"1\",\n\t\t\t\t\tID:    \"a\",\n\t\t\t\t\tScore: 1.0,\n\t\t\t\t\tSort:  []string{string(score1)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 1.0,\n\t\t}}\n\tei2 := &stubIndex{\n\t\tname: \"ei2\",\n\t\tcheckRequest: func(req *SearchRequest) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase <-time.After(250 * time.Millisecond):\n\t\t\t\treturn nil\n\t\t\t}\n\t\t},\n\t\terr: nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"2\",\n\t\t\t\t\tID:    \"b\",\n\t\t\t\t\tScore: 2.0,\n\t\t\t\t\tSort:  []string{string(score2)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 2.0,\n\t\t}}\n\n\tei3 := &stubIndex{\n\t\tname: \"ei3\",\n\t\tcheckRequest: func(req *SearchRequest) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase <-time.After(250 * time.Millisecond):\n\t\t\t\treturn nil\n\t\t\t}\n\t\t},\n\t\terr: nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"3\",\n\t\t\t\t\tID:    \"c\",\n\t\t\t\t\tScore: 3.0,\n\t\t\t\t\tSort:  []string{string(score3)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 3.0,\n\t\t}}\n\n\tei4 := &stubIndex{\n\t\tname: \"ei4\",\n\t\terr:  nil,\n\t\tsearchResult: &SearchResult{\n\t\t\tStatus: &SearchStatus{\n\t\t\t\tTotal:      1,\n\t\t\t\tSuccessful: 1,\n\t\t\t\tErrors:     make(map[string]error),\n\t\t\t},\n\t\t\tTotal: 1,\n\t\t\tHits: []*search.DocumentMatch{\n\t\t\t\t{\n\t\t\t\t\tIndex: \"4\",\n\t\t\t\t\tID:    \"d\",\n\t\t\t\t\tScore: 4.0,\n\t\t\t\t\tSort:  []string{string(score4)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tMaxScore: 4.0,\n\t\t}}\n\n\talias1 := NewIndexAlias(ei1, ei2)\n\talias2 := NewIndexAlias(ei3, ei4)\n\taliasTop := NewIndexAlias(alias1, alias2)\n\n\t// ei2 and ei3 have 50ms delay\n\t// search across aliasTop should still get results from ei1 and ei4\n\t// total should still be 4\n\tvar cancel context.CancelFunc\n\tctx, cancel = context.WithTimeout(context.Background(), 25*time.Millisecond)\n\tdefer cancel()\n\tquery := NewTermQuery(\"test\")\n\tsr := NewSearchRequest(query)\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      4,\n\t\t\tSuccessful: 2,\n\t\t\tFailed:     2,\n\t\t\tErrors: map[string]error{\n\t\t\t\t\"ei2\": context.DeadlineExceeded,\n\t\t\t\t\"ei3\": context.DeadlineExceeded,\n\t\t\t},\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"4\",\n\t\t\t\tID:    \"d\",\n\t\t\t\tScore: 4.0,\n\t\t\t\tSort:  []string{string(score4)},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{string(score1)},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 4.0,\n\t}\n\n\tres, err := aliasTop.SearchInContext(ctx, sr)\n\tif err != nil {\n\t\tt.Fatalf(\"expected no err, got %v\", err)\n\t}\n\texpected.Took = res.Took\n\tif !reflect.DeepEqual(res, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, res)\n\t}\n}\n\n// TestMultiSearchCustomSort\nfunc TestMultiSearchCustomSort(t *testing.T) {\n\tei1 := &stubIndex{err: nil, searchResult: &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{\"albert\"},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{\"crown\"},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 2.0,\n\t}}\n\tei2 := &stubIndex{err: nil, searchResult: &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"c\",\n\t\t\t\tScore: 2.5,\n\t\t\t\tSort:  []string{\"frank\"},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"d\",\n\t\t\t\tScore: 3.0,\n\t\t\t\tSort:  []string{\"zombie\"},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 3.0,\n\t}}\n\n\tsr := NewSearchRequest(NewTermQuery(\"test\"))\n\tsr.Explain = true\n\tsr.SortBy([]string{\"name\"})\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      2,\n\t\t\tSuccessful: 2,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tRequest: sr,\n\t\tTotal:   4,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1.0,\n\t\t\t\tSort:  []string{\"albert\"},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"1\",\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2.0,\n\t\t\t\tSort:  []string{\"crown\"},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"c\",\n\t\t\t\tScore: 2.5,\n\t\t\t\tSort:  []string{\"frank\"},\n\t\t\t},\n\t\t\t{\n\t\t\t\tIndex: \"2\",\n\t\t\t\tID:    \"d\",\n\t\t\t\tScore: 3.0,\n\t\t\t\tSort:  []string{\"zombie\"},\n\t\t\t},\n\t\t},\n\t\tMaxScore: 3.0,\n\t}\n\n\tresults, err := MultiSearch(context.Background(), sr, nil, ei1, ei2)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\t// cheat and ensure that Took field matches since it involves time\n\texpected.Took = results.Took\n\tif !reflect.DeepEqual(results, expected) {\n\t\tt.Errorf(\"expected %v, got %v\", expected, results)\n\t}\n}\n\n// stubIndex is an Index impl for which all operations\n// return the configured error value, unless the\n// corresponding operation result value has been\n// set, in which case that is returned instead\ntype stubIndex struct {\n\tname           string\n\terr            error\n\tsearchResult   *SearchResult\n\tdocumentResult *document.Document\n\tdocCountResult *uint64\n\tcheckRequest   func(*SearchRequest) error\n}\n\nfunc (i *stubIndex) Index(id string, data interface{}) error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) Delete(id string) error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) Batch(b *Batch) error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) Document(id string) (index.Document, error) {\n\tif i.documentResult != nil {\n\t\treturn i.documentResult, nil\n\t}\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) DocCount() (uint64, error) {\n\tif i.docCountResult != nil {\n\t\treturn *i.docCountResult, nil\n\t}\n\treturn 0, i.err\n}\n\nfunc (i *stubIndex) Search(req *SearchRequest) (*SearchResult, error) {\n\treturn i.SearchInContext(context.Background(), req)\n}\n\nfunc (i *stubIndex) SearchInContext(ctx context.Context, req *SearchRequest) (*SearchResult, error) {\n\tif i.checkRequest != nil {\n\t\terr := i.checkRequest(req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif i.searchResult != nil {\n\t\treturn i.searchResult, nil\n\t}\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) Fields() ([]string, error) {\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) FieldDict(field string) (index.FieldDict, error) {\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) FieldDictRange(field string, startTerm []byte, endTerm []byte) (index.FieldDict, error) {\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) FieldDictPrefix(field string, termPrefix []byte) (index.FieldDict, error) {\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) Close() error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) Mapping() mapping.IndexMapping {\n\treturn nil\n}\n\nfunc (i *stubIndex) Stats() *IndexStat {\n\treturn nil\n}\n\nfunc (i *stubIndex) StatsMap() map[string]interface{} {\n\treturn nil\n}\n\nfunc (i *stubIndex) GetInternal(key []byte) ([]byte, error) {\n\treturn nil, i.err\n}\n\nfunc (i *stubIndex) SetInternal(key, val []byte) error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) DeleteInternal(key []byte) error {\n\treturn i.err\n}\n\nfunc (i *stubIndex) Advanced() (index.Index, error) {\n\treturn nil, nil\n}\n\nfunc (i *stubIndex) NewBatch() *Batch {\n\treturn &Batch{}\n}\n\nfunc (i *stubIndex) Name() string {\n\treturn i.name\n}\n\nfunc (i *stubIndex) SetName(name string) {\n\ti.name = name\n}\n"
        },
        {
          "name": "index_impl.go",
          "type": "blob",
          "size": 28.8701171875,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/microseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/milliseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/nanoseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/seconds\"\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/registry\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/collector\"\n\t\"github.com/blevesearch/bleve/v2/search/facet\"\n\t\"github.com/blevesearch/bleve/v2/search/highlight\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\t\"github.com/blevesearch/bleve/v2/util\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n\t\"github.com/blevesearch/geo/s2\"\n)\n\ntype indexImpl struct {\n\tpath  string\n\tname  string\n\tmeta  *indexMeta\n\ti     index.Index\n\tm     mapping.IndexMapping\n\tmutex sync.RWMutex\n\topen  bool\n\tstats *IndexStat\n}\n\nconst storePath = \"store\"\n\nvar mappingInternalKey = []byte(\"_mapping\")\n\nconst SearchQueryStartCallbackKey = \"_search_query_start_callback_key\"\nconst SearchQueryEndCallbackKey = \"_search_query_end_callback_key\"\n\ntype SearchQueryStartCallbackFn func(size uint64) error\ntype SearchQueryEndCallbackFn func(size uint64) error\n\nfunc indexStorePath(path string) string {\n\treturn path + string(os.PathSeparator) + storePath\n}\n\nfunc newIndexUsing(path string, mapping mapping.IndexMapping, indexType string, kvstore string, kvconfig map[string]interface{}) (*indexImpl, error) {\n\t// first validate the mapping\n\terr := mapping.Validate()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif kvconfig == nil {\n\t\tkvconfig = map[string]interface{}{}\n\t}\n\n\tif kvstore == \"\" {\n\t\treturn nil, fmt.Errorf(\"bleve not configured for file based indexing\")\n\t}\n\n\trv := indexImpl{\n\t\tpath: path,\n\t\tname: path,\n\t\tm:    mapping,\n\t\tmeta: newIndexMeta(indexType, kvstore, kvconfig),\n\t}\n\trv.stats = &IndexStat{i: &rv}\n\t// at this point there is hope that we can be successful, so save index meta\n\tif path != \"\" {\n\t\terr = rv.meta.Save(path)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tkvconfig[\"create_if_missing\"] = true\n\t\tkvconfig[\"error_if_exists\"] = true\n\t\tkvconfig[\"path\"] = indexStorePath(path)\n\t} else {\n\t\tkvconfig[\"path\"] = \"\"\n\t}\n\n\t// open the index\n\tindexTypeConstructor := registry.IndexTypeConstructorByName(rv.meta.IndexType)\n\tif indexTypeConstructor == nil {\n\t\treturn nil, ErrorUnknownIndexType\n\t}\n\n\trv.i, err = indexTypeConstructor(rv.meta.Storage, kvconfig, Config.analysisQueue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = rv.i.Open()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func(rv *indexImpl) {\n\t\tif !rv.open {\n\t\t\trv.i.Close()\n\t\t}\n\t}(&rv)\n\n\t// now persist the mapping\n\tmappingBytes, err := util.MarshalJSON(mapping)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = rv.i.SetInternal(mappingInternalKey, mappingBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// mark the index as open\n\trv.mutex.Lock()\n\tdefer rv.mutex.Unlock()\n\trv.open = true\n\tindexStats.Register(&rv)\n\treturn &rv, nil\n}\n\nfunc openIndexUsing(path string, runtimeConfig map[string]interface{}) (rv *indexImpl, err error) {\n\trv = &indexImpl{\n\t\tpath: path,\n\t\tname: path,\n\t}\n\trv.stats = &IndexStat{i: rv}\n\n\trv.meta, err = openIndexMeta(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// backwards compatibility if index type is missing\n\tif rv.meta.IndexType == \"\" {\n\t\trv.meta.IndexType = upsidedown.Name\n\t}\n\n\tstoreConfig := rv.meta.Config\n\tif storeConfig == nil {\n\t\tstoreConfig = map[string]interface{}{}\n\t}\n\n\tstoreConfig[\"path\"] = indexStorePath(path)\n\tstoreConfig[\"create_if_missing\"] = false\n\tstoreConfig[\"error_if_exists\"] = false\n\tfor rck, rcv := range runtimeConfig {\n\t\tstoreConfig[rck] = rcv\n\t}\n\n\t// open the index\n\tindexTypeConstructor := registry.IndexTypeConstructorByName(rv.meta.IndexType)\n\tif indexTypeConstructor == nil {\n\t\treturn nil, ErrorUnknownIndexType\n\t}\n\n\trv.i, err = indexTypeConstructor(rv.meta.Storage, storeConfig, Config.analysisQueue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = rv.i.Open()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func(rv *indexImpl) {\n\t\tif !rv.open {\n\t\t\trv.i.Close()\n\t\t}\n\t}(rv)\n\n\t// now load the mapping\n\tindexReader, err := rv.i.Reader()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif cerr := indexReader.Close(); cerr != nil && err == nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tmappingBytes, err := indexReader.GetInternal(mappingInternalKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar im *mapping.IndexMappingImpl\n\terr = util.UnmarshalJSON(mappingBytes, &im)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing mapping JSON: %v\\nmapping contents:\\n%s\", err, string(mappingBytes))\n\t}\n\n\t// mark the index as open\n\trv.mutex.Lock()\n\tdefer rv.mutex.Unlock()\n\trv.open = true\n\n\t// validate the mapping\n\terr = im.Validate()\n\tif err != nil {\n\t\t// note even if the mapping is invalid\n\t\t// we still return an open usable index\n\t\treturn rv, err\n\t}\n\n\trv.m = im\n\tindexStats.Register(rv)\n\treturn rv, err\n}\n\n// Advanced returns internal index implementation\nfunc (i *indexImpl) Advanced() (index.Index, error) {\n\treturn i.i, nil\n}\n\n// Mapping returns the IndexMapping in use by this\n// Index.\nfunc (i *indexImpl) Mapping() mapping.IndexMapping {\n\treturn i.m\n}\n\n// Index the object with the specified identifier.\n// The IndexMapping for this index will determine\n// how the object is indexed.\nfunc (i *indexImpl) Index(id string, data interface{}) (err error) {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\ti.FireIndexEvent()\n\n\tdoc := document.NewDocument(id)\n\terr = i.m.MapDocument(doc, data)\n\tif err != nil {\n\t\treturn\n\t}\n\terr = i.i.Update(doc)\n\treturn\n}\n\n// IndexSynonym indexes a synonym definition, with the specified id and belonging to the specified collection.\n// Synonym definition defines term relationships for query expansion in searches.\nfunc (i *indexImpl) IndexSynonym(id string, collection string, definition *SynonymDefinition) error {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\ti.FireIndexEvent()\n\n\tsynMap, ok := i.m.(mapping.SynonymMapping)\n\tif !ok {\n\t\treturn ErrorSynonymSearchNotSupported\n\t}\n\n\tif err := definition.Validate(); err != nil {\n\t\treturn err\n\t}\n\n\tdoc := document.NewSynonymDocument(id)\n\terr := synMap.MapSynonymDocument(doc, collection, definition.Input, definition.Synonyms)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = i.i.Update(doc)\n\treturn err\n}\n\n// IndexAdvanced takes a document.Document object\n// skips the mapping and indexes it.\nfunc (i *indexImpl) IndexAdvanced(doc *document.Document) (err error) {\n\tif doc.ID() == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr = i.i.Update(doc)\n\treturn\n}\n\n// Delete entries for the specified identifier from\n// the index.\nfunc (i *indexImpl) Delete(id string) (err error) {\n\tif id == \"\" {\n\t\treturn ErrorEmptyID\n\t}\n\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\terr = i.i.Delete(id)\n\treturn\n}\n\n// Batch executes multiple Index and Delete\n// operations at the same time.  There are often\n// significant performance benefits when performing\n// operations in a batch.\nfunc (i *indexImpl) Batch(b *Batch) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\treturn i.i.Batch(b.internal)\n}\n\n// Document is used to find the values of all the\n// stored fields for a document in the index.  These\n// stored fields are put back into a Document object\n// and returned.\nfunc (i *indexImpl) Document(id string) (doc index.Document, err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif cerr := indexReader.Close(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tdoc, err = indexReader.Document(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn doc, nil\n}\n\n// DocCount returns the number of documents in the\n// index.\nfunc (i *indexImpl) DocCount() (count uint64, err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn 0, ErrorIndexClosed\n\t}\n\n\t// open a reader for this search\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"error opening index reader %v\", err)\n\t}\n\tdefer func() {\n\t\tif cerr := indexReader.Close(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tcount, err = indexReader.DocCount()\n\treturn\n}\n\n// Search executes a search request operation.\n// Returns a SearchResult object or an error.\nfunc (i *indexImpl) Search(req *SearchRequest) (sr *SearchResult, err error) {\n\treturn i.SearchInContext(context.Background(), req)\n}\n\nvar documentMatchEmptySize int\nvar searchContextEmptySize int\nvar facetResultEmptySize int\nvar documentEmptySize int\n\nfunc init() {\n\tvar dm search.DocumentMatch\n\tdocumentMatchEmptySize = dm.Size()\n\n\tvar sc search.SearchContext\n\tsearchContextEmptySize = sc.Size()\n\n\tvar fr search.FacetResult\n\tfacetResultEmptySize = fr.Size()\n\n\tvar d document.Document\n\tdocumentEmptySize = d.Size()\n}\n\n// memNeededForSearch is a helper function that returns an estimate of RAM\n// needed to execute a search request.\nfunc memNeededForSearch(req *SearchRequest,\n\tsearcher search.Searcher,\n\ttopnCollector *collector.TopNCollector) uint64 {\n\n\tbackingSize := req.Size + req.From + 1\n\tif req.Size+req.From > collector.PreAllocSizeSkipCap {\n\t\tbackingSize = collector.PreAllocSizeSkipCap + 1\n\t}\n\tnumDocMatches := backingSize + searcher.DocumentMatchPoolSize()\n\n\testimate := 0\n\n\t// overhead, size in bytes from collector\n\testimate += topnCollector.Size()\n\n\t// pre-allocing DocumentMatchPool\n\testimate += searchContextEmptySize + numDocMatches*documentMatchEmptySize\n\n\t// searcher overhead\n\testimate += searcher.Size()\n\n\t// overhead from results, lowestMatchOutsideResults\n\testimate += (numDocMatches + 1) * documentMatchEmptySize\n\n\t// additional overhead from SearchResult\n\testimate += reflectStaticSizeSearchResult + reflectStaticSizeSearchStatus\n\n\t// overhead from facet results\n\tif req.Facets != nil {\n\t\testimate += len(req.Facets) * facetResultEmptySize\n\t}\n\n\t// highlighting, store\n\tif len(req.Fields) > 0 || req.Highlight != nil {\n\t\t// Size + From => number of hits\n\t\testimate += (req.Size + req.From) * documentEmptySize\n\t}\n\n\treturn uint64(estimate)\n}\n\nfunc (i *indexImpl) preSearch(ctx context.Context, req *SearchRequest, reader index.IndexReader) (*SearchResult, error) {\n\tvar knnHits []*search.DocumentMatch\n\tvar err error\n\tif requestHasKNN(req) {\n\t\tknnHits, err = i.runKnnCollector(ctx, req, reader, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar fts search.FieldTermSynonymMap\n\tif !isMatchNoneQuery(req.Query) {\n\t\tif synMap, ok := i.m.(mapping.SynonymMapping); ok {\n\t\t\tif synReader, ok := reader.(index.ThesaurusReader); ok {\n\t\t\t\tfts, err = query.ExtractSynonyms(ctx, synMap, synReader, req.Query, fts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t},\n\t\tHits:          knnHits,\n\t\tSynonymResult: fts,\n\t}, nil\n}\n\n// SearchInContext executes a search request operation within the provided\n// Context. Returns a SearchResult object or an error.\nfunc (i *indexImpl) SearchInContext(ctx context.Context, req *SearchRequest) (sr *SearchResult, err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tsearchStart := time.Now()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\t// open a reader for this search\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error opening index reader %v\", err)\n\t}\n\tdefer func() {\n\t\tif cerr := indexReader.Close(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tif _, ok := ctx.Value(search.PreSearchKey).(bool); ok {\n\t\tpreSearchResult, err := i.preSearch(ctx, req, indexReader)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn preSearchResult, nil\n\t}\n\n\tvar reverseQueryExecution bool\n\tif req.SearchBefore != nil {\n\t\treverseQueryExecution = true\n\t\treq.Sort.Reverse()\n\t\treq.SearchAfter = req.SearchBefore\n\t\treq.SearchBefore = nil\n\t}\n\n\tvar coll *collector.TopNCollector\n\tif req.SearchAfter != nil {\n\t\tcoll = collector.NewTopNCollectorAfter(req.Size, req.Sort, req.SearchAfter)\n\t} else {\n\t\tcoll = collector.NewTopNCollector(req.Size, req.From, req.Sort)\n\t}\n\n\tvar knnHits []*search.DocumentMatch\n\tvar skipKNNCollector bool\n\n\tvar fts search.FieldTermSynonymMap\n\tvar skipSynonymCollector bool\n\n\tvar ok bool\n\tif req.PreSearchData != nil {\n\t\tfor k, v := range req.PreSearchData {\n\t\t\tswitch k {\n\t\t\tcase search.KnnPreSearchDataKey:\n\t\t\t\tif v != nil {\n\t\t\t\t\tknnHits, ok = v.([]*search.DocumentMatch)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"knn preSearchData must be of type []*search.DocumentMatch\")\n\t\t\t\t\t}\n\t\t\t\t\tskipKNNCollector = true\n\t\t\t\t}\n\t\t\tcase search.SynonymPreSearchDataKey:\n\t\t\t\tif v != nil {\n\t\t\t\t\tfts, ok = v.(search.FieldTermSynonymMap)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"synonym preSearchData must be of type search.FieldTermSynonymMap\")\n\t\t\t\t\t}\n\t\t\t\t\tskipSynonymCollector = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif !skipKNNCollector && requestHasKNN(req) {\n\t\tknnHits, err = i.runKnnCollector(ctx, req, indexReader, false)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif !skipSynonymCollector {\n\t\tif synMap, ok := i.m.(mapping.SynonymMapping); ok && synMap.SynonymCount() > 0 {\n\t\t\tif synReader, ok := indexReader.(index.ThesaurusReader); ok {\n\t\t\t\tfts, err = query.ExtractSynonyms(ctx, synMap, synReader, req.Query, fts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tsetKnnHitsInCollector(knnHits, req, coll)\n\n\tif fts != nil {\n\t\tctx = context.WithValue(ctx, search.FieldTermSynonymMapKey, fts)\n\t}\n\n\t// This callback and variable handles the tracking of bytes read\n\t//  1. as part of creation of tfr and its Next() calls which is\n\t//     accounted by invoking this callback when the TFR is closed.\n\t//  2. the docvalues portion (accounted in collector) and the retrieval\n\t//     of stored fields bytes (by LoadAndHighlightFields)\n\tvar totalSearchCost uint64\n\tsendBytesRead := func(bytesRead uint64) {\n\t\ttotalSearchCost += bytesRead\n\t}\n\n\tctx = context.WithValue(ctx, search.SearchIOStatsCallbackKey,\n\t\tsearch.SearchIOStatsCallbackFunc(sendBytesRead))\n\n\tvar bufPool *s2.GeoBufferPool\n\tgetBufferPool := func() *s2.GeoBufferPool {\n\t\tif bufPool == nil {\n\t\t\tbufPool = s2.NewGeoBufferPool(search.MaxGeoBufPoolSize, search.MinGeoBufPoolSize)\n\t\t}\n\n\t\treturn bufPool\n\t}\n\n\tctx = context.WithValue(ctx, search.GeoBufferPoolCallbackKey,\n\t\tsearch.GeoBufferPoolCallbackFunc(getBufferPool))\n\n\tsearcher, err := req.Query.Searcher(ctx, indexReader, i.m, search.SearcherOptions{\n\t\tExplain:            req.Explain,\n\t\tIncludeTermVectors: req.IncludeLocations || req.Highlight != nil,\n\t\tScore:              req.Score,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif serr := searcher.Close(); err == nil && serr != nil {\n\t\t\terr = serr\n\t\t}\n\t\tif sr != nil {\n\t\t\tsr.Cost = totalSearchCost\n\t\t}\n\t\tif sr, ok := indexReader.(*scorch.IndexSnapshot); ok {\n\t\t\tsr.UpdateIOStats(totalSearchCost)\n\t\t}\n\n\t\tsearch.RecordSearchCost(ctx, search.DoneM, 0)\n\t}()\n\n\tif req.Facets != nil {\n\t\tfacetsBuilder := search.NewFacetsBuilder(indexReader)\n\t\tfor facetName, facetRequest := range req.Facets {\n\t\t\tif facetRequest.NumericRanges != nil {\n\t\t\t\t// build numeric range facet\n\t\t\t\tfacetBuilder := facet.NewNumericFacetBuilder(facetRequest.Field, facetRequest.Size)\n\t\t\t\tfor _, nr := range facetRequest.NumericRanges {\n\t\t\t\t\tfacetBuilder.AddRange(nr.Name, nr.Min, nr.Max)\n\t\t\t\t}\n\t\t\t\tfacetsBuilder.Add(facetName, facetBuilder)\n\t\t\t} else if facetRequest.DateTimeRanges != nil {\n\t\t\t\t// build date range facet\n\t\t\t\tfacetBuilder := facet.NewDateTimeFacetBuilder(facetRequest.Field, facetRequest.Size)\n\t\t\t\tfor _, dr := range facetRequest.DateTimeRanges {\n\t\t\t\t\tdateTimeParserName := defaultDateTimeParser\n\t\t\t\t\tif dr.DateTimeParser != \"\" {\n\t\t\t\t\t\tdateTimeParserName = dr.DateTimeParser\n\t\t\t\t\t}\n\t\t\t\t\tdateTimeParser := i.m.DateTimeParserNamed(dateTimeParserName)\n\t\t\t\t\tif dateTimeParser == nil {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"no date time parser named `%s` registered\", dateTimeParserName)\n\t\t\t\t\t}\n\t\t\t\t\tstart, end, err := dr.ParseDates(dateTimeParser)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"ParseDates err: %v, using date time parser named %s\", err, dateTimeParserName)\n\t\t\t\t\t}\n\t\t\t\t\tif start.IsZero() && end.IsZero() {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"date range query must specify either start, end or both for date range name '%s'\", dr.Name)\n\t\t\t\t\t}\n\t\t\t\t\tfacetBuilder.AddRange(dr.Name, start, end)\n\t\t\t\t}\n\t\t\t\tfacetsBuilder.Add(facetName, facetBuilder)\n\t\t\t} else {\n\t\t\t\t// build terms facet\n\t\t\t\tfacetBuilder := facet.NewTermsFacetBuilder(facetRequest.Field, facetRequest.Size)\n\t\t\t\tfacetsBuilder.Add(facetName, facetBuilder)\n\t\t\t}\n\t\t}\n\t\tcoll.SetFacetsBuilder(facetsBuilder)\n\t}\n\n\tmemNeeded := memNeededForSearch(req, searcher, coll)\n\tif cb := ctx.Value(SearchQueryStartCallbackKey); cb != nil {\n\t\tif cbF, ok := cb.(SearchQueryStartCallbackFn); ok {\n\t\t\terr = cbF(memNeeded)\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif cb := ctx.Value(SearchQueryEndCallbackKey); cb != nil {\n\t\tif cbF, ok := cb.(SearchQueryEndCallbackFn); ok {\n\t\t\tdefer func() {\n\t\t\t\t_ = cbF(memNeeded)\n\t\t\t}()\n\t\t}\n\t}\n\n\terr = coll.Collect(ctx, searcher, indexReader)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\thits := coll.Results()\n\n\tvar highlighter highlight.Highlighter\n\n\tif req.Highlight != nil {\n\t\t// get the right highlighter\n\t\thighlighter, err = Config.Cache.HighlighterNamed(Config.DefaultHighlighter)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif req.Highlight.Style != nil {\n\t\t\thighlighter, err = Config.Cache.HighlighterNamed(*req.Highlight.Style)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tif highlighter == nil {\n\t\t\treturn nil, fmt.Errorf(\"no highlighter named `%s` registered\", *req.Highlight.Style)\n\t\t}\n\t}\n\n\tvar storedFieldsCost uint64\n\tfor _, hit := range hits {\n\t\t// KNN documents will already have their Index value set as part of the knn collector output\n\t\t// so check if the index is empty and set it to the current index name\n\t\tif i.name != \"\" && hit.Index == \"\" {\n\t\t\thit.Index = i.name\n\t\t}\n\t\terr, storedFieldsBytes := LoadAndHighlightFields(hit, req, i.name, indexReader, highlighter)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstoredFieldsCost += storedFieldsBytes\n\t}\n\n\ttotalSearchCost += storedFieldsCost\n\tsearch.RecordSearchCost(ctx, search.AddM, storedFieldsCost)\n\n\tatomic.AddUint64(&i.stats.searches, 1)\n\tsearchDuration := time.Since(searchStart)\n\tatomic.AddUint64(&i.stats.searchTime, uint64(searchDuration))\n\n\tif Config.SlowSearchLogThreshold > 0 &&\n\t\tsearchDuration > Config.SlowSearchLogThreshold {\n\t\tlogger.Printf(\"slow search took %s - %v\", searchDuration, req)\n\t}\n\n\tif reverseQueryExecution {\n\t\t// reverse the sort back to the original\n\t\treq.Sort.Reverse()\n\t\t// resort using the original order\n\t\tmhs := newSearchHitSorter(req.Sort, hits)\n\t\treq.SortFunc()(mhs)\n\t\t// reset request\n\t\treq.SearchBefore = req.SearchAfter\n\t\treq.SearchAfter = nil\n\t}\n\n\trv := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t},\n\t\tHits:     hits,\n\t\tTotal:    coll.Total(),\n\t\tMaxScore: coll.MaxScore(),\n\t\tTook:     searchDuration,\n\t\tFacets:   coll.FacetResults(),\n\t}\n\n\tif req.Explain {\n\t\trv.Request = req\n\t}\n\n\treturn rv, nil\n}\n\nfunc LoadAndHighlightFields(hit *search.DocumentMatch, req *SearchRequest,\n\tindexName string, r index.IndexReader,\n\thighlighter highlight.Highlighter) (error, uint64) {\n\tvar totalStoredFieldsBytes uint64\n\tif len(req.Fields) > 0 || highlighter != nil {\n\t\tdoc, err := r.Document(hit.ID)\n\t\tif err == nil && doc != nil {\n\t\t\tif len(req.Fields) > 0 && hit.Fields == nil {\n\t\t\t\ttotalStoredFieldsBytes = doc.StoredFieldsBytes()\n\t\t\t\tfieldsToLoad := deDuplicate(req.Fields)\n\t\t\t\tfor _, f := range fieldsToLoad {\n\t\t\t\t\tdoc.VisitFields(func(docF index.Field) {\n\t\t\t\t\t\tif f == \"*\" || docF.Name() == f {\n\t\t\t\t\t\t\tvar value interface{}\n\t\t\t\t\t\t\tswitch docF := docF.(type) {\n\t\t\t\t\t\t\tcase index.TextField:\n\t\t\t\t\t\t\t\tvalue = docF.Text()\n\t\t\t\t\t\t\tcase index.NumericField:\n\t\t\t\t\t\t\t\tnum, err := docF.Number()\n\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\tvalue = num\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcase index.DateTimeField:\n\t\t\t\t\t\t\t\tdatetime, layout, err := docF.DateTime()\n\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\tif layout == \"\" {\n\t\t\t\t\t\t\t\t\t\t// missing layout means we fallback to\n\t\t\t\t\t\t\t\t\t\t// the default layout which is RFC3339\n\t\t\t\t\t\t\t\t\t\tvalue = datetime.Format(time.RFC3339)\n\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t// the layout here can now either be representative\n\t\t\t\t\t\t\t\t\t\t// of an actual datetime layout or a timestamp\n\t\t\t\t\t\t\t\t\t\tswitch layout {\n\t\t\t\t\t\t\t\t\t\tcase seconds.Name:\n\t\t\t\t\t\t\t\t\t\t\tvalue = strconv.FormatInt(datetime.Unix(), 10)\n\t\t\t\t\t\t\t\t\t\tcase milliseconds.Name:\n\t\t\t\t\t\t\t\t\t\t\tvalue = strconv.FormatInt(datetime.UnixMilli(), 10)\n\t\t\t\t\t\t\t\t\t\tcase microseconds.Name:\n\t\t\t\t\t\t\t\t\t\t\tvalue = strconv.FormatInt(datetime.UnixMicro(), 10)\n\t\t\t\t\t\t\t\t\t\tcase nanoseconds.Name:\n\t\t\t\t\t\t\t\t\t\t\tvalue = strconv.FormatInt(datetime.UnixNano(), 10)\n\t\t\t\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\t\t\t\t// the layout for formatting the date to a string\n\t\t\t\t\t\t\t\t\t\t\t// is provided by a datetime parser which is not\n\t\t\t\t\t\t\t\t\t\t\t// handling the timestamp case, hence the layout\n\t\t\t\t\t\t\t\t\t\t\t// can be directly used to format the date\n\t\t\t\t\t\t\t\t\t\t\tvalue = datetime.Format(layout)\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcase index.BooleanField:\n\t\t\t\t\t\t\t\tboolean, err := docF.Boolean()\n\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\tvalue = boolean\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcase index.GeoPointField:\n\t\t\t\t\t\t\t\tlon, err := docF.Lon()\n\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\tlat, err := docF.Lat()\n\t\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\t\tvalue = []float64{lon, lat}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcase index.GeoShapeField:\n\t\t\t\t\t\t\t\tv, err := docF.GeoShape()\n\t\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\t\tvalue = v\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif value != nil {\n\t\t\t\t\t\t\t\thit.AddFieldValue(docF.Name(), value)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t\tif highlighter != nil {\n\t\t\t\thighlightFields := req.Highlight.Fields\n\t\t\t\tif highlightFields == nil {\n\t\t\t\t\t// add all fields with matches\n\t\t\t\t\thighlightFields = make([]string, 0, len(hit.Locations))\n\t\t\t\t\tfor k := range hit.Locations {\n\t\t\t\t\t\thighlightFields = append(highlightFields, k)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor _, hf := range highlightFields {\n\t\t\t\t\thighlighter.BestFragmentsInField(hit, doc, hf, 1)\n\t\t\t\t}\n\t\t\t}\n\t\t} else if doc == nil {\n\t\t\t// unexpected case, a doc ID that was found as a search hit\n\t\t\t// was unable to be found during document lookup\n\t\t\treturn ErrorIndexReadInconsistency, 0\n\t\t}\n\t}\n\n\treturn nil, totalStoredFieldsBytes\n}\n\n// Fields returns the name of all the fields this\n// Index has operated on.\nfunc (i *indexImpl) Fields() (fields []string, err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif cerr := indexReader.Close(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tfields, err = indexReader.Fields()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn fields, nil\n}\n\nfunc (i *indexImpl) FieldDict(field string) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := indexReader.FieldDict(field)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexImplFieldDict{\n\t\tindex:       i,\n\t\tindexReader: indexReader,\n\t\tfieldDict:   fieldDict,\n\t}, nil\n}\n\nfunc (i *indexImpl) FieldDictRange(field string, startTerm []byte, endTerm []byte) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := indexReader.FieldDictRange(field, startTerm, endTerm)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexImplFieldDict{\n\t\tindex:       i,\n\t\tindexReader: indexReader,\n\t\tfieldDict:   fieldDict,\n\t}, nil\n}\n\nfunc (i *indexImpl) FieldDictPrefix(field string, termPrefix []byte) (index.FieldDict, error) {\n\ti.mutex.RLock()\n\n\tif !i.open {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\tindexReader, err := i.i.Reader()\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\tfieldDict, err := indexReader.FieldDictPrefix(field, termPrefix)\n\tif err != nil {\n\t\ti.mutex.RUnlock()\n\t\treturn nil, err\n\t}\n\n\treturn &indexImplFieldDict{\n\t\tindex:       i,\n\t\tindexReader: indexReader,\n\t\tfieldDict:   fieldDict,\n\t}, nil\n}\n\nfunc (i *indexImpl) Close() error {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\n\tindexStats.UnRegister(i)\n\n\ti.open = false\n\treturn i.i.Close()\n}\n\nfunc (i *indexImpl) Stats() *IndexStat {\n\treturn i.stats\n}\n\nfunc (i *indexImpl) StatsMap() map[string]interface{} {\n\treturn i.stats.statsMap()\n}\n\nfunc (i *indexImpl) GetInternal(key []byte) (val []byte, err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn nil, ErrorIndexClosed\n\t}\n\n\treader, err := i.i.Reader()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif cerr := reader.Close(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\tval, err = reader.GetInternal(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn val, nil\n}\n\nfunc (i *indexImpl) SetInternal(key, val []byte) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\treturn i.i.SetInternal(key, val)\n}\n\nfunc (i *indexImpl) DeleteInternal(key []byte) error {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\treturn i.i.DeleteInternal(key)\n}\n\n// NewBatch creates a new empty batch.\nfunc (i *indexImpl) NewBatch() *Batch {\n\treturn &Batch{\n\t\tindex:    i,\n\t\tinternal: index.NewBatch(),\n\t}\n}\n\nfunc (i *indexImpl) Name() string {\n\treturn i.name\n}\n\nfunc (i *indexImpl) SetName(name string) {\n\tindexStats.UnRegister(i)\n\ti.name = name\n\tindexStats.Register(i)\n}\n\ntype indexImplFieldDict struct {\n\tindex       *indexImpl\n\tindexReader index.IndexReader\n\tfieldDict   index.FieldDict\n}\n\nfunc (f *indexImplFieldDict) BytesRead() uint64 {\n\treturn f.fieldDict.BytesRead()\n}\n\nfunc (f *indexImplFieldDict) Next() (*index.DictEntry, error) {\n\treturn f.fieldDict.Next()\n}\n\nfunc (f *indexImplFieldDict) Close() error {\n\tdefer f.index.mutex.RUnlock()\n\terr := f.fieldDict.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn f.indexReader.Close()\n}\n\n// helper function to remove duplicate entries from slice of strings\nfunc deDuplicate(fields []string) []string {\n\tentries := make(map[string]struct{})\n\tret := []string{}\n\tfor _, entry := range fields {\n\t\tif _, exists := entries[entry]; !exists {\n\t\t\tentries[entry] = struct{}{}\n\t\t\tret = append(ret, entry)\n\t\t}\n\t}\n\treturn ret\n}\n\ntype searchHitSorter struct {\n\thits          search.DocumentMatchCollection\n\tsort          search.SortOrder\n\tcachedScoring []bool\n\tcachedDesc    []bool\n}\n\nfunc newSearchHitSorter(sort search.SortOrder, hits search.DocumentMatchCollection) *searchHitSorter {\n\treturn &searchHitSorter{\n\t\tsort:          sort,\n\t\thits:          hits,\n\t\tcachedScoring: sort.CacheIsScore(),\n\t\tcachedDesc:    sort.CacheDescending(),\n\t}\n}\n\nfunc (m *searchHitSorter) Len() int      { return len(m.hits) }\nfunc (m *searchHitSorter) Swap(i, j int) { m.hits[i], m.hits[j] = m.hits[j], m.hits[i] }\nfunc (m *searchHitSorter) Less(i, j int) bool {\n\tc := m.sort.Compare(m.cachedScoring, m.cachedDesc, m.hits[i], m.hits[j])\n\treturn c < 0\n}\n\nfunc (i *indexImpl) CopyTo(d index.Directory) (err error) {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\n\tif !i.open {\n\t\treturn ErrorIndexClosed\n\t}\n\n\tcopyIndex, ok := i.i.(index.CopyIndex)\n\tif !ok {\n\t\treturn fmt.Errorf(\"index implementation does not support copy reader\")\n\t}\n\n\tcopyReader := copyIndex.CopyReader()\n\tif copyReader == nil {\n\t\treturn fmt.Errorf(\"index's copyReader is nil\")\n\t}\n\n\tdefer func() {\n\t\tif cerr := copyReader.CloseCopyReader(); err == nil && cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\n\terr = copyReader.CopyTo(d)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error copying index metadata: %v\", err)\n\t}\n\n\t// copy the metadata\n\treturn i.meta.CopyTo(d)\n}\n\nfunc (f FileSystemDirectory) GetWriter(filePath string) (io.WriteCloser,\n\terror) {\n\tdir, file := filepath.Split(filePath)\n\tif dir != \"\" {\n\t\terr := os.MkdirAll(filepath.Join(string(f), dir), os.ModePerm)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn os.OpenFile(filepath.Join(string(f), dir, file),\n\t\tos.O_RDWR|os.O_CREATE, 0600)\n}\n\nfunc (i *indexImpl) FireIndexEvent() {\n\t// get the internal index implementation\n\tinternalIndex, err := i.Advanced()\n\tif err != nil {\n\t\treturn\n\t}\n\t// check if the internal index implementation supports events\n\tif internalEventIndex, ok := internalIndex.(index.EventIndex); ok {\n\t\t// fire the Index() event\n\t\tinternalEventIndex.FireIndexEvent()\n\t}\n}\n"
        },
        {
          "name": "index_meta.go",
          "type": "blob",
          "size": 2.7705078125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown\"\n\t\"github.com/blevesearch/bleve/v2/util\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nconst metaFilename = \"index_meta.json\"\n\ntype indexMeta struct {\n\tStorage   string                 `json:\"storage\"`\n\tIndexType string                 `json:\"index_type\"`\n\tConfig    map[string]interface{} `json:\"config,omitempty\"`\n}\n\nfunc newIndexMeta(indexType string, storage string, config map[string]interface{}) *indexMeta {\n\treturn &indexMeta{\n\t\tIndexType: indexType,\n\t\tStorage:   storage,\n\t\tConfig:    config,\n\t}\n}\n\nfunc openIndexMeta(path string) (*indexMeta, error) {\n\tif _, err := os.Stat(path); os.IsNotExist(err) {\n\t\treturn nil, ErrorIndexPathDoesNotExist\n\t}\n\tindexMetaPath := indexMetaPath(path)\n\tmetaBytes, err := os.ReadFile(indexMetaPath)\n\tif err != nil {\n\t\treturn nil, ErrorIndexMetaMissing\n\t}\n\tvar im indexMeta\n\terr = util.UnmarshalJSON(metaBytes, &im)\n\tif err != nil {\n\t\treturn nil, ErrorIndexMetaCorrupt\n\t}\n\tif im.IndexType == \"\" {\n\t\tim.IndexType = upsidedown.Name\n\t}\n\treturn &im, nil\n}\n\nfunc (i *indexMeta) Save(path string) (err error) {\n\tindexMetaPath := indexMetaPath(path)\n\t// ensure any necessary parent directories exist\n\terr = os.MkdirAll(path, 0700)\n\tif err != nil {\n\t\tif os.IsExist(err) {\n\t\t\treturn ErrorIndexPathExists\n\t\t}\n\t\treturn err\n\t}\n\tmetaBytes, err := util.MarshalJSON(i)\n\tif err != nil {\n\t\treturn err\n\t}\n\tindexMetaFile, err := os.OpenFile(indexMetaPath, os.O_RDWR|os.O_CREATE|os.O_EXCL, 0666)\n\tif err != nil {\n\t\tif os.IsExist(err) {\n\t\t\treturn ErrorIndexPathExists\n\t\t}\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif ierr := indexMetaFile.Close(); err == nil && ierr != nil {\n\t\t\terr = ierr\n\t\t}\n\t}()\n\t_, err = indexMetaFile.Write(metaBytes)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (i *indexMeta) CopyTo(d index.Directory) (err error) {\n\tmetaBytes, err := util.MarshalJSON(i)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tw, err := d.GetWriter(metaFilename)\n\tif w == nil || err != nil {\n\t\treturn fmt.Errorf(\"invalid writer for file: %s, err: %v\",\n\t\t\tmetaFilename, err)\n\t}\n\tdefer w.Close()\n\n\t_, err = w.Write(metaBytes)\n\treturn err\n}\n\nfunc indexMetaPath(path string) string {\n\treturn filepath.Join(path, metaFilename)\n}\n"
        },
        {
          "name": "index_meta_test.go",
          "type": "blob",
          "size": 1.3798828125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"os\"\n\t\"testing\"\n)\n\nfunc TestIndexMeta(t *testing.T) {\n\tvar testIndexPath = \"doesnotexit.bleve\"\n\tdefer func() {\n\t\terr := os.RemoveAll(testIndexPath)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// open non-existent meta should give an error\n\t_, err := openIndexMeta(testIndexPath)\n\tif err == nil {\n\t\tt.Errorf(\"expected error, got nil\")\n\t}\n\n\t// create meta\n\tim := &indexMeta{Storage: \"boltdb\"}\n\terr = im.Save(testIndexPath)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tim = nil\n\n\t// open a meta that exists\n\tim, err = openIndexMeta(testIndexPath)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif im.Storage != \"boltdb\" {\n\t\tt.Errorf(\"expected storage 'boltdb', got '%s'\", im.Storage)\n\t}\n\n\t// save a meta that already exists\n\terr = im.Save(testIndexPath)\n\tif err == nil {\n\t\tt.Errorf(\"expected error, got nil\")\n\t}\n}\n"
        },
        {
          "name": "index_stats.go",
          "type": "blob",
          "size": 1.70703125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"encoding/json\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\ntype IndexStat struct {\n\tsearches   uint64\n\tsearchTime uint64\n\ti          *indexImpl\n}\n\nfunc (is *IndexStat) statsMap() map[string]interface{} {\n\tm := map[string]interface{}{}\n\tm[\"index\"] = is.i.i.StatsMap()\n\tm[\"searches\"] = atomic.LoadUint64(&is.searches)\n\tm[\"search_time\"] = atomic.LoadUint64(&is.searchTime)\n\treturn m\n}\n\nfunc (is *IndexStat) MarshalJSON() ([]byte, error) {\n\tm := is.statsMap()\n\treturn json.Marshal(m)\n}\n\ntype IndexStats struct {\n\tindexes map[string]*IndexStat\n\tmutex   sync.RWMutex\n}\n\nfunc NewIndexStats() *IndexStats {\n\treturn &IndexStats{\n\t\tindexes: make(map[string]*IndexStat),\n\t}\n}\n\nfunc (i *IndexStats) Register(index Index) {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\ti.indexes[index.Name()] = index.Stats()\n}\n\nfunc (i *IndexStats) UnRegister(index Index) {\n\ti.mutex.Lock()\n\tdefer i.mutex.Unlock()\n\tdelete(i.indexes, index.Name())\n}\n\nfunc (i *IndexStats) String() string {\n\ti.mutex.RLock()\n\tdefer i.mutex.RUnlock()\n\tbytes, err := json.Marshal(i.indexes)\n\tif err != nil {\n\t\treturn \"error marshaling stats\"\n\t}\n\treturn string(bytes)\n}\n\nvar indexStats *IndexStats\n"
        },
        {
          "name": "index_test.go",
          "type": "blob",
          "size": 67.50390625,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/keyword\"\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown/store/boltdb\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown/store/null\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown\"\n)\n\ntype Fatalfable interface {\n\tFatalf(format string, args ...interface{})\n}\n\nfunc createTmpIndexPath(f Fatalfable) string {\n\ttmpIndexPath, err := os.MkdirTemp(\"\", \"bleve-testidx\")\n\tif err != nil {\n\t\tf.Fatalf(\"error creating temp dir: %v\", err)\n\t}\n\treturn tmpIndexPath\n}\n\nfunc cleanupTmpIndexPath(f Fatalfable, path string) {\n\terr := os.RemoveAll(path)\n\tif err != nil {\n\t\tf.Fatalf(\"error removing temp dir: %v\", err)\n\t}\n}\n\nfunc TestCrud(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoca := map[string]interface{}{\n\t\t\"name\": \"marty\",\n\t\t\"desc\": \"gophercon india\",\n\t}\n\terr = idx.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocy := map[string]interface{}{\n\t\t\"name\": \"jasper\",\n\t\t\"desc\": \"clojure\",\n\t}\n\terr = idx.Index(\"y\", docy)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\terr = idx.Delete(\"y\")\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocx := map[string]interface{}{\n\t\t\"name\": \"rose\",\n\t\t\"desc\": \"googler\",\n\t}\n\terr = idx.Index(\"x\", docx)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\terr = idx.SetInternal([]byte(\"status\"), []byte(\"pending\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocb := map[string]interface{}{\n\t\t\"name\": \"steve\",\n\t\t\"desc\": \"cbft master\",\n\t}\n\tbatch := idx.NewBatch()\n\terr = batch.Index(\"b\", docb)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatch.Delete(\"x\")\n\tbatch.SetInternal([]byte(\"batchi\"), []byte(\"batchv\"))\n\tbatch.DeleteInternal([]byte(\"status\"))\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tval, err := idx.GetInternal([]byte(\"batchi\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif string(val) != \"batchv\" {\n\t\tt.Errorf(\"expected 'batchv', got '%s'\", val)\n\t}\n\tval, err = idx.GetInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif val != nil {\n\t\tt.Errorf(\"expected nil, got '%s'\", val)\n\t}\n\n\terr = idx.SetInternal([]byte(\"seqno\"), []byte(\"7\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = idx.SetInternal([]byte(\"status\"), []byte(\"ready\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = idx.DeleteInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tval, err = idx.GetInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif val != nil {\n\t\tt.Errorf(\"expected nil, got '%s'\", val)\n\t}\n\n\tval, err = idx.GetInternal([]byte(\"seqno\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif string(val) != \"7\" {\n\t\tt.Errorf(\"expected '7', got '%s'\", val)\n\t}\n\n\t// close the index, open it again, and try some more things\n\terr = idx.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidx, err = Open(tmpIndexPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcount, err := idx.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif count != 2 {\n\t\tt.Errorf(\"expected doc count 2, got %d\", count)\n\t}\n\n\tdoc, err := idx.Document(\"a\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfoundNameField := false\n\tdoc.VisitFields(func(field index.Field) {\n\t\tif field.Name() == \"name\" && string(field.Value()) == \"marty\" {\n\t\t\tfoundNameField = true\n\t\t}\n\t})\n\tif !foundNameField {\n\t\tt.Errorf(\"expected to find field named 'name' with value 'marty'\")\n\t}\n\n\tfields, err := idx.Fields()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\texpectedFields := map[string]bool{\n\t\t\"_all\": false,\n\t\t\"name\": false,\n\t\t\"desc\": false,\n\t}\n\tif len(fields) < len(expectedFields) {\n\t\tt.Fatalf(\"expected %d fields got %d\", len(expectedFields), len(fields))\n\t}\n\tfor _, f := range fields {\n\t\texpectedFields[f] = true\n\t}\n\tfor ef, efp := range expectedFields {\n\t\tif !efp {\n\t\t\tt.Errorf(\"field %s is missing\", ef)\n\t\t}\n\t}\n}\n\nfunc approxSame(actual, expected uint64) bool {\n\tmodulus := func(a, b uint64) uint64 {\n\t\tif a > b {\n\t\t\treturn a - b\n\t\t}\n\t\treturn b - a\n\t}\n\n\treturn float64(modulus(actual, expected))/float64(expected) < float64(0.30)\n}\n\nfunc checkStatsOnIndexedBatch(indexPath string, indexMapping mapping.IndexMapping,\n\texpectedVal uint64) error {\n\tvar wg sync.WaitGroup\n\tvar statValError error\n\n\tidx, err := NewUsing(indexPath, indexMapping, Config.DefaultIndexType, Config.DefaultMemKVStore, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tbatch, err := getBatchFromData(idx, \"sample-data.json\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to form a batch %v\\n\", err)\n\t}\n\twg.Add(1)\n\tbatch.SetPersistedCallback(func(e error) {\n\t\tdefer wg.Done()\n\t\tstats, _ := idx.StatsMap()[\"index\"].(map[string]interface{})\n\t\tbytesWritten, _ := stats[\"num_bytes_written_at_index_time\"].(uint64)\n\t\tif !approxSame(bytesWritten, expectedVal) {\n\t\t\tstatValError = fmt.Errorf(\"expected bytes written is %d, got %v\", expectedVal,\n\t\t\t\tbytesWritten)\n\t\t}\n\t})\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to index batch %v\\n\", err)\n\t}\n\twg.Wait()\n\tidx.Close()\n\n\treturn statValError\n}\n\nfunc TestBytesWritten(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"hotel\", documentMapping)\n\n\tindexMapping.DocValuesDynamic = false\n\tindexMapping.StoreDynamic = false\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Index = true\n\tcontentFieldMapping.Store = false\n\tcontentFieldMapping.IncludeInAll = false\n\tcontentFieldMapping.IncludeTermVectors = false\n\tcontentFieldMapping.DocValues = false\n\n\treviewsMapping := NewDocumentMapping()\n\treviewsMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tdocumentMapping.AddSubDocumentMapping(\"reviews\", reviewsMapping)\n\n\ttypeFieldMapping := NewTextFieldMapping()\n\ttypeFieldMapping.Store = false\n\ttypeFieldMapping.IncludeInAll = false\n\ttypeFieldMapping.IncludeTermVectors = false\n\ttypeFieldMapping.DocValues = false\n\tdocumentMapping.AddFieldMappingsAt(\"type\", typeFieldMapping)\n\n\terr = checkStatsOnIndexedBatch(tmpIndexPath, indexMapping, 57273)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcleanupTmpIndexPath(t, tmpIndexPath)\n\n\tcontentFieldMapping.Store = true\n\ttmpIndexPath1 := createTmpIndexPath(t)\n\n\terr := checkStatsOnIndexedBatch(tmpIndexPath1, indexMapping, 76069)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcleanupTmpIndexPath(t, tmpIndexPath1)\n\n\tcontentFieldMapping.Store = false\n\tcontentFieldMapping.IncludeInAll = true\n\ttmpIndexPath2 := createTmpIndexPath(t)\n\n\terr = checkStatsOnIndexedBatch(tmpIndexPath2, indexMapping, 68875)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcleanupTmpIndexPath(t, tmpIndexPath2)\n\n\tcontentFieldMapping.IncludeInAll = false\n\tcontentFieldMapping.IncludeTermVectors = true\n\ttmpIndexPath3 := createTmpIndexPath(t)\n\n\terr = checkStatsOnIndexedBatch(tmpIndexPath3, indexMapping, 78985)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcleanupTmpIndexPath(t, tmpIndexPath3)\n\n\tcontentFieldMapping.IncludeTermVectors = false\n\tcontentFieldMapping.DocValues = true\n\ttmpIndexPath4 := createTmpIndexPath(t)\n\n\terr = checkStatsOnIndexedBatch(tmpIndexPath4, indexMapping, 64228)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcleanupTmpIndexPath(t, tmpIndexPath4)\n}\n\nfunc TestBytesRead(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"hotel\", documentMapping)\n\tindexMapping.StoreDynamic = false\n\tindexMapping.DocValuesDynamic = false\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Store = false\n\n\treviewsMapping := NewDocumentMapping()\n\treviewsMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tdocumentMapping.AddSubDocumentMapping(\"reviews\", reviewsMapping)\n\n\ttypeFieldMapping := NewTextFieldMapping()\n\ttypeFieldMapping.Store = false\n\tdocumentMapping.AddFieldMappingsAt(\"type\", typeFieldMapping)\n\n\tidx, err := NewUsing(tmpIndexPath, indexMapping, Config.DefaultIndexType, Config.DefaultMemKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch, err := getBatchFromData(idx, \"sample-data.json\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to form a batch\")\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to index batch %v\\n\", err)\n\t}\n\tquery := NewQueryStringQuery(\"united\")\n\tsearchRequest := NewSearchRequestOptions(query, int(10), 0, true)\n\n\tres, err := idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tstats, _ := idx.StatsMap()[\"index\"].(map[string]interface{})\n\tprevBytesRead, _ := stats[\"num_bytes_read_at_query_time\"].(uint64)\n\n\texpectedBytesRead := uint64(22049)\n\tif supportForVectorSearch {\n\t\texpectedBytesRead = 22459\n\t}\n\n\tif prevBytesRead != expectedBytesRead && res.Cost == prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for query string %v, got %v\",\n\t\t\texpectedBytesRead, prevBytesRead)\n\t}\n\n\t// subsequent queries on the same field results in lesser amount\n\t// of bytes read because the segment static and dictionary is reused and not\n\t// loaded from mmap'd filed\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ := stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 66 && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for query string 66, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tfuzz := NewFuzzyQuery(\"hotel\")\n\tfuzz.FieldVal = \"reviews.content\"\n\tfuzz.Fuzziness = 2\n\tsearchRequest = NewSearchRequest(fuzz)\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 8468 && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for fuzzy query is 8468, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\ttypeFacet := NewFacetRequest(\"type\", 2)\n\tquery = NewQueryStringQuery(\"united\")\n\tsearchRequest = NewSearchRequestOptions(query, int(0), 0, true)\n\tsearchRequest.AddFacet(\"types\", typeFacet)\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif !approxSame(bytesRead-prevBytesRead, 196) && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for faceted query is around 196, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tmin := float64(8660)\n\tmax := float64(8665)\n\tnumRangeQuery := NewNumericRangeQuery(&min, &max)\n\tnumRangeQuery.FieldVal = \"id\"\n\tsearchRequest = NewSearchRequestOptions(numRangeQuery, int(10), 0, true)\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 924 && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for numeric range query is 924, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tsearchRequest = NewSearchRequestOptions(query, int(10), 0, true)\n\tsearchRequest.Highlight = &HighlightRequest{}\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 105 && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for query with highlighter is 105, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tdisQuery := NewDisjunctionQuery(NewMatchQuery(\"hotel\"), NewMatchQuery(\"united\"))\n\tsearchRequest = NewSearchRequestOptions(disQuery, int(10), 0, true)\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\t// expectation is that the bytes read is roughly equal to sum of sub queries in\n\t// the disjunction query plus the segment loading portion for the second subquery\n\t// since it's created afresh and not reused\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 120 && res.Cost == bytesRead-prevBytesRead {\n\t\tt.Fatalf(\"expected bytes read for disjunction query is 120, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n}\n\nfunc TestBytesReadStored(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"hotel\", documentMapping)\n\n\tindexMapping.DocValuesDynamic = false\n\tindexMapping.StoreDynamic = false\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Store = true\n\tcontentFieldMapping.IncludeInAll = false\n\tcontentFieldMapping.IncludeTermVectors = false\n\n\treviewsMapping := NewDocumentMapping()\n\treviewsMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tdocumentMapping.AddSubDocumentMapping(\"reviews\", reviewsMapping)\n\n\ttypeFieldMapping := NewTextFieldMapping()\n\ttypeFieldMapping.Store = false\n\ttypeFieldMapping.IncludeInAll = false\n\ttypeFieldMapping.IncludeTermVectors = false\n\tdocumentMapping.AddFieldMappingsAt(\"type\", typeFieldMapping)\n\tidx, err := NewUsing(tmpIndexPath, indexMapping, Config.DefaultIndexType, Config.DefaultMemKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tbatch, err := getBatchFromData(idx, \"sample-data.json\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to form a batch %v\\n\", err)\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to index batch %v\\n\", err)\n\t}\n\tquery := NewTermQuery(\"hotel\")\n\tquery.FieldVal = \"reviews.content\"\n\tsearchRequest := NewSearchRequestOptions(query, int(10), 0, true)\n\tres, err := idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ := idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ := stats[\"num_bytes_read_at_query_time\"].(uint64)\n\n\texpectedBytesRead := uint64(11911)\n\tif supportForVectorSearch {\n\t\texpectedBytesRead = 12321\n\t}\n\n\tif bytesRead != expectedBytesRead && bytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around %v, got %v\", expectedBytesRead, bytesRead)\n\t}\n\tprevBytesRead := bytesRead\n\n\tsearchRequest = NewSearchRequestOptions(query, int(10), 0, true)\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 48 && bytesRead-prevBytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around 48, got %v\", bytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tsearchRequest = NewSearchRequestOptions(query, int(10), 0, true)\n\tsearchRequest.Fields = []string{\"*\"}\n\tres, err = idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\n\tif bytesRead-prevBytesRead != 26511 && bytesRead-prevBytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around 26511, got %v\",\n\t\t\tbytesRead-prevBytesRead)\n\t}\n\tidx.Close()\n\tcleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// same type of querying but on field \"type\"\n\tcontentFieldMapping.Store = false\n\ttypeFieldMapping.Store = true\n\n\ttmpIndexPath1 := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath1)\n\n\tidx1, err := NewUsing(tmpIndexPath1, indexMapping, Config.DefaultIndexType, Config.DefaultMemKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx1.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch, err = getBatchFromData(idx1, \"sample-data.json\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to form a batch %v\\n\", err)\n\t}\n\terr = idx1.Batch(batch)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to index batch %v\\n\", err)\n\t}\n\n\tquery = NewTermQuery(\"hotel\")\n\tquery.FieldVal = \"type\"\n\tsearchRequest = NewSearchRequestOptions(query, int(10), 0, true)\n\tres, err = idx1.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx1.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\n\texpectedBytesRead = uint64(4097)\n\tif supportForVectorSearch {\n\t\texpectedBytesRead = 4507\n\t}\n\n\tif bytesRead != expectedBytesRead && bytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around %v, got %v\", expectedBytesRead, bytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tres, err = idx1.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tstats, _ = idx1.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 47 && bytesRead-prevBytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around 47, got %v\", bytesRead-prevBytesRead)\n\t}\n\tprevBytesRead = bytesRead\n\n\tsearchRequest.Fields = []string{\"*\"}\n\tres, err = idx1.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tstats, _ = idx1.StatsMap()[\"index\"].(map[string]interface{})\n\tbytesRead, _ = stats[\"num_bytes_read_at_query_time\"].(uint64)\n\tif bytesRead-prevBytesRead != 77 && bytesRead-prevBytesRead == res.Cost {\n\t\tt.Fatalf(\"expected the bytes read stat to be around 77, got %v\", bytesRead-prevBytesRead)\n\t}\n}\n\nfunc getBatchFromData(idx Index, fileName string) (*Batch, error) {\n\tpwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpath := filepath.Join(pwd, \"data\", \"test\", fileName)\n\tbatch := idx.NewBatch()\n\tvar dataset []map[string]interface{}\n\tfileContent, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = json.Unmarshal(fileContent, &dataset)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, doc := range dataset {\n\t\terr = batch.Index(fmt.Sprintf(\"%d\", doc[\"id\"]), doc)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn batch, err\n}\n\nfunc TestIndexCreateNewOverExisting(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tindex, err = New(tmpIndexPath, NewIndexMapping())\n\tif err != ErrorIndexPathExists {\n\t\tt.Fatalf(\"expected error index path exists, got %v\", err)\n\t}\n}\n\nfunc TestIndexOpenNonExisting(t *testing.T) {\n\t_, err := Open(\"doesnotexist\")\n\tif err != ErrorIndexPathDoesNotExist {\n\t\tt.Fatalf(\"expected error index path does not exist, got %v\", err)\n\t}\n}\n\nfunc TestIndexOpenMetaMissingOrCorrupt(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttmpIndexPathMeta := filepath.Join(tmpIndexPath, \"index_meta.json\")\n\n\t// now intentionally change the storage type\n\terr = os.WriteFile(tmpIndexPathMeta, []byte(`{\"storage\":\"mystery\"}`), 0666)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tindex, err = Open(tmpIndexPath)\n\tif err == nil {\n\t\tt.Fatalf(\"expected error for unknown storage type, got %v\", err)\n\t}\n\n\t// now intentionally corrupt the metadata\n\terr = os.WriteFile(tmpIndexPathMeta, []byte(\"corrupted\"), 0666)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tindex, err = Open(tmpIndexPath)\n\tif err != ErrorIndexMetaCorrupt {\n\t\tt.Fatalf(\"expected error index metadata corrupted, got %v\", err)\n\t}\n\n\t// now intentionally remove the metadata\n\terr = os.Remove(tmpIndexPathMeta)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tindex, err = Open(tmpIndexPath)\n\tif err != ErrorIndexMetaMissing {\n\t\tt.Fatalf(\"expected error index metadata missing, got %v\", err)\n\t}\n}\n\nfunc TestInMemIndex(t *testing.T) {\n\n\tindex, err := NewMemOnly(NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestClosedIndex(t *testing.T) {\n\tindex, err := NewMemOnly(NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = index.Index(\"test\", \"test\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\terr = index.Delete(\"test\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\tb := index.NewBatch()\n\terr = index.Batch(b)\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\t_, err = index.Document(\"test\")\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\t_, err = index.DocCount()\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\t_, err = index.Search(NewSearchRequest(NewTermQuery(\"test\")))\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n\n\t_, err = index.Fields()\n\tif err != ErrorIndexClosed {\n\t\tt.Errorf(\"expected error index closed, got %v\", err)\n\t}\n}\n\ntype slowQuery struct {\n\tactual query.Query\n\tdelay  time.Duration\n}\n\nfunc (s *slowQuery) Searcher(ctx context.Context, i index.IndexReader, m mapping.IndexMapping, options search.SearcherOptions) (search.Searcher, error) {\n\ttime.Sleep(s.delay)\n\treturn s.actual.Searcher(ctx, i, m, options)\n}\n\nfunc TestSlowSearch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tdefer func() {\n\t\t// reset logger back to normal\n\t\tSetLog(log.New(io.Discard, \"bleve\", log.LstdFlags))\n\t}()\n\t// set custom logger\n\tvar sdw sawDataWriter\n\tSetLog(log.New(&sdw, \"bleve\", log.LstdFlags))\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tConfig.SlowSearchLogThreshold = 1 * time.Minute\n\n\tquery := NewTermQuery(\"water\")\n\treq := NewSearchRequest(query)\n\t_, err = index.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif sdw.sawData {\n\t\tt.Errorf(\"expected to not see slow query logged, but did\")\n\t}\n\n\tsq := &slowQuery{\n\t\tactual: query,\n\t\tdelay:  50 * time.Millisecond, // on Windows timer resolution is 15ms\n\t}\n\treq.Query = sq\n\tConfig.SlowSearchLogThreshold = 1 * time.Microsecond\n\t_, err = index.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !sdw.sawData {\n\t\tt.Errorf(\"expected to see slow query logged, but didn't\")\n\t}\n}\n\ntype sawDataWriter struct {\n\tsawData bool\n}\n\nfunc (s *sawDataWriter) Write(p []byte) (n int, err error) {\n\ts.sawData = true\n\treturn len(p), nil\n}\n\nfunc TestStoredFieldPreserved(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoca := map[string]interface{}{\n\t\t\"name\": \"Marty\",\n\t\t\"desc\": \"GopherCON India\",\n\t\t\"bool\": true,\n\t\t\"num\":  float64(1),\n\t}\n\terr = index.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tq := NewTermQuery(\"marty\")\n\treq := NewSearchRequest(q)\n\treq.Fields = []string{\"name\", \"desc\", \"bool\", \"num\"}\n\tres, err := index.Search(req)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif len(res.Hits) != 1 {\n\t\tt.Fatalf(\"expected 1 hit, got %d\", len(res.Hits))\n\t}\n\tif res.Hits[0].Fields[\"name\"] != \"Marty\" {\n\t\tt.Errorf(\"expected 'Marty' got '%s'\", res.Hits[0].Fields[\"name\"])\n\t}\n\tif res.Hits[0].Fields[\"desc\"] != \"GopherCON India\" {\n\t\tt.Errorf(\"expected 'GopherCON India' got '%s'\", res.Hits[0].Fields[\"desc\"])\n\t}\n\tif res.Hits[0].Fields[\"num\"] != float64(1) {\n\t\tt.Errorf(\"expected '1' got '%v'\", res.Hits[0].Fields[\"num\"])\n\t}\n\tif res.Hits[0].Fields[\"bool\"] != true {\n\t\tt.Errorf(\"expected 'true' got '%v'\", res.Hits[0].Fields[\"bool\"])\n\t}\n}\n\nfunc TestDict(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoca := map[string]interface{}{\n\t\t\"name\": \"marty\",\n\t\t\"desc\": \"gophercon india\",\n\t}\n\terr = index.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocy := map[string]interface{}{\n\t\t\"name\": \"jasper\",\n\t\t\"desc\": \"clojure\",\n\t}\n\terr = index.Index(\"y\", docy)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocx := map[string]interface{}{\n\t\t\"name\": \"rose\",\n\t\t\"desc\": \"googler\",\n\t}\n\terr = index.Index(\"x\", docx)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdict, err := index.FieldDict(\"name\")\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tterms := []string{}\n\tde, err := dict.Next()\n\tfor err == nil && de != nil {\n\t\tterms = append(terms, string(de.Term))\n\t\tde, err = dict.Next()\n\t}\n\n\texpectedTerms := []string{\"jasper\", \"marty\", \"rose\"}\n\tif !reflect.DeepEqual(terms, expectedTerms) {\n\t\tt.Errorf(\"expected %v, got %v\", expectedTerms, terms)\n\t}\n\n\terr = dict.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// test start and end range\n\tdict, err = index.FieldDictRange(\"name\", []byte(\"marty\"), []byte(\"rose\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tterms = []string{}\n\tde, err = dict.Next()\n\tfor err == nil && de != nil {\n\t\tterms = append(terms, string(de.Term))\n\t\tde, err = dict.Next()\n\t}\n\n\texpectedTerms = []string{\"marty\", \"rose\"}\n\tif !reflect.DeepEqual(terms, expectedTerms) {\n\t\tt.Errorf(\"expected %v, got %v\", expectedTerms, terms)\n\t}\n\n\terr = dict.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdocz := map[string]interface{}{\n\t\t\"name\": \"prefix\",\n\t\t\"desc\": \"bob cat cats catting dog doggy zoo\",\n\t}\n\terr = index.Index(\"z\", docz)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdict, err = index.FieldDictPrefix(\"desc\", []byte(\"cat\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tterms = []string{}\n\tde, err = dict.Next()\n\tfor err == nil && de != nil {\n\t\tterms = append(terms, string(de.Term))\n\t\tde, err = dict.Next()\n\t}\n\n\texpectedTerms = []string{\"cat\", \"cats\", \"catting\"}\n\tif !reflect.DeepEqual(terms, expectedTerms) {\n\t\tt.Errorf(\"expected %v, got %v\", expectedTerms, terms)\n\t}\n\n\tstats := index.Stats()\n\tif stats == nil {\n\t\tt.Errorf(\"expected IndexStat, got nil\")\n\t}\n\n\terr = dict.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestBatchString(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := index.NewBatch()\n\terr = batch.Index(\"a\", []byte(\"{}\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tbatch.Delete(\"b\")\n\tbatch.SetInternal([]byte(\"c\"), []byte{})\n\tbatch.DeleteInternal([]byte(\"d\"))\n\n\tbatchStr := batch.String()\n\tif !strings.HasPrefix(batchStr, \"Batch (2 ops, 2 internal ops)\") {\n\t\tt.Errorf(\"expected to start with Batch (2 ops, 2 internal ops), did not\")\n\t}\n\tif !strings.Contains(batchStr, \"INDEX - 'a'\") {\n\t\tt.Errorf(\"expected to contain INDEX - 'a', did not\")\n\t}\n\tif !strings.Contains(batchStr, \"DELETE - 'b'\") {\n\t\tt.Errorf(\"expected to contain DELETE - 'b', did not\")\n\t}\n\tif !strings.Contains(batchStr, \"SET INTERNAL - 'c'\") {\n\t\tt.Errorf(\"expected to contain SET INTERNAL - 'c', did not\")\n\t}\n\tif !strings.Contains(batchStr, \"DELETE INTERNAL - 'd'\") {\n\t\tt.Errorf(\"expected to contain DELETE INTERNAL - 'd', did not\")\n\t}\n\n}\n\nfunc TestIndexMetadataRaceBug198(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\twg := sync.WaitGroup{}\n\twg.Add(1)\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\t\twg.Done()\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\t_, err2 := index.DocCount()\n\t\t\t\tif err2 != nil {\n\t\t\t\t\tt.Fatal(err2)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor i := 0; i < 100; i++ {\n\t\tbatch := index.NewBatch()\n\t\terr = batch.Index(\"a\", []byte(\"{}\"))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\terr = index.Batch(batch)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tclose(done)\n\twg.Wait()\n}\n\nfunc TestSortMatchSearch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnames := []string{\"Noam\", \"Uri\", \"David\", \"Yosef\", \"Eitan\", \"Itay\", \"Ariel\", \"Daniel\", \"Omer\", \"Yogev\", \"Yehonatan\", \"Moshe\", \"Mohammed\", \"Yusuf\", \"Omar\"}\n\tdays := []string{\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"}\n\tnumbers := []string{\"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Eleven\", \"Twelve\"}\n\tb := index.NewBatch()\n\tfor i := 0; i < 200; i++ {\n\t\tdoc := make(map[string]interface{})\n\t\tdoc[\"Name\"] = names[i%len(names)]\n\t\tdoc[\"Day\"] = days[i%len(days)]\n\t\tdoc[\"Number\"] = numbers[i%len(numbers)]\n\t\terr = b.Index(fmt.Sprintf(\"%d\", i), doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = index.Batch(b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treq := NewSearchRequest(NewMatchQuery(\"One\"))\n\treq.SortBy([]string{\"Day\", \"Name\"})\n\treq.Fields = []string{\"*\"}\n\tsr, err := index.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tprev := \"\"\n\tfor _, hit := range sr.Hits {\n\t\tval := hit.Fields[\"Day\"].(string)\n\t\tif prev > val {\n\t\t\tt.Errorf(\"Hits must be sorted by 'Day'. Found '%s' before '%s'\", prev, val)\n\t\t}\n\t\tprev = val\n\t}\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestIndexCountMatchSearch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 10; i++ {\n\t\twg.Add(1)\n\t\tgo func(i int) {\n\t\t\tb := index.NewBatch()\n\t\t\tfor j := 0; j < 200; j++ {\n\t\t\t\tid := fmt.Sprintf(\"%d\", (i*200)+j)\n\t\t\t\tdoc := struct {\n\t\t\t\t\tBody string\n\t\t\t\t}{\n\t\t\t\t\tBody: \"match\",\n\t\t\t\t}\n\t\t\t\terr := b.Index(id, doc)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t\terr := index.Batch(b)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\twg.Wait()\n\n\t// search for something that should match all documents\n\tsr, err := index.Search(NewSearchRequest(NewMatchQuery(\"match\")))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// get the index document count\n\tdc, err := index.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// make sure test is working correctly, doc count should 2000\n\tif dc != 2000 {\n\t\tt.Errorf(\"expected doc count 2000, got %d\", dc)\n\t}\n\n\t// make sure our search found all the documents\n\tif dc != sr.Total {\n\t\tt.Errorf(\"expected search result total %d to match doc count %d\", sr.Total, dc)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestBatchReset(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tbatch := index.NewBatch()\n\terr = batch.Index(\"k1\", struct {\n\t\tBody string\n\t}{\n\t\tBody: \"v1\",\n\t})\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatch.Delete(\"k2\")\n\tbatch.SetInternal([]byte(\"k3\"), []byte(\"v3\"))\n\tbatch.DeleteInternal([]byte(\"k4\"))\n\n\tif batch.Size() != 4 {\n\t\tt.Logf(\"%v\", batch)\n\t\tt.Errorf(\"expected batch size 4, got %d\", batch.Size())\n\t}\n\n\tbatch.Reset()\n\n\tif batch.Size() != 0 {\n\t\tt.Errorf(\"expected batch size 0 after reset, got %d\", batch.Size())\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestDocumentFieldArrayPositions(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// index a document with an array of strings\n\terr = idx.Index(\"k\", struct {\n\t\tMessages []string\n\t}{\n\t\tMessages: []string{\n\t\t\t\"first\",\n\t\t\t\"second\",\n\t\t\t\"third\",\n\t\t\t\"last\",\n\t\t},\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// load the document\n\tdoc, err := idx.Document(\"k\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoc.VisitFields(func(f index.Field) {\n\t\tif reflect.DeepEqual(f.Value(), []byte(\"first\")) {\n\t\t\tap := f.ArrayPositions()\n\t\t\tif len(ap) < 1 {\n\t\t\t\tt.Errorf(\"expected an array position, got none\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ap[0] != 0 {\n\t\t\t\tt.Errorf(\"expected 'first' in array position 0, got %d\", ap[0])\n\t\t\t}\n\t\t}\n\t\tif reflect.DeepEqual(f.Value(), []byte(\"second\")) {\n\t\t\tap := f.ArrayPositions()\n\t\t\tif len(ap) < 1 {\n\t\t\t\tt.Errorf(\"expected an array position, got none\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ap[0] != 1 {\n\t\t\t\tt.Errorf(\"expected 'second' in array position 1, got %d\", ap[0])\n\t\t\t}\n\t\t}\n\t\tif reflect.DeepEqual(f.Value(), []byte(\"third\")) {\n\t\t\tap := f.ArrayPositions()\n\t\t\tif len(ap) < 1 {\n\t\t\t\tt.Errorf(\"expected an array position, got none\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ap[0] != 2 {\n\t\t\t\tt.Errorf(\"expected 'third' in array position 2, got %d\", ap[0])\n\t\t\t}\n\t\t}\n\t\tif reflect.DeepEqual(f.Value(), []byte(\"last\")) {\n\t\t\tap := f.ArrayPositions()\n\t\t\tif len(ap) < 1 {\n\t\t\t\tt.Errorf(\"expected an array position, got none\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ap[0] != 3 {\n\t\t\t\tt.Errorf(\"expected 'last' in array position 3, got %d\", ap[0])\n\t\t\t}\n\t\t}\n\t})\n\n\t// now index a document in the same field with a single string\n\terr = idx.Index(\"k2\", struct {\n\t\tMessages string\n\t}{\n\t\tMessages: \"only\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// load the document\n\tdoc, err = idx.Document(\"k2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoc.VisitFields(func(f index.Field) {\n\t\tif reflect.DeepEqual(f.Value(), []byte(\"only\")) {\n\t\t\tap := f.ArrayPositions()\n\t\t\tif len(ap) != 0 {\n\t\t\t\tt.Errorf(\"expected no array positions, got %d\", len(ap))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\terr = idx.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestKeywordSearchBug207(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tf := NewTextFieldMapping()\n\tf.Analyzer = keyword.Name\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = NewDocumentMapping()\n\tm.DefaultMapping.AddFieldMappingsAt(\"Body\", f)\n\n\tindex, err := New(tmpIndexPath, m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoc1 := struct {\n\t\tBody string\n\t}{\n\t\tBody: \"a555c3bb06f7a127cda000005\",\n\t}\n\n\terr = index.Index(\"a\", doc1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoc2 := struct {\n\t\tBody string\n\t}{\n\t\tBody: \"555c3bb06f7a127cda000005\",\n\t}\n\n\terr = index.Index(\"b\", doc2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// now search for these terms\n\tsreq := NewSearchRequest(NewTermQuery(\"a555c3bb06f7a127cda000005\"))\n\tsres, err := index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 result, got %d\", sres.Total)\n\t}\n\tif sres.Hits[0].ID != \"a\" {\n\t\tt.Errorf(\"expecated id 'a', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\tsreq = NewSearchRequest(NewTermQuery(\"555c3bb06f7a127cda000005\"))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 result, got %d\", sres.Total)\n\t}\n\tif sres.Hits[0].ID != \"b\" {\n\t\tt.Errorf(\"expecated id 'b', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\t// now do the same searches using query strings\n\tsreq = NewSearchRequest(NewQueryStringQuery(\"Body:a555c3bb06f7a127cda000005\"))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 result, got %d\", sres.Total)\n\t}\n\tif sres.Hits[0].ID != \"a\" {\n\t\tt.Errorf(\"expecated id 'a', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\tsreq = NewSearchRequest(NewQueryStringQuery(`Body:555c3bb06f7a127cda000005`))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 result, got %d\", sres.Total)\n\t}\n\tif sres.Hits[0].ID != \"b\" {\n\t\tt.Errorf(\"expecated id 'b', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestTermVectorArrayPositions(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// index a document with an array of strings\n\terr = index.Index(\"k\", struct {\n\t\tMessages []string\n\t}{\n\t\tMessages: []string{\n\t\t\t\"first\",\n\t\t\t\"second\",\n\t\t\t\"third\",\n\t\t\t\"last\",\n\t\t},\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// search for this document in all field\n\ttq := NewTermQuery(\"second\")\n\ttsr := NewSearchRequest(tq)\n\ttsr.IncludeLocations = true\n\tresults, err := index.Search(tsr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif results.Total != 1 {\n\t\tt.Fatalf(\"expected 1 result, got %d\", results.Total)\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"second\"]) < 1 {\n\t\tt.Fatalf(\"expected at least one location\")\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"second\"][0].ArrayPositions) < 1 {\n\t\tt.Fatalf(\"expected at least one location array position\")\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"second\"][0].ArrayPositions[0] != 1 {\n\t\tt.Fatalf(\"expected array position 1, got %d\", results.Hits[0].Locations[\"Messages\"][\"second\"][0].ArrayPositions[0])\n\t}\n\n\t// repeat search for this document in Messages field\n\ttq2 := NewTermQuery(\"third\")\n\ttq2.SetField(\"Messages\")\n\ttsr = NewSearchRequest(tq2)\n\ttsr.IncludeLocations = true\n\tresults, err = index.Search(tsr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif results.Total != 1 {\n\t\tt.Fatalf(\"expected 1 result, got %d\", results.Total)\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"third\"]) < 1 {\n\t\tt.Fatalf(\"expected at least one location\")\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"third\"][0].ArrayPositions) < 1 {\n\t\tt.Fatalf(\"expected at least one location array position\")\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"third\"][0].ArrayPositions[0] != 2 {\n\t\tt.Fatalf(\"expected array position 2, got %d\", results.Hits[0].Locations[\"Messages\"][\"third\"][0].ArrayPositions[0])\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestDocumentStaticMapping(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = NewDocumentStaticMapping()\n\tm.DefaultMapping.AddFieldMappingsAt(\"Text\", NewTextFieldMapping())\n\tm.DefaultMapping.AddFieldMappingsAt(\"Date\", NewDateTimeFieldMapping())\n\tm.DefaultMapping.AddFieldMappingsAt(\"Numeric\", NewNumericFieldMapping())\n\n\tindex, err := New(tmpIndexPath, m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoc1 := struct {\n\t\tText           string\n\t\tIgnoredText    string\n\t\tNumeric        float64\n\t\tIgnoredNumeric float64\n\t\tDate           time.Time\n\t\tIgnoredDate    time.Time\n\t}{\n\t\tText:           \"valid text\",\n\t\tIgnoredText:    \"ignored text\",\n\t\tNumeric:        10,\n\t\tIgnoredNumeric: 20,\n\t\tDate:           time.Unix(1, 0),\n\t\tIgnoredDate:    time.Unix(2, 0),\n\t}\n\n\terr = index.Index(\"a\", doc1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfields, err := index.Fields()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tsort.Strings(fields)\n\texpectedFields := []string{\"Date\", \"Numeric\", \"Text\", \"_all\"}\n\tif len(fields) < len(expectedFields) {\n\t\tt.Fatalf(\"invalid field count: %d\", len(fields))\n\t}\n\tfor i, expected := range expectedFields {\n\t\tif expected != fields[i] {\n\t\t\tt.Fatalf(\"unexpected field[%d]: %s\", i, fields[i])\n\t\t}\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestIndexEmptyDocId(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"body\": \"nodocid\",\n\t}\n\n\terr = index.Index(\"\", doc)\n\tif err != ErrorEmptyID {\n\t\tt.Errorf(\"expect index empty doc id to fail\")\n\t}\n\n\terr = index.Delete(\"\")\n\tif err != ErrorEmptyID {\n\t\tt.Errorf(\"expect delete empty doc id to fail\")\n\t}\n\n\tbatch := index.NewBatch()\n\terr = batch.Index(\"\", doc)\n\tif err != ErrorEmptyID {\n\t\tt.Errorf(\"expect index empty doc id in batch to fail\")\n\t}\n\n\tbatch.Delete(\"\")\n\tif batch.Size() > 0 {\n\t\tt.Errorf(\"expect delete empty doc id in batch to be ignored\")\n\t}\n}\n\nfunc TestDateTimeFieldMappingIssue287(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tf := NewDateTimeFieldMapping()\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = NewDocumentMapping()\n\tm.DefaultMapping.AddFieldMappingsAt(\"Date\", f)\n\n\tindex, err := New(tmpIndexPath, m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype doc struct {\n\t\tDate time.Time\n\t}\n\n\tnow := time.Now()\n\n\t// 3hr ago to 1hr ago\n\tfor i := 0; i < 3; i++ {\n\t\td := doc{now.Add(time.Duration((i - 3)) * time.Hour)}\n\n\t\terr = index.Index(strconv.FormatInt(int64(i), 10), d)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// search range across all docs\n\tstart := now.Add(-4 * time.Hour)\n\tend := now\n\tsreq := NewSearchRequest(NewDateRangeQuery(start, end))\n\tsres, err := index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 3 {\n\t\tt.Errorf(\"expected 3 results, got %d\", sres.Total)\n\t}\n\n\t// search range includes only oldest\n\tstart = now.Add(-4 * time.Hour)\n\tend = now.Add(-121 * time.Minute)\n\tsreq = NewSearchRequest(NewDateRangeQuery(start, end))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 results, got %d\", sres.Total)\n\t}\n\tif sres.Total > 0 && sres.Hits[0].ID != \"0\" {\n\t\tt.Errorf(\"expecated id '0', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\t// search range includes only newest\n\tstart = now.Add(-61 * time.Minute)\n\tend = now\n\tsreq = NewSearchRequest(NewDateRangeQuery(start, end))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 results, got %d\", sres.Total)\n\t}\n\tif sres.Total > 0 && sres.Hits[0].ID != \"2\" {\n\t\tt.Errorf(\"expecated id '2', got '%s'\", sres.Hits[0].ID)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestDocumentFieldArrayPositionsBug295(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// index a document with an array of strings\n\terr = index.Index(\"k\", struct {\n\t\tMessages []string\n\t\tAnother  string\n\t\tMoreData []string\n\t}{\n\t\tMessages: []string{\n\t\t\t\"bleve\",\n\t\t\t\"bleve\",\n\t\t},\n\t\tAnother: \"text\",\n\t\tMoreData: []string{\n\t\t\t\"a\",\n\t\t\t\"b\",\n\t\t\t\"c\",\n\t\t\t\"bleve\",\n\t\t},\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// search for it in the messages field\n\ttq := NewTermQuery(\"bleve\")\n\ttq.SetField(\"Messages\")\n\ttsr := NewSearchRequest(tq)\n\ttsr.IncludeLocations = true\n\tresults, err := index.Search(tsr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif results.Total != 1 {\n\t\tt.Fatalf(\"expected 1 result, got %d\", results.Total)\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"bleve\"]) != 2 {\n\t\tt.Fatalf(\"expected 2 locations of 'bleve', got %d\", len(results.Hits[0].Locations[\"Messages\"][\"bleve\"]))\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"bleve\"][0].ArrayPositions[0] != 0 {\n\t\tt.Errorf(\"expected array position to be 0\")\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"bleve\"][1].ArrayPositions[0] != 1 {\n\t\tt.Errorf(\"expected array position to be 1\")\n\t}\n\n\t// search for it in all\n\ttq = NewTermQuery(\"bleve\")\n\ttsr = NewSearchRequest(tq)\n\ttsr.IncludeLocations = true\n\tresults, err = index.Search(tsr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif results.Total != 1 {\n\t\tt.Fatalf(\"expected 1 result, got %d\", results.Total)\n\t}\n\tif len(results.Hits[0].Locations[\"Messages\"][\"bleve\"]) != 2 {\n\t\tt.Fatalf(\"expected 2 locations of 'bleve', got %d\", len(results.Hits[0].Locations[\"Messages\"][\"bleve\"]))\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"bleve\"][0].ArrayPositions[0] != 0 {\n\t\tt.Errorf(\"expected array position to be 0\")\n\t}\n\tif results.Hits[0].Locations[\"Messages\"][\"bleve\"][1].ArrayPositions[0] != 1 {\n\t\tt.Errorf(\"expected array position to be 1\")\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestBooleanFieldMappingIssue109(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = NewDocumentMapping()\n\tm.DefaultMapping.AddFieldMappingsAt(\"Bool\", NewBooleanFieldMapping())\n\n\tindex, err := New(tmpIndexPath, m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype doc struct {\n\t\tBool bool\n\t}\n\terr = index.Index(\"true\", &doc{Bool: true})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index.Index(\"false\", &doc{Bool: false})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tq := NewBoolFieldQuery(true)\n\tq.SetField(\"Bool\")\n\tsreq := NewSearchRequest(q)\n\tsres, err := index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 results, got %d\", sres.Total)\n\t}\n\n\tq = NewBoolFieldQuery(false)\n\tq.SetField(\"Bool\")\n\tsreq = NewSearchRequest(q)\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 results, got %d\", sres.Total)\n\t}\n\n\tsreq = NewSearchRequest(NewBoolFieldQuery(true))\n\tsres, err = index.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif sres.Total != 1 {\n\t\tt.Errorf(\"expected 1 results, got %d\", sres.Total)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestSearchTimeout(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// first run a search with an absurdly long timeout (should succeed)\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tquery := NewTermQuery(\"water\")\n\treq := NewSearchRequest(query)\n\t_, err = index.SearchInContext(ctx, req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// now run a search again with an absurdly low timeout (should timeout)\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Microsecond)\n\tdefer cancel()\n\tsq := &slowQuery{\n\t\tactual: query,\n\t\tdelay:  50 * time.Millisecond, // on Windows timer resolution is 15ms\n\t}\n\treq.Query = sq\n\t_, err = index.SearchInContext(ctx, req)\n\tif err != context.DeadlineExceeded {\n\t\tt.Fatalf(\"exected %v, got: %v\", context.DeadlineExceeded, err)\n\t}\n\n\t// now run a search with a long timeout, but with a long query, and cancel it\n\tctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n\tsq = &slowQuery{\n\t\tactual: query,\n\t\tdelay:  100 * time.Millisecond, // on Windows timer resolution is 15ms\n\t}\n\treq = NewSearchRequest(sq)\n\tcancel()\n\t_, err = index.SearchInContext(ctx, req)\n\tif err != context.Canceled {\n\t\tt.Fatalf(\"exected %v, got: %v\", context.Canceled, err)\n\t}\n}\n\n// TestConfigCache exposes a concurrent map write with go 1.6\nfunc TestConfigCache(t *testing.T) {\n\tfor i := 0; i < 100; i++ {\n\t\tgo func() {\n\t\t\t_, err := Config.Cache.HighlighterNamed(Config.DefaultHighlighter)\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc TestBatchRaceBug260(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\ti, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := i.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\tb := i.NewBatch()\n\terr = b.Index(\"1\", 1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = i.Batch(b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tb.Reset()\n\terr = b.Index(\"2\", 2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = i.Batch(b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tb.Reset()\n}\n\nfunc BenchmarkBatchOverhead(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\tm := NewIndexMapping()\n\ti, err := NewUsing(tmpIndexPath, m, Config.DefaultIndexType, null.Name, nil)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\tfor n := 0; n < b.N; n++ {\n\t\t// put 1000 items in a batch\n\t\tbatch := i.NewBatch()\n\t\tfor i := 0; i < 1000; i++ {\n\t\t\terr = batch.Index(fmt.Sprintf(\"%d\", i), map[string]interface{}{\"name\": \"bleve\"})\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}\n\t\terr = i.Batch(batch)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tbatch.Reset()\n\t}\n}\n\nfunc TestOpenReadonlyMultiple(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// build an index and close it\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdoca := map[string]interface{}{\n\t\t\"name\": \"marty\",\n\t\t\"desc\": \"gophercon india\",\n\t}\n\terr = index.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// now open it read-only\n\tindex, err = OpenUsing(tmpIndexPath, map[string]interface{}{\n\t\t\"read_only\": true,\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// now open it again\n\tindex2, err := OpenUsing(tmpIndexPath, map[string]interface{}{\n\t\t\"read_only\": true,\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = index2.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// TestBug408 tests for VERY large values of size, even though actual result\n// set may be reasonable size\nfunc TestBug408(t *testing.T) {\n\ttype TestStruct struct {\n\t\tID     string  `json:\"id\"`\n\t\tUserID *string `json:\"user_id\"`\n\t}\n\n\tdocMapping := NewDocumentMapping()\n\tdocMapping.AddFieldMappingsAt(\"id\", NewTextFieldMapping())\n\tdocMapping.AddFieldMappingsAt(\"user_id\", NewTextFieldMapping())\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.DefaultMapping = docMapping\n\n\tindex, err := NewMemOnly(indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnumToTest := 10\n\tmatchUserID := \"match\"\n\tnoMatchUserID := \"no_match\"\n\tmatchingDocIds := make(map[string]struct{})\n\n\tfor i := 0; i < numToTest; i++ {\n\t\tds := &TestStruct{\"id_\" + strconv.Itoa(i), nil}\n\t\tif i%2 == 0 {\n\t\t\tds.UserID = &noMatchUserID\n\t\t} else {\n\t\t\tds.UserID = &matchUserID\n\t\t\tmatchingDocIds[ds.ID] = struct{}{}\n\t\t}\n\t\terr = index.Index(ds.ID, ds)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tcnt, err := index.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif int(cnt) != numToTest {\n\t\tt.Fatalf(\"expected %d documents in index, got %d\", numToTest, cnt)\n\t}\n\n\tq := NewTermQuery(matchUserID)\n\tq.SetField(\"user_id\")\n\tsearchReq := NewSearchRequestOptions(q, math.MaxInt32, 0, false)\n\tresults, err := index.Search(searchReq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif int(results.Total) != numToTest/2 {\n\t\tt.Fatalf(\"expected %d search hits, got %d\", numToTest/2, results.Total)\n\t}\n\n\tfor _, result := range results.Hits {\n\t\tif _, found := matchingDocIds[result.ID]; !found {\n\t\t\tt.Fatalf(\"document with ID %s not in results as expected\", result.ID)\n\t\t}\n\t}\n}\n\nfunc TestIndexAdvancedCountMatchSearch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 10; i++ {\n\t\twg.Add(1)\n\t\tgo func(i int) {\n\t\t\tb := index.NewBatch()\n\t\t\tfor j := 0; j < 200; j++ {\n\t\t\t\tid := fmt.Sprintf(\"%d\", (i*200)+j)\n\n\t\t\t\tdoc := document.NewDocument(id)\n\t\t\t\tdoc.AddField(document.NewTextField(\"body\", []uint64{}, []byte(\"match\")))\n\t\t\t\tdoc.AddField(document.NewCompositeField(\"_all\", true, []string{}, []string{}))\n\n\t\t\t\terr := b.IndexAdvanced(doc)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t\terr := index.Batch(b)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\twg.Wait()\n\n\t// search for something that should match all documents\n\tsr, err := index.Search(NewSearchRequest(NewMatchQuery(\"match\")))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// get the index document count\n\tdc, err := index.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// make sure test is working correctly, doc count should 2000\n\tif dc != 2000 {\n\t\tt.Errorf(\"expected doc count 2000, got %d\", dc)\n\t}\n\n\t// make sure our search found all the documents\n\tif dc != sr.Total {\n\t\tt.Errorf(\"expected search result total %d to match doc count %d\", sr.Total, dc)\n\t}\n\n\terr = index.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc benchmarkSearchOverhead(indexType string, b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tindex, err := NewUsing(tmpIndexPath, NewIndexMapping(),\n\t\tindexType, Config.DefaultKVStore, nil)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\telements := []string{\"air\", \"water\", \"fire\", \"earth\"}\n\tfor j := 0; j < 10000; j++ {\n\t\terr = index.Index(fmt.Sprintf(\"%d\", j),\n\t\t\tmap[string]interface{}{\"name\": elements[j%len(elements)]})\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tquery1 := NewTermQuery(\"water\")\n\tquery2 := NewTermQuery(\"fire\")\n\tquery := NewDisjunctionQuery(query1, query2)\n\treq := NewSearchRequest(query)\n\n\tb.ResetTimer()\n\n\tfor n := 0; n < b.N; n++ {\n\t\t_, err = index.Search(req)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkUpsidedownSearchOverhead(b *testing.B) {\n\tbenchmarkSearchOverhead(upsidedown.Name, b)\n}\n\nfunc BenchmarkScorchSearchOverhead(b *testing.B) {\n\tbenchmarkSearchOverhead(scorch.Name, b)\n}\n\nfunc TestSearchQueryCallback(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindex, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tquery := NewTermQuery(\"water\")\n\treq := NewSearchRequest(query)\n\n\texpErr := fmt.Errorf(\"MEM_LIMIT_EXCEEDED\")\n\tf := func(size uint64) error {\n\t\t// the intended usage of this callback is to see the estimated\n\t\t// memory usage before executing, and possibly abort early\n\t\t// in this test we simulate returning such an error\n\t\treturn expErr\n\t}\n\n\tctx := context.WithValue(context.Background(), SearchQueryStartCallbackKey,\n\t\tSearchQueryStartCallbackFn(f))\n\t_, err = index.SearchInContext(ctx, req)\n\tif err != expErr {\n\t\tt.Fatalf(\"Expected: %v, Got: %v\", expErr, err)\n\t}\n}\n\nfunc TestBatchMerge(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdoca := map[string]interface{}{\n\t\t\"name\":   \"scorch\",\n\t\t\"desc\":   \"gophercon india\",\n\t\t\"nation\": \"india\",\n\t}\n\n\tbatchA := idx.NewBatch()\n\terr = batchA.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatchA.SetInternal([]byte(\"batchkA\"), []byte(\"batchvA\"))\n\n\tdocb := map[string]interface{}{\n\t\t\"name\": \"moss\",\n\t\t\"desc\": \"gophercon MV\",\n\t}\n\n\tbatchB := idx.NewBatch()\n\terr = batchB.Index(\"b\", docb)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatchB.SetInternal([]byte(\"batchkB\"), []byte(\"batchvB\"))\n\n\tdocC := map[string]interface{}{\n\t\t\"name\":    \"blahblah\",\n\t\t\"desc\":    \"inProgress\",\n\t\t\"country\": \"usa\",\n\t}\n\n\tbatchC := idx.NewBatch()\n\terr = batchC.Index(\"c\", docC)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatchC.SetInternal([]byte(\"batchkC\"), []byte(\"batchvC\"))\n\tbatchC.SetInternal([]byte(\"batchkB\"), []byte(\"batchvBNew\"))\n\tbatchC.Delete(\"a\")\n\tbatchC.DeleteInternal([]byte(\"batchkA\"))\n\n\tbatchA.Merge(batchB)\n\n\tif batchA.Size() != 4 {\n\t\tt.Errorf(\"expected batch size 4, got %d\", batchA.Size())\n\t}\n\n\tbatchA.Merge(batchC)\n\n\tif batchA.Size() != 6 {\n\t\tt.Errorf(\"expected batch size 6, got %d\", batchA.Size())\n\t}\n\n\terr = idx.Batch(batchA)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// close the index, open it again, and try some more things\n\terr = idx.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidx, err = Open(tmpIndexPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcount, err := idx.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif count != 2 {\n\t\tt.Errorf(\"expected doc count 2, got %d\", count)\n\t}\n\n\tdoc, err := idx.Document(\"c\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tval, err := idx.GetInternal([]byte(\"batchkB\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif val == nil || string(val) != \"batchvBNew\" {\n\t\tt.Errorf(\"expected val: batchvBNew , got %s\", val)\n\t}\n\n\tval, err = idx.GetInternal([]byte(\"batchkA\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif val != nil {\n\t\tt.Errorf(\"expected nil, got %s\", val)\n\t}\n\n\tfoundNameField := false\n\tdoc.VisitFields(func(field index.Field) {\n\t\tif field.Name() == \"name\" && string(field.Value()) == \"blahblah\" {\n\t\t\tfoundNameField = true\n\t\t}\n\t})\n\tif !foundNameField {\n\t\tt.Errorf(\"expected to find field named 'name' with value 'blahblah'\")\n\t}\n\n\tfields, err := idx.Fields()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\texpectedFields := map[string]bool{\n\t\t\"_all\":    false,\n\t\t\"name\":    false,\n\t\t\"desc\":    false,\n\t\t\"country\": false,\n\t}\n\tif len(fields) < len(expectedFields) {\n\t\tt.Fatalf(\"expected %d fields got %d\", len(expectedFields), len(fields))\n\t}\n\n\tfor _, f := range fields {\n\t\texpectedFields[f] = true\n\t}\n\n\tfor ef, efp := range expectedFields {\n\t\tif !efp {\n\t\t\tt.Errorf(\"field %s is missing\", ef)\n\t\t}\n\t}\n\n}\n\nfunc TestBug1096(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// use default mapping\n\tmapping := NewIndexMapping()\n\n\t// create a scorch index with default SAFE batches\n\tvar idx Index\n\tidx, err = NewUsing(tmpIndexPath, mapping, \"scorch\", \"scorch\", nil)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// create a single batch instance that we will reuse\n\t// this should be safe because we have single goroutine\n\t// and we always wait for batch execution to finish\n\tbatch := idx.NewBatch()\n\n\t// number of batches to execute\n\tfor i := 0; i < 10; i++ {\n\n\t\t// number of documents to put into the batch\n\t\tfor j := 0; j < 91; j++ {\n\n\t\t\t// create a doc id 0-90 (important so that we get id's 9 and 90)\n\t\t\t// this could duplicate something already in the index\n\t\t\t//   this too should be OK and update the item in the index\n\t\t\tid := fmt.Sprintf(\"%d\", j)\n\n\t\t\terr = batch.Index(id, map[string]interface{}{\n\t\t\t\t\"name\":  id,\n\t\t\t\t\"batch\": fmt.Sprintf(\"%d\", i),\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\t// execute the batch\n\t\terr = idx.Batch(batch)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\n\t\t// reset the batch before reusing it\n\t\tbatch.Reset()\n\t}\n\n\t// search for docs having name starting with the number 9\n\tq := NewWildcardQuery(\"9*\")\n\tq.SetField(\"name\")\n\treq := NewSearchRequestOptions(q, 1000, 0, false)\n\treq.Fields = []string{\"*\"}\n\tvar res *SearchResult\n\tres, err = idx.Search(req)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// we expect only 2 hits, for docs 9 and 90\n\tif res.Total > 2 {\n\t\tt.Fatalf(\"expected only 2 hits '9' and '90', got %v\", res)\n\t}\n}\n\nfunc TestDataRaceBug1092(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// use default mapping\n\tmapping := NewIndexMapping()\n\n\tvar idx Index\n\tidx, err = NewUsing(tmpIndexPath, mapping, upsidedown.Name, boltdb.Name, nil)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer func() {\n\t\tcerr := idx.Close()\n\t\tif cerr != nil {\n\t\t\tt.Fatal(cerr)\n\t\t}\n\t}()\n\n\tbatch := idx.NewBatch()\n\tfor i := 0; i < 10; i++ {\n\t\terr = idx.Batch(batch)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tbatch.Reset()\n\t}\n}\n\nfunc TestBatchRaceBug1149(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\ti, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := i.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\ttestBatchRaceBug1149(t, i)\n}\n\nfunc TestBatchRaceBug1149Scorch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\ti, err := NewUsing(tmpIndexPath, NewIndexMapping(), \"scorch\", \"scorch\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := i.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\ttestBatchRaceBug1149(t, i)\n}\n\nfunc testBatchRaceBug1149(t *testing.T, i Index) {\n\tb := i.NewBatch()\n\tb.Delete(\"1\")\n\terr = i.Batch(b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tb.Reset()\n\terr = i.Batch(b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tb.Reset()\n}\n\nfunc TestOptimisedConjunctionSearchHits(t *testing.T) {\n\tscorch.OptimizeDisjunctionUnadorned = false\n\tdefer func() {\n\t\tscorch.OptimizeDisjunctionUnadorned = true\n\t}()\n\n\tdefer func() {\n\t\terr := os.RemoveAll(\"testidx\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tidx, err := NewUsing(\"testidx\", NewIndexMapping(), \"scorch\", \"scorch\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdoca := map[string]interface{}{\n\t\t\"country\":    \"united\",\n\t\t\"name\":       \"Mercure Hotel\",\n\t\t\"directions\": \"B560 and B56 Follow signs to the M56\",\n\t}\n\tdocb := map[string]interface{}{\n\t\t\"country\":    \"united\",\n\t\t\"name\":       \"Mercure Altrincham Bowdon Hotel\",\n\t\t\"directions\": \"A570 and A57 Follow signs to the M56 Manchester Airport\",\n\t}\n\tdocc := map[string]interface{}{\n\t\t\"country\":    \"india united\",\n\t\t\"name\":       \"Sonoma Hotel\",\n\t\t\"directions\": \"Northwest\",\n\t}\n\tdocd := map[string]interface{}{\n\t\t\"country\":    \"United Kingdom\",\n\t\t\"name\":       \"Cresta Court Hotel\",\n\t\t\"directions\": \"junction of A560 and A56\",\n\t}\n\n\tb := idx.NewBatch()\n\terr = b.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = b.Index(\"b\", docb)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = b.Index(\"c\", docc)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = b.Index(\"d\", docd)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\t// execute the batch\n\terr = idx.Batch(b)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tmq := NewMatchQuery(\"united\")\n\tmq.SetField(\"country\")\n\n\tcq := NewConjunctionQuery(mq)\n\n\tmq1 := NewMatchQuery(\"hotel\")\n\tmq1.SetField(\"name\")\n\tcq.AddQuery(mq1)\n\n\tmq2 := NewMatchQuery(\"56\")\n\tmq2.SetField(\"directions\")\n\tmq2.SetFuzziness(1)\n\tcq.AddQuery(mq2)\n\n\treq := NewSearchRequest(cq)\n\treq.Score = \"none\"\n\n\tres, err := idx.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\thitsWithOutScore := res.Total\n\n\treq = NewSearchRequest(cq)\n\treq.Score = \"\"\n\n\tres, err = idx.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\thitsWithScore := res.Total\n\n\tif hitsWithOutScore != hitsWithScore {\n\t\tt.Errorf(\"expected %d hits without score, got %d\", hitsWithScore, hitsWithOutScore)\n\t}\n\n\terr = idx.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestIndexMappingDocValuesDynamic(t *testing.T) {\n\tim := NewIndexMapping()\n\t// DocValuesDynamic's default is true\n\t// Now explicitly set it to false\n\tim.DocValuesDynamic = false\n\n\t// Next, retrieve the JSON dump of the index mapping\n\tvar data []byte\n\tdata, err = json.Marshal(im)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Now, edit an unrelated setting in the index mapping\n\tvar m map[string]interface{}\n\terr = json.Unmarshal(data, &m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tm[\"index_dynamic\"] = false\n\tdata, err = json.Marshal(m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Unmarshal back the changes into the index mapping struct\n\tif err = im.UnmarshalJSON(data); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Expect DocValuesDynamic to remain false!\n\tif im.DocValuesDynamic {\n\t\tt.Fatalf(\"Expected DocValuesDynamic to remain false after the index mapping edit\")\n\t}\n}\n\nfunc TestCopyIndex(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoca := map[string]interface{}{\n\t\t\"name\": \"tester\",\n\t\t\"desc\": \"gophercon india testing\",\n\t}\n\terr = idx.Index(\"a\", doca)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocy := map[string]interface{}{\n\t\t\"name\": \"jasper\",\n\t\t\"desc\": \"clojure\",\n\t}\n\terr = idx.Index(\"y\", docy)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\terr = idx.Delete(\"y\")\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocx := map[string]interface{}{\n\t\t\"name\": \"rose\",\n\t\t\"desc\": \"xoogler\",\n\t}\n\terr = idx.Index(\"x\", docx)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\terr = idx.SetInternal([]byte(\"status\"), []byte(\"pending\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tdocb := map[string]interface{}{\n\t\t\"name\": \"sree\",\n\t\t\"desc\": \"cbft janitor\",\n\t}\n\tbatch := idx.NewBatch()\n\terr = batch.Index(\"b\", docb)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tbatch.Delete(\"x\")\n\tbatch.SetInternal([]byte(\"batchi\"), []byte(\"batchv\"))\n\tbatch.DeleteInternal([]byte(\"status\"))\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tval, err := idx.GetInternal([]byte(\"batchi\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif string(val) != \"batchv\" {\n\t\tt.Errorf(\"expected 'batchv', got '%s'\", val)\n\t}\n\tval, err = idx.GetInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif val != nil {\n\t\tt.Errorf(\"expected nil, got '%s'\", val)\n\t}\n\n\terr = idx.SetInternal([]byte(\"seqno\"), []byte(\"7\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = idx.SetInternal([]byte(\"status\"), []byte(\"ready\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\terr = idx.DeleteInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tval, err = idx.GetInternal([]byte(\"status\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif val != nil {\n\t\tt.Errorf(\"expected nil, got '%s'\", val)\n\t}\n\n\tval, err = idx.GetInternal([]byte(\"seqno\"))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif string(val) != \"7\" {\n\t\tt.Errorf(\"expected '7', got '%s'\", val)\n\t}\n\n\tcount, err := idx.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif count != 2 {\n\t\tt.Errorf(\"expected doc count 2, got %d\", count)\n\t}\n\n\tdoc, err := idx.Document(\"a\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfoundNameField := false\n\tdoc.VisitFields(func(field index.Field) {\n\t\tif field.Name() == \"name\" && string(field.Value()) == \"tester\" {\n\t\t\tfoundNameField = true\n\t\t}\n\t})\n\tif !foundNameField {\n\t\tt.Errorf(\"expected to find field named 'name' with value 'tester'\")\n\t}\n\n\tfields, err := idx.Fields()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\texpectedFields := map[string]bool{\n\t\t\"_all\": false,\n\t\t\"name\": false,\n\t\t\"desc\": false,\n\t}\n\tif len(fields) < len(expectedFields) {\n\t\tt.Fatalf(\"expected %d fields got %d\", len(expectedFields), len(fields))\n\t}\n\tfor _, f := range fields {\n\t\texpectedFields[f] = true\n\t}\n\tfor ef, efp := range expectedFields {\n\t\tif !efp {\n\t\t\tt.Errorf(\"field %s is missing\", ef)\n\t\t}\n\t}\n\n\t// now create a copy of the index, and repeat assertions on it\n\tcopyableIndex, ok := idx.(IndexCopyable)\n\tif !ok {\n\t\tt.Fatal(\"index doesn't support copy\")\n\t}\n\tbackupIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, backupIndexPath)\n\n\terr = copyableIndex.CopyTo(FileSystemDirectory(backupIndexPath))\n\tif err != nil {\n\t\tt.Fatalf(\"error copying the index: %v\", err)\n\t}\n\n\t// open the copied  index\n\tidxCopied, err := Open(backupIndexPath)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to open copy index\")\n\t}\n\tdefer func() {\n\t\terr := idxCopied.Close()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error closing copy index: %v\", err)\n\t\t}\n\t}()\n\n\t// assertions on copied index\n\tcopyCount, err := idxCopied.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif copyCount != 2 {\n\t\tt.Errorf(\"expected doc count 2, got %d\", copyCount)\n\t}\n\n\tcopyDoc, err := idxCopied.Document(\"a\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcopyFoundNameField := false\n\tcopyDoc.VisitFields(func(field index.Field) {\n\t\tif field.Name() == \"name\" && string(field.Value()) == \"tester\" {\n\t\t\tcopyFoundNameField = true\n\t\t}\n\t})\n\tif !copyFoundNameField {\n\t\tt.Errorf(\"expected copy index to find field named 'name' with value 'tester'\")\n\t}\n\n\tcopyFields, err := idx.Fields()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcopyExpectedFields := map[string]bool{\n\t\t\"_all\": false,\n\t\t\"name\": false,\n\t\t\"desc\": false,\n\t}\n\tif len(copyFields) < len(copyExpectedFields) {\n\t\tt.Fatalf(\"expected %d fields got %d\", len(copyExpectedFields), len(copyFields))\n\t}\n\tfor _, f := range copyFields {\n\t\tcopyExpectedFields[f] = true\n\t}\n\tfor ef, efp := range copyExpectedFields {\n\t\tif !efp {\n\t\t\tt.Errorf(\"copy field %s is missing\", ef)\n\t\t}\n\t}\n}\n\nfunc TestFuzzyScoring(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tmp := NewIndexMapping()\n\tmp.DefaultAnalyzer = \"simple\"\n\tidx, err := New(tmpIndexPath, mp)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tbatch := idx.NewBatch()\n\n\tdocs := []map[string]interface{}{\n\t\t{\n\t\t\t\"textField\": \"ab\",\n\t\t},\n\t\t{\n\t\t\t\"textField\": \"abc\",\n\t\t},\n\t\t{\n\t\t\t\"textField\": \"abcd\",\n\t\t},\n\t}\n\n\tfor _, doc := range docs {\n\t\terr := batch.Index(fmt.Sprintf(\"%v\", doc[\"textField\"]), doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tquery := NewFuzzyQuery(\"ab\")\n\tquery.Fuzziness = 2\n\tsearchRequest := NewSearchRequestOptions(query, 10, 0, true)\n\tres, err := idx.Search(searchRequest)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tmaxScore := res.Hits[0].Score\n\n\tfor i, hit := range res.Hits {\n\t\tif maxScore/float64(i+1) != hit.Score {\n\t\t\tt.Errorf(\"expected score - %f, got score - %f\", maxScore/float64(i+1), hit.Score)\n\t\t}\n\t}\n\n\terr = idx.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "mapping.go",
          "type": "blob",
          "size": 2.6142578125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport \"github.com/blevesearch/bleve/v2/mapping\"\n\n// NewIndexMapping creates a new IndexMapping that will use all the default indexing rules\nfunc NewIndexMapping() *mapping.IndexMappingImpl {\n\treturn mapping.NewIndexMapping()\n}\n\n// NewDocumentMapping returns a new document mapping\n// with all the default values.\nfunc NewDocumentMapping() *mapping.DocumentMapping {\n\treturn mapping.NewDocumentMapping()\n}\n\n// NewDocumentStaticMapping returns a new document\n// mapping that will not automatically index parts\n// of a document without an explicit mapping.\nfunc NewDocumentStaticMapping() *mapping.DocumentMapping {\n\treturn mapping.NewDocumentStaticMapping()\n}\n\n// NewDocumentDisabledMapping returns a new document\n// mapping that will not perform any indexing.\nfunc NewDocumentDisabledMapping() *mapping.DocumentMapping {\n\treturn mapping.NewDocumentDisabledMapping()\n}\n\n// NewTextFieldMapping returns a default field mapping for text\nfunc NewTextFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewTextFieldMapping()\n}\n\n// NewKeywordFieldMapping returns a field mapping for text using the keyword\n// analyzer, which essentially doesn't apply any specific text analysis.\nfunc NewKeywordFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewKeywordFieldMapping()\n}\n\n// NewNumericFieldMapping returns a default field mapping for numbers\nfunc NewNumericFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewNumericFieldMapping()\n}\n\n// NewDateTimeFieldMapping returns a default field mapping for dates\nfunc NewDateTimeFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewDateTimeFieldMapping()\n}\n\n// NewBooleanFieldMapping returns a default field mapping for booleans\nfunc NewBooleanFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewBooleanFieldMapping()\n}\n\nfunc NewGeoPointFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewGeoPointFieldMapping()\n}\n\nfunc NewGeoShapeFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewGeoShapeFieldMapping()\n}\n\nfunc NewIPFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewIPFieldMapping()\n}\n"
        },
        {
          "name": "mapping",
          "type": "tree",
          "content": null
        },
        {
          "name": "mapping_vector.go",
          "type": "blob",
          "size": 0.880859375,
          "content": "//  Copyright (c) 2023 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build vectors\n// +build vectors\n\npackage bleve\n\nimport \"github.com/blevesearch/bleve/v2/mapping\"\n\nfunc NewVectorFieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewVectorFieldMapping()\n}\n\nfunc NewVectorBase64FieldMapping() *mapping.FieldMapping {\n\treturn mapping.NewVectorBase64FieldMapping()\n}\n"
        },
        {
          "name": "numeric",
          "type": "tree",
          "content": null
        },
        {
          "name": "pre_search.go",
          "type": "blob",
          "size": 4.333984375,
          "content": "//  Copyright (c) 2024 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"github.com/blevesearch/bleve/v2/search\"\n)\n\n// A preSearchResultProcessor processes the data in\n// the preSearch result from multiple\n// indexes in an alias and merges them together to\n// create the final preSearch result\ntype preSearchResultProcessor interface {\n\t// adds the preSearch result to the processor\n\tadd(*SearchResult, string)\n\t// updates the final search result with the finalized\n\t// data from the processor\n\tfinalize(*SearchResult)\n}\n\n// -----------------------------------------------------------------------------\n// KNN preSearchResultProcessor for handling KNN presearch results\ntype knnPreSearchResultProcessor struct {\n\taddFn      func(sr *SearchResult, indexName string)\n\tfinalizeFn func(sr *SearchResult)\n}\n\nfunc (k *knnPreSearchResultProcessor) add(sr *SearchResult, indexName string) {\n\tif k.addFn != nil {\n\t\tk.addFn(sr, indexName)\n\t}\n}\n\nfunc (k *knnPreSearchResultProcessor) finalize(sr *SearchResult) {\n\tif k.finalizeFn != nil {\n\t\tk.finalizeFn(sr)\n\t}\n}\n\n// -----------------------------------------------------------------------------\n// Synonym preSearchResultProcessor for handling Synonym presearch results\ntype synonymPreSearchResultProcessor struct {\n\tfinalizedFts search.FieldTermSynonymMap\n}\n\nfunc newSynonymPreSearchResultProcessor() *synonymPreSearchResultProcessor {\n\treturn &synonymPreSearchResultProcessor{}\n}\n\nfunc (s *synonymPreSearchResultProcessor) add(sr *SearchResult, indexName string) {\n\t// Check if SynonymResult or the synonym data key is nil\n\tif sr.SynonymResult == nil {\n\t\treturn\n\t}\n\n\t// Attempt to cast PreSearchResults to FieldTermSynonymMap\n\n\t// Merge with finalizedFts or initialize it if nil\n\tif s.finalizedFts == nil {\n\t\ts.finalizedFts = sr.SynonymResult\n\t} else {\n\t\ts.finalizedFts.MergeWith(sr.SynonymResult)\n\t}\n}\n\nfunc (s *synonymPreSearchResultProcessor) finalize(sr *SearchResult) {\n\t// Set the finalized synonym data to the PreSearchResults\n\tif s.finalizedFts != nil {\n\t\tsr.SynonymResult = s.finalizedFts\n\t}\n}\n\n// -----------------------------------------------------------------------------\n// Master struct that can hold any number of presearch result processors\ntype compositePreSearchResultProcessor struct {\n\tpresearchResultProcessors []preSearchResultProcessor\n}\n\n// Implements the add method, which forwards to all the internal processors\nfunc (m *compositePreSearchResultProcessor) add(sr *SearchResult, indexName string) {\n\tfor _, p := range m.presearchResultProcessors {\n\t\tp.add(sr, indexName)\n\t}\n}\n\n// Implements the finalize method, which forwards to all the internal processors\nfunc (m *compositePreSearchResultProcessor) finalize(sr *SearchResult) {\n\tfor _, p := range m.presearchResultProcessors {\n\t\tp.finalize(sr)\n\t}\n}\n\n// -----------------------------------------------------------------------------\n// Function to create the appropriate preSearchResultProcessor(s)\nfunc createPreSearchResultProcessor(req *SearchRequest, flags *preSearchFlags) preSearchResultProcessor {\n\t// return nil for invalid input\n\tif flags == nil || req == nil {\n\t\treturn nil\n\t}\n\tvar processors []preSearchResultProcessor\n\t// Add KNN processor if the request has KNN\n\tif flags.knn {\n\t\tif knnProcessor := newKnnPreSearchResultProcessor(req); knnProcessor != nil {\n\t\t\tprocessors = append(processors, knnProcessor)\n\t\t}\n\t}\n\t// Add Synonym processor if the request has Synonym\n\tif flags.synonyms {\n\t\tif synonymProcessor := newSynonymPreSearchResultProcessor(); synonymProcessor != nil {\n\t\t\tprocessors = append(processors, synonymProcessor)\n\t\t}\n\t}\n\t// Return based on the number of processors, optimizing for the common case of 1 processor\n\t// If there are no processors, return nil\n\tswitch len(processors) {\n\tcase 0:\n\t\treturn nil\n\tcase 1:\n\t\treturn processors[0]\n\tdefault:\n\t\treturn &compositePreSearchResultProcessor{\n\t\t\tpresearchResultProcessors: processors,\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "query.go",
          "type": "blob",
          "size": 11.05859375,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n)\n\n// NewBoolFieldQuery creates a new Query for boolean fields\nfunc NewBoolFieldQuery(val bool) *query.BoolFieldQuery {\n\treturn query.NewBoolFieldQuery(val)\n}\n\n// NewBooleanQuery creates a compound Query composed\n// of several other Query objects.\n// These other query objects are added using the\n// AddMust() AddShould() and AddMustNot() methods.\n// Result documents must satisfy ALL of the\n// must Queries.\n// Result documents must satisfy NONE of the must not\n// Queries.\n// Result documents that ALSO satisfy any of the should\n// Queries will score higher.\nfunc NewBooleanQuery() *query.BooleanQuery {\n\treturn query.NewBooleanQuery(nil, nil, nil)\n}\n\n// NewConjunctionQuery creates a new compound Query.\n// Result documents must satisfy all of the queries.\nfunc NewConjunctionQuery(conjuncts ...query.Query) *query.ConjunctionQuery {\n\treturn query.NewConjunctionQuery(conjuncts)\n}\n\n// NewDateRangeQuery creates a new Query for ranges\n// of date values.\n// Date strings are parsed using the DateTimeParser configured in the\n//\n//\ttop-level config.QueryDateTimeParser\n//\n// Either, but not both endpoints can be nil.\nfunc NewDateRangeQuery(start, end time.Time) *query.DateRangeQuery {\n\treturn query.NewDateRangeQuery(start, end)\n}\n\n// NewDateRangeInclusiveQuery creates a new Query for ranges\n// of date values.\n// Date strings are parsed using the DateTimeParser configured in the\n//\n//\ttop-level config.QueryDateTimeParser\n//\n// Either, but not both endpoints can be nil.\n// startInclusive and endInclusive control inclusion of the endpoints.\nfunc NewDateRangeInclusiveQuery(start, end time.Time, startInclusive, endInclusive *bool) *query.DateRangeQuery {\n\treturn query.NewDateRangeInclusiveQuery(start, end, startInclusive, endInclusive)\n}\n\n// NewDateRangeStringQuery creates a new Query for ranges\n// of date values.\n// Date strings are parsed using the DateTimeParser set using\n//\n//\tthe DateRangeStringQuery.SetDateTimeParser() method.\n//\n// If no DateTimeParser is set, then the\n//\n//\ttop-level config.QueryDateTimeParser\n//\n// is used.\nfunc NewDateRangeStringQuery(start, end string) *query.DateRangeStringQuery {\n\treturn query.NewDateRangeStringQuery(start, end)\n}\n\n// NewDateRangeInclusiveStringQuery creates a new Query for ranges\n// of date values.\n// Date strings are parsed using the DateTimeParser set using\n//\n//\tthe DateRangeStringQuery.SetDateTimeParser() method.\n//\n// this DateTimeParser is a custom date time parser defined in the index mapping,\n// using AddCustomDateTimeParser() method.\n// If no DateTimeParser is set, then the\n//\n//\ttop-level config.QueryDateTimeParser\n//\n// is used.\n// Either, but not both endpoints can be nil.\n// startInclusive and endInclusive control inclusion of the endpoints.\nfunc NewDateRangeInclusiveStringQuery(start, end string, startInclusive, endInclusive *bool) *query.DateRangeStringQuery {\n\treturn query.NewDateRangeStringInclusiveQuery(start, end, startInclusive, endInclusive)\n}\n\n// NewDisjunctionQuery creates a new compound Query.\n// Result documents satisfy at least one Query.\nfunc NewDisjunctionQuery(disjuncts ...query.Query) *query.DisjunctionQuery {\n\treturn query.NewDisjunctionQuery(disjuncts)\n}\n\n// NewDocIDQuery creates a new Query object returning indexed documents among\n// the specified set. Combine it with ConjunctionQuery to restrict the scope of\n// other queries output.\nfunc NewDocIDQuery(ids []string) *query.DocIDQuery {\n\treturn query.NewDocIDQuery(ids)\n}\n\n// NewFuzzyQuery creates a new Query which finds\n// documents containing terms within a specific\n// fuzziness of the specified term.\n// The default fuzziness is 1.\n//\n// The current implementation uses Levenshtein edit\n// distance as the fuzziness metric.\nfunc NewFuzzyQuery(term string) *query.FuzzyQuery {\n\treturn query.NewFuzzyQuery(term)\n}\n\n// NewMatchAllQuery creates a Query which will\n// match all documents in the index.\nfunc NewMatchAllQuery() *query.MatchAllQuery {\n\treturn query.NewMatchAllQuery()\n}\n\n// NewMatchNoneQuery creates a Query which will not\n// match any documents in the index.\nfunc NewMatchNoneQuery() *query.MatchNoneQuery {\n\treturn query.NewMatchNoneQuery()\n}\n\n// NewMatchPhraseQuery creates a new Query object\n// for matching phrases in the index.\n// An Analyzer is chosen based on the field.\n// Input text is analyzed using this analyzer.\n// Token terms resulting from this analysis are\n// used to build a search phrase.  Result documents\n// must match this phrase. Queried field must have been indexed with\n// IncludeTermVectors set to true.\nfunc NewMatchPhraseQuery(matchPhrase string) *query.MatchPhraseQuery {\n\treturn query.NewMatchPhraseQuery(matchPhrase)\n}\n\n// NewMatchQuery creates a Query for matching text.\n// An Analyzer is chosen based on the field.\n// Input text is analyzed using this analyzer.\n// Token terms resulting from this analysis are\n// used to perform term searches.  Result documents\n// must satisfy at least one of these term searches.\nfunc NewMatchQuery(match string) *query.MatchQuery {\n\treturn query.NewMatchQuery(match)\n}\n\n// NewNumericRangeQuery creates a new Query for ranges\n// of numeric values.\n// Either, but not both endpoints can be nil.\n// The minimum value is inclusive.\n// The maximum value is exclusive.\nfunc NewNumericRangeQuery(min, max *float64) *query.NumericRangeQuery {\n\treturn query.NewNumericRangeQuery(min, max)\n}\n\n// NewNumericRangeInclusiveQuery creates a new Query for ranges\n// of numeric values.\n// Either, but not both endpoints can be nil.\n// Control endpoint inclusion with inclusiveMin, inclusiveMax.\nfunc NewNumericRangeInclusiveQuery(min, max *float64, minInclusive, maxInclusive *bool) *query.NumericRangeQuery {\n\treturn query.NewNumericRangeInclusiveQuery(min, max, minInclusive, maxInclusive)\n}\n\n// NewTermRangeQuery creates a new Query for ranges\n// of text terms.\n// Either, but not both endpoints can be \"\".\n// The minimum value is inclusive.\n// The maximum value is exclusive.\nfunc NewTermRangeQuery(min, max string) *query.TermRangeQuery {\n\treturn query.NewTermRangeQuery(min, max)\n}\n\n// NewTermRangeInclusiveQuery creates a new Query for ranges\n// of text terms.\n// Either, but not both endpoints can be \"\".\n// Control endpoint inclusion with inclusiveMin, inclusiveMax.\nfunc NewTermRangeInclusiveQuery(min, max string, minInclusive, maxInclusive *bool) *query.TermRangeQuery {\n\treturn query.NewTermRangeInclusiveQuery(min, max, minInclusive, maxInclusive)\n}\n\n// NewPhraseQuery creates a new Query for finding\n// exact term phrases in the index.\n// The provided terms must exist in the correct\n// order, at the correct index offsets, in the\n// specified field. Queried field must have been indexed with\n// IncludeTermVectors set to true.\nfunc NewPhraseQuery(terms []string, field string) *query.PhraseQuery {\n\treturn query.NewPhraseQuery(terms, field)\n}\n\n// NewPrefixQuery creates a new Query which finds\n// documents containing terms that start with the\n// specified prefix.\nfunc NewPrefixQuery(prefix string) *query.PrefixQuery {\n\treturn query.NewPrefixQuery(prefix)\n}\n\n// NewRegexpQuery creates a new Query which finds\n// documents containing terms that match the\n// specified regular expression.\nfunc NewRegexpQuery(regexp string) *query.RegexpQuery {\n\treturn query.NewRegexpQuery(regexp)\n}\n\n// NewQueryStringQuery creates a new Query used for\n// finding documents that satisfy a query string.  The\n// query string is a small query language for humans.\nfunc NewQueryStringQuery(q string) *query.QueryStringQuery {\n\treturn query.NewQueryStringQuery(q)\n}\n\n// NewTermQuery creates a new Query for finding an\n// exact term match in the index.\nfunc NewTermQuery(term string) *query.TermQuery {\n\treturn query.NewTermQuery(term)\n}\n\n// NewWildcardQuery creates a new Query which finds\n// documents containing terms that match the\n// specified wildcard.  In the wildcard pattern '*'\n// will match any sequence of 0 or more characters,\n// and '?' will match any single character.\nfunc NewWildcardQuery(wildcard string) *query.WildcardQuery {\n\treturn query.NewWildcardQuery(wildcard)\n}\n\n// NewGeoBoundingBoxQuery creates a new Query for performing geo bounding\n// box searches. The arguments describe the position of the box and documents\n// which have an indexed geo point inside the box will be returned.\nfunc NewGeoBoundingBoxQuery(topLeftLon, topLeftLat, bottomRightLon, bottomRightLat float64) *query.GeoBoundingBoxQuery {\n\treturn query.NewGeoBoundingBoxQuery(topLeftLon, topLeftLat, bottomRightLon, bottomRightLat)\n}\n\n// NewGeoDistanceQuery creates a new Query for performing geo distance\n// searches. The arguments describe a position and a distance. Documents\n// which have an indexed geo point which is less than or equal to the provided\n// distance from the given position will be returned.\nfunc NewGeoDistanceQuery(lon, lat float64, distance string) *query.GeoDistanceQuery {\n\treturn query.NewGeoDistanceQuery(lon, lat, distance)\n}\n\n// NewIPRangeQuery creates a new Query for matching IP addresses.\n// If the argument is in CIDR format, then the query will match all\n// IP addresses in the network specified. If the argument is an IP address,\n// then the query will return documents which contain that IP.\n// Both ipv4 and ipv6 are supported.\nfunc NewIPRangeQuery(cidr string) *query.IPRangeQuery {\n\treturn query.NewIPRangeQuery(cidr)\n}\n\n// NewGeoShapeQuery creates a new Query for matching the given geo shape.\n// This method can be used for creating geoshape queries for shape types\n// like: point, linestring, polygon, multipoint, multilinestring,\n// multipolygon and envelope.\nfunc NewGeoShapeQuery(coordinates [][][][]float64, typ, relation string) (*query.GeoShapeQuery, error) {\n\treturn query.NewGeoShapeQuery(coordinates, typ, relation)\n}\n\n// NewGeoShapeCircleQuery creates a new query for a geoshape that is a\n// circle given center point and the radius. Radius formats supported:\n// \"5in\" \"5inch\" \"7yd\" \"7yards\" \"9ft\" \"9feet\" \"11km\" \"11kilometers\"\n// \"3nm\" \"3nauticalmiles\" \"13mm\" \"13millimeters\" \"15cm\" \"15centimeters\"\n// \"17mi\" \"17miles\" \"19m\" \"19meters\" If the unit cannot be determined,\n// the entire string is parsed and the unit of meters is assumed.\nfunc NewGeoShapeCircleQuery(coordinates []float64, radius, relation string) (*query.GeoShapeQuery, error) {\n\treturn query.NewGeoShapeCircleQuery(coordinates, radius, relation)\n}\n\n// NewGeometryCollectionQuery creates a new query for the provided\n// geometrycollection coordinates and types, which could contain\n// multiple geo shapes.\nfunc NewGeometryCollectionQuery(coordinates [][][][][]float64, types []string, relation string) (*query.GeoShapeQuery, error) {\n\treturn query.NewGeometryCollectionQuery(coordinates, types, relation)\n}\n"
        },
        {
          "name": "query_bench_test.go",
          "type": "blob",
          "size": 8.98046875,
          "content": "//  Copyright (c) 2022 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/keyword\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n)\n\nfunc BenchmarkQueryTerm(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewTextFieldMapping()\n\tfm.Analyzer = keyword.Name\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"text\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := []string{\"abc\", \"abcdef\", \"ghi\", \"jkl\", \"jklmno\"}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"text\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tq := NewTermQuery(members[i%len(members)])\n\t\tq.SetField(\"text\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryTermRange(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewTextFieldMapping()\n\tfm.Analyzer = keyword.Name\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"text\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := []string{\"abc\", \"abcdef\", \"ghi\", \"jkl\", \"jklmno\"}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"text\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tinclusive := true\n\tfor i := 0; i < b.N; i++ {\n\t\tq := NewTermRangeInclusiveQuery(\n\t\t\tmembers[i%(len(members)-2)],\n\t\t\tmembers[(i+2)%(len(members)-2)],\n\t\t\t&inclusive,\n\t\t\t&inclusive,\n\t\t)\n\t\tq.SetField(\"text\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryWildcard(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewTextFieldMapping()\n\tfm.Analyzer = keyword.Name\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"text\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := []string{\"abc\", \"abcdef\", \"ghi\", \"jkl\", \"jklmno\"}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"text\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\twildcards := []string{\"ab*\", \"jk*\"}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tq := NewWildcardQuery(wildcards[i%len(wildcards)])\n\t\tq.SetField(\"text\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryNumericRange(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewNumericFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"number\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"number\": i}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tinclusive := true\n\tfor i := 0; i < b.N; i++ {\n\t\tstart := float64(i % 90)\n\t\tend := float64((i + 10) % 90)\n\t\tq := NewNumericRangeInclusiveQuery(&start, &end, &inclusive, &inclusive)\n\t\tq.SetField(\"number\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryDateRange(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewDateTimeFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"date\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := []string{\n\t\t\"2022-11-16T18:45:45Z\",\n\t\t\"2022-11-17T18:45:45Z\",\n\t\t\"2022-11-18T18:45:45Z\",\n\t\t\"2022-11-19T18:45:45Z\",\n\t\t\"2022-11-20T18:45:45Z\",\n\t}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"date\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tinclusive := true\n\tfor i := 0; i < b.N; i++ {\n\t\tstart, _ := time.Parse(\"2006-01-02T12:00:00Z\", members[i%(len(members)-2)])\n\t\tend, _ := time.Parse(\"2006-01-02T12:00:00Z\", members[(i+2)%(len(members)-2)])\n\t\tq := NewDateRangeInclusiveQuery(start, end, &inclusive, &inclusive)\n\t\tq.SetField(\"date\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryGeoDistance(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewGeoPointFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"geo\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := [][]float64{\n\t\t{-121.96713072883645, 37.380331474621045},\n\t\t{-97.75518866579938, 30.38974491308761},\n\t\t{-0.08653451918110022, 51.51063984942306},\n\t\t{-2.230759791360498, 53.481514330841236},\n\t\t{77.59542326042589, 12.97215865921956},\n\t}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"geo\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tcoordinates := members[i%len(members)]\n\t\tq := NewGeoDistanceQuery(coordinates[0], coordinates[1], \"1mi\")\n\t\tq.SetField(\"geo\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkQueryGeoBoundingBox(b *testing.B) {\n\ttmpIndexPath := createTmpIndexPath(b)\n\tdefer cleanupTmpIndexPath(b, tmpIndexPath)\n\n\tfm := mapping.NewGeoPointFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"geo\", fm)\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}()\n\n\tmembers := [][]float64{\n\t\t{-121.96713072883645, 37.380331474621045},\n\t\t{-97.75518866579938, 30.38974491308761},\n\t\t{-0.08653451918110022, 51.51063984942306},\n\t\t{-2.230759791360498, 53.481514330841236},\n\t\t{77.59542326042589, 12.97215865921956},\n\t}\n\tfor i := 0; i < 100; i++ {\n\t\tif err = idx.Index(strconv.Itoa(i),\n\t\t\tmap[string]interface{}{\"geo\": members[i%len(members)]}); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tboundingBoxes := []struct {\n\t\ttopLeft     []float64\n\t\tbottomRight []float64\n\t}{\n\t\t{\n\t\t\ttopLeft:     []float64{-122.14424992609722, 37.49751487670511},\n\t\t\tbottomRight: []float64{-121.78076546622579, 37.26963069737202},\n\t\t},\n\t\t{\n\t\t\ttopLeft:     []float64{-97.85362236226437, 30.473743975245725},\n\t\t\tbottomRight: []float64{-97.58691085968482, 30.285211697102895},\n\t\t},\n\t\t{\n\t\t\ttopLeft:     []float64{-0.28538822102223094, 51.61106497119687},\n\t\t\tbottomRight: []float64{0.16776748108466677, 51.395702237541286},\n\t\t},\n\t\t{\n\t\t\ttopLeft:     []float64{-2.373683904907921, 53.54371945714075},\n\t\t\tbottomRight: []float64{-2.134365533113197, 53.41788831720595},\n\t\t},\n\t\t{\n\t\t\ttopLeft:     []float64{77.52617635172015, 13.037587208986437},\n\t\t\tbottomRight: []float64{77.66508989028102, 12.924426170584738},\n\t\t},\n\t}\n\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\n\tfor i := 0; i < b.N; i++ {\n\t\ttopLeftCoordinates := boundingBoxes[i%len(boundingBoxes)].topLeft\n\t\tbottomRightCoordinates := boundingBoxes[i%len(boundingBoxes)].bottomRight\n\t\tq := NewGeoBoundingBoxQuery(\n\t\t\ttopLeftCoordinates[0],\n\t\t\ttopLeftCoordinates[1],\n\t\t\tbottomRightCoordinates[0],\n\t\t\tbottomRightCoordinates[1],\n\t\t)\n\t\tq.SetField(\"geo\")\n\t\treq := NewSearchRequest(q)\n\t\tif _, err = idx.Search(req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "registry",
          "type": "tree",
          "content": null
        },
        {
          "name": "search.go",
          "type": "blob",
          "size": 17.546875,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/optional\"\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/registry\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/collector\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\t\"github.com/blevesearch/bleve/v2/size\"\n\t\"github.com/blevesearch/bleve/v2/util\"\n)\n\nvar reflectStaticSizeSearchResult int\nvar reflectStaticSizeSearchStatus int\n\nfunc init() {\n\tvar sr SearchResult\n\treflectStaticSizeSearchResult = int(reflect.TypeOf(sr).Size())\n\tvar ss SearchStatus\n\treflectStaticSizeSearchStatus = int(reflect.TypeOf(ss).Size())\n}\n\nvar cache = registry.NewCache()\n\nconst defaultDateTimeParser = optional.Name\n\ntype dateTimeRange struct {\n\tName           string    `json:\"name,omitempty\"`\n\tStart          time.Time `json:\"start,omitempty\"`\n\tEnd            time.Time `json:\"end,omitempty\"`\n\tDateTimeParser string    `json:\"datetime_parser,omitempty\"`\n\tstartString    *string\n\tendString      *string\n}\n\nfunc (dr *dateTimeRange) ParseDates(dateTimeParser analysis.DateTimeParser) (start, end time.Time, err error) {\n\tstart = dr.Start\n\tif dr.Start.IsZero() && dr.startString != nil {\n\t\ts, _, parseError := dateTimeParser.ParseDateTime(*dr.startString)\n\t\tif parseError != nil {\n\t\t\treturn start, end, fmt.Errorf(\"error parsing start date '%s' for date range name '%s': %v\", *dr.startString, dr.Name, parseError)\n\t\t}\n\t\tstart = s\n\t}\n\tend = dr.End\n\tif dr.End.IsZero() && dr.endString != nil {\n\t\te, _, parseError := dateTimeParser.ParseDateTime(*dr.endString)\n\t\tif parseError != nil {\n\t\t\treturn start, end, fmt.Errorf(\"error parsing end date '%s' for date range name '%s': %v\", *dr.endString, dr.Name, parseError)\n\t\t}\n\t\tend = e\n\t}\n\treturn start, end, err\n}\n\nfunc (dr *dateTimeRange) UnmarshalJSON(input []byte) error {\n\tvar temp struct {\n\t\tName           string  `json:\"name,omitempty\"`\n\t\tStart          *string `json:\"start,omitempty\"`\n\t\tEnd            *string `json:\"end,omitempty\"`\n\t\tDateTimeParser string  `json:\"datetime_parser,omitempty\"`\n\t}\n\n\tif err := util.UnmarshalJSON(input, &temp); err != nil {\n\t\treturn err\n\t}\n\n\tdr.Name = temp.Name\n\tif temp.Start != nil {\n\t\tdr.startString = temp.Start\n\t}\n\tif temp.End != nil {\n\t\tdr.endString = temp.End\n\t}\n\tif temp.DateTimeParser != \"\" {\n\t\tdr.DateTimeParser = temp.DateTimeParser\n\t}\n\n\treturn nil\n}\n\nfunc (dr *dateTimeRange) MarshalJSON() ([]byte, error) {\n\trv := map[string]interface{}{\n\t\t\"name\": dr.Name,\n\t}\n\n\tif !dr.Start.IsZero() {\n\t\trv[\"start\"] = dr.Start\n\t} else if dr.startString != nil {\n\t\trv[\"start\"] = dr.startString\n\t}\n\n\tif !dr.End.IsZero() {\n\t\trv[\"end\"] = dr.End\n\t} else if dr.endString != nil {\n\t\trv[\"end\"] = dr.endString\n\t}\n\n\tif dr.DateTimeParser != \"\" {\n\t\trv[\"datetime_parser\"] = dr.DateTimeParser\n\t}\n\treturn util.MarshalJSON(rv)\n}\n\ntype numericRange struct {\n\tName string   `json:\"name,omitempty\"`\n\tMin  *float64 `json:\"min,omitempty\"`\n\tMax  *float64 `json:\"max,omitempty\"`\n}\n\n// A FacetRequest describes a facet or aggregation\n// of the result document set you would like to be\n// built.\ntype FacetRequest struct {\n\tSize           int              `json:\"size\"`\n\tField          string           `json:\"field\"`\n\tNumericRanges  []*numericRange  `json:\"numeric_ranges,omitempty\"`\n\tDateTimeRanges []*dateTimeRange `json:\"date_ranges,omitempty\"`\n}\n\n// NewFacetRequest creates a facet on the specified\n// field that limits the number of entries to the\n// specified size.\nfunc NewFacetRequest(field string, size int) *FacetRequest {\n\treturn &FacetRequest{\n\t\tField: field,\n\t\tSize:  size,\n\t}\n}\n\nfunc (fr *FacetRequest) Validate() error {\n\tnrCount := len(fr.NumericRanges)\n\tdrCount := len(fr.DateTimeRanges)\n\tif nrCount > 0 && drCount > 0 {\n\t\treturn fmt.Errorf(\"facet can only contain numeric ranges or date ranges, not both\")\n\t}\n\n\tif nrCount > 0 {\n\t\tnrNames := map[string]interface{}{}\n\t\tfor _, nr := range fr.NumericRanges {\n\t\t\tif _, ok := nrNames[nr.Name]; ok {\n\t\t\t\treturn fmt.Errorf(\"numeric ranges contains duplicate name '%s'\", nr.Name)\n\t\t\t}\n\t\t\tnrNames[nr.Name] = struct{}{}\n\t\t\tif nr.Min == nil && nr.Max == nil {\n\t\t\t\treturn fmt.Errorf(\"numeric range query must specify either min, max or both for range name '%s'\", nr.Name)\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tdateTimeParser, err := cache.DateTimeParserNamed(defaultDateTimeParser)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdrNames := map[string]interface{}{}\n\t\tfor _, dr := range fr.DateTimeRanges {\n\t\t\tif _, ok := drNames[dr.Name]; ok {\n\t\t\t\treturn fmt.Errorf(\"date ranges contains duplicate name '%s'\", dr.Name)\n\t\t\t}\n\t\t\tdrNames[dr.Name] = struct{}{}\n\t\t\tif dr.DateTimeParser == \"\" {\n\t\t\t\t// cannot parse the date range dates as the defaultDateTimeParser is overridden\n\t\t\t\t// so perform this validation at query time\n\t\t\t\tstart, end, err := dr.ParseDates(dateTimeParser)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"ParseDates err: %v, using date time parser named %s\", err, defaultDateTimeParser)\n\t\t\t\t}\n\t\t\t\tif start.IsZero() && end.IsZero() {\n\t\t\t\t\treturn fmt.Errorf(\"date range query must specify either start, end or both for range name '%s'\", dr.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// AddDateTimeRange adds a bucket to a field\n// containing date values.  Documents with a\n// date value falling into this range are tabulated\n// as part of this bucket/range.\nfunc (fr *FacetRequest) AddDateTimeRange(name string, start, end time.Time) {\n\tif fr.DateTimeRanges == nil {\n\t\tfr.DateTimeRanges = make([]*dateTimeRange, 0, 1)\n\t}\n\tfr.DateTimeRanges = append(fr.DateTimeRanges, &dateTimeRange{Name: name, Start: start, End: end})\n}\n\n// AddDateTimeRangeString adds a bucket to a field\n// containing date values. Uses defaultDateTimeParser to parse the date strings.\nfunc (fr *FacetRequest) AddDateTimeRangeString(name string, start, end *string) {\n\tif fr.DateTimeRanges == nil {\n\t\tfr.DateTimeRanges = make([]*dateTimeRange, 0, 1)\n\t}\n\tfr.DateTimeRanges = append(fr.DateTimeRanges,\n\t\t&dateTimeRange{Name: name, startString: start, endString: end})\n}\n\n// AddDateTimeRangeString adds a bucket to a field\n// containing date values. Uses the specified parser to parse the date strings.\n// provided the parser is registered in the index mapping.\nfunc (fr *FacetRequest) AddDateTimeRangeStringWithParser(name string, start, end *string, parser string) {\n\tif fr.DateTimeRanges == nil {\n\t\tfr.DateTimeRanges = make([]*dateTimeRange, 0, 1)\n\t}\n\tfr.DateTimeRanges = append(fr.DateTimeRanges,\n\t\t&dateTimeRange{Name: name, startString: start, endString: end, DateTimeParser: parser})\n}\n\n// AddNumericRange adds a bucket to a field\n// containing numeric values.  Documents with a\n// numeric value falling into this range are\n// tabulated as part of this bucket/range.\nfunc (fr *FacetRequest) AddNumericRange(name string, min, max *float64) {\n\tif fr.NumericRanges == nil {\n\t\tfr.NumericRanges = make([]*numericRange, 0, 1)\n\t}\n\tfr.NumericRanges = append(fr.NumericRanges, &numericRange{Name: name, Min: min, Max: max})\n}\n\n// FacetsRequest groups together all the\n// FacetRequest objects for a single query.\ntype FacetsRequest map[string]*FacetRequest\n\nfunc (fr FacetsRequest) Validate() error {\n\tfor _, v := range fr {\n\t\tif err := v.Validate(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// HighlightRequest describes how field matches\n// should be highlighted.\ntype HighlightRequest struct {\n\tStyle  *string  `json:\"style\"`\n\tFields []string `json:\"fields\"`\n}\n\n// NewHighlight creates a default\n// HighlightRequest.\nfunc NewHighlight() *HighlightRequest {\n\treturn &HighlightRequest{}\n}\n\n// NewHighlightWithStyle creates a HighlightRequest\n// with an alternate style.\nfunc NewHighlightWithStyle(style string) *HighlightRequest {\n\treturn &HighlightRequest{\n\t\tStyle: &style,\n\t}\n}\n\nfunc (h *HighlightRequest) AddField(field string) {\n\tif h.Fields == nil {\n\t\th.Fields = make([]string, 0, 1)\n\t}\n\th.Fields = append(h.Fields, field)\n}\n\nfunc (r *SearchRequest) Validate() error {\n\tif srq, ok := r.Query.(query.ValidatableQuery); ok {\n\t\terr := srq.Validate()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif r.SearchAfter != nil && r.SearchBefore != nil {\n\t\treturn fmt.Errorf(\"cannot use search after and search before together\")\n\t}\n\n\tif r.SearchAfter != nil {\n\t\tif r.From != 0 {\n\t\t\treturn fmt.Errorf(\"cannot use search after with from !=0\")\n\t\t}\n\t\tif len(r.SearchAfter) != len(r.Sort) {\n\t\t\treturn fmt.Errorf(\"search after must have same size as sort order\")\n\t\t}\n\t}\n\tif r.SearchBefore != nil {\n\t\tif r.From != 0 {\n\t\t\treturn fmt.Errorf(\"cannot use search before with from !=0\")\n\t\t}\n\t\tif len(r.SearchBefore) != len(r.Sort) {\n\t\t\treturn fmt.Errorf(\"search before must have same size as sort order\")\n\t\t}\n\t}\n\n\terr := validateKNN(r)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn r.Facets.Validate()\n}\n\n// AddFacet adds a FacetRequest to this SearchRequest\nfunc (r *SearchRequest) AddFacet(facetName string, f *FacetRequest) {\n\tif r.Facets == nil {\n\t\tr.Facets = make(FacetsRequest, 1)\n\t}\n\tr.Facets[facetName] = f\n}\n\n// SortBy changes the request to use the requested sort order\n// this form uses the simplified syntax with an array of strings\n// each string can either be a field name\n// or the magic value _id and _score which refer to the doc id and search score\n// any of these values can optionally be prefixed with - to reverse the order\nfunc (r *SearchRequest) SortBy(order []string) {\n\tso := search.ParseSortOrderStrings(order)\n\tr.Sort = so\n}\n\n// SortByCustom changes the request to use the requested sort order\nfunc (r *SearchRequest) SortByCustom(order search.SortOrder) {\n\tr.Sort = order\n}\n\n// SetSearchAfter sets the request to skip over hits with a sort\n// value less than the provided sort after key\nfunc (r *SearchRequest) SetSearchAfter(after []string) {\n\tr.SearchAfter = after\n}\n\n// SetSearchBefore sets the request to skip over hits with a sort\n// value greater than the provided sort before key\nfunc (r *SearchRequest) SetSearchBefore(before []string) {\n\tr.SearchBefore = before\n}\n\n// NewSearchRequest creates a new SearchRequest\n// for the Query, using default values for all\n// other search parameters.\nfunc NewSearchRequest(q query.Query) *SearchRequest {\n\treturn NewSearchRequestOptions(q, 10, 0, false)\n}\n\n// NewSearchRequestOptions creates a new SearchRequest\n// for the Query, with the requested size, from\n// and explanation search parameters.\n// By default results are ordered by score, descending.\nfunc NewSearchRequestOptions(q query.Query, size, from int, explain bool) *SearchRequest {\n\treturn &SearchRequest{\n\t\tQuery:   q,\n\t\tSize:    size,\n\t\tFrom:    from,\n\t\tExplain: explain,\n\t\tSort:    search.SortOrder{&search.SortScore{Desc: true}},\n\t}\n}\n\n// IndexErrMap tracks errors with the name of the index where it occurred\ntype IndexErrMap map[string]error\n\n// MarshalJSON seralizes the error into a string for JSON consumption\nfunc (iem IndexErrMap) MarshalJSON() ([]byte, error) {\n\ttmp := make(map[string]string, len(iem))\n\tfor k, v := range iem {\n\t\ttmp[k] = v.Error()\n\t}\n\treturn util.MarshalJSON(tmp)\n}\n\nfunc (iem IndexErrMap) UnmarshalJSON(data []byte) error {\n\tvar tmp map[string]string\n\terr := util.UnmarshalJSON(data, &tmp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor k, v := range tmp {\n\t\tiem[k] = fmt.Errorf(\"%s\", v)\n\t}\n\treturn nil\n}\n\n// SearchStatus is a secion in the SearchResult reporting how many\n// underlying indexes were queried, how many were successful/failed\n// and a map of any errors that were encountered\ntype SearchStatus struct {\n\tTotal      int         `json:\"total\"`\n\tFailed     int         `json:\"failed\"`\n\tSuccessful int         `json:\"successful\"`\n\tErrors     IndexErrMap `json:\"errors,omitempty\"`\n}\n\n// Merge will merge together multiple SearchStatuses during a MultiSearch\nfunc (ss *SearchStatus) Merge(other *SearchStatus) {\n\tss.Total += other.Total\n\tss.Failed += other.Failed\n\tss.Successful += other.Successful\n\tif len(other.Errors) > 0 {\n\t\tif ss.Errors == nil {\n\t\t\tss.Errors = make(map[string]error)\n\t\t}\n\t\tfor otherIndex, otherError := range other.Errors {\n\t\t\tss.Errors[otherIndex] = otherError\n\t\t}\n\t}\n}\n\n// A SearchResult describes the results of executing\n// a SearchRequest.\n//\n// Status - Whether the search was executed on the underlying indexes successfully\n// or failed, and the corresponding errors.\n// Request - The SearchRequest that was executed.\n// Hits - The list of documents that matched the query and their corresponding\n// scores, score explanation, location info and so on.\n// Total - The total number of documents that matched the query.\n// Cost - indicates how expensive was the query with respect to bytes read\n// from the mmaped index files.\n// MaxScore - The maximum score seen across all document hits seen for this query.\n// Took - The time taken to execute the search.\n// Facets - The facet results for the search.\ntype SearchResult struct {\n\tStatus   *SearchStatus                  `json:\"status\"`\n\tRequest  *SearchRequest                 `json:\"request,omitempty\"`\n\tHits     search.DocumentMatchCollection `json:\"hits\"`\n\tTotal    uint64                         `json:\"total_hits\"`\n\tCost     uint64                         `json:\"cost\"`\n\tMaxScore float64                        `json:\"max_score\"`\n\tTook     time.Duration                  `json:\"took\"`\n\tFacets   search.FacetResults            `json:\"facets\"`\n\t// special fields that are applicable only for search\n\t// results that are obtained from a presearch\n\tSynonymResult search.FieldTermSynonymMap `json:\"synonym_result,omitempty\"`\n}\n\nfunc (sr *SearchResult) Size() int {\n\tsizeInBytes := reflectStaticSizeSearchResult + size.SizeOfPtr +\n\t\treflectStaticSizeSearchStatus\n\n\tfor _, entry := range sr.Hits {\n\t\tif entry != nil {\n\t\t\tsizeInBytes += entry.Size()\n\t\t}\n\t}\n\n\tfor k, v := range sr.Facets {\n\t\tsizeInBytes += size.SizeOfString + len(k) +\n\t\t\tv.Size()\n\t}\n\n\treturn sizeInBytes\n}\n\nfunc (sr *SearchResult) String() string {\n\trv := \"\"\n\tif sr.Total > 0 {\n\t\tif sr.Request != nil && sr.Request.Size > 0 {\n\t\t\trv = fmt.Sprintf(\"%d matches, showing %d through %d, took %s\\n\", sr.Total, sr.Request.From+1, sr.Request.From+len(sr.Hits), sr.Took)\n\t\t\tfor i, hit := range sr.Hits {\n\t\t\t\trv += fmt.Sprintf(\"%5d. %s (%f)\\n\", i+sr.Request.From+1, hit.ID, hit.Score)\n\t\t\t\tfor fragmentField, fragments := range hit.Fragments {\n\t\t\t\t\trv += fmt.Sprintf(\"\\t%s\\n\", fragmentField)\n\t\t\t\t\tfor _, fragment := range fragments {\n\t\t\t\t\t\trv += fmt.Sprintf(\"\\t\\t%s\\n\", fragment)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor otherFieldName, otherFieldValue := range hit.Fields {\n\t\t\t\t\tif _, ok := hit.Fragments[otherFieldName]; !ok {\n\t\t\t\t\t\trv += fmt.Sprintf(\"\\t%s\\n\", otherFieldName)\n\t\t\t\t\t\trv += fmt.Sprintf(\"\\t\\t%v\\n\", otherFieldValue)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\trv = fmt.Sprintf(\"%d matches, took %s\\n\", sr.Total, sr.Took)\n\t\t}\n\t} else {\n\t\trv = \"No matches\"\n\t}\n\tif len(sr.Facets) > 0 {\n\t\trv += fmt.Sprintf(\"Facets:\\n\")\n\t\tfor fn, f := range sr.Facets {\n\t\t\trv += fmt.Sprintf(\"%s(%d)\\n\", fn, f.Total)\n\t\t\tfor _, t := range f.Terms.Terms() {\n\t\t\t\trv += fmt.Sprintf(\"\\t%s(%d)\\n\", t.Term, t.Count)\n\t\t\t}\n\t\t\tfor _, n := range f.NumericRanges {\n\t\t\t\trv += fmt.Sprintf(\"\\t%s(%d)\\n\", n.Name, n.Count)\n\t\t\t}\n\t\t\tfor _, d := range f.DateRanges {\n\t\t\t\trv += fmt.Sprintf(\"\\t%s(%d)\\n\", d.Name, d.Count)\n\t\t\t}\n\t\t\tif f.Other != 0 {\n\t\t\t\trv += fmt.Sprintf(\"\\tOther(%d)\\n\", f.Other)\n\t\t\t}\n\t\t}\n\t}\n\treturn rv\n}\n\n// Merge will merge together multiple SearchResults during a MultiSearch\nfunc (sr *SearchResult) Merge(other *SearchResult) {\n\tsr.Status.Merge(other.Status)\n\tsr.Hits = append(sr.Hits, other.Hits...)\n\tsr.Total += other.Total\n\tsr.Cost += other.Cost\n\tif other.MaxScore > sr.MaxScore {\n\t\tsr.MaxScore = other.MaxScore\n\t}\n\tif sr.Facets == nil && len(other.Facets) != 0 {\n\t\tsr.Facets = other.Facets\n\t\treturn\n\t}\n\n\tsr.Facets.Merge(other.Facets)\n}\n\n// MemoryNeededForSearchResult is an exported helper function to determine the RAM\n// needed to accommodate the results for a given search request.\nfunc MemoryNeededForSearchResult(req *SearchRequest) uint64 {\n\tif req == nil {\n\t\treturn 0\n\t}\n\n\tnumDocMatches := req.Size + req.From\n\tif req.Size+req.From > collector.PreAllocSizeSkipCap {\n\t\tnumDocMatches = collector.PreAllocSizeSkipCap\n\t}\n\n\testimate := 0\n\n\t// overhead from the SearchResult structure\n\tvar sr SearchResult\n\testimate += sr.Size()\n\n\tvar dm search.DocumentMatch\n\tsizeOfDocumentMatch := dm.Size()\n\n\t// overhead from results\n\testimate += numDocMatches * sizeOfDocumentMatch\n\n\t// overhead from facet results\n\tif req.Facets != nil {\n\t\tvar fr search.FacetResult\n\t\testimate += len(req.Facets) * fr.Size()\n\t}\n\n\t// overhead from fields, highlighting\n\tvar d document.Document\n\tif len(req.Fields) > 0 || req.Highlight != nil {\n\t\tnumDocsApplicable := req.Size\n\t\tif numDocsApplicable > collector.PreAllocSizeSkipCap {\n\t\t\tnumDocsApplicable = collector.PreAllocSizeSkipCap\n\t\t}\n\t\testimate += numDocsApplicable * d.Size()\n\t}\n\n\treturn uint64(estimate)\n}\n\n// SetSortFunc sets the sort implementation to use when sorting hits.\n//\n// SearchRequests can specify a custom sort implementation to meet\n// their needs. For instance, by specifying a parallel sort\n// that uses all available cores.\nfunc (r *SearchRequest) SetSortFunc(s func(sort.Interface)) {\n\tr.sortFunc = s\n}\n\n// SortFunc returns the sort implementation to use when sorting hits.\n// Defaults to sort.Sort.\nfunc (r *SearchRequest) SortFunc() func(data sort.Interface) {\n\tif r.sortFunc != nil {\n\t\treturn r.sortFunc\n\t}\n\n\treturn sort.Sort\n}\n\nfunc isMatchNoneQuery(q query.Query) bool {\n\t_, ok := q.(*query.MatchNoneQuery)\n\treturn ok\n}\n"
        },
        {
          "name": "search",
          "type": "tree",
          "content": null
        },
        {
          "name": "search_knn.go",
          "type": "blob",
          "size": 20.4521484375,
          "content": "//  Copyright (c) 2023 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build vectors\n// +build vectors\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sort\"\n\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/collector\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nconst supportForVectorSearch = true\n\ntype knnOperator string\n\n// Must be updated only at init\nvar BleveMaxK = int64(10000)\n\ntype SearchRequest struct {\n\tClientContextID  string            `json:\"client_context_id,omitempty\"`\n\tQuery            query.Query       `json:\"query\"`\n\tSize             int               `json:\"size\"`\n\tFrom             int               `json:\"from\"`\n\tHighlight        *HighlightRequest `json:\"highlight\"`\n\tFields           []string          `json:\"fields\"`\n\tFacets           FacetsRequest     `json:\"facets\"`\n\tExplain          bool              `json:\"explain\"`\n\tSort             search.SortOrder  `json:\"sort\"`\n\tIncludeLocations bool              `json:\"includeLocations\"`\n\tScore            string            `json:\"score,omitempty\"`\n\tSearchAfter      []string          `json:\"search_after\"`\n\tSearchBefore     []string          `json:\"search_before\"`\n\n\tKNN         []*KNNRequest `json:\"knn\"`\n\tKNNOperator knnOperator   `json:\"knn_operator\"`\n\n\t// PreSearchData will be a  map that will be used\n\t// in the second phase of any 2-phase search, to provide additional\n\t// context to the second phase. This is useful in the case of index\n\t// aliases where the first phase will gather the PreSearchData from all\n\t// the indexes in the alias, and the second phase will use that\n\t// PreSearchData to perform the actual search.\n\t// The currently accepted map configuration is:\n\t//\n\t// \"_knn_pre_search_data_key\": []*search.DocumentMatch\n\n\tPreSearchData map[string]interface{} `json:\"pre_search_data,omitempty\"`\n\n\tsortFunc func(sort.Interface)\n}\n\n// Vector takes precedence over vectorBase64 in case both fields are given\ntype KNNRequest struct {\n\tField        string       `json:\"field\"`\n\tVector       []float32    `json:\"vector\"`\n\tVectorBase64 string       `json:\"vector_base64\"`\n\tK            int64        `json:\"k\"`\n\tBoost        *query.Boost `json:\"boost,omitempty\"`\n\n\t// Search parameters for the field's vector index part of the segment.\n\t// Value of it depends on the field's backing vector index implementation.\n\t//\n\t// For Faiss IVF index, supported search params are:\n\t//  - ivf_nprobe_pct    : int  // percentage of total clusters to search\n\t//  - ivf_max_codes_pct : float // percentage of total vectors to visit to do a query (across all clusters)\n\t//\n\t// Consult go-faiss to know all supported search params\n\tParams json.RawMessage `json:\"params\"`\n\n\t// Filter query to use with kNN pre-filtering.\n\t// Supports pre-filtering with all existing types of query clauses.\n\tFilterQuery query.Query `json:\"filter,omitempty\"`\n}\n\nfunc (r *SearchRequest) AddKNN(field string, vector []float32, k int64, boost float64) {\n\tb := query.Boost(boost)\n\tr.KNN = append(r.KNN, &KNNRequest{\n\t\tField:  field,\n\t\tVector: vector,\n\t\tK:      k,\n\t\tBoost:  &b,\n\t})\n}\n\nfunc (r *SearchRequest) AddKNNWithFilter(field string, vector []float32, k int64,\n\tboost float64, filterQuery query.Query) {\n\tb := query.Boost(boost)\n\tr.KNN = append(r.KNN, &KNNRequest{\n\t\tField:       field,\n\t\tVector:      vector,\n\t\tK:           k,\n\t\tBoost:       &b,\n\t\tFilterQuery: filterQuery,\n\t})\n}\n\nfunc (r *SearchRequest) AddKNNOperator(operator knnOperator) {\n\tr.KNNOperator = operator\n}\n\n// UnmarshalJSON deserializes a JSON representation of\n// a SearchRequest\nfunc (r *SearchRequest) UnmarshalJSON(input []byte) error {\n\ttype tempKNNReq struct {\n\t\tField        string          `json:\"field\"`\n\t\tVector       []float32       `json:\"vector\"`\n\t\tVectorBase64 string          `json:\"vector_base64\"`\n\t\tK            int64           `json:\"k\"`\n\t\tBoost        *query.Boost    `json:\"boost,omitempty\"`\n\t\tParams       json.RawMessage `json:\"params\"`\n\t\tFilterQuery  json.RawMessage `json:\"filter,omitempty\"`\n\t}\n\n\tvar temp struct {\n\t\tQ                json.RawMessage   `json:\"query\"`\n\t\tSize             *int              `json:\"size\"`\n\t\tFrom             int               `json:\"from\"`\n\t\tHighlight        *HighlightRequest `json:\"highlight\"`\n\t\tFields           []string          `json:\"fields\"`\n\t\tFacets           FacetsRequest     `json:\"facets\"`\n\t\tExplain          bool              `json:\"explain\"`\n\t\tSort             []json.RawMessage `json:\"sort\"`\n\t\tIncludeLocations bool              `json:\"includeLocations\"`\n\t\tScore            string            `json:\"score\"`\n\t\tSearchAfter      []string          `json:\"search_after\"`\n\t\tSearchBefore     []string          `json:\"search_before\"`\n\t\tKNN              []*tempKNNReq     `json:\"knn\"`\n\t\tKNNOperator      knnOperator       `json:\"knn_operator\"`\n\t\tPreSearchData    json.RawMessage   `json:\"pre_search_data\"`\n\t}\n\n\terr := json.Unmarshal(input, &temp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif temp.Size == nil {\n\t\tr.Size = 10\n\t} else {\n\t\tr.Size = *temp.Size\n\t}\n\tif temp.Sort == nil {\n\t\tr.Sort = search.SortOrder{&search.SortScore{Desc: true}}\n\t} else {\n\t\tr.Sort, err = search.ParseSortOrderJSON(temp.Sort)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tr.From = temp.From\n\tr.Explain = temp.Explain\n\tr.Highlight = temp.Highlight\n\tr.Fields = temp.Fields\n\tr.Facets = temp.Facets\n\tr.IncludeLocations = temp.IncludeLocations\n\tr.Score = temp.Score\n\tr.SearchAfter = temp.SearchAfter\n\tr.SearchBefore = temp.SearchBefore\n\tr.Query, err = query.ParseQuery(temp.Q)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif r.Size < 0 {\n\t\tr.Size = 10\n\t}\n\tif r.From < 0 {\n\t\tr.From = 0\n\t}\n\n\tr.KNN = make([]*KNNRequest, len(temp.KNN))\n\tfor i, knnReq := range temp.KNN {\n\t\tr.KNN[i] = &KNNRequest{}\n\t\tr.KNN[i].Field = temp.KNN[i].Field\n\t\tr.KNN[i].Vector = temp.KNN[i].Vector\n\t\tr.KNN[i].VectorBase64 = temp.KNN[i].VectorBase64\n\t\tr.KNN[i].K = temp.KNN[i].K\n\t\tr.KNN[i].Boost = temp.KNN[i].Boost\n\t\tr.KNN[i].Params = temp.KNN[i].Params\n\t\tif len(knnReq.FilterQuery) == 0 {\n\t\t\t// Setting this to nil to avoid ParseQuery() setting it to a match none\n\t\t\tr.KNN[i].FilterQuery = nil\n\t\t} else {\n\t\t\tr.KNN[i].FilterQuery, err = query.ParseQuery(knnReq.FilterQuery)\n\t\t}\n\t}\n\tr.KNNOperator = temp.KNNOperator\n\tif r.KNNOperator == \"\" {\n\t\tr.KNNOperator = knnOperatorOr\n\t}\n\n\tif temp.PreSearchData != nil {\n\t\tr.PreSearchData, err = query.ParsePreSearchData(temp.PreSearchData)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n\n}\n\n// -----------------------------------------------------------------------------\n\nfunc copySearchRequest(req *SearchRequest, preSearchData map[string]interface{}) *SearchRequest {\n\trv := SearchRequest{\n\t\tQuery:            req.Query,\n\t\tSize:             req.Size + req.From,\n\t\tFrom:             0,\n\t\tHighlight:        req.Highlight,\n\t\tFields:           req.Fields,\n\t\tFacets:           req.Facets,\n\t\tExplain:          req.Explain,\n\t\tSort:             req.Sort.Copy(),\n\t\tIncludeLocations: req.IncludeLocations,\n\t\tScore:            req.Score,\n\t\tSearchAfter:      req.SearchAfter,\n\t\tSearchBefore:     req.SearchBefore,\n\t\tKNN:              req.KNN,\n\t\tKNNOperator:      req.KNNOperator,\n\t\tPreSearchData:    preSearchData,\n\t}\n\treturn &rv\n\n}\n\nvar (\n\tknnOperatorAnd = knnOperator(\"and\")\n\tknnOperatorOr  = knnOperator(\"or\")\n)\n\nfunc createKNNQuery(req *SearchRequest, eligibleDocsMap map[int][]index.IndexInternalID,\n\trequiresFiltering map[int]bool) (\n\tquery.Query, []int64, int64, error) {\n\tif requestHasKNN(req) {\n\t\t// first perform validation\n\t\terr := validateKNN(req)\n\t\tif err != nil {\n\t\t\treturn nil, nil, 0, err\n\t\t}\n\t\tvar subQueries []query.Query\n\t\tkArray := make([]int64, 0, len(req.KNN))\n\t\tsumOfK := int64(0)\n\t\tfor i, knn := range req.KNN {\n\t\t\t// If it's a filtered kNN but has no eligible filter hits, then\n\t\t\t// do not run the kNN query.\n\t\t\tif requiresFiltering[i] && len(eligibleDocsMap[i]) <= 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tknnQuery := query.NewKNNQuery(knn.Vector)\n\t\t\tknnQuery.SetFieldVal(knn.Field)\n\t\t\tknnQuery.SetK(knn.K)\n\t\t\tknnQuery.SetBoost(knn.Boost.Value())\n\t\t\tknnQuery.SetParams(knn.Params)\n\t\t\tif len(eligibleDocsMap[i]) > 0 {\n\t\t\t\tknnQuery.SetFilterQuery(knn.FilterQuery)\n\t\t\t\tfilterResults, exists := eligibleDocsMap[i]\n\t\t\t\tif exists {\n\t\t\t\t\tknnQuery.SetFilterResults(filterResults)\n\t\t\t\t}\n\t\t\t}\n\t\t\tsubQueries = append(subQueries, knnQuery)\n\t\t\tkArray = append(kArray, knn.K)\n\t\t\tsumOfK += knn.K\n\t\t}\n\t\trv := query.NewDisjunctionQuery(subQueries)\n\t\trv.RetrieveScoreBreakdown(true)\n\t\treturn rv, kArray, sumOfK, nil\n\t}\n\treturn nil, nil, 0, nil\n}\n\nfunc validateKNN(req *SearchRequest) error {\n\tif req.KNN != nil &&\n\t\treq.KNNOperator != \"\" &&\n\t\treq.KNNOperator != knnOperatorOr &&\n\t\treq.KNNOperator != knnOperatorAnd {\n\t\treturn fmt.Errorf(\"unknown knn operator: %s\", req.KNNOperator)\n\t}\n\tfor _, q := range req.KNN {\n\t\tif q == nil {\n\t\t\treturn fmt.Errorf(\"knn query cannot be nil\")\n\t\t}\n\t\tif len(q.Vector) == 0 && q.VectorBase64 != \"\" {\n\t\t\t// consider vector_base64 only if vector is not provided\n\t\t\tdecodedVector, err := document.DecodeVector(q.VectorBase64)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tq.Vector = decodedVector\n\t\t}\n\t\tif q.K <= 0 || len(q.Vector) == 0 {\n\t\t\treturn fmt.Errorf(\"k must be greater than 0 and vector must be non-empty\")\n\t\t}\n\t\tif q.K > BleveMaxK {\n\t\t\treturn fmt.Errorf(\"k must be less than %d\", BleveMaxK)\n\t\t}\n\t}\n\tswitch req.KNNOperator {\n\tcase knnOperatorAnd, knnOperatorOr, \"\":\n\t\t// Valid cases, do nothing\n\tdefault:\n\t\treturn fmt.Errorf(\"knn_operator must be either 'and' / 'or'\")\n\t}\n\treturn nil\n}\n\nfunc addSortAndFieldsToKNNHits(req *SearchRequest, knnHits []*search.DocumentMatch, reader index.IndexReader, name string) (err error) {\n\trequiredSortFields := req.Sort.RequiredFields()\n\tvar dvReader index.DocValueReader\n\tvar updateFieldVisitor index.DocValueVisitor\n\tif len(requiredSortFields) > 0 {\n\t\tdvReader, err = reader.DocValueReader(requiredSortFields)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tupdateFieldVisitor = func(field string, term []byte) {\n\t\t\treq.Sort.UpdateVisitor(field, term)\n\t\t}\n\t}\n\tfor _, hit := range knnHits {\n\t\tif len(requiredSortFields) > 0 {\n\t\t\terr = dvReader.VisitDocValues(hit.IndexInternalID, updateFieldVisitor)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treq.Sort.Value(hit)\n\t\terr, _ = LoadAndHighlightFields(hit, req, \"\", reader, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\thit.Index = name\n\t}\n\treturn nil\n}\n\nfunc (i *indexImpl) runKnnCollector(ctx context.Context, req *SearchRequest, reader index.IndexReader, preSearch bool) ([]*search.DocumentMatch, error) {\n\t// maps the index of the KNN query in the req to the pre-filter hits aka\n\t// eligible docs' internal IDs .\n\tfilterHitsMap := make(map[int][]index.IndexInternalID)\n\t// Indicates if this query requires filtering downstream\n\t// No filtering required if it's a match all query/no filters applied.\n\trequiresFiltering := make(map[int]bool)\n\n\tfor idx, knnReq := range req.KNN {\n\t\t// TODO Can use goroutines for this filter query stuff - do it if perf results\n\t\t// show this to be significantly slow otherwise.\n\t\tfilterQ := knnReq.FilterQuery\n\t\tif filterQ == nil {\n\t\t\trequiresFiltering[idx] = false\n\t\t\tcontinue\n\t\t}\n\n\t\tif _, ok := filterQ.(*query.MatchAllQuery); ok {\n\t\t\t// Equivalent to not having a filter query.\n\t\t\trequiresFiltering[idx] = false\n\t\t\tcontinue\n\t\t}\n\n\t\tif isMatchNoneQuery(filterQ) {\n\t\t\t// Filtering required since no hits are eligible.\n\t\t\trequiresFiltering[idx] = true\n\t\t\t// a match none query just means none the documents are eligible\n\t\t\t// hence, we can save on running the query.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Applies to all supported types of queries.\n\t\tfilterSearcher, _ := filterQ.Searcher(ctx, reader, i.m, search.SearcherOptions{\n\t\t\tScore: \"none\", // just want eligible hits --> don't compute scores if not needed\n\t\t})\n\t\t// Using the index doc count to determine collector size since we do not\n\t\t// have an estimate of the number of eligible docs in the index yet.\n\t\tindexDocCount, err := i.DocCount()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfilterColl := collector.NewEligibleCollector(int(indexDocCount))\n\t\terr = filterColl.Collect(ctx, filterSearcher, reader)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfilterHits := filterColl.IDs()\n\t\tif len(filterHits) > 0 {\n\t\t\tfilterHitsMap[idx] = filterHits\n\t\t}\n\t\t// set requiresFiltering regardless of whether there're filtered hits or\n\t\t// not to later decide whether to consider the knnQuery or not\n\t\trequiresFiltering[idx] = true\n\t}\n\n\t// Add the filter hits when creating the kNN query\n\tKNNQuery, kArray, sumOfK, err := createKNNQuery(req, filterHitsMap, requiresFiltering)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tknnSearcher, err := KNNQuery.Searcher(ctx, reader, i.m, search.SearcherOptions{\n\t\tExplain: req.Explain,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tknnCollector := collector.NewKNNCollector(kArray, sumOfK)\n\terr = knnCollector.Collect(ctx, knnSearcher, reader)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tknnHits := knnCollector.Results()\n\tif !preSearch {\n\t\tknnHits = finalizeKNNResults(req, knnHits)\n\t}\n\t// at this point, irrespective of whether it is a preSearch or not,\n\t// the knn hits are populated with Sort and Fields.\n\t// it must be ensured downstream that the Sort and Fields are not\n\t// re-evaluated, for these hits.\n\t// also add the index names to the hits, so that when early\n\t// exit takes place after the first phase, the hits will have\n\t// a valid value for Index.\n\terr = addSortAndFieldsToKNNHits(req, knnHits, reader, i.name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn knnHits, nil\n}\n\nfunc setKnnHitsInCollector(knnHits []*search.DocumentMatch, req *SearchRequest, coll *collector.TopNCollector) {\n\tif len(knnHits) > 0 {\n\t\tnewScoreExplComputer := func(queryMatch *search.DocumentMatch, knnMatch *search.DocumentMatch) (float64, *search.Explanation) {\n\t\t\ttotalScore := queryMatch.Score + knnMatch.Score\n\t\t\tif !req.Explain {\n\t\t\t\t// exit early as we don't need to compute the explanation\n\t\t\t\treturn totalScore, nil\n\t\t\t}\n\t\t\treturn totalScore, &search.Explanation{Value: totalScore, Message: \"sum of:\", Children: []*search.Explanation{queryMatch.Expl, knnMatch.Expl}}\n\t\t}\n\t\tcoll.SetKNNHits(knnHits, search.ScoreExplCorrectionCallbackFunc(newScoreExplComputer))\n\t}\n}\n\nfunc finalizeKNNResults(req *SearchRequest, knnHits []*search.DocumentMatch) []*search.DocumentMatch {\n\t// if the KNN operator is AND, then we need to filter out the hits that\n\t// do not have match the KNN queries.\n\tif req.KNNOperator == knnOperatorAnd {\n\t\tidx := 0\n\t\tfor _, hit := range knnHits {\n\t\t\tif len(hit.ScoreBreakdown) == len(req.KNN) {\n\t\t\t\tknnHits[idx] = hit\n\t\t\t\tidx++\n\t\t\t}\n\t\t}\n\t\tknnHits = knnHits[:idx]\n\t}\n\t// fix the score using score breakdown now\n\t// if the score is none, then we need to set the score to 0.0\n\t// if req.Explain is true, then we need to use the expl breakdown to\n\t// finalize the correct explanation.\n\tfor _, hit := range knnHits {\n\t\thit.Score = 0.0\n\t\tif req.Score != \"none\" {\n\t\t\tfor _, score := range hit.ScoreBreakdown {\n\t\t\t\thit.Score += score\n\t\t\t}\n\t\t}\n\t\tif req.Explain {\n\t\t\tchildrenExpl := make([]*search.Explanation, 0, len(hit.ScoreBreakdown))\n\t\t\tfor i := range hit.ScoreBreakdown {\n\t\t\t\tchildrenExpl = append(childrenExpl, hit.Expl.Children[i])\n\t\t\t}\n\t\t\thit.Expl = &search.Explanation{Value: hit.Score, Message: \"sum of:\", Children: childrenExpl}\n\t\t}\n\t\t// we don't need the score breakdown anymore\n\t\t// so we can set it to nil\n\t\thit.ScoreBreakdown = nil\n\t}\n\treturn knnHits\n}\n\n// when we are setting KNN hits in the preSearchData, we need to make sure that\n// the KNN hit goes to the right index. This is because the KNN hits are\n// collected from all the indexes in the alias, but the preSearchData is\n// specific to each index. If alias A1 contains indexes I1 and I2 and\n// the KNN hits collected from both I1 and I2, and merged to get top K\n// hits, then the top K hits need to be distributed to I1 and I2,\n// so that the preSearchData for I1 contains the top K hits from I1 and\n// the preSearchData for I2 contains the top K hits from I2.\nfunc validateAndDistributeKNNHits(knnHits []*search.DocumentMatch, indexes []Index) (map[string][]*search.DocumentMatch, error) {\n\t// create a set of all the index names of this alias\n\tindexNames := make(map[string]struct{}, len(indexes))\n\tfor _, index := range indexes {\n\t\tindexNames[index.Name()] = struct{}{}\n\t}\n\tsegregatedKnnHits := make(map[string][]*search.DocumentMatch)\n\tfor _, hit := range knnHits {\n\t\t// for each hit, we need to perform a validation check to ensure that the stack\n\t\t// is still valid.\n\t\t//\n\t\t// if the stack is empty, then we have an inconsistency/abnormality\n\t\t// since any hit with an empty stack is supposed to land on a leaf index,\n\t\t// and not an alias. This cannot happen in normal circumstances. But\n\t\t// performing this check to be safe. Since we extract the stack top\n\t\t// in the following steps.\n\t\tif len(hit.IndexNames) == 0 {\n\t\t\treturn nil, ErrorTwoPhaseSearchInconsistency\n\t\t}\n\t\t// since the stack is not empty, we need to check if the top of the stack\n\t\t// is a valid index name, of an index that is part of this alias. If not,\n\t\t// then we have an inconsistency that could be caused due to a topology\n\t\t// change.\n\t\tstackTopIdx := len(hit.IndexNames) - 1\n\t\ttop := hit.IndexNames[stackTopIdx]\n\t\tif _, exists := indexNames[top]; !exists {\n\t\t\treturn nil, ErrorTwoPhaseSearchInconsistency\n\t\t}\n\t\tif stackTopIdx == 0 {\n\t\t\t// if the stack consists of only one index, then popping the top\n\t\t\t// would result in an empty slice, and handle this case by setting\n\t\t\t// indexNames to nil. So that the final search results will not\n\t\t\t// contain the indexNames field.\n\t\t\thit.IndexNames = nil\n\t\t} else {\n\t\t\thit.IndexNames = hit.IndexNames[:stackTopIdx]\n\t\t}\n\t\tsegregatedKnnHits[top] = append(segregatedKnnHits[top], hit)\n\t}\n\treturn segregatedKnnHits, nil\n}\n\nfunc requestHasKNN(req *SearchRequest) bool {\n\treturn len(req.KNN) > 0\n}\n\n// returns true if the search request contains a KNN request that can be\n// satisfied by just performing a preSearch, completely bypassing the\n// actual search.\nfunc isKNNrequestSatisfiedByPreSearch(req *SearchRequest) bool {\n\t// if req.Query is not match_none => then we need to go to phase 2\n\t// to perform the actual query.\n\tif !isMatchNoneQuery(req.Query) {\n\t\treturn false\n\t}\n\t// req.Query is a match_none query\n\t//\n\t// if request contains facets, we need to perform phase 2 to calculate\n\t// the facet result. Since documents were removed as part of the\n\t// merging process after phase 1, if the facet results were to be calculated\n\t// during phase 1, then they will be now be incorrect, since merging would\n\t// remove some documents.\n\tif req.Facets != nil {\n\t\treturn false\n\t}\n\t// the request is a match_none query and does not contain any facets\n\t// so we can satisfy the request using just the preSearch result.\n\treturn true\n}\n\nfunc constructKnnPreSearchData(mergedOut map[string]map[string]interface{}, preSearchResult *SearchResult,\n\tindexes []Index) (map[string]map[string]interface{}, error) {\n\n\tdistributedHits, err := validateAndDistributeKNNHits([]*search.DocumentMatch(preSearchResult.Hits), indexes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, index := range indexes {\n\t\tmergedOut[index.Name()][search.KnnPreSearchDataKey] = distributedHits[index.Name()]\n\t}\n\treturn mergedOut, nil\n}\n\nfunc addKnnToDummyRequest(dummyReq *SearchRequest, realReq *SearchRequest) {\n\tdummyReq.KNN = realReq.KNN\n\tdummyReq.KNNOperator = knnOperatorOr\n\tdummyReq.Explain = realReq.Explain\n\tdummyReq.Fields = realReq.Fields\n\tdummyReq.Sort = realReq.Sort\n}\n\nfunc newKnnPreSearchResultProcessor(req *SearchRequest) *knnPreSearchResultProcessor {\n\tkArray := make([]int64, len(req.KNN))\n\tfor i, knnReq := range req.KNN {\n\t\tkArray[i] = knnReq.K\n\t}\n\tknnStore := collector.GetNewKNNCollectorStore(kArray)\n\treturn &knnPreSearchResultProcessor{\n\t\taddFn: func(sr *SearchResult, indexName string) {\n\t\t\tfor _, hit := range sr.Hits {\n\t\t\t\t// tag the hit with the index name, so that when the\n\t\t\t\t// final search result is constructed, the hit will have\n\t\t\t\t// a valid path to follow along the alias tree to reach\n\t\t\t\t// the index.\n\t\t\t\thit.IndexNames = append(hit.IndexNames, indexName)\n\t\t\t\tknnStore.AddDocument(hit)\n\t\t\t}\n\t\t},\n\t\tfinalizeFn: func(sr *SearchResult) {\n\t\t\t// passing nil as the document fixup function, because we don't need to\n\t\t\t// fixup the document, since this was already done in the first phase,\n\t\t\t// hence error is always nil.\n\t\t\t// the merged knn hits are finalized and set in the search result.\n\t\t\tsr.Hits, _ = knnStore.Final(nil)\n\t\t},\n\t}\n}\n"
        },
        {
          "name": "search_knn_test.go",
          "type": "blob",
          "size": 43.76953125,
          "content": "//  Copyright (c) 2023 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build vectors\n// +build vectors\n\npackage bleve\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"strconv\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis/lang/en\"\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nconst testInputCompressedFile = \"test/knn/knn_dataset_queries.zip\"\nconst testDatasetFileName = \"knn_dataset.json\"\nconst testQueryFileName = \"knn_queries.json\"\n\nconst testDatasetDims = 384\n\nvar knnOperators []knnOperator = []knnOperator{knnOperatorAnd, knnOperatorOr}\n\nfunc TestSimilaritySearchPartitionedIndex(t *testing.T) {\n\tdataset, searchRequests, err := readDatasetAndQueries(testInputCompressedFile)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdocuments := makeDatasetIntoDocuments(dataset)\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Analyzer = en.AnalyzerName\n\n\tvecFieldMappingL2 := mapping.NewVectorFieldMapping()\n\tvecFieldMappingL2.Dims = testDatasetDims\n\tvecFieldMappingL2.Similarity = index.EuclideanDistance\n\n\tindexMappingL2Norm := NewIndexMapping()\n\tindexMappingL2Norm.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingL2Norm.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingL2)\n\n\tvecFieldMappingDot := mapping.NewVectorFieldMapping()\n\tvecFieldMappingDot.Dims = testDatasetDims\n\tvecFieldMappingDot.Similarity = index.InnerProduct\n\n\tindexMappingDotProduct := NewIndexMapping()\n\tindexMappingDotProduct.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingDotProduct.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingDot)\n\n\tvecFieldMappingCosine := mapping.NewVectorFieldMapping()\n\tvecFieldMappingCosine.Dims = testDatasetDims\n\tvecFieldMappingCosine.Similarity = index.CosineSimilarity\n\n\tindexMappingCosine := NewIndexMapping()\n\tindexMappingCosine.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingCosine.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingCosine)\n\n\ttype testCase struct {\n\t\ttestType           string\n\t\tqueryIndex         int\n\t\tnumIndexPartitions int\n\t\tmapping            mapping.IndexMapping\n\t}\n\n\ttestCases := []testCase{\n\t\t// l2 norm similarity\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=3\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 4,\n\t\t\tmapping:            indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 10,\n\t\t\tmapping:            indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         1,\n\t\t\tnumIndexPartitions: 5,\n\t\t\tmapping:            indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:disjunction:twoKNNreq:k=2,2\",\n\t\t\tqueryIndex:         2,\n\t\t\tnumIndexPartitions: 4,\n\t\t\tmapping:            indexMappingL2Norm,\n\t\t},\n\t\t// dot product similarity\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=3\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 4,\n\t\t\tmapping:            indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 10,\n\t\t\tmapping:            indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         1,\n\t\t\tnumIndexPartitions: 5,\n\t\t\tmapping:            indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:disjunction:twoKNNreq:k=2,2\",\n\t\t\tqueryIndex:         2,\n\t\t\tnumIndexPartitions: 4,\n\t\t\tmapping:            indexMappingDotProduct,\n\t\t},\n\t\t// cosine similarity\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=3\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 7,\n\t\t\tmapping:            indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match_none:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         0,\n\t\t\tnumIndexPartitions: 5,\n\t\t\tmapping:            indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:match:oneKNNreq:k=2\",\n\t\t\tqueryIndex:         1,\n\t\t\tnumIndexPartitions: 3,\n\t\t\tmapping:            indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\ttestType:           \"multi_partition:disjunction:twoKNNreq:k=2,2\",\n\t\t\tqueryIndex:         2,\n\t\t\tnumIndexPartitions: 9,\n\t\t\tmapping:            indexMappingCosine,\n\t\t},\n\t}\n\n\tindex := NewIndexAlias()\n\tvar reqSort = search.SortOrder{&search.SortScore{Desc: true}, &search.SortDocID{Desc: true}, &search.SortField{Desc: false, Field: \"content\"}}\n\tfor testCaseNum, testCase := range testCases {\n\t\toriginalRequest := searchRequests[testCase.queryIndex]\n\t\tfor _, operator := range knnOperators {\n\n\t\t\tindex.indexes = make([]Index, 0)\n\t\t\tquery := copySearchRequest(originalRequest, nil)\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\tquery.Explain = true\n\n\t\t\tnameToIndex := createPartitionedIndex(documents, index, 1, testCase.mapping, t, false)\n\t\t\tcontrolResult, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(controlResult.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected control result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\t\t\tcleanUp(t, nameToIndex)\n\n\t\t\tindex.indexes = make([]Index, 0)\n\t\t\tquery = copySearchRequest(originalRequest, nil)\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\tquery.Explain = true\n\n\t\t\tnameToIndex = createPartitionedIndex(documents, index, testCase.numIndexPartitions, testCase.mapping, t, false)\n\t\t\texperimentalResult, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(experimentalResult.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\t\t\tverifyResult(t, controlResult, experimentalResult, testCaseNum, true)\n\t\t\tcleanUp(t, nameToIndex)\n\n\t\t\tindex.indexes = make([]Index, 0)\n\t\t\tquery = copySearchRequest(originalRequest, nil)\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\tquery.Explain = true\n\n\t\t\tnameToIndex = createPartitionedIndex(documents, index, testCase.numIndexPartitions, testCase.mapping, t, true)\n\t\t\tmultiLevelIndexResult, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(multiLevelIndexResult.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\t\t\tverifyResult(t, multiLevelIndexResult, experimentalResult, testCaseNum, false)\n\t\t\tcleanUp(t, nameToIndex)\n\n\t\t}\n\t}\n\n\tvar facets = map[string]*FacetRequest{\n\t\t\"content\": {\n\t\t\tField: \"content\",\n\t\t\tSize:  10,\n\t\t},\n\t}\n\n\tindex = NewIndexAlias()\n\tfor testCaseNum, testCase := range testCases {\n\t\tindex.indexes = make([]Index, 0)\n\t\tnameToIndex := createPartitionedIndex(documents, index, testCase.numIndexPartitions, testCase.mapping, t, false)\n\t\toriginalRequest := searchRequests[testCase.queryIndex]\n\t\tfor _, operator := range knnOperators {\n\t\t\tfrom, size := originalRequest.From, originalRequest.Size\n\t\t\tquery := copySearchRequest(originalRequest, nil)\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Explain = true\n\t\t\tquery.From = from\n\t\t\tquery.Size = size\n\n\t\t\t// Three types of queries to run wrt sort and facet fields that require fields.\n\t\t\t// 1. Sort And Facet are there\n\t\t\t// 2. Sort is there, Facet is not there\n\t\t\t// 3. Sort is not there, Facet is there\n\t\t\t// The case where both sort and facet are not there is already covered in the previous tests.\n\n\t\t\t// 1. Sort And Facet are there\n\t\t\tquery.Facets = facets\n\t\t\tquery.Sort = reqSort.Copy()\n\n\t\t\tres1, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(res1.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\n\t\t\tfacetRes1 := res1.Facets\n\t\t\tfacetRes1Str, err := json.Marshal(facetRes1)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\t// 2. Sort is there, Facet is not there\n\t\t\tquery.Facets = nil\n\t\t\tquery.Sort = reqSort.Copy()\n\n\t\t\tres2, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(res2.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\n\t\t\t// 3. Sort is not there, Facet is there\n\t\t\tquery.Facets = facets\n\t\t\tquery.Sort = nil\n\t\t\tres3, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(res3.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\n\t\t\tfacetRes3 := res3.Facets\n\t\t\tfacetRes3Str, err := json.Marshal(facetRes3)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\t// Verify the facet results\n\t\t\tif string(facetRes1Str) != string(facetRes3Str) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected facet results to be equal\", testCaseNum)\n\t\t\t}\n\n\t\t\t// Verify the results\n\t\t\tverifyResult(t, res1, res2, testCaseNum, false)\n\t\t\tverifyResult(t, res2, res3, testCaseNum, true)\n\n\t\t\t// Test early exit fail case -> matchNone + facetRequest\n\t\t\tquery.Query = NewMatchNoneQuery()\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\t// control case\n\t\t\tquery.Facets = nil\n\t\t\tres4Ctrl, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(res4Ctrl.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected control Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\n\t\t\t// experimental case\n\t\t\tquery.Facets = facets\n\t\t\tres4Exp, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(res4Exp.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental Result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\n\t\t\tif !(operator == knnOperatorAnd && res4Ctrl.Total == 0 && res4Exp.Total == 0) {\n\t\t\t\t// catch case where no hits are returned\n\t\t\t\t// due to matchNone query with a KNN request with operator AND\n\t\t\t\t// where no hits are part of the intersection in multi knn request\n\t\t\t\tverifyResult(t, res4Ctrl, res4Exp, testCaseNum, false)\n\t\t\t}\n\t\t}\n\t\tcleanUp(t, nameToIndex)\n\t}\n\n\t// Test Pagination with multi partitioned index\n\tindex = NewIndexAlias()\n\tindex.indexes = make([]Index, 0)\n\tnameToIndex := createPartitionedIndex(documents, index, 8, indexMappingL2Norm, t, true)\n\n\t// Test From + Size pagination for Hybrid Search (2-Phase)\n\tquery := copySearchRequest(searchRequests[4], nil)\n\tquery.Sort = reqSort.Copy()\n\tquery.Facets = facets\n\tquery.Explain = true\n\n\ttestFromSizePagination(t, query, index, nameToIndex)\n\n\t// Test From + Size pagination for Early Exit Hybrid Search (1-Phase)\n\tquery = copySearchRequest(searchRequests[4], nil)\n\tquery.Query = NewMatchNoneQuery()\n\tquery.Sort = reqSort.Copy()\n\tquery.Facets = nil\n\tquery.Explain = true\n\n\ttestFromSizePagination(t, query, index, nameToIndex)\n\n\tcleanUp(t, nameToIndex)\n}\n\nfunc testFromSizePagination(t *testing.T, query *SearchRequest, index Index, nameToIndex map[string]Index) {\n\tquery.From = 0\n\tquery.Size = 30\n\n\tresCtrl, err := index.Search(query)\n\tif err != nil {\n\t\tcleanUp(t, nameToIndex)\n\t\tt.Fatal(err)\n\t}\n\n\tctrlHitIds := make([]string, len(resCtrl.Hits))\n\tfor i, doc := range resCtrl.Hits {\n\t\tctrlHitIds[i] = doc.ID\n\t}\n\t// experimental case\n\n\tfromValues := []int{0, 5, 10, 15, 20, 25}\n\tsize := 5\n\tfor fromIdx := 0; fromIdx < len(fromValues); fromIdx++ {\n\t\tfrom := fromValues[fromIdx]\n\t\tquery.From = from\n\t\tquery.Size = size\n\t\tresExp, err := index.Search(query)\n\t\tif err != nil {\n\t\t\tcleanUp(t, nameToIndex)\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif from >= len(ctrlHitIds) {\n\t\t\tif len(resExp.Hits) != 0 {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"expected 0 hits, got %d\", len(resExp.Hits))\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tnumHitsExp := len(resExp.Hits)\n\t\tnumHitsCtrl := min(len(ctrlHitIds)-from, size)\n\t\tif numHitsExp != numHitsCtrl {\n\t\t\tcleanUp(t, nameToIndex)\n\t\t\tt.Fatalf(\"expected %d hits, got %d\", numHitsCtrl, numHitsExp)\n\t\t}\n\t\tfor i := 0; i < numHitsExp; i++ {\n\t\t\tdoc := resExp.Hits[i]\n\t\t\tstartOffset := from + i\n\t\t\tif doc.ID != ctrlHitIds[startOffset] {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"expected %s at index %d, got %s\", ctrlHitIds[startOffset], i, doc.ID)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestVectorBase64Index(t *testing.T) {\n\tdataset, searchRequests, err := readDatasetAndQueries(testInputCompressedFile)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdocuments := makeDatasetIntoDocuments(dataset)\n\n\t_, searchRequestsCopy, err := readDatasetAndQueries(testInputCompressedFile)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor _, doc := range documents {\n\t\tvec, ok := doc[\"vector\"].([]float32)\n\t\tif !ok {\n\t\t\tt.Fatal(\"Typecasting vector to float array failed\")\n\t\t}\n\n\t\tbuf := new(bytes.Buffer)\n\t\tfor _, v := range vec {\n\t\t\terr := binary.Write(buf, binary.LittleEndian, v)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tdoc[\"vectorEncoded\"] = base64.StdEncoding.EncodeToString(buf.Bytes())\n\t}\n\n\tfor _, sr := range searchRequestsCopy {\n\t\tfor _, kr := range sr.KNN {\n\t\t\tkr.Field = \"vectorEncoded\"\n\t\t}\n\t}\n\n\tcontentFM := NewTextFieldMapping()\n\tcontentFM.Analyzer = en.AnalyzerName\n\n\tvecFML2 := mapping.NewVectorFieldMapping()\n\tvecFML2.Dims = testDatasetDims\n\tvecFML2.Similarity = index.EuclideanDistance\n\n\tvecBFML2 := mapping.NewVectorBase64FieldMapping()\n\tvecBFML2.Dims = testDatasetDims\n\tvecBFML2.Similarity = index.EuclideanDistance\n\n\tvecFMDot := mapping.NewVectorFieldMapping()\n\tvecFMDot.Dims = testDatasetDims\n\tvecFMDot.Similarity = index.InnerProduct\n\n\tvecBFMDot := mapping.NewVectorBase64FieldMapping()\n\tvecBFMDot.Dims = testDatasetDims\n\tvecBFMDot.Similarity = index.InnerProduct\n\n\tindexMappingL2 := NewIndexMapping()\n\tindexMappingL2.DefaultMapping.AddFieldMappingsAt(\"content\", contentFM)\n\tindexMappingL2.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFML2)\n\tindexMappingL2.DefaultMapping.AddFieldMappingsAt(\"vectorEncoded\", vecBFML2)\n\n\tindexMappingDot := NewIndexMapping()\n\tindexMappingDot.DefaultMapping.AddFieldMappingsAt(\"content\", contentFM)\n\tindexMappingDot.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFMDot)\n\tindexMappingDot.DefaultMapping.AddFieldMappingsAt(\"vectorEncoded\", vecBFMDot)\n\n\ttmpIndexPathL2 := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPathL2)\n\n\ttmpIndexPathDot := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPathDot)\n\n\tindexL2, err := New(tmpIndexPathL2, indexMappingL2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := indexL2.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tindexDot, err := New(tmpIndexPathDot, indexMappingDot)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := indexDot.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatchL2 := indexL2.NewBatch()\n\tbatchDot := indexDot.NewBatch()\n\n\tfor _, doc := range documents {\n\t\terr = batchL2.Index(doc[\"id\"].(string), doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\terr = batchDot.Index(doc[\"id\"].(string), doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\terr = indexL2.Batch(batchL2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = indexDot.Batch(batchDot)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor i, _ := range searchRequests {\n\t\tfor _, operator := range knnOperators {\n\t\t\tcontrolQuery := searchRequests[i]\n\t\t\ttestQuery := searchRequestsCopy[i]\n\n\t\t\tcontrolQuery.AddKNNOperator(operator)\n\t\t\ttestQuery.AddKNNOperator(operator)\n\n\t\t\tcontrolResultL2, err := indexL2.Search(controlQuery)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttestResultL2, err := indexL2.Search(testQuery)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif controlResultL2 != nil && testResultL2 != nil {\n\t\t\t\tif len(controlResultL2.Hits) == len(testResultL2.Hits) {\n\t\t\t\t\tfor j, _ := range controlResultL2.Hits {\n\t\t\t\t\t\tif controlResultL2.Hits[j].ID != testResultL2.Hits[j].ID {\n\t\t\t\t\t\t\tt.Fatalf(\"testcase %d failed: expected hit id %s, got hit id %s\", i, controlResultL2.Hits[j].ID, testResultL2.Hits[j].ID)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (controlResultL2 == nil && testResultL2 != nil) ||\n\t\t\t\t(controlResultL2 != nil && testResultL2 == nil) {\n\t\t\t\tt.Fatalf(\"testcase %d failed: expected result %s, got result %s\", i, controlResultL2, testResultL2)\n\t\t\t}\n\n\t\t\tcontrolResultDot, err := indexDot.Search(controlQuery)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttestResultDot, err := indexDot.Search(testQuery)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif controlResultDot != nil && testResultDot != nil {\n\t\t\t\tif len(controlResultDot.Hits) == len(testResultDot.Hits) {\n\t\t\t\t\tfor j, _ := range controlResultDot.Hits {\n\t\t\t\t\t\tif controlResultDot.Hits[j].ID != testResultDot.Hits[j].ID {\n\t\t\t\t\t\t\tt.Fatalf(\"testcase %d failed: expected hit id %s, got hit id %s\", i, controlResultDot.Hits[j].ID, testResultDot.Hits[j].ID)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (controlResultDot == nil && testResultDot != nil) ||\n\t\t\t\t(controlResultDot != nil && testResultDot == nil) {\n\t\t\t\tt.Fatalf(\"testcase %d failed: expected result %s, got result %s\", i, controlResultDot, testResultDot)\n\t\t\t}\n\t\t}\n\t}\n}\n\ntype testDocument struct {\n\tID      string    `json:\"id\"`\n\tContent string    `json:\"content\"`\n\tVector  []float32 `json:\"vector\"`\n}\n\nfunc readDatasetAndQueries(fileName string) ([]testDocument, []*SearchRequest, error) {\n\t// Open the zip archive for reading\n\tr, err := zip.OpenReader(fileName)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar dataset []testDocument\n\tvar queries []*SearchRequest\n\n\tdefer r.Close()\n\tfor _, f := range r.File {\n\t\tjsonFile, err := f.Open()\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tdefer jsonFile.Close()\n\t\tif f.Name == testDatasetFileName {\n\t\t\terr = json.NewDecoder(jsonFile).Decode(&dataset)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t} else if f.Name == testQueryFileName {\n\t\t\terr = json.NewDecoder(jsonFile).Decode(&queries)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t}\n\t}\n\treturn dataset, queries, nil\n}\n\nfunc makeDatasetIntoDocuments(dataset []testDocument) []map[string]interface{} {\n\tdocuments := make([]map[string]interface{}, len(dataset))\n\tfor i := 0; i < len(dataset); i++ {\n\t\tdocument := make(map[string]interface{})\n\t\tdocument[\"id\"] = dataset[i].ID\n\t\tdocument[\"content\"] = dataset[i].Content\n\t\tdocument[\"vector\"] = dataset[i].Vector\n\t\tdocuments[i] = document\n\t}\n\treturn documents\n}\n\nfunc cleanUp(t *testing.T, nameToIndex map[string]Index) {\n\tfor path, childIndex := range nameToIndex {\n\t\terr := childIndex.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcleanupTmpIndexPath(t, path)\n\t}\n}\n\nfunc createChildIndex(docs []map[string]interface{}, mapping mapping.IndexMapping, t *testing.T, nameToIndex map[string]Index) Index {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tindex, err := New(tmpIndexPath, mapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tnameToIndex[index.Name()] = index\n\tbatch := index.NewBatch()\n\tfor _, doc := range docs {\n\t\terr := batch.Index(doc[\"id\"].(string), doc)\n\t\tif err != nil {\n\t\t\tcleanUp(t, nameToIndex)\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = index.Batch(batch)\n\tif err != nil {\n\t\tcleanUp(t, nameToIndex)\n\t\tt.Fatal(err)\n\t}\n\treturn index\n}\n\nfunc createPartitionedIndex(documents []map[string]interface{}, index *indexAliasImpl, numPartitions int,\n\tmapping mapping.IndexMapping, t *testing.T, multiLevel bool) map[string]Index {\n\n\tpartitionSize := len(documents) / numPartitions\n\textraDocs := len(documents) % numPartitions\n\tnumDocsPerPartition := make([]int, numPartitions)\n\tfor i := 0; i < numPartitions; i++ {\n\t\tnumDocsPerPartition[i] = partitionSize\n\t\tif extraDocs > 0 {\n\t\t\tnumDocsPerPartition[i]++\n\t\t\textraDocs--\n\t\t}\n\t}\n\tdocsPerPartition := make([][]map[string]interface{}, numPartitions)\n\tprevCutoff := 0\n\tfor i := 0; i < numPartitions; i++ {\n\t\tdocsPerPartition[i] = make([]map[string]interface{}, numDocsPerPartition[i])\n\t\tfor j := 0; j < numDocsPerPartition[i]; j++ {\n\t\t\tdocsPerPartition[i][j] = documents[prevCutoff+j]\n\t\t}\n\t\tprevCutoff += numDocsPerPartition[i]\n\t}\n\n\trv := make(map[string]Index)\n\tif !multiLevel {\n\t\t// all indexes are at the same level\n\t\tfor i := 0; i < numPartitions; i++ {\n\t\t\tindex.Add(createChildIndex(docsPerPartition[i], mapping, t, rv))\n\t\t}\n\t} else {\n\t\t// alias tree\n\t\tindexes := make([]Index, numPartitions)\n\t\tfor i := 0; i < numPartitions; i++ {\n\t\t\tindexes[i] = createChildIndex(docsPerPartition[i], mapping, t, rv)\n\t\t}\n\t\tnumAlias := int(math.Ceil(float64(numPartitions) / 2.0))\n\t\taliases := make([]IndexAlias, numAlias)\n\t\tfor i := 0; i < numAlias; i++ {\n\t\t\taliases[i] = NewIndexAlias()\n\t\t\taliases[i].SetName(fmt.Sprintf(\"alias%d\", i))\n\t\t\tfor j := 0; j < 2; j++ {\n\t\t\t\tif i*2+j < numPartitions {\n\t\t\t\t\taliases[i].Add(indexes[i*2+j])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < numAlias; i++ {\n\t\t\tindex.Add(aliases[i])\n\t\t}\n\t}\n\treturn rv\n}\n\nfunc createMultipleSegmentsIndex(documents []map[string]interface{}, index Index, numSegments int) error {\n\t// create multiple batches to simulate more than one segment\n\tnumBatches := numSegments\n\n\tbatches := make([]*Batch, numBatches)\n\tnumDocsPerBatch := len(documents) / numBatches\n\textraDocs := len(documents) % numBatches\n\n\tdocsPerBatch := make([]int, numBatches)\n\tfor i := 0; i < numBatches; i++ {\n\t\tdocsPerBatch[i] = numDocsPerBatch\n\t\tif extraDocs > 0 {\n\t\t\tdocsPerBatch[i]++\n\t\t\textraDocs--\n\t\t}\n\t}\n\tprevCutoff := 0\n\tfor i := 0; i < numBatches; i++ {\n\t\tbatches[i] = index.NewBatch()\n\t\tfor j := prevCutoff; j < prevCutoff+docsPerBatch[i]; j++ {\n\t\t\tdoc := documents[j]\n\t\t\terr := batches[i].Index(doc[\"id\"].(string), doc)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tprevCutoff += docsPerBatch[i]\n\t}\n\terrMutex := sync.Mutex{}\n\tvar errors []error\n\twg := sync.WaitGroup{}\n\twg.Add(len(batches))\n\tfor i, batch := range batches {\n\t\tgo func(ix int, batchx *Batch) {\n\t\t\tdefer wg.Done()\n\t\t\terr := index.Batch(batchx)\n\t\t\tif err != nil {\n\t\t\t\terrMutex.Lock()\n\t\t\t\terrors = append(errors, err)\n\t\t\t\terrMutex.Unlock()\n\t\t\t}\n\t\t}(i, batch)\n\t}\n\twg.Wait()\n\tif len(errors) > 0 {\n\t\treturn errors[0]\n\t}\n\treturn nil\n}\n\nfunc truncateScore(score float64) float64 {\n\tepsilon := 1e-4\n\ttruncated := float64(int(score*1e6)) / 1e6\n\tif math.Abs(truncated-1.0) <= epsilon {\n\t\treturn 1.0\n\t}\n\treturn truncated\n}\n\n// Function to compare two Explanation structs recursively\nfunc compareExplanation(a, b *search.Explanation) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\tif a == nil || b == nil {\n\t\treturn false\n\t}\n\n\tif truncateScore(a.Value) != truncateScore(b.Value) || len(a.Children) != len(b.Children) {\n\t\treturn false\n\t}\n\n\t// Sort the children slices before comparison\n\tsortChildren(a.Children)\n\tsortChildren(b.Children)\n\n\tfor i := range a.Children {\n\t\tif !compareExplanation(a.Children[i], b.Children[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Function to sort the children slices\nfunc sortChildren(children []*search.Explanation) {\n\tsort.Slice(children, func(i, j int) bool {\n\t\treturn children[i].Value < children[j].Value\n\t})\n}\n\n// All hits from a hybrid search/knn search should not have\n// index names or score breakdown.\nfunc finalHitsOmitKNNMetadata(hits []*search.DocumentMatch) bool {\n\tfor _, hit := range hits {\n\t\tif hit.IndexNames != nil || hit.ScoreBreakdown != nil {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc finalHitsHaveValidIndex(hits []*search.DocumentMatch, indexes map[string]Index) bool {\n\tfor _, hit := range hits {\n\t\tif hit.Index == \"\" {\n\t\t\treturn false\n\t\t}\n\t\tvar idx Index\n\t\tvar ok bool\n\t\tif idx, ok = indexes[hit.Index]; !ok {\n\t\t\treturn false\n\t\t}\n\t\tif idx == nil {\n\t\t\treturn false\n\t\t}\n\t\tvar doc index.Document\n\t\tdoc, err = idx.Document(hit.ID)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\t\tif doc == nil {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc verifyResult(t *testing.T, controlResult *SearchResult, experimentalResult *SearchResult, testCaseNum int, verifyOnlyDocIDs bool) {\n\tif controlResult.Hits.Len() == 0 || experimentalResult.Hits.Len() == 0 {\n\t\tt.Fatalf(\"test case #%d failed: 0 hits returned\", testCaseNum)\n\t}\n\tif len(controlResult.Hits) != len(experimentalResult.Hits) {\n\t\tt.Fatalf(\"test case #%d failed: expected %d results, got %d\", testCaseNum, len(controlResult.Hits), len(experimentalResult.Hits))\n\t}\n\tif controlResult.Total != experimentalResult.Total {\n\t\tt.Fatalf(\"test case #%d failed: expected total hits to be %d, got %d\", testCaseNum, controlResult.Total, experimentalResult.Total)\n\t}\n\t// KNN Metadata -> Score Breakdown and IndexNames MUST be omitted from the final hits\n\tif !finalHitsOmitKNNMetadata(controlResult.Hits) || !finalHitsOmitKNNMetadata(experimentalResult.Hits) {\n\t\tt.Fatalf(\"test case #%d failed: expected no KNN metadata in hits\", testCaseNum)\n\t}\n\tif controlResult.Took == 0 || experimentalResult.Took == 0 {\n\t\tt.Fatalf(\"test case #%d failed: expected non-zero took time\", testCaseNum)\n\t}\n\tif controlResult.Request == nil || experimentalResult.Request == nil {\n\t\tt.Fatalf(\"test case #%d failed: expected non-nil request\", testCaseNum)\n\t}\n\tif verifyOnlyDocIDs {\n\t\t// in multi partitioned index, we cannot be sure of the score or the ordering of the hits as the tf-idf scores are localized to each partition\n\t\t// so we only check the ids\n\t\tcontrolMap := make(map[string]struct{})\n\t\texperimentalMap := make(map[string]struct{})\n\t\tfor _, hit := range controlResult.Hits {\n\t\t\tcontrolMap[hit.ID] = struct{}{}\n\t\t}\n\t\tfor _, hit := range experimentalResult.Hits {\n\t\t\texperimentalMap[hit.ID] = struct{}{}\n\t\t}\n\t\tif len(controlMap) != len(experimentalMap) {\n\t\t\tt.Fatalf(\"test case #%d failed: expected %d results, got %d\", testCaseNum, len(controlMap), len(experimentalMap))\n\t\t}\n\t\tfor id := range controlMap {\n\t\t\tif _, ok := experimentalMap[id]; !ok {\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected id %s to be in experimental result\", testCaseNum, id)\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\tfor i := 0; i < len(controlResult.Hits); i++ {\n\t\tif controlResult.Hits[i].ID != experimentalResult.Hits[i].ID {\n\t\t\tt.Fatalf(\"test case #%d failed: expected hit %d to have id %s, got %s\", testCaseNum, i, controlResult.Hits[i].ID, experimentalResult.Hits[i].ID)\n\t\t}\n\t\t// Truncate to 6 decimal places\n\t\tactualScore := truncateScore(experimentalResult.Hits[i].Score)\n\t\texpectScore := truncateScore(controlResult.Hits[i].Score)\n\t\tif expectScore != actualScore {\n\t\t\tt.Fatalf(\"test case #%d failed: expected hit %d to have score %f, got %f\", testCaseNum, i, expectScore, actualScore)\n\t\t}\n\t\tif !compareExplanation(controlResult.Hits[i].Expl, experimentalResult.Hits[i].Expl) {\n\t\t\tt.Fatalf(\"test case #%d failed: expected hit %d to have explanation %v, got %v\", testCaseNum, i, controlResult.Hits[i].Expl, experimentalResult.Hits[i].Expl)\n\t\t}\n\t}\n\tif truncateScore(controlResult.MaxScore) != truncateScore(experimentalResult.MaxScore) {\n\t\tt.Fatalf(\"test case #%d: expected maxScore to be %f, got %f\", testCaseNum, controlResult.MaxScore, experimentalResult.MaxScore)\n\t}\n}\n\nfunc TestSimilaritySearchMultipleSegments(t *testing.T) {\n\t// using scorch options to prevent merges during the course of this test\n\t// so that the knnCollector can be accurately tested\n\tscorch.DefaultMemoryPressurePauseThreshold = 0\n\tscorch.DefaultMinSegmentsForInMemoryMerge = math.MaxInt\n\tdataset, searchRequests, err := readDatasetAndQueries(testInputCompressedFile)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdocuments := makeDatasetIntoDocuments(dataset)\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Analyzer = en.AnalyzerName\n\n\tvecFieldMappingL2 := mapping.NewVectorFieldMapping()\n\tvecFieldMappingL2.Dims = testDatasetDims\n\tvecFieldMappingL2.Similarity = index.EuclideanDistance\n\n\tvecFieldMappingDot := mapping.NewVectorFieldMapping()\n\tvecFieldMappingDot.Dims = testDatasetDims\n\tvecFieldMappingDot.Similarity = index.InnerProduct\n\n\tvecFieldMappingCosine := mapping.NewVectorFieldMapping()\n\tvecFieldMappingCosine.Dims = testDatasetDims\n\tvecFieldMappingCosine.Similarity = index.CosineSimilarity\n\n\tindexMappingL2Norm := NewIndexMapping()\n\tindexMappingL2Norm.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingL2Norm.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingL2)\n\n\tindexMappingDotProduct := NewIndexMapping()\n\tindexMappingDotProduct.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingDotProduct.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingDot)\n\n\tindexMappingCosine := NewIndexMapping()\n\tindexMappingCosine.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tindexMappingCosine.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMappingCosine)\n\n\tvar reqSort = search.SortOrder{&search.SortScore{Desc: true}, &search.SortDocID{Desc: true}, &search.SortField{Desc: false, Field: \"content\"}}\n\n\ttestCases := []struct {\n\t\tnumSegments int\n\t\tqueryIndex  int\n\t\tmapping     mapping.IndexMapping\n\t\tscoreValue  string\n\t}{\n\t\t// L2 norm similarity\n\t\t{\n\t\t\tnumSegments: 6,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 8,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 9,\n\t\t\tqueryIndex:  3,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 10,\n\t\t\tqueryIndex:  4,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 11,\n\t\t\tqueryIndex:  5,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t},\n\t\t// dot_product similarity\n\t\t{\n\t\t\tnumSegments: 6,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 8,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 9,\n\t\t\tqueryIndex:  3,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 10,\n\t\t\tqueryIndex:  4,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 11,\n\t\t\tqueryIndex:  5,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t},\n\t\t// cosine similarity\n\t\t{\n\t\t\tnumSegments: 9,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 5,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 4,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 12,\n\t\t\tqueryIndex:  3,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  4,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t{\n\t\t\tnumSegments: 11,\n\t\t\tqueryIndex:  5,\n\t\t\tmapping:     indexMappingCosine,\n\t\t},\n\t\t// score none test\n\t\t{\n\t\t\tnumSegments: 3,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 8,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingL2Norm,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 3,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 8,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingDotProduct,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 3,\n\t\t\tqueryIndex:  0,\n\t\t\tmapping:     indexMappingCosine,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 7,\n\t\t\tqueryIndex:  1,\n\t\t\tmapping:     indexMappingCosine,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t\t{\n\t\t\tnumSegments: 8,\n\t\t\tqueryIndex:  2,\n\t\t\tmapping:     indexMappingCosine,\n\t\t\tscoreValue:  \"none\",\n\t\t},\n\t}\n\tfor testCaseNum, testCase := range testCases {\n\t\toriginalRequest := searchRequests[testCase.queryIndex]\n\t\tfor _, operator := range knnOperators {\n\t\t\t// run single segment test first\n\t\t\ttmpIndexPath := createTmpIndexPath(t)\n\t\t\tindex, err := New(tmpIndexPath, testCase.mapping)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tquery := copySearchRequest(originalRequest, nil)\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Explain = true\n\n\t\t\tnameToIndex := make(map[string]Index)\n\t\t\tnameToIndex[index.Name()] = index\n\n\t\t\terr = createMultipleSegmentsIndex(documents, index, 1)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tcontrolResult, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(controlResult.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected control result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\t\t\tif testCase.scoreValue == \"none\" {\n\n\t\t\t\tquery := copySearchRequest(originalRequest, nil)\n\t\t\t\tquery.Sort = reqSort.Copy()\n\t\t\t\tquery.AddKNNOperator(operator)\n\t\t\t\tquery.Explain = true\n\t\t\t\tquery.Score = testCase.scoreValue\n\n\t\t\t\texpectedResultScoreNone, err := index.Search(query)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\tif !finalHitsHaveValidIndex(expectedResultScoreNone.Hits, nameToIndex) {\n\t\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\t\tt.Fatalf(\"test case #%d failed: expected score none hits to have valid `Index`\", testCaseNum)\n\t\t\t\t}\n\t\t\t\tverifyResult(t, controlResult, expectedResultScoreNone, testCaseNum, true)\n\t\t\t}\n\t\t\tcleanUp(t, nameToIndex)\n\n\t\t\t// run multiple segments test\n\t\t\ttmpIndexPath = createTmpIndexPath(t)\n\t\t\tindex, err = New(tmpIndexPath, testCase.mapping)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tnameToIndex = make(map[string]Index)\n\t\t\tnameToIndex[index.Name()] = index\n\t\t\terr = createMultipleSegmentsIndex(documents, index, testCase.numSegments)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tquery = copySearchRequest(originalRequest, nil)\n\t\t\tquery.Sort = reqSort.Copy()\n\t\t\tquery.AddKNNOperator(operator)\n\t\t\tquery.Explain = true\n\n\t\t\texperimentalResult, err := index.Search(query)\n\t\t\tif err != nil {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !finalHitsHaveValidIndex(experimentalResult.Hits, nameToIndex) {\n\t\t\t\tcleanUp(t, nameToIndex)\n\t\t\t\tt.Fatalf(\"test case #%d failed: expected experimental result hits to have valid `Index`\", testCaseNum)\n\t\t\t}\n\t\t\tverifyResult(t, controlResult, experimentalResult, testCaseNum, false)\n\t\t\tcleanUp(t, nameToIndex)\n\t\t}\n\t}\n}\n\n// Test to see if KNN Operators get added right to the query.\nfunc TestKNNOperator(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tconst dims = 5\n\tgetRandomVector := func() []float32 {\n\t\tvec := make([]float32, dims)\n\t\tfor i := 0; i < dims; i++ {\n\t\t\tvec[i] = rand.Float32()\n\t\t}\n\t\treturn vec\n\t}\n\n\tdataset := make([]map[string]interface{}, 10)\n\n\t// Indexing just a few docs to populate index.\n\tfor i := 0; i < 10; i++ {\n\t\tdataset = append(dataset, map[string]interface{}{\n\t\t\t\"type\":    \"vectorStuff\",\n\t\t\t\"content\": strconv.Itoa(i),\n\t\t\t\"vector\":  getRandomVector(),\n\t\t})\n\t}\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"vectorStuff\", documentMapping)\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Index = true\n\tcontentFieldMapping.Store = true\n\tdocumentMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\n\tvecFieldMapping := mapping.NewVectorFieldMapping()\n\tvecFieldMapping.Index = true\n\tvecFieldMapping.Dims = 5\n\tvecFieldMapping.Similarity = \"dot_product\"\n\tdocumentMapping.AddFieldMappingsAt(\"vector\", vecFieldMapping)\n\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := index.NewBatch()\n\tfor i := 0; i < len(dataset); i++ {\n\t\tbatch.Index(strconv.Itoa(i), dataset[i])\n\t}\n\n\terr = index.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttermQuery := query.NewTermQuery(\"2\")\n\n\tsearchRequest := NewSearchRequest(termQuery)\n\tsearchRequest.AddKNN(\"vector\", getRandomVector(), 3, 2.0)\n\tsearchRequest.AddKNN(\"vector\", getRandomVector(), 2, 1.5)\n\tsearchRequest.Fields = []string{\"content\", \"vector\"}\n\n\t// Conjunction\n\tsearchRequest.AddKNNOperator(knnOperatorAnd)\n\trequiresFiltering := make(map[int]bool)\n\tconjunction, _, _, err := createKNNQuery(searchRequest, nil, requiresFiltering)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error for AND knn operator\")\n\t}\n\n\tconj, ok := conjunction.(*query.DisjunctionQuery)\n\tif !ok {\n\t\tt.Fatalf(\"expected disjunction query\")\n\t}\n\n\tif len(conj.Disjuncts) != 2 {\n\t\tt.Fatalf(\"expected 2 disjuncts\")\n\t}\n\n\t// Disjunction\n\tsearchRequest.AddKNNOperator(knnOperatorOr)\n\tdisjunction, _, _, err := createKNNQuery(searchRequest, nil, requiresFiltering)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error for OR knn operator\")\n\t}\n\n\tdisj, ok := disjunction.(*query.DisjunctionQuery)\n\tif !ok {\n\t\tt.Fatalf(\"expected disjunction query\")\n\t}\n\n\tif len(disj.Disjuncts) != 2 {\n\t\tt.Fatalf(\"expected 2 disjuncts\")\n\t}\n\n\t// Incorrect operator.\n\tsearchRequest.AddKNNOperator(\"bs_op\")\n\tsearchRequest.Query, _, _, err = createKNNQuery(searchRequest, nil, requiresFiltering)\n\tif err == nil {\n\t\tt.Fatalf(\"expected error for incorrect knn operator\")\n\t}\n}\n\nfunc TestKNNFiltering(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tconst dims = 5\n\tgetRandomVector := func() []float32 {\n\t\tvec := make([]float32, dims)\n\t\tfor i := 0; i < dims; i++ {\n\t\t\tvec[i] = rand.Float32()\n\t\t}\n\t\treturn vec\n\t}\n\n\tdataset := make([]map[string]interface{}, 0)\n\n\t// Indexing just a few docs to populate index.\n\tfor i := 0; i < 10; i++ {\n\t\tdataset = append(dataset, map[string]interface{}{\n\t\t\t\"type\":    \"vectorStuff\",\n\t\t\t\"content\": strconv.Itoa(i + 1000),\n\t\t\t\"vector\":  getRandomVector(),\n\t\t})\n\t}\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"vectorStuff\", documentMapping)\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Index = true\n\tcontentFieldMapping.Store = true\n\tdocumentMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\n\tvecFieldMapping := mapping.NewVectorFieldMapping()\n\tvecFieldMapping.Index = true\n\tvecFieldMapping.Dims = 5\n\tvecFieldMapping.Similarity = \"dot_product\"\n\tdocumentMapping.AddFieldMappingsAt(\"vector\", vecFieldMapping)\n\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := index.NewBatch()\n\tfor i := 0; i < len(dataset); i++ {\n\t\t// the id of term \"i\" is (i-1000)\n\t\tbatch.Index(strconv.Itoa(i), dataset[i])\n\t}\n\n\terr = index.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttermQuery := query.NewTermQuery(\"1004\")\n\tfilterRequest := NewSearchRequest(termQuery)\n\tfilteredHits, err := index.Search(filterRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfilteredDocIDs := make(map[string]struct{})\n\tfor _, match := range filteredHits.Hits {\n\t\tfilteredDocIDs[match.ID] = struct{}{}\n\t}\n\n\tsearchRequest := NewSearchRequest(NewMatchNoneQuery())\n\tsearchRequest.AddKNNWithFilter(\"vector\", getRandomVector(), 3, 2.0, termQuery)\n\tsearchRequest.Fields = []string{\"content\", \"vector\"}\n\n\tres, err := index.Search(searchRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// check if any of the returned results are not part of the filtered hits.\n\tfor _, match := range res.Hits {\n\t\tif _, exists := filteredDocIDs[match.ID]; !exists {\n\t\t\tt.Errorf(\"returned result not present in filtered hits\")\n\t\t}\n\t}\n\n\t// No results should be returned with a match_none filter.\n\tsearchRequest = NewSearchRequest(NewMatchNoneQuery())\n\tsearchRequest.AddKNNWithFilter(\"vector\", getRandomVector(), 3, 2.0,\n\t\tNewMatchNoneQuery())\n\tres, err = index.Search(searchRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif len(res.Hits) != 0 {\n\t\tt.Errorf(\"match none filter should return no hits\")\n\t}\n\n\t// Testing with a disjunction query.\n\n\ttermQuery = query.NewTermQuery(\"1003\")\n\ttermQuery2 := query.NewTermQuery(\"1005\")\n\tdisjQuery := query.NewDisjunctionQuery([]query.Query{termQuery, termQuery2})\n\tfilterRequest = NewSearchRequest(disjQuery)\n\tfilteredHits, err = index.Search(filterRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfilteredDocIDs = make(map[string]struct{})\n\tfor _, match := range filteredHits.Hits {\n\t\tfilteredDocIDs[match.ID] = struct{}{}\n\t}\n\n\tsearchRequest = NewSearchRequest(NewMatchNoneQuery())\n\tsearchRequest.AddKNNWithFilter(\"vector\", getRandomVector(), 3, 2.0, disjQuery)\n\tsearchRequest.Fields = []string{\"content\", \"vector\"}\n\n\tres, err = index.Search(searchRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor _, match := range res.Hits {\n\t\tif _, exists := filteredDocIDs[match.ID]; !exists {\n\t\t\tt.Errorf(\"returned result not present in filtered hits\")\n\t\t}\n\t}\n}\n\n// -----------------------------------------------------------------------------\n// Test nested vectors\n\nfunc TestNestedVectors(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tconst dims = 3\n\tconst k = 1 // one nearest neighbor\n\tconst vecFieldName = \"vecData\"\n\n\tdataset := map[string]map[string]interface{}{ // docID -> Doc\n\t\t\"doc1\": {\n\t\t\tvecFieldName: []float32{100, 100, 100},\n\t\t},\n\t\t\"doc2\": {\n\t\t\tvecFieldName: [][]float32{{0, 0, 0}, {1000, 1000, 1000}},\n\t\t},\n\t}\n\n\t// Index mapping\n\tindexMapping := NewIndexMapping()\n\tvm := mapping.NewVectorFieldMapping()\n\tvm.Dims = dims\n\tvm.Similarity = \"l2_norm\"\n\tindexMapping.DefaultMapping.AddFieldMappingsAt(vecFieldName, vm)\n\n\t// Create index and upload documents\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := index.NewBatch()\n\tfor docID, doc := range dataset {\n\t\tbatch.Index(docID, doc)\n\t}\n\n\terr = index.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Run searches\n\n\ttests := []struct {\n\t\tqueryVec      []float32\n\t\texpectedDocID string\n\t}{\n\t\t{\n\t\t\tqueryVec:      []float32{100, 100, 100},\n\t\t\texpectedDocID: \"doc1\",\n\t\t},\n\t\t{\n\t\t\tqueryVec:      []float32{0, 0, 0},\n\t\t\texpectedDocID: \"doc2\",\n\t\t},\n\t\t{\n\t\t\tqueryVec:      []float32{1000, 1000, 1000},\n\t\t\texpectedDocID: \"doc2\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tsearchReq := NewSearchRequest(query.NewMatchNoneQuery())\n\t\tsearchReq.AddKNNWithFilter(vecFieldName, test.queryVec, k, 1000,\n\t\t\tNewMatchAllQuery())\n\n\t\tres, err := index.Search(searchReq)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif len(res.Hits) != 1 {\n\t\t\tt.Fatalf(\"expected 1 hit, got %d\", len(res.Hits))\n\t\t}\n\n\t\tif res.Hits[0].ID != test.expectedDocID {\n\t\t\tt.Fatalf(\"expected docID %s, got %s\", test.expectedDocID,\n\t\t\t\tres.Hits[0].ID)\n\t\t}\n\t}\n}\n\nfunc TestNumVecsStat(t *testing.T) {\n\n\tdataset, _, err := readDatasetAndQueries(testInputCompressedFile)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdocuments := makeDatasetIntoDocuments(dataset)\n\n\tindexMapping := NewIndexMapping()\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Analyzer = en.AnalyzerName\n\tindexMapping.DefaultMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\n\tvecFieldMapping1 := mapping.NewVectorFieldMapping()\n\tvecFieldMapping1.Dims = testDatasetDims\n\tvecFieldMapping1.Similarity = index.EuclideanDistance\n\tindexMapping.DefaultMapping.AddFieldMappingsAt(\"vector\", vecFieldMapping1)\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tfor i := 0; i < 10; i++ {\n\t\tbatch := index.NewBatch()\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tfor k := 0; k < 10; k++ {\n\t\t\t\terr := batch.Index(fmt.Sprintf(\"%d\", i*30+j*10+k), documents[j*10+k])\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = index.Batch(batch)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tstatsMap := index.StatsMap()\n\n\tif indexStats, exists := statsMap[\"index\"]; exists {\n\t\tif indexStatsMap, ok := indexStats.(map[string]interface{}); ok {\n\t\t\tv1, ok := indexStatsMap[\"field:vector:num_vectors\"].(uint64)\n\t\t\tif !ok || v1 != uint64(300) {\n\t\t\t\tt.Fatalf(\"mismatch in the number of vectors, expected 300, got %d\", indexStatsMap[\"field:vector:num_vectors\"])\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "search_no_knn.go",
          "type": "blob",
          "size": 6.64453125,
          "content": "//  Copyright (c) 2023 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !vectors\n// +build !vectors\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"sort\"\n\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/collector\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nconst supportForVectorSearch = false\n\n// A SearchRequest describes all the parameters\n// needed to search the index.\n// Query is required.\n// Size/From describe how much and which part of the\n// result set to return.\n// Highlight describes optional search result\n// highlighting.\n// Fields describes a list of field values which\n// should be retrieved for result documents, provided they\n// were stored while indexing.\n// Facets describe the set of facets to be computed.\n// Explain triggers inclusion of additional search\n// result score explanations.\n// Sort describes the desired order for the results to be returned.\n// Score controls the kind of scoring performed\n// SearchAfter supports deep paging by providing a minimum sort key\n// SearchBefore supports deep paging by providing a maximum sort key\n// sortFunc specifies the sort implementation to use for sorting results.\n//\n// A special field named \"*\" can be used to return all fields.\ntype SearchRequest struct {\n\tClientContextID  string            `json:\"client_context_id,omitempty\"`\n\tQuery            query.Query       `json:\"query\"`\n\tSize             int               `json:\"size\"`\n\tFrom             int               `json:\"from\"`\n\tHighlight        *HighlightRequest `json:\"highlight\"`\n\tFields           []string          `json:\"fields\"`\n\tFacets           FacetsRequest     `json:\"facets\"`\n\tExplain          bool              `json:\"explain\"`\n\tSort             search.SortOrder  `json:\"sort\"`\n\tIncludeLocations bool              `json:\"includeLocations\"`\n\tScore            string            `json:\"score,omitempty\"`\n\tSearchAfter      []string          `json:\"search_after\"`\n\tSearchBefore     []string          `json:\"search_before\"`\n\n\t// PreSearchData will be a  map that will be used\n\t// in the second phase of any 2-phase search, to provide additional\n\t// context to the second phase. This is useful in the case of index\n\t// aliases where the first phase will gather the PreSearchData from all\n\t// the indexes in the alias, and the second phase will use that\n\t// PreSearchData to perform the actual search.\n\t// The currently accepted map configuration is:\n\t//\n\t// \"_knn_pre_search_data_key\": []*search.DocumentMatch\n\n\tPreSearchData map[string]interface{} `json:\"pre_search_data,omitempty\"`\n\n\tsortFunc func(sort.Interface)\n}\n\n// UnmarshalJSON deserializes a JSON representation of\n// a SearchRequest\nfunc (r *SearchRequest) UnmarshalJSON(input []byte) error {\n\tvar temp struct {\n\t\tQ                json.RawMessage   `json:\"query\"`\n\t\tSize             *int              `json:\"size\"`\n\t\tFrom             int               `json:\"from\"`\n\t\tHighlight        *HighlightRequest `json:\"highlight\"`\n\t\tFields           []string          `json:\"fields\"`\n\t\tFacets           FacetsRequest     `json:\"facets\"`\n\t\tExplain          bool              `json:\"explain\"`\n\t\tSort             []json.RawMessage `json:\"sort\"`\n\t\tIncludeLocations bool              `json:\"includeLocations\"`\n\t\tScore            string            `json:\"score\"`\n\t\tSearchAfter      []string          `json:\"search_after\"`\n\t\tSearchBefore     []string          `json:\"search_before\"`\n\t\tPreSearchData    json.RawMessage   `json:\"pre_search_data\"`\n\t}\n\n\terr := json.Unmarshal(input, &temp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif temp.Size == nil {\n\t\tr.Size = 10\n\t} else {\n\t\tr.Size = *temp.Size\n\t}\n\tif temp.Sort == nil {\n\t\tr.Sort = search.SortOrder{&search.SortScore{Desc: true}}\n\t} else {\n\t\tr.Sort, err = search.ParseSortOrderJSON(temp.Sort)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tr.From = temp.From\n\tr.Explain = temp.Explain\n\tr.Highlight = temp.Highlight\n\tr.Fields = temp.Fields\n\tr.Facets = temp.Facets\n\tr.IncludeLocations = temp.IncludeLocations\n\tr.Score = temp.Score\n\tr.SearchAfter = temp.SearchAfter\n\tr.SearchBefore = temp.SearchBefore\n\tr.Query, err = query.ParseQuery(temp.Q)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif r.Size < 0 {\n\t\tr.Size = 10\n\t}\n\tif r.From < 0 {\n\t\tr.From = 0\n\t}\n\tif temp.PreSearchData != nil {\n\t\tr.PreSearchData, err = query.ParsePreSearchData(temp.PreSearchData)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n\n}\n\n// -----------------------------------------------------------------------------\n\nfunc copySearchRequest(req *SearchRequest, preSearchData map[string]interface{}) *SearchRequest {\n\trv := SearchRequest{\n\t\tQuery:            req.Query,\n\t\tSize:             req.Size + req.From,\n\t\tFrom:             0,\n\t\tHighlight:        req.Highlight,\n\t\tFields:           req.Fields,\n\t\tFacets:           req.Facets,\n\t\tExplain:          req.Explain,\n\t\tSort:             req.Sort.Copy(),\n\t\tIncludeLocations: req.IncludeLocations,\n\t\tScore:            req.Score,\n\t\tSearchAfter:      req.SearchAfter,\n\t\tSearchBefore:     req.SearchBefore,\n\t\tPreSearchData:    preSearchData,\n\t}\n\treturn &rv\n}\n\nfunc validateKNN(req *SearchRequest) error {\n\treturn nil\n}\n\nfunc (i *indexImpl) runKnnCollector(ctx context.Context, req *SearchRequest, reader index.IndexReader, preSearch bool) ([]*search.DocumentMatch, error) {\n\treturn nil, nil\n}\n\nfunc setKnnHitsInCollector(knnHits []*search.DocumentMatch, req *SearchRequest, coll *collector.TopNCollector) {\n}\n\nfunc requestHasKNN(req *SearchRequest) bool {\n\treturn false\n}\n\nfunc addKnnToDummyRequest(dummyReq *SearchRequest, realReq *SearchRequest) {\n}\n\nfunc validateAndDistributeKNNHits(knnHits []*search.DocumentMatch, indexes []Index) (map[string][]*search.DocumentMatch, error) {\n\treturn nil, nil\n}\n\nfunc isKNNrequestSatisfiedByPreSearch(req *SearchRequest) bool {\n\treturn false\n}\n\nfunc constructKnnPreSearchData(mergedOut map[string]map[string]interface{}, preSearchResult *SearchResult,\n\tindexes []Index) (map[string]map[string]interface{}, error) {\n\treturn mergedOut, nil\n}\n\nfunc finalizeKNNResults(req *SearchRequest, knnHits []*search.DocumentMatch) []*search.DocumentMatch {\n\treturn knnHits\n}\n\nfunc newKnnPreSearchResultProcessor(req *SearchRequest) *knnPreSearchResultProcessor {\n\treturn &knnPreSearchResultProcessor{} // equivalent to nil\n}\n"
        },
        {
          "name": "search_test.go",
          "type": "blob",
          "size": 105.4111328125,
          "content": "//  Copyright (c) 2014 Couchbase, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// \t\thttp://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bleve\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/blevesearch/bleve/v2/analysis\"\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/custom\"\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/keyword\"\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/simple\"\n\t\"github.com/blevesearch/bleve/v2/analysis/analyzer/standard\"\n\thtml_char_filter \"github.com/blevesearch/bleve/v2/analysis/char/html\"\n\tregexp_char_filter \"github.com/blevesearch/bleve/v2/analysis/char/regexp\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/flexible\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/iso\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/percent\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/sanitized\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/microseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/milliseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/nanoseconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/datetime/timestamp/seconds\"\n\t\"github.com/blevesearch/bleve/v2/analysis/lang/en\"\n\t\"github.com/blevesearch/bleve/v2/analysis/token/length\"\n\t\"github.com/blevesearch/bleve/v2/analysis/token/lowercase\"\n\t\"github.com/blevesearch/bleve/v2/analysis/token/shingle\"\n\t\"github.com/blevesearch/bleve/v2/analysis/tokenizer/single\"\n\t\"github.com/blevesearch/bleve/v2/analysis/tokenizer/whitespace\"\n\t\"github.com/blevesearch/bleve/v2/document\"\n\t\"github.com/blevesearch/bleve/v2/geo\"\n\t\"github.com/blevesearch/bleve/v2/index/scorch\"\n\t\"github.com/blevesearch/bleve/v2/index/upsidedown\"\n\t\"github.com/blevesearch/bleve/v2/mapping\"\n\t\"github.com/blevesearch/bleve/v2/search\"\n\t\"github.com/blevesearch/bleve/v2/search/highlight/highlighter/ansi\"\n\t\"github.com/blevesearch/bleve/v2/search/highlight/highlighter/html\"\n\t\"github.com/blevesearch/bleve/v2/search/query\"\n\tindex \"github.com/blevesearch/bleve_index_api\"\n)\n\nfunc TestSortedFacetedQuery(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\tindexMapping.AddDocumentMapping(\"hotel\", documentMapping)\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Index = true\n\tcontentFieldMapping.DocValues = true\n\tdocumentMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\tdocumentMapping.AddFieldMappingsAt(\"country\", contentFieldMapping)\n\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tindex.Index(\"1\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"k\",\n\t})\n\tindex.Index(\"2\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"l\",\n\t})\n\tindex.Index(\"3\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"k\",\n\t})\n\n\td, err := index.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif d != 3 {\n\t\tt.Errorf(\"expected 3, got %d\", d)\n\t}\n\n\tquery := NewMatchPhraseQuery(\"india\")\n\tquery.SetField(\"country\")\n\tsearchRequest := NewSearchRequest(query)\n\tsearchRequest.SortBy([]string{\"content\"})\n\tfr := NewFacetRequest(\"content\", 100)\n\tsearchRequest.AddFacet(\"content_facet\", fr)\n\n\tsearchResults, err := index.Search(searchRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\texpectedResults := map[string]int{\"k\": 2, \"l\": 1}\n\n\tfor _, v := range searchResults.Facets {\n\t\tfor _, v1 := range v.Terms.Terms() {\n\t\t\tif v1.Count != expectedResults[v1.Term] {\n\t\t\t\tt.Errorf(\"expected %d, got %d\", expectedResults[v1.Term], v1.Count)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestMatchAllScorer(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tindexMapping := NewIndexMapping()\n\tindexMapping.TypeField = \"type\"\n\tindexMapping.DefaultAnalyzer = \"en\"\n\tdocumentMapping := NewDocumentMapping()\n\n\tcontentFieldMapping := NewTextFieldMapping()\n\tcontentFieldMapping.Index = true\n\tcontentFieldMapping.Store = true\n\tdocumentMapping.AddFieldMappingsAt(\"content\", contentFieldMapping)\n\n\tindex, err := New(tmpIndexPath, indexMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := index.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tindex.Index(\"1\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"k\",\n\t})\n\tindex.Index(\"2\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"l\",\n\t})\n\tindex.Index(\"3\", map[string]interface{}{\n\t\t\"country\": \"india\",\n\t\t\"content\": \"k\",\n\t})\n\n\td, err := index.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif d != 3 {\n\t\tt.Errorf(\"expected 3, got %d\", d)\n\t}\n\n\tsearchRequest := NewSearchRequest(NewMatchAllQuery())\n\tsearchRequest.Score = \"none\"\n\tsearchResults, err := index.Search(searchRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif searchResults.Total != 3 {\n\t\tt.Fatalf(\"expected all the 3 docs in the index, got %v\", searchResults.Total)\n\t}\n\n\tfor _, hit := range searchResults.Hits {\n\t\tif hit.Score != 0.0 {\n\t\t\tt.Fatalf(\"expected 0 score since score = none, got %v\", hit.Score)\n\t\t}\n\t}\n}\n\nfunc TestSearchResultString(t *testing.T) {\n\n\ttests := []struct {\n\t\tresult *SearchResult\n\t\tstr    string\n\t}{\n\t\t{\n\t\t\tresult: &SearchResult{\n\t\t\t\tRequest: &SearchRequest{\n\t\t\t\t\tSize: 10,\n\t\t\t\t},\n\t\t\t\tTotal: 5,\n\t\t\t\tTook:  1 * time.Second,\n\t\t\t\tHits: search.DocumentMatchCollection{\n\t\t\t\t\t&search.DocumentMatch{},\n\t\t\t\t\t&search.DocumentMatch{},\n\t\t\t\t\t&search.DocumentMatch{},\n\t\t\t\t\t&search.DocumentMatch{},\n\t\t\t\t\t&search.DocumentMatch{},\n\t\t\t\t},\n\t\t\t},\n\t\t\tstr: \"5 matches, showing 1 through 5, took 1s\",\n\t\t},\n\t\t{\n\t\t\tresult: &SearchResult{\n\t\t\t\tRequest: &SearchRequest{\n\t\t\t\t\tSize: 0,\n\t\t\t\t},\n\t\t\t\tTotal: 5,\n\t\t\t\tHits:  search.DocumentMatchCollection{},\n\t\t\t},\n\t\t\tstr: \"5 matches\",\n\t\t},\n\t\t{\n\t\t\tresult: &SearchResult{\n\t\t\t\tRequest: &SearchRequest{\n\t\t\t\t\tSize: 10,\n\t\t\t\t},\n\t\t\t\tTotal: 0,\n\t\t\t\tHits:  search.DocumentMatchCollection{},\n\t\t\t},\n\t\t\tstr: \"No matches\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tsrstring := test.result.String()\n\t\tif !strings.HasPrefix(srstring, test.str) {\n\t\t\tt.Errorf(\"expected to start %s, got %s\", test.str, srstring)\n\t\t}\n\t}\n}\n\nfunc TestSearchResultMerge(t *testing.T) {\n\tl := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    1,\n\t\tMaxScore: 1,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t&search.DocumentMatch{\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1,\n\t\t\t},\n\t\t},\n\t}\n\n\tr := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    1,\n\t\tMaxScore: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t&search.DocumentMatch{\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2,\n\t\t\t},\n\t\t},\n\t}\n\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      2,\n\t\t\tSuccessful: 2,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    2,\n\t\tMaxScore: 2,\n\t\tHits: search.DocumentMatchCollection{\n\t\t\t&search.DocumentMatch{\n\t\t\t\tID:    \"a\",\n\t\t\t\tScore: 1,\n\t\t\t},\n\t\t\t&search.DocumentMatch{\n\t\t\t\tID:    \"b\",\n\t\t\t\tScore: 2,\n\t\t\t},\n\t\t},\n\t}\n\n\tl.Merge(r)\n\n\tif !reflect.DeepEqual(l, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, l)\n\t}\n}\n\nfunc TestUnmarshalingSearchResult(t *testing.T) {\n\n\tsearchResponse := []byte(`{\n    \"status\":{\n      \"total\":1,\n      \"failed\":1,\n      \"successful\":0,\n      \"errors\":{\n        \"default_index_362ce020b3d62b13_348f5c3c\":\"context deadline exceeded\"\n      }\n    },\n    \"request\":{\n      \"query\":{\n        \"match\":\"emp\",\n        \"field\":\"type\",\n        \"boost\":1,\n        \"prefix_length\":0,\n        \"fuzziness\":0\n      },\n    \"size\":10000000,\n    \"from\":0,\n    \"highlight\":null,\n    \"fields\":[],\n    \"facets\":null,\n    \"explain\":false\n  },\n  \"hits\":null,\n  \"total_hits\":0,\n  \"max_score\":0,\n  \"took\":0,\n  \"facets\":null\n}`)\n\n\trv := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tErrors: make(map[string]error),\n\t\t},\n\t}\n\terr = json.Unmarshal(searchResponse, rv)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif len(rv.Status.Errors) != 1 {\n\t\tt.Errorf(\"expected 1 error, got %d\", len(rv.Status.Errors))\n\t}\n}\n\nfunc TestFacetNumericDateRangeRequests(t *testing.T) {\n\tvar drMissingErr = fmt.Errorf(\"date range query must specify either start, end or both for range name 'testName'\")\n\tvar nrMissingErr = fmt.Errorf(\"numeric range query must specify either min, max or both for range name 'testName'\")\n\tvar drNrErr = fmt.Errorf(\"facet can only contain numeric ranges or date ranges, not both\")\n\tvar drNameDupErr = fmt.Errorf(\"date ranges contains duplicate name 'testName'\")\n\tvar nrNameDupErr = fmt.Errorf(\"numeric ranges contains duplicate name 'testName'\")\n\tvalue := float64(5)\n\n\ttests := []struct {\n\t\tfacet  *FacetRequest\n\t\tresult error\n\t}{\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Date_Range_Success_With_StartEnd\",\n\t\t\t\tSize:  1,\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName\", Start: time.Unix(0, 0), End: time.Now()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Date_Range_Success_With_Start\",\n\t\t\t\tSize:  1,\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName\", Start: time.Unix(0, 0)},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Date_Range_Success_With_End\",\n\t\t\t\tSize:  1,\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName\", End: time.Now()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_Range_Success_With_MinMax\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName\", Min: &value, Max: &value},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_Range_Success_With_Min\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName\", Min: &value},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_Range_Success_With_Max\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName\", Max: &value},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nil,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Date_Range_Missing_Failure\",\n\t\t\t\tSize:  1,\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName2\", Start: time.Unix(0, 0)},\n\t\t\t\t\t{Name: \"testName1\", End: time.Now()},\n\t\t\t\t\t{Name: \"testName\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: drMissingErr,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_Range_Missing_Failure\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName2\", Min: &value},\n\t\t\t\t\t{Name: \"testName1\", Max: &value},\n\t\t\t\t\t{Name: \"testName\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nrMissingErr,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_And_DateRanges_Failure\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName\", Max: &value},\n\t\t\t\t},\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName\", End: time.Now()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: drNrErr,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Numeric_Range_Name_Repeat_Failure\",\n\t\t\t\tSize:  1,\n\t\t\t\tNumericRanges: []*numericRange{\n\t\t\t\t\t{Name: \"testName\", Min: &value},\n\t\t\t\t\t{Name: \"testName\", Max: &value},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: nrNameDupErr,\n\t\t},\n\t\t{\n\t\t\tfacet: &FacetRequest{\n\t\t\t\tField: \"Date_Range_Name_Repeat_Failure\",\n\t\t\t\tSize:  1,\n\t\t\t\tDateTimeRanges: []*dateTimeRange{\n\t\t\t\t\t{Name: \"testName\", Start: time.Unix(0, 0)},\n\t\t\t\t\t{Name: \"testName\", End: time.Now()},\n\t\t\t\t},\n\t\t\t},\n\t\t\tresult: drNameDupErr,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tresult := test.facet.Validate()\n\t\tif !reflect.DeepEqual(result, test.result) {\n\t\t\tt.Errorf(\"expected %#v, got %#v\", test.result, result)\n\t\t}\n\t}\n\n}\n\nfunc TestSearchResultFacetsMerge(t *testing.T) {\n\tlowmed := \"2010-01-01\"\n\tmedhi := \"2011-01-01\"\n\thihigher := \"2012-01-01\"\n\n\tfr := &search.FacetResult{\n\t\tField:   \"birthday\",\n\t\tTotal:   100,\n\t\tMissing: 25,\n\t\tOther:   25,\n\t\tDateRanges: []*search.DateRangeFacet{\n\t\t\t{\n\t\t\t\tName:  \"low\",\n\t\t\t\tEnd:   &lowmed,\n\t\t\t\tCount: 25,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"med\",\n\t\t\t\tCount: 24,\n\t\t\t\tStart: &lowmed,\n\t\t\t\tEnd:   &medhi,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"hi\",\n\t\t\t\tCount: 1,\n\t\t\t\tStart: &medhi,\n\t\t\t\tEnd:   &hihigher,\n\t\t\t},\n\t\t},\n\t}\n\tfrs := search.FacetResults{\n\t\t\"birthdays\": fr,\n\t}\n\n\tl := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      10,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    10,\n\t\tMaxScore: 1,\n\t}\n\n\tr := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      1,\n\t\t\tSuccessful: 1,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    1,\n\t\tMaxScore: 2,\n\t\tFacets:   frs,\n\t}\n\n\texpected := &SearchResult{\n\t\tStatus: &SearchStatus{\n\t\t\tTotal:      11,\n\t\t\tSuccessful: 2,\n\t\t\tErrors:     make(map[string]error),\n\t\t},\n\t\tTotal:    11,\n\t\tMaxScore: 2,\n\t\tFacets:   frs,\n\t}\n\n\tl.Merge(r)\n\n\tif !reflect.DeepEqual(l, expected) {\n\t\tt.Errorf(\"expected %#v, got %#v\", expected, l)\n\t}\n}\n\nfunc TestMemoryNeededForSearchResult(t *testing.T) {\n\tquery := NewTermQuery(\"blah\")\n\treq := NewSearchRequest(query)\n\n\tvar sr SearchResult\n\texpect := sr.Size()\n\tvar dm search.DocumentMatch\n\texpect += 10 * dm.Size()\n\n\testimate := MemoryNeededForSearchResult(req)\n\tif estimate != uint64(expect) {\n\t\tt.Errorf(\"estimate not what is expected: %v != %v\", estimate, expect)\n\t}\n}\n\n// https://github.com/blevesearch/bleve/issues/954\nfunc TestNestedBooleanSearchers(t *testing.T) {\n\t// create an index with a custom analyzer\n\tidxMapping := NewIndexMapping()\n\tif err := idxMapping.AddCustomAnalyzer(\"3xbla\", map[string]interface{}{\n\t\t\"type\":          custom.Name,\n\t\t\"tokenizer\":     whitespace.Name,\n\t\t\"token_filters\": []interface{}{lowercase.Name, \"stop_en\"},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidxMapping.DefaultAnalyzer = \"3xbla\"\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// create and insert documents as a batch\n\tbatch := idx.NewBatch()\n\tmatches := 0\n\tfor i := 0; i < 100; i++ {\n\t\thostname := fmt.Sprintf(\"planner_hostname_%d\", i%5)\n\t\tmetadata := map[string]string{\"region\": fmt.Sprintf(\"planner_us-east-%d\", i%5)}\n\n\t\t// Expected matches\n\t\tif (hostname == \"planner_hostname_1\" || hostname == \"planner_hostname_2\") &&\n\t\t\tmetadata[\"region\"] == \"planner_us-east-1\" {\n\t\t\tmatches++\n\t\t}\n\n\t\tdoc := document.NewDocument(strconv.Itoa(i))\n\t\tdoc.Fields = []document.Field{\n\t\t\tdocument.NewTextFieldCustom(\"hostname\", []uint64{}, []byte(hostname),\n\t\t\t\tindex.IndexField,\n\t\t\t\t&analysis.DefaultAnalyzer{\n\t\t\t\t\tTokenizer: single.NewSingleTokenTokenizer(),\n\t\t\t\t\tTokenFilters: []analysis.TokenFilter{\n\t\t\t\t\t\tlowercase.NewLowerCaseFilter(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t),\n\t\t}\n\t\tfor k, v := range metadata {\n\t\t\tdoc.AddField(document.NewTextFieldWithIndexingOptions(\n\t\t\t\tfmt.Sprintf(\"metadata.%s\", k), []uint64{}, []byte(v), index.IndexField))\n\t\t}\n\t\tdoc.CompositeFields = []*document.CompositeField{\n\t\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t\t}\n\n\t\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tque, err := query.ParseQuery([]byte(\n\t\t`{\n\t\t\t\"conjuncts\": [\n\t\t\t{\n\t\t\t\t\"must\": {\n\t\t\t\t\t\"conjuncts\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"disjuncts\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"match\": \"planner_hostname_1\",\n\t\t\t\t\t\t\t\"field\": \"hostname\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"match\": \"planner_hostname_2\",\n\t\t\t\t\t\t\t\"field\": \"hostname\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"must\": {\n\t\t\t\t\t\"conjuncts\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"match\": \"planner_us-east-1\",\n\t\t\t\t\t\t\"field\": \"metadata.region\"\n\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t\t]\n\t\t}`,\n\t))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treq := NewSearchRequest(que)\n\treq.Size = 100\n\treq.Fields = []string{\"hostname\", \"metadata.region\"}\n\tsearchResults, err := idx.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif matches != len(searchResults.Hits) {\n\t\tt.Fatalf(\"Unexpected result set, %v != %v\", matches, len(searchResults.Hits))\n\t}\n}\n\nfunc TestNestedBooleanMustNotSearcherUpsidedown(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// create an index with default settings\n\tidxMapping := NewIndexMapping()\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// create and insert documents as a batch\n\tbatch := idx.NewBatch()\n\n\tdocs := []struct {\n\t\tid              string\n\t\thasRole         bool\n\t\tinvestigationId string\n\t}{\n\t\t{\n\t\t\tid:              \"1@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"1@2\",\n\t\t\thasRole:         false,\n\t\t\tinvestigationId: \"2\",\n\t\t},\n\t\t{\n\t\t\tid:              \"2@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"2@2\",\n\t\t\thasRole:         false,\n\t\t\tinvestigationId: \"2\",\n\t\t},\n\t\t{\n\t\t\tid:              \"3@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"3@2\",\n\t\t\thasRole:         false,\n\t\t\tinvestigationId: \"2\",\n\t\t},\n\t\t{\n\t\t\tid:              \"4@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"5@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"6@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t\t{\n\t\t\tid:              \"7@1\",\n\t\t\thasRole:         true,\n\t\t\tinvestigationId: \"1\",\n\t\t},\n\t}\n\n\tfor i := 0; i < len(docs); i++ {\n\t\tdoc := document.NewDocument(docs[i].id)\n\t\tdoc.Fields = []document.Field{\n\t\t\tdocument.NewTextField(\"id\", []uint64{}, []byte(docs[i].id)),\n\t\t\tdocument.NewBooleanField(\"hasRole\", []uint64{}, docs[i].hasRole),\n\t\t\tdocument.NewTextField(\"investigationId\", []uint64{}, []byte(docs[i].investigationId)),\n\t\t}\n\n\t\tdoc.CompositeFields = []*document.CompositeField{\n\t\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t\t}\n\n\t\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttq := NewTermQuery(\"1\")\n\ttq.SetField(\"investigationId\")\n\t// using must not, for cases that the field did not exists at all\n\thasRole := NewBoolFieldQuery(true)\n\thasRole.SetField(\"hasRole\")\n\tnoRole := NewBooleanQuery()\n\tnoRole.AddMustNot(hasRole)\n\toneRolesOrNoRoles := NewBooleanQuery()\n\toneRolesOrNoRoles.AddShould(noRole)\n\toneRolesOrNoRoles.SetMinShould(1)\n\tq := NewConjunctionQuery(tq, oneRolesOrNoRoles)\n\n\tsr := NewSearchRequestOptions(q, 100, 0, false)\n\tsr.Fields = []string{\"hasRole\"}\n\tsr.Highlight = NewHighlight()\n\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 0 {\n\t\tt.Fatalf(\"Unexpected result, %v != 0\", res.Total)\n\t}\n}\n\nfunc TestSearchScorchOverEmptyKeyword(t *testing.T) {\n\tdefaultIndexType := Config.DefaultIndexType\n\tConfig.DefaultIndexType = scorch.Name\n\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.DefaultAnalyzer = standard.Name\n\n\tfm := mapping.NewTextFieldMapping()\n\tfm.Analyzer = keyword.Name\n\n\tfm1 := mapping.NewTextFieldMapping()\n\tfm1.Analyzer = standard.Name\n\n\tdmap.AddFieldMappingsAt(\"id\", fm)\n\tdmap.AddFieldMappingsAt(\"name\", fm1)\n\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping = dmap\n\timap.DefaultAnalyzer = standard.Name\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tConfig.DefaultIndexType = defaultIndexType\n\t}()\n\n\tfor i := 0; i < 10; i++ {\n\t\terr = idx.Index(fmt.Sprint(i), map[string]string{\"name\": fmt.Sprintf(\"test%d\", i), \"id\": \"\"})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tcount, err := idx.DocCount()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif count != 10 {\n\t\tt.Fatalf(\"Unexpected doc count: %v, expected 10\", count)\n\t}\n\n\tq := query.NewWildcardQuery(\"test*\")\n\tsr := NewSearchRequestOptions(q, 40, 0, false)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 10 {\n\t\tt.Fatalf(\"Unexpected search hits: %v, expected 10\", res.Total)\n\t}\n}\n\nfunc TestMultipleNestedBooleanMustNotSearchersOnScorch(t *testing.T) {\n\tdefaultIndexType := Config.DefaultIndexType\n\tConfig.DefaultIndexType = scorch.Name\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// create an index with default settings\n\tidxMapping := NewIndexMapping()\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tConfig.DefaultIndexType = defaultIndexType\n\t}()\n\n\t// create and insert documents as a batch\n\tbatch := idx.NewBatch()\n\n\tdoc := document.NewDocument(\"1-child-0\")\n\tdoc.Fields = []document.Field{\n\t\tdocument.NewTextField(\"id\", []uint64{}, []byte(\"1-child-0\")),\n\t\tdocument.NewBooleanField(\"hasRole\", []uint64{}, false),\n\t\tdocument.NewTextField(\"roles\", []uint64{}, []byte(\"R1\")),\n\t\tdocument.NewNumericField(\"type\", []uint64{}, 0),\n\t}\n\tdoc.CompositeFields = []*document.CompositeField{\n\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t}\n\n\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdocs := []struct {\n\t\tid      string\n\t\thasRole bool\n\t\ttyp     int\n\t}{\n\t\t{\n\t\t\tid:      \"16d6fa37-48fd-4dea-8b3d-a52bddf73951\",\n\t\t\thasRole: false,\n\t\t\ttyp:     9,\n\t\t},\n\t\t{\n\t\t\tid:      \"18fa9eb2-8b1f-46f0-8b56-b4c551213f78\",\n\t\t\thasRole: false,\n\t\t\ttyp:     9,\n\t\t},\n\t\t{\n\t\t\tid:      \"3085855b-d74b-474a-86c3-9bf3e4504382\",\n\t\t\thasRole: false,\n\t\t\ttyp:     9,\n\t\t},\n\t\t{\n\t\t\tid:      \"38ef5d28-0f85-4fb0-8a94-dd20751c3364\",\n\t\t\thasRole: false,\n\t\t\ttyp:     9,\n\t\t},\n\t}\n\n\tfor i := 0; i < len(docs); i++ {\n\t\tdoc := document.NewDocument(docs[i].id)\n\t\tdoc.Fields = []document.Field{\n\t\t\tdocument.NewTextField(\"id\", []uint64{}, []byte(docs[i].id)),\n\t\t\tdocument.NewBooleanField(\"hasRole\", []uint64{}, docs[i].hasRole),\n\t\t\tdocument.NewNumericField(\"type\", []uint64{}, float64(docs[i].typ)),\n\t\t}\n\n\t\tdoc.CompositeFields = []*document.CompositeField{\n\t\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t\t}\n\n\t\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tbatch = idx.NewBatch()\n\n\t// Update 1st doc\n\tdoc = document.NewDocument(\"1-child-0\")\n\tdoc.Fields = []document.Field{\n\t\tdocument.NewTextField(\"id\", []uint64{}, []byte(\"1-child-0\")),\n\t\tdocument.NewBooleanField(\"hasRole\", []uint64{}, false),\n\t\tdocument.NewNumericField(\"type\", []uint64{}, 0),\n\t}\n\tdoc.CompositeFields = []*document.CompositeField{\n\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t}\n\n\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tinclusive := true\n\tval := float64(9)\n\tq := query.NewNumericRangeInclusiveQuery(&val, &val, &inclusive, &inclusive)\n\tq.SetField(\"type\")\n\tinitialQuery := query.NewBooleanQuery(nil, nil, []query.Query{q})\n\n\t// using must not, for cases that the field did not exists at all\n\thasRole := NewBoolFieldQuery(true)\n\thasRole.SetField(\"hasRole\")\n\tnoRole := NewBooleanQuery()\n\tnoRole.AddMustNot(hasRole)\n\n\trq := query.NewBooleanQuery([]query.Query{initialQuery, noRole}, nil, nil)\n\n\tsr := NewSearchRequestOptions(rq, 100, 0, false)\n\tsr.Fields = []string{\"id\", \"hasRole\", \"type\"}\n\tsr.Highlight = NewHighlight()\n\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total != 1 {\n\t\tt.Fatalf(\"Unexpected result, %v != 1\", res.Total)\n\t}\n}\n\nfunc testBooleanMustNotSearcher(t *testing.T, indexName string) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tim := NewIndexMapping()\n\tidx, err := NewUsing(tmpIndexPath, im, indexName, Config.DefaultKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdocs := []struct {\n\t\tName    string\n\t\tHasRole bool\n\t}{\n\t\t{\n\t\t\tName: \"13900\",\n\t\t},\n\t\t{\n\t\t\tName: \"13901\",\n\t\t},\n\t\t{\n\t\t\tName: \"13965\",\n\t\t},\n\t\t{\n\t\t\tName:    \"13966\",\n\t\t\tHasRole: true,\n\t\t},\n\t\t{\n\t\t\tName:    \"13967\",\n\t\t\tHasRole: true,\n\t\t},\n\t}\n\n\tfor _, doc := range docs {\n\t\terr := idx.Index(doc.Name, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tlhs := NewDocIDQuery([]string{\"13965\", \"13966\", \"13967\"})\n\thasRole := NewBoolFieldQuery(true)\n\thasRole.SetField(\"HasRole\")\n\trhs := NewBooleanQuery()\n\trhs.AddMustNot(hasRole)\n\n\tvar compareLeftRightAndConjunction = func(idx Index, left, right query.Query) error {\n\t\t// left\n\t\tlr := NewSearchRequestOptions(left, 100, 0, false)\n\t\tlres, err := idx.Search(lr)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error left: %v\", err)\n\t\t}\n\t\tlresIds := map[string]struct{}{}\n\t\tfor i := range lres.Hits {\n\t\t\tlresIds[lres.Hits[i].ID] = struct{}{}\n\t\t}\n\t\t// right\n\t\trr := NewSearchRequestOptions(right, 100, 0, false)\n\t\trres, err := idx.Search(rr)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error right: %v\", err)\n\t\t}\n\t\trresIds := map[string]struct{}{}\n\t\tfor i := range rres.Hits {\n\t\t\trresIds[rres.Hits[i].ID] = struct{}{}\n\t\t}\n\t\t// conjunction\n\t\tcr := NewSearchRequestOptions(NewConjunctionQuery(left, right), 100, 0, false)\n\t\tcres, err := idx.Search(cr)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error conjunction: %v\", err)\n\t\t}\n\t\tfor i := range cres.Hits {\n\t\t\tif _, ok := lresIds[cres.Hits[i].ID]; ok {\n\t\t\t\tif _, ok := rresIds[cres.Hits[i].ID]; !ok {\n\t\t\t\t\treturn fmt.Errorf(\"error id %s missing from right\", cres.Hits[i].ID)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn fmt.Errorf(\"error id %s missing from left\", cres.Hits[i].ID)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\terr = compareLeftRightAndConjunction(idx, lhs, rhs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestBooleanMustNotSearcherUpsidedown(t *testing.T) {\n\ttestBooleanMustNotSearcher(t, upsidedown.Name)\n}\n\nfunc TestBooleanMustNotSearcherScorch(t *testing.T) {\n\ttestBooleanMustNotSearcher(t, scorch.Name)\n}\n\nfunc TestQueryStringEmptyConjunctionSearcher(t *testing.T) {\n\tmapping := NewIndexMapping()\n\tmapping.DefaultAnalyzer = keyword.Name\n\tindex, err := NewMemOnly(mapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\t_ = index.Close()\n\t}()\n\n\tquery := NewQueryStringQuery(\"foo:bar +baz:\\\"\\\"\")\n\tsearchReq := NewSearchRequest(query)\n\n\t_, _ = index.Search(searchReq)\n}\n\nfunc TestDisjunctionQueryIncorrectMin(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\t// create an index with default settings\n\tidxMapping := NewIndexMapping()\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// create and insert documents as a batch\n\tbatch := idx.NewBatch()\n\tdocs := []struct {\n\t\tfield1 string\n\t\tfield2 int\n\t}{\n\t\t{\n\t\t\tfield1: \"one\",\n\t\t\tfield2: 1,\n\t\t},\n\t\t{\n\t\t\tfield1: \"two\",\n\t\t\tfield2: 2,\n\t\t},\n\t}\n\n\tfor i := 0; i < len(docs); i++ {\n\t\tdoc := document.NewDocument(strconv.Itoa(docs[i].field2))\n\t\tdoc.Fields = []document.Field{\n\t\t\tdocument.NewTextField(\"field1\", []uint64{}, []byte(docs[i].field1)),\n\t\t\tdocument.NewNumericField(\"field2\", []uint64{}, float64(docs[i].field2)),\n\t\t}\n\t\tdoc.CompositeFields = []*document.CompositeField{\n\t\t\tdocument.NewCompositeFieldWithIndexingOptions(\n\t\t\t\t\"_all\", true, []string{\"text\"}, []string{},\n\t\t\t\tindex.IndexField|index.IncludeTermVectors),\n\t\t}\n\t\tif err = batch.IndexAdvanced(doc); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttq := NewTermQuery(\"one\")\n\tdq := NewDisjunctionQuery(tq)\n\tdq.SetMin(2)\n\tsr := NewSearchRequestOptions(dq, 1, 0, false)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total > 0 {\n\t\tt.Fatalf(\"Expected 0 matches as disjunction query contains a single clause\"+\n\t\t\t\" but got: %v\", res.Total)\n\t}\n}\n\nfunc TestMatchQueryPartialMatch(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\tdoc1 := map[string]interface{}{\n\t\t\"description\": \"Patrick is first name Stewart is last name\",\n\t}\n\tdoc2 := map[string]interface{}{\n\t\t\"description\": \"Manager given name is Patrick\",\n\t}\n\tbatch := idx.NewBatch()\n\tif err = batch.Index(\"doc1\", doc1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = batch.Index(\"doc2\", doc2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Test 1 - Both Docs hit, doc 1 = Full Match and doc 2 = Partial Match\n\tmq1 := NewMatchQuery(\"patrick stewart\")\n\tmq1.SetField(\"description\")\n\n\tsr := NewSearchRequest(mq1)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 2 {\n\t\tt.Errorf(\"Expected 2 results, but got: %v\", res.Total)\n\t}\n\tfor _, hit := range res.Hits {\n\t\tif hit.ID == \"doc1\" && hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc1 to be a full match\")\n\t\t}\n\t\tif hit.ID == \"doc2\" && !hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc2 to be a partial match\")\n\t\t}\n\t}\n\n\t// Test 2 - Both Docs hit, doc 1 = Partial Match and doc 2 = Full Match\n\tmq2 := NewMatchQuery(\"paltric manner\")\n\tmq2.SetField(\"description\")\n\tmq2.SetFuzziness(2)\n\n\tsr = NewSearchRequest(mq2)\n\tres, err = idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 2 {\n\t\tt.Errorf(\"Expected 2 results, but got: %v\", res.Total)\n\t}\n\tfor _, hit := range res.Hits {\n\t\tif hit.ID == \"doc1\" && !hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc1 to be a partial match\")\n\t\t}\n\t\tif hit.ID == \"doc2\" && hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc2 to be a full match\")\n\t\t}\n\t}\n\t// Test 3 - Two Docs hits, both full match\n\tmq3 := NewMatchQuery(\"patrick\")\n\tmq3.SetField(\"description\")\n\n\tsr = NewSearchRequest(mq3)\n\tres, err = idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 2 {\n\t\tt.Errorf(\"Expected 2 results, but got: %v\", res.Total)\n\t}\n\tfor _, hit := range res.Hits {\n\t\tif hit.ID == \"doc1\" && hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc1 to be a full match\")\n\t\t}\n\t\tif hit.ID == \"doc2\" && hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc2 to be a full match\")\n\t\t}\n\t}\n\t// Test 4 - Two Docs hits, both partial match\n\tmq4 := NewMatchQuery(\"patrick stewart manager\")\n\tmq4.SetField(\"description\")\n\n\tsr = NewSearchRequest(mq4)\n\tres, err = idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 2 {\n\t\tt.Errorf(\"Expected 2 results, but got: %v\", res.Total)\n\t}\n\tfor _, hit := range res.Hits {\n\t\tif hit.ID == \"doc1\" && !hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc1 to be a partial match\")\n\t\t}\n\t\tif hit.ID == \"doc2\" && !hit.PartialMatch {\n\t\t\tt.Errorf(\"Expected doc2 to be a partial match\")\n\t\t}\n\t}\n\n\t// Test 5 - Match Query AND operator always results in full match\n\tmq5 := NewMatchQuery(\"patrick stewart\")\n\tmq5.SetField(\"description\")\n\tmq5.SetOperator(1)\n\n\tsr = NewSearchRequest(mq5)\n\tres, err = idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif res.Total != 1 {\n\t\tt.Errorf(\"Expected 1 result, but got: %v\", res.Total)\n\t}\n\tif res.Hits[0].ID == \"doc2\" || res.Hits[0].PartialMatch {\n\t\tt.Errorf(\"Expected doc1 to be a full match\")\n\t}\n}\n\nfunc TestBooleanShouldMinPropagation(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc1 := map[string]interface{}{\n\t\t\"dept\": \"queen\",\n\t\t\"name\": \"cersei lannister\",\n\t}\n\n\tdoc2 := map[string]interface{}{\n\t\t\"dept\": \"kings guard\",\n\t\t\"name\": \"jaime lannister\",\n\t}\n\n\tbatch := idx.NewBatch()\n\n\tif err = batch.Index(\"doc1\", doc1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = batch.Index(\"doc2\", doc2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// term dictionaries in the index for field..\n\t//  dept: queen kings guard\n\t//  name: cersei jaime lannister\n\n\t// the following match query would match doc2\n\tmq1 := NewMatchQuery(\"kings guard\")\n\tmq1.SetField(\"dept\")\n\n\t// the following match query would match both doc1 and doc2,\n\t// as both docs share common lastname\n\tmq2 := NewMatchQuery(\"jaime lannister\")\n\tmq2.SetField(\"name\")\n\n\tbq := NewBooleanQuery()\n\tbq.AddShould(mq1)\n\tbq.AddMust(mq2)\n\n\tsr := NewSearchRequest(bq)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total != 2 {\n\t\tt.Errorf(\"Expected 2 results, but got: %v\", res.Total)\n\t}\n}\n\nfunc TestDisjunctionMinPropagation(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, NewIndexMapping())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc1 := map[string]interface{}{\n\t\t\"dept\": \"finance\",\n\t\t\"name\": \"xyz\",\n\t}\n\n\tdoc2 := map[string]interface{}{\n\t\t\"dept\": \"marketing\",\n\t\t\"name\": \"xyz\",\n\t}\n\n\tdoc3 := map[string]interface{}{\n\t\t\"dept\": \"engineering\",\n\t\t\"name\": \"abc\",\n\t}\n\n\tbatch := idx.NewBatch()\n\n\tif err = batch.Index(\"doc1\", doc1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = batch.Index(\"doc2\", doc2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = batch.Index(\"doc3\", doc3); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmq1 := NewMatchQuery(\"finance\")\n\tmq2 := NewMatchQuery(\"marketing\")\n\tdq := NewDisjunctionQuery(mq1, mq2)\n\tdq.SetMin(3)\n\n\tdq2 := NewDisjunctionQuery(dq)\n\tdq2.SetMin(1)\n\n\tsr := NewSearchRequest(dq2)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total != 0 {\n\t\tt.Fatalf(\"Expect 0 results, but got: %v\", res.Total)\n\t}\n}\n\nfunc TestDuplicateLocationsIssue1168(t *testing.T) {\n\tfm1 := NewTextFieldMapping()\n\tfm1.Analyzer = keyword.Name\n\tfm1.Name = \"name1\"\n\n\tdm := NewDocumentStaticMapping()\n\tdm.AddFieldMappingsAt(\"name\", fm1)\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = dm\n\n\tidx, err := NewMemOnly(m)\n\tif err != nil {\n\t\tt.Fatalf(\"bleve new err: %v\", err)\n\t}\n\n\terr = idx.Index(\"x\", map[string]interface{}{\n\t\t\"name\": \"marty\",\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"bleve index err: %v\", err)\n\t}\n\n\tq1 := NewTermQuery(\"marty\")\n\tq2 := NewTermQuery(\"marty\")\n\tdq := NewDisjunctionQuery(q1, q2)\n\n\tsreq := NewSearchRequest(dq)\n\tsreq.Fields = []string{\"*\"}\n\tsreq.Highlight = NewHighlightWithStyle(html.Name)\n\n\tsres, err := idx.Search(sreq)\n\tif err != nil {\n\t\tt.Fatalf(\"bleve search err: %v\", err)\n\t}\n\tif len(sres.Hits[0].Locations[\"name1\"][\"marty\"]) != 1 {\n\t\tt.Fatalf(\"duplicate marty\")\n\t}\n}\n\nfunc TestBooleanMustSingleMatchNone(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\tif err := idxMapping.AddCustomTokenFilter(length.Name, map[string]interface{}{\n\t\t\"min\":  3.0,\n\t\t\"max\":  5.0,\n\t\t\"type\": length.Name,\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := idxMapping.AddCustomAnalyzer(\"custom1\", map[string]interface{}{\n\t\t\"type\":          \"custom\",\n\t\t\"tokenizer\":     \"single\",\n\t\t\"token_filters\": []interface{}{length.Name},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidxMapping.DefaultAnalyzer = \"custom1\"\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"languages_known\": \"Dutch\",\n\t\t\"dept\":            \"Sales\",\n\t}\n\n\tbatch := idx.NewBatch()\n\tif err = batch.Index(\"doc\", doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// this is a successful match\n\tmatchSales := NewMatchQuery(\"Sales\")\n\tmatchSales.SetField(\"dept\")\n\n\t// this would spin off a MatchNoneSearcher as the\n\t// token filter rules out the word \"French\"\n\tmatchFrench := NewMatchQuery(\"French\")\n\tmatchFrench.SetField(\"languages_known\")\n\n\tbq := NewBooleanQuery()\n\tbq.AddShould(matchSales)\n\tbq.AddMust(matchFrench)\n\n\tsr := NewSearchRequest(bq)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total != 0 {\n\t\tt.Fatalf(\"Expected 0 results but got: %v\", res.Total)\n\t}\n}\n\nfunc TestBooleanMustNotSingleMatchNone(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\tif err := idxMapping.AddCustomTokenFilter(shingle.Name, map[string]interface{}{\n\t\t\"min\":  3.0,\n\t\t\"max\":  5.0,\n\t\t\"type\": shingle.Name,\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := idxMapping.AddCustomAnalyzer(\"custom1\", map[string]interface{}{\n\t\t\"type\":          \"custom\",\n\t\t\"tokenizer\":     \"unicode\",\n\t\t\"token_filters\": []interface{}{shingle.Name},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidxMapping.DefaultAnalyzer = \"custom1\"\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"languages_known\": \"Dutch\",\n\t\t\"dept\":            \"Sales\",\n\t}\n\n\tbatch := idx.NewBatch()\n\tif err = batch.Index(\"doc\", doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// this is a successful match\n\tmatchSales := NewMatchQuery(\"Sales\")\n\tmatchSales.SetField(\"dept\")\n\n\t// this would spin off a MatchNoneSearcher as the\n\t// token filter rules out the word \"Dutch\"\n\tmatchDutch := NewMatchQuery(\"Dutch\")\n\tmatchDutch.SetField(\"languages_known\")\n\n\tmatchEngineering := NewMatchQuery(\"Engineering\")\n\tmatchEngineering.SetField(\"dept\")\n\n\tbq := NewBooleanQuery()\n\tbq.AddShould(matchSales)\n\tbq.AddMustNot(matchDutch, matchEngineering)\n\n\tsr := NewSearchRequest(bq)\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif res.Total != 0 {\n\t\tt.Fatalf(\"Expected 0 results but got: %v\", res.Total)\n\t}\n}\n\nfunc TestBooleanSearchBug1185(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tof := NewTextFieldMapping()\n\tof.Analyzer = keyword.Name\n\tof.Name = \"owner\"\n\n\tdm := NewDocumentMapping()\n\tdm.AddFieldMappingsAt(\"owner\", of)\n\n\tm := NewIndexMapping()\n\tm.DefaultMapping = dm\n\n\tidx, err := NewUsing(tmpIndexPath, m, \"scorch\", \"scorch\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = idx.Index(\"17112\", map[string]interface{}{\n\t\t\"owner\": \"marty\",\n\t\t\"type\":  \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17139\", map[string]interface{}{\n\t\t\"type\": \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"177777\", map[string]interface{}{\n\t\t\"type\": \"x\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = idx.Index(\"177778\", map[string]interface{}{\n\t\t\"type\": \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17140\", map[string]interface{}{\n\t\t\"type\": \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17000\", map[string]interface{}{\n\t\t\"owner\": \"marty\",\n\t\t\"type\":  \"x\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17141\", map[string]interface{}{\n\t\t\"type\": \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17428\", map[string]interface{}{\n\t\t\"owner\": \"marty\",\n\t\t\"type\":  \"A Demo Type\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idx.Index(\"17113\", map[string]interface{}{\n\t\t\"owner\": \"marty\",\n\t\t\"type\":  \"x\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmatchTypeQ := NewMatchPhraseQuery(\"A Demo Type\")\n\tmatchTypeQ.SetField(\"type\")\n\n\tmatchAnyOwnerRegQ := NewRegexpQuery(\".+\")\n\tmatchAnyOwnerRegQ.SetField(\"owner\")\n\n\tmatchNoOwner := NewBooleanQuery()\n\tmatchNoOwner.AddMustNot(matchAnyOwnerRegQ)\n\n\tnotNoOwner := NewBooleanQuery()\n\tnotNoOwner.AddMustNot(matchNoOwner)\n\n\tmatchTypeAndNoOwner := NewConjunctionQuery()\n\tmatchTypeAndNoOwner.AddQuery(matchTypeQ)\n\tmatchTypeAndNoOwner.AddQuery(notNoOwner)\n\n\treq := NewSearchRequest(matchTypeAndNoOwner)\n\tres, err := idx.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// query 2\n\tmatchTypeAndNoOwnerBoolean := NewBooleanQuery()\n\tmatchTypeAndNoOwnerBoolean.AddMust(matchTypeQ)\n\tmatchTypeAndNoOwnerBoolean.AddMustNot(matchNoOwner)\n\n\treq2 := NewSearchRequest(matchTypeAndNoOwnerBoolean)\n\tres2, err := idx.Search(req2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(res.Hits) != len(res2.Hits) {\n\t\tt.Fatalf(\"expected same number of hits, got: %d and %d\", len(res.Hits), len(res2.Hits))\n\t}\n}\n\nfunc TestSearchScoreNone(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := NewUsing(tmpIndexPath, NewIndexMapping(), scorch.Name, Config.DefaultKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"field1\": \"asd fgh jkl\",\n\t\t\"field2\": \"more content blah blah\",\n\t\t\"id\":     \"doc\",\n\t}\n\n\tif err = idx.Index(\"doc\", doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tq := NewQueryStringQuery(\"content\")\n\tsr := NewSearchRequest(q)\n\tsr.IncludeLocations = true\n\tsr.Score = \"none\"\n\n\tres, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(res.Hits) != 1 {\n\t\tt.Fatal(\"unexpected number of hits\")\n\t}\n\n\tif len(res.Hits[0].Locations) != 1 {\n\t\tt.Fatal(\"unexpected locations for the hit\")\n\t}\n\n\tif res.Hits[0].Score != 0 {\n\t\tt.Fatal(\"unexpected score for the hit\")\n\t}\n}\n\nfunc TestGeoDistanceIssue1301(t *testing.T) {\n\tshopMapping := NewDocumentMapping()\n\tshopMapping.AddFieldMappingsAt(\"GEO\", NewGeoPointFieldMapping())\n\tshopIndexMapping := NewIndexMapping()\n\tshopIndexMapping.DefaultMapping = shopMapping\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := NewUsing(tmpIndexPath, shopIndexMapping, scorch.Name, Config.DefaultKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tfor i, g := range []string{\"wecpkbeddsmf\", \"wecpk8tne453\", \"wecpkb80s09t\"} {\n\t\tif err = idx.Index(strconv.Itoa(i), map[string]interface{}{\n\t\t\t\"ID\":  i,\n\t\t\t\"GEO\": g,\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// Not setting \"Field\" for the following query, targets it against the _all\n\t// field and this is returning inconsistent results, when there's another\n\t// field indexed along with the geopoint which is numeric.\n\t// As reported in: https://github.com/blevesearch/bleve/issues/1301\n\tlat, lon := 22.371154, 114.112603\n\tq := NewGeoDistanceQuery(lon, lat, \"1km\")\n\n\treq := NewSearchRequest(q)\n\tsr, err := idx.Search(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif sr.Total != 3 {\n\t\tt.Fatalf(\"Size expected: 3, actual %d\\n\", sr.Total)\n\t}\n}\n\nfunc TestSearchHighlightingWithRegexpReplacement(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\tif err := idxMapping.AddCustomCharFilter(regexp_char_filter.Name, map[string]interface{}{\n\t\t\"regexp\":  `([a-z])\\s+(\\d)`,\n\t\t\"replace\": \"ooooo$1-$2\",\n\t\t\"type\":    regexp_char_filter.Name,\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := idxMapping.AddCustomAnalyzer(\"regexp_replace\", map[string]interface{}{\n\t\t\"type\":      custom.Name,\n\t\t\"tokenizer\": \"unicode\",\n\t\t\"char_filters\": []string{\n\t\t\tregexp_char_filter.Name,\n\t\t},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidxMapping.DefaultAnalyzer = \"regexp_replace\"\n\tidxMapping.StoreDynamic = true\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := NewUsing(tmpIndexPath, idxMapping, scorch.Name, Config.DefaultKVStore, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr := idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"status\": \"fool 10\",\n\t}\n\n\tbatch := idx.NewBatch()\n\tif err = batch.Index(\"doc\", doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = idx.Batch(batch); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tquery := NewMatchQuery(\"fool 10\")\n\tsreq := NewSearchRequest(query)\n\tsreq.Fields = []string{\"*\"}\n\tsreq.Highlight = NewHighlightWithStyle(ansi.Name)\n\n\tsres, err := idx.Search(sreq)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif sres.Total != 1 {\n\t\tt.Fatalf(\"Expected 1 hit, got: %v\", sres.Total)\n\t}\n}\n\nfunc TestAnalyzerInheritance(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tmappingStr string\n\t\tdoc        map[string]interface{}\n\t\tqueryField string\n\t\tqueryTerm  string\n\t}{\n\t\t{\n\t\t\t/*\n\t\t\t\tindex_mapping: keyword\n\t\t\t\tdefault_mapping: \"\"\n\t\t\t\t\t-> child field (should inherit keyword)\n\t\t\t*/\n\t\t\tname: \"Child field to inherit index mapping's default analyzer\",\n\t\t\tmappingStr: `{\"default_mapping\":{\"enabled\":true,\"dynamic\":false,\"properties\":` +\n\t\t\t\t`{\"city\":{\"enabled\":true,\"dynamic\":false,\"fields\":[{\"name\":\"city\",\"type\":\"text\",` +\n\t\t\t\t`\"store\":false,\"index\":true}]}}},\"default_analyzer\":\"keyword\"}`,\n\t\t\tdoc:        map[string]interface{}{\"city\": \"San Francisco\"},\n\t\t\tqueryField: \"city\",\n\t\t\tqueryTerm:  \"San Francisco\",\n\t\t},\n\t\t{\n\t\t\t/*\n\t\t\t\tindex_mapping: standard\n\t\t\t\tdefault_mapping: keyword\n\t\t\t\t    -> child field (should inherit keyword)\n\t\t\t*/\n\t\t\tname: \"Child field to inherit default mapping's default analyzer\",\n\t\t\tmappingStr: `{\"default_mapping\":{\"enabled\":true,\"dynamic\":false,\"properties\":` +\n\t\t\t\t`{\"city\":{\"enabled\":true,\"dynamic\":false,\"fields\":[{\"name\":\"city\",\"type\":\"text\",` +\n\t\t\t\t`\"index\":true}]}},\"default_analyzer\":\"keyword\"},\"default_analyzer\":\"standard\"}`,\n\t\t\tdoc:        map[string]interface{}{\"city\": \"San Francisco\"},\n\t\t\tqueryField: \"city\",\n\t\t\tqueryTerm:  \"San Francisco\",\n\t\t},\n\t\t{\n\t\t\t/*\n\t\t\t\tindex_mapping: standard\n\t\t\t\tdefault_mapping: keyword (dynamic)\n\t\t\t\t    -> search over field to (should inherit keyword)\n\t\t\t*/\n\t\t\tname: \"Child field to inherit default mapping's default analyzer\",\n\t\t\tmappingStr: `{\"default_mapping\":{\"enabled\":true,\"dynamic\":true,\"default_analyzer\":\"keyword\"}` +\n\t\t\t\t`,\"default_analyzer\":\"standard\"}`,\n\t\t\tdoc:        map[string]interface{}{\"city\": \"San Francisco\"},\n\t\t\tqueryField: \"city\",\n\t\t\tqueryTerm:  \"San Francisco\",\n\t\t},\n\t\t{\n\t\t\t/*\n\t\t\t\tindex_mapping: standard\n\t\t\t\tdefault_mapping: keyword\n\t\t\t\t    -> child mapping: \"\"\n\t\t\t\t\t    -> child field: (should inherit keyword)\n\t\t\t*/\n\t\t\tname: \"Nested child field to inherit default mapping's default analyzer\",\n\t\t\tmappingStr: `{\"default_mapping\":{\"enabled\":true,\"dynamic\":false,\"default_analyzer\":` +\n\t\t\t\t`\"keyword\",\"properties\":{\"address\":{\"enabled\":true,\"dynamic\":false,\"properties\":` +\n\t\t\t\t`{\"city\":{\"enabled\":true,\"dynamic\":false,\"fields\":[{\"name\":\"city\",\"type\":\"text\",` +\n\t\t\t\t`\"index\":true}]}}}}},\"default_analyzer\":\"standard\"}`,\n\t\t\tdoc: map[string]interface{}{\n\t\t\t\t\"address\": map[string]interface{}{\"city\": \"San Francisco\"},\n\t\t\t},\n\t\t\tqueryField: \"address.city\",\n\t\t\tqueryTerm:  \"San Francisco\",\n\t\t},\n\t\t{\n\t\t\t/*\n\t\t\t\tindex_mapping: standard\n\t\t\t\tdefault_mapping: \"\"\n\t\t\t\t    -> child mapping: \"keyword\"\n\t\t\t\t\t    -> child mapping: \"\"\n\t\t\t\t\t\t    -> child field: (should inherit keyword)\n\t\t\t*/\n\t\t\tname: \"Nested child field to inherit first child mapping's default analyzer\",\n\t\t\tmappingStr: `{\"default_mapping\":{\"enabled\":true,\"dynamic\":false,\"properties\":` +\n\t\t\t\t`{\"address\":{\"enabled\":true,\"dynamic\":false,\"default_analyzer\":\"keyword\",` +\n\t\t\t\t`\"properties\":{\"state\":{\"enabled\":true,\"dynamic\":false,\"properties\":{\"city\":` +\n\t\t\t\t`{\"enabled\":true,\"dynamic\":false,\"fields\":[{\"name\":\"city\",\"type\":\"text\",` +\n\t\t\t\t`\"store\":false,\"index\":true}]}}}}}}},\"default_analyer\":\"standard\"}`,\n\t\t\tdoc: map[string]interface{}{\n\t\t\t\t\"address\": map[string]interface{}{\n\t\t\t\t\t\"state\": map[string]interface{}{\"city\": \"San Francisco\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqueryField: \"address.state.city\",\n\t\t\tqueryTerm:  \"San Francisco\",\n\t\t},\n\t}\n\n\tfor i := range tests {\n\t\tt.Run(fmt.Sprintf(\"%s\", tests[i].name), func(t *testing.T) {\n\t\t\tidxMapping := NewIndexMapping()\n\t\t\tif err := idxMapping.UnmarshalJSON([]byte(tests[i].mappingStr)); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\ttmpIndexPath := createTmpIndexPath(t)\n\t\t\tidx, err := New(tmpIndexPath, idxMapping)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tdefer func() {\n\t\t\t\tif err := idx.Close(); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tif err = idx.Index(\"doc\", tests[i].doc); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tq := NewTermQuery(tests[i].queryTerm)\n\t\t\tq.SetField(tests[i].queryField)\n\n\t\t\tres, err := idx.Search(NewSearchRequest(q))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif len(res.Hits) != 1 {\n\t\t\t\tt.Errorf(\"Unexpected number of hits: %v\", len(res.Hits))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestHightlightingWithHTMLCharacterFilter(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\tif err := idxMapping.AddCustomAnalyzer(\"custom-html\", map[string]interface{}{\n\t\t\"type\":         custom.Name,\n\t\t\"tokenizer\":    \"unicode\",\n\t\t\"char_filters\": []interface{}{html_char_filter.Name},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfm := mapping.NewTextFieldMapping()\n\tfm.Analyzer = \"custom-html\"\n\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"content\", fm)\n\n\tidxMapping.DefaultMapping = dmap\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcontent := \"<div> Welcome to blevesearch. </div>\"\n\tif err = idx.Index(\"doc\", map[string]string{\n\t\t\"content\": content,\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsearchStr := \"blevesearch\"\n\tq := query.NewMatchQuery(searchStr)\n\tq.SetField(\"content\")\n\tsr := NewSearchRequest(q)\n\tsr.IncludeLocations = true\n\tsr.Fields = []string{\"*\"}\n\tsr.Highlight = NewHighlightWithStyle(html.Name)\n\tsearchResults, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(searchResults.Hits) != 1 ||\n\t\tlen(searchResults.Hits[0].Locations[\"content\"][searchStr]) != 1 {\n\t\tt.Fatalf(\"Expected 1 hit with 1 location\")\n\t}\n\n\texpectedLocation := &search.Location{\n\t\tPos:   3,\n\t\tStart: uint64(strings.Index(content, searchStr)),\n\t\tEnd:   uint64(strings.Index(content, searchStr) + len(searchStr)),\n\t}\n\texpectedFragment := \"&lt;div&gt; Welcome to <mark>blevesearch</mark>. &lt;/div&gt;\"\n\n\tgotLocation := searchResults.Hits[0].Locations[\"content\"][\"blevesearch\"][0]\n\tgotFragment := searchResults.Hits[0].Fragments[\"content\"][0]\n\n\tif !reflect.DeepEqual(expectedLocation, gotLocation) {\n\t\tt.Fatalf(\"Mismatch in locations, got: %v, expected: %v\",\n\t\t\tgotLocation, expectedLocation)\n\t}\n\n\tif expectedFragment != gotFragment {\n\t\tt.Fatalf(\"Mismatch in fragment, got: %v, expected: %v\",\n\t\t\tgotFragment, expectedFragment)\n\t}\n}\n\nfunc TestIPRangeQuery(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\tim := NewIPFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"ip_content\", im)\n\tidxMapping.DefaultMapping = dmap\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tipContent := \"192.168.10.11\"\n\tif err = idx.Index(\"doc\", map[string]string{\n\t\t\"ip_content\": ipContent,\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tq := query.NewIPRangeQuery(\"192.168.10.0/24\")\n\tq.SetField(\"ip_content\")\n\tsr := NewSearchRequest(q)\n\n\tsearchResults, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(searchResults.Hits) != 1 ||\n\t\tsearchResults.Hits[0].ID != \"doc\" {\n\t\tt.Fatal(\"Expected the 1 result - doc\")\n\t}\n}\n\nfunc TestGeoShapePolygonContainsPoint(t *testing.T) {\n\tfm := mapping.NewGeoShapeFieldMapping()\n\tdmap := mapping.NewDocumentMapping()\n\tdmap.AddFieldMappingsAt(\"geometry\", fm)\n\n\tidxMapping := NewIndexMapping()\n\tidxMapping.DefaultMapping = dmap\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Polygon coordinates to be ordered in counter-clock-wise order\n\t// for the outer loop, and holes to follow clock-wise order.\n\t// See: https://www.rfc-editor.org/rfc/rfc7946.html#section-3.1.6\n\n\tone := []byte(`{\n\t\t\"geometry\":{\n\t\t\t\"type\":\"Polygon\",\n\t\t\t\"coordinates\":[[\n\t\t\t\t[4.8089,46.9307],\n\t\t\t\t[4.8223,46.8915],\n\t\t\t\t[4.8149,46.886],\n\t\t\t\t[4.8252,46.8647],\n\t\t\t\t[4.8305,46.8531],\n\t\t\t\t[4.8506,46.8509],\n\t\t\t\t[4.8574,46.8621],\n\t\t\t\t[4.8576,46.8769],\n\t\t\t\t[4.8753,46.8774],\n\t\t\t\t[4.8909,46.8519],\n\t\t\t\t[4.8837,46.8485],\n\t\t\t\t[4.9014,46.8318],\n\t\t\t\t[4.9067,46.8179],\n\t\t\t\t[4.8986,46.8122],\n\t\t\t\t[4.9081,46.7969],\n\t\t\t\t[4.9535,46.8254],\n\t\t\t\t[4.9577,46.8053],\n\t\t\t\t[5.0201,46.821],\n\t\t\t\t[5.0357,46.8207],\n\t\t\t\t[5.0656,46.8434],\n\t\t\t\t[5.0955,46.8411],\n\t\t\t\t[5.1149,46.8435],\n\t\t\t\t[5.1259,46.8395],\n\t\t\t\t[5.1433,46.8463],\n\t\t\t\t[5.1415,46.8589],\n\t\t\t\t[5.1533,46.873],\n\t\t\t\t[5.138,46.8843],\n\t\t\t\t[5.1525,46.9012],\n\t\t\t\t[5.1485,46.9165],\n\t\t\t\t[5.1582,46.926],\n\t\t\t\t[5.1882,46.9251],\n\t\t\t\t[5.2039,46.9129],\n\t\t\t\t[5.2223,46.9175],\n\t\t\t\t[5.2168,46.926],\n\t\t\t\t[5.2338,46.9316],\n\t\t\t\t[5.228,46.9505],\n\t\t\t\t[5.2078,46.9722],\n\t\t\t\t[5.2117,46.98],\n\t\t\t\t[5.1961,46.9783],\n\t\t\t\t[5.1663,46.9638],\n\t\t\t\t[5.1213,46.9634],\n\t\t\t\t[5.1086,46.9596],\n\t\t\t\t[5.0729,46.9604],\n\t\t\t\t[5.0731,46.9668],\n\t\t\t\t[5.0493,46.9817],\n\t\t\t\t[5.0034,46.9722],\n\t\t\t\t[4.9852,46.9585],\n\t\t\t\t[4.9479,46.9664],\n\t\t\t\t[4.8943,46.9663],\n\t\t\t\t[4.8937,46.951],\n\t\t\t\t[4.8534,46.9458],\n\t\t\t\t[4.8089,46.9307]\n\t\t\t]]\n\t\t}\n\t}`)\n\n\ttwo := []byte(`{\n\t\t\"geometry\":{\n\t\t\t\"type\":\"Polygon\",\n\t\t\t\"coordinates\":[[\n\t\t\t\t[2.2266,48.7816],\n\t\t\t\t[2.2266,48.7761],\n\t\t\t\t[2.2288,48.7745],\n\t\t\t\t[2.2717,48.7905],\n\t\t\t\t[2.2799,48.8109],\n\t\t\t\t[2.3013,48.8251],\n\t\t\t\t[2.2894,48.8283],\n\t\t\t\t[2.2726,48.8144],\n\t\t\t\t[2.2518,48.8164],\n\t\t\t\t[2.255,48.8101],\n\t\t\t\t[2.2348,48.7954],\n\t\t\t\t[2.2266,48.7816]\n\t\t\t]]\n\t\t}\n\t}`)\n\n\tvar doc1, doc2 map[string]interface{}\n\n\tif err = json.Unmarshal(one, &doc1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = idx.Index(\"doc1\", doc1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = json.Unmarshal(two, &doc2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = idx.Index(\"doc2\", doc2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor testi, test := range []struct {\n\t\tcoordinates []float64\n\t\texpectHits  []string\n\t}{\n\t\t{\n\t\t\tcoordinates: []float64{5, 46.9},\n\t\t\texpectHits:  []string{\"doc1\"},\n\t\t},\n\t\t{\n\t\t\tcoordinates: []float64{1.5, 48.2},\n\t\t},\n\t} {\n\t\tq, err := NewGeoShapeQuery(\n\t\t\t[][][][]float64{{{test.coordinates}}},\n\t\t\tgeo.PointType,\n\t\t\t\"contains\",\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"test: %d, query err: %v\", testi+1, err)\n\t\t}\n\t\tq.SetField(\"geometry\")\n\n\t\tres, err := idx.Search(NewSearchRequest(q))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"test: %d, search err: %v\", testi+1, err)\n\t\t}\n\n\t\tif len(res.Hits) != len(test.expectHits) {\n\t\t\tt.Errorf(\"test: %d, unexpected hits: %v\", testi+1, len(res.Hits))\n\t\t}\n\n\tOUTER:\n\t\tfor _, expect := range test.expectHits {\n\t\t\tfor _, got := range res.Hits {\n\t\t\t\tif got.ID == expect {\n\t\t\t\t\tcontinue OUTER\n\t\t\t\t}\n\t\t\t}\n\t\t\tt.Errorf(\"test: %d, couldn't get: %v\", testi+1, expect)\n\t\t}\n\t}\n}\n\nfunc TestAnalyzerInheritanceForDefaultDynamicMapping(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping.DefaultAnalyzer = keyword.Name\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdoc := map[string]interface{}{\n\t\t\"fieldX\": \"AbCdEf\",\n\t}\n\n\tif err = idx.Index(\"doc\", doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Match query to apply keyword analyzer to fieldX.\n\tmq := NewMatchQuery(\"AbCdEf\")\n\tmq.SetField(\"fieldX\")\n\n\tsr := NewSearchRequest(mq)\n\tresults, err := idx.Search(sr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(results.Hits) != 1 {\n\t\tt.Fatalf(\"expected 1 hit, got %d\", len(results.Hits))\n\t}\n}\n\nfunc TestCustomDateTimeParserLayoutValidation(t *testing.T) {\n\tflexiblegoName := flexible.Name\n\tsanitizedgoName := sanitized.Name\n\timap := mapping.NewIndexMapping()\n\tcorrectConfig := map[string]interface{}{\n\t\t\"type\": sanitizedgoName,\n\t\t\"layouts\": []interface{}{\n\t\t\t// some custom layouts\n\t\t\t\"2006-01-02 15:04:05.0000\",\n\t\t\t\"2006\\\\01\\\\02T03:04:05PM\",\n\t\t\t\"2006/01/02\",\n\t\t\t\"2006-01-02T15:04:05.999Z0700PMMST\",\n\t\t\t\"15:04:05.0000Z07:00 Monday\",\n\n\t\t\t// standard layouts\n\t\t\ttime.Layout,\n\t\t\ttime.ANSIC,\n\t\t\ttime.UnixDate,\n\t\t\ttime.RubyDate,\n\t\t\ttime.RFC822,\n\t\t\ttime.RFC822Z,\n\t\t\ttime.RFC850,\n\t\t\ttime.RFC1123,\n\t\t\ttime.RFC1123Z,\n\t\t\ttime.RFC3339,\n\t\t\ttime.RFC3339Nano,\n\t\t\ttime.Kitchen,\n\t\t\ttime.Stamp,\n\t\t\ttime.StampMilli,\n\t\t\ttime.StampMicro,\n\t\t\ttime.StampNano,\n\t\t\t\"2006-01-02 15:04:05\", //time.DateTime\n\t\t\t\"2006-01-02\",          //time.DateOnly\n\t\t\t\"15:04:05\",            //time.TimeOnly\n\n\t\t\t// Corrected layouts to the incorrect ones below.\n\t\t\t\"2006-01-02 03:04:05 -0700\",\n\t\t\t\"2006-01-02 15:04:05 -0700\",\n\t\t\t\"3:04PM\",\n\t\t\t\"2006-01-02 15:04:05.000 -0700 MST\",\n\t\t\t\"January 2 2006 3:04 PM\",\n\t\t\t\"02/Jan/06 3:04PM\",\n\t\t\t\"Mon 02 Jan 3:04:05 PM\",\n\t\t},\n\t}\n\n\t// Correct layouts - sanitizedgo should work without errors.\n\terr := imap.AddCustomDateTimeParser(\"custDT\", correctConfig)\n\tif err != nil {\n\t\tt.Fatalf(\"expected no error, got: %v\", err)\n\t}\n\t// Flexiblego should work without errors as well.\n\tcorrectConfig[\"type\"] = flexiblegoName\n\terr = imap.AddCustomDateTimeParser(\"custDT_Flexi\", correctConfig)\n\tif err != nil {\n\t\tt.Fatalf(\"expected no error, got: %v\", err)\n\t}\n\n\tincorrectLayouts := [][]interface{}{\n\t\t{\n\t\t\t\"2000-03-31 01:33:51 +0300\",\n\t\t},\n\t\t{\n\t\t\t\"2006-01-02 15:04:51 +0300\",\n\t\t},\n\t\t{\n\t\t\t\"2000-03-31 01:33:05 +0300\",\n\t\t},\n\t\t{\n\t\t\t\"4:45PM\",\n\t\t},\n\t\t{\n\t\t\t\"2006-01-02 15:04:05.445 -0700 MST\",\n\t\t},\n\t\t{\n\t\t\t\"August 20 2001 8:55 AM\",\n\t\t},\n\t\t{\n\t\t\t\"28/Jul/23 12:48PM\",\n\t\t},\n\t\t{\n\t\t\t\"Tue 22 Aug 6:37:30 AM\",\n\t\t},\n\t}\n\n\t// first check sanitizedgo, should throw error for each of the incorrect layouts.\n\tnumExpectedErrors := len(incorrectLayouts)\n\tnumActualErrors := 0\n\tfor idx, badLayout := range incorrectLayouts {\n\t\tincorrectConfig := map[string]interface{}{\n\t\t\t\"type\":    sanitizedgoName,\n\t\t\t\"layouts\": badLayout,\n\t\t}\n\t\terr := imap.AddCustomDateTimeParser(fmt.Sprintf(\"%d_DT\", idx), incorrectConfig)\n\t\tif err != nil {\n\t\t\tnumActualErrors++\n\t\t}\n\t}\n\t// Expecting all layouts to be incorrect, since sanitizedgo is being used.\n\tif numActualErrors != numExpectedErrors {\n\t\tt.Fatalf(\"expected %d errors, got: %d\", numExpectedErrors, numActualErrors)\n\t}\n\n\t// sanity test - flexiblego should still allow the incorrect layouts, for legacy purposes\n\tfor idx, badLayout := range incorrectLayouts {\n\t\tincorrectConfig := map[string]interface{}{\n\t\t\t\"type\":    flexiblegoName,\n\t\t\t\"layouts\": badLayout,\n\t\t}\n\t\terr := imap.AddCustomDateTimeParser(fmt.Sprintf(\"%d_DT_Flexi\", idx), incorrectConfig)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected no error, got: %v\", err)\n\t\t}\n\t}\n}\n\nfunc TestDateRangeStringQuery(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\n\terr := idxMapping.AddCustomDateTimeParser(\"customDT\", map[string]interface{}{\n\t\t\"type\": sanitized.Name,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"02/01/2006 15:04:05\",\n\t\t\t\"2006/01/02 3:04PM\",\n\t\t},\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idxMapping.AddCustomDateTimeParser(\"queryDT\", map[string]interface{}{\n\t\t\"type\": sanitized.Name,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"02/01/2006 3:04PM\",\n\t\t},\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdtmap := NewDateTimeFieldMapping()\n\tdtmap.DateFormat = \"customDT\"\n\tidxMapping.DefaultMapping.AddFieldMappingsAt(\"date\", dtmap)\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"date\": \"2001/08/20 6:00PM\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"date\": \"20/08/2001 18:00:20\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"date\": \"20/08/2001 18:10:00\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"date\": \"2001/08/20 6:15PM\",\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"date\": \"20/08/2001 18:20:00\",\n\t\t},\n\t}\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype testResult struct {\n\t\tdocID    string // doc ID of the hit\n\t\thitField string // fields returned as part of the hit\n\t}\n\n\ttype testStruct struct {\n\t\tstart          string\n\t\tend            string\n\t\tfield          string\n\t\tdateTimeParser string // name of the custom date time parser to use if nil, use QueryDateTimeParser\n\t\tincludeStart   bool\n\t\tincludeEnd     bool\n\t\texpectedHits   []testResult\n\t\terr            error\n\t}\n\n\ttestQueries := []testStruct{\n\t\t// test cases with RFC3339 parser and toggling includeStart and includeEnd\n\t\t{\n\t\t\tstart:        \"2001-08-20T18:00:00\",\n\t\t\tend:          \"2001-08-20T18:10:00\",\n\t\t\tfield:        \"date\",\n\t\t\tincludeStart: true,\n\t\t\tincludeEnd:   true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc1\",\n\t\t\t\t\thitField: \"2001/08/20 6:00PM\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc2\",\n\t\t\t\t\thitField: \"20/08/2001 18:00:20\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc3\",\n\t\t\t\t\thitField: \"20/08/2001 18:10:00\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart:        \"2001-08-20T18:00:00\",\n\t\t\tend:          \"2001-08-20T18:10:00\",\n\t\t\tfield:        \"date\",\n\t\t\tincludeStart: false,\n\t\t\tincludeEnd:   true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc2\",\n\t\t\t\t\thitField: \"20/08/2001 18:00:20\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc3\",\n\t\t\t\t\thitField: \"20/08/2001 18:10:00\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart:        \"2001-08-20T18:00:00\",\n\t\t\tend:          \"2001-08-20T18:10:00\",\n\t\t\tfield:        \"date\",\n\t\t\tincludeStart: false,\n\t\t\tincludeEnd:   false,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc2\",\n\t\t\t\t\thitField: \"20/08/2001 18:00:20\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t// test cases with custom parser and omitting start and end\n\t\t{\n\t\t\tstart:          \"20/08/2001 18:00:00\",\n\t\t\tend:            \"2001/08/20 6:10PM\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\tincludeStart:   true,\n\t\t\tincludeEnd:     true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc1\",\n\t\t\t\t\thitField: \"2001/08/20 6:00PM\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc2\",\n\t\t\t\t\thitField: \"20/08/2001 18:00:20\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc3\",\n\t\t\t\t\thitField: \"20/08/2001 18:10:00\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tend:            \"20/08/2001 18:15:00\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\tincludeStart:   true,\n\t\t\tincludeEnd:     true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc1\",\n\t\t\t\t\thitField: \"2001/08/20 6:00PM\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc2\",\n\t\t\t\t\thitField: \"20/08/2001 18:00:20\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc3\",\n\t\t\t\t\thitField: \"20/08/2001 18:10:00\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc4\",\n\t\t\t\t\thitField: \"2001/08/20 6:15PM\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart:          \"2001/08/20 6:15PM\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\tincludeStart:   true,\n\t\t\tincludeEnd:     true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc4\",\n\t\t\t\t\thitField: \"2001/08/20 6:15PM\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc5\",\n\t\t\t\t\thitField: \"20/08/2001 18:20:00\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart:          \"20/08/2001 6:15PM\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"queryDT\",\n\t\t\tincludeStart:   true,\n\t\t\tincludeEnd:     true,\n\t\t\texpectedHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc4\",\n\t\t\t\t\thitField: \"2001/08/20 6:15PM\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:    \"doc5\",\n\t\t\t\t\thitField: \"20/08/2001 18:20:00\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t// error path test cases\n\t\t{\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\tincludeStart:   true,\n\t\t\tincludeEnd:     true,\n\t\t\terr:            fmt.Errorf(\"date range query must specify at least one of start/end\"),\n\t\t},\n\t\t{\n\t\t\tfield:        \"date\",\n\t\t\tincludeStart: true,\n\t\t\tincludeEnd:   true,\n\t\t\terr:          fmt.Errorf(\"date range query must specify at least one of start/end\"),\n\t\t},\n\t\t{\n\t\t\tstart:          \"2001-08-20T18:00:00\",\n\t\t\tend:            \"2001-08-20T18:10:00\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\terr:            fmt.Errorf(\"unable to parse datetime with any of the layouts, date time parser name: customDT\"),\n\t\t},\n\t\t{\n\t\t\tstart: \"3001-08-20T18:00:00\",\n\t\t\tend:   \"2001-08-20T18:10:00\",\n\t\t\tfield: \"date\",\n\t\t\terr:   fmt.Errorf(\"invalid/unsupported date range, start: 3001-08-20T18:00:00\"),\n\t\t},\n\t\t{\n\t\t\tstart:          \"2001/08/20 6:00PM\",\n\t\t\tend:            \"3001/08/20 6:30PM\",\n\t\t\tfield:          \"date\",\n\t\t\tdateTimeParser: \"customDT\",\n\t\t\terr:            fmt.Errorf(\"invalid/unsupported date range, end: 3001/08/20 6:30PM\"),\n\t\t},\n\t}\n\n\tfor _, dtq := range testQueries {\n\t\tvar err error\n\t\tdateQuery := NewDateRangeInclusiveStringQuery(dtq.start, dtq.end, &dtq.includeStart, &dtq.includeEnd)\n\t\tdateQuery.SetDateTimeParser(dtq.dateTimeParser)\n\t\tdateQuery.SetField(dtq.field)\n\n\t\tsr := NewSearchRequest(dateQuery)\n\t\tsr.SortBy([]string{dtq.field})\n\t\tsr.Fields = []string{dtq.field}\n\n\t\tres, err := idx.Search(sr)\n\t\tif err != nil {\n\t\t\tif dtq.err == nil {\n\t\t\t\tt.Fatalf(\"expected no error, got: %v\", err)\n\t\t\t}\n\t\t\tif dtq.err.Error() != err.Error() {\n\t\t\t\tt.Fatalf(\"expected error: %v, got: %v\", dtq.err, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif len(res.Hits) != len(dtq.expectedHits) {\n\t\t\tt.Fatalf(\"expected %d hits, got %d\", len(dtq.expectedHits), len(res.Hits))\n\t\t}\n\t\tfor i, hit := range res.Hits {\n\t\t\tif hit.ID != dtq.expectedHits[i].docID {\n\t\t\t\tt.Fatalf(\"expected docID %s, got %s\", dtq.expectedHits[i].docID, hit.ID)\n\t\t\t}\n\t\t\tif hit.Fields[dtq.field].(string) != dtq.expectedHits[i].hitField {\n\t\t\t\tt.Fatalf(\"expected hit field %s, got %s\", dtq.expectedHits[i].hitField, hit.Fields[dtq.field])\n\t\t\t}\n\t\t}\n\t}\n}\nfunc TestDateRangeFacetQueriesWithCustomDateTimeParser(t *testing.T) {\n\tidxMapping := NewIndexMapping()\n\n\terr := idxMapping.AddCustomDateTimeParser(\"customDT\", map[string]interface{}{\n\t\t\"type\": sanitized.Name,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"02/01/2006 15:04:05\",\n\t\t\t\"2006/01/02 3:04PM\",\n\t\t},\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = idxMapping.AddCustomDateTimeParser(\"queryDT\", map[string]interface{}{\n\t\t\"type\": sanitized.Name,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"02/01/2006 3:04PM\",\n\t\t},\n\t})\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdtmap := NewDateTimeFieldMapping()\n\tdtmap.DateFormat = \"customDT\"\n\tidxMapping.DefaultMapping.AddFieldMappingsAt(\"date\", dtmap)\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, idxMapping)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"date\": \"2001/08/20 6:00PM\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"date\": \"20/08/2001 18:00:20\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"date\": \"20/08/2001 18:10:00\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"date\": \"2001/08/20 6:15PM\",\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"date\": \"20/08/2001 18:20:00\",\n\t\t},\n\t}\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tquery := NewMatchAllQuery()\n\n\ttype testFacetResult struct {\n\t\tname  string\n\t\tstart string\n\t\tend   string\n\t\tcount int\n\t\terr   error\n\t}\n\n\ttype testFacetRequest struct {\n\t\tname   string\n\t\tstart  string\n\t\tend    string\n\t\tparser string\n\t\tresult testFacetResult\n\t}\n\n\ttests := []testFacetRequest{\n\t\t{\n\t\t\t// Test without a query time override of the parser (use default parser)\n\t\t\tname:  \"test\",\n\t\t\tstart: \"2001-08-20 18:00:00\",\n\t\t\tend:   \"2001-08-20 18:10:00\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname:  \"test\",\n\t\t\t\tstart: \"2001-08-20T18:00:00Z\",\n\t\t\t\tend:   \"2001-08-20T18:10:00Z\",\n\t\t\t\tcount: 2,\n\t\t\t\terr:   nil,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"test\",\n\t\t\tstart:  \"20/08/2001 6:00PM\",\n\t\t\tend:    \"20/08/2001 6:10PM\",\n\t\t\tparser: \"queryDT\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname:  \"test\",\n\t\t\t\tstart: \"2001-08-20T18:00:00Z\",\n\t\t\t\tend:   \"2001-08-20T18:10:00Z\",\n\t\t\t\tcount: 2,\n\t\t\t\terr:   nil,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"test\",\n\t\t\tstart:  \"20/08/2001 15:00:00\",\n\t\t\tend:    \"2001/08/20 6:10PM\",\n\t\t\tparser: \"customDT\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname:  \"test\",\n\t\t\t\tstart: \"2001-08-20T15:00:00Z\",\n\t\t\t\tend:   \"2001-08-20T18:10:00Z\",\n\t\t\t\tcount: 2,\n\t\t\t\terr:   nil,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"test\",\n\t\t\tend:    \"2001/08/20 6:15PM\",\n\t\t\tparser: \"customDT\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname:  \"test\",\n\t\t\t\tend:   \"2001-08-20T18:15:00Z\",\n\t\t\t\tcount: 3,\n\t\t\t\terr:   nil,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"test\",\n\t\t\tstart:  \"20/08/2001 6:15PM\",\n\t\t\tparser: \"queryDT\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname:  \"test\",\n\t\t\t\tstart: \"2001-08-20T18:15:00Z\",\n\t\t\t\tcount: 2,\n\t\t\t\terr:   nil,\n\t\t\t},\n\t\t},\n\t\t// some error cases\n\t\t{\n\t\t\tname:   \"test\",\n\t\t\tparser: \"queryDT\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname: \"test\",\n\t\t\t\terr:  fmt.Errorf(\"date range query must specify either start, end or both for date range name 'test'\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t// default parser is used for the query, but the start time is not in the correct format (RFC3339),\n\t\t\t// so it should throw an error\n\t\t\tname:  \"test\",\n\t\t\tstart: \"20/08/2001 6:15PM\",\n\t\t\tresult: testFacetResult{\n\t\t\t\tname: \"test\",\n\t\t\t\terr:  fmt.Errorf(\"ParseDates err: error parsing start date '20/08/2001 6:15PM' for date range name 'test': unable to parse datetime with any of the layouts, using date time parser named dateTimeOptional\"),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tsearchRequest := NewSearchRequest(query)\n\n\t\tfr := NewFacetRequest(\"date\", 100)\n\t\tstart := &test.start\n\t\tif test.start == \"\" {\n\t\t\tstart = nil\n\t\t}\n\t\tend := &test.end\n\t\tif test.end == \"\" {\n\t\t\tend = nil\n\t\t}\n\n\t\tfr.AddDateTimeRangeStringWithParser(test.name, start, end, test.parser)\n\t\tsearchRequest.AddFacet(\"dateFacet\", fr)\n\n\t\tsearchResults, err := idx.Search(searchRequest)\n\t\tif err != nil {\n\t\t\tif test.result.err == nil {\n\t\t\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t\t\t}\n\t\t\tif err.Error() != test.result.err.Error() {\n\t\t\t\tt.Fatalf(\"Expected error %v, got %v\", test.result.err, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tfor _, facetResult := range searchResults.Facets {\n\t\t\tif len(facetResult.DateRanges) != 1 {\n\t\t\t\tt.Fatal(\"Expected 1 date range facet\")\n\t\t\t}\n\t\t\tresult := facetResult.DateRanges[0]\n\t\t\tif result.Name != test.result.name {\n\t\t\t\tt.Fatalf(\"Expected name %s, got %s\", test.result.name, result.Name)\n\t\t\t}\n\t\t\tif result.Start != nil && *result.Start != test.result.start {\n\t\t\t\tt.Fatalf(\"Expected start %s, got %s\", test.result.start, *result.Start)\n\t\t\t}\n\t\t\tif result.End != nil && *result.End != test.result.end {\n\t\t\t\tt.Fatalf(\"Expected end %s, got %s\", test.result.end, *result.End)\n\t\t\t}\n\t\t\tif result.Start == nil && test.result.start != \"\" {\n\t\t\t\tt.Fatalf(\"Expected start %s, got nil\", test.result.start)\n\t\t\t}\n\t\t\tif result.End == nil && test.result.end != \"\" {\n\t\t\t\tt.Fatalf(\"Expected end %s, got nil\", test.result.end)\n\t\t\t}\n\t\t\tif result.Count != test.result.count {\n\t\t\t\tt.Fatalf(\"Expected count %d, got %d\", test.result.count, result.Count)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestDateRangeTimestampQueries(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\timap := mapping.NewIndexMapping()\n\n\t// add a date field with a valid format to the default mapping\n\t// for good measure\n\n\tdtParserConfig := map[string]interface{}{\n\t\t\"type\":    flexible.Name,\n\t\t\"layouts\": []interface{}{\"2006/01/02 15:04:05\"},\n\t}\n\terr := imap.AddCustomDateTimeParser(\"custDT\", dtParserConfig)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdateField := mapping.NewDateTimeFieldMapping()\n\tdateField.DateFormat = \"custDT\"\n\n\tunixSecField := mapping.NewDateTimeFieldMapping()\n\tunixSecField.DateFormat = seconds.Name\n\n\tunixMilliSecField := mapping.NewDateTimeFieldMapping()\n\tunixMilliSecField.DateFormat = milliseconds.Name\n\n\tunixMicroSecField := mapping.NewDateTimeFieldMapping()\n\tunixMicroSecField.DateFormat = microseconds.Name\n\n\tunixNanoSecField := mapping.NewDateTimeFieldMapping()\n\tunixNanoSecField.DateFormat = nanoseconds.Name\n\n\timap.DefaultMapping.AddFieldMappingsAt(\"date\", dateField)\n\timap.DefaultMapping.AddFieldMappingsAt(\"seconds\", unixSecField)\n\timap.DefaultMapping.AddFieldMappingsAt(\"milliseconds\", unixMilliSecField)\n\timap.DefaultMapping.AddFieldMappingsAt(\"microseconds\", unixMicroSecField)\n\timap.DefaultMapping.AddFieldMappingsAt(\"nanoseconds\", unixNanoSecField)\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdocuments := map[string]map[string]string{\n\t\t\"doc1\": {\n\t\t\t\"date\":         \"2001/08/20 03:00:10\",\n\t\t\t\"seconds\":      \"998276410\",\n\t\t\t\"milliseconds\": \"998276410100\",\n\t\t\t\"microseconds\": \"998276410100300\",\n\t\t\t\"nanoseconds\":  \"998276410100300400\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"date\":         \"2001/08/20 03:00:20\",\n\t\t\t\"seconds\":      \"998276420\",\n\t\t\t\"milliseconds\": \"998276410200\",\n\t\t\t\"microseconds\": \"998276410100400\",\n\t\t\t\"nanoseconds\":  \"998276410100300500\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"date\":         \"2001/08/20 03:00:30\",\n\t\t\t\"seconds\":      \"998276430\",\n\t\t\t\"milliseconds\": \"998276410300\",\n\t\t\t\"microseconds\": \"998276410100500\",\n\t\t\t\"nanoseconds\":  \"998276410100300600\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"date\":         \"2001/08/20 03:00:40\",\n\t\t\t\"seconds\":      \"998276440\",\n\t\t\t\"milliseconds\": \"998276410400\",\n\t\t\t\"microseconds\": \"998276410100600\",\n\t\t\t\"nanoseconds\":  \"998276410100300700\",\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"date\":         \"2001/08/20 03:00:50\",\n\t\t\t\"seconds\":      \"998276450\",\n\t\t\t\"milliseconds\": \"998276410500\",\n\t\t\t\"microseconds\": \"998276410100700\",\n\t\t\t\"nanoseconds\":  \"998276410100300800\",\n\t\t},\n\t}\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype testStruct struct {\n\t\tstart        string\n\t\tend          string\n\t\tfield        string\n\t\texpectedHits []string\n\t}\n\n\ttestQueries := []testStruct{\n\t\t{\n\t\t\tstart: \"2001-08-20T03:00:05\",\n\t\t\tend:   \"2001-08-20T03:00:25\",\n\t\t\tfield: \"date\",\n\t\t\texpectedHits: []string{\n\t\t\t\t\"doc1\",\n\t\t\t\t\"doc2\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart: \"2001-08-20T03:00:15\",\n\t\t\tend:   \"2001-08-20T03:00:35\",\n\t\t\tfield: \"seconds\",\n\t\t\texpectedHits: []string{\n\t\t\t\t\"doc2\",\n\t\t\t\t\"doc3\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart: \"2001-08-20T03:00:10.150\",\n\t\t\tend:   \"2001-08-20T03:00:10.450\",\n\t\t\tfield: \"milliseconds\",\n\t\t\texpectedHits: []string{\n\t\t\t\t\"doc2\",\n\t\t\t\t\"doc3\",\n\t\t\t\t\"doc4\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart: \"2001-08-20T03:00:10.100450\",\n\t\t\tend:   \"2001-08-20T03:00:10.100650\",\n\t\t\tfield: \"microseconds\",\n\t\t\texpectedHits: []string{\n\t\t\t\t\"doc3\",\n\t\t\t\t\"doc4\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstart: \"2001-08-20T03:00:10.100300550\",\n\t\t\tend:   \"2001-08-20T03:00:10.100300850\",\n\t\t\tfield: \"nanoseconds\",\n\t\t\texpectedHits: []string{\n\t\t\t\t\"doc3\",\n\t\t\t\t\"doc4\",\n\t\t\t\t\"doc5\",\n\t\t\t},\n\t\t},\n\t}\n\ttestLayout := \"2006-01-02T15:04:05\"\n\tfor _, dtq := range testQueries {\n\t\tstartTime, err := time.Parse(testLayout, dtq.start)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tendTime, err := time.Parse(testLayout, dtq.end)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdrq := NewDateRangeQuery(startTime, endTime)\n\t\tdrq.SetField(dtq.field)\n\n\t\tsr := NewSearchRequest(drq)\n\t\tsr.SortBy([]string{dtq.field})\n\t\tsr.Fields = []string{\"*\"}\n\n\t\tres, err := idx.Search(sr)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif len(res.Hits) != len(dtq.expectedHits) {\n\t\t\tt.Fatalf(\"expected %d hits, got %d\", len(dtq.expectedHits), len(res.Hits))\n\t\t}\n\t\tfor i, hit := range res.Hits {\n\t\t\tif hit.ID != dtq.expectedHits[i] {\n\t\t\t\tt.Fatalf(\"expected docID %s, got %s\", dtq.expectedHits[i], hit.ID)\n\t\t\t}\n\t\t\tif len(hit.Fields) != len(documents[hit.ID]) {\n\t\t\t\tt.Fatalf(\"expected hit %s to have %d fields, got %d\", hit.ID, len(documents[hit.ID]), len(hit.Fields))\n\t\t\t}\n\t\t\tfor k, v := range documents[hit.ID] {\n\t\t\t\tif hit.Fields[k] != v {\n\t\t\t\t\tt.Fatalf(\"expected field %s to be %s, got %s\", k, v, hit.Fields[k])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestPercentAndIsoStyleDates(t *testing.T) {\n\tpercentName := percent.Name\n\tisoName := iso.Name\n\n\timap := mapping.NewIndexMapping()\n\tpercentConfig := map[string]interface{}{\n\t\t\"type\": percentName,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"%Y/%m/%d %l:%M%p\",                // doc 1\n\t\t\t\"%d/%m/%Y %H:%M:%S\",               // doc 2\n\t\t\t\"%Y-%m-%dT%H:%M:%S%z\",             // doc 3\n\t\t\t\"%d %B %y %l%p %Z\",                // doc 4\n\t\t\t\"%Y; %b %d (%a) %I:%M:%S.%N%P %z\", // doc 5\n\t\t},\n\t}\n\tisoConfig := map[string]interface{}{\n\t\t\"type\": isoName,\n\t\t\"layouts\": []interface{}{\n\t\t\t\"yyyy/MM/dd h:mma\",                       // doc 1\n\t\t\t\"dd/MM/yyyy HH:mm:ss\",                    // doc 2\n\t\t\t\"yyyy-MM-dd'T'HH:mm:ssXX\",                // doc 3\n\t\t\t\"dd MMMM yy ha z\",                        // doc 4\n\t\t\t\"yyyy; MMM dd (EEE) hh:mm:ss.SSSSSaa xx\", // doc 5\n\t\t},\n\t}\n\n\terr := imap.AddCustomDateTimeParser(\"percentDate\", percentConfig)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = imap.AddCustomDateTimeParser(\"isoDate\", isoConfig)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpercentField := mapping.NewDateTimeFieldMapping()\n\tpercentField.DateFormat = \"percentDate\"\n\n\tisoField := mapping.NewDateTimeFieldMapping()\n\tisoField.DateFormat = \"isoDate\"\n\n\timap.DefaultMapping.AddFieldMappingsAt(\"percentDate\", percentField)\n\timap.DefaultMapping.AddFieldMappingsAt(\"isoDate\", isoField)\n\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"percentDate\": \"2001/08/20 6:00PM\",\n\t\t\t\"isoDate\":     \"2001/08/20 6:00PM\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"percentDate\": \"20/08/2001 18:05:00\",\n\t\t\t\"isoDate\":     \"20/08/2001 18:05:00\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"percentDate\": \"2001-08-20T18:10:00Z\",\n\t\t\t\"isoDate\":     \"2001-08-20T18:10:00Z\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"percentDate\": \"20 August 01 6PM UTC\",\n\t\t\t\"isoDate\":     \"20 August 01 6PM UTC\",\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"percentDate\": \"2001; Aug 20 (Mon) 06:15:15.23456pm +0000\",\n\t\t\t\"isoDate\":     \"2001; Aug 20 (Mon) 06:15:15.23456pm +0000\",\n\t\t},\n\t}\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype testStruct struct {\n\t\tstart string\n\t\tend   string\n\t\tfield string\n\t}\n\n\tfor _, field := range []string{\"percentDate\", \"isoDate\"} {\n\t\ttestQueries := []testStruct{\n\t\t\t{\n\t\t\t\tstart: \"2001/08/20 6:00PM\",\n\t\t\t\tend:   \"2001/08/20 6:20PM\",\n\t\t\t\tfield: field,\n\t\t\t},\n\t\t\t{\n\t\t\t\tstart: \"20/08/2001 18:00:00\",\n\t\t\t\tend:   \"20/08/2001 18:20:00\",\n\t\t\t\tfield: field,\n\t\t\t},\n\t\t\t{\n\t\t\t\tstart: \"2001-08-20T18:00:00Z\",\n\t\t\t\tend:   \"2001-08-20T18:20:00Z\",\n\t\t\t\tfield: field,\n\t\t\t},\n\t\t\t{\n\t\t\t\tstart: \"20 August 01 6PM UTC\",\n\t\t\t\tend:   \"20 August 01 7PM UTC\",\n\t\t\t\tfield: field,\n\t\t\t},\n\t\t\t{\n\t\t\t\tstart: \"2001; Aug 20 (Mon) 06:00:00.00000pm +0000\",\n\t\t\t\tend:   \"2001; Aug 20 (Mon) 06:20:20.00000pm +0000\",\n\t\t\t\tfield: field,\n\t\t\t},\n\t\t}\n\t\tincludeStart := true\n\t\tincludeEnd := true\n\t\tfor _, dtq := range testQueries {\n\t\t\tdrq := NewDateRangeInclusiveStringQuery(dtq.start, dtq.end, &includeStart, &includeEnd)\n\t\t\tdrq.SetField(dtq.field)\n\t\t\tdrq.SetDateTimeParser(field)\n\t\t\tsr := NewSearchRequest(drq)\n\t\t\tres, err := idx.Search(sr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif len(res.Hits) != 5 {\n\t\t\t\tt.Fatalf(\"expected %d hits, got %d\", 5, len(res.Hits))\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc roundToDecimalPlace(num float64, decimalPlaces int) float64 {\n\tprecision := math.Pow(10, float64(decimalPlaces))\n\treturn math.Round(num*precision) / precision\n}\n\nfunc TestScoreBreakdown(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\timap := mapping.NewIndexMapping()\n\ttextField := mapping.NewTextFieldMapping()\n\ttextField.Analyzer = simple.Name\n\timap.DefaultMapping.AddFieldMappingsAt(\"text\", textField)\n\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"text\": \"lorem ipsum dolor sit amet consectetur adipiscing elit do eiusmod tempor\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"text\": \"lorem dolor amet adipiscing sed eiusmod\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"text\": \"ipsum sit consectetur elit do tempor\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"text\": \"lorem ipsum sit amet adipiscing elit do eiusmod\",\n\t\t},\n\t}\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype testResult struct {\n\t\tdocID          string // doc ID of the hit\n\t\tscore          float64\n\t\tscoreBreakdown map[int]float64\n\t}\n\ttype testStruct struct {\n\t\tquery      string\n\t\ttyp        string\n\t\texpectHits []testResult\n\t}\n\ttestQueries := []testStruct{\n\t\t{\n\t\t\t// trigger disjunction heap searcher (>10 searchers)\n\t\t\t// expect score breakdown to have a 0 at BLANK\n\t\t\tquery: `{\"disjuncts\":[{\"term\":\"lorem\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"ipsum\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"dolor\",\"field\":\"text\"},{\"term\":\"sit\",\"field\":\"text\"},{\"term\":\"amet\",\"field\":\"text\"},{\"term\":\"consectetur\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"adipiscing\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"elit\",\"field\":\"text\"},{\"term\":\"sed\",\"field\":\"text\"},{\"term\":\"do\",\"field\":\"text\"},{\"term\":\"eiusmod\",\"field\":\"text\"},{\"term\":\"tempor\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"}]}`,\n\t\t\ttyp:   \"disjunction\",\n\t\t\texpectHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc1\",\n\t\t\t\t\tscore:          0.3034548543819603,\n\t\t\t\t\tscoreBreakdown: map[int]float64{0: 0.040398807605268316, 2: 0.040398807605268316, 5: 0.0669862776967768, 6: 0.040398807605268316, 7: 0.040398807605268316, 8: 0.0669862776967768, 10: 0.040398807605268316, 12: 0.040398807605268316, 14: 0.040398807605268316, 15: 0.040398807605268316, 16: 0.0669862776967768},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc2\",\n\t\t\t\t\tscore:          0.14725661652397853,\n\t\t\t\t\tscoreBreakdown: map[int]float64{0: 0.05470024557900147, 5: 0.09069985124905133, 7: 0.05470024557900147, 10: 0.05470024557900147, 13: 0.15681178542754148, 15: 0.05470024557900147},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc3\",\n\t\t\t\t\tscore:          0.12637916362550797,\n\t\t\t\t\tscoreBreakdown: map[int]float64{2: 0.05470024557900147, 6: 0.05470024557900147, 8: 0.09069985124905133, 12: 0.05470024557900147, 14: 0.05470024557900147, 16: 0.09069985124905133},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc4\",\n\t\t\t\t\tscore:          0.15956816751152955,\n\t\t\t\t\tscoreBreakdown: map[int]float64{0: 0.04737179972998534, 2: 0.04737179972998534, 6: 0.04737179972998534, 7: 0.04737179972998534, 10: 0.04737179972998534, 12: 0.04737179972998534, 14: 0.04737179972998534, 15: 0.04737179972998534},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t// trigger disjunction slice searcher (< 10 searchers)\n\t\t\t// expect BLANK to give a 0 in score breakdown\n\t\t\tquery: `{\"disjuncts\":[{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"lorem\",\"field\":\"text\"},{\"term\":\"ipsum\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"},{\"term\":\"dolor\",\"field\":\"text\"},{\"term\":\"sit\",\"field\":\"text\"},{\"term\":\"blank\",\"field\":\"text\"}]}`,\n\t\t\ttyp:   \"disjunction\",\n\t\t\texpectHits: []testResult{\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc1\",\n\t\t\t\t\tscore:          0.1340684440934241,\n\t\t\t\t\tscoreBreakdown: map[int]float64{1: 0.05756326446708409, 2: 0.05756326446708409, 5: 0.09544709478559595, 6: 0.05756326446708409},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc2\",\n\t\t\t\t\tscore:          0.05179425287147191,\n\t\t\t\t\tscoreBreakdown: map[int]float64{1: 0.0779410306721006, 5: 0.129235980813787},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc3\",\n\t\t\t\t\tscore:          0.0389705153360503,\n\t\t\t\t\tscoreBreakdown: map[int]float64{2: 0.0779410306721006, 6: 0.0779410306721006},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tdocID:          \"doc4\",\n\t\t\t\t\tscore:          0.07593627256602972,\n\t\t\t\t\tscoreBreakdown: map[int]float64{1: 0.06749890894758198, 2: 0.06749890894758198, 6: 0.06749890894758198},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tfor _, dtq := range testQueries {\n\t\tvar q query.Query\n\t\tvar rv query.DisjunctionQuery\n\t\terr := json.Unmarshal([]byte(dtq.query), &rv)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\trv.RetrieveScoreBreakdown(true)\n\t\tq = &rv\n\t\tsr := NewSearchRequest(q)\n\t\tsr.SortBy([]string{\"_id\"})\n\t\tsr.Explain = true\n\t\tres, err := idx.Search(sr)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif len(res.Hits) != len(dtq.expectHits) {\n\t\t\tt.Fatalf(\"expected %d hits, got %d\", len(dtq.expectHits), len(res.Hits))\n\t\t}\n\t\tfor i, hit := range res.Hits {\n\t\t\tif hit.ID != dtq.expectHits[i].docID {\n\t\t\t\tt.Fatalf(\"expected docID %s, got %s\", dtq.expectHits[i].docID, hit.ID)\n\t\t\t}\n\t\t\tif len(hit.ScoreBreakdown) != len(dtq.expectHits[i].scoreBreakdown) {\n\t\t\t\tt.Fatalf(\"expected %d score breakdown, got %d\", len(dtq.expectHits[i].scoreBreakdown), len(hit.ScoreBreakdown))\n\t\t\t}\n\t\t\tfor j, score := range hit.ScoreBreakdown {\n\t\t\t\tactualScore := roundToDecimalPlace(score, 3)\n\t\t\t\texpectScore := roundToDecimalPlace(dtq.expectHits[i].scoreBreakdown[j], 3)\n\t\t\t\tif actualScore != expectScore {\n\t\t\t\t\tt.Fatalf(\"expected score breakdown %f, got %f\", dtq.expectHits[i].scoreBreakdown[j], score)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestAutoFuzzy(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\timap := mapping.NewIndexMapping()\n\n\tif err := imap.AddCustomAnalyzer(\"splitter\", map[string]interface{}{\n\t\t\"type\":          custom.Name,\n\t\t\"tokenizer\":     whitespace.Name,\n\t\t\"token_filters\": []interface{}{lowercase.Name},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttextField := mapping.NewTextFieldMapping()\n\ttextField.Analyzer = \"splitter\"\n\ttextField.Store = true\n\ttextField.IncludeTermVectors = true\n\ttextField.IncludeInAll = true\n\n\timap.DefaultMapping.Dynamic = false\n\timap.DefaultMapping.AddFieldMappingsAt(\"model\", textField)\n\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"product1\": {\n\t\t\t\"model\": \"apple iphone 12\",\n\t\t},\n\t\t\"product2\": {\n\t\t\t\"model\": \"apple iphone 13\",\n\t\t},\n\t\t\"product3\": {\n\t\t\t\"model\": \"samsung galaxy s22\",\n\t\t},\n\t\t\"product4\": {\n\t\t\t\"model\": \"samsung galaxy note\",\n\t\t},\n\t\t\"product5\": {\n\t\t\t\"model\": \"google pixel 5\",\n\t\t},\n\t\t\"product6\": {\n\t\t\t\"model\": \"oneplus 9 pro\",\n\t\t},\n\t\t\"product7\": {\n\t\t\t\"model\": \"xiaomi mi 11\",\n\t\t},\n\t\t\"product8\": {\n\t\t\t\"model\": \"oppo find x3\",\n\t\t},\n\t\t\"product9\": {\n\t\t\t\"model\": \"vivo x60 pro\",\n\t\t},\n\t\t\"product10\": {\n\t\t\t\"model\": \"oneplus 8t pro\",\n\t\t},\n\t\t\"product11\": {\n\t\t\t\"model\": \"nokia xr20\",\n\t\t},\n\t\t\"product12\": {\n\t\t\t\"model\": \"poco f1\",\n\t\t},\n\t\t\"product13\": {\n\t\t\t\"model\": \"asus rog 5\",\n\t\t},\n\t\t\"product14\": {\n\t\t\t\"model\": \"samsung galaxy a15 5g\",\n\t\t},\n\t\t\"product15\": {\n\t\t\t\"model\": \"tecno camon 17\",\n\t\t},\n\t}\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype testStruct struct {\n\t\tquery      string\n\t\texpectHits []string\n\t}\n\ttestQueries := []testStruct{\n\t\t{\n\t\t\t// match query with fuzziness set to 2\n\t\t\tquery: `{\n\t\t\t\t\t\"match\" : \"applle iphone 12\",\n\t\t\t\t\t\"fuzziness\": 2,\n\t\t\t\t\t\"field\" : \"model\"\n\t\t\t\t}`,\n\t\t\texpectHits: []string{\"product1\", \"product2\", \"product7\", \"product14\", \"product15\", \"product12\", \"product10\", \"product3\", \"product6\", \"product8\"},\n\t\t},\n\t\t{\n\t\t\t// match query with fuzziness set to \"auto\"\n\t\t\tquery: `{\n\t\t\t\t\t\"match\" : \"applle iphone 12\",\n\t\t\t\t\t\"fuzziness\": \"auto\",\n\t\t\t\t\t\"field\" : \"model\"\n\t\t\t\t}`,\n\t\t\texpectHits: []string{\"product1\", \"product2\"},\n\t\t},\n\t\t{\n\t\t\t// match query with fuzziness set to 2 with `and` operator\n\t\t\tquery: `{\n\t\t\t\t\t\"match\" : \"applle iphone 12\",\n\t\t\t\t\t\"fuzziness\": 2,\n\t\t\t\t\t\"field\" : \"model\",\n\t\t\t\t\t\"operator\": \"and\"\n\t\t\t\t}`,\n\t\t\texpectHits: []string{\"product1\", \"product2\"},\n\t\t},\n\t\t{\n\t\t\t// match query with fuzziness set to \"auto\" with `and`` operator\n\t\t\tquery: `{\n\t\t\t\t\t\"match\" : \"applle iphone 12\",\n\t\t\t\t\t\"fuzziness\": \"auto\",\n\t\t\t\t\t\"field\" : \"model\",\n\t\t\t\t\t\"operator\": \"and\"\n\t\t\t\t}`,\n\t\t\texpectHits: []string{\"product1\"},\n\t\t},\n\t\t// match phrase query with fuzziness set to 2\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\t\"match_phrase\" : \"onplus 9 pro\",\n\t\t\t\t\t\"fuzziness\": 2,\n\t\t\t\t\t\"field\" : \"model\"\n\t\t\t\t}`,\n\t\t\texpectHits: []string{\"product6\", \"product10\"},\n\t\t},\n\t\t// match phrase query with fuzziness set to \"auto\"\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"match_phrase\" : \"onplus 9 pro\",\n\t\t\t\t\"fuzziness\": \"auto\",\n\t\t\t\t\"field\" : \"model\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\"product6\"},\n\t\t},\n\t}\n\n\tfor _, dtq := range testQueries {\n\t\tq, err := query.ParseQuery([]byte(dtq.query))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tsr := NewSearchRequest(q)\n\t\tsr.Highlight = NewHighlightWithStyle(ansi.Name)\n\t\tsr.SortBy([]string{\"-_score\", \"_id\"})\n\t\tsr.Fields = []string{\"*\"}\n\t\tsr.Explain = true\n\n\t\tres, err := idx.Search(sr)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif len(res.Hits) != len(dtq.expectHits) {\n\t\t\tt.Fatalf(\"expected %d hits, got %d\", len(dtq.expectHits), len(res.Hits))\n\t\t}\n\t\tfor i, hit := range res.Hits {\n\t\t\tif hit.ID != dtq.expectHits[i] {\n\t\t\t\tt.Fatalf(\"expected docID %s, got %s\", dtq.expectHits[i], hit.ID)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestThesaurusTermReader(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tsynonymCollection := \"collection1\"\n\n\tsynonymSourceName := \"english\"\n\n\tanalyzer := simple.Name\n\n\tsynonymSourceConfig := map[string]interface{}{\n\t\t\"collection\": synonymCollection,\n\t\t\"analyzer\":   analyzer,\n\t}\n\n\ttextField := mapping.NewTextFieldMapping()\n\ttextField.Analyzer = analyzer\n\ttextField.SynonymSource = synonymSourceName\n\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping.AddFieldMappingsAt(\"text\", textField)\n\terr := imap.AddSynonymSource(synonymSourceName, synonymSourceConfig)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = imap.Validate()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"text\": \"quick brown fox eats\",\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"text\": \"fast red wolf jumps\",\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"text\": \"quick red cat runs\",\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"text\": \"speedy brown dog barks\",\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"text\": \"fast green rabbit hops\",\n\t\t},\n\t}\n\n\tbatch := idx.NewBatch()\n\tfor docID, doc := range documents {\n\t\terr := batch.Index(docID, doc)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tsynonymDocuments := map[string]*SynonymDefinition{\n\t\t\"synDoc1\": {\n\t\t\tSynonyms: []string{\"quick\", \"fast\", \"speedy\"},\n\t\t},\n\t\t\"synDoc2\": {\n\t\t\tInput:    []string{\"color\", \"colour\"},\n\t\t\tSynonyms: []string{\"red\", \"green\", \"blue\", \"yellow\", \"brown\"},\n\t\t},\n\t\t\"synDoc3\": {\n\t\t\tInput:    []string{\"animal\", \"creature\"},\n\t\t\tSynonyms: []string{\"fox\", \"wolf\", \"cat\", \"dog\", \"rabbit\"},\n\t\t},\n\t\t\"synDoc4\": {\n\t\t\tSynonyms: []string{\"eats\", \"jumps\", \"runs\", \"barks\", \"hops\"},\n\t\t},\n\t}\n\n\tfor synName, synDef := range synonymDocuments {\n\t\terr := batch.IndexSynonym(synName, synonymCollection, synDef)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\terr = idx.Batch(batch)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsco, err := idx.Advanced()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treader, err := sco.Reader()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = reader.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tthesReader, ok := reader.(index.ThesaurusReader)\n\tif !ok {\n\t\tt.Fatal(\"expected thesaurus reader\")\n\t}\n\n\ttype testStruct struct {\n\t\tqueryTerm        string\n\t\texpectedSynonyms []string\n\t}\n\n\ttestQueries := []testStruct{\n\t\t{\n\t\t\tqueryTerm:        \"quick\",\n\t\t\texpectedSynonyms: []string{\"fast\", \"speedy\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"red\",\n\t\t\texpectedSynonyms: []string{},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"color\",\n\t\t\texpectedSynonyms: []string{\"red\", \"green\", \"blue\", \"yellow\", \"brown\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"colour\",\n\t\t\texpectedSynonyms: []string{\"red\", \"green\", \"blue\", \"yellow\", \"brown\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"animal\",\n\t\t\texpectedSynonyms: []string{\"fox\", \"wolf\", \"cat\", \"dog\", \"rabbit\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"creature\",\n\t\t\texpectedSynonyms: []string{\"fox\", \"wolf\", \"cat\", \"dog\", \"rabbit\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"fox\",\n\t\t\texpectedSynonyms: []string{},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"eats\",\n\t\t\texpectedSynonyms: []string{\"jumps\", \"runs\", \"barks\", \"hops\"},\n\t\t},\n\t\t{\n\t\t\tqueryTerm:        \"jumps\",\n\t\t\texpectedSynonyms: []string{\"eats\", \"runs\", \"barks\", \"hops\"},\n\t\t},\n\t}\n\n\tfor _, test := range testQueries {\n\t\tstr, err := thesReader.ThesaurusTermReader(context.Background(), synonymSourceName, []byte(test.queryTerm))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tvar gotSynonyms []string\n\t\tfor {\n\t\t\tsynonym, err := str.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif synonym == \"\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tgotSynonyms = append(gotSynonyms, string(synonym))\n\t\t}\n\t\tif len(gotSynonyms) != len(test.expectedSynonyms) {\n\t\t\tt.Fatalf(\"expected %d synonyms, got %d\", len(test.expectedSynonyms), len(gotSynonyms))\n\t\t}\n\t\tsort.Strings(gotSynonyms)\n\t\tsort.Strings(test.expectedSynonyms)\n\t\tfor i, syn := range gotSynonyms {\n\t\t\tif syn != test.expectedSynonyms[i] {\n\t\t\t\tt.Fatalf(\"expected synonym %s, got %s\", test.expectedSynonyms[i], syn)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestSynonymSearchQueries(t *testing.T) {\n\ttmpIndexPath := createTmpIndexPath(t)\n\tdefer cleanupTmpIndexPath(t, tmpIndexPath)\n\n\tsynonymCollection := \"collection1\"\n\n\tsynonymSourceName := \"english\"\n\n\tanalyzer := en.AnalyzerName\n\n\tsynonymSourceConfig := map[string]interface{}{\n\t\t\"collection\": synonymCollection,\n\t\t\"analyzer\":   analyzer,\n\t}\n\n\ttextField := mapping.NewTextFieldMapping()\n\ttextField.Analyzer = analyzer\n\ttextField.SynonymSource = synonymSourceName\n\n\timap := mapping.NewIndexMapping()\n\timap.DefaultMapping.AddFieldMappingsAt(\"text\", textField)\n\terr := imap.AddSynonymSource(synonymSourceName, synonymSourceConfig)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = imap.Validate()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidx, err := New(tmpIndexPath, imap)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = idx.Close()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tdocuments := map[string]map[string]interface{}{\n\t\t\"doc1\": {\n\t\t\t\"text\": `The hardworking employee consistently strives to exceed expectations.\n\t\t\t\t\tHis industrious nature makes him a valuable asset to any team.\n\t\t\t\t\tHis conscientious attention to detail ensures that projects are completed efficiently and accurately.\n\t\t\t\t\tHe remains persistent even in the face of challenges.`,\n\t\t},\n\t\t\"doc2\": {\n\t\t\t\"text\": `The tranquil surroundings of the retreat provide a perfect escape from the hustle and bustle of city life. \n\t\t\t\t\tGuests enjoy the peaceful atmosphere, which is perfect for relaxation and rejuvenation. \n\t\t\t\t\tThe calm environment offers the ideal place to meditate and connect with nature. \n\t\t\t\t\tEven the most stressed individuals find themselves feeling relaxed and at ease.`,\n\t\t},\n\t\t\"doc3\": {\n\t\t\t\"text\": `The house was burned down, leaving only a charred shell behind. \n\t\t\t\t\tThe intense heat of the flames caused the walls to warp and the roof to cave in. \n\t\t\t\t\tThe seared remains of the furniture told the story of the blaze. \n\t\t\t\t\tThe incinerated remains left little more than ashes to remember what once was.`,\n\t\t},\n\t\t\"doc4\": {\n\t\t\t\"text\": `The faithful dog followed its owner everywhere, always loyal and steadfast. \n\t\t\t\t\tIt was devoted to protecting its family, and its reliable nature meant it could always be trusted. \n\t\t\t\t\tIn the face of danger, the dog remained calm, knowing its role was to stay vigilant. \n\t\t\t\t\tIts trustworthy companionship provided comfort and security.`,\n\t\t},\n\t\t\"doc5\": {\n\t\t\t\"text\": `The lively market is bustling with activity from morning to night. \n\t\t\t\t\tThe dynamic energy of the crowd fills the air as vendors sell their wares. \n\t\t\t\t\tShoppers wander from stall to stall, captivated by the vibrant colors and energetic atmosphere. \n\t\t\t\t\tThis place is alive with movement and life.`,\n\t\t},\n\t\t\"doc6\": {\n\t\t\t\"text\": `In moments of crisis, bravery shines through. \n\t\t\t\t\tIt takes valor to step forward when others are afraid to act. \n\t\t\t\t\tHeroes are defined by their guts and nerve, taking risks to protect others. \n\t\t\t\t\tBoldness in the face of danger is what sets them apart.`,\n\t\t},\n\t\t\"doc7\": {\n\t\t\t\"text\": `Innovation is the driving force behind progress in every industry. \n\t\t\t\t\tThe company fosters an environment of invention, encouraging creativity at every level. \n\t\t\t\t\tThe focus on novelty and improvement means that ideas are always evolving. \n\t\t\t\t\tThe development of new solutions is at the core of the company's mission.`,\n\t\t},\n\t\t\"doc8\": {\n\t\t\t\"text\": `The blazing sunset cast a radiant glow over the horizon, painting the sky with hues of red and orange. \n\t\t\t\t\tThe intense heat of the day gave way to a fiery display of color. \n\t\t\t\t\tAs the sun set, the glowing light illuminated the landscape, creating a breathtaking scene. \n\t\t\t\t\tThe fiery sky was a sight to behold.`,\n\t\t},\n\t\t\"doc9\": {\n\t\t\t\"text\": `The fertile soil of the valley makes it perfect for farming. \n\t\t\t\t\tThe productive land yields abundant crops year after year. \n\t\t\t\t\tFarmers rely on the rich, fruitful ground to sustain their livelihoods. \n\t\t\t\t\tThe area is known for its plentiful harvests, supporting both local communities and export markets.`,\n\t\t},\n\t\t\"doc10\": {\n\t\t\t\"text\": `The arid desert is a vast, dry expanse with little water or vegetation. \n\t\t\t\t\tThe barren landscape stretches as far as the eye can see, offering little respite from the scorching sun. \n\t\t\t\t\tThe desolate environment is unforgiving to those who venture too far without preparation. \n\t\t\t\t\tThe parched earth cracks under the heat, creating a harsh, unyielding terrain.`,\n\t\t},\n\t\t\"doc11\": {\n\t\t\t\"text\": `The fox is known for its cunning and intelligence. \n\t\t\t\t\tAs a predator, it relies on its sharp instincts to outwit its prey. \n\t\t\t\t\tIts vulpine nature makes it both mysterious and fascinating. \n\t\t\t\t\tThe fox's ability to hunt with precision and stealth is what makes it such a formidable hunter.`,\n\t\t},\n\t\t\"doc12\": {\n\t\t\t\"text\": `The dog is often considered man's best friend due to its loyal nature. \n\t\t\t\t\tAs a companion, the hound provides both protection and affection. \n\t\t\t\t\tThe puppy quickly becomes a member of the family, always by your side. \n\t\t\t\t\tIts playful energy and unshakable loyalty make it a beloved pet.`,\n\t\t},\n\t\t\"doc13\": {\n\t\t\t\"text\": `He worked tirelessly through the night, always persistent in his efforts. \n\t\t\t\t\tHis industrious approach to problem-solving kept the project moving forward. \n\t\t\t\t\tNo matter how difficult the task, he remained focused, always giving his best. \n\t\t\t\t\tHis dedication paid off when the project was completed ahead of schedule.`,\n\t\t},\n\t\t\"doc14\": {\n\t\t\t\"text\": `The river flowed calmly through the valley, its peaceful current offering a sense of tranquility. \n\t\t\t\t\tFishermen relaxed by the banks, enjoying the calm waters that reflected the sky above. \n\t\t\t\t\tThe tranquil nature of the river made it a perfect spot for meditation. \n\t\t\t\t\tAs the day ended, the river's quiet flow brought a sense of peace.`,\n\t\t},\n\t\t\"doc15\": {\n\t\t\t\"text\": `After the fire, all that was left was the charred remains of what once was. \n\t\t\t\t\tThe seared walls of the house told a tragic story. \n\t\t\t\t\tThe intensity of the blaze had burned everything in its path, leaving only the smoldering wreckage behind. \n\t\t\t\t\tThe incinerated objects could not be salvaged, and the damage was beyond repair.`,\n\t\t},\n\t\t\"doc16\": {\n\t\t\t\"text\": `The devoted employee always went above and beyond to complete his tasks. \n\t\t\t\t\tHis steadfast commitment to the company made him a valuable team member. \n\t\t\t\t\tHe was reliable, never failing to meet deadlines. \n\t\t\t\t\tHis trustworthiness earned him the respect of his colleagues, and was considered an\n\t\t\t\t\tingenious expert in his field.`,\n\t\t},\n\t\t\"doc17\": {\n\t\t\t\"text\": `The city is vibrant, full of life and energy. \n\t\t\t\t\tThe dynamic pace of the streets reflects the diverse culture of its inhabitants. \n\t\t\t\t\tPeople from all walks of life contribute to the energetic atmosphere. \n\t\t\t\t\tThe city's lively spirit can be felt in every corner, from the bustling markets to the lively festivals.`,\n\t\t},\n\t\t\"doc18\": {\n\t\t\t\"text\": `In a moment of uncertainty, he made a bold decision that would change his life forever. \n\t\t\t\t\tIt took courage and nerve to take the leap, but his bravery paid off. \n\t\t\t\t\tThe guts to face the unknown allowed him to achieve something remarkable. \n\t\t\t\t\tBeing an bright scholar, the skill he demonstrated inspired those around him.`,\n\t\t},\n\t\t\"doc19\": {\n\t\t\t\"text\": `Innovation is often born from necessity, and the lightbulb is a prime example. \n\t\t\t\t\tThomas Edison's invention changed the world, offering a new way to see the night. \n\t\t\t\t\tThe creativity involved in developing such a groundbreaking product sparked a wave of \n\t\t\t\t\tnovelty in the scientific community. This improvement in technology continues to shape the modern world.\n\t\t\t\t\tHe was a clever academic and a smart researcher.`,\n\t\t},\n\t\t\"doc20\": {\n\t\t\t\"text\": `The fiery volcano erupted with a force that shook the earth. Its radiant lava flowed down the sides, \n\t\t\t\t\tilluminating the night sky. The intense heat from the eruption could be felt miles away, as the \n\t\t\t\t\tglowing lava burned everything in its path. The fiery display was both terrifying and mesmerizing.`,\n\t\t},\n\t}\n\n\tsynonymDocuments := map[string]*SynonymDefinition{\n\t\t\"synDoc1\": {\n\t\t\tSynonyms: []string{\"hardworking\", \"industrious\", \"conscientious\", \"persistent\", \"focused\", \"devoted\"},\n\t\t},\n\t\t\"synDoc2\": {\n\t\t\tSynonyms: []string{\"tranquil\", \"peaceful\", \"calm\", \"relaxed\", \"unruffled\"},\n\t\t},\n\t\t\"synDoc3\": {\n\t\t\tSynonyms: []string{\"burned\", \"charred\", \"seared\", \"incinerated\", \"singed\"},\n\t\t},\n\t\t\"synDoc4\": {\n\t\t\tSynonyms: []string{\"faithful\", \"steadfast\", \"devoted\", \"reliable\", \"trustworthy\"},\n\t\t},\n\t\t\"synDoc5\": {\n\t\t\tSynonyms: []string{\"lively\", \"dynamic\", \"energetic\", \"vivid\", \"vibrating\"},\n\t\t},\n\t\t\"synDoc6\": {\n\t\t\tSynonyms: []string{\"bravery\", \"valor\", \"guts\", \"nerve\", \"boldness\"},\n\t\t},\n\t\t\"synDoc7\": {\n\t\t\tInput:    []string{\"innovation\"},\n\t\t\tSynonyms: []string{\"invention\", \"creativity\", \"novelty\", \"improvement\", \"development\"},\n\t\t},\n\t\t\"synDoc8\": {\n\t\t\tInput:    []string{\"blazing\"},\n\t\t\tSynonyms: []string{\"intense\", \"radiant\", \"burning\", \"fiery\", \"glowing\"},\n\t\t},\n\t\t\"synDoc9\": {\n\t\t\tInput:    []string{\"fertile\"},\n\t\t\tSynonyms: []string{\"productive\", \"fruitful\", \"rich\", \"abundant\", \"plentiful\"},\n\t\t},\n\t\t\"synDoc10\": {\n\t\t\tInput:    []string{\"arid\"},\n\t\t\tSynonyms: []string{\"dry\", \"barren\", \"desolate\", \"parched\", \"unfertile\"},\n\t\t},\n\t\t\"synDoc11\": {\n\t\t\tInput:    []string{\"fox\"},\n\t\t\tSynonyms: []string{\"vulpine\", \"canine\", \"predator\", \"hunter\", \"pursuer\"},\n\t\t},\n\t\t\"synDoc12\": {\n\t\t\tInput:    []string{\"dog\"},\n\t\t\tSynonyms: []string{\"canine\", \"hound\", \"puppy\", \"pup\", \"companion\"},\n\t\t},\n\t\t\"synDoc13\": {\n\t\t\tSynonyms: []string{\"researcher\", \"scientist\", \"scholar\", \"academic\", \"expert\"},\n\t\t},\n\t\t\"synDoc14\": {\n\t\t\tSynonyms: []string{\"bright\", \"clever\", \"ingenious\", \"sharp\", \"astute\", \"smart\"},\n\t\t},\n\t}\n\n\t// Combine both maps into a slice of map entries (as they both have similar structure)\n\tvar combinedDocIDs []string\n\tfor id := range synonymDocuments {\n\t\tcombinedDocIDs = append(combinedDocIDs, id)\n\t}\n\tfor id := range documents {\n\t\tcombinedDocIDs = append(combinedDocIDs, id)\n\t}\n\trand.Shuffle(len(combinedDocIDs), func(i, j int) {\n\t\tcombinedDocIDs[i], combinedDocIDs[j] = combinedDocIDs[j], combinedDocIDs[i]\n\t})\n\n\t// Function to create batches of 5\n\tcreateDocBatches := func(docs []string, batchSize int) [][]string {\n\t\tvar batches [][]string\n\t\tfor i := 0; i < len(docs); i += batchSize {\n\t\t\tend := i + batchSize\n\t\t\tif end > len(docs) {\n\t\t\t\tend = len(docs)\n\t\t\t}\n\t\t\tbatches = append(batches, docs[i:end])\n\t\t}\n\t\treturn batches\n\t}\n\t// Create batches of 5 documents\n\tvar batchSize = 5\n\tdocBatches := createDocBatches(combinedDocIDs, batchSize)\n\tif len(docBatches) == 0 {\n\t\tt.Fatal(\"expected batches\")\n\t}\n\ttotalDocs := 0\n\tfor _, batch := range docBatches {\n\t\ttotalDocs += len(batch)\n\t}\n\tif totalDocs != len(combinedDocIDs) {\n\t\tt.Fatalf(\"expected %d documents, got %d\", len(combinedDocIDs), totalDocs)\n\t}\n\n\tvar batches []*Batch\n\tfor _, docBatch := range docBatches {\n\t\tbatch := idx.NewBatch()\n\t\tfor _, docID := range docBatch {\n\t\t\tif synDef, ok := synonymDocuments[docID]; ok {\n\t\t\t\terr := batch.IndexSynonym(docID, synonymCollection, synDef)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr := batch.Index(docID, documents[docID])\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbatches = append(batches, batch)\n\t}\n\tfor _, batch := range batches {\n\t\terr = idx.Batch(batch)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\ttype testStruct struct {\n\t\tquery      string\n\t\texpectHits []string\n\t}\n\n\ttestQueries := []testStruct{\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"match\": \"hardworking employee\",\n\t\t\t\t\"field\": \"text\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\"doc1\", \"doc13\", \"doc16\", \"doc4\", \"doc7\"},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"match\": \"Hardwork and industrius efforts bring lovely and tranqual moments, with a glazing blow of valour.\",\n\t\t\t\t\"field\": \"text\",\n\t\t\t\t\"fuzziness\": \"auto\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\n\t\t\t\t\"doc1\", \"doc13\", \"doc14\", \"doc15\", \"doc16\",\n\t\t\t\t\"doc17\", \"doc18\", \"doc2\", \"doc20\", \"doc3\",\n\t\t\t\t\"doc4\", \"doc5\", \"doc6\", \"doc7\", \"doc8\", \"doc9\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"prefix\": \"in\",\n\t\t\t\t\"field\": \"text\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\n\t\t\t\t\"doc1\", \"doc11\", \"doc13\", \"doc15\", \"doc16\",\n\t\t\t\t\"doc17\", \"doc18\", \"doc19\", \"doc2\", \"doc20\",\n\t\t\t\t\"doc3\", \"doc4\", \"doc7\", \"doc8\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"prefix\": \"vivid\",\n\t\t\t\t\"field\": \"text\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\n\t\t\t\t\"doc17\", \"doc5\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"match_phrase\": \"smart academic\",\n\t\t\t\t\"field\": \"text\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\"doc16\", \"doc18\", \"doc19\"},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"match_phrase\": \"smrat acedemic\",\n\t\t\t\t\"field\": \"text\",\n\t\t\t\t\"fuzziness\": \"auto\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\"doc16\", \"doc18\", \"doc19\"},\n\t\t},\n\t\t{\n\t\t\tquery: `{\n\t\t\t\t\"wildcard\": \"br*\",\n\t\t\t\t\"field\": \"text\"\n\t\t\t}`,\n\t\t\texpectHits: []string{\"doc11\", \"doc14\", \"doc16\", \"doc18\", \"doc19\", \"doc6\", \"doc8\"},\n\t\t},\n\t}\n\n\trunTestQueries := func(idx Index) error {\n\t\tfor _, dtq := range testQueries {\n\t\t\tq, err := query.ParseQuery([]byte(dtq.query))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tsr := NewSearchRequest(q)\n\t\t\tsr.Highlight = NewHighlightWithStyle(ansi.Name)\n\t\t\tsr.SortBy([]string{\"_id\"})\n\t\t\tsr.Fields = []string{\"*\"}\n\t\t\tsr.Size = 30\n\t\t\tsr.Explain = true\n\t\t\tres, err := idx.Search(sr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif len(res.Hits) != len(dtq.expectHits) {\n\t\t\t\treturn fmt.Errorf(\"expected %d hits, got %d\", len(dtq.expectHits), len(res.Hits))\n\t\t\t}\n\t\t\t// sort the expected hits to match the order of the search results\n\t\t\tsort.Strings(dtq.expectHits)\n\t\t\tfor i, hit := range res.Hits {\n\t\t\t\tif hit.ID != dtq.expectHits[i] {\n\t\t\t\t\treturn fmt.Errorf(\"expected docID %s, got %s\", dtq.expectHits[i], hit.ID)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\terr = runTestQueries(idx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// test with index alias - with 1 batch per index\n\tnumIndexes := len(batches)\n\tindexes := make([]Index, numIndexes)\n\tindexesPath := make([]string, numIndexes)\n\tfor i := 0; i < numIndexes; i++ {\n\t\ttmpIndexPath := createTmpIndexPath(t)\n\t\tidx, err := New(tmpIndexPath, imap)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\terr = idx.Batch(batches[i])\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tindexes[i] = idx\n\t\tindexesPath[i] = tmpIndexPath\n\t}\n\tdefer func() {\n\t\tfor i := 0; i < numIndexes; i++ {\n\t\t\terr = indexes[i].Close()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tcleanupTmpIndexPath(t, indexesPath[i])\n\t\t}\n\t}()\n\talias := NewIndexAlias(indexes...)\n\talias.SetIndexMapping(imap)\n\terr = runTestQueries(alias)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// test with multi-level alias now with two index per alias\n\t// and having any extra index being in the final alias\n\tnumAliases := numIndexes / 2\n\textraIndex := numIndexes % 2\n\taliases := make([]IndexAlias, numAliases)\n\tfor i := 0; i < numAliases; i++ {\n\t\talias := NewIndexAlias(indexes[i*2], indexes[i*2+1])\n\t\taliases[i] = alias\n\t}\n\tif extraIndex > 0 {\n\t\taliases[numAliases-1].Add(indexes[numIndexes-1])\n\t}\n\talias = NewIndexAlias()\n\talias.SetIndexMapping(imap)\n\tfor i := 0; i < numAliases; i++ {\n\t\talias.Add(aliases[i])\n\t}\n\terr = runTestQueries(alias)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "size",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}