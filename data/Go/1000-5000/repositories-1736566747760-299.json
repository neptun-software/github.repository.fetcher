{
  "metadata": {
    "timestamp": 1736566747760,
    "page": 299,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "riverqueue/river",
      "stars": 3706,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.087890625,
          "content": "/go.work.sum\n/river\n/internal/cmd/riverbench/riverbench\n/internal/cmd/testdbman/testdbman\n"
        },
        {
          "name": ".golangci.yaml",
          "type": "blob",
          "size": 3.1328125,
          "content": "issues:\n  exclude:\n    - 'Error return value of .(\\w+\\.Rollback(.*)). is not checked'\n\nlinters:\n  presets:\n    - bugs\n    - comment\n    - format\n    - performance\n    - style\n    - test\n    - unused\n\n  disable:\n    # disabled, but which we should enable after dropping support for Go 1.21 as\n    # they're kind of a good idea\n    - copyloopvar # lints to make sure that loop variables are _not_ captured since it's not required in Go 1.22+\n    - intrange # encourages for loops to range over integers like `for i := range(5)` instead of a C-style for\n\n    # disabled, but which we should enable with discussion\n    - wrapcheck # checks that errors are wrapped; currently not done anywhere\n\n    # disabled because we're not compliant, but which we should think about\n    - exhaustruct # checks that properties in structs are exhaustively defined; may be a good idea\n    - testpackage # requires tests in test packages like `river_test`\n\n    # disabled because they're annoying/bad\n    - interfacebloat # we do in fact want >10 methods on the Adapter interface or wherever we see fit.\n    - godox # bans TODO statements; total non-starter at the moment\n    - err113 # wants all errors to be defined as variables at the package level; quite obnoxious\n    - mnd # detects \"magic numbers\", which it defines as any number; annoying\n    - ireturn # bans returning interfaces; questionable as is, but also buggy as hell; very, very annoying\n    - lll # restricts maximum line length; annoying\n    - nlreturn # requires a blank line before returns; annoying\n    - wsl # a bunch of style/whitespace stuff; annoying\n\nlinters-settings:\n  depguard:\n    rules:\n      all:\n        files: [\"$all\"]\n        allow:\n        deny:\n          - desc: \"Use `github.com/google/uuid` package for UUIDs instead.\"\n            pkg: \"github.com/xtgo/uuid\"\n      not-test:\n        files: [\"!$test\"]\n        deny:\n          - desc: \"Don't use `dbadaptertest` package outside of test environments.\"\n            pkg: \"github.com/riverqueue/river/internal/dbadaptertest\"\n          - desc: \"Don't use `riverinternaltest` package outside of test environments.\"\n            pkg: \"github.com/riverqueue/river/internal/riverinternaltest\"\n\n  forbidigo:\n    forbid:\n      - msg: \"Use `require` variants instead.\"\n        p: '^assert\\.'\n      - msg: \"Use `Func` suffix for function variables instead.\"\n        p: 'Fn\\b'\n      - msg: \"Use built-in `max` function instead.\"\n        p: '\\bmath\\.Max\\b'\n      - msg: \"Use built-in `min` function instead.\"\n        p: '\\bmath\\.Min\\b'\n\n  gci:\n    sections:\n      - Standard\n      - Default\n      - Prefix(github.com/riverqueue)\n\n  gomoddirectives:\n    replace-local: true\n\n  gosec:\n    excludes:\n      - G404 # use of non-crypto random; overly broad for our use case\n\n  revive:\n    rules:\n      - name: unused-parameter\n        disabled: true\n\n  tagliatelle:\n    case:\n      rules:\n        json: snake\n\n  testifylint:\n    enable-all: true\n    disable:\n      - go-require\n\n  varnamelen:\n    ignore-names:\n      - db\n      - eg\n      - f\n      - i\n      - id\n      - j\n      - mu\n      - sb # common convention for string builder\n      - t\n      - tt # common convention for table tests\n      - tx\n      - wg\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 45.34375,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [0.15.0] - 2024-12-26\n\n### Added\n\n- The River CLI will now respect the standard set of `PG*` environment variables like `PGHOST`, `PGPORT`, `PGDATABASE`, `PGUSER`, `PGPASSWORD`, and `PGSSLMODE` to configure a target database when the `--database-url` parameter is omitted. [PR #702](https://github.com/riverqueue/river/pull/702).\n- Add missing doc for `JobRow.UniqueStates` + reveal `rivertype.UniqueOptsByStateDefault()` to provide access to the default set of unique job states. [PR #707](https://github.com/riverqueue/river/pull/707).\n\n### Changed\n\n- Sleep durations are now logged as Go-like duration strings (e.g. \"10s\") in either text or JSON instead of duration strings in text and nanoseconds in JSON. [PR #699](https://github.com/riverqueue/river/pull/699).\n- Altered the migration comments from `river migrate-get` to include the \"line\" of the migration being run (`main`, or for River Pro `workflow` and `sequence`) to make them more distinguishable. [PR #703](https://github.com/riverqueue/river/pull/703).\n- Fewer slice allocations during unique insertions. [PR #705](https://github.com/riverqueue/river/pull/705).\n\n### Fixed\n\n- Exponential backoffs at degenerately high job attempts (>= 310) no longer risk overflowing `time.Duration`. [PR #698](https://github.com/riverqueue/river/pull/698).\n\n## [0.14.3] - 2024-12-14\n\n### Changed\n\n- Dropped internal random generators in favor of `math/rand/v2`, which will have the effect of making code fully incompatible with Go 1.21 (`go.mod` has specified a minimum of 1.22 for some time already though). [PR #691](https://github.com/riverqueue/river/pull/691).\n\n### Fixed\n\n- 006 migration now tolerates previous existence of a `unique_states` column in case it was added separately so that the new index could be raised with `CONCURRENTLY`. [PR #690](https://github.com/riverqueue/river/pull/690).\n\n## [0.14.2] - 2024-11-16\n\n### Fixed\n\n- Cancellation of running jobs relied on a channel that was only being received when in the job fetch routine, meaning that jobs which were cancelled would not be cancelled until the next scheduled fetch. This was fixed by also receiving from the job cancellation channel when in the main producer loop, even if no fetches are happening. [PR #678](https://github.com/riverqueue/river/pull/678).\n- Job insert middleware were not being utilized for periodic jobs. This insertion path has been refactored to rely on the unified insertion path from the client. Fixes #675. [PR #679](https://github.com/riverqueue/river/pull/679).\n\n## [0.14.1] - 2024-11-04\n\n### Fixed\n\n- In [PR #663](https://github.com/riverqueue/river/pull/663) the client was changed to be more aggressive about re-fetching when it had previously fetched a full batch. Unfortunately a clause was missed, which resulted in the client being more aggressive any time even a single job was fetched on the previous attempt. This was corrected with a conditional to ensure it only happens when the last fetch was full. [PR #668](https://github.com/riverqueue/river/pull/668).\n\n## [0.14.0] - 2024-11-03\n\n### Added\n\n- Expose `JobCancelError` and `JobSnoozeError` types to more easily facilitate testing. [PR #665](https://github.com/riverqueue/river/pull/665).\n\n### Changed\n\n- Tune the client to be more aggressive about fetching when it just fetched a full batch of jobs, or when it skipped its previous triggered fetch because it was already full. This should bring more consistent throughput to poll-only mode and in cases where there is a backlog of existing jobs but new ones aren't being actively inserted. This will result in increased fetch load on many installations, with the benefit of increased throughput. As before, `FetchCooldown` still limits how frequently these fetches can occur on each client and can be increased to reduce the amount of fetch querying. Thanks Chris Gaffney ([@gaffneyc](https://github.com/gaffneyc)) for the idea, initial implementation, and benchmarks. [PR #663](https://github.com/riverqueue/river/pull/663).\n\n### Fixed\n\n- `riverpgxv5` driver: `Hijack()` the underlying listener connection as soon as it is acquired from the `pgxpool.Pool` in order to prevent the pool from automatically closing it after it reaches its max age. A max lifetime makes sense in the context of a pool with many conns, but a long-lived listener does not need a max lifetime as long as it can ensure the conn remains healthy. [PR #661](https://github.com/riverqueue/river/pull/661).\n\n## [0.13.0] - 2024-10-07\n\n‚ö†Ô∏è Version 0.13.0 removes the original advisory lock based unique jobs implementation that was deprecated in v0.12.0. See details in the note below or the v0.12.0 release notes.\n\n### Added\n\n- A middleware system was added for job insertion and execution, providing the ability to extract shared functionality across workers. Both `JobInsertMiddleware` and `WorkerMiddleware` can be configured globally on the `Client`, and `WorkerMiddleware` can also be added on a per-worker basis using the new `Middleware` method on `Worker[T]`. Middleware can be useful for logging, telemetry, or for building higher level abstractions on top of base River functionality.\n\n  Despite the interface expansion, users should not encounter any breakage if they're embedding the `WorkerDefaults` type in their workers as recommended. [PR #632](https://github.com/riverqueue/river/pull/632).\n\n### Changed\n\n- **Breaking change:** The advisory lock unique jobs implementation which was deprecated in v0.12.0 has been removed. Users of that feature should first upgrade to v0.12.1 to ensure they don't see any warning logs about using the deprecated advisory lock uniqueness. The new, faster unique implementation will be used automatically as long as the `UniqueOpts.ByState` list hasn't been customized to remove [required states](https://riverqueue.com/docs/unique-jobs#unique-by-state) (`pending`, `scheduled`, `available`, and `running`). As of this release, customizing `ByState` without these required states returns an error. [PR #614](https://github.com/riverqueue/river/pull/614).\n- Single job inserts are now unified under the hood to use the `InsertMany` bulk insert query. This should not be noticeable to users, and the unified code path will make it easier to build new features going forward. [PR #614](https://github.com/riverqueue/river/pull/614).\n\n### Fixed\n\n- Allow `river.JobCancel` to accept a `nil` error as input without panicking. [PR #634](https://github.com/riverqueue/river/pull/634).\n\n## [0.12.1] - 2024-09-26\n\n### Changed\n\n- The `BatchCompleter` that marks jobs as completed can now batch database updates for _all_ states of jobs that have finished execution. Prior to this change, only `completed` jobs were batched into a single `UPDATE` call, while jobs moving to any other state used a single `UPDATE` per job. This change should significantly reduce database and pool contention on high volume system when jobs get retried, snoozed, cancelled, or discarded following execution. [PR #617](https://github.com/riverqueue/river/pull/617).\n\n### Fixed\n\n- Unique job changes from v0.12.0 / [PR #590](https://github.com/riverqueue/river/pull/590) introduced a bug with scheduled or retryable unique jobs where they could be considered in conflict with themselves and moved to `discarded` by mistake. There was also a possibility of a broken job scheduler if duplicate `retryable` unique jobs were attempted to be scheduled at the same time. The job scheduling query was corrected to address these issues along with missing test coverage. [PR #619](https://github.com/riverqueue/river/pull/619).\n\n## [0.12.0] - 2024-09-23\n\n‚ö†Ô∏è Version 0.12.0 contains a new database migration, version 6. See [documentation on running River migrations](https://riverqueue.com/docs/migrations). If migrating with the CLI, make sure to update it to its latest version:\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-up --database-url \"$DATABASE_URL\"\n```\n\nIf not using River's internal migration system, the raw SQL can alternatively be dumped with:\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-get --version 6 --up > river6.up.sql\nriver migrate-get --version 6 --down > river6.down.sql\n```\n\nThe migration **includes a new index**. Users with a very large job table may want to consider raising the index separately using `CONCURRENTLY` (which must be run outside of a transaction), then run `river migrate-up` to finalize the process (it will tolerate an index that already exists):\n\n```sql\nALTER TABLE river_job ADD COLUMN unique_states BIT(8);\n\nCREATE UNIQUE INDEX CONCURRENTLY river_job_unique_idx ON river_job (unique_key)\n    WHERE unique_key IS NOT NULL\n      AND unique_states IS NOT NULL\n      AND river_job_state_in_bitmask(unique_states, state);\n```\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-up --database-url \"$DATABASE_URL\"\n```\n\n## Added\n\n- `rivertest.WorkContext`, a test function that can be used to initialize a context to test a `JobArgs.Work` implementation that will have a client set to context for use with `river.ClientFromContext`. [PR #526](https://github.com/riverqueue/river/pull/526).\n- A new `river migrate-list` command is available which lists available migrations and which version a target database is migrated to. [PR #534](https://github.com/riverqueue/river/pull/534).\n- `river version` or `river --version` now prints River version information. [PR #537](https://github.com/riverqueue/river/pull/537).\n- `Config.JobCleanerTimeout` was added to allow configuration of the job cleaner query timeout. In some deployments with millions of stale jobs, the cleaner may not be able to complete its query within the default 30 seconds. [PR #576](https://github.com/riverqueue/river/pull/576).\n\n### Changed\n\n‚ö†Ô∏è Version 0.12.0 has two small breaking changes, one for `InsertMany` and one in `rivermigrate`. As before, we try never to make breaking changes, but these ones were deemed worth it because of minimal impact and to help avoid panics.\n\n- **Breaking change:** `Client.InsertMany` / `InsertManyTx` now return the inserted rows rather than merely returning a count of the inserted rows. The new implementations no longer use Postgres' `COPY FROM` protocol in order to facilitate return values.\n\n  Users who relied on the return count can merely wrap the returned rows in a `len()` to return to that behavior, or you can continue using the old APIs using their new names `InsertManyFast` and `InsertManyFastTx`. [PR #589](https://github.com/riverqueue/river/pull/589).\n\n- **Breaking change:** `rivermigrate.New` now returns a possible error along with a migrator. An error may be returned, for example, when a migration line is configured that doesn't exist. [PR #558](https://github.com/riverqueue/river/pull/558).\n\n  ```go\n  # before\n  migrator := rivermigrate.New(riverpgxv5.New(dbPool), nil)\n\n  # after\n  migrator, err := rivermigrate.New(riverpgxv5.New(dbPool), nil)\n  if err != nil {\n      // handle error\n  }\n  ```\n\n- Unique jobs have been improved to allow bulk insertion of unique jobs via `InsertMany` / `InsertManyTx`, and to allow customizing the `ByState` list to add or remove certain states. This enables users to expand the set of unique states to also include `cancelled` and `discarded` jobs, or to remove `retryable` from uniqueness consideration. This updated implementation maintains the speed advantage of the newer index-backed uniqueness system, while allowing some flexibility in which job states.\n\n  Unique jobs utilizing `ByArgs` can now also opt to have a subset of the job's arguments considered for uniqueness. For example, you could choose to consider only the `customer_id` field while ignoring the `trace_id` field:\n\n  ```go\n  type MyJobArgs {\n    CustomerID string `json:\"customer_id\" river:\"unique`\n    TraceID string `json:\"trace_id\"`\n  }\n  ```\n\n  Any fields considered in uniqueness are also sorted alphabetically in order to guarantee a consistent result, even if the encoded JSON isn't sorted consistently. For example `encoding/json` encodes struct fields in their defined order, so merely reordering struct fields would previously have been enough to cause a new job to not be considered identical to a pre-existing one with different JSON order.\n\n  The `UniqueOpts` type also gains an `ExcludeKind` option for cases where uniqueness needs to be guaranteed across multiple job types.\n\n  In-flight unique jobs using the previous designs will continue to be executed successfully with these changes, so there should be no need for downtime as part of the migration. However the v6 migration adds a new unique job index while also removing the old one, so users with in-flight unique jobs may also wish to avoid removing the old index until the new River release has been deployed in order to guarantee that jobs aren't duplicated by old River code once that index is removed.\n\n  **Deprecated**: The original unique jobs implementation which relied on advisory locks has been deprecated, but not yet removed. The only way to trigger this old code path is with a single insert (`Insert`/`InsertTx`) and using `UniqueOpts.ByState` with a custom list of states that omits some of the now-required states for unique jobs. Specifically, `pending`, `scheduled`, `available`, and `running` can not be removed from the `ByState` list with the new implementation. These are included in the default list so only the places which customize this attribute need to be updated to opt into the new (much faster) unique jobs. The advisory lock unique implementation will be removed in an upcoming release, and until then emits warning level logs when it's used.\n\n  [PR #590](https://github.com/riverqueue/river/pull/590).\n\n- **Deprecated**: The `MigrateTx` method of `rivermigrate` has been deprecated. It turns out there are certain combinations of schema changes which cannot be run within a single transaction, and the migrator now prefers to run each migration in its own transaction, one-at-a-time. `MigrateTx` will be removed in future version.\n\n- The migrator now produces a better error in case of a non-existent migration line including suggestions for known migration lines that are similar in name to the invalid one. [PR #558](https://github.com/riverqueue/river/pull/558).\n\n## Fixed\n\n- Fixed a panic that'd occur if `StopAndCancel` was invoked before a client was started. [PR #557](https://github.com/riverqueue/river/pull/557).\n- A `PeriodicJobConstructor` should be able to return `nil` `JobArgs` if it wishes to not have any job inserted. However, this was either never working or was broken at some point. It's now fixed. Thanks [@semanser](https://github.com/semanser)! [PR #572](https://github.com/riverqueue/river/pull/572).\n- Fixed a nil pointer exception if `Client.Subscribe` was called when the client had no configured workers (it still, panics with a more instructive error message now). [PR #599](https://github.com/riverqueue/river/pull/599).\n\n## [0.11.4] - 2024-08-20\n\n### Fixed\n\n- Fixed release script that caused CLI to become uninstallable because its reference to `rivershared` wasn't updated. [PR #541](https://github.com/riverqueue/river/pull/541).\n\n## [0.11.3] - 2024-08-19\n\n### Changed\n\n- Producer's logs are quieter unless jobs are actively being worked. [PR #529](https://github.com/riverqueue/river/pull/529).\n\n### Fixed\n\n- River CLI now accepts `postgresql://` URL schemes in addition to `postgres://`. [PR #532](https://github.com/riverqueue/river/pull/532).\n\n## [0.11.2] - 2024-08-08\n\n### Fixed\n\n- Derive all internal contexts from user-provided `Client` context. This includes the job fetch context, notifier unlisten, and completer. [PR #514](https://github.com/riverqueue/river/pull/514).\n- Lowered the `go` directives in `go.mod` to Go 1.21, which River aims to support. A more modern version of Go is specified with the `toolchain` directive. This should provide more flexibility on the minimum required Go version for programs importing River. [PR #522](https://github.com/riverqueue/river/pull/522).\n\n## [0.11.1] - 2024-08-05\n\n### Fixed\n\n- `database/sql` driver: fix default value of `scheduled_at` for `InsertManyTx` when it is not specified in `InsertOpts`. [PR #504](https://github.com/riverqueue/river/pull/504).\n- Change `ColumnExists` query to respect `search_path`, thereby allowing migrations to be runnable outside of default schema. [PR #505](https://github.com/riverqueue/river/pull/505).\n\n## [0.11.0] - 2024-08-02\n\n### Added\n\n- Expose `Driver` on `Client` for additional River Pro integrations. This is not a stable API and should generally not be used by others. [PR #497](https://github.com/riverqueue/river/pull/497).\n\n## [0.10.2] - 2024-07-31\n\n### Fixed\n\n- Include `pending` state in `JobListParams` by default so pending jobs are included in `JobList` / `JobListTx` results. [PR #477](https://github.com/riverqueue/river/pull/477).\n- Quote strings when using `Client.JobList` functions with the `database/sql` driver. [PR #481](https://github.com/riverqueue/river/pull/481).\n- Remove use of `filepath` for interacting with embedded migration files, fixing the migration CLI for Windows. [PR #485](https://github.com/riverqueue/river/pull/485).\n- Respect `ScheduledAt` if set to a non-zero value by `JobArgsWithInsertOpts`. This allows for job arg definitions to utilize custom logic at the args level for determining when the job should be scheduled. [PR #487](https://github.com/riverqueue/river/pull/487).\n\n## [0.10.1] - 2024-07-23\n\n### Fixed\n\n- Migration version 005 has been altered so that it can run even if the `river_migration` table isn't present, making it more friendly for projects that aren't using River's internal migration system. [PR #465](https://github.com/riverqueue/river/pull/465).\n\n## [0.10.0] - 2024-07-19\n\n‚ö†Ô∏è Version 0.10.0 contains a new database migration, version 5. See [documentation on running River migrations](https://riverqueue.com/docs/migrations). If migrating with the CLI, make sure to update it to its latest version:\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-up --database-url \"$DATABASE_URL\"\n```\n\nIf not using River's internal migration system, the raw SQL can alternatively be dumped with:\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-get --version 5 --up > river5.up.sql\nriver migrate-get --version 5 --down > river5.down.sql\n```\n\nThe migration **includes a new index**. Users with a very large job table may want to consider raising the index separately using `CONCURRENTLY` (which must be run outside of a transaction), then run `river migrate-up` to finalize the process (it will tolerate an index that already exists):\n\n```sql\nALTER TABLE river_job\n    ADD COLUMN unique_key bytea;\n\nCREATE UNIQUE INDEX CONCURRENTLY river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;\n```\n\n```shell\ngo install github.com/riverqueue/river/cmd/river@latest\nriver migrate-up --database-url \"$DATABASE_URL\"\n```\n\n### Added\n\n- Fully functional driver for `database/sql` for use with packages like Bun and GORM. [PR #351](https://github.com/riverqueue/river/pull/351).\n- Queues can be added after a client is initialized using `client.Queues().Add(queueName string, queueConfig QueueConfig)`. [PR #410](https://github.com/riverqueue/river/pull/410).\n- Migration that adds a `line` column to the `river_migration` table so that it can support multiple migration lines. [PR #435](https://github.com/riverqueue/river/pull/435).\n- `--line` flag added to the River CLI. [PR #454](https://github.com/riverqueue/river/pull/454).\n\n### Changed\n\n- Tags are now limited to 255 characters in length, and should match the regex `\\A[\\w][\\w\\-]+[\\w]\\z` (importantly, they can't contain commas). [PR #351](https://github.com/riverqueue/river/pull/351).\n- Many info logging statements have been demoted to debug level. [PR #452](https://github.com/riverqueue/river/pull/452).\n- `pending` is now part of the default set of unique job states. [PR #461](https://github.com/riverqueue/river/pull/461).\n\n## [0.9.0] - 2024-07-04\n\n### Added\n\n- `Config.TestOnly` has been added. It disables various features in the River client like staggered maintenance service start that are useful in production, but may be somewhat harmful in tests because they make start/stop slower. [PR #414](https://github.com/riverqueue/river/pull/414).\n\n### Changed\n\n‚ö†Ô∏è Version 0.9.0 has a small breaking change in `ErrorHandler`. As before, we try never to make breaking changes, but this one was deemed quite important because `ErrorHandler` was fundamentally lacking important functionality.\n\n- **Breaking change:** Add stack trace to `ErrorHandler.HandlePanicFunc`. Fixing code only requires adding a new `trace string` argument to `HandlePanicFunc`. [PR #423](https://github.com/riverqueue/river/pull/423).\n\n  ```go\n  # before\n  HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any) *ErrorHandlerResult\n\n  # after\n  HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult\n  ```\n\n### Fixed\n\n- Pausing or resuming a queue that was already paused or not paused respectively no longer returns `rivertype.ErrNotFound`. The same goes for pausing or resuming using the all queues string (`*`) when no queues are in the database (previously that also returned `rivertype.ErrNotFound`). [PR #408](https://github.com/riverqueue/river/pull/408).\n- Fix a bug where periodic job constructors were only called once when adding the periodic job rather than being invoked every time the periodic job is scheduled. [PR #420](https://github.com/riverqueue/river/pull/420).\n\n## [0.8.0] - 2024-06-25\n\n### Added\n\n- Add transaction variants for queue-related client functions: `QueueGetTx`, `QueueListTx`, `QueuePauseTx`, and `QueueResumeTx`. [PR #402](https://github.com/riverqueue/river/pull/402).\n\n### Fixed\n\n- Fix possible Client shutdown panics if the user-provided context is cancelled while jobs are still running. [PR #401](https://github.com/riverqueue/river/pull/401).\n\n## [0.7.0] - 2024-06-13\n\n### Added\n\n- The default max attempts of 25 can now be customized on a per-client basis using `Config.MaxAttempts`. This is in addition to the ability to customize at the job type level with `JobArgs`, or on a per-job basis using `InsertOpts`. [PR #383](https://github.com/riverqueue/river/pull/383).\n- Add `JobDelete` / `JobDeleteTx` APIs on `Client` to allow permanently deleting any job that's not currently running. [PR #390](https://github.com/riverqueue/river/pull/390).\n\n### Fixed\n\n- Fix `StopAndCancel` to not hang if called in parallel to an ongoing `Stop` call. [PR #376](https://github.com/riverqueue/river/pull/376).\n\n## [0.6.1] - 2024-05-21\n\n### Fixed\n\n- River now considers per-worker timeout overrides when rescuing jobs so that jobs with a long custom timeout won't be rescued prematurely. [PR #350](https://github.com/riverqueue/river/pull/350).\n- River CLI now exits with status 1 in the case of a problem with commands or flags, like an unknown command or missing required flag. [PR #363](https://github.com/riverqueue/river/pull/363).\n- Fix migration version 4 (from 0.5.0) so that the up migration can be re-run after it was originally rolled back. [PR #364](https://github.com/riverqueue/river/pull/364).\n\n## [0.6.0] - 2024-05-08\n\n### Added\n\n- `RequireNotInserted` test helper (in addition to the existing `RequireInserted`) that verifies that a job with matching conditions was _not_ inserted. [PR #237](https://github.com/riverqueue/river/pull/237).\n\n### Changed\n\n- The periodic job enqueuer now sets `scheduled_at` of inserted jobs to the more precise time of when they were scheduled to run, as opposed to when they were inserted. [PR #341](https://github.com/riverqueue/river/pull/341).\n\n### Fixed\n\n- Remove use of `github.com/lib/pq`, making it once again a test-only dependency. [PR #337](https://github.com/riverqueue/river/pull/337).\n\n## [0.5.0] - 2024-05-03\n\n‚ö†Ô∏è Version 0.5.0 contains a new database migration, version 4. This migration is backward compatible with any River installation running the v3 migration. Be sure to run the v4 migration prior to deploying the code from this release.\n\n### Added\n\n- Add `pending` job state. This is currently unused, but will be used to build higher level functionality for staging jobs that are not yet ready to run (for some reason other than their scheduled time being in the future). Pending jobs will never be run or deleted and must first be moved to another state by external code. [PR #301](https://github.com/riverqueue/river/pull/301).\n- Queue status tracking, pause and resume. [PR #301](https://github.com/riverqueue/river/pull/301).\n\n  A useful operational lever is the ability to pause and resume a queue without shutting down clients. In addition to pause/resume being a feature request from [#54](https://github.com/riverqueue/river/pull/54), as part of the work on River's UI it's been useful to list out the active queues so that they can be displayed and manipulated.\n\n  A new `river_queue` table is introduced in the v4 migration for this purpose. Upon startup, every producer in each River `Client` will make an `UPSERT` query to the database to either register the queue as being active, or if it already exists it will instead bump the timestamp to keep it active. This query will be run periodically in each producer as long as the `Client` is alive, even if the queue is paused. A separate query will delete/purge any queues which have not been active in awhile (currently fixed to 24 hours).\n\n  `QueuePause` and `QueueResume` APIs have been introduced to `Client` pause and resume a single queue by name, or _all_ queues using the special `*` value. Each producer will watch for notifications on the relevant `LISTEN/NOTIFY` topic unless operating in poll-only mode, in which case they will periodically poll for changes to their queue record in the database.\n\n### Changed\n\n- Job insert notifications are now handled within application code rather than within the database using triggers. [PR #301](https://github.com/riverqueue/river/pull/301).\n\n  The initial design for River utilized a trigger on job insert that issued notifications (`NOTIFY`) so that listening clients could quickly pick up the work if they were idle. While this is good for lowering latency, it does have the side effect of emitting a large amount of notifications any time there are lots of jobs being inserted. This adds overhead, particularly to high-throughput installations.\n\n  To improve this situation and reduce overhead in high-throughput installations, the notifications have been refactored to be emitted at the application level. A client-level debouncer ensures that these notifications are not emitted more often than they could be useful. If a queue is due for an insert notification (on a particular Postgres schema), the notification is piggy-backed onto the insert query within the transaction. While this has the impact of increasing insert latency for a certain percentage of cases, the effect should be small.\n\n  Additionally, initial releases of River did not properly scope notification topics within the global `LISTEN/NOTIFY` namespace. If two River installations were operating on the same Postgres database but within different schemas (search paths), their notifications would be emitted on a shared topic name. This is no longer the case and all notifications are prefixed with a `{schema_name}.` string.\n\n- Add `NOT NULL` constraints to the database for `river_job.args` and `river_job.metadata`. Normal code paths should never have allowed for null values any way, but this constraint further strengthens the guarantee. [PR #301](https://github.com/riverqueue/river/pull/301).\n- Stricter constraint on `river_job.finalized_at` to ensure it is only set when paired with a finalized state (completed, discarded, cancelled). Normal code paths should never have allowed for invalid values any way, but this constraint further strengthens the guarantee. [PR #301](https://github.com/riverqueue/river/pull/301).\n\n## [0.4.1] - 2024-04-22\n\n### Fixed\n\n- Update job state references in `./cmd/river` and some documentation to `rivertype`. Thanks Danny Hermes (@dhermes)! üôèüèª [PR #315](https://github.com/riverqueue/river/pull/315).\n\n## [0.4.0] - 2024-04-20\n\n### Changed\n\n‚ö†Ô∏è Version 0.4.0 has a number of small breaking changes which we've decided to release all as part of a single version. More breaking changes in one release is inconvenient, but we've tried to coordinate them in hopes that any future breaking changes will be non-existent or very rare. All changes will get picked up by the Go compiler, and each one should be quite easy to fix. The changes don't apply to any of the most common core APIs, and likely many projects won't have to change any code.\n\n- **Breaking change:** There are a number of small breaking changes in the job list API using `JobList`/`JobListTx`:\n  - Now support querying jobs by a list of Job Kinds and States. Also allows for filtering by specific timestamp values. Thank you Jos Kraaijeveld (@thatjos)! üôèüèª [PR #236](https://github.com/riverqueue/river/pull/236).\n  - Job listing now defaults to ordering by job ID (`JobListOrderByID`) instead of a job timestamp dependent on requested job state. The previous ordering behavior is still available with `NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc)`. [PR #307](https://github.com/riverqueue/river/pull/307).\n  - The function `JobListCursorFromJob` no longer needs a sort order parameter. Instead, sort order is determined based on the job list parameters that the cursor is subsequently used with. [PR #307](https://github.com/riverqueue/river/pull/307).\n- **Breaking change:** Client `Insert` and `InsertTx` functions now return a `JobInsertResult` struct instead of a `JobRow`. This allows the result to include metadata like the new `UniqueSkippedAsDuplicate` property, so callers can tell whether an inserted job was skipped due to unique constraint. [PR #292](https://github.com/riverqueue/river/pull/292).\n- **Breaking change:** Client `InsertMany` and `InsertManyTx` now return number of jobs inserted as `int` instead of `int64`. This change was made to make the type in use a little more idiomatic. [PR #293](https://github.com/riverqueue/river/pull/293).\n- **Breaking change:** `river.JobState*` type aliases have been removed. All job state constants should be accessed through `rivertype.JobState*` instead. [PR #300](https://github.com/riverqueue/river/pull/300).\n\nSee also the [0.4.0 release blog post](https://riverqueue.com/blog/a-few-breaking-changes) with code samples and rationale behind various changes.\n\n## [0.3.0] - 2024-04-15\n\n### Added\n\n- The River client now supports \"poll only\" mode with `Config.PollOnly` which makes it avoid issuing `LISTEN` statements to wait for new events like a leadership resignation or new job available. The program instead polls periodically to look for changes. A leader resigning or a new job being available will be noticed less quickly, but `PollOnly` potentially makes River operable on systems without listen/notify support, like PgBouncer operating in transaction pooling mode. [PR #281](https://github.com/riverqueue/river/pull/281).\n- Added `rivertype.JobStates()` that returns the full list of possible job states. [PR #297](https://github.com/riverqueue/river/pull/297).\n\n## [0.2.0] - 2024-03-28\n\n### Added\n\n- New periodic jobs can now be added after a client's already started using `Client.PeriodicJobs().Add()` and removed with `Remove()`. [PR #288](https://github.com/riverqueue/river/pull/288).\n\n### Changed\n\n- The level of some of River's common log statements has changed, most often demoting `info` statements to `debug` so that `info`-level logging is overall less verbose. [PR #275](https://github.com/riverqueue/river/pull/275).\n\n### Fixed\n\n- Fixed a bug in the (log-only for now) reindexer service in which it might repeat its work loop multiple times unexpectedly while stopping. [PR #280](https://github.com/riverqueue/river/pull/280).\n- Periodic job enqueuer now bases next run times on each periodic job's last target run time, instead of the time at which the enqueuer is currently running. This is a small difference that will be unnoticeable for most purposes, but makes scheduling of jobs with short cron frequencies a little more accurate. [PR #284](https://github.com/riverqueue/river/pull/284).\n- Fixed a bug in the elector in which it was possible for a resigning, but not completely stopped, elector to reelect despite having just resigned. [PR #286](https://github.com/riverqueue/river/pull/286).\n\n## [0.1.0] - 2024-03-17\n\nAlthough it comes with a number of improvements, there's nothing particularly notable about version 0.1.0. Until now we've only been incrementing the patch version given the project's nascent nature, but from here on we'll try to adhere more closely to semantic versioning, using the patch version for bug fixes, and incrementing the minor version when new functionality is added.\n\n### Added\n\n- The River CLI now supports `river bench` to benchmark River's job throughput against a database. [PR #254](https://github.com/riverqueue/river/pull/254).\n- The River CLI now has a `river migrate-get` command to dump SQL for River migrations for use in alternative migration frameworks. Use it like `river migrate-get --up --version 3 > version3.up.sql`. [PR #273](https://github.com/riverqueue/river/pull/273).\n- The River CLI's `migrate-down` and `migrate-up` options get two new options for `--dry-run` and `--show-sql`. They can be combined to easily run a preflight check on a River upgrade to see which migration commands would be run on a database, but without actually running them. [PR #273](https://github.com/riverqueue/river/pull/273).\n- The River client gets a new `Client.SubscribeConfig` function that lets a subscriber specify the maximum size of their subscription channel. [PR #258](https://github.com/riverqueue/river/pull/258).\n\n### Changed\n\n- River uses a new job completer that batches up completion work so that large numbers of them can be performed more efficiently. In a purely synthetic (i.e. mostly unrealistic) benchmark, River's job throughput increases ~4.5x. [PR #258](https://github.com/riverqueue/river/pull/258).\n- Changed default client IDs to be a combination of hostname and the time which the client started. This can still be changed by specifying `Config.ID`. [PR #255](https://github.com/riverqueue/river/pull/255).\n- Notifier refactored for better robustness and testability. [PR #253](https://github.com/riverqueue/river/pull/253).\n\n## [0.0.25] - 2024-03-01\n\n### Fixed\n\n- Fixed a problem in `riverpgxv5`'s `Listener` where it wouldn't unset an internal connection if `Close` returned an error, making the listener not reusable. Thanks @mfrister for pointing this one out! [PR #246](https://github.com/riverqueue/river/pull/246).\n\n## [0.0.24] - 2024-02-29\n\n### Fixed\n\n- Fixed a memory leak caused by not always cancelling the context used to enable jobs to be cancelled remotely. [PR #243](https://github.com/riverqueue/river/pull/243).\n\n## [0.0.23] - 2024-02-29\n\n### Added\n\n- `JobListParams.Kinds()` has been added so that jobs can now be listed by kind. [PR #212](https://github.com/riverqueue/river/pull/212).\n\n### Changed\n\n- The underlying driver system's been entirely revamped so that River's non-test code is now decoupled from `pgx/v5`. This will allow additional drivers to be implemented, although there are no additional ones for now. [PR #212](https://github.com/riverqueue/river/pull/212).\n\n### Fixed\n\n- Fixed a memory leak caused by allocating a new random source on every job execution. Thank you @shawnstephens for reporting ‚ù§Ô∏è [PR #240](https://github.com/riverqueue/river/pull/240).\n- Fix a problem where `JobListParams.Queues()` didn't filter correctly based on its arguments. [PR #212](https://github.com/riverqueue/river/pull/212).\n- Fix a problem in `DebouncedChan` where it would fire on its \"out\" channel too often when it was being signaled continuously on its \"in\" channel. This would have caused work to be fetched more often than intended in busy systems. [PR #222](https://github.com/riverqueue/river/pull/222).\n\n## [0.0.22] - 2024-02-19\n\n### Fixed\n\n- Brings in another leadership election fix similar to #217 in which a TTL equal to the elector's run interval plus a configured TTL padding is also used for the initial attempt to gain leadership (#217 brought it in for reelection only). [PR #219](https://github.com/riverqueue/river/pull/219).\n\n## [0.0.21] - 2024-02-19\n\n### Changed\n\n- Tweaked behavior of `JobRetry` so that it does actually update the `ScheduledAt` time of the job in all cases where the job is actually being rescheduled. As before, jobs which are already available with a past `ScheduledAt` will not be touched by this query so that they retain their place in line. [PR #211](https://github.com/riverqueue/river/pull/211).\n\n### Fixed\n\n- Fixed a leadership re-election issue that was exposed by the fix in #199. Because we were internally using the same TTL for both an internal timer/ticker and the database update to set the new leader expiration time, a leader wasn't guaranteed to successfully re-elect itself even under normal operation. [PR #217](https://github.com/riverqueue/river/pull/217).\n\n## [0.0.20] - 2024-02-14\n\n### Added\n\n- Added an `ID` setting to the `Client` `Config` type to allow users to override client IDs with their own naming convention. Expose the client ID programmatically (in case it's generated) in a new `Client.ID()` method. [PR #206](https://github.com/riverqueue/river/pull/206).\n\n### Fixed\n\n- Fix a leadership re-election query bug that would cause past leaders to think they were continuing to win elections. [PR #199](https://github.com/riverqueue/river/pull/199).\n\n## [0.0.19] - 2024-02-10\n\n### Added\n\n- Added `JobGet` and `JobGetTx` to the `Client` to enable easily fetching a single job row from code for introspection. [PR #186].\n- Added `JobRetry` and `JobRetryTx` to the `Client` to enable a job to be retried immediately, even if it has already completed, been cancelled, or been discarded. [PR #190].\n\n### Changed\n\n- Validate queue name on job insertion. Allow queue names with hyphen separators in addition to underscore. [PR #184](https://github.com/riverqueue/river/pull/184).\n\n## [0.0.18] - 2024-01-25\n\n### Fixed\n\n- Remove a debug statement from periodic job enqueuer that was accidentally left in. [PR #176](https://github.com/riverqueue/river/pull/176).\n\n## [0.0.17] - 2024-01-22\n\n### Added\n\n- Added `JobCancel` and `JobCancelTx` to the `Client` to enable cancellation of jobs. [PR #141](https://github.com/riverqueue/river/pull/141) and [PR #152](https://github.com/riverqueue/river/pull/152).\n- Added `ClientFromContext` and `ClientFromContextSafely` helpers to extract the `Client` from the worker's context where it is now available to workers. This simplifies making the River client available within your workers for i.e. enqueueing additional jobs. [PR #145](https://github.com/riverqueue/river/pull/145).\n- Add `JobList` API for listing jobs. [PR #117](https://github.com/riverqueue/river/pull/117).\n- Added `river validate` command which fails with a non-zero exit code unless all migrations are applied. [PR #170](https://github.com/riverqueue/river/pull/170).\n\n### Changed\n\n- For short `JobSnooze` times (smaller than the scheduler's run interval) put the job straight into an `available` state with the specified `scheduled_at` time. This avoids an artificially long delay waiting for the next scheduler run. [PR #162](https://github.com/riverqueue/river/pull/162).\n\n### Fixed\n\n- Fixed incorrect default value handling for `ScheduledAt` option with `InsertMany` / `InsertManyTx`. [PR #149](https://github.com/riverqueue/river/pull/149).\n- Add missing `t.Helper()` calls in `rivertest` internal functions that caused it to report itself as the site of a test failure. [PR #151](https://github.com/riverqueue/river/pull/151).\n- Fixed problem where job uniqueness wasn't being respected when used in conjunction with periodic jobs. [PR #168](https://github.com/riverqueue/river/pull/168).\n\n## [0.0.16] - 2024-01-06\n\n### Changed\n\n- Calls to `Stop` error if the client hasn't been started yet. [PR #138](https://github.com/riverqueue/river/pull/138).\n\n### Fixed\n\n- Fix typo in leadership resignation query to ensure faster new leader takeover. [PR #134](https://github.com/riverqueue/river/pull/134).\n- Elector now uses the same `log/slog` instance configured by its parent client. [PR #137](https://github.com/riverqueue/river/pull/137).\n- Notifier now uses the same `log/slog` instance configured by its parent client. [PR #140](https://github.com/riverqueue/river/pull/140).\n\n## [0.0.15] - 2023-12-21\n\n### Fixed\n\n- Ensure `ScheduledAt` is respected on `InsertManyTx`. [PR #121](https://github.com/riverqueue/river/pull/121).\n\n## [0.0.14] - 2023-12-13\n\n### Fixed\n\n- River CLI `go.sum` entries fixed for 0.0.13 release.\n\n## [0.0.13] - 2023-12-12\n\n### Added\n\n- Added `riverdriver/riverdatabasesql` driver to enable River Go migrations through Go's built in `database/sql` package. [PR #98](https://github.com/riverqueue/river/pull/98).\n\n### Changed\n\n- Errored jobs that have a very short duration before their next retry (<5 seconds) are set to `available` immediately instead of being made `scheduled` and having to wait for the scheduler to make a pass to make them workable. [PR #105](https://github.com/riverqueue/river/pull/105).\n- `riverdriver` becomes its own submodule. It contains types that `riverdriver/riverdatabasesql` and `riverdriver/riverpgxv5` need to reference. [PR #98](https://github.com/riverqueue/river/pull/98).\n- The `river/cmd/river` CLI has been made its own Go module. This is possible now that it uses the exported `river/rivermigrate` API, and will help with project maintainability. [PR #107](https://github.com/riverqueue/river/pull/107).\n\n## [0.0.12] - 2023-12-02\n\n### Added\n\n- Added `river/rivermigrate` package to enable migrations from Go code as an alternative to using the CLI. PR #67.\n\n## [0.0.11] - 2023-12-02\n\n### Added\n\n- `Stop` and `StopAndCancel` have been changed to respect the provided context argument. When that context is cancelled or times out, those methods will now immediately return with the context's error, even if the Client's shutdown has not yet completed. Apps may need to adjust their graceful shutdown logic to account for this. PR #79.\n\n### Changed\n\n- `NewClient` no longer errors if it was provided a workers bundle with zero workers. Instead, that check's been moved to `Client.Start` instead. This allows adding workers to a bundle that'd like to reference a River client by letting `AddWorker` be invoked after a client reference is available from `NewClient`. PR #87.\n\n## [0.0.10] - 2023-11-26\n\n### Added\n\n- Added `Example_scheduledJob`, demonstrating how to schedule a job to be run in the future.\n- Added `Stopped` method to `Client` to make it easier to wait for graceful shutdown to complete.\n\n### Fixed\n\n- Fixed a panic in the periodic job enqueuer caused by sometimes trying to reset a `time.Ticker` with a negative or zero duration. Fixed in PR #73.\n\n### Changed\n\n- `DefaultClientRetryPolicy`: calculate the next attempt based on the current time instead of the time the prior attempt began.\n\n## [0.0.9] - 2023-11-23\n\n### Fixed\n\n- **DATABASE MIGRATION**: Database schema v3 was introduced in v0.0.8 and contained an obvious flaw preventing it from running against existing tables. This migration was altered to execute the migration in multiple steps.\n\n## [0.0.8] - 2023-11-21\n\n### Changed\n\n- License changed from LGPLv3 to MPL-2.0.\n- **DATABASE MIGRATION**: Database schema v3, alter river_job tags column to set a default of `[]` and add not null constraint.\n\n## [0.0.7] - 2023-11-20\n\n### Changed\n\n- Constants renamed so that adjectives like `Default` and `Min` become suffixes instead of prefixes. So for example, `DefaultFetchCooldown` becomes `FetchCooldownDefault`.\n- Rename `AttemptError.Num` to `AttemptError.Attempt` to better fit with the name of `JobRow.Attempt`.\n- Document `JobState`, `AttemptError`, and all fields its fields.\n- A `NULL` tags value read from a database job is left as `[]string(nil)` on `JobRow.Tags` rather than a zero-element slice of `[]string{}`. `append` and `len` both work on a `nil` slice, so this should be functionally identical.\n\n## [0.0.6] - 2023-11-19\n\n### Changed\n\n- `JobRow`, `JobState`, and other related types move into `river/rivertype` so they can more easily be shared amongst packages. Most of the River API doesn't change because `JobRow` is embedded on `river.Job`, which doesn't move.\n\n## [0.0.5] - 2023-11-19\n\n### Changed\n\n- Remove `replace` directive from the project's `go.mod` so that it's possible to install River CLI with `@latest`.\n\n## [0.0.4] - 2023-11-17\n\n### Changed\n\n- Allow River clients to be created with a driver with `nil` database pool for use in testing.\n- Update River test helpers API to use River drivers like `riverdriver/riverpgxv5` to make them agnostic to the third party database package in use.\n- Document `Config.JobTimeout`'s default value.\n- Functionally disable the `Reindexer` queue maintenance service. It'd previously only operated on currently unused indexes anyway, indexes probably do _not_ need to be rebuilt except under fairly rare circumstances, and it needs more work to make sure it's shored up against edge cases like indexes that fail to rebuild before a client restart.\n\n## [0.0.3] - 2023-11-13\n\n### Changed\n\n- Fix license detection issues with `riverdriver/riverpgxv5` submodule.\n- Ensure that river requires the `riverpgxv5` module with the same version.\n\n## [0.0.2] - 2023-11-13\n\n### Changed\n\n- Pin own `riverpgxv5` dependency to v0.0.1 and make it a direct locally-replaced dependency. This should allow projects to import versioned deps of both river and `riverpgxv5`.\n\n## [0.0.1] - 2023-11-12\n\n### Added\n\n- This is the initial prerelease of River.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 16.333984375,
          "content": "Mozilla Public License Version 2.0\n==================================\n\n1. Definitions\n--------------\n\n1.1. \"Contributor\"\n    means each individual or legal entity that creates, contributes to\n    the creation of, or owns Covered Software.\n\n1.2. \"Contributor Version\"\n    means the combination of the Contributions of others (if any) used\n    by a Contributor and that particular Contributor's Contribution.\n\n1.3. \"Contribution\"\n    means Covered Software of a particular Contributor.\n\n1.4. \"Covered Software\"\n    means Source Code Form to which the initial Contributor has attached\n    the notice in Exhibit A, the Executable Form of such Source Code\n    Form, and Modifications of such Source Code Form, in each case\n    including portions thereof.\n\n1.5. \"Incompatible With Secondary Licenses\"\n    means\n\n    (a) that the initial Contributor has attached the notice described\n        in Exhibit B to the Covered Software; or\n\n    (b) that the Covered Software was made available under the terms of\n        version 1.1 or earlier of the License, but not also under the\n        terms of a Secondary License.\n\n1.6. \"Executable Form\"\n    means any form of the work other than Source Code Form.\n\n1.7. \"Larger Work\"\n    means a work that combines Covered Software with other material, in\n    a separate file or files, that is not Covered Software.\n\n1.8. \"License\"\n    means this document.\n\n1.9. \"Licensable\"\n    means having the right to grant, to the maximum extent possible,\n    whether at the time of the initial grant or subsequently, any and\n    all of the rights conveyed by this License.\n\n1.10. \"Modifications\"\n    means any of the following:\n\n    (a) any file in Source Code Form that results from an addition to,\n        deletion from, or modification of the contents of Covered\n        Software; or\n\n    (b) any new file in Source Code Form that contains any Covered\n        Software.\n\n1.11. \"Patent Claims\" of a Contributor\n    means any patent claim(s), including without limitation, method,\n    process, and apparatus claims, in any patent Licensable by such\n    Contributor that would be infringed, but for the grant of the\n    License, by the making, using, selling, offering for sale, having\n    made, import, or transfer of either its Contributions or its\n    Contributor Version.\n\n1.12. \"Secondary License\"\n    means either the GNU General Public License, Version 2.0, the GNU\n    Lesser General Public License, Version 2.1, the GNU Affero General\n    Public License, Version 3.0, or any later versions of those\n    licenses.\n\n1.13. \"Source Code Form\"\n    means the form of the work preferred for making modifications.\n\n1.14. \"You\" (or \"Your\")\n    means an individual or a legal entity exercising rights under this\n    License. For legal entities, \"You\" includes any entity that\n    controls, is controlled by, or is under common control with You. For\n    purposes of this definition, \"control\" means (a) the power, direct\n    or indirect, to cause the direction or management of such entity,\n    whether by contract or otherwise, or (b) ownership of more than\n    fifty percent (50%) of the outstanding shares or beneficial\n    ownership of such entity.\n\n2. License Grants and Conditions\n--------------------------------\n\n2.1. Grants\n\nEach Contributor hereby grants You a world-wide, royalty-free,\nnon-exclusive license:\n\n(a) under intellectual property rights (other than patent or trademark)\n    Licensable by such Contributor to use, reproduce, make available,\n    modify, display, perform, distribute, and otherwise exploit its\n    Contributions, either on an unmodified basis, with Modifications, or\n    as part of a Larger Work; and\n\n(b) under Patent Claims of such Contributor to make, use, sell, offer\n    for sale, have made, import, and otherwise transfer either its\n    Contributions or its Contributor Version.\n\n2.2. Effective Date\n\nThe licenses granted in Section 2.1 with respect to any Contribution\nbecome effective for each Contribution on the date the Contributor first\ndistributes such Contribution.\n\n2.3. Limitations on Grant Scope\n\nThe licenses granted in this Section 2 are the only rights granted under\nthis License. No additional rights or licenses will be implied from the\ndistribution or licensing of Covered Software under this License.\nNotwithstanding Section 2.1(b) above, no patent license is granted by a\nContributor:\n\n(a) for any code that a Contributor has removed from Covered Software;\n    or\n\n(b) for infringements caused by: (i) Your and any other third party's\n    modifications of Covered Software, or (ii) the combination of its\n    Contributions with other software (except as part of its Contributor\n    Version); or\n\n(c) under Patent Claims infringed by Covered Software in the absence of\n    its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks,\nor logos of any Contributor (except as may be necessary to comply with\nthe notice requirements in Section 3.4).\n\n2.4. Subsequent Licenses\n\nNo Contributor makes additional grants as a result of Your choice to\ndistribute the Covered Software under a subsequent version of this\nLicense (see Section 10.2) or under the terms of a Secondary License (if\npermitted under the terms of Section 3.3).\n\n2.5. Representation\n\nEach Contributor represents that the Contributor believes its\nContributions are its original creation(s) or it has sufficient rights\nto grant the rights to its Contributions conveyed by this License.\n\n2.6. Fair Use\n\nThis License is not intended to limit any rights You have under\napplicable copyright doctrines of fair use, fair dealing, or other\nequivalents.\n\n2.7. Conditions\n\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted\nin Section 2.1.\n\n3. Responsibilities\n-------------------\n\n3.1. Distribution of Source Form\n\nAll distribution of Covered Software in Source Code Form, including any\nModifications that You create or to which You contribute, must be under\nthe terms of this License. You must inform recipients that the Source\nCode Form of the Covered Software is governed by the terms of this\nLicense, and how they can obtain a copy of this License. You may not\nattempt to alter or restrict the recipients' rights in the Source Code\nForm.\n\n3.2. Distribution of Executable Form\n\nIf You distribute Covered Software in Executable Form then:\n\n(a) such Covered Software must also be made available in Source Code\n    Form, as described in Section 3.1, and You must inform recipients of\n    the Executable Form how they can obtain a copy of such Source Code\n    Form by reasonable means in a timely manner, at a charge no more\n    than the cost of distribution to the recipient; and\n\n(b) You may distribute such Executable Form under the terms of this\n    License, or sublicense it under different terms, provided that the\n    license for the Executable Form does not attempt to limit or alter\n    the recipients' rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\n\nYou may create and distribute a Larger Work under terms of Your choice,\nprovided that You also comply with the requirements of this License for\nthe Covered Software. If the Larger Work is a combination of Covered\nSoftware with a work governed by one or more Secondary Licenses, and the\nCovered Software is not Incompatible With Secondary Licenses, this\nLicense permits You to additionally distribute such Covered Software\nunder the terms of such Secondary License(s), so that the recipient of\nthe Larger Work may, at their option, further distribute the Covered\nSoftware under the terms of either this License or such Secondary\nLicense(s).\n\n3.4. Notices\n\nYou may not remove or alter the substance of any license notices\n(including copyright notices, patent notices, disclaimers of warranty,\nor limitations of liability) contained within the Source Code Form of\nthe Covered Software, except that You may alter any license notices to\nthe extent required to remedy known factual inaccuracies.\n\n3.5. Application of Additional Terms\n\nYou may choose to offer, and to charge a fee for, warranty, support,\nindemnity or liability obligations to one or more recipients of Covered\nSoftware. However, You may do so only on Your own behalf, and not on\nbehalf of any Contributor. You must make it absolutely clear that any\nsuch warranty, support, indemnity, or liability obligation is offered by\nYou alone, and You hereby agree to indemnify every Contributor for any\nliability incurred by such Contributor as a result of warranty, support,\nindemnity or liability terms You offer. You may include additional\ndisclaimers of warranty and limitations of liability specific to any\njurisdiction.\n\n4. Inability to Comply Due to Statute or Regulation\n---------------------------------------------------\n\nIf it is impossible for You to comply with any of the terms of this\nLicense with respect to some or all of the Covered Software due to\nstatute, judicial order, or regulation then You must: (a) comply with\nthe terms of this License to the maximum extent possible; and (b)\ndescribe the limitations and the code they affect. Such description must\nbe placed in a text file included with all distributions of the Covered\nSoftware under this License. Except to the extent prohibited by statute\nor regulation, such description must be sufficiently detailed for a\nrecipient of ordinary skill to be able to understand it.\n\n5. Termination\n--------------\n\n5.1. The rights granted under this License will terminate automatically\nif You fail to comply with any of its terms. However, if You become\ncompliant, then the rights granted under this License from a particular\nContributor are reinstated (a) provisionally, unless and until such\nContributor explicitly and finally terminates Your grants, and (b) on an\nongoing basis, if such Contributor fails to notify You of the\nnon-compliance by some reasonable means prior to 60 days after You have\ncome back into compliance. Moreover, Your grants from a particular\nContributor are reinstated on an ongoing basis if such Contributor\nnotifies You of the non-compliance by some reasonable means, this is the\nfirst time You have received notice of non-compliance with this License\nfrom such Contributor, and You become compliant prior to 30 days after\nYour receipt of the notice.\n\n5.2. If You initiate litigation against any entity by asserting a patent\ninfringement claim (excluding declaratory judgment actions,\ncounter-claims, and cross-claims) alleging that a Contributor Version\ndirectly or indirectly infringes any patent, then the rights granted to\nYou by any and all Contributors for the Covered Software under Section\n2.1 of this License shall terminate.\n\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all\nend user license agreements (excluding distributors and resellers) which\nhave been validly granted by You or Your distributors under this License\nprior to termination shall survive termination.\n\n************************************************************************\n*                                                                      *\n*  6. Disclaimer of Warranty                                           *\n*  -------------------------                                           *\n*                                                                      *\n*  Covered Software is provided under this License on an \"as is\"       *\n*  basis, without warranty of any kind, either expressed, implied, or  *\n*  statutory, including, without limitation, warranties that the       *\n*  Covered Software is free of defects, merchantable, fit for a        *\n*  particular purpose or non-infringing. The entire risk as to the     *\n*  quality and performance of the Covered Software is with You.        *\n*  Should any Covered Software prove defective in any respect, You     *\n*  (not any Contributor) assume the cost of any necessary servicing,   *\n*  repair, or correction. This disclaimer of warranty constitutes an   *\n*  essential part of this License. No use of any Covered Software is   *\n*  authorized under this License except under this disclaimer.         *\n*                                                                      *\n************************************************************************\n\n************************************************************************\n*                                                                      *\n*  7. Limitation of Liability                                          *\n*  --------------------------                                          *\n*                                                                      *\n*  Under no circumstances and under no legal theory, whether tort      *\n*  (including negligence), contract, or otherwise, shall any           *\n*  Contributor, or anyone who distributes Covered Software as          *\n*  permitted above, be liable to You for any direct, indirect,         *\n*  special, incidental, or consequential damages of any character      *\n*  including, without limitation, damages for lost profits, loss of    *\n*  goodwill, work stoppage, computer failure or malfunction, or any    *\n*  and all other commercial damages or losses, even if such party      *\n*  shall have been informed of the possibility of such damages. This   *\n*  limitation of liability shall not apply to liability for death or   *\n*  personal injury resulting from such party's negligence to the       *\n*  extent applicable law prohibits such limitation. Some               *\n*  jurisdictions do not allow the exclusion or limitation of           *\n*  incidental or consequential damages, so this exclusion and          *\n*  limitation may not apply to You.                                    *\n*                                                                      *\n************************************************************************\n\n8. Litigation\n-------------\n\nAny litigation relating to this License may be brought only in the\ncourts of a jurisdiction where the defendant maintains its principal\nplace of business and such litigation shall be governed by laws of that\njurisdiction, without reference to its conflict-of-law provisions.\nNothing in this Section shall prevent a party's ability to bring\ncross-claims or counter-claims.\n\n9. Miscellaneous\n----------------\n\nThis License represents the complete agreement concerning the subject\nmatter hereof. If any provision of this License is held to be\nunenforceable, such provision shall be reformed only to the extent\nnecessary to make it enforceable. Any law or regulation which provides\nthat the language of a contract shall be construed against the drafter\nshall not be used to construe this License against a Contributor.\n\n10. Versions of the License\n---------------------------\n\n10.1. New Versions\n\nMozilla Foundation is the license steward. Except as provided in Section\n10.3, no one other than the license steward has the right to modify or\npublish new versions of this License. Each version will be given a\ndistinguishing version number.\n\n10.2. Effect of New Versions\n\nYou may distribute the Covered Software under the terms of the version\nof the License under which You originally received the Covered Software,\nor under the terms of any subsequent version published by the license\nsteward.\n\n10.3. Modified Versions\n\nIf you create software not governed by this License, and you want to\ncreate a new license for such software, you may create and use a\nmodified version of this License if you rename the license and remove\nany references to the name of the license steward (except to note that\nsuch modified license differs from this License).\n\n10.4. Distributing Source Code Form that is Incompatible With Secondary\nLicenses\n\nIf You choose to distribute Source Code Form that is Incompatible With\nSecondary Licenses under the terms of this version of the License, the\nnotice described in Exhibit B of this License must be attached.\n\nExhibit A - Source Code Form License Notice\n-------------------------------------------\n\n  This Source Code Form is subject to the terms of the Mozilla Public\n  License, v. 2.0. If a copy of the MPL was not distributed with this\n  file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nIf it is not possible or desirable to put the notice in a particular\nfile, then You may include the notice in a location (such as a LICENSE\nfile in a relevant directory) where a recipient would be likely to look\nfor such a notice.\n\nYou may add additional accurate notices of copyright ownership.\n\nExhibit B - \"Incompatible With Secondary Licenses\" Notice\n---------------------------------------------------------\n\n  This Source Code Form is \"Incompatible With Secondary Licenses\", as\n  defined by the Mozilla Public License, v. 2.0.\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 2.935546875,
          "content": ".DEFAULT_GOAL := help\n\n.PHONY: db/reset\ndb/reset: ## Drop, create, and migrate dev and test databases\ndb/reset: db/reset/dev\ndb/reset: db/reset/test\n\n.PHONY: db/reset/dev\ndb/reset/dev: ## Drop, create, and migrate dev database\n\tdropdb river_dev --force --if-exists\n\tcreatedb river_dev\n\tcd cmd/river && go run . migrate-up --database-url \"postgres://localhost/river_dev\"\n\n.PHONY: db/reset/test\ndb/reset/test: ## Drop, create, and migrate test databases\n\tgo run ./internal/cmd/testdbman reset\n\n.PHONY: generate\ngenerate: ## Generate generated artifacts\ngenerate: generate/migrations\ngenerate: generate/sqlc\n\n.PHONY: generate/migrations\ngenerate/migrations: ## Sync changes of pgxv5 migrations to database/sql\n\trsync -au --delete \"riverdriver/riverpgxv5/migration/\" \"riverdriver/riverdatabasesql/migration/\"\n\n.PHONY: generate/sqlc\ngenerate/sqlc: ## Generate sqlc\n\tcd riverdriver/riverdatabasesql/internal/dbsqlc && sqlc generate\n\tcd riverdriver/riverpgxv5/internal/dbsqlc && sqlc generate\n\n# Looks at comments using ## on targets and uses them to produce a help output.\n.PHONY: help\nhelp: ALIGN=22\nhelp: ## Print this message\n\t@awk -F '::? .*## ' -- \"/^[^':]+::? .*## /\"' { printf \"'$$(tput bold)'%-$(ALIGN)s'$$(tput sgr0)' %s\\n\", $$1, $$2 }' $(MAKEFILE_LIST)\n\n# Each directory of a submodule in the Go workspace. Go commands provide no\n# built-in way to run for all workspace submodules. Add a new submodule to the\n# workspace with `go work use ./driver/new`.\nsubmodules := $(shell go list -f '{{.Dir}}' -m)\n\n# Definitions of following tasks look ugly, but they're done this way because to\n# produce the best/most comprehensible output by far (e.g. compared to a shell\n# loop).\n.PHONY: lint\nlint:: ## Run linter (golangci-lint) for all submodules\ndefine lint-target\n    lint:: ; cd $1 && golangci-lint run --fix\nendef\n$(foreach mod,$(submodules),$(eval $(call lint-target,$(mod))))\n\n.PHONY: test\ntest:: ## Run test suite for all submodules\ndefine test-target\n    test:: ; cd $1 && go test ./... -p 1\nendef\n$(foreach mod,$(submodules),$(eval $(call test-target,$(mod))))\n\n.PHONY: tidy\ntidy:: ## Run `go mod tidy` for all submodules\ndefine tidy-target\n    tidy:: ; cd $1 && go mod tidy\nendef\n$(foreach mod,$(submodules),$(eval $(call tidy-target,$(mod))))\n\n.PHONY: update-mod-go\nupdate-mod-go: ## Update `go`/`toolchain` directives in all submodules to match `go.work`\n\tgo run ./rivershared/cmd/update-mod-go ./go.work\n\n.PHONY: update-mod-version\nupdate-mod-version: ## Update River packages in all submodules to $VERSION\n\tgo run ./rivershared/cmd/update-mod-version ./go.work\n\n.PHONY: verify\nverify: ## Verify generated artifacts\nverify: verify/migrations\nverify: verify/sqlc\n\n.PHONY: verify/migrations\nverify/migrations: ## Verify synced migrations\n\tdiff -qr riverdriver/riverpgxv5/migration riverdriver/riverdatabasesql/migration\n\n.PHONY: verify/sqlc\nverify/sqlc: ## Verify generated sqlc\n\tcd riverdriver/riverdatabasesql/internal/dbsqlc && sqlc diff\n\tcd riverdriver/riverpgxv5/internal/dbsqlc && sqlc diff\n"
        },
        {
          "name": "client.go",
          "type": "blob",
          "size": 77.30859375,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"regexp\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/dblist\"\n\t\"github.com/riverqueue/river/internal/dbunique\"\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/leadership\"\n\t\"github.com/riverqueue/river/internal/maintenance\"\n\t\"github.com/riverqueue/river/internal/notifier\"\n\t\"github.com/riverqueue/river/internal/notifylimiter\"\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/riverpilot\"\n\t\"github.com/riverqueue/river/rivershared/startstop\"\n\t\"github.com/riverqueue/river/rivershared/testsignal\"\n\t\"github.com/riverqueue/river/rivershared/util/maputil\"\n\t\"github.com/riverqueue/river/rivershared/util/sliceutil\"\n\t\"github.com/riverqueue/river/rivershared/util/valutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nconst (\n\tFetchCooldownDefault = 100 * time.Millisecond\n\tFetchCooldownMin     = 1 * time.Millisecond\n\n\tFetchPollIntervalDefault = 1 * time.Second\n\tFetchPollIntervalMin     = 1 * time.Millisecond\n\n\tJobTimeoutDefault  = 1 * time.Minute\n\tMaxAttemptsDefault = rivercommon.MaxAttemptsDefault\n\tPriorityDefault    = rivercommon.PriorityDefault\n\tQueueDefault       = rivercommon.QueueDefault\n\tQueueNumWorkersMax = 10_000\n)\n\n// Config is the configuration for a Client.\n//\n// Both Queues and Workers are required for a client to work jobs, but an\n// insert-only client can be initialized by omitting Queues, and not calling\n// Start for the client. Workers can also be omitted, but it's better to include\n// it so River can check that inserted job kinds have a worker that can run\n// them.\ntype Config struct {\n\t// AdvisoryLockPrefix is a configurable 32-bit prefix that River will use\n\t// when generating any key to acquire a Postgres advisory lock. All advisory\n\t// locks share the same 64-bit number space, so this allows a calling\n\t// application to guarantee that a River advisory lock will never conflict\n\t// with one of its own by cordoning each type to its own prefix.\n\t//\n\t// If this value isn't set, River defaults to generating key hashes across\n\t// the entire 64-bit advisory lock number space, which is large enough that\n\t// conflicts are exceedingly unlikely. If callers don't strictly need this\n\t// option then it's recommended to leave it unset because the prefix leaves\n\t// only 32 bits of number space for advisory lock hashes, so it makes\n\t// internally conflicting River-generated keys more likely.\n\t//\n\t// Advisory locks are currently only used for the deprecated fallback/slow\n\t// path of unique job insertion when pending, scheduled, available, or running\n\t// are omitted from a customized ByState configuration.\n\tAdvisoryLockPrefix int32\n\n\t// CancelledJobRetentionPeriod is the amount of time to keep cancelled jobs\n\t// around before they're removed permanently.\n\t//\n\t// Defaults to 24 hours.\n\tCancelledJobRetentionPeriod time.Duration\n\n\t// CompletedJobRetentionPeriod is the amount of time to keep completed jobs\n\t// around before they're removed permanently.\n\t//\n\t// Defaults to 24 hours.\n\tCompletedJobRetentionPeriod time.Duration\n\n\t// DiscardedJobRetentionPeriod is the amount of time to keep discarded jobs\n\t// around before they're removed permanently.\n\t//\n\t// Defaults to 7 days.\n\tDiscardedJobRetentionPeriod time.Duration\n\n\t// ErrorHandler can be configured to be invoked in case of an error or panic\n\t// occurring in a job. This is often useful for logging and exception\n\t// tracking, but can also be used to customize retry behavior.\n\tErrorHandler ErrorHandler\n\n\t// FetchCooldown is the minimum amount of time to wait between fetches of new\n\t// jobs. Jobs will only be fetched *at most* this often, but if no new jobs\n\t// are coming in via LISTEN/NOTIFY then fetches may be delayed as long as\n\t// FetchPollInterval.\n\t//\n\t// Throughput is limited by this value.\n\t//\n\t// Defaults to 100 ms.\n\tFetchCooldown time.Duration\n\n\t// FetchPollInterval is the amount of time between periodic fetches for new\n\t// jobs. Typically new jobs will be picked up ~immediately after insert via\n\t// LISTEN/NOTIFY, but this provides a fallback.\n\t//\n\t// Defaults to 1 second.\n\tFetchPollInterval time.Duration\n\n\t// ID is the unique identifier for this client. If not set, a random\n\t// identifier will be generated.\n\t//\n\t// This is used to identify the client in job attempts and for leader election.\n\t// This value must be unique across all clients in the same database and\n\t// schema and there must not be more than one process running with the same\n\t// ID at the same time.\n\t//\n\t// A client ID should differ between different programs and must be unique\n\t// across all clients in the same database and schema. There must not be\n\t// more than one process running with the same ID at the same time.\n\t// Duplicate IDs between processes will lead to facilities like leader\n\t// election or client statistics to fail in novel ways. However, the client\n\t// ID is shared by all executors within any given client. (i.e.  different\n\t// Go processes have different IDs, but IDs are shared within any given\n\t// process.)\n\t//\n\t// If in doubt, leave this property empty.\n\tID string\n\n\t// JobCleanerTimeout is the timeout of the individual queries within the job\n\t// cleaner.\n\t//\n\t// Defaults to 30 seconds, which should be more than enough time for most\n\t// deployments.\n\tJobCleanerTimeout time.Duration\n\n\t// JobInsertMiddleware are optional functions that can be called around job\n\t// insertion.\n\tJobInsertMiddleware []rivertype.JobInsertMiddleware\n\n\t// JobTimeout is the maximum amount of time a job is allowed to run before its\n\t// context is cancelled. A timeout of zero means JobTimeoutDefault will be\n\t// used, whereas a value of -1 means the job's context will not be cancelled\n\t// unless the Client is shutting down.\n\t//\n\t// Defaults to 1 minute.\n\tJobTimeout time.Duration\n\n\t// Logger is the structured logger to use for logging purposes. If none is\n\t// specified, logs will be emitted to STDOUT with messages at warn level\n\t// or higher.\n\tLogger *slog.Logger\n\n\t// MaxAttempts is the default number of times a job will be retried before\n\t// being discarded. This value is applied to all jobs by default, and can be\n\t// overridden on individual job types on the JobArgs or on a per-job basis at\n\t// insertion time.\n\t//\n\t// If not specified, defaults to 25 (MaxAttemptsDefault).\n\tMaxAttempts int\n\n\t// PeriodicJobs are a set of periodic jobs to run at the specified intervals\n\t// in the client.\n\tPeriodicJobs []*PeriodicJob\n\n\t// PollOnly starts the client in \"poll only\" mode, which avoids issuing\n\t// `LISTEN` statements to wait for events like a leadership resignation or\n\t// new job available. The program instead polls periodically to look for\n\t// changes (checking for new jobs on the period in FetchPollInterval).\n\t//\n\t// The downside of this mode of operation is that events will usually be\n\t// noticed less quickly. A new job in the queue may have to wait up to\n\t// FetchPollInterval to be locked for work. When a leader resigns, it will\n\t// be up to five seconds before a new one elects itself.\n\t//\n\t// The upside is that it makes River compatible with systems where\n\t// listen/notify isn't available. For example, PgBouncer in transaction\n\t// pooling mode.\n\tPollOnly bool\n\n\t// Queues is a list of queue names for this client to operate on along with\n\t// configuration for the queue like the maximum number of workers to run for\n\t// each queue.\n\t//\n\t// This field may be omitted for a program that's only queueing jobs rather\n\t// than working them. If it's specified, then Workers must also be given.\n\tQueues map[string]QueueConfig\n\n\t// ReindexerSchedule is the schedule for running the reindexer. If nil, the\n\t// reindexer will run at midnight UTC every day.\n\tReindexerSchedule PeriodicSchedule\n\n\t// RescueStuckJobsAfter is the amount of time a job can be running before it\n\t// is considered stuck. A stuck job which has not yet reached its max attempts\n\t// will be scheduled for a retry, while one which has exhausted its attempts\n\t// will be discarded.  This prevents jobs from being stuck forever if a worker\n\t// crashes or is killed.\n\t//\n\t// Note that this can result in repeat or duplicate execution of a job that is\n\t// not actually stuck but is still working. The value should be set higher\n\t// than the maximum duration you expect your jobs to run. Setting a value too\n\t// low will result in more duplicate executions, whereas too high of a value\n\t// will result in jobs being stuck for longer than necessary before they are\n\t// retried.\n\t//\n\t// RescueStuckJobsAfter must be greater than JobTimeout. Otherwise, jobs\n\t// would become eligible for rescue while they're still running.\n\t//\n\t// Defaults to 1 hour, or in cases where JobTimeout has been configured and\n\t// is greater than 1 hour, JobTimeout + 1 hour.\n\tRescueStuckJobsAfter time.Duration\n\n\t// RetryPolicy is a configurable retry policy for the client.\n\t//\n\t// Defaults to DefaultRetryPolicy.\n\tRetryPolicy ClientRetryPolicy\n\n\t// TestOnly can be set to true to disable certain features that are useful\n\t// in production, but which may be harmful to tests, in ways like having the\n\t// effect of making them slower. It should not be used outside of test\n\t// suites.\n\t//\n\t// For example, queue maintenance services normally stagger their startup\n\t// with a random jittered sleep so they don't all try to work at the same\n\t// time. This is nice in production, but makes starting and stopping the\n\t// client in a test case slower.\n\tTestOnly bool\n\n\t// Workers is a bundle of registered job workers.\n\t//\n\t// This field may be omitted for a program that's only enqueueing jobs\n\t// rather than working them, but if it is configured the client can validate\n\t// ahead of time that a worker is properly registered for an inserted job.\n\t// (i.e.  That it wasn't forgotten by accident.)\n\tWorkers *Workers\n\n\t// WorkerMiddleware are optional functions that can be called around\n\t// all job executions.\n\tWorkerMiddleware []rivertype.WorkerMiddleware\n\n\t// Scheduler run interval. Shared between the scheduler and producer/job\n\t// executors, but not currently exposed for configuration.\n\tschedulerInterval time.Duration\n\n\t// Time generator to make time stubbable in tests.\n\ttime baseservice.TimeGenerator\n}\n\nfunc (c *Config) validate() error {\n\tif c.CancelledJobRetentionPeriod < 0 {\n\t\treturn errors.New(\"CancelledJobRetentionPeriod time cannot be less than zero\")\n\t}\n\tif c.CompletedJobRetentionPeriod < 0 {\n\t\treturn errors.New(\"CompletedJobRetentionPeriod cannot be less than zero\")\n\t}\n\tif c.DiscardedJobRetentionPeriod < 0 {\n\t\treturn errors.New(\"DiscardedJobRetentionPeriod cannot be less than zero\")\n\t}\n\tif c.FetchCooldown < FetchCooldownMin {\n\t\treturn fmt.Errorf(\"FetchCooldown must be at least %s\", FetchCooldownMin)\n\t}\n\tif c.FetchPollInterval < FetchPollIntervalMin {\n\t\treturn fmt.Errorf(\"FetchPollInterval must be at least %s\", FetchPollIntervalMin)\n\t}\n\tif c.FetchPollInterval < c.FetchCooldown {\n\t\treturn fmt.Errorf(\"FetchPollInterval cannot be shorter than FetchCooldown (%s)\", c.FetchCooldown)\n\t}\n\tif len(c.ID) > 100 {\n\t\treturn errors.New(\"ID cannot be longer than 100 characters\")\n\t}\n\tif c.JobTimeout < -1 {\n\t\treturn errors.New(\"JobTimeout cannot be negative, except for -1 (infinite)\")\n\t}\n\tif c.MaxAttempts < 0 {\n\t\treturn errors.New(\"MaxAttempts cannot be less than zero\")\n\t}\n\tif c.RescueStuckJobsAfter < 0 {\n\t\treturn errors.New(\"RescueStuckJobsAfter cannot be less than zero\")\n\t}\n\tif c.RescueStuckJobsAfter < c.JobTimeout {\n\t\treturn errors.New(\"RescueStuckJobsAfter cannot be less than JobTimeout\")\n\t}\n\n\tfor queue, queueConfig := range c.Queues {\n\t\tif err := queueConfig.validate(queue); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.Workers == nil && c.Queues != nil {\n\t\treturn errors.New(\"Workers must be set if Queues is set\")\n\t}\n\n\treturn nil\n}\n\n// Indicates whether with the given configuration, this client will be expected\n// to execute jobs (rather than just being used to enqueue them). Executing jobs\n// requires a set of configured queues.\nfunc (c *Config) willExecuteJobs() bool {\n\treturn len(c.Queues) > 0\n}\n\n// QueueConfig contains queue-specific configuration.\ntype QueueConfig struct {\n\t// MaxWorkers is the maximum number of workers to run for the queue, or put\n\t// otherwise, the maximum parallelism to run.\n\t//\n\t// This is the maximum number of workers within this particular client\n\t// instance, but note that it doesn't control the total number of workers\n\t// across parallel processes. Installations will want to calculate their\n\t// total number by multiplying this number by the number of parallel nodes\n\t// running River clients configured to the same database and queue.\n\t//\n\t// Requires a minimum of 1, and a maximum of 10,000.\n\tMaxWorkers int\n}\n\nfunc (c QueueConfig) validate(queueName string) error {\n\tif c.MaxWorkers < 1 || c.MaxWorkers > QueueNumWorkersMax {\n\t\treturn fmt.Errorf(\"invalid number of workers for queue %q: %d\", queueName, c.MaxWorkers)\n\t}\n\tif err := validateQueueName(queueName); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Client is a single isolated instance of River. Your application may use\n// multiple instances operating on different databases or Postgres schemas\n// within a single database.\ntype Client[TTx any] struct {\n\t// BaseService and BaseStartStop can't be embedded like on other services\n\t// because their properties would leak to the external API.\n\tbaseService   baseservice.BaseService\n\tbaseStartStop startstop.BaseStartStop\n\n\tcompleter            jobcompleter.JobCompleter\n\tconfig               *Config\n\tdriver               riverdriver.Driver[TTx]\n\telector              *leadership.Elector\n\tinsertNotifyLimiter  *notifylimiter.Limiter\n\tnotifier             *notifier.Notifier // may be nil in poll-only mode\n\tperiodicJobs         *PeriodicJobBundle\n\tpilot                riverpilot.Pilot\n\tproducersByQueueName map[string]*producer\n\tqueueMaintainer      *maintenance.QueueMaintainer\n\tqueues               *QueueBundle\n\tservices             []startstop.Service\n\tstopped              <-chan struct{}\n\tsubscriptionManager  *subscriptionManager\n\ttestSignals          clientTestSignals\n\n\t// workCancel cancels the context used for all work goroutines. Normal Stop\n\t// does not cancel that context.\n\tworkCancel context.CancelCauseFunc\n}\n\n// Test-only signals.\ntype clientTestSignals struct {\n\telectedLeader testsignal.TestSignal[struct{}] // notifies when elected leader\n\n\tjobCleaner          *maintenance.JobCleanerTestSignals\n\tjobRescuer          *maintenance.JobRescuerTestSignals\n\tjobScheduler        *maintenance.JobSchedulerTestSignals\n\tperiodicJobEnqueuer *maintenance.PeriodicJobEnqueuerTestSignals\n\tqueueCleaner        *maintenance.QueueCleanerTestSignals\n\treindexer           *maintenance.ReindexerTestSignals\n}\n\nfunc (ts *clientTestSignals) Init() {\n\tts.electedLeader.Init()\n\n\tif ts.jobCleaner != nil {\n\t\tts.jobCleaner.Init()\n\t}\n\tif ts.jobRescuer != nil {\n\t\tts.jobRescuer.Init()\n\t}\n\tif ts.jobScheduler != nil {\n\t\tts.jobScheduler.Init()\n\t}\n\tif ts.periodicJobEnqueuer != nil {\n\t\tts.periodicJobEnqueuer.Init()\n\t}\n\tif ts.queueCleaner != nil {\n\t\tts.queueCleaner.Init()\n\t}\n\tif ts.reindexer != nil {\n\t\tts.reindexer.Init()\n\t}\n}\n\nvar (\n\t// ErrNotFound is returned when a query by ID does not match any existing\n\t// rows. For example, attempting to cancel a job that doesn't exist will\n\t// return this error.\n\tErrNotFound = rivertype.ErrNotFound\n\n\terrMissingConfig                 = errors.New(\"missing config\")\n\terrMissingDatabasePoolWithQueues = errors.New(\"must have a non-nil database pool to execute jobs (either use a driver with database pool or don't configure Queues)\")\n\terrMissingDriver                 = errors.New(\"missing database driver (try wrapping a Pgx pool with river/riverdriver/riverpgxv5.New)\")\n)\n\n// NewClient creates a new Client with the given database driver and\n// configuration.\n//\n// Currently only one driver is supported, which is Pgx v5. See package\n// riverpgxv5.\n//\n// The function takes a generic parameter TTx representing a transaction type,\n// but it can be omitted because it'll generally always be inferred from the\n// driver. For example:\n//\n//\timport \"github.com/riverqueue/river\"\n//\timport \"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n//\n//\t...\n//\n//\tdbPool, err := pgxpool.New(ctx, os.Getenv(\"DATABASE_URL\"))\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\tdefer dbPool.Close()\n//\n//\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n//\t\t...\n//\t})\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc NewClient[TTx any](driver riverdriver.Driver[TTx], config *Config) (*Client[TTx], error) {\n\tif driver == nil {\n\t\treturn nil, errMissingDriver\n\t}\n\tif config == nil {\n\t\treturn nil, errMissingConfig\n\t}\n\n\tlogger := config.Logger\n\tif logger == nil {\n\t\tlogger = slog.New(slog.NewTextHandler(os.Stdout, &slog.HandlerOptions{\n\t\t\tLevel: slog.LevelWarn,\n\t\t}))\n\t}\n\n\tretryPolicy := config.RetryPolicy\n\tif retryPolicy == nil {\n\t\tretryPolicy = &DefaultClientRetryPolicy{}\n\t}\n\n\t// For convenience, in case the user's specified a large JobTimeout but no\n\t// RescueStuckJobsAfter, since RescueStuckJobsAfter must be greater than\n\t// JobTimeout, set a reasonable default value that's longer than JobTimeout.\n\trescueAfter := maintenance.JobRescuerRescueAfterDefault\n\tif config.JobTimeout > 0 && config.RescueStuckJobsAfter < 1 && config.JobTimeout > config.RescueStuckJobsAfter {\n\t\trescueAfter = config.JobTimeout + maintenance.JobRescuerRescueAfterDefault\n\t}\n\n\t// Create a new version of config with defaults filled in. This replaces the\n\t// original object, so everything that we care about must be initialized\n\t// here, even if it's only carrying over the original value.\n\tconfig = &Config{\n\t\tAdvisoryLockPrefix:          config.AdvisoryLockPrefix,\n\t\tCancelledJobRetentionPeriod: valutil.ValOrDefault(config.CancelledJobRetentionPeriod, maintenance.CancelledJobRetentionPeriodDefault),\n\t\tCompletedJobRetentionPeriod: valutil.ValOrDefault(config.CompletedJobRetentionPeriod, maintenance.CompletedJobRetentionPeriodDefault),\n\t\tDiscardedJobRetentionPeriod: valutil.ValOrDefault(config.DiscardedJobRetentionPeriod, maintenance.DiscardedJobRetentionPeriodDefault),\n\t\tErrorHandler:                config.ErrorHandler,\n\t\tFetchCooldown:               valutil.ValOrDefault(config.FetchCooldown, FetchCooldownDefault),\n\t\tFetchPollInterval:           valutil.ValOrDefault(config.FetchPollInterval, FetchPollIntervalDefault),\n\t\tID:                          valutil.ValOrDefaultFunc(config.ID, func() string { return defaultClientID(time.Now().UTC()) }),\n\t\tJobInsertMiddleware:         config.JobInsertMiddleware,\n\t\tJobTimeout:                  valutil.ValOrDefault(config.JobTimeout, JobTimeoutDefault),\n\t\tLogger:                      logger,\n\t\tMaxAttempts:                 valutil.ValOrDefault(config.MaxAttempts, MaxAttemptsDefault),\n\t\tPeriodicJobs:                config.PeriodicJobs,\n\t\tPollOnly:                    config.PollOnly,\n\t\tQueues:                      config.Queues,\n\t\tReindexerSchedule:           config.ReindexerSchedule,\n\t\tRescueStuckJobsAfter:        valutil.ValOrDefault(config.RescueStuckJobsAfter, rescueAfter),\n\t\tRetryPolicy:                 retryPolicy,\n\t\tTestOnly:                    config.TestOnly,\n\t\tWorkers:                     config.Workers,\n\t\tWorkerMiddleware:            config.WorkerMiddleware,\n\t\tschedulerInterval:           valutil.ValOrDefault(config.schedulerInterval, maintenance.JobSchedulerIntervalDefault),\n\t\ttime:                        config.time,\n\t}\n\n\tif err := config.validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tarchetype := baseservice.NewArchetype(config.Logger)\n\tif config.time != nil {\n\t\tarchetype.Time = config.time\n\t}\n\n\tclient := &Client[TTx]{\n\t\tconfig:               config,\n\t\tdriver:               driver,\n\t\tproducersByQueueName: make(map[string]*producer),\n\t\ttestSignals:          clientTestSignals{},\n\t\tworkCancel:           func(cause error) {}, // replaced on start, but here in case StopAndCancel is called before start up\n\t}\n\tclient.queues = &QueueBundle{addProducer: client.addProducer}\n\n\tbaseservice.Init(archetype, &client.baseService)\n\tclient.baseService.Name = \"Client\" // Have to correct the name because base service isn't embedded like it usually is\n\tclient.insertNotifyLimiter = notifylimiter.NewLimiter(archetype, config.FetchCooldown)\n\n\tplugin, _ := driver.(driverPlugin[TTx])\n\tif plugin != nil {\n\t\tplugin.PluginInit(archetype, client)\n\t\tclient.pilot = plugin.PluginPilot()\n\t}\n\tif client.pilot == nil {\n\t\tclient.pilot = &riverpilot.StandardPilot{}\n\t}\n\tclient.pilot.PilotInit(archetype)\n\n\t// There are a number of internal components that are only needed/desired if\n\t// we're actually going to be working jobs (as opposed to just enqueueing\n\t// them):\n\tif config.willExecuteJobs() {\n\t\tif !driver.HasPool() {\n\t\t\treturn nil, errMissingDatabasePoolWithQueues\n\t\t}\n\n\t\tclient.completer = jobcompleter.NewBatchCompleter(archetype, driver.GetExecutor(), client.pilot, nil)\n\t\tclient.subscriptionManager = newSubscriptionManager(archetype, nil)\n\t\tclient.services = append(client.services, client.completer, client.subscriptionManager)\n\n\t\tif driver.SupportsListener() {\n\t\t\t// In poll only mode, we don't try to initialize a notifier that\n\t\t\t// uses listen/notify. Instead, each service polls for changes it's\n\t\t\t// interested in. e.g. Elector polls to see if leader has expired.\n\t\t\tif !config.PollOnly {\n\t\t\t\tclient.notifier = notifier.New(archetype, driver.GetListener())\n\t\t\t\tclient.services = append(client.services, client.notifier)\n\t\t\t}\n\t\t} else {\n\t\t\tlogger.Info(\"Driver does not support listener; entering poll only mode\")\n\t\t}\n\n\t\tclient.elector = leadership.NewElector(archetype, driver.GetExecutor(), client.notifier, &leadership.Config{\n\t\t\tClientID: config.ID,\n\t\t})\n\t\tclient.services = append(client.services, client.elector)\n\n\t\tfor queue, queueConfig := range config.Queues {\n\t\t\tclient.addProducer(queue, queueConfig)\n\t\t}\n\n\t\tclient.services = append(client.services,\n\t\t\tstartstop.StartStopFunc(client.logStatsLoop))\n\n\t\tclient.services = append(client.services,\n\t\t\tstartstop.StartStopFunc(client.handleLeadershipChangeLoop))\n\n\t\tif plugin != nil {\n\t\t\tclient.services = append(client.services, plugin.PluginServices()...)\n\t\t}\n\n\t\t//\n\t\t// Maintenance services\n\t\t//\n\n\t\tmaintenanceServices := []startstop.Service{}\n\n\t\t{\n\t\t\tjobCleaner := maintenance.NewJobCleaner(archetype, &maintenance.JobCleanerConfig{\n\t\t\t\tCancelledJobRetentionPeriod: config.CancelledJobRetentionPeriod,\n\t\t\t\tCompletedJobRetentionPeriod: config.CompletedJobRetentionPeriod,\n\t\t\t\tDiscardedJobRetentionPeriod: config.DiscardedJobRetentionPeriod,\n\t\t\t\tTimeout:                     config.JobCleanerTimeout,\n\t\t\t}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, jobCleaner)\n\t\t\tclient.testSignals.jobCleaner = &jobCleaner.TestSignals\n\t\t}\n\n\t\t{\n\t\t\tjobRescuer := maintenance.NewRescuer(archetype, &maintenance.JobRescuerConfig{\n\t\t\t\tClientRetryPolicy: retryPolicy,\n\t\t\t\tRescueAfter:       config.RescueStuckJobsAfter,\n\t\t\t\tWorkUnitFactoryFunc: func(kind string) workunit.WorkUnitFactory {\n\t\t\t\t\tif workerInfo, ok := config.Workers.workersMap[kind]; ok {\n\t\t\t\t\t\treturn workerInfo.workUnitFactory\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, jobRescuer)\n\t\t\tclient.testSignals.jobRescuer = &jobRescuer.TestSignals\n\t\t}\n\n\t\t{\n\t\t\tjobScheduler := maintenance.NewJobScheduler(archetype, &maintenance.JobSchedulerConfig{\n\t\t\t\tInterval:     config.schedulerInterval,\n\t\t\t\tNotifyInsert: client.maybeNotifyInsertForQueues,\n\t\t\t}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, jobScheduler)\n\t\t\tclient.testSignals.jobScheduler = &jobScheduler.TestSignals\n\t\t}\n\n\t\t{\n\t\t\tperiodicJobEnqueuer := maintenance.NewPeriodicJobEnqueuer(archetype, &maintenance.PeriodicJobEnqueuerConfig{\n\t\t\t\tAdvisoryLockPrefix: config.AdvisoryLockPrefix,\n\t\t\t\tInsert:             client.insertMany,\n\t\t\t}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, periodicJobEnqueuer)\n\t\t\tclient.testSignals.periodicJobEnqueuer = &periodicJobEnqueuer.TestSignals\n\n\t\t\tclient.periodicJobs = newPeriodicJobBundle(client.config, periodicJobEnqueuer)\n\t\t\tclient.periodicJobs.AddMany(config.PeriodicJobs)\n\t\t}\n\n\t\t{\n\t\t\tqueueCleaner := maintenance.NewQueueCleaner(archetype, &maintenance.QueueCleanerConfig{\n\t\t\t\tRetentionPeriod: maintenance.QueueRetentionPeriodDefault,\n\t\t\t}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, queueCleaner)\n\t\t\tclient.testSignals.queueCleaner = &queueCleaner.TestSignals\n\t\t}\n\n\t\t{\n\t\t\tvar scheduleFunc func(time.Time) time.Time\n\t\t\tif config.ReindexerSchedule != nil {\n\t\t\t\tscheduleFunc = config.ReindexerSchedule.Next\n\t\t\t}\n\n\t\t\treindexer := maintenance.NewReindexer(archetype, &maintenance.ReindexerConfig{ScheduleFunc: scheduleFunc}, driver.GetExecutor())\n\t\t\tmaintenanceServices = append(maintenanceServices, reindexer)\n\t\t\tclient.testSignals.reindexer = &reindexer.TestSignals\n\t\t}\n\n\t\tif plugin != nil {\n\t\t\tmaintenanceServices = append(maintenanceServices, plugin.PluginMaintenanceServices()...)\n\t\t}\n\n\t\t// Not added to the main services list because the queue maintainer is\n\t\t// started conditionally based on whether the client is the leader.\n\t\tclient.queueMaintainer = maintenance.NewQueueMaintainer(archetype, maintenanceServices)\n\n\t\tif config.TestOnly {\n\t\t\tclient.queueMaintainer.StaggerStartupDisable(true)\n\t\t}\n\t}\n\n\treturn client, nil\n}\n\n// Start starts the client's job fetching and working loops. Once this is called,\n// the client will run in a background goroutine until stopped. All jobs are\n// run with a context inheriting from the provided context, but with a timeout\n// deadline applied based on the job's settings.\n//\n// A graceful shutdown stops fetching new jobs but allows any previously fetched\n// jobs to complete. This can be initiated with the Stop method.\n//\n// A more abrupt shutdown can be achieved by either cancelling the provided\n// context or by calling StopAndCancel. This will not only stop fetching new\n// jobs, but will also cancel the context for any currently-running jobs. If\n// using StopAndCancel, there's no need to also call Stop.\nfunc (c *Client[TTx]) Start(ctx context.Context) error {\n\tfetchCtx, shouldStart, started, stopped := c.baseStartStop.StartInit(ctx)\n\tif !shouldStart {\n\t\treturn nil\n\t}\n\n\tc.queues.startStopMu.Lock()\n\tdefer c.queues.startStopMu.Unlock()\n\n\t// BaseStartStop will set its stopped channel to nil after it stops, so make\n\t// sure to take a channel reference before finishing stopped.\n\tc.stopped = c.baseStartStop.StoppedUnsafe()\n\n\tproducersAsServices := func() []startstop.Service {\n\t\treturn sliceutil.Map(\n\t\t\tmaputil.Values(c.producersByQueueName),\n\t\t\tfunc(p *producer) startstop.Service { return p },\n\t\t)\n\t}\n\n\t// Startup code. Wrapped in a closure so it doesn't have to remember to\n\t// close the stopped channel if returning with an error.\n\tif err := func() error {\n\t\tif !c.config.willExecuteJobs() {\n\t\t\treturn errors.New(\"client Queues and Workers must be configured for a client to start working\")\n\t\t}\n\t\tif c.config.Workers != nil && len(c.config.Workers.workersMap) < 1 {\n\t\t\treturn errors.New(\"at least one Worker must be added to the Workers bundle\")\n\t\t}\n\n\t\t// Before doing anything else, make an initial connection to the database to\n\t\t// verify that it appears healthy. Many of the subcomponents below start up\n\t\t// in a goroutine and in case of initial failure, only produce a log line,\n\t\t// so even in the case of a fundamental failure like the database not being\n\t\t// available, the client appears to have started even though it's completely\n\t\t// non-functional. Here we try to make an initial assessment of health and\n\t\t// return quickly in case of an apparent problem.\n\t\t_, err := c.driver.GetExecutor().Exec(fetchCtx, \"SELECT 1\")\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error making initial connection to database: %w\", err)\n\t\t}\n\n\t\t// Each time we start, we need a fresh completer subscribe channel to\n\t\t// send job completion events on, because the completer will close it\n\t\t// each time it shuts down.\n\t\tcompleterSubscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 10)\n\t\tc.completer.ResetSubscribeChan(completerSubscribeCh)\n\t\tc.subscriptionManager.ResetSubscribeChan(completerSubscribeCh)\n\n\t\t// In case of error, stop any services that might have started. This\n\t\t// is safe because even services that were never started will still\n\t\t// tolerate being stopped.\n\t\tstopServicesOnError := func() {\n\t\t\tstartstop.StopAllParallel(c.services...)\n\t\t}\n\n\t\t// The completer is part of the services list below, but although it can\n\t\t// stop gracefully along with all the other services, it needs to be\n\t\t// started with a context that's _not_ cancelled if the user-provided\n\t\t// context is cancelled.  This ensures that even when fetch is cancelled on\n\t\t// shutdown, the completer is still given a separate opportunity to start\n\t\t// stopping only after the producers have finished up and returned.\n\t\tif err := c.completer.Start(context.WithoutCancel(ctx)); err != nil {\n\t\t\tstopServicesOnError()\n\t\t\treturn err\n\t\t}\n\n\t\t// We use separate contexts for fetching and working to allow for a graceful\n\t\t// stop. Both inherit from the provided context, so if it's cancelled, a\n\t\t// more aggressive stop will be initiated.\n\t\tworkCtx, workCancel := context.WithCancelCause(withClient[TTx](ctx, c))\n\n\t\tif err := startstop.StartAll(fetchCtx, c.services...); err != nil {\n\t\t\tstopServicesOnError()\n\t\t\treturn err\n\t\t}\n\n\t\tfor _, producer := range c.producersByQueueName {\n\t\t\tproducer := producer\n\n\t\t\tif err := producer.StartWorkContext(fetchCtx, workCtx); err != nil {\n\t\t\t\tstartstop.StopAllParallel(producersAsServices()...)\n\t\t\t\tstopServicesOnError()\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tc.queues.fetchCtx = fetchCtx //nolint:fatcontext\n\t\tc.queues.workCtx = workCtx\n\t\tc.workCancel = workCancel\n\n\t\treturn nil\n\t}(); err != nil {\n\t\tdefer stopped()\n\t\tif errors.Is(context.Cause(fetchCtx), startstop.ErrStop) {\n\t\t\treturn rivercommon.ErrShutdown\n\t\t}\n\t\treturn err\n\t}\n\n\t// Generate producer services while c.queues.startStopMu.Lock() is still\n\t// held. This is used for WaitAllStarted below, but don't use it elsewhere\n\t// because new producers may have been added while the client is running.\n\tproducerServices := producersAsServices()\n\n\tgo func() {\n\t\t// Wait for all subservices to start up before signaling our own start.\n\t\t// This isn't strictly needed, but gives tests a way to fully confirm\n\t\t// that all goroutines for subservices are spun up before continuing.\n\t\t//\n\t\t// Stop also cancels the \"started\" channel, so in case of a context\n\t\t// cancellation, this statement will fall through. The client will\n\t\t// briefly start, but then immediately stop again.\n\t\tstartstop.WaitAllStarted(append(\n\t\t\tc.services,\n\t\t\tproducerServices..., // see comment on this variable\n\t\t)...)\n\n\t\tstarted()\n\t\tdefer stopped()\n\n\t\tc.baseService.Logger.InfoContext(ctx, \"River client started\", slog.String(\"client_id\", c.ID()))\n\t\tdefer c.baseService.Logger.InfoContext(ctx, \"River client stopped\", slog.String(\"client_id\", c.ID()))\n\n\t\t// The call to Stop cancels this context. Block here until shutdown.\n\t\t<-fetchCtx.Done()\n\n\t\tc.queues.startStopMu.Lock()\n\t\tdefer c.queues.startStopMu.Unlock()\n\n\t\t// On stop, have the producers stop fetching first of all.\n\t\tc.baseService.Logger.DebugContext(ctx, c.baseService.Name+\": Stopping producers\")\n\t\tstartstop.StopAllParallel(producersAsServices()...)\n\t\tc.baseService.Logger.DebugContext(ctx, c.baseService.Name+\": All producers stopped\")\n\n\t\t// Stop all mainline services where stop order isn't important.\n\t\tstartstop.StopAllParallel(append(\n\t\t\t// This list of services contains the completer, which should always\n\t\t\t// stop after the producers so that any remaining work that was enqueued\n\t\t\t// will have a chance to have its state completed as it finishes.\n\t\t\t//\n\t\t\t// TODO: there's a risk here that the completer is stuck on a job that\n\t\t\t// won't complete. We probably need a timeout or way to move on in those\n\t\t\t// cases.\n\t\t\tc.services,\n\n\t\t\t// Will only be started if this client was leader, but can tolerate a\n\t\t\t// stop without having been started.\n\t\t\tc.queueMaintainer,\n\t\t)...)\n\t}()\n\n\treturn nil\n}\n\n// Stop performs a graceful shutdown of the Client. It signals all producers\n// to stop fetching new jobs and waits for any fetched or in-progress jobs to\n// complete before exiting. If the provided context is done before shutdown has\n// completed, Stop will return immediately with the context's error.\n//\n// There's no need to call this method if a hard stop has already been initiated\n// by cancelling the context passed to Start or by calling StopAndCancel.\nfunc (c *Client[TTx]) Stop(ctx context.Context) error {\n\tshouldStop, stopped, finalizeStop := c.baseStartStop.StopInit()\n\tif !shouldStop {\n\t\treturn nil\n\t}\n\n\tselect {\n\tcase <-ctx.Done(): // stop context cancelled\n\t\tfinalizeStop(false) // not stopped; allow Stop to be called again\n\t\treturn ctx.Err()\n\tcase <-stopped:\n\t\tfinalizeStop(true)\n\t\treturn nil\n\t}\n}\n\n// StopAndCancel shuts down the client and cancels all work in progress. It is a\n// more aggressive stop than Stop because the contexts for any in-progress jobs\n// are cancelled. However, it still waits for jobs to complete before returning,\n// even though their contexts are cancelled. If the provided context is done\n// before shutdown has completed, Stop will return immediately with the\n// context's error.\n//\n// This can also be initiated by cancelling the context passed to Run. There is\n// no need to call this method if the context passed to Run is cancelled\n// instead.\nfunc (c *Client[TTx]) StopAndCancel(ctx context.Context) error {\n\tc.baseService.Logger.InfoContext(ctx, c.baseService.Name+\": Hard stop started; cancelling all work\")\n\tc.workCancel(rivercommon.ErrShutdown)\n\n\tshouldStop, stopped, finalizeStop := c.baseStartStop.StopInit()\n\tif !shouldStop {\n\t\treturn nil\n\t}\n\n\tselect {\n\tcase <-ctx.Done(): // stop context cancelled\n\t\tfinalizeStop(false) // not stopped; allow Stop to be called again\n\t\treturn ctx.Err()\n\tcase <-stopped:\n\t\tfinalizeStop(true)\n\t\treturn nil\n\t}\n}\n\n// Stopped returns a channel that will be closed when the Client has stopped.\n// It can be used to wait for a graceful shutdown to complete.\n//\n// It is not affected by any contexts passed to Stop or StopAndCancel.\nfunc (c *Client[TTx]) Stopped() <-chan struct{} {\n\treturn c.stopped\n}\n\n// Subscribe subscribes to the provided kinds of events that occur within the\n// client, like EventKindJobCompleted for when a job completes.\n//\n// Returns a channel over which to receive events along with a cancel function\n// that can be used to cancel and tear down resources associated with the\n// subscription. It's recommended but not necessary to invoke the cancel\n// function. Resources will be freed when the client stops in case it's not.\n//\n// The event channel is buffered and sends on it are non-blocking. Consumers\n// must process events in a timely manner or it's possible for events to be\n// dropped. Any slow operations performed in a response to a receipt (e.g.\n// persisting to a database) should be made asynchronous to avoid event loss.\n//\n// Callers must specify the kinds of events they're interested in. This allows\n// for forward compatibility in case new kinds of events are added in future\n// versions. If new event kinds are added, callers will have to explicitly add\n// them to their requested list and ensure they can be handled correctly.\nfunc (c *Client[TTx]) Subscribe(kinds ...EventKind) (<-chan *Event, func()) {\n\treturn c.SubscribeConfig(&SubscribeConfig{Kinds: kinds})\n}\n\n// The default maximum size of the subscribe channel. Events that would overflow\n// it will be dropped.\nconst subscribeChanSizeDefault = 1_000\n\n// SubscribeConfig is more thorough subscription configuration used for\n// Client.SubscribeConfig.\ntype SubscribeConfig struct {\n\t// ChanSize is the size of the buffered channel that will be created for the\n\t// subscription. Incoming events that overall this number because a listener\n\t// isn't reading from the channel in a timely manner will be dropped.\n\t//\n\t// Defaults to 1000.\n\tChanSize int\n\n\t// Kinds are the kinds of events that the subscription will receive.\n\t// Requiring that kinds are specified explicitly allows for forward\n\t// compatibility in case new kinds of events are added in future versions.\n\t// If new event kinds are added, callers will have to explicitly add them to\n\t// their requested list and ensure they can be handled correctly.\n\tKinds []EventKind\n}\n\n// Special internal variant that lets us inject an overridden size.\nfunc (c *Client[TTx]) SubscribeConfig(config *SubscribeConfig) (<-chan *Event, func()) {\n\tif c.subscriptionManager == nil {\n\t\tpanic(\"created a subscription on a client that will never work jobs (Workers not configured)\")\n\t}\n\n\treturn c.subscriptionManager.SubscribeConfig(config)\n}\n\n// Dump aggregate stats from job completions to logs periodically.  These\n// numbers don't mean much in themselves, but can give a rough idea of the\n// proportions of each compared to each other, and may help flag outlying values\n// indicative of a problem.\nfunc (c *Client[TTx]) logStatsLoop(ctx context.Context, shouldStart bool, started, stopped func()) error {\n\tif !shouldStart {\n\t\treturn nil\n\t}\n\n\tgo func() {\n\t\tstarted()\n\t\tdefer stopped() // this defer should come first so it's last out\n\n\t\tticker := time.NewTicker(5 * time.Second)\n\t\tdefer ticker.Stop()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\n\t\t\tcase <-ticker.C:\n\t\t\t\tc.subscriptionManager.logStats(ctx, c.baseService.Name)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\nfunc (c *Client[TTx]) handleLeadershipChangeLoop(ctx context.Context, shouldStart bool, started, stopped func()) error {\n\thandleLeadershipChange := func(ctx context.Context, notification *leadership.Notification) {\n\t\tc.baseService.Logger.DebugContext(ctx, c.baseService.Name+\": Election change received\",\n\t\t\tslog.String(\"client_id\", c.config.ID), slog.Bool(\"is_leader\", notification.IsLeader))\n\n\t\tswitch {\n\t\tcase notification.IsLeader:\n\t\t\t// Starting the queue maintainer can take a little time so send to\n\t\t\t// this test signal _first_ so tests waiting on it can finish,\n\t\t\t// cancel the queue maintainer start, and overall run much faster.\n\t\t\tc.testSignals.electedLeader.Signal(struct{}{})\n\n\t\t\tif err := c.queueMaintainer.Start(ctx); err != nil {\n\t\t\t\tc.baseService.Logger.ErrorContext(ctx, \"Error starting queue maintainer\", slog.String(\"err\", err.Error()))\n\t\t\t}\n\n\t\tdefault:\n\t\t\tc.queueMaintainer.Stop()\n\t\t}\n\t}\n\n\tif !shouldStart {\n\t\treturn nil\n\t}\n\n\tgo func() {\n\t\tstarted()\n\t\tdefer stopped() // this defer should come first so it's last out\n\n\t\tsub := c.elector.Listen()\n\t\tdefer sub.Unlisten()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\n\t\t\tcase notification := <-sub.C():\n\t\t\t\thandleLeadershipChange(ctx, notification)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// Driver exposes the underlying driver used by the client.\n//\n// API is not stable. DO NOT USE.\nfunc (c *Client[TTx]) Driver() riverdriver.Driver[TTx] {\n\treturn c.driver\n}\n\n// JobCancel cancels the job with the given ID. If possible, the job is\n// cancelled immediately and will not be retried. The provided context is used\n// for the underlying Postgres update and can be used to cancel the operation or\n// apply a timeout.\n//\n// If the job is still in the queue (available, scheduled, or retryable), it is\n// immediately marked as cancelled and will not be retried.\n//\n// If the job is already finalized (cancelled, completed, or discarded), no\n// changes are made.\n//\n// If the job is currently running, it is not immediately cancelled, but is\n// instead marked for cancellation. The client running the job will also be\n// notified (via LISTEN/NOTIFY) to cancel the running job's context. Although\n// the job's context will be cancelled, since Go does not provide a mechanism to\n// interrupt a running goroutine the job will continue running until it returns.\n// As always, it is important for workers to respect context cancellation and\n// return promptly when the job context is done.\n//\n// Once the cancellation signal is received by the client running the job, any\n// error returned by that job will result in it being cancelled permanently and\n// not retried. However if the job returns no error, it will be completed as\n// usual.\n//\n// In the event the running job finishes executing _before_ the cancellation\n// signal is received but _after_ this update was made, the behavior depends on\n// which state the job is being transitioned into (based on its return error):\n//\n//   - If the job completed successfully, was cancelled from within, or was\n//     discarded due to exceeding its max attempts, the job will be updated as\n//     usual.\n//   - If the job was snoozed to run again later or encountered a retryable error,\n//     the job will be marked as cancelled and will not be attempted again.\n//\n// Returns the up-to-date JobRow for the specified jobID if it exists. Returns\n// ErrNotFound if the job doesn't exist.\nfunc (c *Client[TTx]) JobCancel(ctx context.Context, jobID int64) (*rivertype.JobRow, error) {\n\treturn c.jobCancel(ctx, c.driver.GetExecutor(), jobID)\n}\n\n// JobCancelTx cancels the job with the given ID within the specified\n// transaction. This variant lets a caller cancel a job atomically alongside\n// other database changes. A cancelled job doesn't take effect until the\n// transaction commits, and if the transaction rolls back, so too is the\n// cancelled job.\n//\n// If possible, the job is cancelled immediately and will not be retried. The\n// provided context is used for the underlying Postgres update and can be used\n// to cancel the operation or apply a timeout.\n//\n// If the job is still in the queue (available, scheduled, or retryable), it is\n// immediately marked as cancelled and will not be retried.\n//\n// If the job is already finalized (cancelled, completed, or discarded), no\n// changes are made.\n//\n// If the job is currently running, it is not immediately cancelled, but is\n// instead marked for cancellation. The client running the job will also be\n// notified (via LISTEN/NOTIFY) to cancel the running job's context. Although\n// the job's context will be cancelled, since Go does not provide a mechanism to\n// interrupt a running goroutine the job will continue running until it returns.\n// As always, it is important for workers to respect context cancellation and\n// return promptly when the job context is done.\n//\n// Once the cancellation signal is received by the client running the job, any\n// error returned by that job will result in it being cancelled permanently and\n// not retried. However if the job returns no error, it will be completed as\n// usual.\n//\n// In the event the running job finishes executing _before_ the cancellation\n// signal is received but _after_ this update was made, the behavior depends on\n// which state the job is being transitioned into (based on its return error):\n//\n//   - If the job completed successfully, was cancelled from within, or was\n//     discarded due to exceeding its max attempts, the job will be updated as\n//     usual.\n//   - If the job was snoozed to run again later or encountered a retryable error,\n//     the job will be marked as cancelled and will not be attempted again.\n//\n// Returns the up-to-date JobRow for the specified jobID if it exists. Returns\n// ErrNotFound if the job doesn't exist.\nfunc (c *Client[TTx]) JobCancelTx(ctx context.Context, tx TTx, jobID int64) (*rivertype.JobRow, error) {\n\treturn c.jobCancel(ctx, c.driver.UnwrapExecutor(tx), jobID)\n}\n\nfunc (c *Client[TTx]) jobCancel(ctx context.Context, exec riverdriver.Executor, jobID int64) (*rivertype.JobRow, error) {\n\treturn exec.JobCancel(ctx, &riverdriver.JobCancelParams{\n\t\tID:                jobID,\n\t\tCancelAttemptedAt: c.baseService.Time.NowUTC(),\n\t\tControlTopic:      string(notifier.NotificationTopicControl),\n\t})\n}\n\n// JobDelete deletes the job with the given ID from the database, returning the\n// deleted row if it was deleted. Jobs in the running state are not deleted,\n// instead returning rivertype.ErrJobRunning.\nfunc (c *Client[TTx]) JobDelete(ctx context.Context, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.GetExecutor().JobDelete(ctx, id)\n}\n\n// JobDelete deletes the job with the given ID from the database, returning the\n// deleted row if it was deleted. Jobs in the running state are not deleted,\n// instead returning rivertype.ErrJobRunning. This variant lets a caller retry a\n// job atomically alongside other database changes. A deleted job isn't deleted\n// until the transaction commits, and if the transaction rolls back, so too is\n// the deleted job.\nfunc (c *Client[TTx]) JobDeleteTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.UnwrapExecutor(tx).JobDelete(ctx, id)\n}\n\n// JobGet fetches a single job by its ID. Returns the up-to-date JobRow for the\n// specified jobID if it exists. Returns ErrNotFound if the job doesn't exist.\nfunc (c *Client[TTx]) JobGet(ctx context.Context, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.GetExecutor().JobGetByID(ctx, id)\n}\n\n// JobGetTx fetches a single job by its ID, within a transaction. Returns the\n// up-to-date JobRow for the specified jobID if it exists. Returns ErrNotFound\n// if the job doesn't exist.\nfunc (c *Client[TTx]) JobGetTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.UnwrapExecutor(tx).JobGetByID(ctx, id)\n}\n\n// JobRetry updates the job with the given ID to make it immediately available\n// to be retried. Jobs in the running state are not touched, while jobs in any\n// other state are made available. To prevent jobs already waiting in the queue\n// from being set back in line, the job's scheduled_at field is set to the\n// current time only if it's not already in the past.\n//\n// MaxAttempts is also incremented by one if the job has already exhausted its\n// max attempts.\nfunc (c *Client[TTx]) JobRetry(ctx context.Context, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.GetExecutor().JobRetry(ctx, id)\n}\n\n// JobRetryTx updates the job with the given ID to make it immediately available\n// to be retried, within the specified transaction. This variant lets a caller\n// retry a job atomically alongside other database changes. A retried job isn't\n// visible to be worked until the transaction commits, and if the transaction\n// rolls back, so too is the retried job.\n//\n// Jobs in the running state are not touched, while jobs in any other state are\n// made available. To prevent jobs already waiting in the queue from being set\n// back in line, the job's scheduled_at field is set to the current time only if\n// it's not already in the past.\n//\n// MaxAttempts is also incremented by one if the job has already exhausted its\n// max attempts.\nfunc (c *Client[TTx]) JobRetryTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {\n\treturn c.driver.UnwrapExecutor(tx).JobRetry(ctx, id)\n}\n\n// ID returns the unique ID of this client as set in its config or\n// auto-generated if not specified.\nfunc (c *Client[TTx]) ID() string {\n\treturn c.config.ID\n}\n\nfunc insertParamsFromConfigArgsAndOptions(archetype *baseservice.Archetype, config *Config, args JobArgs, insertOpts *InsertOpts) (*rivertype.JobInsertParams, error) {\n\tencodedArgs, err := json.Marshal(args)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error marshaling args to JSON: %w\", err)\n\t}\n\n\tif insertOpts == nil {\n\t\tinsertOpts = &InsertOpts{}\n\t}\n\n\tvar jobInsertOpts InsertOpts\n\tif argsWithOpts, ok := args.(JobArgsWithInsertOpts); ok {\n\t\tjobInsertOpts = argsWithOpts.InsertOpts()\n\t}\n\n\t// If the time is stubbed (in a test), use that for `created_at`. Otherwise,\n\t// leave an empty value which will either use the database's `now()` or be defaulted\n\t// by drivers as necessary.\n\tcreatedAt := archetype.Time.NowUTCOrNil()\n\n\tmaxAttempts := valutil.FirstNonZero(insertOpts.MaxAttempts, jobInsertOpts.MaxAttempts, config.MaxAttempts)\n\tpriority := valutil.FirstNonZero(insertOpts.Priority, jobInsertOpts.Priority, rivercommon.PriorityDefault)\n\tqueue := valutil.FirstNonZero(insertOpts.Queue, jobInsertOpts.Queue, rivercommon.QueueDefault)\n\n\tif err := validateQueueName(queue); err != nil {\n\t\treturn nil, err\n\t}\n\n\ttags := insertOpts.Tags\n\tif insertOpts.Tags == nil {\n\t\ttags = jobInsertOpts.Tags\n\t}\n\tif tags == nil {\n\t\ttags = []string{}\n\t} else {\n\t\tfor _, tag := range tags {\n\t\t\tif len(tag) > 255 {\n\t\t\t\treturn nil, errors.New(\"tags should be a maximum of 255 characters long\")\n\t\t\t}\n\t\t\tif !tagRE.MatchString(tag) {\n\t\t\t\treturn nil, errors.New(\"tags should match regex \" + tagRE.String())\n\t\t\t}\n\t\t}\n\t}\n\n\tif priority > 4 {\n\t\treturn nil, errors.New(\"priority must be between 1 and 4\")\n\t}\n\n\tuniqueOpts := insertOpts.UniqueOpts\n\tif uniqueOpts.isEmpty() {\n\t\tuniqueOpts = jobInsertOpts.UniqueOpts\n\t}\n\tif err := uniqueOpts.validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := insertOpts.Metadata\n\tif len(metadata) == 0 {\n\t\tmetadata = []byte(\"{}\")\n\t}\n\n\tinsertParams := &rivertype.JobInsertParams{\n\t\tArgs:        args,\n\t\tCreatedAt:   createdAt,\n\t\tEncodedArgs: encodedArgs,\n\t\tKind:        args.Kind(),\n\t\tMaxAttempts: maxAttempts,\n\t\tMetadata:    metadata,\n\t\tPriority:    priority,\n\t\tQueue:       queue,\n\t\tState:       rivertype.JobStateAvailable,\n\t\tTags:        tags,\n\t}\n\tif !uniqueOpts.isEmpty() {\n\t\tinternalUniqueOpts := (*dbunique.UniqueOpts)(&uniqueOpts)\n\t\tinsertParams.UniqueKey, err = dbunique.UniqueKey(archetype.Time, internalUniqueOpts, insertParams)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tinsertParams.UniqueStates = internalUniqueOpts.StateBitmask()\n\t}\n\n\tswitch {\n\tcase !insertOpts.ScheduledAt.IsZero():\n\t\tinsertParams.ScheduledAt = &insertOpts.ScheduledAt\n\t\tinsertParams.State = rivertype.JobStateScheduled\n\tcase !jobInsertOpts.ScheduledAt.IsZero():\n\t\tinsertParams.ScheduledAt = &jobInsertOpts.ScheduledAt\n\t\tinsertParams.State = rivertype.JobStateScheduled\n\tdefault:\n\t\t// Use a stubbed time if there was one, but otherwise prefer the value\n\t\t// generated by the database. createdAt is nil unless time is stubbed.\n\t\tinsertParams.ScheduledAt = createdAt\n\t}\n\n\tif insertOpts.Pending {\n\t\tinsertParams.State = rivertype.JobStatePending\n\t}\n\n\treturn insertParams, nil\n}\n\nvar errNoDriverDBPool = errors.New(\"driver must have non-nil database pool to use non-transactional methods like Insert and InsertMany (try InsertTx or InsertManyTx instead\")\n\n// Insert inserts a new job with the provided args. Job opts can be used to\n// override any defaults that may have been provided by an implementation of\n// JobArgsWithInsertOpts.InsertOpts, as well as any global defaults. The\n// provided context is used for the underlying Postgres insert and can be used\n// to cancel the operation or apply a timeout.\n//\n//\tjobRow, err := client.Insert(insertCtx, MyArgs{}, nil)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc (c *Client[TTx]) Insert(ctx context.Context, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {\n\tif !c.driver.HasPool() {\n\t\treturn nil, errNoDriverDBPool\n\t}\n\n\ttx, err := c.driver.GetExecutor().Begin(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tinserted, err := c.insert(ctx, tx, args, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := tx.Commit(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\treturn inserted, nil\n}\n\n// InsertTx inserts a new job with the provided args on the given transaction.\n// Job opts can be used to override any defaults that may have been provided by\n// an implementation of JobArgsWithInsertOpts.InsertOpts, as well as any global\n// defaults. The provided context is used for the underlying Postgres insert and\n// can be used to cancel the operation or apply a timeout.\n//\n//\tjobRow, err := client.InsertTx(insertCtx, tx, MyArgs{}, nil)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// This variant lets a caller insert jobs atomically alongside other database\n// changes. It's also possible to insert a job outside a transaction, but this\n// usage is recommended to ensure that all data a job needs to run is available\n// by the time it starts. Because of snapshot visibility guarantees across\n// transactions, the job will not be worked until the transaction has committed,\n// and if the transaction rolls back, so too is the inserted job.\nfunc (c *Client[TTx]) InsertTx(ctx context.Context, tx TTx, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {\n\treturn c.insert(ctx, c.driver.UnwrapExecutor(tx), args, opts)\n}\n\nfunc (c *Client[TTx]) insert(ctx context.Context, tx riverdriver.ExecutorTx, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {\n\tparams := []InsertManyParams{{Args: args, InsertOpts: opts}}\n\tresults, err := c.validateParamsAndInsertMany(ctx, tx, params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn results[0], nil\n}\n\n// InsertManyParams encapsulates a single job combined with insert options for\n// use with batch insertion.\ntype InsertManyParams struct {\n\t// Args are the arguments of the job to insert.\n\tArgs JobArgs\n\n\t// InsertOpts are insertion options for this job.\n\tInsertOpts *InsertOpts\n}\n\n// InsertMany inserts many jobs at once. Each job is inserted as an\n// InsertManyParams tuple, which takes job args along with an optional set of\n// insert options, which override insert options provided by an\n// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.\n// The provided context is used for the underlying Postgres inserts and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tcount, err := client.InsertMany(ctx, []river.InsertManyParams{\n//\t\t{Args: BatchInsertArgs{}},\n//\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},\n//\t})\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// Job uniqueness is not respected when using InsertMany due to unique inserts\n// using an internal transaction and advisory lock that might lead to\n// significant lock contention. Insert unique jobs using Insert instead.\n//\n// Job uniqueness is not respected when using InsertMany due to unique inserts\n// using an internal transaction and advisory lock that might lead to\n// significant lock contention. Insert unique jobs using Insert instead.\nfunc (c *Client[TTx]) InsertMany(ctx context.Context, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {\n\tif !c.driver.HasPool() {\n\t\treturn nil, errNoDriverDBPool\n\t}\n\n\ttx, err := c.driver.GetExecutor().Begin(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tinserted, err := c.validateParamsAndInsertMany(ctx, tx, params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := tx.Commit(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\treturn inserted, nil\n}\n\n// InsertManyTx inserts many jobs at once. Each job is inserted as an\n// InsertManyParams tuple, which takes job args along with an optional set of\n// insert options, which override insert options provided by an\n// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.\n// The provided context is used for the underlying Postgres inserts and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tcount, err := client.InsertManyTx(ctx, tx, []river.InsertManyParams{\n//\t\t{Args: BatchInsertArgs{}},\n//\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},\n//\t})\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// Job uniqueness is not respected when using InsertMany due to unique inserts\n// using an internal transaction and advisory lock that might lead to\n// significant lock contention. Insert unique jobs using Insert instead.\n//\n// This variant lets a caller insert jobs atomically alongside other database\n// changes. An inserted job isn't visible to be worked until the transaction\n// commits, and if the transaction rolls back, so too is the inserted job.\nfunc (c *Client[TTx]) InsertManyTx(ctx context.Context, tx TTx, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {\n\texec := c.driver.UnwrapExecutor(tx)\n\treturn c.validateParamsAndInsertMany(ctx, exec, params)\n}\n\n// validateParamsAndInsertMany is a helper method that wraps the insertMany\n// method to provide param validation and conversion prior to calling the actual\n// insertMany method. This allows insertMany to be reused by the\n// PeriodicJobEnqueuer which cannot reference top-level river package types.\nfunc (c *Client[TTx]) validateParamsAndInsertMany(ctx context.Context, tx riverdriver.ExecutorTx, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {\n\tinsertParams, err := c.insertManyParams(params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn c.insertMany(ctx, tx, insertParams)\n}\n\n// insertMany is a shared code path for InsertMany and InsertManyTx, also used\n// by the PeriodicJobEnqueuer.\nfunc (c *Client[TTx]) insertMany(ctx context.Context, tx riverdriver.ExecutorTx, insertParams []*rivertype.JobInsertParams) ([]*rivertype.JobInsertResult, error) {\n\treturn c.insertManyShared(ctx, tx, insertParams, func(ctx context.Context, insertParams []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error) {\n\t\tresults, err := c.pilot.JobInsertMany(ctx, tx, insertParams)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn sliceutil.Map(results,\n\t\t\tfunc(result *riverdriver.JobInsertFastResult) *rivertype.JobInsertResult {\n\t\t\t\treturn (*rivertype.JobInsertResult)(result)\n\t\t\t},\n\t\t), nil\n\t})\n}\n\n// The shared code path for all Insert and InsertMany methods. It takes a\n// function that executes the actual insert operation and allows for different\n// implementations of the insert query to be passed in, each mapping their\n// results back to a common result type.\nfunc (c *Client[TTx]) insertManyShared(\n\tctx context.Context,\n\ttx riverdriver.ExecutorTx,\n\tinsertParams []*rivertype.JobInsertParams,\n\texecute func(context.Context, []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error),\n) ([]*rivertype.JobInsertResult, error) {\n\tdoInner := func(ctx context.Context) ([]*rivertype.JobInsertResult, error) {\n\t\tfinalInsertParams := sliceutil.Map(insertParams, func(params *rivertype.JobInsertParams) *riverdriver.JobInsertFastParams {\n\t\t\treturn (*riverdriver.JobInsertFastParams)(params)\n\t\t})\n\t\tresults, err := execute(ctx, finalInsertParams)\n\t\tif err != nil {\n\t\t\treturn results, err\n\t\t}\n\n\t\tqueues := make([]string, 0, 10)\n\t\tfor _, params := range insertParams {\n\t\t\tif params.State == rivertype.JobStateAvailable {\n\t\t\t\tqueues = append(queues, params.Queue)\n\t\t\t}\n\t\t}\n\t\tif err := c.maybeNotifyInsertForQueues(ctx, tx, queues); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn results, nil\n\t}\n\n\tif len(c.config.JobInsertMiddleware) > 0 {\n\t\t// Wrap middlewares in reverse order so the one defined first is wrapped\n\t\t// as the outermost function and is first to receive the operation.\n\t\tfor i := len(c.config.JobInsertMiddleware) - 1; i >= 0; i-- {\n\t\t\tmiddlewareItem := c.config.JobInsertMiddleware[i] // capture the current middleware item\n\t\t\tpreviousDoInner := doInner                        // Capture the current doInner function\n\t\t\tdoInner = func(ctx context.Context) ([]*rivertype.JobInsertResult, error) {\n\t\t\t\treturn middlewareItem.InsertMany(ctx, insertParams, previousDoInner)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn doInner(ctx)\n}\n\n// Validates input parameters for a batch insert operation and generates a set\n// of batch insert parameters.\nfunc (c *Client[TTx]) insertManyParams(params []InsertManyParams) ([]*rivertype.JobInsertParams, error) {\n\tif len(params) < 1 {\n\t\treturn nil, errors.New(\"no jobs to insert\")\n\t}\n\n\tinsertParams := make([]*rivertype.JobInsertParams, len(params))\n\tfor i, param := range params {\n\t\tif err := c.validateJobArgs(param.Args); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tinsertParamsItem, err := insertParamsFromConfigArgsAndOptions(&c.baseService.Archetype, c.config, param.Args, param.InsertOpts)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tinsertParams[i] = insertParamsItem\n\t}\n\n\treturn insertParams, nil\n}\n\n// InsertManyFast inserts many jobs at once using Postgres' `COPY FROM` mechanism,\n// making the operation quite fast and memory efficient. Each job is inserted as\n// an InsertManyParams tuple, which takes job args along with an optional set of\n// insert options, which override insert options provided by an\n// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.\n// The provided context is used for the underlying Postgres inserts and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tcount, err := client.InsertMany(ctx, []river.InsertManyParams{\n//\t\t{Args: BatchInsertArgs{}},\n//\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},\n//\t})\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// Job uniqueness is supported using this path, but unlike with `InsertMany`\n// unique conflicts cannot be handled gracefully. If a unique constraint is\n// violated, the operation will fail and no jobs will be inserted.\nfunc (c *Client[TTx]) InsertManyFast(ctx context.Context, params []InsertManyParams) (int, error) {\n\tif !c.driver.HasPool() {\n\t\treturn 0, errNoDriverDBPool\n\t}\n\n\t// Wrap in a transaction in case we need to notify about inserts.\n\ttx, err := c.driver.GetExecutor().Begin(ctx)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tinserted, err := c.insertManyFast(ctx, tx, params)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif err := tx.Commit(ctx); err != nil {\n\t\treturn 0, err\n\t}\n\treturn inserted, nil\n}\n\n// InsertManyTx inserts many jobs at once using Postgres' `COPY FROM` mechanism,\n// making the operation quite fast and memory efficient. Each job is inserted as\n// an InsertManyParams tuple, which takes job args along with an optional set of\n// insert options, which override insert options provided by an\n// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.\n// The provided context is used for the underlying Postgres inserts and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tcount, err := client.InsertManyTx(ctx, tx, []river.InsertManyParams{\n//\t\t{Args: BatchInsertArgs{}},\n//\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},\n//\t})\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// This variant lets a caller insert jobs atomically alongside other database\n// changes. An inserted job isn't visible to be worked until the transaction\n// commits, and if the transaction rolls back, so too is the inserted job.\n//\n// Job uniqueness is supported using this path, but unlike with `InsertManyTx`\n// unique conflicts cannot be handled gracefully. If a unique constraint is\n// violated, the operation will fail and no jobs will be inserted.\nfunc (c *Client[TTx]) InsertManyFastTx(ctx context.Context, tx TTx, params []InsertManyParams) (int, error) {\n\texec := c.driver.UnwrapExecutor(tx)\n\treturn c.insertManyFast(ctx, exec, params)\n}\n\nfunc (c *Client[TTx]) insertManyFast(ctx context.Context, tx riverdriver.ExecutorTx, params []InsertManyParams) (int, error) {\n\tinsertParams, err := c.insertManyParams(params)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tresults, err := c.insertManyShared(ctx, tx, insertParams, func(ctx context.Context, insertParams []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error) {\n\t\tcount, err := tx.JobInsertFastManyNoReturning(ctx, insertParams)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn make([]*rivertype.JobInsertResult, count), nil\n\t})\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn len(results), nil\n}\n\n// Notify the given queues that new jobs are available. The queues list will be\n// deduplicated and each will be checked to see if it is due for an insert\n// notification from this client.\nfunc (c *Client[TTx]) maybeNotifyInsertForQueues(ctx context.Context, tx riverdriver.ExecutorTx, queues []string) error {\n\tif len(queues) < 1 {\n\t\treturn nil\n\t}\n\n\tqueueMap := make(map[string]struct{})\n\tqueuesDeduped := make([]string, 0, len(queues))\n\tpayloads := make([]string, 0, len(queues))\n\n\tfor _, queue := range queues {\n\t\tif _, ok := queueMap[queue]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tqueueMap[queue] = struct{}{}\n\t\tif c.insertNotifyLimiter.ShouldTrigger(queue) {\n\t\t\tpayloads = append(payloads, fmt.Sprintf(\"{\\\"queue\\\": %q}\", queue))\n\t\t\tqueuesDeduped = append(queuesDeduped, queue)\n\t\t}\n\t}\n\n\tif len(payloads) < 1 {\n\t\treturn nil\n\t}\n\n\terr := tx.NotifyMany(ctx, &riverdriver.NotifyManyParams{\n\t\tTopic:   string(notifier.NotificationTopicInsert),\n\t\tPayload: payloads,\n\t})\n\tif err != nil {\n\t\tc.baseService.Logger.ErrorContext(\n\t\t\tctx,\n\t\t\tc.baseService.Name+\": Failed to send job insert notification\",\n\t\t\tslog.String(\"queues\", strings.Join(queuesDeduped, \",\")),\n\t\t\tslog.String(\"err\", err.Error()),\n\t\t)\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// emit a notification about a queue being paused or resumed.\nfunc (c *Client[TTx]) notifyQueuePauseOrResume(ctx context.Context, tx riverdriver.ExecutorTx, action controlAction, queue string, opts *QueuePauseOpts) error {\n\tc.baseService.Logger.DebugContext(ctx,\n\t\tc.baseService.Name+\": Notifying about queue state change\",\n\t\tslog.String(\"action\", string(action)),\n\t\tslog.String(\"queue\", queue),\n\t\tslog.String(\"opts\", fmt.Sprintf(\"%+v\", opts)),\n\t)\n\n\tpayload, err := json.Marshal(jobControlPayload{Action: action, Queue: queue})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = tx.NotifyMany(ctx, &riverdriver.NotifyManyParams{\n\t\tPayload: []string{string(payload)},\n\t\tTopic:   string(notifier.NotificationTopicControl),\n\t})\n\tif err != nil {\n\t\tc.baseService.Logger.ErrorContext(\n\t\t\tctx,\n\t\t\tc.baseService.Name+\": Failed to send queue state change notification\",\n\t\t\tslog.String(\"err\", err.Error()),\n\t\t)\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Validates job args prior to insertion. Currently, verifies that a worker to\n// handle the kind is registered in the configured workers bundle. An\n// insert-only client doesn't require a workers bundle be configured though, so\n// no validation occurs if one wasn't.\nfunc (c *Client[TTx]) validateJobArgs(args JobArgs) error {\n\tif c.config.Workers == nil {\n\t\treturn nil\n\t}\n\n\tif _, ok := c.config.Workers.workersMap[args.Kind()]; !ok {\n\t\treturn &UnknownJobKindError{Kind: args.Kind()}\n\t}\n\n\treturn nil\n}\n\nfunc (c *Client[TTx]) addProducer(queueName string, queueConfig QueueConfig) *producer {\n\tproducer := newProducer(&c.baseService.Archetype, c.driver.GetExecutor(), &producerConfig{\n\t\tClientID:           c.config.ID,\n\t\tCompleter:          c.completer,\n\t\tErrorHandler:       c.config.ErrorHandler,\n\t\tFetchCooldown:      c.config.FetchCooldown,\n\t\tFetchPollInterval:  c.config.FetchPollInterval,\n\t\tGlobalMiddleware:   c.config.WorkerMiddleware,\n\t\tJobTimeout:         c.config.JobTimeout,\n\t\tMaxWorkers:         queueConfig.MaxWorkers,\n\t\tNotifier:           c.notifier,\n\t\tQueue:              queueName,\n\t\tQueueEventCallback: c.subscriptionManager.distributeQueueEvent,\n\t\tRetryPolicy:        c.config.RetryPolicy,\n\t\tSchedulerInterval:  c.config.schedulerInterval,\n\t\tWorkers:            c.config.Workers,\n\t})\n\tc.producersByQueueName[queueName] = producer\n\treturn producer\n}\n\nvar nameRegex = regexp.MustCompile(`^(?:[a-z0-9])+(?:[_|\\-]?[a-z0-9]+)*$`)\n\nfunc validateQueueName(queueName string) error {\n\tif queueName == \"\" {\n\t\treturn errors.New(\"queue name cannot be empty\")\n\t}\n\tif len(queueName) > 64 {\n\t\treturn errors.New(\"queue name cannot be longer than 64 characters\")\n\t}\n\tif !nameRegex.MatchString(queueName) {\n\t\treturn fmt.Errorf(\"queue name is invalid, expected letters and numbers separated by underscores or hyphens: %q\", queueName)\n\t}\n\treturn nil\n}\n\n// JobListResult is the result of a job list operation. It contains a list of\n// jobs and a cursor for fetching the next page of results.\ntype JobListResult struct {\n\t// Jobs is a slice of job returned as part of the list operation.\n\tJobs []*rivertype.JobRow\n\n\t// LastCursor is a cursor that can be used to list the next page of jobs.\n\tLastCursor *JobListCursor\n}\n\n// JobList returns a paginated list of jobs matching the provided filters. The\n// provided context is used for the underlying Postgres query and can be used to\n// cancel the operation or apply a timeout.\n//\n//\tparams := river.NewJobListParams().First(10).State(rivertype.JobStateCompleted)\n//\tjobRows, err := client.JobList(ctx, params)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc (c *Client[TTx]) JobList(ctx context.Context, params *JobListParams) (*JobListResult, error) {\n\tif !c.driver.HasPool() {\n\t\treturn nil, errNoDriverDBPool\n\t}\n\n\tif params == nil {\n\t\tparams = NewJobListParams()\n\t}\n\tdbParams, err := params.toDBParams()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tjobs, err := dblist.JobList(ctx, c.driver.GetExecutor(), dbParams)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tres := &JobListResult{Jobs: jobs}\n\tif len(jobs) > 0 {\n\t\tres.LastCursor = jobListCursorFromJobAndParams(jobs[len(jobs)-1], params)\n\t}\n\treturn res, nil\n}\n\n// JobListTx returns a paginated list of jobs matching the provided filters. The\n// provided context is used for the underlying Postgres query and can be used to\n// cancel the operation or apply a timeout.\n//\n//\tparams := river.NewJobListParams().First(10).States(river.JobStateCompleted)\n//\tjobRows, err := client.JobListTx(ctx, tx, params)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc (c *Client[TTx]) JobListTx(ctx context.Context, tx TTx, params *JobListParams) (*JobListResult, error) {\n\tif params == nil {\n\t\tparams = NewJobListParams()\n\t}\n\n\tdbParams, err := params.toDBParams()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tjobs, err := dblist.JobList(ctx, c.driver.UnwrapExecutor(tx), dbParams)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tres := &JobListResult{Jobs: jobs}\n\tif len(jobs) > 0 {\n\t\tres.LastCursor = jobListCursorFromJobAndParams(jobs[len(jobs)-1], params)\n\t}\n\treturn res, nil\n}\n\n// PeriodicJobs returns the currently configured set of periodic jobs for the\n// client, and can be used to add new ones or remove existing ones.\nfunc (c *Client[TTx]) PeriodicJobs() *PeriodicJobBundle { return c.periodicJobs }\n\n// Driver exposes the underlying pilot used by the client.\n//\n// API is not stable. DO NOT USE.\nfunc (c *Client[TTx]) Pilot() riverpilot.Pilot {\n\treturn c.pilot\n}\n\n// Queues returns the currently configured set of queues for the client, and can\n// be used to add new ones.\nfunc (c *Client[TTx]) Queues() *QueueBundle { return c.queues }\n\n// QueueGet returns the queue with the given name. If the queue has not recently\n// been active or does not exist, returns ErrNotFound.\n//\n// The provided context is used for the underlying Postgres query and can be\n// used to cancel the operation or apply a timeout.\nfunc (c *Client[TTx]) QueueGet(ctx context.Context, name string) (*rivertype.Queue, error) {\n\treturn c.driver.GetExecutor().QueueGet(ctx, name)\n}\n\n// QueueGetTx returns the queue with the given name. If the queue has not recently\n// been active or does not exist, returns ErrNotFound.\n//\n// The provided context is used for the underlying Postgres query and can be\n// used to cancel the operation or apply a timeout.\nfunc (c *Client[TTx]) QueueGetTx(ctx context.Context, tx TTx, name string) (*rivertype.Queue, error) {\n\treturn c.driver.UnwrapExecutor(tx).QueueGet(ctx, name)\n}\n\n// QueueListResult is the result of a job list operation. It contains a list of\n// jobs and leaves room for future cursor functionality.\ntype QueueListResult struct {\n\t// Queues is a slice of queues returned as part of the list operation.\n\tQueues []*rivertype.Queue\n}\n\n// QueueList returns a list of all queues that are currently active or were\n// recently active. Limit and offset can be used to paginate the results.\n//\n// The provided context is used for the underlying Postgres query and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tparams := river.NewQueueListParams().First(10)\n//\tqueueRows, err := client.QueueListTx(ctx, tx, params)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc (c *Client[TTx]) QueueList(ctx context.Context, params *QueueListParams) (*QueueListResult, error) {\n\tif params == nil {\n\t\tparams = NewQueueListParams()\n\t}\n\n\tqueues, err := c.driver.GetExecutor().QueueList(ctx, int(params.paginationCount))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &QueueListResult{Queues: queues}, nil\n}\n\n// QueueListTx returns a list of all queues that are currently active or were\n// recently active. Limit and offset can be used to paginate the results.\n//\n// The provided context is used for the underlying Postgres query and can be\n// used to cancel the operation or apply a timeout.\n//\n//\tparams := river.NewQueueListParams().First(10)\n//\tqueueRows, err := client.QueueListTx(ctx, tx, params)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\nfunc (c *Client[TTx]) QueueListTx(ctx context.Context, tx TTx, params *QueueListParams) (*QueueListResult, error) {\n\tif params == nil {\n\t\tparams = NewQueueListParams()\n\t}\n\n\tqueues, err := c.driver.UnwrapExecutor(tx).QueueList(ctx, int(params.paginationCount))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &QueueListResult{Queues: queues}, nil\n}\n\n// QueuePause pauses the queue with the given name. When a queue is paused,\n// clients will not fetch any more jobs for that particular queue. To pause all\n// queues at once, use the special queue name \"*\".\n//\n// Clients with a configured notifier should receive a notification about the\n// paused queue(s) within a few milliseconds of the transaction commit. Clients\n// in poll-only mode will pause after their next poll for queue configuration.\n//\n// The provided context is used for the underlying Postgres update and can be\n// used to cancel the operation or apply a timeout. The opts are reserved for\n// future functionality.\nfunc (c *Client[TTx]) QueuePause(ctx context.Context, name string, opts *QueuePauseOpts) error {\n\ttx, err := c.driver.GetExecutor().Begin(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tif err := tx.QueuePause(ctx, name); err != nil {\n\t\treturn err\n\t}\n\n\tif err := c.notifyQueuePauseOrResume(ctx, tx, controlActionPause, name, opts); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.Commit(ctx)\n}\n\n// QueuePauseTx pauses the queue with the given name. When a queue is paused,\n// clients will not fetch any more jobs for that particular queue. To pause all\n// queues at once, use the special queue name \"*\".\n//\n// Clients with a configured notifier should receive a notification about the\n// paused queue(s) within a few milliseconds of the transaction commit. Clients\n// in poll-only mode will pause after their next poll for queue configuration.\n//\n// The provided context is used for the underlying Postgres update and can be\n// used to cancel the operation or apply a timeout. The opts are reserved for\n// future functionality.\nfunc (c *Client[TTx]) QueuePauseTx(ctx context.Context, tx TTx, name string, opts *QueuePauseOpts) error {\n\texecutorTx := c.driver.UnwrapExecutor(tx)\n\n\tif err := executorTx.QueuePause(ctx, name); err != nil {\n\t\treturn err\n\t}\n\n\tif err := c.notifyQueuePauseOrResume(ctx, executorTx, controlActionPause, name, opts); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// QueueResume resumes the queue with the given name. If the queue was\n// previously paused, any clients configured to work that queue will resume\n// fetching additional jobs. To resume all queues at once, use the special queue\n// name \"*\".\n//\n// Clients with a configured notifier should receive a notification about the\n// resumed queue(s) within a few milliseconds of the transaction commit. Clients\n// in poll-only mode will resume after their next poll for queue configuration.\n//\n// The provided context is used for the underlying Postgres update and can be\n// used to cancel the operation or apply a timeout. The opts are reserved for\n// future functionality.\nfunc (c *Client[TTx]) QueueResume(ctx context.Context, name string, opts *QueuePauseOpts) error {\n\ttx, err := c.driver.GetExecutor().Begin(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tif err := tx.QueueResume(ctx, name); err != nil {\n\t\treturn err\n\t}\n\n\tif err := c.notifyQueuePauseOrResume(ctx, tx, controlActionResume, name, opts); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.Commit(ctx)\n}\n\n// QueueResume resumes the queue with the given name. If the queue was\n// previously paused, any clients configured to work that queue will resume\n// fetching additional jobs. To resume all queues at once, use the special queue\n// name \"*\".\n//\n// Clients with a configured notifier should receive a notification about the\n// resumed queue(s) within a few milliseconds of the transaction commit. Clients\n// in poll-only mode will resume after their next poll for queue configuration.\n//\n// The provided context is used for the underlying Postgres update and can be\n// used to cancel the operation or apply a timeout. The opts are reserved for\n// future functionality.\nfunc (c *Client[TTx]) QueueResumeTx(ctx context.Context, tx TTx, name string, opts *QueuePauseOpts) error {\n\texecutorTx := c.driver.UnwrapExecutor(tx)\n\n\tif err := executorTx.QueueResume(ctx, name); err != nil {\n\t\treturn err\n\t}\n\n\tif err := c.notifyQueuePauseOrResume(ctx, executorTx, controlActionResume, name, opts); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// QueueBundle is a bundle for adding additional queues. It's made accessible\n// through Client.Queues.\ntype QueueBundle struct {\n\t// Function that adds a producer to the associated client.\n\taddProducer func(queueName string, queueConfig QueueConfig) *producer\n\n\tfetchCtx context.Context //nolint:containedctx\n\n\t// Mutex that's acquired when client is starting and stopping and when a\n\t// queue is being added so that we can be sure that a client is fully\n\t// stopped or fully started when adding a new queue.\n\tstartStopMu sync.Mutex\n\n\tworkCtx context.Context //nolint:containedctx\n}\n\n// Add adds a new queue to the client. If the client is already started, a\n// producer for the queue is started. Context is inherited from the one given to\n// Client.Start.\nfunc (b *QueueBundle) Add(queueName string, queueConfig QueueConfig) error {\n\tif err := queueConfig.validate(queueName); err != nil {\n\t\treturn err\n\t}\n\n\tb.startStopMu.Lock()\n\tdefer b.startStopMu.Unlock()\n\n\tproducer := b.addProducer(queueName, queueConfig)\n\n\t// Start the queue if the client is already started.\n\tif b.fetchCtx != nil && b.fetchCtx.Err() == nil {\n\t\tif err := producer.StartWorkContext(b.fetchCtx, b.workCtx); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Generates a default client ID using the current hostname and time.\nfunc defaultClientID(startedAt time.Time) string {\n\thost, _ := os.Hostname()\n\tif host == \"\" {\n\t\thost = \"unknown_host\"\n\t}\n\n\treturn defaultClientIDWithHost(startedAt, host)\n}\n\n// Same as the above, but allows host injection for testability.\nfunc defaultClientIDWithHost(startedAt time.Time, host string) string {\n\tconst maxHostLength = 60\n\n\t// Truncate degenerately long host names.\n\thost = strings.ReplaceAll(host, \".\", \"_\")\n\tif len(host) > maxHostLength {\n\t\thost = host[0:maxHostLength]\n\t}\n\n\t// Dots, hyphens, and colons aren't particularly friendly for double click\n\t// to select (depends on application and configuration), so avoid them all\n\t// in favor of underscores.\n\t//\n\t// Go's time package is really dumb and can't format subseconds without\n\t// using a dot. So use the dot, then replace it with an underscore below.\n\tconst rfc3339Compact = \"2006_01_02T15_04_05.000000\"\n\n\treturn host + \"_\" + strings.Replace(startedAt.Format(rfc3339Compact), \".\", \"_\", 1)\n}\n"
        },
        {
          "name": "client_test.go",
          "type": "blob",
          "size": 188.0634765625,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgerrcode\"\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/jackc/pgx/v5/stdlib\"\n\t\"github.com/robfig/cron/v3\"\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/tidwall/sjson\"\n\n\t\"github.com/riverqueue/river/internal/dbunique\"\n\t\"github.com/riverqueue/river/internal/maintenance\"\n\t\"github.com/riverqueue/river/internal/notifier\"\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/internal/util/dbutil\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverdatabasesql\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/startstoptest\"\n\t\"github.com/riverqueue/river/rivershared/testfactory\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivershared/util/randutil\"\n\t\"github.com/riverqueue/river/rivershared/util/serviceutil\"\n\t\"github.com/riverqueue/river/rivershared/util/sliceutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\ntype noOpArgs struct {\n\tName string `json:\"name\"`\n}\n\nfunc (noOpArgs) Kind() string { return \"noOp\" }\n\ntype noOpWorker struct {\n\tWorkerDefaults[noOpArgs]\n}\n\nfunc (w *noOpWorker) Work(ctx context.Context, job *Job[noOpArgs]) error { return nil }\n\ntype periodicJobArgs struct{}\n\nfunc (periodicJobArgs) Kind() string { return \"periodic_job\" }\n\ntype periodicJobWorker struct {\n\tWorkerDefaults[periodicJobArgs]\n}\n\nfunc (w *periodicJobWorker) Work(ctx context.Context, job *Job[periodicJobArgs]) error {\n\treturn nil\n}\n\ntype callbackFunc func(context.Context, *Job[callbackArgs]) error\n\nfunc makeAwaitCallback(startedCh chan<- int64, doneCh chan struct{}) callbackFunc {\n\treturn func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\tclient := ClientFromContext[pgx.Tx](ctx)\n\t\tclient.config.Logger.InfoContext(ctx, \"callback job started with id=\"+strconv.FormatInt(job.ID, 10))\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase startedCh <- job.ID:\n\t\t}\n\n\t\t// await done signal, or context cancellation:\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-doneCh:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\ntype callbackArgs struct {\n\tName string `json:\"name\"`\n}\n\nfunc (callbackArgs) Kind() string { return \"callback\" }\n\ntype callbackWorker struct {\n\tWorkerDefaults[callbackArgs]\n\tfn callbackFunc\n}\n\nfunc (w *callbackWorker) Work(ctx context.Context, job *Job[callbackArgs]) error {\n\treturn w.fn(ctx, job)\n}\n\n// A small wrapper around Client that gives us a struct that corrects the\n// client's Stop function so that it can implement startstop.Service.\ntype clientWithSimpleStop[TTx any] struct {\n\t*Client[TTx]\n}\n\nfunc (c *clientWithSimpleStop[TTx]) Started() <-chan struct{} {\n\treturn c.baseStartStop.Started()\n}\n\nfunc (c *clientWithSimpleStop[TTx]) Stop() {\n\t_ = c.Client.Stop(context.Background())\n}\n\nfunc newTestConfig(t *testing.T, callback callbackFunc) *Config {\n\tt.Helper()\n\tworkers := NewWorkers()\n\tif callback != nil {\n\t\tAddWorker(workers, &callbackWorker{fn: callback})\n\t}\n\tAddWorker(workers, &noOpWorker{})\n\n\treturn &Config{\n\t\tFetchCooldown:     20 * time.Millisecond,\n\t\tFetchPollInterval: 50 * time.Millisecond,\n\t\tLogger:            riversharedtest.Logger(t),\n\t\tMaxAttempts:       MaxAttemptsDefault,\n\t\tQueues:            map[string]QueueConfig{QueueDefault: {MaxWorkers: 50}},\n\t\tTestOnly:          true, // disables staggered start in maintenance services\n\t\tWorkers:           workers,\n\t\tschedulerInterval: riverinternaltest.SchedulerShortInterval,\n\t\ttime:              &riversharedtest.TimeStub{},\n\t}\n}\n\nfunc newTestClient(t *testing.T, dbPool *pgxpool.Pool, config *Config) *Client[pgx.Tx] {\n\tt.Helper()\n\n\tclient, err := NewClient(riverpgxv5.New(dbPool), config)\n\trequire.NoError(t, err)\n\n\treturn client\n}\n\nfunc startClient[TTx any](ctx context.Context, t *testing.T, client *Client[TTx]) {\n\tt.Helper()\n\n\tif err := client.Start(ctx); err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tt.Cleanup(func() {\n\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\tdefer cancel()\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n}\n\nfunc runNewTestClient(ctx context.Context, t *testing.T, config *Config) *Client[pgx.Tx] {\n\tt.Helper()\n\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\tclient := newTestClient(t, dbPool, config)\n\tstartClient(ctx, t, client)\n\treturn client\n}\n\nfunc subscribe[TTx any](t *testing.T, client *Client[TTx]) <-chan *Event {\n\tt.Helper()\n\n\tsubscribeChan, cancel := client.Subscribe(\n\t\tEventKindJobCancelled,\n\t\tEventKindJobCompleted,\n\t\tEventKindJobFailed,\n\t\tEventKindJobSnoozed,\n\t\tEventKindQueuePaused,\n\t\tEventKindQueueResumed,\n\t)\n\tt.Cleanup(cancel)\n\treturn subscribeChan\n}\n\nfunc Test_Client(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tconfig *Config\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\t// Alternate setup returning only client Config rather than a full Client.\n\tsetupConfig := func(t *testing.T) (*Config, *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\n\t\treturn config, &testBundle{\n\t\t\tconfig: config,\n\t\t\tdbPool: dbPool,\n\t\t}\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\treturn newTestClient(t, bundle.dbPool, config), bundle\n\t}\n\n\tt.Run(\"StartInsertAndWork\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tworkedChan := make(chan struct{})\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tworkedChan <- struct{}{}\n\t\t\treturn nil\n\t\t}))\n\n\t\tstartClient(ctx, t, client)\n\n\t\t_, err := client.Insert(ctx, &JobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, workedChan)\n\t})\n\n\tt.Run(\"Queues_Add_BeforeStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tworkedChan := make(chan struct{})\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tworkedChan <- struct{}{}\n\t\t\treturn nil\n\t\t}))\n\n\t\tqueueName := \"new_queue\"\n\t\terr := client.Queues().Add(queueName, QueueConfig{\n\t\t\tMaxWorkers: 2,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tstartClient(ctx, t, client)\n\n\t\t_, err = client.Insert(ctx, &JobArgs{}, &InsertOpts{\n\t\t\tQueue: queueName,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, workedChan)\n\t})\n\n\tt.Run(\"Queues_Add_AfterStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tworkedChan := make(chan struct{})\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tworkedChan <- struct{}{}\n\t\t\treturn nil\n\t\t}))\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tqueueName := \"new_queue\"\n\t\terr := client.Queues().Add(queueName, QueueConfig{\n\t\t\tMaxWorkers: 2,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t_, err = client.Insert(ctx, &JobArgs{}, &InsertOpts{\n\t\t\tQueue: queueName,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, workedChan)\n\t})\n\n\tt.Run(\"Queues_Add_Stress\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tvar wg sync.WaitGroup\n\n\t\t// Uses a smaller number of workers and iterations than most stress\n\t\t// tests because there's quite a lot of mutex contention so that too\n\t\t// many can make the test run long.\n\t\tfor i := 0; i < 3; i++ {\n\t\t\twg.Add(1)\n\t\t\tworkerNum := i\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\tfor j := 0; j < 5; j++ {\n\t\t\t\t\terr := client.Queues().Add(fmt.Sprintf(\"new_queue_%d_%d_before\", workerNum, j), QueueConfig{MaxWorkers: 1})\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\terr = client.Start(ctx)\n\t\t\t\t\tif !errors.Is(err, rivercommon.ErrShutdown) {\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t}\n\n\t\t\t\t\terr = client.Queues().Add(fmt.Sprintf(\"new_queue_%d_%d_after\", workerNum, j), QueueConfig{MaxWorkers: 1})\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\tstopped := make(chan struct{})\n\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\tdefer close(stopped)\n\t\t\t\t\t\trequire.NoError(t, client.Stop(ctx))\n\t\t\t\t\t}()\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-stopped:\n\t\t\t\t\tcase <-time.After(5 * time.Second):\n\t\t\t\t\t\trequire.FailNow(t, \"Timed out waiting for service to stop\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\t\twg.Wait()\n\t})\n\n\tt.Run(\"JobCancelErrorReturned\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\treturn JobCancel(errors.New(\"a persisted internal error\"))\n\t\t}))\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes, err := client.Insert(ctx, &JobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCancelled, event.Kind)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, event.Job.State)\n\t\trequire.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)\n\n\t\tupdatedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, updatedJob.State)\n\t\trequire.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)\n\t})\n\n\tt.Run(\"JobCancelErrorReturnedWithNilErr\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\treturn JobCancel(nil)\n\t\t}))\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes, err := client.Insert(ctx, &JobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCancelled, event.Kind)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, event.Job.State)\n\t\trequire.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)\n\n\t\tupdatedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, updatedJob.State)\n\t\trequire.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)\n\t})\n\n\tt.Run(\"JobSnoozeErrorReturned\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\treturn JobSnooze(15 * time.Minute)\n\t\t}))\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes, err := client.Insert(ctx, &JobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobSnoozed, event.Kind)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, event.Job.State)\n\t\trequire.WithinDuration(t, time.Now().Add(15*time.Minute), event.Job.ScheduledAt, 2*time.Second)\n\n\t\tupdatedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, updatedJob.State)\n\t\trequire.WithinDuration(t, time.Now().Add(15*time.Minute), updatedJob.ScheduledAt, 2*time.Second)\n\t})\n\n\t// This helper is used to test cancelling a job both _in_ a transaction and\n\t// _outside of_ a transaction. The exact same test logic applies to each case,\n\t// the only difference is a different cancelFunc provided by the specific\n\t// subtest.\n\tcancelRunningJobTestHelper := func(t *testing.T, config *Config, cancelFunc func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error)) { //nolint:thelper\n\t\tdefaultConfig, bundle := setupConfig(t)\n\t\tif config == nil {\n\t\t\tconfig = defaultConfig\n\t\t}\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tjobStartedChan := make(chan int64)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tjobStartedChan <- job.ID\n\t\t\t<-ctx.Done()\n\t\t\treturn ctx.Err()\n\t\t}))\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tinsertRes, err := client.Insert(ctx, &JobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tstartedJobID := riversharedtest.WaitOrTimeout(t, jobStartedChan)\n\t\trequire.Equal(t, insertRes.Job.ID, startedJobID)\n\n\t\t// Cancel the job:\n\t\tupdatedJob, err := cancelFunc(ctx, bundle.dbPool, client, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, updatedJob)\n\t\t// Job is still actively running at this point because the query wouldn't\n\t\t// modify that column for a running job:\n\t\trequire.Equal(t, rivertype.JobStateRunning, updatedJob.State)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCancelled, event.Kind)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, event.Job.State)\n\t\trequire.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)\n\n\t\tjobAfterCancel, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, jobAfterCancel.State)\n\t\trequire.WithinDuration(t, time.Now(), *jobAfterCancel.FinalizedAt, 2*time.Second)\n\t}\n\n\tt.Run(\"CancelRunningJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tcancelRunningJobTestHelper(t, nil, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {\n\t\t\treturn client.JobCancel(ctx, jobID)\n\t\t})\n\t})\n\n\tt.Run(\"CancelRunningJobWithLongPollInterval\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.FetchPollInterval = 60 * time.Second\n\t\tcancelRunningJobTestHelper(t, config, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {\n\t\t\treturn client.JobCancel(ctx, jobID)\n\t\t})\n\t})\n\n\tt.Run(\"CancelRunningJobInTx\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tcancelRunningJobTestHelper(t, nil, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {\n\t\t\tvar (\n\t\t\t\tjob *rivertype.JobRow\n\t\t\t\terr error\n\t\t\t)\n\t\t\ttxErr := pgx.BeginFunc(ctx, dbPool, func(tx pgx.Tx) error {\n\t\t\t\tjob, err = client.JobCancelTx(ctx, tx, jobID)\n\t\t\t\treturn err\n\t\t\t})\n\t\t\trequire.NoError(t, txErr)\n\t\t\treturn job, err\n\t\t})\n\t})\n\n\tt.Run(\"CancelScheduledJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\treturn nil\n\t\t}))\n\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes, err := client.Insert(ctx, &JobArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(5 * time.Minute)})\n\t\trequire.NoError(t, err)\n\n\t\t// Cancel the job:\n\t\tupdatedJob, err := client.JobCancel(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, updatedJob)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, updatedJob.State)\n\t\trequire.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)\n\t})\n\n\tt.Run(\"CancelNonExistentJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\t\tstartClient(ctx, t, client)\n\n\t\t// Cancel an unknown job ID:\n\t\tjobAfter, err := client.JobCancel(ctx, 0)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, jobAfter)\n\n\t\t// Cancel an unknown job ID, within a transaction:\n\t\terr = dbutil.WithTx(ctx, client.driver.GetExecutor(), func(ctx context.Context, exec riverdriver.ExecutorTx) error {\n\t\t\tjobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{ID: 0})\n\t\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\t\trequire.Nil(t, jobAfter)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n\n\tt.Run(\"AlternateSchema\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\n\t\t// Reconfigure the pool with an alternate schema, initialize a new pool\n\t\tdbPoolConfig := bundle.dbPool.Config() // a copy of the original config\n\t\tdbPoolConfig.ConnConfig.RuntimeParams[\"search_path\"] = \"alternate_schema\"\n\n\t\tdbPool, err := pgxpool.NewWithConfig(ctx, dbPoolConfig)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(dbPool.Close)\n\n\t\tclient, err := NewClient(riverpgxv5.New(dbPool), bundle.config)\n\t\trequire.NoError(t, err)\n\n\t\t// We don't actually verify that River's functional on another schema so\n\t\t// that we don't have to raise and migrate it. We cheat a little by\n\t\t// configuring a different schema and then verifying that we can't find\n\t\t// a `river_job` to confirm we're point there.\n\t\t_, err = client.Insert(ctx, &noOpArgs{}, nil)\n\t\tvar pgErr *pgconn.PgError\n\t\trequire.ErrorAs(t, err, &pgErr)\n\t\trequire.Equal(t, pgerrcode.UndefinedTable, pgErr.Code)\n\t\t// PgError has SchemaName and TableName properties, but unfortunately\n\t\t// neither contain a useful value in this case.\n\t\trequire.Equal(t, `relation \"river_job\" does not exist`, pgErr.Message)\n\t})\n\n\tt.Run(\"WithWorkerMiddleware\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\t\tmiddlewareCalled := false\n\n\t\ttype privateKey string\n\n\t\tmiddleware := &overridableJobMiddleware{\n\t\t\tworkFunc: func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {\n\t\t\t\tctx = context.WithValue(ctx, privateKey(\"middleware\"), \"called\")\n\t\t\t\tmiddlewareCalled = true\n\t\t\t\treturn doInner(ctx)\n\t\t\t},\n\t\t}\n\t\tbundle.config.WorkerMiddleware = []rivertype.WorkerMiddleware{middleware}\n\n\t\tAddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\trequire.Equal(t, \"called\", ctx.Value(privateKey(\"middleware\")))\n\t\t\treturn nil\n\t\t}))\n\n\t\tdriver := riverpgxv5.New(bundle.dbPool)\n\t\tclient, err := NewClient(driver, bundle.config)\n\t\trequire.NoError(t, err)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tresult, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, result.Job.ID, event.Job.ID)\n\t\trequire.True(t, middlewareCalled)\n\t})\n\n\tt.Run(\"WithWorkerMiddlewareOnWorker\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\t\tmiddlewareCalled := false\n\n\t\ttype privateKey string\n\n\t\tworker := &workerWithMiddleware[callbackArgs]{\n\t\t\tworkFunc: func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\t\trequire.Equal(t, \"called\", ctx.Value(privateKey(\"middleware\")))\n\t\t\t\treturn nil\n\t\t\t},\n\t\t\tmiddlewareFunc: func(job *Job[callbackArgs]) []rivertype.WorkerMiddleware {\n\t\t\t\trequire.Equal(t, \"middleware_test\", job.Args.Name, \"JSON should be decoded before middleware is called\")\n\n\t\t\t\treturn []rivertype.WorkerMiddleware{\n\t\t\t\t\t&overridableJobMiddleware{\n\t\t\t\t\t\tworkFunc: func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {\n\t\t\t\t\t\t\tctx = context.WithValue(ctx, privateKey(\"middleware\"), \"called\")\n\t\t\t\t\t\t\tmiddlewareCalled = true\n\t\t\t\t\t\t\treturn doInner(ctx)\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t},\n\t\t}\n\n\t\tAddWorker(bundle.config.Workers, worker)\n\n\t\tdriver := riverpgxv5.New(bundle.dbPool)\n\t\tclient, err := NewClient(driver, bundle.config)\n\t\trequire.NoError(t, err)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tresult, err := client.Insert(ctx, callbackArgs{Name: \"middleware_test\"}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, result.Job.ID, event.Job.ID)\n\t\trequire.True(t, middlewareCalled)\n\t})\n\n\tt.Run(\"PauseAndResumeSingleQueue\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes1, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes1.Job.ID, event.Job.ID)\n\n\t\trequire.NoError(t, client.QueuePause(ctx, QueueDefault, nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: QueueDefault}}, event)\n\n\t\tinsertRes2, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tselect {\n\t\tcase <-subscribeChan:\n\t\t\tt.Fatal(\"expected job 2 to not start on paused queue\")\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\n\t\trequire.NoError(t, client.QueueResume(ctx, QueueDefault, nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: QueueDefault}}, event)\n\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes2.Job.ID, event.Job.ID)\n\t})\n\n\tt.Run(\"PauseAndResumeMultipleQueues\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\tconfig.Queues[\"alternate\"] = QueueConfig{MaxWorkers: 10}\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\tinsertRes1, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes1.Job.ID, event.Job.ID)\n\n\t\t// Pause only the default queue:\n\t\trequire.NoError(t, client.QueuePause(ctx, QueueDefault, nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: QueueDefault}}, event)\n\n\t\tinsertRes2, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tselect {\n\t\tcase <-subscribeChan:\n\t\t\tt.Fatal(\"expected job 2 to not start on paused queue\")\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\n\t\t// alternate queue should still be running:\n\t\tinsertResAlternate1, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: \"alternate\"})\n\t\trequire.NoError(t, err)\n\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertResAlternate1.Job.ID, event.Job.ID)\n\n\t\t// Pause all queues:\n\t\trequire.NoError(t, client.QueuePause(ctx, rivercommon.AllQueuesString, nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: \"alternate\"}}, event)\n\n\t\tinsertResAlternate2, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: \"alternate\"})\n\t\trequire.NoError(t, err)\n\n\t\tselect {\n\t\tcase <-subscribeChan:\n\t\t\tt.Fatal(\"expected alternate job 2 to not start on paused queue\")\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\n\t\t// Resume only the alternate queue:\n\t\trequire.NoError(t, client.QueueResume(ctx, \"alternate\", nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: \"alternate\"}}, event)\n\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertResAlternate2.Job.ID, event.Job.ID)\n\n\t\t// Resume all queues:\n\t\trequire.NoError(t, client.QueueResume(ctx, rivercommon.AllQueuesString, nil))\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: QueueDefault}}, event)\n\n\t\tevent = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes2.Job.ID, event.Job.ID)\n\t})\n\n\tt.Run(\"PauseAndResumeSingleQueueTx\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tqueue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)\n\n\t\ttx, err := bundle.dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\trequire.NoError(t, client.QueuePauseTx(ctx, tx, queue.Name, nil))\n\n\t\tqueueRes, err := client.QueueGetTx(ctx, tx, queue.Name)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), *queueRes.PausedAt, 2*time.Second)\n\n\t\t// Not paused outside transaction.\n\t\tqueueRes, err = client.QueueGet(ctx, queue.Name)\n\t\trequire.NoError(t, err)\n\t\trequire.Nil(t, queueRes.PausedAt)\n\n\t\trequire.NoError(t, client.QueueResumeTx(ctx, tx, queue.Name, nil))\n\n\t\tqueueRes, err = client.QueueGetTx(ctx, tx, queue.Name)\n\t\trequire.NoError(t, err)\n\t\trequire.Nil(t, queueRes.PausedAt)\n\t})\n\n\tt.Run(\"PausedBeforeStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tjobStartedChan := make(chan int64)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tjobStartedChan <- job.ID\n\t\t\treturn nil\n\t\t}))\n\n\t\t// Ensure queue record exists:\n\t\tqueue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)\n\n\t\t// Pause only the default queue:\n\t\trequire.NoError(t, client.QueuePause(ctx, queue.Name, nil))\n\n\t\tstartClient(ctx, t, client)\n\n\t\t_, err := client.Insert(ctx, &JobArgs{}, &InsertOpts{Queue: queue.Name})\n\t\trequire.NoError(t, err)\n\n\t\tselect {\n\t\tcase <-jobStartedChan:\n\t\t\tt.Fatal(\"expected job to not start on paused queue\")\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\t})\n\n\tt.Run(\"PollOnlyDriver\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\tbundle.config.PollOnly = true\n\n\t\tstdPool := stdlib.OpenDBFromPool(bundle.dbPool)\n\t\tt.Cleanup(func() { require.NoError(t, stdPool.Close()) })\n\n\t\tclient, err := NewClient(riverdatabasesql.New(stdPool), config)\n\t\trequire.NoError(t, err)\n\n\t\tclient.testSignals.Init()\n\n\t\t// Notifier should not have been initialized at all.\n\t\trequire.Nil(t, client.notifier)\n\n\t\tinsertRes, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\t// Despite no notifier, the client should still be able to elect itself\n\t\t// leader.\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, event.Job.State)\n\t})\n\n\tt.Run(\"PollOnlyOption\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig, bundle := setupConfig(t)\n\t\tbundle.config.PollOnly = true\n\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\t\tclient.testSignals.Init()\n\n\t\t// Notifier should not have been initialized at all.\n\t\trequire.Nil(t, client.notifier)\n\n\t\tinsertRes, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tsubscribeChan := subscribe(t, client)\n\t\tstartClient(ctx, t, client)\n\n\t\t// Despite no notifier, the client should still be able to elect itself\n\t\t// leader.\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\trequire.Equal(t, EventKindJobCompleted, event.Kind)\n\t\trequire.Equal(t, insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, event.Job.State)\n\t})\n\n\tt.Run(\"StartStopStress\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tclientWithStop := &clientWithSimpleStop[pgx.Tx]{Client: client}\n\n\t\tstartstoptest.StressErr(ctx, t, clientWithStop, rivercommon.ErrShutdown)\n\t})\n}\n\ntype workerWithMiddleware[T JobArgs] struct {\n\tWorkerDefaults[T]\n\tworkFunc       func(context.Context, *Job[T]) error\n\tmiddlewareFunc func(*Job[T]) []rivertype.WorkerMiddleware\n}\n\nfunc (w *workerWithMiddleware[T]) Work(ctx context.Context, job *Job[T]) error {\n\treturn w.workFunc(ctx, job)\n}\n\nfunc (w *workerWithMiddleware[T]) Middleware(job *Job[T]) []rivertype.WorkerMiddleware {\n\treturn w.middlewareFunc(job)\n}\n\nfunc Test_Client_Stop(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\t// Performs continual job insertion on a number of background goroutines.\n\t// Returns a `finish` function that should be deferred to stop insertion and\n\t// safely stop goroutines.\n\tdoParallelContinualInsertion := func(ctx context.Context, t *testing.T, client *Client[pgx.Tx]) func() {\n\t\tt.Helper()\n\n\t\tctx, cancel := context.WithCancel(ctx)\n\n\t\tvar wg sync.WaitGroup\n\t\tfor i := 0; i < 10; i++ {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\t\treturn\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\n\t\t\t\t\t_, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\t\t\t\t// A cancelled context may produce a variety of underlying\n\t\t\t\t\t// errors in pgx, so rather than comparing the return error,\n\t\t\t\t\t// first check if context is cancelled, and ignore an error\n\t\t\t\t\t// return if it is.\n\t\t\t\t\tif ctx.Err() != nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\t// Sleep a brief time between inserts.\n\t\t\t\t\tserviceutil.CancellableSleep(ctx, randutil.DurationBetween(1*time.Microsecond, 10*time.Millisecond))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\t\treturn func() {\n\t\t\tcancel()\n\t\t\twg.Wait()\n\t\t}\n\t}\n\n\tt.Run(\"no jobs in progress\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\tclient := runNewTestClient(ctx, t, newTestConfig(t, nil))\n\n\t\t// Should shut down quickly:\n\t\tctx, cancel := context.WithTimeout(ctx, time.Second)\n\t\tdefer cancel()\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"jobs in progress, completing promptly\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\trequire := require.New(t)\n\t\tdoneCh := make(chan struct{})\n\t\tstartedCh := make(chan int64)\n\n\t\tclient := runNewTestClient(ctx, t, newTestConfig(t, makeAwaitCallback(startedCh, doneCh)))\n\n\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\tdefer cancel()\n\n\t\t// enqueue job:\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tvar startedJobID int64\n\t\tselect {\n\t\tcase startedJobID = <-startedCh:\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t\tt.Fatal(\"timed out waiting for job to start\")\n\t\t}\n\t\trequire.Equal(insertRes.Job.ID, startedJobID)\n\n\t\t// Should not shut down immediately, not until jobs are given the signal to\n\t\t// complete:\n\t\tgo func() {\n\t\t\t<-time.After(50 * time.Millisecond)\n\t\t\tclose(doneCh)\n\t\t}()\n\n\t\trequire.NoError(client.Stop(ctx))\n\t})\n\n\tt.Run(\"jobs in progress, failing to complete before stop context\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tjobDoneChan := make(chan struct{})\n\t\tjobStartedChan := make(chan int64)\n\n\t\tcallbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase jobStartedChan <- job.ID:\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\trequire.FailNow(t, \"Did not expect job to be cancelled\")\n\t\t\tcase <-jobDoneChan:\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\tclient := runNewTestClient(ctx, t, newTestConfig(t, callbackFunc))\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tstartedJobID := riversharedtest.WaitOrTimeout(t, jobStartedChan)\n\t\trequire.Equal(t, insertRes.Job.ID, startedJobID)\n\n\t\tgo func() {\n\t\t\t<-time.After(100 * time.Millisecond)\n\t\t\tclose(jobDoneChan)\n\t\t}()\n\n\t\tt.Logf(\"Shutting down client with timeout, but while jobs are still in progress\")\n\n\t\t// Context should expire while jobs are still in progress:\n\t\tstopCtx, stopCancel := context.WithTimeout(ctx, 50*time.Millisecond)\n\t\tt.Cleanup(stopCancel)\n\n\t\terr = client.Stop(stopCtx)\n\t\trequire.Equal(t, context.DeadlineExceeded, err)\n\n\t\tselect {\n\t\tcase <-jobDoneChan:\n\t\t\trequire.FailNow(t, \"Expected Stop to return before job was done\")\n\t\tdefault:\n\t\t}\n\t})\n\n\tt.Run(\"with continual insertion, no jobs are left running\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tstartedCh := make(chan int64)\n\t\tcallbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tselect {\n\t\t\tcase startedCh <- job.ID:\n\t\t\tdefault:\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\tconfig := newTestConfig(t, callbackFunc)\n\t\tclient := runNewTestClient(ctx, t, config)\n\n\t\tfinish := doParallelContinualInsertion(ctx, t, client)\n\t\tt.Cleanup(finish)\n\n\t\t// Wait for at least one job to start\n\t\triversharedtest.WaitOrTimeout(t, startedCh)\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, listRes.Jobs, \"expected no jobs to be left running\")\n\t})\n\n\tt.Run(\"WithSubscriber\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tcallbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error { return nil }\n\n\t\tclient := runNewTestClient(ctx, t, newTestConfig(t, callbackFunc))\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted)\n\t\tdefer cancel()\n\n\t\tfinish := doParallelContinualInsertion(ctx, t, client)\n\t\tdefer finish()\n\n\t\t// Arbitrarily wait for 100 jobs to come through.\n\t\tfor i := 0; i < 100; i++ {\n\t\t\triversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n}\n\nfunc Test_Client_Stop_AfterContextCancelled(t *testing.T) {\n\tt.Parallel()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\t// doneCh will never close, job will exit due to context cancellation:\n\tdoneCh := make(chan struct{})\n\tstartedCh := make(chan int64)\n\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\tclient := newTestClient(t, dbPool, newTestConfig(t, makeAwaitCallback(startedCh, doneCh)))\n\trequire.NoError(t, client.Start(ctx))\n\tt.Cleanup(func() { require.NoError(t, client.Stop(context.Background())) })\n\n\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\trequire.NoError(t, err)\n\tstartedJobID := riversharedtest.WaitOrTimeout(t, startedCh)\n\trequire.Equal(t, insertRes.Job.ID, startedJobID)\n\n\tcancel()\n\n\trequire.ErrorIs(t, client.Stop(ctx), context.Canceled)\n}\n\nfunc Test_Client_StopAndCancel(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tjobDoneChan    chan struct{}\n\t\tjobStartedChan chan int64\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tjobStartedChan := make(chan int64)\n\t\tjobDoneChan := make(chan struct{})\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tjobStartedChan <- job.ID\n\t\t\tt.Logf(\"Job waiting for context cancellation\")\n\t\t\tdefer t.Logf(\"Job finished\")\n\t\t\t<-ctx.Done()\n\t\t\trequire.ErrorIs(t, context.Cause(ctx), rivercommon.ErrShutdown)\n\t\t\tt.Logf(\"Job context done, closing chan and returning\")\n\t\t\tclose(jobDoneChan)\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := runNewTestClient(ctx, t, config)\n\n\t\treturn client, &testBundle{\n\t\t\tjobDoneChan:    jobDoneChan,\n\t\t\tjobStartedChan: jobStartedChan,\n\t\t}\n\t}\n\n\tt.Run(\"OnItsOwn\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\t_, err := client.Insert(ctx, &callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, bundle.jobStartedChan)\n\n\t\trequire.NoError(t, client.StopAndCancel(ctx))\n\t\triversharedtest.WaitOrTimeout(t, client.Stopped())\n\t})\n\n\tt.Run(\"BeforeStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\trequire.NoError(t, client.StopAndCancel(ctx))\n\t\triversharedtest.WaitOrTimeout(t, client.Stopped()) // this works because Stopped is nil\n\t})\n\n\tt.Run(\"AfterStop\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\t_, err := client.Insert(ctx, &callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, bundle.jobStartedChan)\n\n\t\tgo func() {\n\t\t\trequire.NoError(t, client.Stop(ctx))\n\t\t}()\n\n\t\tselect {\n\t\tcase <-client.Stopped():\n\t\t\tt.Fatal(\"expected client to not be stopped yet\")\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\n\t\trequire.NoError(t, client.StopAndCancel(ctx))\n\t\triversharedtest.WaitOrTimeout(t, client.Stopped())\n\n\t\tselect {\n\t\tcase <-bundle.jobDoneChan:\n\t\tdefault:\n\t\t\tt.Fatal(\"expected job to have exited\")\n\t\t}\n\t})\n}\n\ntype callbackWithCustomTimeoutArgs struct {\n\tTimeoutValue time.Duration `json:\"timeout\"`\n}\n\nfunc (callbackWithCustomTimeoutArgs) Kind() string { return \"callbackWithCustomTimeout\" }\n\ntype callbackWorkerWithCustomTimeout struct {\n\tWorkerDefaults[callbackWithCustomTimeoutArgs]\n\tfn func(context.Context, *Job[callbackWithCustomTimeoutArgs]) error\n}\n\nfunc (w *callbackWorkerWithCustomTimeout) Work(ctx context.Context, job *Job[callbackWithCustomTimeoutArgs]) error {\n\treturn w.fn(ctx, job)\n}\n\nfunc (w *callbackWorkerWithCustomTimeout) Timeout(job *Job[callbackWithCustomTimeoutArgs]) time.Duration {\n\treturn job.Args.TimeoutValue\n}\n\nfunc Test_Client_JobContextInheritsFromProvidedContext(t *testing.T) {\n\tt.Parallel()\n\n\tdeadline := time.Now().Add(2 * time.Minute)\n\n\trequire := require.New(t)\n\tjobCtxCh := make(chan context.Context)\n\tdoneCh := make(chan struct{})\n\tclose(doneCh)\n\n\tcallbackFunc := func(ctx context.Context, job *Job[callbackWithCustomTimeoutArgs]) error {\n\t\t// indicate the job has started, unless context is already done:\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase jobCtxCh <- ctx:\n\t\t}\n\t\treturn nil\n\t}\n\tconfig := newTestConfig(t, nil)\n\tAddWorker(config.Workers, &callbackWorkerWithCustomTimeout{fn: callbackFunc})\n\n\t// Set a deadline and a value on the context for the client so we can verify\n\t// it's propagated through to the job:\n\tctx, cancel := context.WithDeadline(context.Background(), deadline)\n\tt.Cleanup(cancel)\n\n\ttype customContextKey string\n\tctx = context.WithValue(ctx, customContextKey(\"BestGoPostgresQueue\"), \"River\")\n\tclient := runNewTestClient(ctx, t, config)\n\n\tinsertCtx, insertCancel := context.WithTimeout(ctx, time.Second)\n\tt.Cleanup(insertCancel)\n\n\t// enqueue job:\n\t_, err := client.Insert(insertCtx, callbackWithCustomTimeoutArgs{TimeoutValue: 5 * time.Minute}, nil)\n\trequire.NoError(err)\n\n\tvar jobCtx context.Context\n\tselect {\n\tcase jobCtx = <-jobCtxCh:\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"timed out waiting for job to start\")\n\t}\n\n\trequire.Equal(\"River\", jobCtx.Value(customContextKey(\"BestGoPostgresQueue\")), \"job should persist the context value from the client context\")\n\tjobDeadline, ok := jobCtx.Deadline()\n\trequire.True(ok, \"job should have a deadline\")\n\trequire.Equal(deadline, jobDeadline, \"job should have the same deadline as the client context (shorter than the job's timeout)\")\n}\n\nfunc Test_Client_ClientFromContext(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tvar clientResult *Client[pgx.Tx]\n\tjobDoneChan := make(chan struct{})\n\tconfig := newTestConfig(t, func(ctx context.Context, j *Job[callbackArgs]) error {\n\t\tclientResult = ClientFromContext[pgx.Tx](ctx)\n\t\tclose(jobDoneChan)\n\t\treturn nil\n\t})\n\tclient := runNewTestClient(ctx, t, config)\n\n\t_, err := client.Insert(ctx, callbackArgs{}, nil)\n\trequire.NoError(t, err)\n\n\triversharedtest.WaitOrTimeout(t, jobDoneChan)\n\n\trequire.NotNil(t, clientResult)\n\trequire.Equal(t, client, clientResult)\n}\n\nfunc Test_Client_JobDelete(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{dbPool: dbPool}\n\t}\n\n\tt.Run(\"DeletesANonRunningJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)\n\n\t\tjobAfter, err := client.JobDelete(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, jobAfter)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, jobAfter.State)\n\n\t\t_, err = client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t})\n\n\tt.Run(\"DoesNotDeleteARunningJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tdoneCh := make(chan struct{})\n\t\tstartedCh := make(chan int64)\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tclose(startedCh)\n\t\t\t<-doneCh\n\t\t\treturn nil\n\t\t}))\n\n\t\trequire.NoError(t, client.Start(ctx))\n\t\tt.Cleanup(func() { require.NoError(t, client.Stop(ctx)) })\n\t\tt.Cleanup(func() { close(doneCh) }) // must close before stopping client\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\t// Wait for the job to start:\n\t\triversharedtest.WaitOrTimeout(t, startedCh)\n\n\t\tjobAfter, err := client.JobDelete(ctx, insertRes.Job.ID)\n\t\trequire.ErrorIs(t, err, rivertype.ErrJobRunning)\n\t\trequire.Nil(t, jobAfter)\n\n\t\tjobFromGet, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRunning, jobFromGet.State)\n\t})\n\n\tt.Run(\"TxVariantAlsoDeletesANonRunningJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)\n\n\t\tvar jobAfter *rivertype.JobRow\n\n\t\terr = pgx.BeginFunc(ctx, bundle.dbPool, func(tx pgx.Tx) error {\n\t\t\tvar err error\n\t\t\tjobAfter, err = client.JobDeleteTx(ctx, tx, insertRes.Job.ID)\n\t\t\treturn err\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, jobAfter)\n\t\trequire.Equal(t, insertRes.Job.ID, jobAfter.ID)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, jobAfter.State)\n\n\t\tjobFromGet, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.ErrorIs(t, ErrNotFound, err)\n\t\trequire.Nil(t, jobFromGet)\n\t})\n\n\tt.Run(\"ReturnsErrNotFoundIfJobDoesNotExist\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tjobAfter, err := client.JobDelete(ctx, 0)\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, jobAfter)\n\t})\n}\n\nfunc Test_Client_Insert(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{dbPool: dbPool}\n\t}\n\n\tt.Run(\"Succeeds\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\tjobRow := insertRes.Job\n\t\trequire.Equal(t, 0, jobRow.Attempt)\n\t\trequire.Equal(t, rivercommon.MaxAttemptsDefault, jobRow.MaxAttempts)\n\t\trequire.JSONEq(t, \"{}\", string(jobRow.Metadata))\n\t\trequire.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)\n\t\trequire.Equal(t, PriorityDefault, jobRow.Priority)\n\t\trequire.Equal(t, QueueDefault, jobRow.Queue)\n\t\trequire.Equal(t, []string{}, jobRow.Tags)\n\t})\n\n\tt.Run(\"WithInsertOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{\n\t\t\tMaxAttempts: 17,\n\t\t\tMetadata:    []byte(`{\"foo\": \"bar\"}`),\n\t\t\tPriority:    3,\n\t\t\tQueue:       \"custom\",\n\t\t\tTags:        []string{\"custom\"},\n\t\t})\n\t\tjobRow := insertRes.Job\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 0, jobRow.Attempt)\n\t\trequire.Equal(t, 17, jobRow.MaxAttempts)\n\t\trequire.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)\n\t\trequire.JSONEq(t, `{\"foo\": \"bar\"}`, string(jobRow.Metadata))\n\t\trequire.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)\n\t\trequire.Equal(t, 3, jobRow.Priority)\n\t\trequire.Equal(t, \"custom\", jobRow.Queue)\n\t\trequire.Equal(t, []string{\"custom\"}, jobRow.Tags)\n\t})\n\n\tt.Run(\"WithInsertOptsScheduledAtZeroTime\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{\n\t\t\tScheduledAt: time.Time{},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), insertRes.Job.ScheduledAt, 2*time.Second)\n\t})\n\n\tt.Run(\"OnlyTriggersInsertNotificationForAvailableJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx := context.Background()\n\n\t\t_, bundle := setup(t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.FetchCooldown = 5 * time.Second\n\t\tconfig.FetchPollInterval = 5 * time.Second\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\t_, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{Queue: \"a\", ScheduledAt: time.Now().Add(1 * time.Hour)})\n\t\trequire.NoError(t, err)\n\n\t\t// Queue `a` should be \"due\" to be triggered because it wasn't triggered above.\n\t\trequire.True(t, client.insertNotifyLimiter.ShouldTrigger(\"a\"))\n\n\t\t_, err = client.Insert(ctx, noOpArgs{}, &InsertOpts{Queue: \"b\"})\n\t\trequire.NoError(t, err)\n\n\t\t// Queue `b` should *not* be \"due\" to be triggered because it was triggered above.\n\t\trequire.False(t, client.insertNotifyLimiter.ShouldTrigger(\"b\"))\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"WithUniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tjob1, err := client.Insert(ctx, noOpArgs{Name: \"foo\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, job1)\n\n\t\t// Dupe, same args:\n\t\tjob2, err := client.Insert(ctx, noOpArgs{Name: \"foo\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, job1.Job.ID, job2.Job.ID)\n\n\t\t// Not a dupe, different args\n\t\tjob3, err := client.Insert(ctx, noOpArgs{Name: \"bar\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.NotEqual(t, job1.Job.ID, job3.Job.ID)\n\t})\n\n\tt.Run(\"ErrorsOnInvalidQueueName\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\t_, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: \"invalid*queue\"})\n\t\trequire.ErrorContains(t, err, \"queue name is invalid\")\n\t})\n\n\tt.Run(\"ErrorsOnDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, _ = setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t_, err = client.Insert(ctx, &noOpArgs{}, nil)\n\t\trequire.ErrorIs(t, err, errNoDriverDBPool)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\t_, err := client.Insert(ctx, &unregisteredJobArgs{}, nil)\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\t_, err := client.Insert(ctx, &unregisteredJobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc Test_Client_InsertTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\ttx pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttx, err := dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\treturn client, &testBundle{\n\t\t\ttx: tx,\n\t\t}\n\t}\n\n\tt.Run(\"Succeeds\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tinsertRes, err := client.InsertTx(ctx, bundle.tx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\tjobRow := insertRes.Job\n\t\trequire.Equal(t, 0, jobRow.Attempt)\n\t\trequire.Equal(t, rivercommon.MaxAttemptsDefault, jobRow.MaxAttempts)\n\t\trequire.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)\n\t\trequire.Equal(t, PriorityDefault, jobRow.Priority)\n\t\trequire.Equal(t, QueueDefault, jobRow.Queue)\n\t\trequire.Equal(t, []string{}, jobRow.Tags)\n\n\t\t// Job is not visible outside of the transaction.\n\t\t_, err = client.JobGet(ctx, jobRow.ID)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t})\n\n\tt.Run(\"WithInsertOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tinsertRes, err := client.InsertTx(ctx, bundle.tx, &noOpArgs{}, &InsertOpts{\n\t\t\tMaxAttempts: 17,\n\t\t\tPriority:    3,\n\t\t\tQueue:       \"custom\",\n\t\t\tTags:        []string{\"custom\"},\n\t\t})\n\t\tjobRow := insertRes.Job\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 0, jobRow.Attempt)\n\t\trequire.Equal(t, 17, jobRow.MaxAttempts)\n\t\trequire.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)\n\t\trequire.Equal(t, 3, jobRow.Priority)\n\t\trequire.Equal(t, \"custom\", jobRow.Queue)\n\t\trequire.Equal(t, []string{\"custom\"}, jobRow.Tags)\n\t})\n\n\t// A client's allowed to send nil to their driver so they can, for example,\n\t// easily use test transactions in their test suite.\n\tt.Run(\"WithDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t_, err = client.InsertTx(ctx, bundle.tx, &noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t})\n\n\tt.Run(\"WithUniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: \"foo\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, job1)\n\n\t\t// Dupe, same args:\n\t\tjob2, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: \"foo\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, job1.Job.ID, job2.Job.ID)\n\n\t\t// Not a dupe, different args\n\t\tjob3, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: \"bar\"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})\n\t\trequire.NoError(t, err)\n\t\trequire.NotEqual(t, job1.Job.ID, job3.Job.ID)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\t_, err := client.InsertTx(ctx, bundle.tx, &unregisteredJobArgs{}, nil)\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\t_, err := client.InsertTx(ctx, bundle.tx, &unregisteredJobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc Test_Client_InsertManyFast(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{dbPool: dbPool}\n\t}\n\n\tt.Run(\"SucceedsWithMultipleJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"foo\", Priority: 2}},\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 2, count)\n\n\t\tjobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 2, \"Expected to find exactly two jobs of kind: \"+(noOpArgs{}).Kind())\n\t})\n\n\tt.Run(\"TriggersImmediateWork\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx := context.Background()\n\t\t_, bundle := setup(t)\n\n\t\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\t\tt.Cleanup(cancel)\n\n\t\tdoneCh := make(chan struct{})\n\t\tclose(doneCh) // don't need to block any jobs from completing\n\t\tstartedCh := make(chan int64)\n\n\t\tconfig := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))\n\t\tconfig.FetchCooldown = 20 * time.Millisecond\n\t\tconfig.FetchPollInterval = 20 * time.Second // essentially disable polling\n\t\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 2}, \"another_queue\": {MaxWorkers: 1}}\n\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: callbackArgs{}},\n\t\t\t{Args: callbackArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 2, count)\n\n\t\t// Wait for the client to be ready by waiting for a job to be executed:\n\t\triversharedtest.WaitOrTimeoutN(t, startedCh, 2)\n\n\t\t// Now that we've run one job, we shouldn't take longer than the cooldown to\n\t\t// fetch another after insertion. LISTEN/NOTIFY should ensure we find out\n\t\t// about the inserted job much faster than the poll interval.\n\t\t//\n\t\t// Note: we specifically use a different queue to ensure that the notify\n\t\t// limiter is immediately to fire on this queue.\n\t\tcount, err = client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: callbackArgs{}, InsertOpts: &InsertOpts{Queue: \"another_queue\"}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, count)\n\n\t\tselect {\n\t\tcase <-startedCh:\n\t\t// As long as this is meaningfully shorter than the poll interval, we can be\n\t\t// sure the re-fetch came from listen/notify.\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Fatal(\"timed out waiting for another_queue job to start\")\n\t\t}\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"DoesNotTriggerInsertNotificationForNonAvailableJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx := context.Background()\n\n\t\t_, bundle := setup(t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.FetchCooldown = 5 * time.Second\n\t\tconfig.FetchPollInterval = 5 * time.Second\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"a\", ScheduledAt: time.Now().Add(1 * time.Hour)}},\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"b\"}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 2, count)\n\n\t\t// Queue `a` should be \"due\" to be triggered because it wasn't triggered above.\n\t\trequire.True(t, client.insertNotifyLimiter.ShouldTrigger(\"a\"))\n\t\t// Queue `b` should *not* be \"due\" to be triggered because it was triggered above.\n\t\trequire.False(t, client.insertNotifyLimiter.ShouldTrigger(\"b\"))\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"WithInsertOptsScheduledAtZeroTime\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: &noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: time.Time{}}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, count)\n\n\t\tjobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 1, \"Expected to find exactly one job of kind: \"+(noOpArgs{}).Kind())\n\t\tjobRow := jobs[0]\n\t\trequire.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)\n\t})\n\n\tt.Run(\"ErrorsOnInvalidQueueName\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: &noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"invalid*queue\"}},\n\t\t})\n\t\trequire.ErrorContains(t, err, \"queue name is invalid\")\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"ErrorsOnDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, _ = setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.ErrorIs(t, err, errNoDriverDBPool)\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"ErrorsWithZeroJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{})\n\t\trequire.EqualError(t, err, \"no jobs to insert\")\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\t_, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n\n\tt.Run(\"ErrorsOnInsertOptsWithoutRequiredUniqueStates\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tcount, err := client.InsertManyFast(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{\n\t\t\t\tByArgs: true,\n\t\t\t\t// force the v1 unique path with a custom state list that isn't supported in v3:\n\t\t\t\tByState: []rivertype.JobState{rivertype.JobStateAvailable},\n\t\t\t}}},\n\t\t})\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled\")\n\t\trequire.Equal(t, 0, count)\n\t})\n}\n\nfunc Test_Client_InsertManyFastTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\ttx pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttx, err := dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\treturn client, &testBundle{\n\t\t\ttx: tx,\n\t\t}\n\t}\n\n\tt.Run(\"SucceedsWithMultipleJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"foo\", Priority: 2}},\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 2, count)\n\n\t\tjobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 2, \"Expected to find exactly two jobs of kind: \"+(noOpArgs{}).Kind())\n\n\t\trequire.NoError(t, bundle.tx.Commit(ctx))\n\n\t\t// Ensure the jobs are visible outside the transaction:\n\t\tjobs, err = client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 2, \"Expected to find exactly two jobs of kind: \"+(noOpArgs{}).Kind())\n\t})\n\n\tt.Run(\"SetsScheduledAtToNowByDefault\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\t_, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, nil}})\n\t\trequire.NoError(t, err)\n\n\t\tinsertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, insertedJobs, 1)\n\t\trequire.Equal(t, rivertype.JobStateAvailable, insertedJobs[0].State)\n\t\trequire.WithinDuration(t, time.Now(), insertedJobs[0].ScheduledAt, 2*time.Second)\n\t})\n\n\tt.Run(\"SupportsScheduledJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Minute)}}})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, count)\n\n\t\tinsertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, insertedJobs, 1)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, insertedJobs[0].State)\n\t\trequire.WithinDuration(t, time.Now().Add(time.Minute), insertedJobs[0].ScheduledAt, 2*time.Second)\n\t})\n\n\t// A client's allowed to send nil to their driver so they can, for example,\n\t// easily use test transactions in their test suite.\n\tt.Run(\"WithDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, count)\n\t})\n\n\tt.Run(\"ErrorsWithZeroJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{})\n\t\trequire.EqualError(t, err, \"no jobs to insert\")\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t\trequire.Equal(t, 0, count)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\t_, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n\n\tt.Run(\"ErrorsOnInsertOptsWithV1UniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tcount, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{\n\t\t\t\tByArgs: true,\n\t\t\t\t// force the v1 unique path with a custom state list that isn't supported in v3:\n\t\t\t\tByState: []rivertype.JobState{rivertype.JobStateAvailable},\n\t\t\t}}},\n\t\t})\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled\")\n\t\trequire.Equal(t, 0, count)\n\t})\n}\n\nfunc Test_Client_InsertMany(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{dbPool: dbPool}\n\t}\n\n\tt.Run(\"SucceedsWithMultipleJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tnow := time.Now().UTC()\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{Name: \"Foo\"}, InsertOpts: &InsertOpts{Metadata: []byte(`{\"a\": \"b\"}`), Queue: \"foo\", Priority: 2}},\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: now.Add(time.Minute)}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 2)\n\n\t\trequire.False(t, results[0].UniqueSkippedAsDuplicate)\n\t\trequire.Equal(t, 0, results[0].Job.Attempt)\n\t\trequire.Nil(t, results[0].Job.AttemptedAt)\n\t\trequire.WithinDuration(t, now, results[0].Job.CreatedAt, 2*time.Second)\n\t\trequire.Empty(t, results[0].Job.AttemptedBy)\n\t\trequire.Positive(t, results[0].Job.ID)\n\t\trequire.JSONEq(t, `{\"name\": \"Foo\"}`, string(results[0].Job.EncodedArgs))\n\t\trequire.Empty(t, results[0].Job.Errors)\n\t\trequire.Nil(t, results[0].Job.FinalizedAt)\n\t\trequire.Equal(t, \"noOp\", results[0].Job.Kind)\n\t\trequire.Equal(t, 25, results[0].Job.MaxAttempts)\n\t\trequire.JSONEq(t, `{\"a\": \"b\"}`, string(results[0].Job.Metadata))\n\t\trequire.Equal(t, 2, results[0].Job.Priority)\n\t\trequire.Equal(t, \"foo\", results[0].Job.Queue)\n\t\trequire.WithinDuration(t, now, results[0].Job.ScheduledAt, 2*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)\n\t\trequire.Empty(t, results[0].Job.Tags)\n\t\trequire.Empty(t, results[0].Job.UniqueKey)\n\n\t\trequire.False(t, results[1].UniqueSkippedAsDuplicate)\n\t\trequire.Equal(t, 0, results[1].Job.Attempt)\n\t\trequire.Nil(t, results[1].Job.AttemptedAt)\n\t\trequire.WithinDuration(t, now, results[1].Job.CreatedAt, 2*time.Second)\n\t\trequire.Empty(t, results[1].Job.AttemptedBy)\n\t\trequire.Positive(t, results[1].Job.ID)\n\t\trequire.JSONEq(t, `{\"name\": \"\"}`, string(results[1].Job.EncodedArgs))\n\t\trequire.Empty(t, results[1].Job.Errors)\n\t\trequire.Nil(t, results[1].Job.FinalizedAt)\n\t\trequire.Equal(t, \"noOp\", results[1].Job.Kind)\n\t\trequire.Equal(t, 25, results[1].Job.MaxAttempts)\n\t\trequire.JSONEq(t, `{}`, string(results[1].Job.Metadata))\n\t\trequire.Equal(t, 1, results[1].Job.Priority)\n\t\trequire.Equal(t, \"default\", results[1].Job.Queue)\n\t\trequire.WithinDuration(t, now.Add(time.Minute), results[1].Job.ScheduledAt, time.Millisecond)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, results[1].Job.State)\n\t\trequire.Empty(t, results[1].Job.Tags)\n\t\trequire.Empty(t, results[1].Job.UniqueKey)\n\n\t\trequire.NotEqual(t, results[0].Job.ID, results[1].Job.ID)\n\n\t\tjobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 2, \"Expected to find exactly two jobs of kind: \"+(noOpArgs{}).Kind())\n\t})\n\n\tt.Run(\"TriggersImmediateWork\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx := context.Background()\n\t\t_, bundle := setup(t)\n\n\t\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\t\tt.Cleanup(cancel)\n\n\t\tdoneCh := make(chan struct{})\n\t\tclose(doneCh) // don't need to block any jobs from completing\n\t\tstartedCh := make(chan int64)\n\n\t\tconfig := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))\n\t\tconfig.FetchCooldown = 20 * time.Millisecond\n\t\tconfig.FetchPollInterval = 20 * time.Second // essentially disable polling\n\t\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 2}, \"another_queue\": {MaxWorkers: 1}}\n\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: callbackArgs{}},\n\t\t\t{Args: callbackArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 2)\n\n\t\t// Wait for the client to be ready by waiting for a job to be executed:\n\t\triversharedtest.WaitOrTimeoutN(t, startedCh, 2)\n\n\t\t// Now that we've run one job, we shouldn't take longer than the cooldown to\n\t\t// fetch another after insertion. LISTEN/NOTIFY should ensure we find out\n\t\t// about the inserted job much faster than the poll interval.\n\t\t//\n\t\t// Note: we specifically use a different queue to ensure that the notify\n\t\t// limiter is immediately to fire on this queue.\n\t\tresults, err = client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: callbackArgs{}, InsertOpts: &InsertOpts{Queue: \"another_queue\"}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\n\t\tselect {\n\t\tcase <-startedCh:\n\t\t// As long as this is meaningfully shorter than the poll interval, we can be\n\t\t// sure the re-fetch came from listen/notify.\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Fatal(\"timed out waiting for another_queue job to start\")\n\t\t}\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"DoesNotTriggerInsertNotificationForNonAvailableJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx := context.Background()\n\n\t\t_, bundle := setup(t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.FetchCooldown = 5 * time.Second\n\t\tconfig.FetchPollInterval = 5 * time.Second\n\t\tclient := newTestClient(t, bundle.dbPool, config)\n\n\t\tstartClient(ctx, t, client)\n\t\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"a\", ScheduledAt: time.Now().Add(1 * time.Hour)}},\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"b\"}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 2)\n\n\t\t// Queue `a` should be \"due\" to be triggered because it wasn't triggered above.\n\t\trequire.True(t, client.insertNotifyLimiter.ShouldTrigger(\"a\"))\n\t\t// Queue `b` should *not* be \"due\" to be triggered because it was triggered above.\n\t\trequire.False(t, client.insertNotifyLimiter.ShouldTrigger(\"b\"))\n\n\t\trequire.NoError(t, client.Stop(ctx))\n\t})\n\n\tt.Run(\"WithInsertOptsScheduledAtZeroTime\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: &noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: time.Time{}}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\n\t\tjobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 1, \"Expected to find exactly one job of kind: \"+(noOpArgs{}).Kind())\n\t\tjobRow := jobs[0]\n\t\trequire.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)\n\t})\n\n\tt.Run(\"ErrorsOnInvalidQueueName\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: &noOpArgs{}, InsertOpts: &InsertOpts{Queue: \"invalid*queue\"}},\n\t\t})\n\t\trequire.ErrorContains(t, err, \"queue name is invalid\")\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"ErrorsOnDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, _ = setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.ErrorIs(t, err, errNoDriverDBPool)\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"ErrorsWithZeroJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{})\n\t\trequire.EqualError(t, err, \"no jobs to insert\")\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\t})\n\n\tt.Run(\"ErrorsOnInsertOptsWithV1UniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tresults, err := client.InsertMany(ctx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{\n\t\t\t\tByArgs: true,\n\t\t\t\t// force the v1 unique path with a custom state list that isn't supported in v3:\n\t\t\t\tByState: []rivertype.JobState{rivertype.JobStateAvailable},\n\t\t\t}}},\n\t\t})\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled\")\n\t\trequire.Empty(t, results)\n\t})\n}\n\nfunc Test_Client_InsertManyTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\ttx pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttx, err := dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\treturn client, &testBundle{\n\t\t\ttx: tx,\n\t\t}\n\t}\n\n\tt.Run(\"SucceedsWithMultipleJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{Name: \"Foo\"}, InsertOpts: &InsertOpts{Metadata: []byte(`{\"a\": \"b\"}`), Queue: \"foo\", Priority: 2}},\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: now.Add(time.Minute)}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 2)\n\n\t\trequire.False(t, results[0].UniqueSkippedAsDuplicate)\n\t\trequire.Equal(t, 0, results[0].Job.Attempt)\n\t\trequire.Nil(t, results[0].Job.AttemptedAt)\n\t\trequire.WithinDuration(t, now, results[0].Job.CreatedAt, 2*time.Second)\n\t\trequire.Empty(t, results[0].Job.AttemptedBy)\n\t\trequire.Positive(t, results[0].Job.ID)\n\t\trequire.JSONEq(t, `{\"name\": \"Foo\"}`, string(results[0].Job.EncodedArgs))\n\t\trequire.Empty(t, results[0].Job.Errors)\n\t\trequire.Nil(t, results[0].Job.FinalizedAt)\n\t\trequire.Equal(t, \"noOp\", results[0].Job.Kind)\n\t\trequire.Equal(t, 25, results[0].Job.MaxAttempts)\n\t\trequire.JSONEq(t, `{\"a\": \"b\"}`, string(results[0].Job.Metadata))\n\t\trequire.Equal(t, 2, results[0].Job.Priority)\n\t\trequire.Equal(t, \"foo\", results[0].Job.Queue)\n\t\trequire.WithinDuration(t, now, results[0].Job.ScheduledAt, 2*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)\n\t\trequire.Empty(t, results[0].Job.Tags)\n\t\trequire.Empty(t, results[0].Job.UniqueKey)\n\n\t\trequire.False(t, results[1].UniqueSkippedAsDuplicate)\n\t\trequire.Equal(t, 0, results[1].Job.Attempt)\n\t\trequire.Nil(t, results[1].Job.AttemptedAt)\n\t\trequire.WithinDuration(t, now, results[1].Job.CreatedAt, 2*time.Second)\n\t\trequire.Empty(t, results[1].Job.AttemptedBy)\n\t\trequire.Positive(t, results[1].Job.ID)\n\t\trequire.JSONEq(t, `{\"name\": \"\"}`, string(results[1].Job.EncodedArgs))\n\t\trequire.Empty(t, results[1].Job.Errors)\n\t\trequire.Nil(t, results[1].Job.FinalizedAt)\n\t\trequire.Equal(t, \"noOp\", results[1].Job.Kind)\n\t\trequire.Equal(t, 25, results[1].Job.MaxAttempts)\n\t\trequire.JSONEq(t, `{}`, string(results[1].Job.Metadata))\n\t\trequire.Equal(t, 1, results[1].Job.Priority)\n\t\trequire.Equal(t, \"default\", results[1].Job.Queue)\n\t\trequire.WithinDuration(t, now.Add(time.Minute), results[1].Job.ScheduledAt, time.Millisecond)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, results[1].Job.State)\n\t\trequire.Empty(t, results[1].Job.Tags)\n\t\trequire.Empty(t, results[1].Job.UniqueKey)\n\n\t\trequire.NotEqual(t, results[0].Job.ID, results[1].Job.ID)\n\n\t\tjobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 2, \"Expected to find exactly two jobs of kind: \"+(noOpArgs{}).Kind())\n\t})\n\n\tt.Run(\"SetsScheduledAtToNowByDefault\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, nil}})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\n\t\trequire.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)\n\t\trequire.WithinDuration(t, time.Now(), results[0].Job.ScheduledAt, 2*time.Second)\n\n\t\tinsertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, insertedJobs, 1)\n\t\trequire.Equal(t, rivertype.JobStateAvailable, insertedJobs[0].State)\n\t\trequire.WithinDuration(t, time.Now(), insertedJobs[0].ScheduledAt, 2*time.Second)\n\t})\n\n\tt.Run(\"SupportsScheduledJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Minute)}}})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\n\t\trequire.Equal(t, rivertype.JobStateScheduled, results[0].Job.State)\n\t\trequire.WithinDuration(t, time.Now().Add(time.Minute), results[0].Job.ScheduledAt, 2*time.Second)\n\t})\n\n\t// A client's allowed to send nil to their driver so they can, for example,\n\t// easily use test transactions in their test suite.\n\tt.Run(\"WithDriverWithoutPool\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\t})\n\n\tt.Run(\"WithJobInsertMiddleware\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, bundle := setup(t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.Queues = nil\n\n\t\tinsertCalled := false\n\t\tvar innerResults []*rivertype.JobInsertResult\n\n\t\tmiddleware := &overridableJobMiddleware{\n\t\t\tinsertManyFunc: func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {\n\t\t\t\tinsertCalled = true\n\t\t\t\tvar err error\n\t\t\t\tfor _, params := range manyParams {\n\t\t\t\t\tparams.Metadata, err = sjson.SetBytes(params.Metadata, \"middleware\", \"called\")\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t}\n\n\t\t\t\tresults, err := doInner(ctx)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tinnerResults = results\n\t\t\t\treturn results, nil\n\t\t\t},\n\t\t}\n\n\t\tconfig.JobInsertMiddleware = []rivertype.JobInsertMiddleware{middleware}\n\t\tdriver := riverpgxv5.New(nil)\n\t\tclient, err := NewClient(driver, config)\n\t\trequire.NoError(t, err)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{}}})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\n\t\trequire.True(t, insertCalled)\n\t\trequire.Len(t, innerResults, 1)\n\t\trequire.Len(t, results, 1)\n\t\trequire.Equal(t, innerResults[0].Job.ID, results[0].Job.ID)\n\t\trequire.JSONEq(t, `{\"middleware\": \"called\"}`, string(results[0].Job.Metadata))\n\t})\n\n\tt.Run(\"WithUniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: \"foo\"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\t\tjob1 := results[0]\n\n\t\t// Dupe, same args:\n\t\tresults, err = client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: \"foo\"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})\n\t\trequire.NoError(t, err)\n\t\tjob2 := results[0]\n\t\trequire.Equal(t, job1.Job.ID, job2.Job.ID)\n\n\t\t// Not a dupe, different args\n\t\tresults, err = client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: \"bar\"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})\n\t\trequire.NoError(t, err)\n\t\tjob3 := results[0]\n\t\trequire.NotEqual(t, job1.Job.ID, job3.Job.ID)\n\t})\n\n\tt.Run(\"ErrorsWithZeroJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{})\n\t\trequire.EqualError(t, err, \"no jobs to insert\")\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"ErrorsOnUnknownJobKindWithWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\trequire.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)\n\t\trequire.Nil(t, results)\n\t})\n\n\tt.Run(\"AllowsUnknownJobKindWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tclient.config.Workers = nil\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: unregisteredJobArgs{}},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, results, 1)\n\t})\n\n\tt.Run(\"ErrorsOnInsertOptsWithV1UniqueOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tresults, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{\n\t\t\t{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{\n\t\t\t\tByArgs: true,\n\t\t\t\t// force the v1 unique path with a custom state list that isn't supported in v3:\n\t\t\t\tByState: []rivertype.JobState{rivertype.JobStateAvailable},\n\t\t\t}}},\n\t\t})\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled\")\n\t\trequire.Empty(t, results)\n\t})\n}\n\nfunc Test_Client_JobGet(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{}\n\t}\n\n\tt.Run(\"FetchesAnExistingJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\tjob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, insertRes.Job.ID, job.ID)\n\t\trequire.Equal(t, insertRes.Job.State, job.State)\n\t})\n\n\tt.Run(\"ReturnsErrNotFoundIfJobDoesNotExist\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tjob, err := client.JobGet(ctx, 0)\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, job)\n\t})\n}\n\nfunc Test_Client_JobList(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texec riverdriver.Executor\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{\n\t\t\texec: client.driver.GetExecutor(),\n\t\t}\n\t}\n\n\tt.Run(\"FiltersByKind\", func(t *testing.T) { //nolint:dupl\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"test_kind_1\")})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"test_kind_1\")})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"test_kind_2\")})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().Kinds(\"test_kind_1\"))\n\t\trequire.NoError(t, err)\n\t\t// jobs ordered by ScheduledAt ASC by default\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().Kinds(\"test_kind_2\"))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"FiltersByQueue\", func(t *testing.T) { //nolint:dupl\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr(\"queue_1\")})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr(\"queue_1\")})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr(\"queue_2\")})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().Queues(\"queue_1\"))\n\t\trequire.NoError(t, err)\n\t\t// jobs ordered by ScheduledAt ASC by default\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().Queues(\"queue_2\"))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"FiltersByState\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})\n\t\tjob4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStatePending)})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateAvailable))\n\t\trequire.NoError(t, err)\n\t\t// jobs ordered by ScheduledAt ASC by default\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\t// All by default:\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams())\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID, job3.ID, job4.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"DefaultsToOrderingByID\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderDesc))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"OrderByTimeSortsAvailableRetryableAndScheduledJobsByScheduledAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\n\t\tstates := []rivertype.JobState{\n\t\t\trivertype.JobStateAvailable,\n\t\t\trivertype.JobStateRetryable,\n\t\t\trivertype.JobStateScheduled,\n\t\t}\n\t\tfor _, state := range states {\n\t\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), ScheduledAt: &now})\n\t\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\n\t\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(state))\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\t\tlistRes, err = client.JobList(ctx, NewJobListParams().States(state).OrderBy(JobListOrderByTime, SortOrderDesc))\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\t}\n\t})\n\n\tt.Run(\"OrderByTimeSortsCancelledCompletedAndDiscardedJobsByFinalizedAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\n\t\tstates := []rivertype.JobState{\n\t\t\trivertype.JobStateCancelled,\n\t\t\trivertype.JobStateCompleted,\n\t\t\trivertype.JobStateDiscarded,\n\t\t}\n\t\tfor _, state := range states {\n\t\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), FinalizedAt: ptrutil.Ptr(now.Add(-10 * time.Second))})\n\t\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), FinalizedAt: ptrutil.Ptr(now.Add(-15 * time.Second))})\n\n\t\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(state))\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\t\tlistRes, err = client.JobList(ctx, NewJobListParams().States(state).OrderBy(JobListOrderByTime, SortOrderDesc))\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\t}\n\t})\n\n\tt.Run(\"OrderByTimeSortsRunningJobsByAttemptedAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: &now})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateRunning))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning).OrderBy(JobListOrderByTime, SortOrderDesc))\n\t\trequire.NoError(t, err)\n\t\t// Sort order was explicitly reversed:\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"WithNilParamsFiltersToAllStatesByDefault\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: &now})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-2 * time.Second))})\n\n\t\tlistRes, err := client.JobList(ctx, nil)\n\t\trequire.NoError(t, err)\n\t\t// sort order defaults to ID\n\t\trequire.Equal(t, []int64{job1.ID, job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"PaginatesWithAfter_JobListOrderByID\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().After(JobListCursorFromJob(job1)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByID, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job3.ID, listRes.LastCursor.id)\n\n\t\t// No more results\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().After(JobListCursorFromJob(job3)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Nil(t, listRes.LastCursor)\n\n\t\t// Descending\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByID, SortOrderDesc).After(JobListCursorFromJob(job3)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByID, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job1.ID, listRes.LastCursor.id)\n\t})\n\n\tt.Run(\"PaginatesWithAfter_JobListOrderByScheduledAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: &now})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: ptrutil.Ptr(now.Add(1 * time.Second))})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: ptrutil.Ptr(now.Add(2 * time.Second))})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderAsc).After(JobListCursorFromJob(job1)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByScheduledAt, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job3.ID, listRes.LastCursor.id)\n\n\t\t// No more results\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderAsc).After(JobListCursorFromJob(job3)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Nil(t, listRes.LastCursor)\n\n\t\t// Descending\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderDesc).After(JobListCursorFromJob(job3)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByScheduledAt, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job1.ID, listRes.LastCursor.id)\n\t})\n\n\tt.Run(\"PaginatesWithAfter_JobListOrderByTime\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tnow := time.Now().UTC()\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: &now})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second)), AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\t\tjob4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-6 * time.Second)), AttemptedAt: &now})\n\t\tjob5 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), ScheduledAt: ptrutil.Ptr(now.Add(-7 * time.Second)), FinalizedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\t\tjob6 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), ScheduledAt: ptrutil.Ptr(now.Add(-7 * time.Second)), FinalizedAt: &now})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateAvailable).After(JobListCursorFromJob(job1)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job2.ID, listRes.LastCursor.id)\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateRunning).After(JobListCursorFromJob(job3)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job4.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job4.ID, listRes.LastCursor.id)\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateCompleted).After(JobListCursorFromJob(job5)))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job6.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t\trequire.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)\n\t\trequire.Equal(t, job6.ID, listRes.LastCursor.id)\n\t})\n\n\tt.Run(\"MetadataOnly\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{\"foo\": \"bar\"}`)})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{\"baz\": \"value\"}`)})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{\"baz\": \"value\"}`)})\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().Metadata(`{\"foo\": \"bar\"}`))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []int64{job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\n\t\tlistRes, err = client.JobList(ctx, NewJobListParams().Metadata(`{\"baz\": \"value\"}`).OrderBy(JobListOrderByTime, SortOrderDesc))\n\t\trequire.NoError(t, err)\n\t\t// Sort order was explicitly reversed:\n\t\trequire.Equal(t, []int64{job3.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))\n\t})\n\n\tt.Run(\"WithCancelledContext\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tctx, cancel := context.WithCancel(ctx)\n\t\tcancel() // cancel immediately\n\n\t\tlistRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))\n\t\trequire.ErrorIs(t, context.Canceled, err)\n\t\trequire.Nil(t, listRes)\n\t})\n}\n\nfunc Test_Client_JobRetry(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tdbPool *pgxpool.Pool\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{dbPool: dbPool}\n\t}\n\n\tt.Run(\"UpdatesAJobScheduledInTheFutureToBeImmediatelyAvailable\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)\n\n\t\tjob, err := client.JobRetry(ctx, insertRes.Job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, job)\n\n\t\trequire.Equal(t, rivertype.JobStateAvailable, job.State)\n\t\trequire.WithinDuration(t, time.Now().UTC(), job.ScheduledAt, 5*time.Second)\n\t})\n\n\tt.Run(\"TxVariantAlsoUpdatesJobToAvailable\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)\n\n\t\tvar jobAfter *rivertype.JobRow\n\n\t\terr = pgx.BeginFunc(ctx, bundle.dbPool, func(tx pgx.Tx) error {\n\t\t\tvar err error\n\t\t\tjobAfter, err = client.JobRetryTx(ctx, tx, insertRes.Job.ID)\n\t\t\treturn err\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, jobAfter)\n\n\t\trequire.Equal(t, rivertype.JobStateAvailable, jobAfter.State)\n\t\trequire.WithinDuration(t, time.Now().UTC(), jobAfter.ScheduledAt, 5*time.Second)\n\t})\n\n\tt.Run(\"ReturnsErrNotFoundIfJobDoesNotExist\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tjob, err := client.JobRetry(ctx, 0)\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, job)\n\t})\n}\n\nfunc Test_Client_ErrorHandler(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tSubscribeChan <-chan *Event\n\t}\n\n\tsetup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tclient := runNewTestClient(ctx, t, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)\n\t\tt.Cleanup(cancel)\n\n\t\treturn client, &testBundle{SubscribeChan: subscribeChan}\n\t}\n\n\trequireInsert := func(ctx context.Context, client *Client[pgx.Tx]) *rivertype.JobRow {\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\treturn insertRes.Job\n\t}\n\n\tt.Run(\"ErrorHandler\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\thandlerErr := errors.New(\"job error\")\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn handlerErr\n\t\t})\n\n\t\tvar errorHandlerCalled bool\n\t\tconfig.ErrorHandler = &testErrorHandler{\n\t\t\tHandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\t\t\t\trequire.Equal(t, handlerErr, err)\n\t\t\t\terrorHandlerCalled = true\n\t\t\t\treturn &ErrorHandlerResult{}\n\t\t\t},\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\trequireInsert(ctx, client)\n\t\triversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\n\t\trequire.True(t, errorHandlerCalled)\n\t})\n\n\tt.Run(\"ErrorHandler_UnknownJobKind\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tvar errorHandlerCalled bool\n\t\tconfig.ErrorHandler = &testErrorHandler{\n\t\t\tHandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\t\t\t\tvar unknownJobKindErr *UnknownJobKindError\n\t\t\t\trequire.ErrorAs(t, err, &unknownJobKindErr)\n\t\t\t\trequire.Equal(t, UnknownJobKindError{Kind: \"RandomWorkerNameThatIsNeverRegistered\"}, *unknownJobKindErr)\n\t\t\t\terrorHandlerCalled = true\n\t\t\t\treturn &ErrorHandlerResult{}\n\t\t\t},\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\t// Bypass the normal Insert function because that will error on an\n\t\t// unknown job.\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(&client.baseService.Archetype, config, unregisteredJobArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\t_, err = client.driver.GetExecutor().JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\n\t\trequire.True(t, errorHandlerCalled)\n\t})\n\n\tt.Run(\"PanicHandler\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tpanic(\"panic val\")\n\t\t})\n\n\t\tvar panicHandlerCalled bool\n\t\tconfig.ErrorHandler = &testErrorHandler{\n\t\t\tHandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\t\t\t\trequire.Equal(t, \"panic val\", panicVal)\n\t\t\t\tpanicHandlerCalled = true\n\t\t\t\treturn &ErrorHandlerResult{}\n\t\t\t},\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\trequireInsert(ctx, client)\n\t\triversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\n\t\trequire.True(t, panicHandlerCalled)\n\t})\n}\n\nfunc Test_Client_Maintenance(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texec riverdriver.Executor\n\t}\n\n\tsetup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tvar (\n\t\t\tdbPool = riverinternaltest.TestDB(ctx, t)\n\t\t\tclient = newTestClient(t, dbPool, config)\n\t\t)\n\n\t\tclient.testSignals.Init()\n\n\t\treturn client, &testBundle{exec: client.driver.GetExecutor()}\n\t}\n\n\t// Starts the client, then waits for it to be elected leader and for the\n\t// queue maintainer to start.\n\tstartAndWaitForQueueMaintainer := func(ctx context.Context, t *testing.T, client *Client[pgx.Tx]) {\n\t\tt.Helper()\n\n\t\tstartClient(ctx, t, client)\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\t\triversharedtest.WaitOrTimeout(t, client.queueMaintainer.Started())\n\t}\n\n\tt.Run(\"JobCleaner\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.CancelledJobRetentionPeriod = 1 * time.Hour\n\t\tconfig.CompletedJobRetentionPeriod = 1 * time.Hour\n\t\tconfig.DiscardedJobRetentionPeriod = 1 * time.Hour\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tdeleteHorizon := time.Now().Add(-config.CompletedJobRetentionPeriod)\n\n\t\t// Take care to insert jobs before starting the client because otherwise\n\t\t// there's a race condition where the cleaner could run its initial\n\t\t// pass before our insertion is complete.\n\t\tineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})\n\t\tineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})\n\t\tineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled)})\n\n\t\tjobBeyondHorizon1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\t\tjobBeyondHorizon2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\t\tjobBeyondHorizon3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\n\t\t// Will not be deleted.\n\t\tjobWithinHorizon1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\t\tjobWithinHorizon2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\t\tjobWithinHorizon3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tjc := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)\n\t\tjc.TestSignals.DeletedBatch.WaitOrTimeout()\n\n\t\tvar err error\n\t\t_, err = client.JobGet(ctx, ineligibleJob1.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.JobGet(ctx, ineligibleJob2.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.JobGet(ctx, ineligibleJob3.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\n\t\t_, err = client.JobGet(ctx, jobBeyondHorizon1.ID)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\t_, err = client.JobGet(ctx, jobBeyondHorizon2.ID)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\t_, err = client.JobGet(ctx, jobBeyondHorizon3.ID)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\n\t\t_, err = client.JobGet(ctx, jobWithinHorizon1.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.JobGet(ctx, jobWithinHorizon2.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.JobGet(ctx, jobWithinHorizon3.ID)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t})\n\n\tt.Run(\"JobRescuer\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.RescueStuckJobsAfter = 5 * time.Minute\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tnow := time.Now()\n\n\t\t// Take care to insert jobs before starting the client because otherwise\n\t\t// there's a race condition where the rescuer could run its initial\n\t\t// pass before our insertion is complete.\n\t\tineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(time.Minute))})\n\t\tineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(time.Minute))})\n\t\tineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(-time.Minute))})\n\n\t\t// large attempt number ensures these don't immediately start executing again:\n\t\tjobStuckToRetry1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(20), AttemptedAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})\n\t\tjobStuckToRetry2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(20), AttemptedAt: ptrutil.Ptr(now.Add(-30 * time.Minute))})\n\t\tjobStuckToDiscard := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{\n\t\t\tState:       ptrutil.Ptr(rivertype.JobStateRunning),\n\t\t\tAttempt:     ptrutil.Ptr(20),\n\t\t\tAttemptedAt: ptrutil.Ptr(now.Add(-5*time.Minute - time.Second)),\n\t\t\tMaxAttempts: ptrutil.Ptr(1),\n\t\t})\n\n\t\t// Will not be rescued.\n\t\tjobNotYetStuck1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-4 * time.Minute))})\n\t\tjobNotYetStuck2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-1 * time.Minute))})\n\t\tjobNotYetStuck3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(\"noOp\"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-10 * time.Second))})\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.JobRescuer](client.queueMaintainer)\n\t\tsvc.TestSignals.FetchedBatch.WaitOrTimeout()\n\t\tsvc.TestSignals.UpdatedBatch.WaitOrTimeout()\n\n\t\trequireJobHasState := func(jobID int64, state rivertype.JobState) {\n\t\t\tt.Helper()\n\t\t\tjob, err := bundle.exec.JobGetByID(ctx, jobID)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, state, job.State)\n\t\t}\n\n\t\t// unchanged\n\t\trequireJobHasState(ineligibleJob1.ID, ineligibleJob1.State)\n\t\trequireJobHasState(ineligibleJob2.ID, ineligibleJob2.State)\n\t\trequireJobHasState(ineligibleJob3.ID, ineligibleJob3.State)\n\n\t\t// Jobs to retry should be retryable:\n\t\trequireJobHasState(jobStuckToRetry1.ID, rivertype.JobStateRetryable)\n\t\trequireJobHasState(jobStuckToRetry2.ID, rivertype.JobStateRetryable)\n\n\t\t// This one should be discarded because it's already at MaxAttempts:\n\t\trequireJobHasState(jobStuckToDiscard.ID, rivertype.JobStateDiscarded)\n\n\t\t// not eligible for rescue, not stuck long enough yet:\n\t\trequireJobHasState(jobNotYetStuck1.ID, jobNotYetStuck1.State)\n\t\trequireJobHasState(jobNotYetStuck2.ID, jobNotYetStuck2.State)\n\t\trequireJobHasState(jobNotYetStuck3.ID, jobNotYetStuck3.State)\n\t})\n\n\tt.Run(\"JobScheduler\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.Queues = map[string]QueueConfig{\"another_queue\": {MaxWorkers: 1}} // don't work jobs on the default queue we're using in this test\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tnow := time.Now()\n\n\t\t// Take care to insert jobs before starting the client because otherwise\n\t\t// there's a race condition where the scheduler could run its initial\n\t\t// pass before our insertion is complete.\n\t\tineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})\n\t\tineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})\n\t\tineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})\n\n\t\tjobInPast1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})\n\t\tjobInPast2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Minute))})\n\t\tjobInPast3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})\n\n\t\t// Will not be scheduled.\n\t\tjobInFuture1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(now.Add(1 * time.Hour))})\n\t\tjobInFuture2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(1 * time.Minute))})\n\t\tjobInFuture3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(now.Add(10 * time.Second))})\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tscheduler := maintenance.GetService[*maintenance.JobScheduler](client.queueMaintainer)\n\t\tscheduler.TestSignals.ScheduledBatch.WaitOrTimeout()\n\n\t\trequireJobHasState := func(jobID int64, state rivertype.JobState) {\n\t\t\tt.Helper()\n\t\t\tjob, err := client.JobGet(ctx, jobID)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, state, job.State)\n\t\t}\n\n\t\t// unchanged\n\t\trequireJobHasState(ineligibleJob1.ID, ineligibleJob1.State)\n\t\trequireJobHasState(ineligibleJob2.ID, ineligibleJob2.State)\n\t\trequireJobHasState(ineligibleJob3.ID, ineligibleJob3.State)\n\n\t\t// Jobs with past timestamps should be now be made available:\n\t\trequireJobHasState(jobInPast1.ID, rivertype.JobStateAvailable)\n\t\trequireJobHasState(jobInPast2.ID, rivertype.JobStateAvailable)\n\t\trequireJobHasState(jobInPast3.ID, rivertype.JobStateAvailable)\n\n\t\t// not scheduled, still in future\n\t\trequireJobHasState(jobInFuture1.ID, jobInFuture1.State)\n\t\trequireJobHasState(jobInFuture2.ID, jobInFuture2.State)\n\t\trequireJobHasState(jobInFuture3.ID, jobInFuture3.State)\n\t})\n\n\tt.Run(\"PeriodicJobEnqueuerWithInsertOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\t\tconfig.PeriodicJobs = []*PeriodicJob{\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn periodicJobArgs{}, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.InsertedJobs.WaitOrTimeout()\n\n\t\tjobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 1, \"Expected to find exactly one job of kind: \"+(periodicJobArgs{}).Kind())\n\t})\n\n\tt.Run(\"PeriodicJobEnqueuerNoInsertOpts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\t\tconfig.PeriodicJobs = []*PeriodicJob{\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn periodicJobArgs{}, nil\n\t\t\t}, nil),\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.EnteredLoop.WaitOrTimeout()\n\n\t\t// No jobs yet because the RunOnStart option was not specified.\n\t\tjobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, jobs)\n\t})\n\n\tt.Run(\"PeriodicJobConstructorReturningNil\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\t\tconfig.PeriodicJobs = []*PeriodicJob{\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\t// Returning nil from the constructor function should not insert a new\n\t\t\t\t// job and should be handled cleanly\n\t\t\t\treturn nil, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.SkippedJob.WaitOrTimeout()\n\n\t\tjobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, jobs, \"Expected to find zero jobs of kind: \"+(periodicJobArgs{}).Kind())\n\t})\n\n\tt.Run(\"PeriodicJobEnqueuerAddDynamically\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tclient := newTestClient(t, dbPool, config)\n\t\tclient.testSignals.Init()\n\t\tstartClient(ctx, t, client)\n\n\t\texec := client.driver.GetExecutor()\n\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\n\t\tclient.PeriodicJobs().Add(\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn periodicJobArgs{}, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t)\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.EnteredLoop.WaitOrTimeout()\n\t\tsvc.TestSignals.InsertedJobs.WaitOrTimeout()\n\n\t\t// We get a queued job because RunOnStart was specified.\n\t\tjobs, err := exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 1)\n\t})\n\n\tt.Run(\"PeriodicJobEnqueuerRemoveDynamically\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\n\t\tclient := newTestClient(t, riverinternaltest.TestDB(ctx, t), config)\n\t\tclient.testSignals.Init()\n\t\texec := client.driver.GetExecutor()\n\n\t\thandle := client.PeriodicJobs().Add(\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn periodicJobArgs{}, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.EnteredLoop.WaitOrTimeout()\n\t\tsvc.TestSignals.InsertedJobs.WaitOrTimeout()\n\n\t\tclient.PeriodicJobs().Remove(handle)\n\n\t\ttype OtherPeriodicArgs struct {\n\t\t\tJobArgsReflectKind[OtherPeriodicArgs]\n\t\t}\n\n\t\tclient.PeriodicJobs().Add(\n\t\t\tNewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn OtherPeriodicArgs{}, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t)\n\n\t\tsvc.TestSignals.InsertedJobs.WaitOrTimeout()\n\n\t\t// One of each because the first periodic job was inserted on the first\n\t\t// go around due to RunOnStart, but then subsequently removed. The next\n\t\t// periodic job was inserted also due to RunOnStart, but only after the\n\t\t// first was removed.\n\t\t{\n\t\t\tjobs, err := exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, jobs, 1)\n\t\t}\n\t\t{\n\t\t\tjobs, err := exec.JobGetByKindMany(ctx, []string{(OtherPeriodicArgs{}).Kind()})\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, jobs, 1)\n\t\t}\n\t})\n\n\tt.Run(\"PeriodicJobEnqueuerUsesMiddleware\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tworker := &periodicJobWorker{}\n\t\tAddWorker(config.Workers, worker)\n\t\tconfig.PeriodicJobs = []*PeriodicJob{\n\t\t\tNewPeriodicJob(cron.Every(time.Minute), func() (JobArgs, *InsertOpts) {\n\t\t\t\treturn periodicJobArgs{}, nil\n\t\t\t}, &PeriodicJobOpts{RunOnStart: true}),\n\t\t}\n\t\tconfig.JobInsertMiddleware = []rivertype.JobInsertMiddleware{&overridableJobMiddleware{\n\t\t\tinsertManyFunc: func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {\n\t\t\t\tfor _, job := range manyParams {\n\t\t\t\t\tjob.EncodedArgs = []byte(`{\"from\": \"middleware\"}`)\n\t\t\t\t}\n\t\t\t\treturn doInner(ctx)\n\t\t\t},\n\t\t}}\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\t\tsvc.TestSignals.InsertedJobs.WaitOrTimeout()\n\n\t\tjobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, jobs, 1, \"Expected to find exactly one job of kind: \"+(periodicJobArgs{}).Kind())\n\t})\n\n\tt.Run(\"QueueCleaner\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\t\tclient.testSignals.Init()\n\t\texec := client.driver.GetExecutor()\n\n\t\tdeleteHorizon := time.Now().Add(-maintenance.QueueRetentionPeriodDefault)\n\n\t\t// Take care to insert queues before starting the client because otherwise\n\t\t// there's a race condition where the cleaner could run its initial\n\t\t// pass before our insertion is complete.\n\t\tqueueBeyondHorizon1 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\t\tqueueBeyondHorizon2 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\t\tqueueBeyondHorizon3 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})\n\n\t\t// Will not be deleted.\n\t\tqueueWithinHorizon1 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\t\tqueueWithinHorizon2 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\t\tqueueWithinHorizon3 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})\n\n\t\tstartClient(ctx, t, client)\n\n\t\tclient.testSignals.electedLeader.WaitOrTimeout()\n\t\tqc := maintenance.GetService[*maintenance.QueueCleaner](client.queueMaintainer)\n\t\tqc.TestSignals.DeletedBatch.WaitOrTimeout()\n\n\t\tvar err error\n\t\t_, err = client.QueueGet(ctx, queueBeyondHorizon1.Name)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\t_, err = client.QueueGet(ctx, queueBeyondHorizon2.Name)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\t_, err = client.QueueGet(ctx, queueBeyondHorizon3.Name)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\n\t\t_, err = client.QueueGet(ctx, queueWithinHorizon1.Name)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.QueueGet(ctx, queueWithinHorizon2.Name)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t\t_, err = client.QueueGet(ctx, queueWithinHorizon3.Name)\n\t\trequire.NotErrorIs(t, err, ErrNotFound) // still there\n\t})\n\n\tt.Run(\"Reindexer\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\tt.Skip(\"Reindexer is disabled for further development\")\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.ReindexerSchedule = cron.Every(time.Second)\n\n\t\tclient, _ := setup(t, config)\n\n\t\tstartAndWaitForQueueMaintainer(ctx, t, client)\n\n\t\tsvc := maintenance.GetService[*maintenance.Reindexer](client.queueMaintainer)\n\t\t// There are two indexes to reindex by default:\n\t\tsvc.TestSignals.Reindexed.WaitOrTimeout()\n\t\tsvc.TestSignals.Reindexed.WaitOrTimeout()\n\t})\n}\n\nfunc Test_Client_QueueGet(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{}\n\t}\n\n\tt.Run(\"FetchesAnExistingQueue\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tqueue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)\n\n\t\tqueueRes, err := client.QueueGet(ctx, queue.Name)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), queueRes.CreatedAt, 2*time.Second)\n\t\trequire.WithinDuration(t, queue.CreatedAt, queueRes.CreatedAt, time.Millisecond)\n\t\trequire.Equal(t, []byte(\"{}\"), queueRes.Metadata)\n\t\trequire.Equal(t, queue.Name, queueRes.Name)\n\t\trequire.Nil(t, queueRes.PausedAt)\n\t})\n\n\tt.Run(\"ReturnsErrNotFoundIfQueueDoesNotExist\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tqueueRes, err := client.QueueGet(ctx, \"a_queue_that_does_not_exist\")\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, queueRes)\n\t})\n}\n\nfunc Test_Client_QueueGetTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texecutorTx riverdriver.ExecutorTx\n\t\ttx         pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttx, err := dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\treturn client, &testBundle{executorTx: client.driver.UnwrapExecutor(tx), tx: tx}\n\t}\n\n\tt.Run(\"FetchesAnExistingQueue\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tqueue := testfactory.Queue(ctx, t, bundle.executorTx, nil)\n\n\t\tqueueRes, err := client.QueueGetTx(ctx, bundle.tx, queue.Name)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, queue.Name, queueRes.Name)\n\n\t\t// Not visible outside of transaction.\n\t\t_, err = client.QueueGet(ctx, queue.Name)\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t})\n\n\tt.Run(\"ReturnsErrNotFoundIfQueueDoesNotExist\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tqueueRes, err := client.QueueGet(ctx, \"a_queue_that_does_not_exist\")\n\t\trequire.Error(t, err)\n\t\trequire.ErrorIs(t, err, ErrNotFound)\n\t\trequire.Nil(t, queueRes)\n\t})\n}\n\nfunc Test_Client_QueueList(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\treturn client, &testBundle{}\n\t}\n\n\tt.Run(\"ListsAndPaginatesQueues\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\trequireQueuesEqual := func(t *testing.T, target, actual *rivertype.Queue) {\n\t\t\tt.Helper()\n\t\t\trequire.WithinDuration(t, target.CreatedAt, actual.CreatedAt, time.Millisecond)\n\t\t\trequire.Equal(t, target.Metadata, actual.Metadata)\n\t\t\trequire.Equal(t, target.Name, actual.Name)\n\t\t\tif target.PausedAt == nil {\n\t\t\t\trequire.Nil(t, actual.PausedAt)\n\t\t\t} else {\n\t\t\t\trequire.NotNil(t, actual.PausedAt)\n\t\t\t\trequire.WithinDuration(t, *target.PausedAt, *actual.PausedAt, time.Millisecond)\n\t\t\t}\n\t\t}\n\n\t\tlistRes, err := client.QueueList(ctx, NewQueueListParams().First(2))\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, listRes.Queues)\n\n\t\t// Make queue1, pause it, refetch:\n\t\tqueue1 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), &testfactory.QueueOpts{Metadata: []byte(`{\"foo\": \"bar\"}`)})\n\t\trequire.NoError(t, client.QueuePause(ctx, queue1.Name, nil))\n\t\tqueue1, err = client.QueueGet(ctx, queue1.Name)\n\t\trequire.NoError(t, err)\n\n\t\tqueue2 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)\n\t\tqueue3 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)\n\n\t\tlistRes, err = client.QueueList(ctx, NewQueueListParams().First(2))\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, listRes.Queues, 2)\n\t\trequireQueuesEqual(t, queue1, listRes.Queues[0])\n\t\trequireQueuesEqual(t, queue2, listRes.Queues[1])\n\n\t\tlistRes, err = client.QueueList(ctx, NewQueueListParams().First(3))\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, listRes.Queues, 3)\n\t\trequireQueuesEqual(t, queue3, listRes.Queues[2])\n\n\t\tlistRes, err = client.QueueList(ctx, NewQueueListParams().First(10))\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, listRes.Queues, 3)\n\t})\n}\n\nfunc Test_Client_QueueListTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texecutorTx riverdriver.ExecutorTx\n\t\ttx         pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tconfig := newTestConfig(t, nil)\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttx, err := dbPool.Begin(ctx)\n\t\trequire.NoError(t, err)\n\t\tt.Cleanup(func() { tx.Rollback(ctx) })\n\n\t\treturn client, &testBundle{executorTx: client.driver.UnwrapExecutor(tx), tx: tx}\n\t}\n\n\tt.Run(\"ListsQueues\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tlistRes, err := client.QueueListTx(ctx, bundle.tx, NewQueueListParams())\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, listRes.Queues)\n\n\t\tqueue := testfactory.Queue(ctx, t, bundle.executorTx, nil)\n\n\t\tlistRes, err = client.QueueListTx(ctx, bundle.tx, NewQueueListParams())\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, listRes.Queues, 1)\n\t\trequire.Equal(t, queue.Name, listRes.Queues[0].Name)\n\n\t\t// Not visible outside of transaction.\n\t\tlistRes, err = client.QueueList(ctx, NewQueueListParams())\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, listRes.Queues)\n\t})\n}\n\nfunc Test_Client_RetryPolicy(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\trequireInsert := func(ctx context.Context, client *Client[pgx.Tx]) *rivertype.JobRow {\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\treturn insertRes.Job\n\t}\n\n\tt.Run(\"RetryUntilDiscarded\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn errors.New(\"job error\")\n\t\t})\n\n\t\t// The default policy would work too, but this takes some variability\n\t\t// out of it to make comparisons easier.\n\t\tconfig.RetryPolicy = &retryPolicyNoJitter{}\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)\n\t\tt.Cleanup(cancel)\n\n\t\toriginalJobs := make([]*rivertype.JobRow, rivercommon.MaxAttemptsDefault)\n\t\tfor i := 0; i < len(originalJobs); i++ {\n\t\t\tjob := requireInsert(ctx, client)\n\t\t\t// regression protection to ensure we're testing the right number of jobs:\n\t\t\trequire.Equal(t, rivercommon.MaxAttemptsDefault, job.MaxAttempts)\n\n\t\t\tupdatedJob, err := client.driver.GetExecutor().JobUpdate(ctx, &riverdriver.JobUpdateParams{\n\t\t\t\tID:                  job.ID,\n\t\t\t\tAttemptedAtDoUpdate: true,\n\t\t\t\tAttemptedAt:         ptrutil.Ptr(time.Now().UTC()),\n\t\t\t\tAttemptDoUpdate:     true,\n\t\t\t\tAttempt:             i, // starts at i, but will be i + 1 by the time it's being worked\n\n\t\t\t\t// Need to find a cleaner way around this, but state is required\n\t\t\t\t// because sqlc can't encode an empty string to the\n\t\t\t\t// corresponding enum. This value is not actually used because\n\t\t\t\t// StateDoUpdate was not supplied.\n\t\t\t\tState: rivertype.JobStateAvailable,\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\toriginalJobs[i] = updatedJob\n\t\t}\n\n\t\tstartClient(ctx, t, client)\n\n\t\t// Wait for the expected number of jobs to be finished.\n\t\tfor i := 0; i < len(originalJobs); i++ {\n\t\t\tt.Logf(\"Waiting on job %d\", i)\n\t\t\t_ = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\tfinishedJobs, err := client.driver.GetExecutor().JobGetByIDMany(ctx,\n\t\t\tsliceutil.Map(originalJobs, func(m *rivertype.JobRow) int64 { return m.ID }))\n\t\trequire.NoError(t, err)\n\n\t\t// Jobs aren't guaranteed to come back out of the queue in the same\n\t\t// order that we inserted them, so make sure to compare using a lookup\n\t\t// map.\n\t\tfinishedJobsByID := sliceutil.KeyBy(finishedJobs,\n\t\t\tfunc(m *rivertype.JobRow) (int64, *rivertype.JobRow) { return m.ID, m })\n\n\t\tfor i, originalJob := range originalJobs {\n\t\t\t// This loop will check all jobs that were to be rescheduled, but\n\t\t\t// not the final job which is discarded.\n\t\t\tif i >= len(originalJobs)-1 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfinishedJob := finishedJobsByID[originalJob.ID]\n\n\t\t\t// We need to advance the original job's attempt number to represent\n\t\t\t// how it would've looked after being run through the queue.\n\t\t\toriginalJob.Attempt += 1\n\n\t\t\texpectedNextScheduledAt := client.config.RetryPolicy.NextRetry(originalJob)\n\n\t\t\tt.Logf(\"Attempt number %d scheduled %v from original `attempted_at`\",\n\t\t\t\toriginalJob.Attempt, finishedJob.ScheduledAt.Sub(*originalJob.AttemptedAt))\n\t\t\tt.Logf(\"    Original attempt at:   %v\", originalJob.AttemptedAt)\n\t\t\tt.Logf(\"    New scheduled at:      %v\", finishedJob.ScheduledAt)\n\t\t\tt.Logf(\"    Expected scheduled at: %v\", expectedNextScheduledAt)\n\n\t\t\t// TODO(brandur): This tolerance could be reduced if we could inject\n\t\t\t// time.Now into adapter which may happen with baseservice\n\t\t\trequire.WithinDuration(t, expectedNextScheduledAt, finishedJob.ScheduledAt, 2*time.Second)\n\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, finishedJob.State)\n\t\t}\n\n\t\t// One last discarded job.\n\t\t{\n\t\t\toriginalJob := originalJobs[len(originalJobs)-1]\n\t\t\tfinishedJob := finishedJobsByID[originalJob.ID]\n\n\t\t\toriginalJob.Attempt += 1\n\n\t\t\tt.Logf(\"Attempt number %d discarded\", originalJob.Attempt)\n\n\t\t\t// TODO(brandur): See note on tolerance above.\n\t\t\trequire.WithinDuration(t, time.Now(), *finishedJob.FinalizedAt, 2*time.Second)\n\t\t\trequire.Equal(t, rivertype.JobStateDiscarded, finishedJob.State)\n\t\t}\n\t})\n}\n\nfunc Test_Client_Subscribe(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tkeyEventsByName := func(events []*Event) map[string]*Event {\n\t\treturn sliceutil.KeyBy(events, func(event *Event) (string, *Event) {\n\t\t\tvar args callbackArgs\n\t\t\trequire.NoError(t, json.Unmarshal(event.Job.EncodedArgs, &args))\n\t\t\treturn args.Name, event\n\t\t})\n\t}\n\n\trequireInsert := func(ctx context.Context, client *Client[pgx.Tx], jobName string) *rivertype.JobRow {\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{Name: jobName}, nil)\n\t\trequire.NoError(t, err)\n\t\treturn insertRes.Job\n\t}\n\n\tt.Run(\"Success\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\t// Fail/succeed jobs based on their name so we can get a mix of both to\n\t\t// verify.\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tif strings.HasPrefix(job.Args.Name, \"failed\") {\n\t\t\t\treturn errors.New(\"job error\")\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)\n\t\tt.Cleanup(cancel)\n\n\t\tjobCompleted1 := requireInsert(ctx, client, \"completed1\")\n\t\tjobCompleted2 := requireInsert(ctx, client, \"completed2\")\n\t\tjobFailed1 := requireInsert(ctx, client, \"failed1\")\n\t\tjobFailed2 := requireInsert(ctx, client, \"failed2\")\n\n\t\texpectedJobs := []*rivertype.JobRow{\n\t\t\tjobCompleted1,\n\t\t\tjobCompleted2,\n\t\t\tjobFailed1,\n\t\t\tjobFailed2,\n\t\t}\n\n\t\tstartClient(ctx, t, client)\n\n\t\tevents := make([]*Event, len(expectedJobs))\n\n\t\tfor i := 0; i < len(expectedJobs); i++ {\n\t\t\tevents[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\teventsByName := keyEventsByName(events)\n\n\t\t{\n\t\t\teventCompleted1 := eventsByName[\"completed1\"]\n\t\t\trequire.Equal(t, EventKindJobCompleted, eventCompleted1.Kind)\n\t\t\trequire.Equal(t, jobCompleted1.ID, eventCompleted1.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateCompleted, eventCompleted1.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventCompleted2 := eventsByName[\"completed2\"]\n\t\t\trequire.Equal(t, EventKindJobCompleted, eventCompleted2.Kind)\n\t\t\trequire.Equal(t, jobCompleted2.ID, eventCompleted2.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateCompleted, eventCompleted2.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventFailed1 := eventsByName[\"failed1\"]\n\t\t\trequire.Equal(t, EventKindJobFailed, eventFailed1.Kind)\n\t\t\trequire.Equal(t, jobFailed1.ID, eventFailed1.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, eventFailed1.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventFailed2 := eventsByName[\"failed2\"]\n\t\t\trequire.Equal(t, EventKindJobFailed, eventFailed2.Kind)\n\t\t\trequire.Equal(t, jobFailed2.ID, eventFailed2.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, eventFailed2.Job.State)\n\t\t}\n\t})\n\n\tt.Run(\"CompletedOnly\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tif strings.HasPrefix(job.Args.Name, \"failed\") {\n\t\t\t\treturn errors.New(\"job error\")\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted)\n\t\tt.Cleanup(cancel)\n\n\t\tjobCompleted := requireInsert(ctx, client, \"completed1\")\n\t\trequireInsert(ctx, client, \"failed1\")\n\n\t\texpectedJobs := []*rivertype.JobRow{\n\t\t\tjobCompleted,\n\t\t}\n\n\t\tstartClient(ctx, t, client)\n\n\t\tevents := make([]*Event, len(expectedJobs))\n\n\t\tfor i := 0; i < len(expectedJobs); i++ {\n\t\t\tevents[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\teventsByName := keyEventsByName(events)\n\n\t\teventCompleted := eventsByName[\"completed1\"]\n\t\trequire.Equal(t, EventKindJobCompleted, eventCompleted.Kind)\n\t\trequire.Equal(t, jobCompleted.ID, eventCompleted.Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, eventCompleted.Job.State)\n\n\t\t_, ok := eventsByName[\"failed1\"] // filtered out\n\t\trequire.False(t, ok)\n\t})\n\n\tt.Run(\"FailedOnly\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tif strings.HasPrefix(job.Args.Name, \"failed\") {\n\t\t\t\treturn errors.New(\"job error\")\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobFailed)\n\t\tt.Cleanup(cancel)\n\n\t\trequireInsert(ctx, client, \"completed1\")\n\t\tjobFailed := requireInsert(ctx, client, \"failed1\")\n\n\t\texpectedJobs := []*rivertype.JobRow{\n\t\t\tjobFailed,\n\t\t}\n\n\t\tstartClient(ctx, t, client)\n\n\t\tevents := make([]*Event, len(expectedJobs))\n\n\t\tfor i := 0; i < len(expectedJobs); i++ {\n\t\t\tevents[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\teventsByName := keyEventsByName(events)\n\n\t\t_, ok := eventsByName[\"completed1\"] // filtered out\n\t\trequire.False(t, ok)\n\n\t\teventFailed := eventsByName[\"failed1\"]\n\t\trequire.Equal(t, EventKindJobFailed, eventFailed.Kind)\n\t\trequire.Equal(t, jobFailed.ID, eventFailed.Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, eventFailed.Job.State)\n\t})\n\n\tt.Run(\"PanicOnUnknownKind\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\trequire.PanicsWithError(t, \"unknown event kind: does_not_exist\", func() {\n\t\t\t_, _ = client.Subscribe(EventKind(\"does_not_exist\"))\n\t\t})\n\t})\n\n\tt.Run(\"SubscriptionCancellation\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted)\n\t\tcancel()\n\n\t\t// Drops through immediately because the channel is closed.\n\t\triversharedtest.WaitOrTimeout(t, subscribeChan)\n\n\t\trequire.Empty(t, client.subscriptionManager.subscriptions)\n\t})\n\n\t// Just make sure this doesn't fail on a nil pointer exception.\n\tt.Run(\"SubscribeOnClientWithoutWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tclient := newTestClient(t, dbPool, &Config{})\n\n\t\trequire.PanicsWithValue(t, \"created a subscription on a client that will never work jobs (Workers not configured)\", func() {\n\t\t\t_, _ = client.Subscribe(EventKindJobCompleted)\n\t\t})\n\t})\n}\n\n// SubscribeConfig uses all the same code as Subscribe, so these are just a\n// minimal set of new tests to make sure that the function also works when used\n// independently.\nfunc Test_Client_SubscribeConfig(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tkeyEventsByName := func(events []*Event) map[string]*Event {\n\t\treturn sliceutil.KeyBy(events, func(event *Event) (string, *Event) {\n\t\t\tvar args callbackArgs\n\t\t\trequire.NoError(t, json.Unmarshal(event.Job.EncodedArgs, &args))\n\t\t\treturn args.Name, event\n\t\t})\n\t}\n\n\trequireInsert := func(ctx context.Context, client *Client[pgx.Tx], jobName string) *rivertype.JobRow {\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{Name: jobName}, nil)\n\t\trequire.NoError(t, err)\n\t\treturn insertRes.Job\n\t}\n\n\tt.Run(\"Success\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\t// Fail/succeed jobs based on their name so we can get a mix of both to\n\t\t// verify.\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\tif strings.HasPrefix(job.Args.Name, \"failed\") {\n\t\t\t\treturn errors.New(\"job error\")\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\tsubscribeChan, cancel := client.SubscribeConfig(&SubscribeConfig{\n\t\t\tKinds: []EventKind{EventKindJobCompleted, EventKindJobFailed},\n\t\t})\n\t\tt.Cleanup(cancel)\n\n\t\tjobCompleted1 := requireInsert(ctx, client, \"completed1\")\n\t\tjobCompleted2 := requireInsert(ctx, client, \"completed2\")\n\t\tjobFailed1 := requireInsert(ctx, client, \"failed1\")\n\t\tjobFailed2 := requireInsert(ctx, client, \"failed2\")\n\n\t\texpectedJobs := []*rivertype.JobRow{\n\t\t\tjobCompleted1,\n\t\t\tjobCompleted2,\n\t\t\tjobFailed1,\n\t\t\tjobFailed2,\n\t\t}\n\n\t\tstartClient(ctx, t, client)\n\n\t\tevents := make([]*Event, len(expectedJobs))\n\n\t\tfor i := 0; i < len(expectedJobs); i++ {\n\t\t\tevents[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)\n\t\t}\n\n\t\teventsByName := keyEventsByName(events)\n\n\t\t{\n\t\t\teventCompleted1 := eventsByName[\"completed1\"]\n\t\t\trequire.Equal(t, EventKindJobCompleted, eventCompleted1.Kind)\n\t\t\trequire.Equal(t, jobCompleted1.ID, eventCompleted1.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateCompleted, eventCompleted1.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventCompleted2 := eventsByName[\"completed2\"]\n\t\t\trequire.Equal(t, EventKindJobCompleted, eventCompleted2.Kind)\n\t\t\trequire.Equal(t, jobCompleted2.ID, eventCompleted2.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateCompleted, eventCompleted2.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventFailed1 := eventsByName[\"failed1\"]\n\t\t\trequire.Equal(t, EventKindJobFailed, eventFailed1.Kind)\n\t\t\trequire.Equal(t, jobFailed1.ID, eventFailed1.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, eventFailed1.Job.State)\n\t\t}\n\n\t\t{\n\t\t\teventFailed2 := eventsByName[\"failed2\"]\n\t\t\trequire.Equal(t, EventKindJobFailed, eventFailed2.Kind)\n\t\t\trequire.Equal(t, jobFailed2.ID, eventFailed2.Job.ID)\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, eventFailed2.Job.State)\n\t\t}\n\t})\n\n\tt.Run(\"EventsDropWithNoListeners\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\treturn nil\n\t\t}))\n\n\t\t// A first channel that we'll use to make sure all the expected jobs are\n\t\t// finished.\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCompleted)\n\t\tt.Cleanup(cancel)\n\n\t\t// Artificially lowered subscribe channel size so we don't have to try\n\t\t// and process thousands of jobs.\n\t\tconst (\n\t\t\tsubscribeChanSize = 100\n\t\t\tnumJobsToInsert   = subscribeChanSize + 1\n\t\t)\n\n\t\t// Another channel with no listeners. Despite no listeners, it shouldn't\n\t\t// block or gum up the client's progress in any way.\n\t\tsubscribeChan2, cancel := client.SubscribeConfig(&SubscribeConfig{\n\t\t\tChanSize: subscribeChanSize,\n\t\t\tKinds:    []EventKind{EventKindJobCompleted},\n\t\t})\n\t\tt.Cleanup(cancel)\n\n\t\tvar (\n\t\t\tinsertParams = make([]*riverdriver.JobInsertFastParams, numJobsToInsert)\n\t\t\tkind         = (&JobArgs{}).Kind()\n\t\t)\n\t\tfor i := 0; i < numJobsToInsert; i++ {\n\t\t\tinsertParams[i] = &riverdriver.JobInsertFastParams{\n\t\t\t\tArgs:        &JobArgs{},\n\t\t\t\tEncodedArgs: []byte(`{}`),\n\t\t\t\tKind:        kind,\n\t\t\t\tMaxAttempts: rivercommon.MaxAttemptsDefault,\n\t\t\t\tPriority:    rivercommon.PriorityDefault,\n\t\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\t\tState:       rivertype.JobStateAvailable,\n\t\t\t}\n\t\t}\n\n\t\t_, err := client.driver.GetExecutor().JobInsertFastMany(ctx, insertParams)\n\t\trequire.NoError(t, err)\n\n\t\t// Need to start waiting on events before running the client or the\n\t\t// channel could overflow before we start listening.\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\t_ = riversharedtest.WaitOrTimeoutN(t, subscribeChan, numJobsToInsert)\n\t\t}()\n\n\t\tstartClient(ctx, t, client)\n\n\t\twg.Wait()\n\n\t\t// Filled to maximum.\n\t\trequire.Len(t, subscribeChan2, subscribeChanSize)\n\t})\n\n\tt.Run(\"PanicOnChanSizeLessThanZero\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\trequire.PanicsWithValue(t, \"SubscribeConfig.ChanSize must be greater or equal to 1\", func() {\n\t\t\t_, _ = client.SubscribeConfig(&SubscribeConfig{\n\t\t\t\tChanSize: -1,\n\t\t\t})\n\t\t})\n\t})\n\n\tt.Run(\"PanicOnUnknownKind\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\trequire.PanicsWithError(t, \"unknown event kind: does_not_exist\", func() {\n\t\t\t_, _ = client.SubscribeConfig(&SubscribeConfig{\n\t\t\t\tKinds: []EventKind{EventKind(\"does_not_exist\")},\n\t\t\t})\n\t\t})\n\t})\n}\n\nfunc Test_Client_InsertTriggersImmediateWork(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\trequire := require.New(t)\n\n\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\tt.Cleanup(cancel)\n\n\tdoneCh := make(chan struct{})\n\tclose(doneCh) // don't need to block any jobs from completing\n\tstartedCh := make(chan int64)\n\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\tconfig := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))\n\tconfig.FetchCooldown = 20 * time.Millisecond\n\tconfig.FetchPollInterval = 20 * time.Second // essentially disable polling\n\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}, \"another_queue\": {MaxWorkers: 1}}\n\n\tclient := newTestClient(t, dbPool, config)\n\n\tstartClient(ctx, t, client)\n\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\trequire.NoError(err)\n\n\t// Wait for the client to be ready by waiting for a job to be executed:\n\tselect {\n\tcase jobID := <-startedCh:\n\t\trequire.Equal(insertRes.Job.ID, jobID)\n\tcase <-ctx.Done():\n\t\tt.Fatal(\"timed out waiting for warmup job to start\")\n\t}\n\n\t// Now that we've run one job, we shouldn't take longer than the cooldown to\n\t// fetch another after insertion. LISTEN/NOTIFY should ensure we find out\n\t// about the inserted job much faster than the poll interval.\n\t//\n\t// Note: we specifically use a different queue to ensure that the notify\n\t// limiter is immediately to fire on this queue.\n\tinsertRes2, err := client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: \"another_queue\"})\n\trequire.NoError(err)\n\n\tselect {\n\tcase jobID := <-startedCh:\n\t\trequire.Equal(insertRes2.Job.ID, jobID)\n\t// As long as this is meaningfully shorter than the poll interval, we can be\n\t// sure the re-fetch came from listen/notify.\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"timed out waiting for 2nd job to start\")\n\t}\n\n\trequire.NoError(client.Stop(ctx))\n}\n\nfunc Test_Client_InsertNotificationsAreDeduplicatedAndDebounced(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\treturn nil\n\t})\n\tconfig.FetchPollInterval = 20 * time.Second // essentially disable polling\n\tconfig.FetchCooldown = time.Second\n\tconfig.schedulerInterval = 20 * time.Second // quiet scheduler\n\tconfig.Queues = map[string]QueueConfig{\"queue1\": {MaxWorkers: 1}, \"queue2\": {MaxWorkers: 1}, \"queue3\": {MaxWorkers: 1}}\n\tclient := newTestClient(t, dbPool, config)\n\n\tstartClient(ctx, t, client)\n\triversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())\n\n\ttype insertPayload struct {\n\t\tQueue string `json:\"queue\"`\n\t}\n\ttype notification struct {\n\t\ttopic   notifier.NotificationTopic\n\t\tpayload insertPayload\n\t}\n\tnotifyCh := make(chan notification, 10)\n\thandleNotification := func(topic notifier.NotificationTopic, payload string) {\n\t\tconfig.Logger.Info(\"received notification\", slog.String(\"topic\", string(topic)), slog.String(\"payload\", payload))\n\t\tnotif := notification{topic: topic}\n\t\trequire.NoError(t, json.Unmarshal([]byte(payload), &notif.payload))\n\t\tnotifyCh <- notif\n\t}\n\tsub, err := client.notifier.Listen(ctx, notifier.NotificationTopicInsert, handleNotification)\n\trequire.NoError(t, err)\n\tt.Cleanup(func() { sub.Unlisten(ctx) })\n\n\texpectImmediateNotification := func(t *testing.T, queue string) {\n\t\tt.Helper()\n\t\tconfig.Logger.Info(\"inserting \" + queue + \" job\")\n\t\t_, err = client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: queue})\n\t\trequire.NoError(t, err)\n\t\tnotif := riversharedtest.WaitOrTimeout(t, notifyCh)\n\t\trequire.Equal(t, notifier.NotificationTopicInsert, notif.topic)\n\t\trequire.Equal(t, queue, notif.payload.Queue)\n\t}\n\n\t// Immediate first fire on queue1:\n\texpectImmediateNotification(t, \"queue1\")\n\ttNotif1 := time.Now()\n\n\tfor i := 0; i < 5; i++ {\n\t\tconfig.Logger.Info(\"inserting queue1 job\")\n\t\t_, err = client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: \"queue1\"})\n\t\trequire.NoError(t, err)\n\t}\n\t// None of these should fire an insert notification due to debouncing:\n\tselect {\n\tcase notification := <-notifyCh:\n\t\tt.Fatalf(\"received insert notification when it should have been debounced %+v\", notification)\n\tcase <-time.After(100 * time.Millisecond):\n\t}\n\n\texpectImmediateNotification(t, \"queue2\") // Immediate first fire on queue2\n\texpectImmediateNotification(t, \"queue3\") // Immediate first fire on queue3\n\n\t// Wait until the queue1 cooldown period has passed:\n\t<-time.After(time.Until(tNotif1.Add(config.FetchCooldown)))\n\n\t// Now we should receive an immediate notification again:\n\texpectImmediateNotification(t, \"queue1\")\n}\n\nfunc Test_Client_JobCompletion(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tDBPool        *pgxpool.Pool\n\t\tSubscribeChan <-chan *Event\n\t}\n\n\tsetup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tclient := newTestClient(t, dbPool, config)\n\t\tstartClient(ctx, t, client)\n\n\t\tsubscribeChan, cancel := client.Subscribe(EventKindJobCancelled, EventKindJobCompleted, EventKindJobFailed)\n\t\tt.Cleanup(cancel)\n\n\t\treturn client, &testBundle{\n\t\t\tDBPool:        dbPool,\n\t\t\tSubscribeChan: subscribeChan,\n\t\t}\n\t}\n\n\tt.Run(\"JobThatReturnsNilIsCompleted\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn nil\n\t\t})\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateCompleted, event.Job.State)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateCompleted, reloadedJob.State)\n\t\trequire.WithinDuration(time.Now(), *reloadedJob.FinalizedAt, 2*time.Second)\n\t})\n\n\tt.Run(\"JobThatIsAlreadyCompletedIsNotAlteredByCompleter\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tvar exec riverdriver.Executor\n\t\tnow := time.Now().UTC()\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\t_, err := exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{\n\t\t\t\tID:                  job.ID,\n\t\t\t\tFinalizedAtDoUpdate: true,\n\t\t\t\tFinalizedAt:         &now,\n\t\t\t\tStateDoUpdate:       true,\n\t\t\t\tState:               rivertype.JobStateCompleted,\n\t\t\t})\n\t\t\trequire.NoError(err)\n\t\t\treturn nil\n\t\t})\n\n\t\tclient, bundle := setup(t, config)\n\t\texec = client.driver.GetExecutor()\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateCompleted, event.Job.State)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateCompleted, reloadedJob.State)\n\t\trequire.WithinDuration(now, *reloadedJob.FinalizedAt, time.Microsecond)\n\t})\n\n\tt.Run(\"JobThatReturnsErrIsRetryable\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn errors.New(\"oops\")\n\t\t})\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateRetryable, event.Job.State)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateRetryable, reloadedJob.State)\n\t\trequire.WithinDuration(time.Now(), reloadedJob.ScheduledAt, 2*time.Second)\n\t\trequire.Nil(reloadedJob.FinalizedAt)\n\t})\n\n\tt.Run(\"JobThatReturnsJobCancelErrorIsImmediatelyCancelled\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tconfig := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\t\treturn JobCancel(errors.New(\"oops\"))\n\t\t})\n\n\t\tclient, bundle := setup(t, config)\n\n\t\tinsertRes, err := client.Insert(ctx, callbackArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateCancelled, event.Job.State)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateCancelled, reloadedJob.State)\n\t\trequire.NotNil(reloadedJob.FinalizedAt)\n\t\trequire.WithinDuration(time.Now(), *reloadedJob.FinalizedAt, 2*time.Second)\n\t})\n\n\tt.Run(\"JobThatIsAlreadyDiscardedIsNotAlteredByCompleter\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tnow := time.Now().UTC()\n\n\t\tclient, bundle := setup(t, newTestConfig(t, nil))\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\t_, err := client.driver.GetExecutor().JobUpdate(ctx, &riverdriver.JobUpdateParams{\n\t\t\t\tID:                  job.ID,\n\t\t\t\tErrorsDoUpdate:      true,\n\t\t\t\tErrors:              [][]byte{[]byte(\"{\\\"error\\\": \\\"oops\\\"}\")},\n\t\t\t\tFinalizedAtDoUpdate: true,\n\t\t\t\tFinalizedAt:         &now,\n\t\t\t\tStateDoUpdate:       true,\n\t\t\t\tState:               rivertype.JobStateDiscarded,\n\t\t\t})\n\t\t\trequire.NoError(err)\n\t\t\treturn errors.New(\"oops\")\n\t\t}))\n\n\t\tinsertRes, err := client.Insert(ctx, JobArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateDiscarded, event.Job.State)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateDiscarded, reloadedJob.State)\n\t\trequire.NotNil(reloadedJob.FinalizedAt)\n\t})\n\n\tt.Run(\"JobThatIsCompletedManuallyIsNotTouchedByCompleter\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\trequire := require.New(t)\n\t\tnow := time.Now().UTC()\n\n\t\tclient, bundle := setup(t, newTestConfig(t, nil))\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tvar updatedJob *Job[JobArgs]\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\ttx, err := bundle.DBPool.Begin(ctx)\n\t\t\trequire.NoError(err)\n\n\t\t\tupdatedJob, err = JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)\n\t\t\trequire.NoError(err)\n\n\t\t\treturn tx.Commit(ctx)\n\t\t}))\n\n\t\tinsertRes, err := client.Insert(ctx, JobArgs{}, nil)\n\t\trequire.NoError(err)\n\n\t\tevent := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)\n\t\trequire.Equal(insertRes.Job.ID, event.Job.ID)\n\t\trequire.Equal(rivertype.JobStateCompleted, event.Job.State)\n\t\trequire.Equal(rivertype.JobStateCompleted, updatedJob.State)\n\t\trequire.NotNil(updatedJob)\n\t\trequire.NotNil(event.Job.FinalizedAt)\n\t\trequire.NotNil(updatedJob.FinalizedAt)\n\n\t\t// Make sure the FinalizedAt is approximately ~now:\n\t\trequire.WithinDuration(now, *updatedJob.FinalizedAt, 2*time.Second)\n\n\t\t// Make sure we're getting the same timestamp back from the event and the\n\t\t// updated job inside the txn:\n\t\trequire.WithinDuration(*updatedJob.FinalizedAt, *event.Job.FinalizedAt, time.Microsecond)\n\n\t\treloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)\n\t\trequire.NoError(err)\n\n\t\trequire.Equal(rivertype.JobStateCompleted, reloadedJob.State)\n\t\trequire.Equal(updatedJob.FinalizedAt, reloadedJob.FinalizedAt)\n\t})\n}\n\ntype unregisteredJobArgs struct{}\n\nfunc (unregisteredJobArgs) Kind() string { return \"RandomWorkerNameThatIsNeverRegistered\" }\n\nfunc Test_Client_UnknownJobKindErrorsTheJob(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\trequire := require.New(t)\n\n\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\tt.Cleanup(cancel)\n\n\tdoneCh := make(chan struct{})\n\tclose(doneCh) // don't need to block any jobs from completing\n\n\tconfig := newTestConfig(t, nil)\n\tclient := runNewTestClient(ctx, t, config)\n\n\tsubscribeChan, cancel := client.Subscribe(EventKindJobFailed)\n\tt.Cleanup(cancel)\n\n\tinsertParams, err := insertParamsFromConfigArgsAndOptions(&client.baseService.Archetype, config, unregisteredJobArgs{}, nil)\n\trequire.NoError(err)\n\tinsertedResults, err := client.driver.GetExecutor().JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})\n\trequire.NoError(err)\n\n\tinsertedResult := insertedResults[0]\n\n\tevent := riversharedtest.WaitOrTimeout(t, subscribeChan)\n\trequire.Equal(insertedResult.Job.ID, event.Job.ID)\n\trequire.Equal(\"RandomWorkerNameThatIsNeverRegistered\", insertedResult.Job.Kind)\n\trequire.Len(event.Job.Errors, 1)\n\trequire.Equal((&UnknownJobKindError{Kind: \"RandomWorkerNameThatIsNeverRegistered\"}).Error(), event.Job.Errors[0].Error)\n\trequire.Equal(rivertype.JobStateRetryable, event.Job.State)\n\t// Ensure that ScheduledAt was updated with next run time:\n\trequire.True(event.Job.ScheduledAt.After(insertedResult.Job.ScheduledAt))\n\t// It's the 1st attempt that failed. Attempt won't be incremented again until\n\t// the job gets fetched a 2nd time.\n\trequire.Equal(1, event.Job.Attempt)\n\n\trequire.NoError(client.Stop(ctx))\n}\n\nfunc Test_Client_Start_Error(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tt.Run(\"NoQueueConfiguration\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.Queues = nil\n\t\tconfig.Workers = nil\n\n\t\tclient := newTestClient(t, dbPool, config)\n\t\terr := client.Start(ctx)\n\t\trequire.EqualError(t, err, \"client Queues and Workers must be configured for a client to start working\")\n\t})\n\n\tt.Run(\"NoRegisteredWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.Workers = NewWorkers() // initialized, but empty\n\n\t\tclient := newTestClient(t, dbPool, config)\n\t\terr := client.Start(ctx)\n\t\trequire.EqualError(t, err, \"at least one Worker must be added to the Workers bundle\")\n\t})\n\n\tt.Run(\"DatabaseError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tdbConfig := riverinternaltest.DatabaseConfig(\"does-not-exist-and-dont-create-it\")\n\n\t\tdbPool, err := pgxpool.NewWithConfig(ctx, dbConfig)\n\t\trequire.NoError(t, err)\n\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tclient := newTestClient(t, dbPool, config)\n\n\t\terr = client.Start(ctx)\n\t\trequire.Error(t, err)\n\n\t\tvar pgErr *pgconn.PgError\n\t\trequire.ErrorAs(t, err, &pgErr)\n\t\trequire.Equal(t, pgerrcode.InvalidCatalogName, pgErr.Code)\n\t})\n}\n\nfunc Test_NewClient_BaseServiceName(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\tclient := newTestClient(t, dbPool, newTestConfig(t, nil))\n\t// Ensure we get the clean name \"Client\" instead of the fully qualified name\n\t// with generic type param:\n\trequire.Equal(t, \"Client\", client.baseService.Name)\n}\n\nfunc Test_NewClient_ClientIDWrittenToJobAttemptedByWhenFetched(t *testing.T) {\n\tt.Parallel()\n\trequire := require.New(t)\n\tctx := context.Background()\n\tdoneCh := make(chan struct{})\n\tstartedCh := make(chan *Job[callbackArgs])\n\n\tcallback := func(ctx context.Context, job *Job[callbackArgs]) error {\n\t\tstartedCh <- job\n\t\t<-doneCh\n\t\treturn nil\n\t}\n\n\tclient := runNewTestClient(ctx, t, newTestConfig(t, callback))\n\tt.Cleanup(func() { close(doneCh) })\n\n\t// enqueue job:\n\tinsertCtx, insertCancel := context.WithTimeout(ctx, 5*time.Second)\n\tt.Cleanup(insertCancel)\n\tinsertRes, err := client.Insert(insertCtx, callbackArgs{}, nil)\n\trequire.NoError(err)\n\trequire.Nil(insertRes.Job.AttemptedAt)\n\trequire.Empty(insertRes.Job.AttemptedBy)\n\n\tvar startedJob *Job[callbackArgs]\n\tselect {\n\tcase startedJob = <-startedCh:\n\t\trequire.Equal([]string{client.ID()}, startedJob.AttemptedBy)\n\t\trequire.NotNil(startedJob.AttemptedAt)\n\t\trequire.WithinDuration(time.Now().UTC(), *startedJob.AttemptedAt, 2*time.Second)\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"timed out waiting for job to start\")\n\t}\n}\n\nfunc Test_NewClient_Defaults(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\tworkers := NewWorkers()\n\tAddWorker(workers, &noOpWorker{})\n\n\tclient, err := NewClient(riverpgxv5.New(dbPool), &Config{\n\t\tQueues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},\n\t\tWorkers: workers,\n\t})\n\trequire.NoError(t, err)\n\n\trequire.Zero(t, client.config.AdvisoryLockPrefix)\n\n\tjobCleaner := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)\n\trequire.Equal(t, maintenance.CancelledJobRetentionPeriodDefault, jobCleaner.Config.CancelledJobRetentionPeriod)\n\trequire.Equal(t, maintenance.CompletedJobRetentionPeriodDefault, jobCleaner.Config.CompletedJobRetentionPeriod)\n\trequire.Equal(t, maintenance.DiscardedJobRetentionPeriodDefault, jobCleaner.Config.DiscardedJobRetentionPeriod)\n\trequire.Equal(t, maintenance.JobCleanerTimeoutDefault, jobCleaner.Config.Timeout)\n\trequire.False(t, jobCleaner.StaggerStartupIsDisabled())\n\n\tenqueuer := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\trequire.Zero(t, enqueuer.Config.AdvisoryLockPrefix)\n\trequire.False(t, enqueuer.StaggerStartupIsDisabled())\n\n\trequire.Nil(t, client.config.ErrorHandler)\n\trequire.Equal(t, FetchCooldownDefault, client.config.FetchCooldown)\n\trequire.Equal(t, FetchPollIntervalDefault, client.config.FetchPollInterval)\n\trequire.Equal(t, JobTimeoutDefault, client.config.JobTimeout)\n\trequire.NotZero(t, client.baseService.Logger)\n\trequire.Equal(t, MaxAttemptsDefault, client.config.MaxAttempts)\n\trequire.IsType(t, &DefaultClientRetryPolicy{}, client.config.RetryPolicy)\n}\n\nfunc Test_NewClient_Overrides(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\terrorHandler := &testErrorHandler{}\n\tlogger := slog.New(slog.NewTextHandler(os.Stderr, nil))\n\n\tworkers := NewWorkers()\n\tAddWorker(workers, &noOpWorker{})\n\n\tretryPolicy := &DefaultClientRetryPolicy{}\n\n\ttype noOpInsertMiddleware struct {\n\t\tJobInsertMiddlewareDefaults\n\t}\n\n\ttype noOpWorkerMiddleware struct {\n\t\tWorkerMiddlewareDefaults\n\t}\n\n\tclient, err := NewClient(riverpgxv5.New(dbPool), &Config{\n\t\tAdvisoryLockPrefix:          123_456,\n\t\tCancelledJobRetentionPeriod: 1 * time.Hour,\n\t\tCompletedJobRetentionPeriod: 2 * time.Hour,\n\t\tDiscardedJobRetentionPeriod: 3 * time.Hour,\n\t\tErrorHandler:                errorHandler,\n\t\tFetchCooldown:               123 * time.Millisecond,\n\t\tFetchPollInterval:           124 * time.Millisecond,\n\t\tJobInsertMiddleware:         []rivertype.JobInsertMiddleware{&noOpInsertMiddleware{}},\n\t\tJobTimeout:                  125 * time.Millisecond,\n\t\tLogger:                      logger,\n\t\tMaxAttempts:                 5,\n\t\tQueues:                      map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},\n\t\tRetryPolicy:                 retryPolicy,\n\t\tTestOnly:                    true, // disables staggered start in maintenance services\n\t\tWorkers:                     workers,\n\t\tWorkerMiddleware:            []rivertype.WorkerMiddleware{&noOpWorkerMiddleware{}},\n\t})\n\trequire.NoError(t, err)\n\n\trequire.Equal(t, int32(123_456), client.config.AdvisoryLockPrefix)\n\n\tjobCleaner := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)\n\trequire.Equal(t, 1*time.Hour, jobCleaner.Config.CancelledJobRetentionPeriod)\n\trequire.Equal(t, 2*time.Hour, jobCleaner.Config.CompletedJobRetentionPeriod)\n\trequire.Equal(t, 3*time.Hour, jobCleaner.Config.DiscardedJobRetentionPeriod)\n\trequire.True(t, jobCleaner.StaggerStartupIsDisabled())\n\n\tenqueuer := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)\n\trequire.Equal(t, int32(123_456), enqueuer.Config.AdvisoryLockPrefix)\n\trequire.True(t, enqueuer.StaggerStartupIsDisabled())\n\n\trequire.Equal(t, errorHandler, client.config.ErrorHandler)\n\trequire.Equal(t, 123*time.Millisecond, client.config.FetchCooldown)\n\trequire.Equal(t, 124*time.Millisecond, client.config.FetchPollInterval)\n\trequire.Len(t, client.config.JobInsertMiddleware, 1)\n\trequire.Equal(t, 125*time.Millisecond, client.config.JobTimeout)\n\trequire.Equal(t, logger, client.baseService.Logger)\n\trequire.Equal(t, 5, client.config.MaxAttempts)\n\trequire.Equal(t, retryPolicy, client.config.RetryPolicy)\n\trequire.Len(t, client.config.WorkerMiddleware, 1)\n}\n\nfunc Test_NewClient_MissingParameters(t *testing.T) {\n\tt.Parallel()\n\n\tt.Run(\"ErrorOnNilDriver\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, err := NewClient[pgx.Tx](nil, &Config{})\n\t\trequire.ErrorIs(t, err, errMissingDriver)\n\t})\n\n\tt.Run(\"ErrorOnNilConfig\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, err := NewClient[pgx.Tx](riverpgxv5.New(nil), nil)\n\t\trequire.ErrorIs(t, err, errMissingConfig)\n\t})\n\n\tt.Run(\"ErrorOnDriverWithNoDatabasePoolAndQueues\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t_, err := NewClient[pgx.Tx](riverpgxv5.New(nil), newTestConfig(t, nil))\n\t\trequire.ErrorIs(t, err, errMissingDatabasePoolWithQueues)\n\t})\n}\n\nfunc Test_NewClient_Validations(t *testing.T) {\n\tt.Parallel()\n\n\ttests := []struct {\n\t\tname           string\n\t\tconfigFunc     func(*Config)\n\t\twantErr        error\n\t\tvalidateResult func(*testing.T, *Client[pgx.Tx])\n\t}{\n\t\t{\n\t\t\tname:       \"CompletedJobRetentionPeriod cannot be less than zero\",\n\t\t\tconfigFunc: func(config *Config) { config.CompletedJobRetentionPeriod = -1 * time.Second },\n\t\t\twantErr:    errors.New(\"CompletedJobRetentionPeriod cannot be less than zero\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchCooldown cannot be less than FetchCooldownMin\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchCooldown = time.Millisecond - 1 },\n\t\t\twantErr:    errors.New(\"FetchCooldown must be at least 1ms\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchCooldown cannot be negative\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchCooldown = -1 },\n\t\t\twantErr:    errors.New(\"FetchCooldown must be at least 1ms\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchCooldown defaults to FetchCooldownDefault\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchCooldown = 0 },\n\t\t\twantErr:    nil,\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\trequire.Equal(t, FetchCooldownDefault, client.config.FetchCooldown)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"FetchCooldown cannot be less than FetchPollInterval\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.FetchCooldown = 20 * time.Millisecond\n\t\t\t\tconfig.FetchPollInterval = 19 * time.Millisecond\n\t\t\t},\n\t\t\twantErr: fmt.Errorf(\"FetchPollInterval cannot be shorter than FetchCooldown (%s)\", 20*time.Millisecond),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchPollInterval cannot be less than MinFetchPollInterval\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchPollInterval = time.Millisecond - 1 },\n\t\t\twantErr:    errors.New(\"FetchPollInterval must be at least 1ms\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchPollInterval cannot be negative\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchPollInterval = -1 },\n\t\t\twantErr:    errors.New(\"FetchPollInterval must be at least 1ms\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"FetchPollInterval defaults to DefaultFetchPollInterval\",\n\t\t\tconfigFunc: func(config *Config) { config.FetchPollInterval = 0 },\n\t\t\twantErr:    nil,\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\trequire.Equal(t, FetchPollIntervalDefault, client.config.FetchPollInterval)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"ID cannot be longer than 100 characters\",\n\t\t\t// configFunc: func(config *Config) { config.ID = strings.Repeat(\"a\", 101) },\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.ID = strings.Repeat(\"a\", 101)\n\t\t\t},\n\t\t\twantErr: errors.New(\"ID cannot be longer than 100 characters\"),\n\t\t},\n\t\t{\n\t\t\tname: \"JobTimeout can be -1 (infinite)\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = -1\n\t\t\t},\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\trequire.Equal(t, time.Duration(-1), client.config.JobTimeout)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"JobTimeout cannot be less than -1\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = -2\n\t\t\t},\n\t\t\twantErr: errors.New(\"JobTimeout cannot be negative, except for -1 (infinite)\"),\n\t\t},\n\t\t{\n\t\t\tname: \"JobTimeout of zero applies DefaultJobTimeout\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = 0\n\t\t\t},\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\t// A client config value of zero gets interpreted as the default timeout:\n\t\t\t\trequire.Equal(t, JobTimeoutDefault, client.config.JobTimeout)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"JobTimeout can be a large positive value\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = 7 * 24 * time.Hour\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"MaxAttempts cannot be less than zero\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.MaxAttempts = -1\n\t\t\t},\n\t\t\twantErr: errors.New(\"MaxAttempts cannot be less than zero\"),\n\t\t},\n\t\t{\n\t\t\tname: \"MaxAttempts of zero applies DefaultMaxAttempts\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.MaxAttempts = 0\n\t\t\t},\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\t// A client config value of zero gets interpreted as the default max attempts:\n\t\t\t\trequire.Equal(t, MaxAttemptsDefault, client.config.MaxAttempts)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"RescueStuckJobsAfter may be overridden\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.RescueStuckJobsAfter = 23 * time.Hour\n\t\t\t},\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\trequire.Equal(t, 23*time.Hour, client.config.RescueStuckJobsAfter)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"RescueStuckJobsAfter must be larger than JobTimeout\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = 7 * time.Hour\n\t\t\t\tconfig.RescueStuckJobsAfter = 6 * time.Hour\n\t\t\t},\n\t\t\twantErr: errors.New(\"RescueStuckJobsAfter cannot be less than JobTimeout\"),\n\t\t},\n\t\t{\n\t\t\tname: \"RescueStuckJobsAfter increased automatically on a high JobTimeout when not set explicitly\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.JobTimeout = 23 * time.Hour\n\t\t\t},\n\t\t\tvalidateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper\n\t\t\t\trequire.Equal(t, 23*time.Hour+maintenance.JobRescuerRescueAfterDefault, client.config.RescueStuckJobsAfter)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Queues can be nil when Workers is also nil\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = nil\n\t\t\t\tconfig.Workers = nil\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Queues can be nil when Workers is not nil\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = nil\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:       \"Queues can be empty\",\n\t\t\tconfigFunc: func(config *Config) { config.Queues = make(map[string]QueueConfig) },\n\t\t},\n\t\t{\n\t\t\tname: \"Queues MaxWorkers can't be negative\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: -1}}\n\t\t\t},\n\t\t\twantErr: errors.New(\"invalid number of workers for queue \\\"default\\\": -1\"),\n\t\t},\n\t\t{\n\t\t\tname: \"Queues can't have limits larger than MaxQueueNumWorkers\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: QueueNumWorkersMax + 1}}\n\t\t\t},\n\t\t\twantErr: fmt.Errorf(\"invalid number of workers for queue \\\"default\\\": %d\", QueueNumWorkersMax+1),\n\t\t},\n\t\t{\n\t\t\tname: \"Queues queue names can't be empty\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{\"\": {MaxWorkers: 1}}\n\t\t\t},\n\t\t\twantErr: errors.New(\"queue name cannot be empty\"),\n\t\t},\n\t\t{\n\t\t\tname: \"Queues queue names can't be too long\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{strings.Repeat(\"a\", 65): {MaxWorkers: 1}}\n\t\t\t},\n\t\t\twantErr: errors.New(\"queue name cannot be longer than 64 characters\"),\n\t\t},\n\t\t{\n\t\t\tname: \"Queues queue names can't have asterisks\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{\"no*hyphens\": {MaxWorkers: 1}}\n\t\t\t},\n\t\t\twantErr: errors.New(\"queue name is invalid, expected letters and numbers separated by underscores or hyphens: \\\"no*hyphens\\\"\"),\n\t\t},\n\t\t{\n\t\t\tname: \"Queues queue names can be letters and numbers joined by underscores\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{\"some_awesome_3rd_queue_namezzz\": {MaxWorkers: 1}}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Queues queue names can be letters and numbers joined by hyphens\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = map[string]QueueConfig{\"some-awesome-3rd-queue-namezzz\": {MaxWorkers: 1}}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Workers can be nil\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Queues = nil\n\t\t\t\tconfig.Workers = nil\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Workers can be empty\", // but notably, not allowed to be empty if started\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Workers = NewWorkers()\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Workers cannot be empty if Queues is set\",\n\t\t\tconfigFunc: func(config *Config) {\n\t\t\t\tconfig.Workers = nil\n\t\t\t},\n\t\t\twantErr: errors.New(\"Workers must be set if Queues is set\"),\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\ttt := tt\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\tctx := context.Background()\n\t\t\trequire := require.New(t)\n\t\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\t\tworkers := NewWorkers()\n\t\t\tAddWorker(workers, &noOpWorker{})\n\n\t\t\tconfig := &Config{\n\t\t\t\tQueues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},\n\t\t\t\tWorkers: workers,\n\t\t\t}\n\t\t\ttt.configFunc(config)\n\n\t\t\tclient, err := NewClient(riverpgxv5.New(dbPool), config)\n\t\t\tif tt.wantErr != nil {\n\t\t\t\trequire.Error(err)\n\t\t\t\trequire.ErrorContains(err, tt.wantErr.Error())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trequire.NoError(err)\n\n\t\t\tif tt.validateResult != nil {\n\t\t\t\ttt.validateResult(t, client)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype timeoutTestArgs struct {\n\tTimeoutValue time.Duration `json:\"timeout_value\"`\n}\n\nfunc (timeoutTestArgs) Kind() string { return \"timeoutTest\" }\n\ntype testWorkerDeadline struct {\n\tdeadline time.Time\n\tok       bool\n}\n\ntype timeoutTestWorker struct {\n\tWorkerDefaults[timeoutTestArgs]\n\tdoneCh chan testWorkerDeadline\n}\n\nfunc (w *timeoutTestWorker) Timeout(job *Job[timeoutTestArgs]) time.Duration {\n\treturn job.Args.TimeoutValue\n}\n\nfunc (w *timeoutTestWorker) Work(ctx context.Context, job *Job[timeoutTestArgs]) error {\n\tdeadline, ok := ctx.Deadline()\n\tw.doneCh <- testWorkerDeadline{deadline: deadline, ok: ok}\n\treturn nil\n}\n\nfunc TestClient_JobTimeout(t *testing.T) {\n\tt.Parallel()\n\n\ttests := []struct {\n\t\tname             string\n\t\tjobArgTimeout    time.Duration\n\t\tclientJobTimeout time.Duration\n\t\twantDuration     time.Duration\n\t}{\n\t\t{\n\t\t\tname:             \"ClientJobTimeoutIsUsedIfJobArgTimeoutIsZero\",\n\t\t\tjobArgTimeout:    0,\n\t\t\tclientJobTimeout: time.Hour,\n\t\t\twantDuration:     time.Hour,\n\t\t},\n\t\t{\n\t\t\tname:             \"JobArgTimeoutTakesPrecedenceIfBothAreSet\",\n\t\t\tjobArgTimeout:    2 * time.Hour,\n\t\t\tclientJobTimeout: time.Hour,\n\t\t\twantDuration:     2 * time.Hour,\n\t\t},\n\t\t{\n\t\t\tname:             \"DefaultJobTimeoutIsUsedIfBothAreZero\",\n\t\t\tjobArgTimeout:    0,\n\t\t\tclientJobTimeout: 0,\n\t\t\twantDuration:     JobTimeoutDefault,\n\t\t},\n\t\t{\n\t\t\tname:             \"NoJobTimeoutIfClientIsNegativeOneAndJobArgIsZero\",\n\t\t\tjobArgTimeout:    0,\n\t\t\tclientJobTimeout: -1,\n\t\t\twantDuration:     0, // infinite\n\t\t},\n\t\t{\n\t\t\tname:             \"NoJobTimeoutIfJobArgIsNegativeOne\",\n\t\t\tjobArgTimeout:    -1,\n\t\t\tclientJobTimeout: time.Hour,\n\t\t\twantDuration:     0, // infinite\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\ttt := tt\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\t\t\trequire := require.New(t)\n\t\t\tctx := context.Background()\n\n\t\t\ttestWorker := &timeoutTestWorker{doneCh: make(chan testWorkerDeadline)}\n\n\t\t\tworkers := NewWorkers()\n\t\t\tAddWorker(workers, testWorker)\n\n\t\t\tconfig := newTestConfig(t, nil)\n\t\t\tconfig.JobTimeout = tt.clientJobTimeout\n\t\t\tconfig.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}}\n\t\t\tconfig.Workers = workers\n\n\t\t\tclient := runNewTestClient(ctx, t, config)\n\t\t\t_, err := client.Insert(ctx, timeoutTestArgs{TimeoutValue: tt.jobArgTimeout}, nil)\n\t\t\trequire.NoError(err)\n\n\t\t\tresult := riversharedtest.WaitOrTimeout(t, testWorker.doneCh)\n\t\t\tif tt.wantDuration == 0 {\n\t\t\t\trequire.False(result.ok, \"expected no deadline\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\trequire.True(result.ok, \"expected a deadline, but none was set\")\n\t\t\trequire.WithinDuration(time.Now().Add(tt.wantDuration), result.deadline, 2*time.Second)\n\t\t})\n\t}\n}\n\ntype JobArgsStaticKind struct {\n\tkind string\n}\n\nfunc (a JobArgsStaticKind) Kind() string {\n\treturn a.kind\n}\n\nfunc TestInsertParamsFromJobArgsAndOptions(t *testing.T) {\n\tt.Parallel()\n\n\tarchetype := riversharedtest.BaseServiceArchetype(t)\n\tconfig := newTestConfig(t, nil)\n\n\tt.Run(\"Defaults\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, `{\"name\":\"\"}`, string(insertParams.EncodedArgs))\n\t\trequire.Equal(t, (noOpArgs{}).Kind(), insertParams.Kind)\n\t\trequire.Equal(t, config.MaxAttempts, insertParams.MaxAttempts)\n\t\trequire.Equal(t, rivercommon.PriorityDefault, insertParams.Priority)\n\t\trequire.Equal(t, QueueDefault, insertParams.Queue)\n\t\trequire.Nil(t, insertParams.ScheduledAt)\n\t\trequire.Equal(t, []string{}, insertParams.Tags)\n\t\trequire.Empty(t, insertParams.UniqueKey)\n\t\trequire.Zero(t, insertParams.UniqueStates)\n\t})\n\n\tt.Run(\"ConfigOverrides\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\toverrideConfig := &Config{\n\t\t\tMaxAttempts: 34,\n\t\t}\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, overrideConfig, noOpArgs{}, nil)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, overrideConfig.MaxAttempts, insertParams.MaxAttempts)\n\t})\n\n\tt.Run(\"InsertOptsOverrides\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\topts := &InsertOpts{\n\t\t\tMaxAttempts: 42,\n\t\t\tPriority:    2,\n\t\t\tQueue:       \"other\",\n\t\t\tScheduledAt: time.Now().Add(time.Hour),\n\t\t\tTags:        []string{\"tag1\", \"tag2\"},\n\t\t}\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, opts)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 42, insertParams.MaxAttempts)\n\t\trequire.Equal(t, 2, insertParams.Priority)\n\t\trequire.Equal(t, \"other\", insertParams.Queue)\n\t\trequire.Equal(t, opts.ScheduledAt, *insertParams.ScheduledAt)\n\t\trequire.Equal(t, []string{\"tag1\", \"tag2\"}, insertParams.Tags)\n\t})\n\n\tt.Run(\"WorkerInsertOptsOverrides\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tnearFuture := time.Now().Add(5 * time.Minute)\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{\n\t\t\tScheduledAt: nearFuture,\n\t\t}, nil)\n\t\trequire.NoError(t, err)\n\t\t// All these come from overrides in customInsertOptsJobArgs's definition:\n\t\trequire.Equal(t, 42, insertParams.MaxAttempts)\n\t\trequire.Equal(t, 2, insertParams.Priority)\n\t\trequire.Equal(t, \"other\", insertParams.Queue)\n\t\trequire.NotNil(t, insertParams.ScheduledAt)\n\t\trequire.Equal(t, nearFuture, *insertParams.ScheduledAt)\n\t\trequire.Equal(t, []string{\"tag1\", \"tag2\"}, insertParams.Tags)\n\t})\n\n\tt.Run(\"WorkerInsertOptsScheduledAtNotRespectedIfZero\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{\n\t\t\tScheduledAt: time.Time{},\n\t\t}, nil)\n\t\trequire.NoError(t, err)\n\t\trequire.Nil(t, insertParams.ScheduledAt)\n\t})\n\n\tt.Run(\"TagFormatValidated\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t{\n\t\t\t_, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{}, &InsertOpts{\n\t\t\t\tTags: []string{strings.Repeat(\"h\", 256)},\n\t\t\t})\n\t\t\trequire.EqualError(t, err, \"tags should be a maximum of 255 characters long\")\n\t\t}\n\n\t\t{\n\t\t\t_, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{}, &InsertOpts{\n\t\t\t\tTags: []string{\"tag,with,comma\"},\n\t\t\t})\n\t\t\trequire.EqualError(t, err, \"tags should match regex \"+tagRE.String())\n\t\t}\n\t})\n\n\tt.Run(\"UniqueOptsDefaultStates\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tarchetype := riversharedtest.BaseServiceArchetype(t)\n\t\tarchetype.Time.StubNowUTC(time.Now().UTC())\n\n\t\tuniqueOpts := UniqueOpts{\n\t\t\tByArgs:      true,\n\t\t\tByPeriod:    10 * time.Second,\n\t\t\tByQueue:     true,\n\t\t\tExcludeKind: true,\n\t\t}\n\n\t\tparams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{UniqueOpts: uniqueOpts})\n\t\trequire.NoError(t, err)\n\t\tinternalUniqueOpts := &dbunique.UniqueOpts{\n\t\t\tByArgs:      true,\n\t\t\tByPeriod:    10 * time.Second,\n\t\t\tByQueue:     true,\n\t\t\tByState:     []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted, rivertype.JobStatePending, rivertype.JobStateRunning, rivertype.JobStateRetryable, rivertype.JobStateScheduled},\n\t\t\tExcludeKind: true,\n\t\t}\n\n\t\texpectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, expectedKey, params.UniqueKey)\n\t\trequire.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)\n\t})\n\n\tt.Run(\"UniqueOptsCustomStates\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tarchetype := riversharedtest.BaseServiceArchetype(t)\n\t\tarchetype.Time.StubNowUTC(time.Now().UTC())\n\n\t\tstates := []rivertype.JobState{\n\t\t\trivertype.JobStateAvailable,\n\t\t\trivertype.JobStatePending,\n\t\t\trivertype.JobStateRetryable,\n\t\t\trivertype.JobStateRunning,\n\t\t\trivertype.JobStateScheduled,\n\t\t}\n\n\t\tuniqueOpts := UniqueOpts{\n\t\t\tByPeriod: 10 * time.Second,\n\t\t\tByQueue:  true,\n\t\t\tByState:  states,\n\t\t}\n\n\t\tparams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{UniqueOpts: uniqueOpts})\n\t\trequire.NoError(t, err)\n\t\tinternalUniqueOpts := &dbunique.UniqueOpts{\n\t\t\tByPeriod: 10 * time.Second,\n\t\t\tByQueue:  true,\n\t\t\tByState:  states,\n\t\t}\n\n\t\texpectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, expectedKey, params.UniqueKey)\n\t\trequire.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)\n\t})\n\n\tt.Run(\"UniqueOptsWithPartialArgs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tuniqueOpts := UniqueOpts{ByArgs: true}\n\n\t\ttype PartialArgs struct {\n\t\t\tJobArgsStaticKind\n\t\t\tIncluded bool `json:\"included\" river:\"unique\"`\n\t\t\tExcluded bool `json:\"excluded\"`\n\t\t}\n\n\t\targs := PartialArgs{\n\t\t\tJobArgsStaticKind: JobArgsStaticKind{kind: \"partialArgs\"},\n\t\t\tIncluded:          true,\n\t\t\tExcluded:          true,\n\t\t}\n\n\t\tparams, err := insertParamsFromConfigArgsAndOptions(archetype, config, args, &InsertOpts{UniqueOpts: uniqueOpts})\n\t\trequire.NoError(t, err)\n\t\tinternalUniqueOpts := &dbunique.UniqueOpts{ByArgs: true}\n\n\t\texpectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, expectedKey, params.UniqueKey)\n\t\trequire.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)\n\n\t\targsWithExcludedFalse := PartialArgs{\n\t\t\tJobArgsStaticKind: JobArgsStaticKind{kind: \"partialArgs\"},\n\t\t\tIncluded:          true,\n\t\t\tExcluded:          false,\n\t\t}\n\n\t\tparams2, err := insertParamsFromConfigArgsAndOptions(archetype, config, argsWithExcludedFalse, &InsertOpts{UniqueOpts: uniqueOpts})\n\t\trequire.NoError(t, err)\n\t\tinternalUniqueOpts2 := &dbunique.UniqueOpts{ByArgs: true}\n\n\t\texpectedKey2, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts2, params2)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, expectedKey2, params2.UniqueKey)\n\t\trequire.Equal(t, internalUniqueOpts2.StateBitmask(), params.UniqueStates)\n\t\trequire.Equal(t, params.UniqueKey, params2.UniqueKey, \"unique keys should be identical because included args are the same, even though others differ\")\n\t})\n\n\tt.Run(\"PriorityIsLimitedTo4\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{Priority: 5})\n\t\trequire.ErrorContains(t, err, \"priority must be between 1 and 4\")\n\t\trequire.Nil(t, insertParams)\n\t})\n\n\tt.Run(\"NonEmptyArgs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\targs := timeoutTestArgs{TimeoutValue: time.Hour}\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, args, nil)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, `{\"timeout_value\":3600000000000}`, string(insertParams.EncodedArgs))\n\t})\n\n\tt.Run(\"UniqueOptsAreValidated\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\t// Ensure that unique opts are validated. No need to be exhaustive here\n\t\t// since we already have tests elsewhere for that. Just make sure validation\n\t\t// is running.\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(\n\t\t\tarchetype,\n\t\t\tconfig,\n\t\t\tnoOpArgs{},\n\t\t\t&InsertOpts{UniqueOpts: UniqueOpts{ByPeriod: 1 * time.Millisecond}},\n\t\t)\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByPeriod should not be less than 1 second\")\n\t\trequire.Nil(t, insertParams)\n\t})\n}\n\nfunc TestID(t *testing.T) {\n\tt.Parallel()\n\tctx := context.Background()\n\n\tt.Run(\"IsGeneratedWhenNotSpecifiedInConfig\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tclient := newTestClient(t, dbPool, newTestConfig(t, nil))\n\t\trequire.NotEmpty(t, client.ID())\n\t})\n\n\tt.Run(\"IsGeneratedWhenNotSpecifiedInConfig\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\tconfig := newTestConfig(t, nil)\n\t\tconfig.ID = \"my-client-id\"\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\tclient := newTestClient(t, dbPool, config)\n\t\trequire.Equal(t, \"my-client-id\", client.ID())\n\t})\n}\n\ntype customInsertOptsJobArgs struct {\n\tScheduledAt time.Time `json:\"scheduled_at\"`\n}\n\nfunc (w *customInsertOptsJobArgs) Kind() string { return \"customInsertOpts\" }\n\nfunc (w *customInsertOptsJobArgs) InsertOpts() InsertOpts {\n\treturn InsertOpts{\n\t\tMaxAttempts: 42,\n\t\tPriority:    2,\n\t\tQueue:       \"other\",\n\t\tScheduledAt: w.ScheduledAt,\n\t\tTags:        []string{\"tag1\", \"tag2\"},\n\t}\n}\n\nfunc (w *customInsertOptsJobArgs) Work(context.Context, *Job[noOpArgs]) error { return nil }\n\nfunc TestInsert(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\tworkers := NewWorkers()\n\tAddWorker(workers, &noOpWorker{})\n\n\tconfig := &Config{\n\t\tQueues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},\n\t\tWorkers: workers,\n\t}\n\n\tclient, err := NewClient(riverpgxv5.New(dbPool), config)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnow := time.Now()\n\trequireEqualArgs := func(t *testing.T, expectedArgs *noOpArgs, actualPayload []byte) {\n\t\tt.Helper()\n\t\tvar actualArgs noOpArgs\n\t\tif err := json.Unmarshal(actualPayload, &actualArgs); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\trequire.Equal(t, *expectedArgs, actualArgs)\n\t}\n\n\ttests := []struct {\n\t\tname   string\n\t\targs   noOpArgs\n\t\topts   *InsertOpts\n\t\tassert func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow)\n\t}{\n\t\t{\n\t\t\tname: \"all options specified\",\n\t\t\targs: noOpArgs{Name: \"testJob\"},\n\t\t\topts: &InsertOpts{\n\t\t\t\tQueue:    \"other\",\n\t\t\t\tPriority: 2, // TODO: enforce a range on priority\n\t\t\t\t// TODO: comprehensive timezone testing\n\t\t\t\tScheduledAt: now.Add(time.Hour).In(time.FixedZone(\"UTC-5\", -5*60*60)),\n\t\t\t\tTags:        []string{\"tag1\", \"tag2\"},\n\t\t\t},\n\t\t\tassert: func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow) {\n\t\t\t\tt.Helper()\n\n\t\t\t\trequire := require.New(t)\n\t\t\t\t// specified by inputs:\n\t\t\t\trequireEqualArgs(t, args, insertedJob.EncodedArgs)\n\t\t\t\trequire.Equal(\"other\", insertedJob.Queue)\n\t\t\t\trequire.Equal(2, insertedJob.Priority)\n\t\t\t\t// Postgres timestamptz only stores microsecond precision so we need to\n\t\t\t\t// assert approximate equality:\n\t\t\t\trequire.WithinDuration(opts.ScheduledAt.UTC(), insertedJob.ScheduledAt, time.Microsecond)\n\t\t\t\trequire.Equal([]string{\"tag1\", \"tag2\"}, insertedJob.Tags)\n\t\t\t\t// derived state:\n\t\t\t\trequire.Equal(rivertype.JobStateScheduled, insertedJob.State)\n\t\t\t\trequire.Equal(\"noOp\", insertedJob.Kind)\n\t\t\t\t// default state:\n\t\t\t\t// require.Equal([]byte(\"{}\"), insertedJob.metadata)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"all defaults\",\n\t\t\targs: noOpArgs{Name: \"testJob\"},\n\t\t\topts: nil,\n\t\t\tassert: func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow) {\n\t\t\t\tt.Helper()\n\n\t\t\t\trequire := require.New(t)\n\t\t\t\t// specified by inputs:\n\t\t\t\trequireEqualArgs(t, args, insertedJob.EncodedArgs)\n\t\t\t\t// derived state:\n\t\t\t\trequire.Equal(rivertype.JobStateAvailable, insertedJob.State)\n\t\t\t\trequire.Equal(\"noOp\", insertedJob.Kind)\n\t\t\t\t// default state:\n\t\t\t\trequire.Equal(QueueDefault, insertedJob.Queue)\n\t\t\t\trequire.Equal(1, insertedJob.Priority)\n\t\t\t\t// Default comes from database now(), and we can't know the exact value:\n\t\t\t\trequire.WithinDuration(time.Now(), insertedJob.ScheduledAt, 2*time.Second)\n\t\t\t\trequire.Equal([]string{}, insertedJob.Tags)\n\t\t\t\t// require.Equal([]byte(\"{}\"), insertedJob.metadata)\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\ttt := tt\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\trequire := require.New(t)\n\t\t\tif tt.assert == nil {\n\t\t\t\tt.Fatalf(\"test %q did not specify an assert function\", tt.name)\n\t\t\t}\n\n\t\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\tinsertRes, err := client.Insert(ctx, tt.args, tt.opts)\n\t\t\trequire.NoError(err)\n\t\t\ttt.assert(t, &tt.args, tt.opts, insertRes.Job)\n\n\t\t\t// Also test InsertTx:\n\t\t\ttx, err := dbPool.Begin(ctx)\n\t\t\trequire.NoError(err)\n\t\t\tdefer tx.Rollback(ctx)\n\n\t\t\tinsertedJob2, err := client.InsertTx(ctx, tx, tt.args, tt.opts)\n\t\t\trequire.NoError(err)\n\t\t\ttt.assert(t, &tt.args, tt.opts, insertedJob2.Job)\n\t\t})\n\t}\n}\n\nfunc TestUniqueOpts(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tworkers := NewWorkers()\n\t\tAddWorker(workers, &noOpWorker{})\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tclient := newTestClient(t, dbPool, newTestConfig(t, nil))\n\n\t\t// Tests that use ByPeriod below can be sensitive to intermittency if\n\t\t// the tests run at say 23:59:59.998, then it's possible to accidentally\n\t\t// cross a period threshold, even if very unlikely. So here, seed mostly\n\t\t// the current time, but make sure it's nicened up a little to be\n\t\t// roughly in the middle of the hour and well clear of any period\n\t\t// boundaries.\n\t\tclient.baseService.Time.StubNowUTC(\n\t\t\ttime.Now().Truncate(1 * time.Hour).Add(37*time.Minute + 23*time.Second + 123*time.Millisecond).UTC(),\n\t\t)\n\n\t\treturn client, &testBundle{}\n\t}\n\n\tt.Run(\"DeduplicatesJobs\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tuniqueOpts := UniqueOpts{\n\t\t\tByPeriod: 24 * time.Hour,\n\t\t}\n\n\t\tinsertRes0, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.False(t, insertRes0.UniqueSkippedAsDuplicate)\n\n\t\tinsertRes1, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, insertRes1.UniqueSkippedAsDuplicate)\n\n\t\t// Expect the same job to come back.\n\t\trequire.Equal(t, insertRes0.Job.ID, insertRes1.Job.ID)\n\t})\n\n\tt.Run(\"UniqueByCustomStates\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tuniqueOpts := UniqueOpts{\n\t\t\tByPeriod: 24 * time.Hour,\n\t\t\tByState:  rivertype.JobStates(),\n\t\t\tByQueue:  true,\n\t\t}\n\n\t\tinsertRes0, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tinsertRes1, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t// Expect the same job to come back because we deduplicate from the original.\n\t\trequire.Equal(t, insertRes0.Job.ID, insertRes1.Job.ID)\n\n\t\tinsertRes2, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\t// Use another queue so the job can be inserted:\n\t\t\tQueue:      \"other\",\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t// This job however is _not_ the same because it's inserted as\n\t\t// `scheduled` which is outside the unique constraints.\n\t\trequire.NotEqual(t, insertRes0.Job.ID, insertRes2.Job.ID)\n\t})\n\n\tt.Run(\"ErrorsWithUniqueV1CustomStates\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tuniqueOpts := UniqueOpts{\n\t\t\tByPeriod: 24 * time.Hour,\n\t\t\tByState:  []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted},\n\t\t}\n\n\t\tinsertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{\n\t\t\tUniqueOpts: uniqueOpts,\n\t\t})\n\t\trequire.EqualError(t, err, \"UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled\")\n\t\trequire.Nil(t, insertRes)\n\t})\n}\n\nfunc TestDefaultClientID(t *testing.T) {\n\tt.Parallel()\n\n\thost, _ := os.Hostname()\n\trequire.NotEmpty(t, host)\n\n\tstartedAt := time.Date(2024, time.March, 7, 4, 39, 12, 123456789, time.UTC)\n\n\trequire.Equal(t, strings.ReplaceAll(host, \".\", \"_\")+\"_2024_03_07T04_39_12_123456\", defaultClientID(startedAt))\n}\n\nfunc TestDefaultClientIDWithHost(t *testing.T) {\n\tt.Parallel()\n\n\thost, _ := os.Hostname()\n\trequire.NotEmpty(t, host)\n\n\tstartedAt := time.Date(2024, time.March, 7, 4, 39, 12, 123456789, time.UTC)\n\n\trequire.Equal(t, \"example_com_2024_03_07T04_39_12_123456\", defaultClientIDWithHost(startedAt,\n\t\t\"example.com\"))\n\trequire.Equal(t, \"this_is_a_degenerately_long_host_name_that_will_be_truncated_2024_03_07T04_39_12_123456\", defaultClientIDWithHost(startedAt,\n\t\t\"this.is.a.degenerately.long.host.name.that.will.be.truncated.so.were.not.storing.massive.strings.to.the.database.com\"))\n\n\t// Test strings right around the boundary to make sure we don't have some off-by-one slice error.\n\trequire.Equal(t, strings.Repeat(\"a\", 59)+\"_2024_03_07T04_39_12_123456\", defaultClientIDWithHost(startedAt, strings.Repeat(\"a\", 59)))\n\trequire.Equal(t, strings.Repeat(\"a\", 60)+\"_2024_03_07T04_39_12_123456\", defaultClientIDWithHost(startedAt, strings.Repeat(\"a\", 60)))\n\trequire.Equal(t, strings.Repeat(\"a\", 60)+\"_2024_03_07T04_39_12_123456\", defaultClientIDWithHost(startedAt, strings.Repeat(\"a\", 61)))\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "common_test.go",
          "type": "blob",
          "size": 0.91796875,
          "content": "package river_test\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n)\n\n//\n// This file used as a holding place for test helpers for examples so that the\n// helpers aren't included in Godoc and keep each example more succinct.\n//\n\n// Wait on the given subscription channel for numJobs. Times out with a panic if\n// jobs take too long to be received.\nfunc waitForNJobs(subscribeChan <-chan *river.Event, numJobs int) {\n\tvar (\n\t\ttimeout  = riversharedtest.WaitTimeout()\n\t\tdeadline = time.Now().Add(timeout)\n\t\tevents   = make([]*river.Event, 0, numJobs)\n\t)\n\n\tfor {\n\t\tselect {\n\t\tcase event := <-subscribeChan:\n\t\t\tevents = append(events, event)\n\n\t\t\tif len(events) >= numJobs {\n\t\t\t\treturn\n\t\t\t}\n\n\t\tcase <-time.After(time.Until(deadline)):\n\t\t\tpanic(fmt.Sprintf(\"WaitOrTimeout timed out after waiting %s (received %d job(s), wanted %d)\",\n\t\t\t\ttimeout, len(events), numJobs))\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 1.96875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n)\n\nvar errClientNotInContext = errors.New(\"river: client not found in context, can only be used in a Worker\")\n\nfunc withClient[TTx any](ctx context.Context, client *Client[TTx]) context.Context {\n\treturn context.WithValue(ctx, rivercommon.ContextKeyClient{}, client)\n}\n\n// ClientFromContext returns the Client from the context. This function can\n// only be used within a Worker's Work() method because that is the only place\n// River sets the Client on the context.\n//\n// It panics if the context does not contain a Client, which will never happen\n// from the context provided to a Worker's Work() method.\n//\n// When testing JobArgs.Work implementations, it might be useful to use\n// rivertest.WorkContext to initialize a context that has an available client.\n//\n// The type parameter TTx is the transaction type used by the [Client],\n// pgx.Tx for the pgx driver, and *sql.Tx for the [database/sql] driver.\nfunc ClientFromContext[TTx any](ctx context.Context) *Client[TTx] {\n\tclient, err := ClientFromContextSafely[TTx](ctx)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn client\n}\n\n// ClientFromContext returns the Client from the context. This function can\n// only be used within a Worker's Work() method because that is the only place\n// River sets the Client on the context.\n//\n// It returns an error if the context does not contain a Client, which will\n// never happen from the context provided to a Worker's Work() method.\n//\n// When testing JobArgs.Work implementations, it might be useful to use\n// rivertest.WorkContext to initialize a context that has an available client.\n//\n// See the examples for [ClientFromContext] to understand how to use this\n// function.\nfunc ClientFromContextSafely[TTx any](ctx context.Context) (*Client[TTx], error) {\n\tclient, exists := ctx.Value(rivercommon.ContextKeyClient{}).(*Client[TTx])\n\tif !exists || client == nil {\n\t\treturn nil, errClientNotInContext\n\t}\n\treturn client, nil\n}\n"
        },
        {
          "name": "context_test.go",
          "type": "blob",
          "size": 0.685546875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestClientFromContext(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tclient := &Client[pgx.Tx]{}\n\tctx = withClient(ctx, client)\n\n\trequire.Equal(t, client, ClientFromContext[pgx.Tx](ctx))\n\n\tresult, err := ClientFromContextSafely[pgx.Tx](ctx)\n\trequire.NoError(t, err)\n\trequire.Equal(t, client, result)\n\n\trequire.PanicsWithError(t, errClientNotInContext.Error(), func() {\n\t\tClientFromContext[pgx.Tx](context.Background())\n\t})\n\n\tresult, err = ClientFromContextSafely[pgx.Tx](context.Background())\n\trequire.ErrorIs(t, err, errClientNotInContext)\n\trequire.Nil(t, result)\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 6.78515625,
          "content": "/*\nPackage river is a robust high-performance job processing system for Go and\nPostgres.\n\nSee [homepage], [docs], and [godoc], as well as the [River UI].\n\nBeing built for Postgres, River encourages the use of the same database for\napplication data and job queue. By enqueueing jobs transactionally along with\nother database changes, whole classes of distributed systems problems are\navoided. Jobs are guaranteed to be enqueued if their transaction commits, are\nremoved if their transaction rolls back, and aren't visible for work _until_\ncommit. See [transactional enqueueing] for more background on this philosophy.\n\n# Job args and workers\n\nJobs are defined in struct pairs, with an implementation of [`JobArgs`] and one\nof [`Worker`].\n\nJob args contain `json` annotations and define how jobs are serialized to and\nfrom the database, along with a \"kind\", a stable string that uniquely identifies\nthe job.\n\n\ttype SortArgs struct {\n\t\t// Strings is a slice of strings to sort.\n\t\tStrings []string `json:\"strings\"`\n\t}\n\n\tfunc (SortArgs) Kind() string { return \"sort\" }\n\nWorkers expose a `Work` function that dictates how jobs run.\n\n\ttype SortWorker struct {\n\t    // An embedded WorkerDefaults sets up default methods to fulfill the rest of\n\t    // the Worker interface:\n\t    river.WorkerDefaults[SortArgs]\n\t}\n\n\tfunc (w *SortWorker) Work(ctx context.Context, job *river.Job[SortArgs]) error {\n\t    sort.Strings(job.Args.Strings)\n\t    fmt.Printf(\"Sorted strings: %+v\\n\", job.Args.Strings)\n\t    return nil\n\t}\n\n# Registering workers\n\nJobs are uniquely identified by their \"kind\" string. Workers are registered on\nstart up so that River knows how to assign jobs to workers:\n\n\tworkers := river.NewWorkers()\n\t// AddWorker panics if the worker is already registered or invalid:\n\triver.AddWorker(workers, &SortWorker{})\n\n# Starting a client\n\nA River [`Client`] provides an interface for job insertion and manages job\nprocessing and [maintenance services]. A client's created with a database pool,\n[driver], and config struct containing a `Workers` bundle and other settings.\nHere's a client `Client` working one queue (`\"default\"`) with up to 100 worker\ngoroutines at a time:\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t    Queues: map[string]river.QueueConfig{\n\t        river.QueueDefault: {MaxWorkers: 100},\n\t    },\n\t    Workers: workers,\n\t})\n\tif err != nil {\n\t    panic(err)\n\t}\n\n\t// Run the client inline. All executed jobs will inherit from ctx:\n\tif err := riverClient.Start(ctx); err != nil {\n\t    panic(err)\n\t}\n\n## Insert-only clients\n\nIt's often desirable to have a client that'll be used for inserting jobs, but\nnot working them. This is possible by omitting the `Queues` configuration, and\nskipping the call to `Start`:\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t    Workers: workers,\n\t})\n\tif err != nil {\n\t    panic(err)\n\t}\n\n`Workers` can also be omitted, but it's better to include it so River can check\nthat inserted job kinds have a worker that can run them.\n\n## Stopping\n\nThe client should also be stopped on program shutdown:\n\n\t// Stop fetching new work and wait for active jobs to finish.\n\tif err := riverClient.Stop(ctx); err != nil {\n\t    panic(err)\n\t}\n\nThere are some complexities around ensuring clients stop cleanly, but also in a\ntimely manner. See [graceful shutdown] for more details on River's stop modes.\n\n# Inserting jobs\n\n[`Client.InsertTx`] is used in conjunction with an instance of job args to\ninsert a job to work on a transaction:\n\n\t_, err = riverClient.InsertTx(ctx, tx, SortArgs{\n\t    Strings: []string{\n\t        \"whale\", \"tiger\", \"bear\",\n\t    },\n\t}, nil)\n\n\tif err != nil {\n\t    panic(err)\n\t}\n\nSee the [`InsertAndWork` example] for complete code.\n\n# Other features\n\n  - [Batch job insertion] for efficiently inserting many jobs at once using\n    Postgres `COPY FROM`.\n\n  - [Cancelling jobs] from inside a work function.\n\n  - [Error and panic handling].\n\n  - [Multiple queues] to better guarantee job throughput, worker availability,\n    and isolation between components.\n\n  - [Periodic and cron jobs].\n\n  - [Scheduled jobs] that run automatically at their scheduled time in the\n    future.\n\n  - [Snoozing jobs] from inside a work function.\n\n  - [Subscriptions] to queue activity and statistics, providing easy hooks for\n    telemetry like logging and metrics.\n\n  - [Test helpers] to verify that jobs are inserted as expected.\n\n  - [Transactional job completion] to guarantee job completion commits with\n    other changes in a transaction.\n\n  - [Unique jobs] by args, period, queue, and state.\n\n  - [Web UI] for inspecting and interacting with jobs and queues.\n\n  - [Work functions] for simplified worker implementation.\n\n## Cross language enqueueing\n\nRiver supports inserting jobs in some non-Go languages which are then worked by Go implementations. This may be desirable in performance sensitive cases so that jobs can take advantage of Go's fast runtime.\n\n  - [Inserting jobs from Python].\n  - [Inserting jobs from Ruby].\n\n# Development\n\nSee [developing River].\n\n[`Client`]: https://pkg.go.dev/github.com/riverqueue/river#Client\n[`Client.InsertTx`]: https://pkg.go.dev/github.com/riverqueue/river#Client.InsertTx\n[`InsertAndWork` example]: https://pkg.go.dev/github.com/riverqueue/river#example-package-InsertAndWork\n[`JobArgs`]: https://pkg.go.dev/github.com/riverqueue/river#JobArgs\n[`Worker`]: https://pkg.go.dev/github.com/riverqueue/river#Worker\n[Batch job insertion]: https://riverqueue.com/docs/batch-job-insertion\n[Cancelling jobs]: https://riverqueue.com/docs/cancelling-jobs\n[Error and panic handling]: https://riverqueue.com/docs/error-handling\n[Inserting jobs from Python]: https://riverqueue.com/docs/python\n[Inserting jobs from Ruby]: https://riverqueue.com/docs/ruby\n[Multiple queues]: https://riverqueue.com/docs/multiple-queues\n[Periodic and cron jobs]: https://riverqueue.com/docs/periodic-jobs\n[River UI]: https://github.com/riverqueue/riverui\n[Scheduled jobs]: https://riverqueue.com/docs/scheduled-jobs\n[Snoozing jobs]: https://riverqueue.com/docs/snoozing-jobs\n[Subscriptions]: https://riverqueue.com/docs/subscriptions\n[Test helpers]: https://riverqueue.com/docs/testing\n[Transactional job completion]: https://riverqueue.com/docs/transactional-job-completion\n[Unique jobs]: https://riverqueue.com/docs/unique-jobs\n[Web UI]: https://github.com/riverqueue/riverui\n[Work functions]: https://riverqueue.com/docs/work-functions\n[docs]: https://riverqueue.com/docs\n[developing River]: https://github.com/riverqueue/river/blob/master/docs/development.md\n[driver]: https://riverqueue.com/docs/database-drivers\n[godoc]: https://pkg.go.dev/github.com/riverqueue/river\n[graceful shutdown]: https://riverqueue.com/docs/graceful-shutdown\n[homepage]: https://riverqueue.com\n[maintenance services]: https://riverqueue.com/docs/maintenance-services\n[transactional enqueueing]: https://riverqueue.com/docs/transactional-enqueueing\n*/\npackage river\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "driver_test.go",
          "type": "blob",
          "size": 5.9287109375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/stdlib\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest/riverdrivertest\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverdatabasesql\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc TestDriverDatabaseSQL(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\tstdPool := stdlib.OpenDBFromPool(dbPool)\n\tt.Cleanup(func() { require.NoError(t, stdPool.Close()) })\n\n\triverdrivertest.Exercise(ctx, t,\n\t\tfunc(ctx context.Context, t *testing.T) riverdriver.Driver[*sql.Tx] {\n\t\t\tt.Helper()\n\n\t\t\treturn riverdatabasesql.New(stdPool)\n\t\t},\n\t\tfunc(ctx context.Context, t *testing.T) riverdriver.Executor {\n\t\t\tt.Helper()\n\n\t\t\ttx, err := stdPool.BeginTx(ctx, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\tt.Cleanup(func() { _ = tx.Rollback() })\n\n\t\t\treturn riverdatabasesql.New(nil).UnwrapExecutor(tx)\n\t\t})\n}\n\nfunc TestDriverRiverPgxV5(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\triverdrivertest.Exercise(ctx, t,\n\t\tfunc(ctx context.Context, t *testing.T) riverdriver.Driver[pgx.Tx] {\n\t\t\tt.Helper()\n\n\t\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\t\t\treturn riverpgxv5.New(dbPool)\n\t\t},\n\t\tfunc(ctx context.Context, t *testing.T) riverdriver.Executor {\n\t\t\tt.Helper()\n\n\t\t\ttx := riverinternaltest.TestTx(ctx, t)\n\t\t\treturn riverpgxv5.New(nil).UnwrapExecutor(tx)\n\t\t})\n}\n\nfunc BenchmarkDriverRiverPgxV5_Executor(b *testing.B) {\n\tconst (\n\t\tclientID = \"test-client-id\"\n\t\ttimeout  = 5 * time.Minute\n\t)\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetupPool := func(b *testing.B) (riverdriver.Executor, *testBundle) {\n\t\tb.Helper()\n\n\t\tdriver := riverpgxv5.New(riverinternaltest.TestDB(ctx, b))\n\n\t\tb.ResetTimer()\n\n\t\treturn driver.GetExecutor(), &testBundle{}\n\t}\n\n\tsetupTx := func(b *testing.B) (riverdriver.Executor, *testBundle) {\n\t\tb.Helper()\n\n\t\tdriver := riverpgxv5.New(nil)\n\t\ttx := riverinternaltest.TestTx(ctx, b)\n\n\t\tb.ResetTimer()\n\n\t\treturn driver.UnwrapExecutor(tx), &testBundle{}\n\t}\n\n\tmakeInsertParams := func() []*riverdriver.JobInsertFastParams {\n\t\treturn []*riverdriver.JobInsertFastParams{{\n\t\t\tEncodedArgs: []byte(`{}`),\n\t\t\tKind:        \"fake_job\",\n\t\t\tMaxAttempts: rivercommon.MaxAttemptsDefault,\n\t\t\tMetadata:    []byte(`{}`),\n\t\t\tPriority:    rivercommon.PriorityDefault,\n\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\tScheduledAt: nil,\n\t\t\tState:       rivertype.JobStateAvailable,\n\t\t}}\n\t}\n\n\tb.Run(\"JobInsert_Sequential\", func(b *testing.B) {\n\t\tctx, cancel := context.WithTimeout(ctx, timeout)\n\t\tdefer cancel()\n\n\t\texec, _ := setupTx(b)\n\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tif _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}\n\t})\n\n\tb.Run(\"JobInsert_Parallel\", func(b *testing.B) {\n\t\tctx, cancel := context.WithTimeout(ctx, timeout)\n\t\tdefer cancel()\n\n\t\texec, _ := setupPool(b)\n\n\t\tb.RunParallel(func(pb *testing.PB) {\n\t\t\ti := 0\n\t\t\tfor pb.Next() {\n\t\t\t\tif _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\t\t})\n\t})\n\n\tb.Run(\"JobGetAvailable_100_Sequential\", func(b *testing.B) {\n\t\tctx, cancel := context.WithTimeout(ctx, timeout)\n\t\tdefer cancel()\n\n\t\texec, _ := setupTx(b)\n\n\t\tfor i := 0; i < b.N*100; i++ {\n\t\t\tif _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tb.ResetTimer()\n\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tif _, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{\n\t\t\t\tAttemptedBy: clientID,\n\t\t\t\tMax:         100,\n\t\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\t}); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}\n\t})\n\n\tb.Run(\"JobGetAvailable_100_Parallel\", func(b *testing.B) {\n\t\tctx, cancel := context.WithTimeout(ctx, timeout)\n\t\tdefer cancel()\n\n\t\texec, _ := setupPool(b)\n\n\t\tfor i := 0; i < b.N*100*runtime.NumCPU(); i++ {\n\t\t\tif _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tb.ResetTimer()\n\n\t\tb.RunParallel(func(pb *testing.PB) {\n\t\t\tfor pb.Next() {\n\t\t\t\tif _, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{\n\t\t\t\t\tAttemptedBy: clientID,\n\t\t\t\t\tMax:         100,\n\t\t\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\t\t}); err != nil {\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc BenchmarkDriverRiverPgxV5Insert(b *testing.B) {\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texec riverdriver.Executor\n\t\ttx   pgx.Tx\n\t}\n\n\tsetup := func(b *testing.B) (*riverpgxv5.Driver, *testBundle) {\n\t\tb.Helper()\n\n\t\tvar (\n\t\t\tdriver = riverpgxv5.New(nil)\n\t\t\ttx     = riverinternaltest.TestTx(ctx, b)\n\t\t)\n\n\t\tbundle := &testBundle{\n\t\t\texec: driver.UnwrapExecutor(tx),\n\t\t\ttx:   tx,\n\t\t}\n\n\t\treturn driver, bundle\n\t}\n\n\tb.Run(\"InsertFastMany\", func(b *testing.B) {\n\t\t_, bundle := setup(b)\n\n\t\tfor n := 0; n < b.N; n++ {\n\t\t\t_, err := bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{\n\t\t\t\tEncodedArgs: []byte(`{\"encoded\": \"args\"}`),\n\t\t\t\tKind:        \"test_kind\",\n\t\t\t\tMaxAttempts: rivercommon.MaxAttemptsDefault,\n\t\t\t\tPriority:    rivercommon.PriorityDefault,\n\t\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\t\tState:       rivertype.JobStateAvailable,\n\t\t\t}})\n\t\t\trequire.NoError(b, err)\n\t\t}\n\t})\n\n\tb.Run(\"InsertFastMany_WithUnique\", func(b *testing.B) {\n\t\t_, bundle := setup(b)\n\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_, err := bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{\n\t\t\t\tEncodedArgs:  []byte(`{\"encoded\": \"args\"}`),\n\t\t\t\tKind:         \"test_kind\",\n\t\t\t\tMaxAttempts:  rivercommon.MaxAttemptsDefault,\n\t\t\t\tPriority:     rivercommon.PriorityDefault,\n\t\t\t\tQueue:        rivercommon.QueueDefault,\n\t\t\t\tState:        rivertype.JobStateAvailable,\n\t\t\t\tUniqueKey:    []byte(\"test_unique_key_\" + strconv.Itoa(i)),\n\t\t\t\tUniqueStates: 0xFB,\n\t\t\t}})\n\t\t\trequire.NoError(b, err)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "error_handler.go",
          "type": "blob",
          "size": 1.0439453125,
          "content": "package river\n\nimport (\n\t\"context\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// ErrorHandler provides an interface that will be invoked in case of an error\n// or panic occurring in the job. This is often useful for logging and exception\n// tracking, but can also be used to customize retry behavior.\ntype ErrorHandler interface {\n\t// HandleError is invoked in case of an error occurring in a job.\n\t//\n\t// Context is descended from the one used to start the River client that\n\t// worked the job.\n\tHandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult\n\n\t// HandlePanic is invoked in case of a panic occurring in a job.\n\t//\n\t// Context is descended from the one used to start the River client that\n\t// worked the job.\n\tHandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult\n}\n\ntype ErrorHandlerResult struct {\n\t// SetCancelled can be set to true to fail the job immediately and\n\t// permanently. By default it'll continue to follow the configured retry\n\t// schedule.\n\tSetCancelled bool\n}\n"
        },
        {
          "name": "event.go",
          "type": "blob",
          "size": 2.935546875,
          "content": "package river\n\nimport (\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/jobstats\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// EventKind is a kind of event to subscribe to from a client.\ntype EventKind string\n\nconst (\n\t// EventKindJobCancelled occurs when a job is cancelled.\n\tEventKindJobCancelled EventKind = \"job_cancelled\"\n\n\t// EventKindJobCompleted occurs when a job is completed.\n\tEventKindJobCompleted EventKind = \"job_completed\"\n\n\t// EventKindJobFailed occurs when a job fails. Occurs both when a job fails\n\t// and will be retried and when a job fails for the last time and will be\n\t// discarded. Callers can use job fields like `Attempt` and `State` to\n\t// differentiate each type of occurrence.\n\tEventKindJobFailed EventKind = \"job_failed\"\n\n\t// EventKindJobSnoozed occurs when a job is snoozed.\n\tEventKindJobSnoozed EventKind = \"job_snoozed\"\n\n\t// EventKindQueuePaused occurs when a queue is paused.\n\tEventKindQueuePaused EventKind = \"queue_paused\"\n\n\t// EventKindQueueResumed occurs when a queue is resumed.\n\tEventKindQueueResumed EventKind = \"queue_resumed\"\n)\n\n// All known event kinds, used to validate incoming kinds. This is purposely not\n// exported because end users should have no way of subscribing to all known\n// kinds for forward compatibility reasons.\nvar allKinds = map[EventKind]struct{}{ //nolint:gochecknoglobals\n\tEventKindJobCancelled: {},\n\tEventKindJobCompleted: {},\n\tEventKindJobFailed:    {},\n\tEventKindJobSnoozed:   {},\n\tEventKindQueuePaused:  {},\n\tEventKindQueueResumed: {},\n}\n\n// Event wraps an event that occurred within a River client, like a job being\n// completed.\ntype Event struct {\n\t// Kind is the kind of event. Receivers should read this field and respond\n\t// accordingly. Subscriptions will only receive event kinds that they\n\t// requested when creating a subscription with Subscribe.\n\tKind EventKind\n\n\t// Job contains job-related information.\n\tJob *rivertype.JobRow\n\n\t// JobStats are statistics about the run of a job.\n\tJobStats *JobStatistics\n\n\t// Queue contains queue-related information.\n\tQueue *rivertype.Queue\n}\n\n// JobStatistics contains information about a single execution of a job.\ntype JobStatistics struct {\n\tCompleteDuration  time.Duration // Time it took to set the job completed, discarded, or errored.\n\tQueueWaitDuration time.Duration // Time the job spent waiting in available state before starting execution.\n\tRunDuration       time.Duration // Time job spent running (measured around job worker.)\n}\n\nfunc jobStatisticsFromInternal(stats *jobstats.JobStatistics) *JobStatistics {\n\treturn &JobStatistics{\n\t\tCompleteDuration:  stats.CompleteDuration,\n\t\tQueueWaitDuration: stats.QueueWaitDuration,\n\t\tRunDuration:       stats.RunDuration,\n\t}\n}\n\n// eventSubscription is an active subscription for events being produced by a\n// client, created with Client.Subscribe.\ntype eventSubscription struct {\n\tChan  chan *Event\n\tKinds map[EventKind]struct{}\n}\n\nfunc (s *eventSubscription) ListensFor(kind EventKind) bool {\n\t_, ok := s.Kinds[kind]\n\treturn ok\n}\n"
        },
        {
          "name": "event_test.go",
          "type": "blob",
          "size": 0.513671875,
          "content": "package river\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/jobstats\"\n)\n\nfunc TestJobStatisticsFromInternal(t *testing.T) {\n\tt.Parallel()\n\n\trequire.Equal(t, &JobStatistics{\n\t\tCompleteDuration:  1 * time.Second,\n\t\tQueueWaitDuration: 2 * time.Second,\n\t\tRunDuration:       3 * time.Second,\n\t}, jobStatisticsFromInternal(&jobstats.JobStatistics{\n\t\tCompleteDuration:  1 * time.Second,\n\t\tQueueWaitDuration: 2 * time.Second,\n\t\tRunDuration:       3 * time.Second,\n\t}))\n}\n"
        },
        {
          "name": "example_batch_insert_test.go",
          "type": "blob",
          "size": 2.451171875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype BatchInsertArgs struct{}\n\nfunc (BatchInsertArgs) Kind() string { return \"batch_insert\" }\n\n// BatchInsertWorker is a job worker demonstrating use of custom\n// job-specific insertion options.\ntype BatchInsertWorker struct {\n\triver.WorkerDefaults[BatchInsertArgs]\n}\n\nfunc (w *BatchInsertWorker) Work(ctx context.Context, job *river.Job[BatchInsertArgs]) error {\n\tfmt.Printf(\"Worked a job\\n\")\n\treturn nil\n}\n\n// Example_batchInsert demonstrates how many jobs can be inserted for work as\n// part of a single operation.\nfunc Example_batchInsert() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &BatchInsertWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\tresults, err := riverClient.InsertMany(ctx, []river.InsertManyParams{\n\t\t{Args: BatchInsertArgs{}},\n\t\t{Args: BatchInsertArgs{}},\n\t\t{Args: BatchInsertArgs{}},\n\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},\n\t\t{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 4}},\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfmt.Printf(\"Inserted %d jobs\\n\", len(results))\n\n\twaitForNJobs(subscribeChan, 5)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Inserted 5 jobs\n\t// Worked a job\n\t// Worked a job\n\t// Worked a job\n\t// Worked a job\n\t// Worked a job\n}\n"
        },
        {
          "name": "example_client_from_context_dbsql_test.go",
          "type": "blob",
          "size": 2.4287109375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t_ \"github.com/jackc/pgx/v5/stdlib\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverdatabasesql\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype ContextClientSQLArgs struct{}\n\nfunc (args ContextClientSQLArgs) Kind() string { return \"ContextClientSQLWorker\" }\n\ntype ContextClientSQLWorker struct {\n\triver.WorkerDefaults[ContextClientSQLArgs]\n}\n\nfunc (w *ContextClientSQLWorker) Work(ctx context.Context, job *river.Job[ContextClientSQLArgs]) error {\n\tclient := river.ClientFromContext[*sql.Tx](ctx)\n\tif client == nil {\n\t\tfmt.Println(\"client not found in context\")\n\t\treturn errors.New(\"client not found in context\")\n\t}\n\n\tfmt.Printf(\"client found in context, id=%s\\n\", client.ID())\n\treturn nil\n}\n\n// ExampleClientFromContext_databaseSQL demonstrates how to extract the River\n// client from the worker context when using the [database/sql] driver.\n// ([github.com/riverqueue/river/riverdriver/riverdatabasesql]).\nfunc ExampleClientFromContext_databaseSQL() {\n\tctx := context.Background()\n\n\tconfig := riverinternaltest.DatabaseConfig(\"river_test_example\")\n\tdb, err := sql.Open(\"pgx\", config.ConnString())\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer db.Close()\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &ContextClientSQLWorker{})\n\n\triverClient, err := river.NewClient(riverdatabasesql.New(db), &river.Config{\n\t\tID:     \"ClientFromContextClientSQL\",\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tFetchCooldown:     10 * time.Millisecond,\n\t\tFetchPollInterval: 10 * time.Millisecond,\n\t\tTestOnly:          true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:           workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\tif _, err := riverClient.Insert(ctx, ContextClientSQLArgs{}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// client found in context, id=ClientFromContextClientSQL\n}\n"
        },
        {
          "name": "example_client_from_context_test.go",
          "type": "blob",
          "size": 2.4091796875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype ContextClientArgs struct{}\n\nfunc (args ContextClientArgs) Kind() string { return \"ContextClientWorker\" }\n\ntype ContextClientWorker struct {\n\triver.WorkerDefaults[ContextClientArgs]\n}\n\nfunc (w *ContextClientWorker) Work(ctx context.Context, job *river.Job[ContextClientArgs]) error {\n\tclient := river.ClientFromContext[pgx.Tx](ctx)\n\tif client == nil {\n\t\tfmt.Println(\"client not found in context\")\n\t\treturn errors.New(\"client not found in context\")\n\t}\n\n\tfmt.Printf(\"client found in context, id=%s\\n\", client.ID())\n\treturn nil\n}\n\n// ExampleClientFromContext_pgx demonstrates how to extract the River client\n// from the worker context when using the pgx/v5 driver.\n// ([github.com/riverqueue/river/riverdriver/riverpgxv5]).\nfunc ExampleClientFromContext_pgx() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &ContextClientWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tID:     \"ClientFromContextClient\",\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\tif _, err = riverClient.Insert(ctx, ContextClientArgs{}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// client found in context, id=ClientFromContextClient\n}\n"
        },
        {
          "name": "example_complete_job_within_tx_test.go",
          "type": "blob",
          "size": 2.9951171875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype TransactionalArgs struct{}\n\nfunc (TransactionalArgs) Kind() string { return \"transactional_worker\" }\n\n// TransactionalWorker is a job worker which runs an operation on the database\n// and transactionally completes the current job.\n//\n// While this example is simplified, any operations could be performed within\n// the transaction such as inserting additional jobs or manipulating other data.\ntype TransactionalWorker struct {\n\triver.WorkerDefaults[TransactionalArgs]\n\tdbPool *pgxpool.Pool\n}\n\nfunc (w *TransactionalWorker) Work(ctx context.Context, job *river.Job[TransactionalArgs]) error {\n\ttx, err := w.dbPool.Begin(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback(ctx)\n\n\tvar result int\n\tif err := tx.QueryRow(ctx, \"SELECT 1\").Scan(&result); err != nil {\n\t\treturn err\n\t}\n\n\t// The function needs to know the type of the database driver in use by the\n\t// Client, but the other generic parameters can be inferred.\n\tjobAfter, err := river.JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfmt.Printf(\"Transitioned TransactionalWorker job from %q to %q\\n\", job.State, jobAfter.State)\n\n\tif err = tx.Commit(ctx); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Example_completeJobWithinTx demonstrates how to transactionally complete\n// a job alongside other database changes being made.\nfunc Example_completeJobWithinTx() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &TransactionalWorker{dbPool: dbPool})\n\triver.AddWorker(workers, &SortWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif _, err = riverClient.Insert(ctx, TransactionalArgs{}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Transitioned TransactionalWorker job from \"running\" to \"completed\"\n}\n"
        },
        {
          "name": "example_cron_job_test.go",
          "type": "blob",
          "size": 2.59375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/robfig/cron/v3\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype CronJobArgs struct{}\n\n// Kind is the unique string name for this job.\nfunc (CronJobArgs) Kind() string { return \"cron\" }\n\n// CronJobWorker is a job worker for sorting strings.\ntype CronJobWorker struct {\n\triver.WorkerDefaults[CronJobArgs]\n}\n\nfunc (w *CronJobWorker) Work(ctx context.Context, job *river.Job[CronJobArgs]) error {\n\tfmt.Printf(\"This job will run once immediately then every hour on the half hour\\n\")\n\treturn nil\n}\n\n// Example_cronJob demonstrates how to create a cron job with a more complex\n// schedule using a third party cron package to parse more elaborate crontab\n// syntax.\nfunc Example_cronJob() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &CronJobWorker{})\n\n\tschedule, err := cron.ParseStandard(\"30 * * * *\") // every hour on the half hour\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tPeriodicJobs: []*river.PeriodicJob{\n\t\t\triver.NewPeriodicJob(\n\t\t\t\tschedule,\n\t\t\t\tfunc() (river.JobArgs, *river.InsertOpts) {\n\t\t\t\t\treturn CronJobArgs{}, nil\n\t\t\t\t},\n\t\t\t\t&river.PeriodicJobOpts{RunOnStart: true},\n\t\t\t),\n\t\t},\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\t// There's no need to explicitly insert a periodic job. One will be inserted\n\t// (and worked soon after) as the client starts up.\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// This job will run once immediately then every hour on the half hour\n}\n"
        },
        {
          "name": "example_custom_insert_opts_test.go",
          "type": "blob",
          "size": 3.4013671875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype AlwaysHighPriorityArgs struct{}\n\nfunc (AlwaysHighPriorityArgs) Kind() string { return \"always_high_priority\" }\n\n// InsertOpts returns custom insert options that every job of this type will\n// inherit by default.\nfunc (AlwaysHighPriorityArgs) InsertOpts() river.InsertOpts {\n\treturn river.InsertOpts{\n\t\tQueue: \"high_priority\",\n\t}\n}\n\n// AlwaysHighPriorityWorker is a job worker demonstrating use of custom\n// job-specific insertion options.\ntype AlwaysHighPriorityWorker struct {\n\triver.WorkerDefaults[AlwaysHighPriorityArgs]\n}\n\nfunc (w *AlwaysHighPriorityWorker) Work(ctx context.Context, job *river.Job[AlwaysHighPriorityArgs]) error {\n\tfmt.Printf(\"Ran in queue: %s\\n\", job.Queue)\n\treturn nil\n}\n\ntype SometimesHighPriorityArgs struct{}\n\nfunc (SometimesHighPriorityArgs) Kind() string { return \"sometimes_high_priority\" }\n\n// SometimesHighPriorityWorker is a job worker that's made high-priority\n// sometimes through the use of options at insertion time.\ntype SometimesHighPriorityWorker struct {\n\triver.WorkerDefaults[SometimesHighPriorityArgs]\n}\n\nfunc (w *SometimesHighPriorityWorker) Work(ctx context.Context, job *river.Job[SometimesHighPriorityArgs]) error {\n\tfmt.Printf(\"Ran in queue: %s\\n\", job.Queue)\n\treturn nil\n}\n\n// Example_customInsertOpts demonstrates the use of a job with custom\n// job-specific insertion options.\nfunc Example_customInsertOpts() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &AlwaysHighPriorityWorker{})\n\triver.AddWorker(workers, &SometimesHighPriorityWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t\t\"high_priority\":    {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// This job always runs in the high-priority queue because its job-specific\n\t// options on the struct above dictate that it will.\n\t_, err = riverClient.Insert(ctx, AlwaysHighPriorityArgs{}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// This job will run in the high-priority queue because of the options given\n\t// at insertion time.\n\t_, err = riverClient.Insert(ctx, SometimesHighPriorityArgs{}, &river.InsertOpts{\n\t\tQueue: \"high_priority\",\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 2)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Ran in queue: high_priority\n\t// Ran in queue: high_priority\n}\n"
        },
        {
          "name": "example_error_handler_test.go",
          "type": "blob",
          "size": 3.326171875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\ntype CustomErrorHandler struct{}\n\nfunc (*CustomErrorHandler) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *river.ErrorHandlerResult {\n\tfmt.Printf(\"Job errored with: %s\\n\", err)\n\treturn nil\n}\n\nfunc (*CustomErrorHandler) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *river.ErrorHandlerResult {\n\tfmt.Printf(\"Job panicked with: %v\\n\", panicVal)\n\n\t// Either function can also set the job to be immediately cancelled.\n\treturn &river.ErrorHandlerResult{SetCancelled: true}\n}\n\ntype ErroringArgs struct {\n\tShouldError bool\n\tShouldPanic bool\n}\n\nfunc (ErroringArgs) Kind() string { return \"erroring\" }\n\n// Here to make sure our jobs are never accidentally retried which would add\n// additional output and fail the example.\nfunc (ErroringArgs) InsertOpts() river.InsertOpts {\n\treturn river.InsertOpts{MaxAttempts: 1}\n}\n\ntype ErroringWorker struct {\n\triver.WorkerDefaults[ErroringArgs]\n}\n\nfunc (w *ErroringWorker) Work(ctx context.Context, job *river.Job[ErroringArgs]) error {\n\tswitch {\n\tcase job.Args.ShouldError:\n\t\treturn errors.New(\"this job errored\")\n\tcase job.Args.ShouldPanic:\n\t\tpanic(\"this job panicked\")\n\t}\n\treturn nil\n}\n\n// Example_errorHandler demonstrates how to use the ErrorHandler interface for\n// custom application telemetry.\nfunc Example_errorHandler() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &ErroringWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tErrorHandler: &CustomErrorHandler{},\n\t\tLogger:       slog.New(&slogutil.SlogMessageOnlyHandler{Level: 9}), // Suppress logging so example output is cleaner (9 > slog.LevelError).\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled, river.EventKindJobFailed)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif _, err = riverClient.Insert(ctx, ErroringArgs{ShouldError: true}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Wait for the first job before inserting another to guarantee test output\n\t// is ordered correctly.\n\twaitForNJobs(subscribeChan, 1)\n\n\tif _, err = riverClient.Insert(ctx, ErroringArgs{ShouldPanic: true}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Job errored with: this job errored\n\t// Job panicked with: this job panicked\n}\n"
        },
        {
          "name": "example_graceful_shutdown_test.go",
          "type": "blob",
          "size": 5.6630859375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype WaitsForCancelOnlyArgs struct{}\n\nfunc (WaitsForCancelOnlyArgs) Kind() string { return \"waits_for_cancel_only\" }\n\n// WaitsForCancelOnlyWorker is a worker that will never finish jobs until its\n// context is cancelled.\ntype WaitsForCancelOnlyWorker struct {\n\triver.WorkerDefaults[WaitsForCancelOnlyArgs]\n\tjobStarted chan struct{}\n}\n\nfunc (w *WaitsForCancelOnlyWorker) Work(ctx context.Context, job *river.Job[WaitsForCancelOnlyArgs]) error {\n\tfmt.Printf(\"Working job that doesn't finish until cancelled\\n\")\n\tclose(w.jobStarted)\n\n\t<-ctx.Done()\n\tfmt.Printf(\"Job cancelled\\n\")\n\n\t// In the event of cancellation, an error should be returned so that the job\n\t// goes back in the retry queue.\n\treturn ctx.Err()\n}\n\n// Example_gracefulShutdown demonstrates a realistic-looking stop loop for\n// River. It listens for SIGINT/SIGTERM (like might be received by a Ctrl+C\n// locally or on a platform like Heroku to stop a process) and when received,\n// tries a soft stop that waits for work to finish. If it doesn't finish in\n// time, a second SIGINT/SIGTERM will initiate a hard stop that cancels all jobs\n// using context cancellation. A third will give up on the stop procedure and\n// exit uncleanly.\nfunc Example_gracefulShutdown() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tjobStarted := make(chan struct{})\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &WaitsForCancelOnlyWorker{jobStarted: jobStarted})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t_, err = riverClient.Insert(ctx, WaitsForCancelOnlyArgs{}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\tsigintOrTerm := make(chan os.Signal, 1)\n\tsignal.Notify(sigintOrTerm, syscall.SIGINT, syscall.SIGTERM)\n\n\t// This is meant to be a realistic-looking stop goroutine that might go in a\n\t// real program. It waits for SIGINT/SIGTERM and when received, tries to stop\n\t// gracefully by allowing a chance for jobs to finish. But if that isn't\n\t// working, a second SIGINT/SIGTERM will tell it to terminate with prejudice and\n\t// it'll issue a hard stop that cancels the context of all active jobs. In\n\t// case that doesn't work, a third SIGINT/SIGTERM ignores River's stop procedure\n\t// completely and exits uncleanly.\n\tgo func() {\n\t\t<-sigintOrTerm\n\t\tfmt.Printf(\"Received SIGINT/SIGTERM; initiating soft stop (try to wait for jobs to finish)\\n\")\n\n\t\tsoftStopCtx, softStopCtxCancel := context.WithTimeout(ctx, 10*time.Second)\n\t\tdefer softStopCtxCancel()\n\n\t\tgo func() {\n\t\t\tselect {\n\t\t\tcase <-sigintOrTerm:\n\t\t\t\tfmt.Printf(\"Received SIGINT/SIGTERM again; initiating hard stop (cancel everything)\\n\")\n\t\t\t\tsoftStopCtxCancel()\n\t\t\tcase <-softStopCtx.Done():\n\t\t\t\tfmt.Printf(\"Soft stop timeout; initiating hard stop (cancel everything)\\n\")\n\t\t\t}\n\t\t}()\n\n\t\terr := riverClient.Stop(softStopCtx)\n\t\tif err != nil && !errors.Is(err, context.DeadlineExceeded) && !errors.Is(err, context.Canceled) {\n\t\t\tpanic(err)\n\t\t}\n\t\tif err == nil {\n\t\t\tfmt.Printf(\"Soft stop succeeded\\n\")\n\t\t\treturn\n\t\t}\n\n\t\thardStopCtx, hardStopCtxCancel := context.WithTimeout(ctx, 10*time.Second)\n\t\tdefer hardStopCtxCancel()\n\n\t\t// As long as all jobs respect context cancellation, StopAndCancel will\n\t\t// always work. However, in the case of a bug where a job blocks despite\n\t\t// being cancelled, it may be necessary to either ignore River's stop\n\t\t// result (what's shown here) or have a supervisor kill the process.\n\t\terr = riverClient.StopAndCancel(hardStopCtx)\n\t\tif err != nil && errors.Is(err, context.DeadlineExceeded) {\n\t\t\tfmt.Printf(\"Hard stop timeout; ignoring stop procedure and exiting unsafely\\n\")\n\t\t} else if err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\t// hard stop succeeded\n\t}()\n\n\t// Make sure our job starts being worked before doing anything else.\n\t<-jobStarted\n\n\t// Cheat a little by sending a SIGTERM manually for the purpose of this\n\t// example (normally this will be sent by user or supervisory process). The\n\t// first SIGTERM tries a soft stop in which jobs are given a chance to\n\t// finish up.\n\tsigintOrTerm <- syscall.SIGTERM\n\n\t// The soft stop will never work in this example because our job only\n\t// respects context cancellation, but wait a short amount of time to give it\n\t// a chance. After it elapses, send another SIGTERM to initiate a hard stop.\n\tselect {\n\tcase <-riverClient.Stopped():\n\t\t// Will never be reached in this example because our job will only ever\n\t\t// finish on context cancellation.\n\t\tfmt.Printf(\"Soft stop succeeded\\n\")\n\n\tcase <-time.After(100 * time.Millisecond):\n\t\tsigintOrTerm <- syscall.SIGTERM\n\t\t<-riverClient.Stopped()\n\t}\n\n\t// Output:\n\t// Working job that doesn't finish until cancelled\n\t// Received SIGINT/SIGTERM; initiating soft stop (try to wait for jobs to finish)\n\t// Received SIGINT/SIGTERM again; initiating hard stop (cancel everything)\n\t// Job cancelled\n\t// jobExecutor: Job errored\n}\n"
        },
        {
          "name": "example_insert_and_work_test.go",
          "type": "blob",
          "size": 2.638671875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"sort\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype SortArgs struct {\n\t// Strings is a slice of strings to sort.\n\tStrings []string `json:\"strings\"`\n}\n\nfunc (SortArgs) Kind() string { return \"sort\" }\n\ntype SortWorker struct {\n\triver.WorkerDefaults[SortArgs]\n}\n\nfunc (w *SortWorker) Work(ctx context.Context, job *river.Job[SortArgs]) error {\n\tsort.Strings(job.Args.Strings)\n\tfmt.Printf(\"Sorted strings: %+v\\n\", job.Args.Strings)\n\treturn nil\n}\n\n// Example_insertAndWork demonstrates how to register job workers, start a\n// client, and insert a job on it to be worked.\nfunc Example_insertAndWork() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &SortWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Start a transaction to insert a job. It's also possible to insert a job\n\t// outside a transaction, but this usage is recommended to ensure that all\n\t// data a job needs to run is available by the time it starts. Because of\n\t// snapshot visibility guarantees across transactions, the job will not be\n\t// worked until the transaction has committed.\n\ttx, err := dbPool.Begin(ctx)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer tx.Rollback(ctx)\n\n\t_, err = riverClient.InsertTx(ctx, tx, SortArgs{\n\t\tStrings: []string{\n\t\t\t\"whale\", \"tiger\", \"bear\",\n\t\t},\n\t}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif err := tx.Commit(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Sorted strings: [bear tiger whale]\n}\n"
        },
        {
          "name": "example_job_cancel_from_client_test.go",
          "type": "blob",
          "size": 2.6962890625,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype SleepingArgs struct{}\n\nfunc (args SleepingArgs) Kind() string { return \"SleepingWorker\" }\n\ntype SleepingWorker struct {\n\triver.WorkerDefaults[CancellingArgs]\n\tjobChan chan int64\n}\n\nfunc (w *SleepingWorker) Work(ctx context.Context, job *river.Job[CancellingArgs]) error {\n\tw.jobChan <- job.ID\n\tselect {\n\tcase <-ctx.Done():\n\tcase <-time.After(5 * time.Second):\n\t\treturn errors.New(\"sleeping worker timed out\")\n\t}\n\treturn ctx.Err()\n}\n\n// Example_jobCancelFromClient demonstrates how to permanently cancel a job from\n// any Client using JobCancel.\nfunc Example_jobCancelFromClient() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tjobChan := make(chan int64)\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &SleepingWorker{jobChan: jobChan})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\tinsertRes, err := riverClient.Insert(ctx, CancellingArgs{ShouldCancel: true}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tselect {\n\tcase <-jobChan:\n\tcase <-time.After(2 * time.Second):\n\t\tpanic(\"no jobChan signal received\")\n\t}\n\n\t// There is presently no way to wait for the client to be 100% ready, so we\n\t// sleep for a bit to give it time to start up. This is only needed in this\n\t// example because we need the notifier to be ready for it to receive the\n\t// cancellation signal.\n\ttime.Sleep(500 * time.Millisecond)\n\n\tif _, err = riverClient.JobCancel(ctx, insertRes.Job.ID); err != nil {\n\t\tpanic(err)\n\t}\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// jobExecutor: job cancelled remotely\n}\n"
        },
        {
          "name": "example_job_cancel_test.go",
          "type": "blob",
          "size": 2.1572265625,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype CancellingArgs struct {\n\tShouldCancel bool\n}\n\nfunc (args CancellingArgs) Kind() string { return \"Cancelling\" }\n\ntype CancellingWorker struct {\n\triver.WorkerDefaults[CancellingArgs]\n}\n\nfunc (w *CancellingWorker) Work(ctx context.Context, job *river.Job[CancellingArgs]) error {\n\tif job.Args.ShouldCancel {\n\t\tfmt.Println(\"cancelling job\")\n\t\treturn river.JobCancel(errors.New(\"this wrapped error message will be persisted to DB\"))\n\t}\n\treturn nil\n}\n\n// Example_jobCancel demonstrates how to permanently cancel a job from within\n// Work using JobCancel.\nfunc Example_jobCancel() { //nolint:dupl\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &CancellingWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Not strictly needed, but used to help this test wait until job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\tif _, err = riverClient.Insert(ctx, CancellingArgs{ShouldCancel: true}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// cancelling job\n}\n"
        },
        {
          "name": "example_job_snooze_test.go",
          "type": "blob",
          "size": 2.3076171875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype SnoozingArgs struct {\n\tShouldSnooze bool\n}\n\nfunc (args SnoozingArgs) Kind() string { return \"Snoozing\" }\n\ntype SnoozingWorker struct {\n\triver.WorkerDefaults[SnoozingArgs]\n}\n\nfunc (w *SnoozingWorker) Work(ctx context.Context, job *river.Job[SnoozingArgs]) error {\n\tif job.Args.ShouldSnooze {\n\t\tfmt.Println(\"snoozing job for 5 minutes\")\n\t\treturn river.JobSnooze(5 * time.Minute)\n\t}\n\treturn nil\n}\n\n// Example_jobSnooze demonstrates how to snooze a job from within Work using\n// JobSnooze. The job will be run again after 5 minutes and the snooze attempt\n// will increment the job's max attempts, ensuring that one can snooze as many\n// times as desired.\nfunc Example_jobSnooze() { //nolint:dupl\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &SnoozingWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// The subscription bits are not needed in real usage, but are used to make\n\t// sure the test waits until the job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobSnoozed)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\tif _, err = riverClient.Insert(ctx, SnoozingArgs{ShouldSnooze: true}, nil); err != nil {\n\t\tpanic(err)\n\t}\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// snoozing job for 5 minutes\n}\n"
        },
        {
          "name": "example_periodic_job_test.go",
          "type": "blob",
          "size": 2.818359375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype PeriodicJobArgs struct{}\n\n// Kind is the unique string name for this job.\nfunc (PeriodicJobArgs) Kind() string { return \"periodic\" }\n\n// PeriodicJobWorker is a job worker for sorting strings.\ntype PeriodicJobWorker struct {\n\triver.WorkerDefaults[PeriodicJobArgs]\n}\n\nfunc (w *PeriodicJobWorker) Work(ctx context.Context, job *river.Job[PeriodicJobArgs]) error {\n\tfmt.Printf(\"This job will run once immediately then approximately once every 15 minutes\\n\")\n\treturn nil\n}\n\n// Example_periodicJob demonstrates the use of a periodic job.\nfunc Example_periodicJob() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &PeriodicJobWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tPeriodicJobs: []*river.PeriodicJob{\n\t\t\triver.NewPeriodicJob(\n\t\t\t\triver.PeriodicInterval(15*time.Minute),\n\t\t\t\tfunc() (river.JobArgs, *river.InsertOpts) {\n\t\t\t\t\treturn PeriodicJobArgs{}, nil\n\t\t\t\t},\n\t\t\t\t&river.PeriodicJobOpts{RunOnStart: true},\n\t\t\t),\n\t\t},\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\t// There's no need to explicitly insert a periodic job. One will be inserted\n\t// (and worked soon after) as the client starts up.\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\t// Periodic jobs can also be configured dynamically after a client has\n\t// already started. Added jobs are scheduled for run immediately.\n\triverClient.PeriodicJobs().Clear()\n\triverClient.PeriodicJobs().Add(\n\t\triver.NewPeriodicJob(\n\t\t\triver.PeriodicInterval(15*time.Minute),\n\t\t\tfunc() (river.JobArgs, *river.InsertOpts) {\n\t\t\t\treturn PeriodicJobArgs{}, nil\n\t\t\t},\n\t\t\tnil,\n\t\t),\n\t)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// This job will run once immediately then approximately once every 15 minutes\n}\n"
        },
        {
          "name": "example_queue_pause_test.go",
          "type": "blob",
          "size": 3.630859375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype ReportingArgs struct{}\n\nfunc (args ReportingArgs) Kind() string { return \"Reporting\" }\n\ntype ReportingWorker struct {\n\triver.WorkerDefaults[ReportingArgs]\n\tjobWorkedCh chan<- string\n}\n\nfunc (w *ReportingWorker) Work(ctx context.Context, job *river.Job[ReportingArgs]) error {\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\tcase w.jobWorkedCh <- job.Queue:\n\t\treturn nil\n\t}\n}\n\n// Example_queuePause demonstrates how to pause queues to prevent them from\n// working new jobs, and later resume them.\nfunc Example_queuePause() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tconst (\n\t\tunreliableQueue = \"unreliable_external_service\"\n\t\treliableQueue   = \"reliable_jobs\"\n\t)\n\n\tworkers := river.NewWorkers()\n\tjobWorkedCh := make(chan string)\n\triver.AddWorker(workers, &ReportingWorker{jobWorkedCh: jobWorkedCh})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\tunreliableQueue: {MaxWorkers: 10},\n\t\t\treliableQueue:   {MaxWorkers: 10},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a queue is paused or unpaused.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindQueuePaused, river.EventKindQueueResumed)\n\tdefer subscribeCancel()\n\n\tfmt.Printf(\"Pausing %s queue\\n\", unreliableQueue)\n\tif err := riverClient.QueuePause(ctx, unreliableQueue, nil); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Wait for queue to be paused:\n\twaitOrTimeout(subscribeChan)\n\n\tfmt.Println(\"Inserting one job each into unreliable and reliable queues\")\n\tif _, err = riverClient.Insert(ctx, ReportingArgs{}, &river.InsertOpts{Queue: unreliableQueue}); err != nil {\n\t\tpanic(err)\n\t}\n\tif _, err = riverClient.Insert(ctx, ReportingArgs{}, &river.InsertOpts{Queue: reliableQueue}); err != nil {\n\t\tpanic(err)\n\t}\n\t// The unreliable queue is paused so its job should get worked yet, while\n\t// reliable queue is not paused so its job should get worked immediately:\n\treceivedQueue := waitOrTimeout(jobWorkedCh)\n\tfmt.Printf(\"Job worked on %s queue\\n\", receivedQueue)\n\n\t// Resume the unreliable queue so it can work the job:\n\tfmt.Printf(\"Resuming %s queue\\n\", unreliableQueue)\n\tif err := riverClient.QueueResume(ctx, unreliableQueue, nil); err != nil {\n\t\tpanic(err)\n\t}\n\treceivedQueue = waitOrTimeout(jobWorkedCh)\n\tfmt.Printf(\"Job worked on %s queue\\n\", receivedQueue)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Pausing unreliable_external_service queue\n\t// Inserting one job each into unreliable and reliable queues\n\t// Job worked on reliable_jobs queue\n\t// Resuming unreliable_external_service queue\n\t// Job worked on unreliable_external_service queue\n}\n\nfunc waitOrTimeout[T any](ch <-chan T) T {\n\tselect {\n\tcase item := <-ch:\n\t\treturn item\n\tcase <-time.After(5 * time.Second):\n\t\tpanic(\"WaitOrTimeout timed out after waiting 5s\")\n\t}\n}\n"
        },
        {
          "name": "example_scheduled_job_test.go",
          "type": "blob",
          "size": 2.130859375,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype ScheduledArgs struct {\n\tMessage string `json:\"message\"`\n}\n\nfunc (ScheduledArgs) Kind() string { return \"scheduled\" }\n\ntype ScheduledWorker struct {\n\triver.WorkerDefaults[ScheduledArgs]\n}\n\nfunc (w *ScheduledWorker) Work(ctx context.Context, job *river.Job[ScheduledArgs]) error {\n\tfmt.Printf(\"Message: %s\\n\", job.Args.Message)\n\treturn nil\n}\n\n// Example_scheduledJob demonstrates how to schedule a job to be worked in the\n// future.\nfunc Example_scheduledJob() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &ScheduledWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t_, err = riverClient.Insert(ctx,\n\t\tScheduledArgs{\n\t\t\tMessage: \"hello from the future\",\n\t\t},\n\t\t&river.InsertOpts{\n\t\t\t// Schedule the job to be worked in three hours.\n\t\t\tScheduledAt: time.Now().Add(3 * time.Hour),\n\t\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Unlike most other examples, we don't wait for the job to be worked since\n\t// doing so would require making the job's scheduled time contrived, and the\n\t// example therefore less realistic/useful.\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n}\n"
        },
        {
          "name": "example_subscription_test.go",
          "type": "blob",
          "size": 3.7578125,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype SubscriptionArgs struct {\n\tCancel bool `json:\"cancel\"`\n\tFail   bool `json:\"fail\"`\n}\n\nfunc (SubscriptionArgs) Kind() string { return \"subscription\" }\n\ntype SubscriptionWorker struct {\n\triver.WorkerDefaults[SubscriptionArgs]\n}\n\nfunc (w *SubscriptionWorker) Work(ctx context.Context, job *river.Job[SubscriptionArgs]) error {\n\tswitch {\n\tcase job.Args.Cancel:\n\t\treturn river.JobCancel(errors.New(\"cancelling job\"))\n\tcase job.Args.Fail:\n\t\treturn errors.New(\"failing job\")\n\t}\n\treturn nil\n}\n\n// Example_subscription demonstrates the use of client subscriptions to receive\n// events containing information about worked jobs.\nfunc Example_subscription() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &SubscriptionWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: 9}), // Suppress logging so example output is cleaner (9 > slog.LevelError).\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Subscribers tell the River client the kinds of events they'd like to receive.\n\tcompletedChan, completedSubscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer completedSubscribeCancel()\n\n\t// Multiple simultaneous subscriptions are allowed.\n\tfailedChan, failedSubscribeCancel := riverClient.Subscribe(river.EventKindJobFailed)\n\tdefer failedSubscribeCancel()\n\n\totherChan, otherSubscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled, river.EventKindJobSnoozed)\n\tdefer otherSubscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Insert one job for each subscription above: one to succeed, one to fail,\n\t// and one that's cancelled that'll arrive on the \"other\" channel.\n\t_, err = riverClient.Insert(ctx, SubscriptionArgs{}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\t_, err = riverClient.Insert(ctx, SubscriptionArgs{Fail: true}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\t_, err = riverClient.Insert(ctx, SubscriptionArgs{Cancel: true}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForJob := func(subscribeChan <-chan *river.Event) {\n\t\tselect {\n\t\tcase event := <-subscribeChan:\n\t\t\tif event == nil {\n\t\t\t\tfmt.Printf(\"Channel is closed\\n\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tfmt.Printf(\"Got job with state: %s\\n\", event.Job.State)\n\t\tcase <-time.After(riversharedtest.WaitTimeout()):\n\t\t\tpanic(\"timed out waiting for job\")\n\t\t}\n\t}\n\n\twaitForJob(completedChan)\n\twaitForJob(failedChan)\n\twaitForJob(otherChan)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Printf(\"Client stopped\\n\")\n\n\t// Try waiting again, but none of these work because stopping the client\n\t// closed all subscription channels automatically.\n\twaitForJob(completedChan)\n\twaitForJob(failedChan)\n\twaitForJob(otherChan)\n\n\t// Output:\n\t// Got job with state: completed\n\t// Got job with state: available\n\t// Got job with state: cancelled\n\t// Client stopped\n\t// Channel is closed\n\t// Channel is closed\n\t// Channel is closed\n}\n"
        },
        {
          "name": "example_unique_job_test.go",
          "type": "blob",
          "size": 3.63671875,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\n// Account represents a minimal account including recent expenditures and a\n// remaining total.\ntype Account struct {\n\tRecentExpenditures int\n\tAccountTotal       int\n}\n\n// Map of account ID -> account.\nvar allAccounts = map[int]Account{ //nolint:gochecknoglobals\n\t1: {RecentExpenditures: 100, AccountTotal: 1_000},\n\t2: {RecentExpenditures: 999, AccountTotal: 1_000},\n}\n\ntype ReconcileAccountArgs struct {\n\tAccountID int `json:\"account_id\"`\n}\n\nfunc (ReconcileAccountArgs) Kind() string { return \"reconcile_account\" }\n\n// InsertOpts returns custom insert options that every job of this type will\n// inherit, including unique options.\nfunc (ReconcileAccountArgs) InsertOpts() river.InsertOpts {\n\treturn river.InsertOpts{\n\t\tUniqueOpts: river.UniqueOpts{\n\t\t\tByArgs:   true,\n\t\t\tByPeriod: 24 * time.Hour,\n\t\t},\n\t}\n}\n\ntype ReconcileAccountWorker struct {\n\triver.WorkerDefaults[ReconcileAccountArgs]\n}\n\nfunc (w *ReconcileAccountWorker) Work(ctx context.Context, job *river.Job[ReconcileAccountArgs]) error {\n\taccount := allAccounts[job.Args.AccountID]\n\n\taccount.AccountTotal -= account.RecentExpenditures\n\taccount.RecentExpenditures = 0\n\n\tfmt.Printf(\"Reconciled account %d; new total: %d\\n\", job.Args.AccountID, account.AccountTotal)\n\n\treturn nil\n}\n\n// Example_uniqueJob demonstrates the use of a job with custom\n// job-specific insertion options.\nfunc Example_uniqueJob() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, &ReconcileAccountWorker{})\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// First job insertion for account 1.\n\t_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 1}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Job is inserted a second time, but it doesn't matter because its unique\n\t// args cause the insertion to be skipped because it's meant to only run\n\t// once per account per 24 hour period.\n\t_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 1}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Cheat a little by waiting for the first job to come back so we can\n\t// guarantee that this example's output comes out in order.\n\twaitForNJobs(subscribeChan, 1)\n\n\t// Because the job is unique ByArgs, another job for account 2 is allowed.\n\t_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 2}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Reconciled account 1; new total: 900\n\t// Reconciled account 2; new total: 1\n}\n"
        },
        {
          "name": "example_work_func_test.go",
          "type": "blob",
          "size": 2.0322265625,
          "content": "package river_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\n\t\"github.com/riverqueue/river\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/util/slogutil\"\n)\n\ntype WorkFuncArgs struct {\n\tMessage string `json:\"message\"`\n}\n\nfunc (WorkFuncArgs) Kind() string { return \"work_func\" }\n\n// Example_workFunc demonstrates the use of river.WorkFunc, which can be used to\n// easily add a worker with only a function instead of having to implement a\n// full worker struct.\nfunc Example_workFunc() {\n\tctx := context.Background()\n\n\tdbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig(\"river_test_example\"))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer dbPool.Close()\n\n\t// Required for the purpose of this test, but not necessary in real usage.\n\tif err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {\n\t\tpanic(err)\n\t}\n\n\tworkers := river.NewWorkers()\n\triver.AddWorker(workers, river.WorkFunc(func(ctx context.Context, job *river.Job[WorkFuncArgs]) error {\n\t\tfmt.Printf(\"Message: %s\", job.Args.Message)\n\t\treturn nil\n\t}))\n\n\triverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{\n\t\tLogger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),\n\t\tQueues: map[string]river.QueueConfig{\n\t\t\triver.QueueDefault: {MaxWorkers: 100},\n\t\t},\n\t\tTestOnly: true, // suitable only for use in tests; remove for live environments\n\t\tWorkers:  workers,\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Out of example scope, but used to wait until a job is worked.\n\tsubscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)\n\tdefer subscribeCancel()\n\n\tif err := riverClient.Start(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t_, err = riverClient.Insert(ctx, WorkFuncArgs{\n\t\tMessage: \"hello from a function!\",\n\t}, nil)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\twaitForNJobs(subscribeChan, 1)\n\n\tif err := riverClient.Stop(ctx); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Output:\n\t// Message: hello from a function!\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.1796875,
          "content": "module github.com/riverqueue/river\n\ngo 1.22\n\ntoolchain go1.23.0\n\nrequire (\n\tgithub.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa\n\tgithub.com/jackc/pgx/v5 v5.7.2\n\tgithub.com/jackc/puddle/v2 v2.2.2\n\tgithub.com/riverqueue/river/riverdriver v0.15.0\n\tgithub.com/riverqueue/river/riverdriver/riverdatabasesql v0.15.0\n\tgithub.com/riverqueue/river/riverdriver/riverpgxv5 v0.15.0\n\tgithub.com/riverqueue/river/rivershared v0.15.0\n\tgithub.com/riverqueue/river/rivertype v0.15.0\n\tgithub.com/robfig/cron/v3 v3.0.1\n\tgithub.com/stretchr/testify v1.10.0\n\tgithub.com/tidwall/gjson v1.18.0\n\tgithub.com/tidwall/sjson v1.2.5\n\tgo.uber.org/goleak v1.3.0\n\tgolang.org/x/sync v0.10.0\n\tgolang.org/x/text v0.21.0\n)\n\nrequire (\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect\n\tgithub.com/lib/pq v1.10.9 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/tidwall/match v1.1.1 // indirect\n\tgithub.com/tidwall/pretty v1.2.1 // indirect\n\tgolang.org/x/crypto v0.31.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n\nreplace github.com/riverqueue/river/rivershared => ./rivershared\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 5.435546875,
          "content": "github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa h1:s+4MhCQ6YrzisK6hFJUX53drDT4UsSW3DEhKn0ifuHw=\ngithub.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa/go.mod h1:a/s9Lp5W7n/DD0VrVoyJ00FbP2ytTPDVOivvn2bMlds=\ngithub.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=\ngithub.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=\ngithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=\ngithub.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=\ngithub.com/jackc/pgx/v5 v5.7.2 h1:mLoDLV6sonKlvjIEsV56SkWNCnuNv531l94GaIzO+XI=\ngithub.com/jackc/pgx/v5 v5.7.2/go.mod h1:ncY89UGWxg82EykZUwSpUKEfccBGGYq1xjrOpsbsfGQ=\ngithub.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=\ngithub.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=\ngithub.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\ngithub.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=\ngithub.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/riverqueue/river/riverdriver v0.14.3 h1:1cB5oR9p9YNXwnjXKLaB4OOyLHXS1zY03IaT0b0nsV0=\ngithub.com/riverqueue/river/riverdriver v0.14.3/go.mod h1:uMpL544T3/0lWsvm9fSehvAw+E9AVOaJlZGgWxJEa7U=\ngithub.com/riverqueue/river/riverdriver/riverdatabasesql v0.14.3 h1:ArFg/2WfvUnb8U3AgpW3jWCoc7V2Gqj4l6HNRP1/vtw=\ngithub.com/riverqueue/river/riverdriver/riverdatabasesql v0.14.3/go.mod h1:Y8A5y4DqHZmHKAyXINEDRZ0PBxOYGQdSZtV9AWK/SLU=\ngithub.com/riverqueue/river/riverdriver/riverpgxv5 v0.14.3 h1:WyT3+ToZbMvlfZqFVfX9L+EZB6e88/+L7PGhUqscPSU=\ngithub.com/riverqueue/river/riverdriver/riverpgxv5 v0.14.3/go.mod h1:jvfJsqQPAWPifx8QuYnhhou/qV6DDE88ZooqbE5uiUE=\ngithub.com/riverqueue/river/rivertype v0.14.3 h1:kKYmgZ0w7YLCQoFTtr4iVyBP/+w8UeleCHQ2qWMrlvs=\ngithub.com/riverqueue/river/rivertype v0.14.3/go.mod h1:4vpt5ZSdZ35mFbRAV4oXgeRdH3Mq5h1pUzQTvaGfCUA=\ngithub.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=\ngithub.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=\ngithub.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=\ngithub.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=\ngithub.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/tidwall/gjson v1.14.2/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=\ngithub.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=\ngithub.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=\ngithub.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=\ngithub.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\ngithub.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=\ngithub.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=\ngithub.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=\ngithub.com/tidwall/sjson v1.2.5 h1:kLy8mja+1c9jlljvWTlSazM7cKDRfJuR/bOJhcY5NcY=\ngithub.com/tidwall/sjson v1.2.5/go.mod h1:Fvgq9kS/6ociJEDnK0Fk1cpYF4FIW6ZF7LAe+6jwd28=\ngo.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=\ngo.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=\ngolang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=\ngolang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\ngolang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=\ngolang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "go.work",
          "type": "blob",
          "size": 0.150390625,
          "content": "go 1.22\n\ntoolchain go1.23.0\n\nuse (\n\t.\n\t./cmd/river\n\t./riverdriver\n\t./riverdriver/riverdatabasesql\n\t./riverdriver/riverpgxv5\n\t./rivershared\n\t./rivertype\n)\n"
        },
        {
          "name": "insert_opts.go",
          "type": "blob",
          "size": 9.365234375,
          "content": "package river\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// Regular expression to which the format of tags must comply. Mainly, no\n// special characters, and with hyphens in the middle.\n//\n// A key property here (in case this is relaxed in the future) is that commas\n// must never be allowed because they're used as a delimiter during batch job\n// insertion for the `riverdatabasesql` driver.\nvar tagRE = regexp.MustCompile(`\\A[\\w][\\w\\-]+[\\w]\\z`)\n\n// InsertOpts are optional settings for a new job which can be provided at job\n// insertion time. These will override any default InsertOpts settings provided\n// by JobArgsWithInsertOpts, as well as any global defaults.\ntype InsertOpts struct {\n\t// MaxAttempts is the maximum number of total attempts (including both the\n\t// original run and all retries) before a job is abandoned and set as\n\t// discarded.\n\tMaxAttempts int\n\n\t// Metadata is a JSON object blob of arbitrary data that will be stored with\n\t// the job. Users should not overwrite or remove anything stored in this\n\t// field by River.\n\tMetadata []byte\n\n\t// Pending indicates that the job should be inserted in the `pending` state.\n\t// Pending jobs are not immediately available to be worked and are never\n\t// deleted, but they can be used to indicate work which should be performed in\n\t// the future once they are made available (or scheduled) by some external\n\t// update.\n\tPending bool\n\n\t// Priority is the priority of the job, with 1 being the highest priority and\n\t// 4 being the lowest. When fetching available jobs to work, the highest\n\t// priority jobs will always be fetched before any lower priority jobs are\n\t// fetched. Note that if your workers are swamped with more high-priority jobs\n\t// then they can handle, lower priority jobs may not be fetched.\n\t//\n\t// Defaults to PriorityDefault.\n\tPriority int\n\n\t// Queue is the name of the job queue in which to insert the job.\n\t//\n\t// Defaults to QueueDefault.\n\tQueue string\n\n\t// ScheduledAt is a time in future at which to schedule the job (i.e. in\n\t// cases where it shouldn't be run immediately). The job is guaranteed not\n\t// to run before this time, but may run slightly after depending on the\n\t// number of other scheduled jobs and how busy the queue is.\n\t//\n\t// Use of this option generally only makes sense when passing options into\n\t// Insert rather than when a job args struct is implementing\n\t// JobArgsWithInsertOpts, however, it will work in both cases.\n\tScheduledAt time.Time\n\n\t// Tags are an arbitrary list of keywords to add to the job. They have no\n\t// functional behavior and are meant entirely as a user-specified construct\n\t// to help group and categorize jobs.\n\t//\n\t// Tags should conform to the regex `\\A[\\w][\\w\\-]+[\\w]\\z` and be a maximum\n\t// of 255 characters long. No special characters are allowed.\n\t//\n\t// If tags are specified from both a job args override and from options on\n\t// Insert, the latter takes precedence. Tags are not merged.\n\tTags []string\n\n\t// UniqueOpts returns options relating to job uniqueness. An empty struct\n\t// avoids setting any worker-level unique options.\n\tUniqueOpts UniqueOpts\n}\n\n// UniqueOpts contains parameters for uniqueness for a job.\n//\n// When the options struct is uninitialized (its zero value) no uniqueness at is\n// enforced. As each property is initialized, it's added as a dimension on the\n// uniqueness matrix. When any property has a non-zero value specified, the\n// job's kind automatically counts toward uniqueness, but can be excluded by\n// setting ExcludeKind to true.\n//\n// So for example, if only ByQueue is on, then for the given job kind, only a\n// single instance is allowed in any given queue, regardless of other properties\n// on the job. If both ByArgs and ByQueue are on, then for the given job kind, a\n// single instance is allowed for each combination of args and queues. If either\n// args or queue is changed on a new job, it's allowed to be inserted as a new\n// job.\n//\n// Uniqueness relies on a hash of the job kind and any unique properties along\n// with a database unique constraint. See the note on ByState for more details\n// including about the fallback to a deprecated advisory lock method.\ntype UniqueOpts struct {\n\t// ByArgs indicates that uniqueness should be enforced for any specific\n\t// instance of encoded args for a job.\n\t//\n\t// Default is false, meaning that as long as any other unique property is\n\t// enabled, uniqueness will be enforced for a kind regardless of input args.\n\t//\n\t// When set to true, the entire encoded args field will be included in the\n\t// uniqueness hash, which requires care to ensure that no irrelevant args are\n\t// factored into the uniqueness check. It is also possible to use a subset of\n\t// the args by indicating on the `JobArgs` struct which fields should be\n\t// included in the uniqueness check using struct tags:\n\t//\n\t// \ttype MyJobArgs struct {\n\t// \t\tCustomerID string `json:\"customer_id\" river:\"unique\"`\n\t// \t\tTraceID string `json:\"trace_id\"\n\t// \t}\n\t//\n\t// In this example, only the encoded `customer_id` key will be included in the\n\t// uniqueness check and the `trace_id` key will be ignored.\n\t//\n\t// All keys are sorted alphabetically before hashing to ensure consistent\n\t// results.\n\tByArgs bool\n\n\t// ByPeriod defines uniqueness within a given period. On an insert time is\n\t// rounded down to the nearest multiple of the given period, and a job is\n\t// only inserted if there isn't an existing job that will run between then\n\t// and the next multiple of the period.\n\t//\n\t// Default is no unique period, meaning that as long as any other unique\n\t// property is enabled, uniqueness will be enforced across all jobs of the\n\t// kind in the database, regardless of when they were scheduled.\n\tByPeriod time.Duration\n\n\t// ByQueue indicates that uniqueness should be enforced within each queue.\n\t//\n\t// Default is false, meaning that as long as any other unique property is\n\t// enabled, uniqueness will be enforced for a kind across all queues.\n\tByQueue bool\n\n\t// ByState indicates that uniqueness should be enforced across any of the\n\t// states in the given set. Unlike other unique options, ByState gets a\n\t// default when it's not set for user convenience. The default is equivalent\n\t// to:\n\t//\n\t// \tByState: []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted, rivertype.JobStatePending, rivertype.JobStateRunning, rivertype.JobStateRetryable, rivertype.JobStateScheduled}\n\t//\n\t// Or more succinctly:\n\t//\n\t// \tByState: rivertype.UniqueOptsByStateDefault()\n\t//\n\t// With this setting, any jobs of the same kind that have been completed or\n\t// discarded, but not yet cleaned out by the system, will still prevent a\n\t// duplicate unique job from being inserted. For example, with the default\n\t// states, if a unique job is actively `running`, a duplicate cannot be\n\t// inserted. Likewise, if a unique job has `completed`, you still can't\n\t// insert a duplicate, at least not until the job cleaner maintenance process\n\t// eventually removes the completed job from the `river_job` table.\n\t//\n\t// The list may be safely customized to _add_ additional states (`cancelled`\n\t// or `discarded`), though only `retryable` may be safely _removed_ from the\n\t// list.\n\t//\n\t// Warning: Removing any states from the default list (other than `retryable`)\n\t// forces a fallback to a slower insertion path that takes an advisory lock\n\t// and performs a look up before insertion. This path is deprecated and should\n\t// be avoided if possible.\n\tByState []rivertype.JobState\n\n\t// ExcludeKind indicates that the job kind should not be included in the\n\t// uniqueness check. This is useful when you want to enforce uniqueness\n\t// across all jobs regardless of kind.\n\tExcludeKind bool\n}\n\n// isEmpty returns true for an empty, uninitialized options struct.\n//\n// This is required because we can't check against `UniqueOpts{}` because slices\n// aren't comparable. Unfortunately it makes things a little more brittle\n// comparatively because any new options must also be considered here for things\n// to work.\nfunc (o *UniqueOpts) isEmpty() bool {\n\treturn !o.ByArgs &&\n\t\to.ByPeriod == time.Duration(0) &&\n\t\t!o.ByQueue &&\n\t\to.ByState == nil\n}\n\nvar jobStateAll = rivertype.JobStates() //nolint:gochecknoglobals\n\nvar requiredV3states = []rivertype.JobState{ //nolint:gochecknoglobals\n\trivertype.JobStateAvailable,\n\trivertype.JobStatePending,\n\trivertype.JobStateRunning,\n\trivertype.JobStateScheduled,\n}\n\nfunc (o *UniqueOpts) validate() error {\n\tif o.isEmpty() {\n\t\treturn nil\n\t}\n\n\tif o.ByPeriod != time.Duration(0) && o.ByPeriod < 1*time.Second {\n\t\treturn errors.New(\"UniqueOpts.ByPeriod should not be less than 1 second\")\n\t}\n\n\t// Job states are typed, but since the underlying type is a string, users\n\t// can put anything they want in there.\n\tfor _, state := range o.ByState {\n\t\t// This could be turned to a map lookup, but last I checked the speed\n\t\t// difference for tiny slice sizes is negligible, and map lookup might\n\t\t// even be slower.\n\t\tif !slices.Contains(jobStateAll, state) {\n\t\t\treturn fmt.Errorf(\"UniqueOpts.ByState contains invalid state %q\", state)\n\t\t}\n\t}\n\n\t// Skip required states validation if no custom states were provided.\n\tif len(o.ByState) == 0 {\n\t\treturn nil\n\t}\n\n\tvar missingStates []string\n\tfor _, state := range requiredV3states {\n\t\tif !slices.Contains(o.ByState, state) {\n\t\t\tmissingStates = append(missingStates, string(state))\n\t\t}\n\t}\n\tif len(missingStates) > 0 {\n\t\treturn fmt.Errorf(\"UniqueOpts.ByState must contain all required states, missing: %s\", strings.Join(missingStates, \", \"))\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "insert_opts_test.go",
          "type": "blob",
          "size": 2.36328125,
          "content": "package river\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc TestTagRE(t *testing.T) {\n\tt.Parallel()\n\n\trequire.Regexp(t, tagRE, \"aaa\")\n\trequire.Regexp(t, tagRE, \"_aaa\")\n\trequire.Regexp(t, tagRE, \"aaa_\")\n\trequire.Regexp(t, tagRE, \"777\")\n\trequire.Regexp(t, tagRE, \"my-tag\")\n\trequire.Regexp(t, tagRE, \"my_tag\")\n\trequire.Regexp(t, tagRE, \"my-longer-tag\")\n\trequire.Regexp(t, tagRE, \"my_longer_tag\")\n\trequire.Regexp(t, tagRE, \"My_Capitalized_Tag\")\n\trequire.Regexp(t, tagRE, \"ALL_CAPS\")\n\trequire.Regexp(t, tagRE, \"1_2_3\")\n\n\trequire.NotRegexp(t, tagRE, \"a\")\n\trequire.NotRegexp(t, tagRE, \"aa\")\n\trequire.NotRegexp(t, tagRE, \"-aaa\")\n\trequire.NotRegexp(t, tagRE, \"aaa-\")\n\trequire.NotRegexp(t, tagRE, \"special@characters$banned\")\n\trequire.NotRegexp(t, tagRE, \"commas,never,allowed\")\n}\n\nfunc TestUniqueOpts_validate(t *testing.T) {\n\tt.Parallel()\n\n\trequire.NoError(t, (&UniqueOpts{}).validate())\n\trequire.NoError(t, (&UniqueOpts{\n\t\tByArgs:   true,\n\t\tByPeriod: 1 * time.Second,\n\t\tByQueue:  true,\n\t}).validate())\n\n\trequire.EqualError(t, (&UniqueOpts{ByPeriod: 1 * time.Millisecond}).validate(), \"UniqueOpts.ByPeriod should not be less than 1 second\")\n\trequire.EqualError(t, (&UniqueOpts{ByState: []rivertype.JobState{rivertype.JobState(\"invalid\")}}).validate(), `UniqueOpts.ByState contains invalid state \"invalid\"`)\n\n\trequiredStates := []rivertype.JobState{\n\t\trivertype.JobStateAvailable,\n\t\trivertype.JobStatePending,\n\t\trivertype.JobStateRunning,\n\t\trivertype.JobStateScheduled,\n\t}\n\n\tfor _, state := range requiredStates {\n\t\t// Test with each state individually removed from requiredStates to ensure\n\t\t// it's validated.\n\n\t\t// Create a copy of requiredStates without the current state\n\t\tvar testStates []rivertype.JobState\n\t\tfor _, s := range requiredStates {\n\t\t\tif s != state {\n\t\t\t\ttestStates = append(testStates, s)\n\t\t\t}\n\t\t}\n\n\t\t// Test validation\n\t\trequire.EqualError(t, (&UniqueOpts{ByState: testStates}).validate(), \"UniqueOpts.ByState must contain all required states, missing: \"+string(state))\n\t}\n\n\t// test with more than one required state missing:\n\trequire.EqualError(t, (&UniqueOpts{ByState: []rivertype.JobState{\n\t\trivertype.JobStateAvailable,\n\t\trivertype.JobStateScheduled,\n\t}}).validate(), \"UniqueOpts.ByState must contain all required states, missing: pending, running\")\n\n\trequire.NoError(t, (&UniqueOpts{ByState: rivertype.JobStates()}).validate())\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "job.go",
          "type": "blob",
          "size": 1.0712890625,
          "content": "package river\n\nimport (\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// Job represents a single unit of work, holding both the arguments and\n// information for a job with args of type T.\ntype Job[T JobArgs] struct {\n\t*rivertype.JobRow\n\n\t// Args are the arguments for the job.\n\tArgs T\n}\n\n// JobArgs is an interface that represents the arguments for a job of type T.\n// These arguments are serialized into JSON and stored in the database.\n//\n// The struct is serialized using `encoding/json`. All exported fields are\n// serialized, unless skipped with a struct field tag.\ntype JobArgs interface {\n\t// Kind is a string that uniquely identifies the type of job. This must be\n\t// provided on your job arguments struct.\n\tKind() string\n}\n\n// JobArgsWithInsertOpts is an extra interface that a job may implement on top\n// of JobArgs to provide insertion-time options for all jobs of this type.\ntype JobArgsWithInsertOpts interface {\n\t// InsertOpts returns options for all jobs of this job type, overriding any\n\t// system defaults. These can also be overridden at insertion time.\n\tInsertOpts() InsertOpts\n}\n"
        },
        {
          "name": "job_args_reflect_kind_test.go",
          "type": "blob",
          "size": 1.39453125,
          "content": "package river\n\nimport \"reflect\"\n\n// JobArgsReflectKind can be embedded on a struct to implement JobArgs such that\n// the job's kind will be the name of TKind. Typically, for convenience TKind\n// will be the same type that does the embedding. Use of JobArgsReflectKind may\n// not be typical, but in combination with WorkFunc, it allows the entirety of a\n// job args and worker pair to be implemented inside the body of a function.\n//\n//\ttype InFuncWorkFuncArgs struct {\n//\t\tJobArgsReflectKind[InFuncWorkFuncArgs]\n//\t\tMessage `json:\"message\"`\n//\t}\n//\n//\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[WorkFuncArgs]) error {\n//\t\t...\n//\n// Its major downside compared to a normal JobArgs implementation is that it's\n// possible to easily break things accidentally by renaming its type, deploying,\n// and then finding that the worker will no longer work any jobs that were\n// inserted before the deploy. It also depends on reflection, which likely makes\n// it marginally slower.\n//\n// We're not sure yet whether it's appropriate to expose this publicly, so for\n// now we've localized it to the test suite only. When a test case needs a job\n// type that won't be reused, it's preferable to make use of JobArgsReflectKind\n// so the type doesn't pollute the global namespace.\ntype JobArgsReflectKind[TKind any] struct{}\n\nfunc (a JobArgsReflectKind[TKind]) Kind() string { return reflect.TypeOf(a).Name() }\n"
        },
        {
          "name": "job_complete_tx.go",
          "type": "blob",
          "size": 1.7568359375,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// JobCompleteTx marks the job as completed as part of transaction tx. If tx is\n// rolled back, the completion will be as well.\n//\n// The function needs to know the type of the River database driver, which is\n// the same as the one in use by Client, but the other generic parameters can be\n// inferred. An invocation should generally look like:\n//\n//\t_, err := river.JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\n// Returns the updated, completed job.\nfunc JobCompleteTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs JobArgs](ctx context.Context, tx TTx, job *Job[TArgs]) (*Job[TArgs], error) {\n\tif job.State != rivertype.JobStateRunning {\n\t\treturn nil, errors.New(\"job must be running\")\n\t}\n\n\tclient := ClientFromContext[TTx](ctx)\n\tif client == nil {\n\t\treturn nil, errors.New(\"client not found in context, can only work within a River worker\")\n\t}\n\n\tdriver := client.Driver()\n\tpilot := client.Pilot()\n\n\texecTx := driver.UnwrapExecutor(tx)\n\tparams := riverdriver.JobSetStateCompleted(job.ID, time.Now())\n\trows, err := pilot.JobSetStateIfRunningMany(ctx, execTx, &riverdriver.JobSetStateIfRunningManyParams{\n\t\tID:          []int64{params.ID},\n\t\tErrData:     [][]byte{params.ErrData},\n\t\tFinalizedAt: []*time.Time{params.FinalizedAt},\n\t\tMaxAttempts: []*int{params.MaxAttempts},\n\t\tScheduledAt: []*time.Time{params.ScheduledAt},\n\t\tState:       []rivertype.JobState{params.State},\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tupdatedJob := &Job[TArgs]{JobRow: rows[0]}\n\n\tif err := json.Unmarshal(updatedJob.EncodedArgs, &updatedJob.Args); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn updatedJob, nil\n}\n"
        },
        {
          "name": "job_complete_tx_test.go",
          "type": "blob",
          "size": 2.1533203125,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/testfactory\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc TestJobCompleteTx(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype JobArgs struct {\n\t\tJobArgsReflectKind[JobArgs]\n\t}\n\n\ttype testBundle struct {\n\t\tclient *Client[pgx.Tx]\n\t\texec   riverdriver.Executor\n\t\ttx     pgx.Tx\n\t}\n\n\tsetup := func(ctx context.Context, t *testing.T) (context.Context, *testBundle) {\n\t\tt.Helper()\n\n\t\ttx := riverinternaltest.TestTx(ctx, t)\n\t\tclient, err := NewClient(riverpgxv5.New(nil), &Config{\n\t\t\tLogger: riversharedtest.Logger(t),\n\t\t})\n\t\trequire.NoError(t, err)\n\t\tctx = context.WithValue(ctx, rivercommon.ContextKeyClient{}, client)\n\n\t\treturn ctx, &testBundle{\n\t\t\tclient: client,\n\t\t\texec:   riverpgxv5.New(nil).UnwrapExecutor(tx),\n\t\t\ttx:     tx,\n\t\t}\n\t}\n\n\tt.Run(\"CompletesJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx, bundle := setup(ctx, t)\n\n\t\tjob := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{\n\t\t\tState: ptrutil.Ptr(rivertype.JobStateRunning),\n\t\t})\n\n\t\tcompletedJob, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, completedJob.State)\n\t\trequire.WithinDuration(t, time.Now(), *completedJob.FinalizedAt, 2*time.Second)\n\n\t\tupdatedJob, err := bundle.exec.JobGetByID(ctx, job.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, updatedJob.State)\n\t})\n\n\tt.Run(\"ErrorIfNotRunning\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tctx, bundle := setup(ctx, t)\n\n\t\tjob := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})\n\n\t\t_, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})\n\t\trequire.EqualError(t, err, \"job must be running\")\n\t})\n}\n"
        },
        {
          "name": "job_executor.go",
          "type": "blob",
          "size": 13.1435546875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"runtime/debug\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/jobstats\"\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// Error used in CancelFunc in cases where the job was not cancelled for\n// purposes of resource cleanup. Should never be user visible.\nvar errExecutorDefaultCancel = errors.New(\"context cancelled as executor finished\")\n\n// UnknownJobKindError is returned when a Client fetches and attempts to\n// work a job that has not been registered on the Client's Workers bundle (using\n// AddWorker).\ntype UnknownJobKindError struct {\n\t// Kind is the string that was returned by the JobArgs Kind method.\n\tKind string\n}\n\n// Error returns the error string.\nfunc (e *UnknownJobKindError) Error() string {\n\treturn \"job kind is not registered in the client's Workers bundle: \" + e.Kind\n}\n\n// Is implements the interface used by errors.Is to determine if errors are\n// equivalent. It returns true for any other UnknownJobKindError without\n// regard to the Kind string so it is possible to detect this type of error\n// with:\n//\n//\terrors.Is(err, &UnknownJobKindError{})\nfunc (e *UnknownJobKindError) Is(target error) bool {\n\t_, ok := target.(*UnknownJobKindError)\n\treturn ok\n}\n\n// JobCancel wraps err and can be returned from a Worker's Work method to cancel\n// the job at the end of execution. Regardless of whether or not the job has any\n// remaining attempts, this will ensure the job does not execute again.\nfunc JobCancel(err error) error {\n\treturn &JobCancelError{err: err}\n}\n\n// JobCancelError is the error type returned by JobCancel. It should not be\n// initialized directly, but is returned from the [JobCancel] function and can\n// be used for test assertions.\ntype JobCancelError struct {\n\terr error\n}\n\nfunc (e *JobCancelError) Error() string {\n\tif e.err == nil {\n\t\treturn \"JobCancelError: <nil>\"\n\t}\n\t// should not ever be called, but add a prefix just in case:\n\treturn \"JobCancelError: \" + e.err.Error()\n}\n\nfunc (e *JobCancelError) Is(target error) bool {\n\t_, ok := target.(*JobCancelError)\n\treturn ok\n}\n\nfunc (e *JobCancelError) Unwrap() error { return e.err }\n\n// JobSnooze can be returned from a Worker's Work method to cause the job to be\n// tried again after the specified duration. This also has the effect of\n// incrementing the job's MaxAttempts by 1, meaning that jobs can be repeatedly\n// snoozed without ever being discarded.\n//\n// Panics if duration is < 0.\nfunc JobSnooze(duration time.Duration) error {\n\tif duration < 0 {\n\t\tpanic(\"JobSnooze: duration must be >= 0\")\n\t}\n\treturn &JobSnoozeError{duration: duration}\n}\n\n// JobSnoozeError is the error type returned by JobSnooze. It should not be\n// initialized directly, but is returned from the [JobSnooze] function and can\n// be used for test assertions.\ntype JobSnoozeError struct {\n\tduration time.Duration\n}\n\nfunc (e *JobSnoozeError) Error() string {\n\t// should not ever be called, but add a prefix just in case:\n\treturn fmt.Sprintf(\"JobSnoozeError: %s\", e.duration)\n}\n\nfunc (e *JobSnoozeError) Is(target error) bool {\n\t_, ok := target.(*JobSnoozeError)\n\treturn ok\n}\n\nvar ErrJobCancelledRemotely = JobCancel(errors.New(\"job cancelled remotely\"))\n\ntype jobExecutorResult struct {\n\tErr        error\n\tNextRetry  time.Time\n\tPanicTrace string\n\tPanicVal   any\n}\n\n// ErrorStr returns an appropriate string to persist to the database based on\n// the type of internal failure (i.e. error or panic). Panics if called on a\n// non-errored result.\nfunc (r *jobExecutorResult) ErrorStr() string {\n\tswitch {\n\tcase r.Err != nil:\n\t\treturn r.Err.Error()\n\tcase r.PanicVal != nil:\n\t\treturn fmt.Sprintf(\"%v\", r.PanicVal)\n\t}\n\n\tpanic(\"ErrorStr should not be called on non-errored result\")\n}\n\ntype jobExecutor struct {\n\tbaseservice.BaseService\n\n\tCancelFunc             context.CancelCauseFunc\n\tClientJobTimeout       time.Duration\n\tCompleter              jobcompleter.JobCompleter\n\tClientRetryPolicy      ClientRetryPolicy\n\tErrorHandler           ErrorHandler\n\tInformProducerDoneFunc func(jobRow *rivertype.JobRow)\n\tJobRow                 *rivertype.JobRow\n\tGlobalMiddleware       []rivertype.WorkerMiddleware\n\tSchedulerInterval      time.Duration\n\tWorkUnit               workunit.WorkUnit\n\n\t// Meant to be used from within the job executor only.\n\tstart time.Time\n\tstats *jobstats.JobStatistics // initialized by the executor, and handed off to completer\n}\n\nfunc (e *jobExecutor) Cancel() {\n\te.Logger.Warn(e.Name+\": job cancelled remotely\", slog.Int64(\"job_id\", e.JobRow.ID))\n\te.CancelFunc(ErrJobCancelledRemotely)\n}\n\nfunc (e *jobExecutor) Execute(ctx context.Context) {\n\t// Ensure that the context is cancelled no matter what, or it will leak:\n\tdefer e.CancelFunc(errExecutorDefaultCancel)\n\n\te.start = e.Time.NowUTC()\n\te.stats = &jobstats.JobStatistics{\n\t\tQueueWaitDuration: e.start.Sub(e.JobRow.ScheduledAt),\n\t}\n\n\tres := e.execute(ctx)\n\tif res.Err != nil && errors.Is(context.Cause(ctx), ErrJobCancelledRemotely) {\n\t\tres.Err = context.Cause(ctx)\n\t}\n\n\te.reportResult(ctx, res)\n\n\te.InformProducerDoneFunc(e.JobRow)\n}\n\n// Executes the job, handling a panic if necessary (and various other error\n// conditions). The named return value is so that we can still return a value in\n// case of a panic.\n//\n//nolint:nonamedreturns\nfunc (e *jobExecutor) execute(ctx context.Context) (res *jobExecutorResult) {\n\tdefer func() {\n\t\tif recovery := recover(); recovery != nil {\n\t\t\te.Logger.ErrorContext(ctx, e.Name+\": panic recovery; possible bug with Worker\",\n\t\t\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\t\t\tslog.String(\"kind\", e.JobRow.Kind),\n\t\t\t\tslog.String(\"panic_val\", fmt.Sprintf(\"%v\", recovery)),\n\t\t\t)\n\n\t\t\tres = &jobExecutorResult{\n\t\t\t\tPanicTrace: string(debug.Stack()),\n\t\t\t\tPanicVal:   recovery,\n\t\t\t}\n\t\t}\n\t\te.stats.RunDuration = e.Time.NowUTC().Sub(e.start)\n\t}()\n\n\tif e.WorkUnit == nil {\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Unhandled job kind\",\n\t\t\tslog.String(\"kind\", e.JobRow.Kind),\n\t\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\t)\n\t\treturn &jobExecutorResult{Err: &UnknownJobKindError{Kind: e.JobRow.Kind}}\n\t}\n\n\tif err := e.WorkUnit.UnmarshalJob(); err != nil {\n\t\treturn &jobExecutorResult{Err: err}\n\t}\n\n\tworkerMiddleware := e.WorkUnit.Middleware()\n\n\tdoInner := func(ctx context.Context) error {\n\t\tjobTimeout := e.WorkUnit.Timeout()\n\t\tif jobTimeout == 0 {\n\t\t\tjobTimeout = e.ClientJobTimeout\n\t\t}\n\n\t\t// No timeout if a -1 was specified.\n\t\tif jobTimeout > 0 {\n\t\t\tvar cancel context.CancelFunc\n\t\t\tctx, cancel = context.WithTimeout(ctx, jobTimeout)\n\t\t\tdefer cancel()\n\t\t}\n\n\t\tif err := e.WorkUnit.Work(ctx); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tallMiddleware := make([]rivertype.WorkerMiddleware, 0, len(e.GlobalMiddleware)+len(workerMiddleware))\n\tallMiddleware = append(allMiddleware, e.GlobalMiddleware...)\n\tallMiddleware = append(allMiddleware, workerMiddleware...)\n\n\tif len(allMiddleware) > 0 {\n\t\t// Wrap middlewares in reverse order so the one defined first is wrapped\n\t\t// as the outermost function and is first to receive the operation.\n\t\tfor i := len(allMiddleware) - 1; i >= 0; i-- {\n\t\t\tmiddlewareItem := allMiddleware[i] // capture the current middleware item\n\t\t\tpreviousDoInner := doInner         // Capture the current doInner function\n\t\t\tdoInner = func(ctx context.Context) error {\n\t\t\t\treturn middlewareItem.Work(ctx, e.JobRow, previousDoInner)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn &jobExecutorResult{Err: doInner(ctx)}\n}\n\nfunc (e *jobExecutor) invokeErrorHandler(ctx context.Context, res *jobExecutorResult) bool {\n\tinvokeAndHandlePanic := func(funcName string, errorHandler func() *ErrorHandlerResult) *ErrorHandlerResult {\n\t\tdefer func() {\n\t\t\tif panicVal := recover(); panicVal != nil {\n\t\t\t\te.Logger.ErrorContext(ctx, e.Name+\": ErrorHandler invocation panicked\",\n\t\t\t\t\tslog.String(\"function_name\", funcName),\n\t\t\t\t\tslog.String(\"panic_val\", fmt.Sprintf(\"%v\", panicVal)),\n\t\t\t\t)\n\t\t\t}\n\t\t}()\n\n\t\treturn errorHandler()\n\t}\n\n\tvar errorHandlerRes *ErrorHandlerResult\n\tswitch {\n\tcase res.Err != nil:\n\t\terrorHandlerRes = invokeAndHandlePanic(\"HandleError\", func() *ErrorHandlerResult {\n\t\t\treturn e.ErrorHandler.HandleError(ctx, e.JobRow, res.Err)\n\t\t})\n\n\tcase res.PanicVal != nil:\n\t\terrorHandlerRes = invokeAndHandlePanic(\"HandlePanic\", func() *ErrorHandlerResult {\n\t\t\treturn e.ErrorHandler.HandlePanic(ctx, e.JobRow, res.PanicVal, res.PanicTrace)\n\t\t})\n\t}\n\n\treturn errorHandlerRes != nil && errorHandlerRes.SetCancelled\n}\n\nfunc (e *jobExecutor) reportResult(ctx context.Context, res *jobExecutorResult) {\n\tvar snoozeErr *JobSnoozeError\n\n\tif res.Err != nil && errors.As(res.Err, &snoozeErr) {\n\t\te.Logger.DebugContext(ctx, e.Name+\": Job snoozed\",\n\t\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\t\tslog.String(\"job_kind\", e.JobRow.Kind),\n\t\t\tslog.Duration(\"duration\", snoozeErr.duration),\n\t\t)\n\t\tnextAttemptScheduledAt := time.Now().Add(snoozeErr.duration)\n\n\t\t// Normally, snoozed jobs are set `scheduled` for the future and it's the\n\t\t// scheduler's job to set them back to `available` so they can be reworked.\n\t\t// Just as with retryable jobs, this isn't friendly for short snooze times\n\t\t// so we instead make the job immediately `available` if the snooze time is\n\t\t// smaller than the scheduler's run interval.\n\t\tvar params *riverdriver.JobSetStateIfRunningParams\n\t\tif nextAttemptScheduledAt.Sub(e.Time.NowUTC()) <= e.SchedulerInterval {\n\t\t\tparams = riverdriver.JobSetStateSnoozedAvailable(e.JobRow.ID, nextAttemptScheduledAt, e.JobRow.MaxAttempts+1)\n\t\t} else {\n\t\t\tparams = riverdriver.JobSetStateSnoozed(e.JobRow.ID, nextAttemptScheduledAt, e.JobRow.MaxAttempts+1)\n\t\t}\n\t\tif err := e.Completer.JobSetStateIfRunning(ctx, e.stats, params); err != nil {\n\t\t\te.Logger.ErrorContext(ctx, e.Name+\": Error snoozing job\",\n\t\t\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\t\t)\n\t\t}\n\t\treturn\n\t}\n\n\tif res.Err != nil || res.PanicVal != nil {\n\t\te.reportError(ctx, res)\n\t\treturn\n\t}\n\n\tif err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateCompleted(e.JobRow.ID, e.Time.NowUTC())); err != nil {\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Error completing job\",\n\t\t\tslog.String(\"err\", err.Error()),\n\t\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\t)\n\t\treturn\n\t}\n}\n\nfunc (e *jobExecutor) reportError(ctx context.Context, res *jobExecutorResult) {\n\tvar (\n\t\tcancelJob bool\n\t\tcancelErr *JobCancelError\n\t)\n\n\tlogAttrs := []any{\n\t\tslog.String(\"error\", res.ErrorStr()),\n\t\tslog.Int64(\"job_id\", e.JobRow.ID),\n\t\tslog.String(\"job_kind\", e.JobRow.Kind),\n\t}\n\n\tswitch {\n\tcase errors.As(res.Err, &cancelErr):\n\t\tcancelJob = true\n\t\te.Logger.DebugContext(ctx, e.Name+\": Job cancelled explicitly\", logAttrs...)\n\tcase res.Err != nil:\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Job errored\", logAttrs...)\n\tcase res.PanicVal != nil:\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Job panicked\", logAttrs...)\n\t}\n\n\tif e.ErrorHandler != nil && !cancelJob {\n\t\t// Error handlers also have an opportunity to cancel the job.\n\t\tcancelJob = e.invokeErrorHandler(ctx, res)\n\t}\n\n\tattemptErr := rivertype.AttemptError{\n\t\tAt:      e.start,\n\t\tAttempt: e.JobRow.Attempt,\n\t\tError:   res.ErrorStr(),\n\t\tTrace:   res.PanicTrace,\n\t}\n\n\terrData, err := json.Marshal(attemptErr)\n\tif err != nil {\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Failed to marshal attempt error\", logAttrs...)\n\t\treturn\n\t}\n\n\tnow := time.Now()\n\n\tif cancelJob {\n\t\tif err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateCancelled(e.JobRow.ID, now, errData)); err != nil {\n\t\t\te.Logger.ErrorContext(ctx, e.Name+\": Failed to cancel job and report error\", logAttrs...)\n\t\t}\n\t\treturn\n\t}\n\n\tif e.JobRow.Attempt >= e.JobRow.MaxAttempts {\n\t\tif err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateDiscarded(e.JobRow.ID, now, errData)); err != nil {\n\t\t\te.Logger.ErrorContext(ctx, e.Name+\": Failed to discard job and report error\", logAttrs...)\n\t\t}\n\t\treturn\n\t}\n\n\tvar nextRetryScheduledAt time.Time\n\tif e.WorkUnit != nil {\n\t\tnextRetryScheduledAt = e.WorkUnit.NextRetry()\n\t}\n\tif nextRetryScheduledAt.IsZero() {\n\t\tnextRetryScheduledAt = e.ClientRetryPolicy.NextRetry(e.JobRow)\n\t}\n\tif nextRetryScheduledAt.Before(now) {\n\t\te.Logger.WarnContext(ctx,\n\t\t\te.Name+\": Retry policy returned invalid next retry before current time; using default retry policy instead\",\n\t\t\tslog.Int(\"error_count\", len(e.JobRow.Errors)+1),\n\t\t\tslog.Time(\"next_retry_scheduled_at\", nextRetryScheduledAt),\n\t\t\tslog.Time(\"now\", now),\n\t\t)\n\t\tnextRetryScheduledAt = (&DefaultClientRetryPolicy{}).NextRetry(e.JobRow)\n\t}\n\n\t// Normally, errored jobs are set `retryable` for the future and it's the\n\t// scheduler's job to set them back to `available` so they can be reworked.\n\t// This isn't friendly for smaller retry times though because it means that\n\t// effectively no retry time smaller than the scheduler's run interval is\n\t// respected. Here, we offset that with a branch that makes jobs immediately\n\t// `available` if their retry was smaller than the scheduler's run interval.\n\tvar params *riverdriver.JobSetStateIfRunningParams\n\tif nextRetryScheduledAt.Sub(e.Time.NowUTC()) <= e.SchedulerInterval {\n\t\tparams = riverdriver.JobSetStateErrorAvailable(e.JobRow.ID, nextRetryScheduledAt, errData)\n\t} else {\n\t\tparams = riverdriver.JobSetStateErrorRetryable(e.JobRow.ID, nextRetryScheduledAt, errData)\n\t}\n\tif err := e.Completer.JobSetStateIfRunning(ctx, e.stats, params); err != nil {\n\t\te.Logger.ErrorContext(ctx, e.Name+\": Failed to report error for job\", logAttrs...)\n\t}\n}\n"
        },
        {
          "name": "job_executor_test.go",
          "type": "blob",
          "size": 25.2060546875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivershared/util/timeutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\ntype customRetryPolicyWorker struct {\n\tWorkerDefaults[callbackArgs]\n\tf         func() error\n\tnextRetry func() time.Time\n}\n\nfunc (w *customRetryPolicyWorker) NextRetry(job *Job[callbackArgs]) time.Time {\n\tif w.nextRetry != nil {\n\t\treturn w.nextRetry()\n\t}\n\treturn time.Time{}\n}\n\nfunc (w *customRetryPolicyWorker) Work(ctx context.Context, job *Job[callbackArgs]) error {\n\treturn w.f()\n}\n\n// Makes a workerInfo using the real workerWrapper with a job that uses a\n// callback Work func and allows for customizable maxAttempts and nextRetry.\nfunc newWorkUnitFactoryWithCustomRetry(f func() error, nextRetry func() time.Time) workunit.WorkUnitFactory {\n\treturn &workUnitFactoryWrapper[callbackArgs]{worker: &customRetryPolicyWorker{\n\t\tf:         f,\n\t\tnextRetry: nextRetry,\n\t}}\n}\n\n// A retry policy demonstrating trivial customization.\ntype retryPolicyCustom struct {\n\tDefaultClientRetryPolicy\n}\n\nfunc (p *retryPolicyCustom) NextRetry(job *rivertype.JobRow) time.Time {\n\tvar backoffDuration time.Duration\n\tswitch job.Attempt {\n\tcase 1:\n\t\tbackoffDuration = 10 * time.Second\n\tcase 2:\n\t\tbackoffDuration = 20 * time.Second\n\tcase 3:\n\t\tbackoffDuration = 30 * time.Second\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"next retry should not have been called for attempt %d\", job.Attempt))\n\t}\n\n\treturn job.AttemptedAt.Add(backoffDuration)\n}\n\n// A retry policy that returns invalid timestamps.\ntype retryPolicyInvalid struct {\n\tDefaultClientRetryPolicy\n}\n\nfunc (p *retryPolicyInvalid) NextRetry(job *rivertype.JobRow) time.Time { return time.Time{} }\n\n// Identical to default retry policy except that it leaves off the jitter to\n// make checking against it more convenient.\ntype retryPolicyNoJitter struct {\n\tDefaultClientRetryPolicy\n}\n\nfunc (p *retryPolicyNoJitter) NextRetry(job *rivertype.JobRow) time.Time {\n\treturn job.AttemptedAt.Add(timeutil.SecondsAsDuration(p.retrySecondsWithoutJitter(job.Attempt)))\n}\n\ntype testErrorHandler struct {\n\tHandleErrorCalled bool\n\tHandleErrorFunc   func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult\n\n\tHandlePanicCalled bool\n\tHandlePanicFunc   func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult\n}\n\n// Test handler with no-ops for both error handling functions.\nfunc newTestErrorHandler() *testErrorHandler {\n\treturn &testErrorHandler{\n\t\tHandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult { return nil },\n\t\tHandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\t\t\treturn nil\n\t\t},\n\t}\n}\n\nfunc (h *testErrorHandler) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\th.HandleErrorCalled = true\n\treturn h.HandleErrorFunc(ctx, job, err)\n}\n\nfunc (h *testErrorHandler) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\th.HandlePanicCalled = true\n\treturn h.HandlePanicFunc(ctx, job, panicVal, trace)\n}\n\nfunc TestJobExecutor_Execute(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tcompleter    *jobcompleter.InlineCompleter\n\t\texec         riverdriver.Executor\n\t\terrorHandler *testErrorHandler\n\t\tjobRow       *rivertype.JobRow\n\t\tupdateCh     <-chan []jobcompleter.CompleterJobUpdated\n\t}\n\n\tsetup := func(t *testing.T) (*jobExecutor, *testBundle) {\n\t\tt.Helper()\n\n\t\tvar (\n\t\t\ttx        = riverinternaltest.TestTx(ctx, t)\n\t\t\tarchetype = riversharedtest.BaseServiceArchetype(t)\n\t\t\texec      = riverpgxv5.New(nil).UnwrapExecutor(tx)\n\t\t\tupdateCh  = make(chan []jobcompleter.CompleterJobUpdated, 10)\n\t\t\tcompleter = jobcompleter.NewInlineCompleter(archetype, exec, updateCh)\n\t\t)\n\n\t\tt.Cleanup(completer.Stop)\n\n\t\tworkUnitFactory := newWorkUnitFactoryWithCustomRetry(func() error { return nil }, nil)\n\n\t\tnow := time.Now().UTC()\n\t\tresults, err := exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{\n\t\t\tEncodedArgs: []byte(\"{}\"),\n\t\t\tKind:        (callbackArgs{}).Kind(),\n\t\t\tMaxAttempts: rivercommon.MaxAttemptsDefault,\n\t\t\tPriority:    rivercommon.PriorityDefault,\n\t\t\tQueue:       rivercommon.QueueDefault,\n\t\t\t// Needs to be explicitly set to a \"now\" horizon that's aligned with the\n\t\t\t// JobGetAvailable call. InsertMany applies a default scheduled_at in Go\n\t\t\t// so it can't pick up the Postgres-level `now()` default.\n\t\t\tScheduledAt: ptrutil.Ptr(now),\n\t\t\tState:       rivertype.JobStateAvailable,\n\t\t}})\n\t\trequire.NoError(t, err)\n\n\t\t// Fetch the job to make sure it's marked as running:\n\t\tjobs, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{\n\t\t\tMax:   1,\n\t\t\tNow:   ptrutil.Ptr(now),\n\t\t\tQueue: rivercommon.QueueDefault,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\trequire.Len(t, jobs, 1)\n\t\trequire.Equal(t, results[0].Job.ID, jobs[0].ID)\n\t\tjob := jobs[0]\n\n\t\tbundle := &testBundle{\n\t\t\tcompleter:    completer,\n\t\t\texec:         exec,\n\t\t\terrorHandler: newTestErrorHandler(),\n\t\t\tjobRow:       job,\n\t\t\tupdateCh:     updateCh,\n\t\t}\n\n\t\t// allocate this context just so we can set the CancelFunc:\n\t\t_, cancel := context.WithCancelCause(ctx)\n\t\tt.Cleanup(func() { cancel(nil) })\n\n\t\texecutor := baseservice.Init(archetype, &jobExecutor{\n\t\t\tCancelFunc:             cancel,\n\t\t\tClientRetryPolicy:      &retryPolicyNoJitter{},\n\t\t\tCompleter:              bundle.completer,\n\t\t\tErrorHandler:           bundle.errorHandler,\n\t\t\tInformProducerDoneFunc: func(job *rivertype.JobRow) {},\n\t\t\tJobRow:                 bundle.jobRow,\n\t\t\tSchedulerInterval:      riverinternaltest.SchedulerShortInterval,\n\t\t\tWorkUnit:               workUnitFactory.MakeUnit(bundle.jobRow),\n\t\t})\n\n\t\treturn executor, bundle\n\t}\n\n\tt.Run(\"Success\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\t// A simple `return nil` occasionally clocks in at exactly 0s of run\n\t\t// duration and fails the assertion on non-zero below. To avoid that,\n\t\t// make sure we sleep a tiny amount of time.\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error {\n\t\t\ttime.Sleep(1 * time.Microsecond)\n\t\t\treturn nil\n\t\t}, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\tjobUpdates := riversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, job.State)\n\n\t\trequire.Len(t, jobUpdates, 1)\n\t\tjobUpdate := jobUpdates[0]\n\t\tt.Logf(\"Job statistics: %+v\", jobUpdate.JobStats)\n\t\trequire.NotZero(t, jobUpdate.JobStats.CompleteDuration)\n\t\trequire.NotZero(t, jobUpdate.JobStats.QueueWaitDuration)\n\t\trequire.NotZero(t, jobUpdate.JobStats.RunDuration)\n\n\t\tselect {\n\t\tcase <-bundle.updateCh:\n\t\t\tt.Fatalf(\"unexpected job update: %+v\", jobUpdate)\n\t\tdefault:\n\t\t}\n\t})\n\n\tt.Run(\"FirstError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tnow := executor.Archetype.Time.StubNowUTC(time.Now().UTC())\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t\trequire.Len(t, job.Errors, 1)\n\t\trequire.Equal(t, now.Truncate(1*time.Microsecond), job.Errors[0].At.Truncate(1*time.Microsecond))\n\t\trequire.Equal(t, 1, job.Errors[0].Attempt)\n\t\trequire.Equal(t, \"job error\", job.Errors[0].Error)\n\t\trequire.Equal(t, \"\", job.Errors[0].Trace)\n\t})\n\n\tt.Run(\"ErrorAgainAfterRetry\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tbundle.jobRow.Attempt = 2\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t})\n\n\tt.Run(\"ErrorSetsJobAvailableBelowSchedulerIntervalThreshold\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\texecutor.SchedulerInterval = 3 * time.Second\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\t{\n\t\t\texecutor.Execute(ctx)\n\t\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\t\trequire.Equal(t, rivertype.JobStateAvailable, job.State)\n\t\t}\n\n\t\t_, err := bundle.exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{\n\t\t\tID:            bundle.jobRow.ID,\n\t\t\tStateDoUpdate: true,\n\t\t\tState:         rivertype.JobStateRunning,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tbundle.jobRow.Attempt = 2\n\n\t\t{\n\t\t\texecutor.Execute(ctx)\n\t\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 16*time.Second)\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t\t}\n\t})\n\n\tt.Run(\"ErrorDiscardsJobAfterTooManyAttempts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tbundle.jobRow.Attempt = bundle.jobRow.MaxAttempts\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), *job.FinalizedAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateDiscarded, job.State)\n\t})\n\n\tt.Run(\"JobCancelErrorCancelsJobEvenWithRemainingAttempts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\t// ensure we still have remaining attempts:\n\t\trequire.Greater(t, bundle.jobRow.MaxAttempts, bundle.jobRow.Attempt)\n\n\t\t// add a unique key so we can verify it's cleared\n\t\tvar err error\n\t\tbundle.jobRow, err = bundle.exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{\n\t\t\tID:    bundle.jobRow.ID,\n\t\t\tState: rivertype.JobStateAvailable, // required for encoding but ignored\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tcancelErr := JobCancel(errors.New(\"throw away this job\"))\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, job.State)\n\t\trequire.Nil(t, job.UniqueKey)\n\t\trequire.Len(t, job.Errors, 1)\n\t\trequire.WithinDuration(t, time.Now(), job.Errors[0].At, 2*time.Second)\n\t\trequire.Equal(t, 1, job.Errors[0].Attempt)\n\t\trequire.Equal(t, \"JobCancelError: throw away this job\", job.Errors[0].Error)\n\t\trequire.Equal(t, \"\", job.Errors[0].Trace)\n\t})\n\n\tt.Run(\"JobSnoozeErrorReschedulesJobAndIncrementsMaxAttempts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\t\tmaxAttemptsBefore := bundle.jobRow.MaxAttempts\n\n\t\tcancelErr := JobSnooze(30 * time.Minute)\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, job.State)\n\t\trequire.WithinDuration(t, time.Now().Add(30*time.Minute), job.ScheduledAt, 2*time.Second)\n\t\trequire.Equal(t, maxAttemptsBefore+1, job.MaxAttempts)\n\t\trequire.Empty(t, job.Errors)\n\t})\n\n\tt.Run(\"JobSnoozeErrorInNearFutureMakesJobAvailableAndIncrementsMaxAttempts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\t\tmaxAttemptsBefore := bundle.jobRow.MaxAttempts\n\n\t\tcancelErr := JobSnooze(time.Millisecond)\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateAvailable, job.State)\n\t\trequire.WithinDuration(t, time.Now(), job.ScheduledAt, 2*time.Second)\n\t\trequire.Equal(t, maxAttemptsBefore+1, job.MaxAttempts)\n\t\trequire.Empty(t, job.Errors)\n\t})\n\n\tt.Run(\"ErrorWithCustomRetryPolicy\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\t\texecutor.ClientRetryPolicy = &retryPolicyCustom{}\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t})\n\n\tt.Run(\"ErrorWithCustomNextRetryReturnedFromWorker\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\tnextRetryAt := time.Now().Add(1 * time.Hour).UTC()\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, func() time.Time {\n\t\t\treturn nextRetryAt\n\t\t}).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t\trequire.WithinDuration(t, nextRetryAt, job.ScheduledAt, time.Microsecond)\n\t})\n\n\tt.Run(\"InvalidNextRetryAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\t\texecutor.ClientRetryPolicy = &retryPolicyInvalid{}\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, (&DefaultClientRetryPolicy{}).NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t})\n\n\tt.Run(\"ErrorWithErrorHandler\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\t\t\trequire.Equal(t, workerErr, err)\n\t\t\treturn nil\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandleErrorCalled)\n\t})\n\n\tt.Run(\"ErrorWithErrorHandlerSetCancelled\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\t\t\treturn &ErrorHandlerResult{SetCancelled: true}\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandleErrorCalled)\n\t})\n\n\tt.Run(\"ErrorWithErrorHandlerPanic\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tworkerErr := errors.New(\"job error\")\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {\n\t\t\tpanic(\"error handled panicked!\")\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandleErrorCalled)\n\t})\n\n\tt.Run(\"Panic\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t\trequire.Len(t, job.Errors, 1)\n\t\t// Sufficient enough to ensure that the stack trace is included:\n\t\trequire.Contains(t, job.Errors[0].Trace, \"river/job_executor.go\")\n\t})\n\n\tt.Run(\"PanicAgainAfterRetry\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tbundle.jobRow.Attempt = 2\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t})\n\n\tt.Run(\"PanicDiscardsJobAfterTooManyAttempts\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\tbundle.jobRow.Attempt = bundle.jobRow.MaxAttempts\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, time.Now(), *job.FinalizedAt, 1*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateDiscarded, job.State)\n\t})\n\n\tt.Run(\"PanicWithPanicHandler\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\t\t\trequire.Equal(t, \"panic val\", panicVal)\n\t\t\trequire.Contains(t, trace, \"runtime/debug.Stack()\\n\")\n\n\t\t\treturn nil\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandlePanicCalled)\n\t})\n\n\tt.Run(\"PanicWithPanicHandlerSetCancelled\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\t\t\treturn &ErrorHandlerResult{SetCancelled: true}\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandlePanicCalled)\n\t})\n\n\tt.Run(\"PanicWithPanicHandlerPanic\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic(\"panic val\") }, nil).MakeUnit(bundle.jobRow)\n\t\tbundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {\n\t\t\tpanic(\"panic handler panicked!\")\n\t\t}\n\n\t\texecutor.Execute(ctx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjob, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\n\t\trequire.True(t, bundle.errorHandler.HandlePanicCalled)\n\t})\n\n\tt.Run(\"CancelFuncCleanedUpEvenWithoutCancel\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\texecutor, bundle := setup(t)\n\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return nil }, nil).MakeUnit(bundle.jobRow)\n\n\t\tworkCtx, cancelFunc := context.WithCancelCause(ctx)\n\t\texecutor.CancelFunc = cancelFunc\n\n\t\texecutor.Execute(workCtx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\trequire.ErrorIs(t, context.Cause(workCtx), errExecutorDefaultCancel)\n\t})\n\n\trunCancelTest := func(t *testing.T, returnErr error) *rivertype.JobRow { //nolint:thelper\n\t\texecutor, bundle := setup(t)\n\n\t\t// ensure we still have remaining attempts:\n\t\trequire.Greater(t, bundle.jobRow.MaxAttempts, bundle.jobRow.Attempt)\n\n\t\tjobStarted := make(chan struct{})\n\t\thaveCancelled := make(chan struct{})\n\t\texecutor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error {\n\t\t\tclose(jobStarted)\n\t\t\t<-haveCancelled\n\t\t\treturn returnErr\n\t\t}, nil).MakeUnit(bundle.jobRow)\n\n\t\tgo func() {\n\t\t\t<-jobStarted\n\t\t\texecutor.Cancel()\n\t\t\tclose(haveCancelled)\n\t\t}()\n\n\t\tworkCtx, cancelFunc := context.WithCancelCause(ctx)\n\t\texecutor.CancelFunc = cancelFunc\n\t\tt.Cleanup(func() { cancelFunc(nil) })\n\n\t\texecutor.Execute(workCtx)\n\t\triversharedtest.WaitOrTimeout(t, bundle.updateCh)\n\n\t\tjobRow, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)\n\t\trequire.NoError(t, err)\n\t\treturn jobRow\n\t}\n\n\tt.Run(\"RemoteCancellationViaCancel\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tjob := runCancelTest(t, errors.New(\"a non-nil error\"))\n\n\t\trequire.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateCancelled, job.State)\n\t\trequire.Len(t, job.Errors, 1)\n\t\trequire.WithinDuration(t, time.Now(), job.Errors[0].At, 2*time.Second)\n\t\trequire.Equal(t, 1, job.Errors[0].Attempt)\n\t\trequire.Equal(t, \"JobCancelError: job cancelled remotely\", job.Errors[0].Error)\n\t\trequire.Equal(t, ErrJobCancelledRemotely.Error(), job.Errors[0].Error)\n\t\trequire.Equal(t, \"\", job.Errors[0].Trace)\n\t})\n\n\tt.Run(\"RemoteCancellationJobNotCancelledIfNoErrorReturned\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tjob := runCancelTest(t, nil)\n\n\t\trequire.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, job.State)\n\t\trequire.Empty(t, job.Errors)\n\t})\n}\n\nfunc TestUnknownJobKindError_As(t *testing.T) {\n\t// This test isn't really necessary because we didn't have to write any code\n\t// to make it pass, but it does demonstrate that we can successfully use\n\t// errors.As with this custom error type.\n\tt.Parallel()\n\n\tt.Run(\"ReturnsTrueForAnotherUnregisteredKindError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\terr1 := &UnknownJobKindError{Kind: \"MyJobArgs\"}\n\t\tvar err2 *UnknownJobKindError\n\t\trequire.ErrorAs(t, err1, &err2)\n\t\trequire.Equal(t, err1, err2)\n\t\trequire.Equal(t, err1.Kind, err2.Kind)\n\t})\n\n\tt.Run(\"ReturnsFalseForADifferentError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tvar err *UnknownJobKindError\n\t\trequire.False(t, errors.As(errors.New(\"some other error\"), &err))\n\t})\n}\n\nfunc TestUnknownJobKindError_Is(t *testing.T) {\n\tt.Parallel()\n\n\tt.Run(\"ReturnsTrueForAnotherUnregisteredKindError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\terr1 := &UnknownJobKindError{Kind: \"MyJobArgs\"}\n\t\trequire.ErrorIs(t, err1, &UnknownJobKindError{})\n\t})\n\n\tt.Run(\"ReturnsFalseForADifferentError\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\terr1 := &UnknownJobKindError{Kind: \"MyJobArgs\"}\n\t\trequire.NotErrorIs(t, err1, errors.New(\"some other error\"))\n\t})\n}\n\nfunc TestJobCancel(t *testing.T) {\n\tt.Parallel()\n\n\tt.Run(\"ErrorsIsReturnsTrueForAnotherErrorOfSameType\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\terr1 := JobCancel(errors.New(\"some message\"))\n\t\trequire.ErrorIs(t, err1, JobCancel(errors.New(\"another message\")))\n\t})\n\n\tt.Run(\"ErrorsIsReturnsFalseForADifferentErrorType\", func(t *testing.T) {\n\t\tt.Parallel()\n\t\terr1 := JobCancel(errors.New(\"some message\"))\n\t\trequire.NotErrorIs(t, err1, &UnknownJobKindError{Kind: \"MyJobArgs\"})\n\t})\n}\n"
        },
        {
          "name": "job_list_params.go",
          "type": "blob",
          "size": 12.953125,
          "content": "package river\n\nimport (\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/dblist\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// JobListCursor is used to specify a starting point for a paginated\n// job list query.\ntype JobListCursor struct {\n\tid        int64\n\tjob       *rivertype.JobRow // used for JobListCursorFromJob path; not serialized\n\tkind      string\n\tqueue     string\n\tsortField JobListOrderByField\n\ttime      time.Time // may be empty\n}\n\n// JobListCursorFromJob creates a JobListCursor from a JobRow.\nfunc JobListCursorFromJob(job *rivertype.JobRow) *JobListCursor {\n\t// Other fields are initialized when the cursor is used in After below.\n\treturn &JobListCursor{job: job}\n}\n\nfunc jobListCursorFromJobAndParams(job *rivertype.JobRow, listParams *JobListParams) *JobListCursor {\n\t// A pointer so that we can detect a condition where we accidentally left\n\t// this value unset.\n\tvar cursorTime *time.Time\n\n\t// Don't include a `default` so `exhaustive` lint can detect omissions.\n\tswitch listParams.sortField {\n\tcase JobListOrderByID:\n\t\tcursorTime = ptrutil.Ptr(time.Time{})\n\tcase JobListOrderByTime:\n\t\tcursorTime = ptrutil.Ptr(jobListTimeValue(job))\n\tcase JobListOrderByFinalizedAt:\n\t\tif job.FinalizedAt != nil {\n\t\t\tcursorTime = job.FinalizedAt\n\t\t}\n\tcase JobListOrderByScheduledAt:\n\t\tcursorTime = &job.ScheduledAt\n\t}\n\n\tif cursorTime == nil {\n\t\tpanic(\"invalid sort field\")\n\t}\n\n\treturn &JobListCursor{\n\t\tid:        job.ID,\n\t\tkind:      job.Kind,\n\t\tqueue:     job.Queue,\n\t\tsortField: listParams.sortField,\n\t\ttime:      *cursorTime,\n\t}\n}\n\n// UnmarshalText implements encoding.TextUnmarshaler to decode the cursor from\n// a previously marshaled string.\nfunc (c *JobListCursor) UnmarshalText(text []byte) error {\n\tdst := make([]byte, base64.StdEncoding.DecodedLen(len(text)))\n\tn, err := base64.StdEncoding.Decode(dst, text)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdst = dst[:n]\n\n\twrapperValue := jobListPaginationCursorJSON{}\n\tif err := json.Unmarshal(dst, &wrapperValue); err != nil {\n\t\treturn err\n\t}\n\t*c = JobListCursor{\n\t\tid:        wrapperValue.ID,\n\t\tkind:      wrapperValue.Kind,\n\t\tqueue:     wrapperValue.Queue,\n\t\tsortField: JobListOrderByField(wrapperValue.SortField),\n\t\ttime:      wrapperValue.Time,\n\t}\n\treturn nil\n}\n\n// MarshalText implements encoding.TextMarshaler to encode the cursor as an\n// opaque string.\nfunc (c JobListCursor) MarshalText() ([]byte, error) {\n\tif c.job != nil {\n\t\treturn nil, errors.New(\"cursor initialized with only a job can't be marshaled; try a cursor from JobListResult instead\")\n\t}\n\n\twrapperValue := jobListPaginationCursorJSON{\n\t\tID:        c.id,\n\t\tKind:      c.kind,\n\t\tQueue:     c.queue,\n\t\tSortField: string(c.sortField),\n\t\tTime:      c.time,\n\t}\n\tdata, err := json.Marshal(wrapperValue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdst := make([]byte, base64.URLEncoding.EncodedLen(len(data)))\n\tbase64.URLEncoding.Encode(dst, data)\n\treturn dst, nil\n}\n\ntype jobListPaginationCursorJSON struct {\n\tID        int64     `json:\"id\"`\n\tKind      string    `json:\"kind\"`\n\tQueue     string    `json:\"queue\"`\n\tSortField string    `json:\"sort_field\"`\n\tTime      time.Time `json:\"time\"`\n}\n\n// SortOrder specifies the direction of a sort.\ntype SortOrder int\n\nconst (\n\t// SortOrderAsc specifies that the sort should in ascending order.\n\tSortOrderAsc SortOrder = iota\n\n\t// SortOrderDesc specifies that the sort should in descending order.\n\tSortOrderDesc\n)\n\n// JobListOrderByField specifies the field to sort by.\ntype JobListOrderByField string\n\nconst (\n\t// JobListOrderByID specifies that the sort should be by job ID.\n\tJobListOrderByID JobListOrderByField = \"id\"\n\n\t// JobListOrderByFinalizedAt specifies that the sort should be by\n\t// `finalized_at`.\n\t//\n\t// This option must be used in conjunction with filtering by only finalized\n\t// job states.\n\tJobListOrderByFinalizedAt JobListOrderByField = \"finalized_at\"\n\n\t// JobListOrderByScheduledAt specifies that the sort should be by\n\t// `scheduled_at`.\n\tJobListOrderByScheduledAt JobListOrderByField = \"scheduled_at\"\n\n\t// JobListOrderByTime specifies that the sort should be by the \"best fit\"\n\t// time field based on listed state. The best fit is determined by looking\n\t// at the first value given to JobListParams.States. If multiple states are\n\t// specified, the ones after the first will be ignored.\n\t//\n\t// The specific time field used for sorting depends on requested state:\n\t//\n\t// * States `available`, `retryable`, or `scheduled` use `scheduled_at`.\n\t// * State `running` uses `attempted_at`.\n\t// * States `cancelled`, `completed`, or `discarded` use `finalized_at`.\n\tJobListOrderByTime JobListOrderByField = \"time\"\n)\n\n// JobListParams specifies the parameters for a JobList query. It must be\n// initialized with NewJobListParams. Params can be built by chaining methods on\n// the JobListParams object:\n//\n//\tparams := NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).First(100)\ntype JobListParams struct {\n\tafter            *JobListCursor\n\tkinds            []string\n\tmetadataFragment string\n\toverrodeState    bool\n\tpaginationCount  int32\n\tqueues           []string\n\tsortField        JobListOrderByField\n\tsortOrder        SortOrder\n\tstates           []rivertype.JobState\n}\n\n// NewJobListParams creates a new JobListParams to return available jobs sorted\n// by time in ascending order, returning 100 jobs at most.\nfunc NewJobListParams() *JobListParams {\n\treturn &JobListParams{\n\t\tpaginationCount: 100,\n\t\tsortField:       JobListOrderByID,\n\t\tsortOrder:       SortOrderAsc,\n\t\tstates: []rivertype.JobState{\n\t\t\trivertype.JobStateAvailable,\n\t\t\trivertype.JobStateCancelled,\n\t\t\trivertype.JobStateCompleted,\n\t\t\trivertype.JobStateDiscarded,\n\t\t\trivertype.JobStatePending,\n\t\t\trivertype.JobStateRetryable,\n\t\t\trivertype.JobStateRunning,\n\t\t\trivertype.JobStateScheduled,\n\t\t},\n\t}\n}\n\nfunc (p *JobListParams) copy() *JobListParams {\n\treturn &JobListParams{\n\t\tafter:            p.after,\n\t\tkinds:            append([]string(nil), p.kinds...),\n\t\tmetadataFragment: p.metadataFragment,\n\t\toverrodeState:    p.overrodeState,\n\t\tpaginationCount:  p.paginationCount,\n\t\tqueues:           append([]string(nil), p.queues...),\n\t\tsortField:        p.sortField,\n\t\tsortOrder:        p.sortOrder,\n\t\tstates:           append([]rivertype.JobState(nil), p.states...),\n\t}\n}\n\nfunc (p *JobListParams) toDBParams() (*dblist.JobListParams, error) {\n\tconditionsBuilder := &strings.Builder{}\n\tconditions := make([]string, 0, 10)\n\tnamedArgs := make(map[string]any)\n\torderBy := make([]dblist.JobListOrderBy, 0, 2)\n\n\tvar sortOrder dblist.SortOrder\n\tswitch p.sortOrder {\n\tcase SortOrderAsc:\n\t\tsortOrder = dblist.SortOrderAsc\n\tcase SortOrderDesc:\n\t\tsortOrder = dblist.SortOrderDesc\n\tdefault:\n\t\treturn nil, errors.New(\"invalid sort order\")\n\t}\n\n\tif p.sortField == JobListOrderByFinalizedAt {\n\t\tcurrentNonFinalizedStates := make([]rivertype.JobState, 0, len(p.states))\n\t\tfor _, state := range p.states {\n\t\t\t//nolint:exhaustive\n\t\t\tswitch state {\n\t\t\tcase rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:\n\t\t\tdefault:\n\t\t\t\tcurrentNonFinalizedStates = append(currentNonFinalizedStates, state)\n\t\t\t}\n\t\t}\n\t\t// This indicates the user overrode the States list with only non-finalized\n\t\t// states prior to then requesting FinalizedAt ordering.\n\t\tif len(currentNonFinalizedStates) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"cannot order by finalized_at with non-finalized state filters %+v\", currentNonFinalizedStates)\n\t\t}\n\t}\n\n\tvar timeField string\n\tswitch {\n\tcase p.sortField == JobListOrderByID:\n\t\t// no time field\n\n\tcase len(p.states) > 0 && p.sortField == JobListOrderByTime:\n\t\ttimeField = jobListTimeFieldForState(p.states[0])\n\t\torderBy = append(orderBy, dblist.JobListOrderBy{Expr: timeField, Order: sortOrder})\n\n\tdefault:\n\t\ttimeField = string(p.sortField)\n\t\torderBy = append(orderBy, dblist.JobListOrderBy{Expr: timeField, Order: sortOrder})\n\t}\n\n\torderBy = append(orderBy, dblist.JobListOrderBy{Expr: \"id\", Order: sortOrder})\n\n\tif p.metadataFragment != \"\" {\n\t\tconditions = append(conditions, `metadata @> @metadata_fragment::jsonb`)\n\t\tnamedArgs[\"metadata_fragment\"] = p.metadataFragment\n\t}\n\n\tif p.after != nil {\n\t\tif p.after.time.IsZero() { // order by ID only\n\t\t\tif sortOrder == dblist.SortOrderAsc {\n\t\t\t\tconditions = append(conditions, \"(id > @after_id)\")\n\t\t\t} else {\n\t\t\t\tconditions = append(conditions, \"(id < @after_id)\")\n\t\t\t}\n\t\t} else {\n\t\t\tif sortOrder == dblist.SortOrderAsc {\n\t\t\t\tconditions = append(conditions, fmt.Sprintf(`(\"%s\" > @cursor_time OR (\"%s\" = @cursor_time AND \"id\" > @after_id))`, timeField, timeField))\n\t\t\t} else {\n\t\t\t\tconditions = append(conditions, fmt.Sprintf(`(\"%s\" < @cursor_time OR (\"%s\" = @cursor_time AND \"id\" < @after_id))`, timeField, timeField))\n\t\t\t}\n\t\t\tnamedArgs[\"cursor_time\"] = p.after.time\n\t\t}\n\t\tnamedArgs[\"after_id\"] = p.after.id\n\t}\n\n\tfor i, condition := range conditions {\n\t\tif i > 0 {\n\t\t\tconditionsBuilder.WriteString(\"\\n  AND \")\n\t\t}\n\t\tconditionsBuilder.WriteString(condition)\n\t}\n\n\tdbParams := &dblist.JobListParams{\n\t\tConditions: conditionsBuilder.String(),\n\t\tKinds:      p.kinds,\n\t\tLimitCount: p.paginationCount,\n\t\tNamedArgs:  namedArgs,\n\t\tOrderBy:    orderBy,\n\t\tPriorities: nil,\n\t\tQueues:     p.queues,\n\t\tStates:     p.states,\n\t}\n\n\treturn dbParams, nil\n}\n\n// After returns an updated filter set that will only return jobs\n// after the given cursor.\nfunc (p *JobListParams) After(cursor *JobListCursor) *JobListParams {\n\tparamsCopy := p.copy()\n\tif cursor.job == nil {\n\t\tparamsCopy.after = cursor\n\t} else {\n\t\tparamsCopy.after = jobListCursorFromJobAndParams(cursor.job, paramsCopy)\n\t}\n\treturn paramsCopy\n}\n\n// First returns an updated filter set that will only return the first\n// count jobs.\n//\n// Count must be between 1 and 10000, inclusive, or this will panic.\nfunc (p *JobListParams) First(count int) *JobListParams {\n\tif count <= 0 {\n\t\tpanic(\"count must be > 0\")\n\t}\n\tif count > 10000 {\n\t\tpanic(\"count must be <= 10000\")\n\t}\n\tparamsCopy := p.copy()\n\tparamsCopy.paginationCount = int32(count)\n\treturn paramsCopy\n}\n\n// Kinds returns an updated filter set that will only return jobs of the given\n// kinds.\nfunc (p *JobListParams) Kinds(kinds ...string) *JobListParams {\n\tparamsCopy := p.copy()\n\tparamsCopy.kinds = make([]string, len(kinds))\n\tcopy(paramsCopy.kinds, kinds)\n\treturn paramsCopy\n}\n\nfunc (p *JobListParams) Metadata(json string) *JobListParams {\n\tparamsCopy := p.copy()\n\tparamsCopy.metadataFragment = json\n\treturn paramsCopy\n}\n\n// Queues returns an updated filter set that will only return jobs from the\n// given queues.\nfunc (p *JobListParams) Queues(queues ...string) *JobListParams {\n\tparamsCopy := p.copy()\n\tparamsCopy.queues = make([]string, len(queues))\n\tcopy(paramsCopy.queues, queues)\n\treturn paramsCopy\n}\n\n// OrderBy returns an updated filter set that will sort the results using the\n// specified field and direction.\n//\n// If ordering by FinalizedAt, the States filter will be set to only include\n// finalized job states unless it has already been overridden.\nfunc (p *JobListParams) OrderBy(field JobListOrderByField, direction SortOrder) *JobListParams {\n\tparamsCopy := p.copy()\n\tswitch field {\n\tcase JobListOrderByID, JobListOrderByTime, JobListOrderByScheduledAt:\n\t\tparamsCopy.sortField = field\n\tcase JobListOrderByFinalizedAt:\n\t\tparamsCopy.sortField = field\n\t\tif !p.overrodeState {\n\t\t\tparamsCopy.states = []rivertype.JobState{\n\t\t\t\trivertype.JobStateCancelled,\n\t\t\t\trivertype.JobStateCompleted,\n\t\t\t\trivertype.JobStateDiscarded,\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tpanic(\"invalid order by field\")\n\t}\n\tparamsCopy.sortField = field\n\tparamsCopy.sortOrder = direction\n\treturn paramsCopy\n}\n\n// States returns an updated filter set that will only return jobs in the given\n// states.\nfunc (p *JobListParams) States(states ...rivertype.JobState) *JobListParams {\n\tparamsCopy := p.copy()\n\tparamsCopy.states = make([]rivertype.JobState, len(states))\n\tparamsCopy.overrodeState = true\n\tcopy(paramsCopy.states, states)\n\treturn paramsCopy\n}\n\nfunc jobListTimeFieldForState(state rivertype.JobState) string {\n\t// Don't include a `default` so `exhaustive` lint can detect omissions.\n\tswitch state {\n\tcase rivertype.JobStateAvailable, rivertype.JobStatePending, rivertype.JobStateRetryable, rivertype.JobStateScheduled:\n\t\treturn \"scheduled_at\"\n\tcase rivertype.JobStateRunning:\n\t\treturn \"attempted_at\"\n\tcase rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:\n\t\treturn \"finalized_at\"\n\t}\n\n\treturn \"created_at\" // should never happen\n}\n\nfunc jobListTimeValue(job *rivertype.JobRow) time.Time {\n\t// Don't include a `default` so `exhaustive` lint can detect omissions.\n\tswitch job.State {\n\tcase rivertype.JobStateAvailable, rivertype.JobStatePending, rivertype.JobStateRetryable, rivertype.JobStateScheduled:\n\t\treturn job.ScheduledAt\n\n\tcase rivertype.JobStateRunning:\n\t\tif job.AttemptedAt == nil {\n\t\t\t// This should never happen unless a job has been manually manipulated.\n\t\t\treturn job.CreatedAt\n\t\t}\n\t\treturn *job.AttemptedAt\n\n\tcase rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:\n\t\tif job.FinalizedAt == nil {\n\t\t\t// This should never happen unless a job has been manually manipulated.\n\t\t\treturn job.CreatedAt\n\t\t}\n\t\treturn *job.FinalizedAt\n\t}\n\n\treturn job.CreatedAt // should never happen\n}\n"
        },
        {
          "name": "job_list_params_test.go",
          "type": "blob",
          "size": 5.2109375,
          "content": "package river\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc Test_JobListCursor_JobListCursorFromJob(t *testing.T) {\n\tt.Parallel()\n\n\tjobRow := &rivertype.JobRow{\n\t\tID:    4,\n\t\tKind:  \"test\",\n\t\tQueue: \"test\",\n\t\tState: rivertype.JobStateRunning,\n\t}\n\n\tcursor := JobListCursorFromJob(jobRow)\n\trequire.Zero(t, cursor.id)\n\trequire.Equal(t, jobRow, cursor.job)\n\trequire.Zero(t, cursor.kind)\n\trequire.Zero(t, cursor.queue)\n\trequire.Zero(t, cursor.sortField)\n\trequire.Zero(t, cursor.time)\n}\n\nfunc Test_JobListCursor_jobListCursorFromJobAndParams(t *testing.T) {\n\tt.Parallel()\n\n\tt.Run(\"OrderByID\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tjobRow := &rivertype.JobRow{\n\t\t\tID:    4,\n\t\t\tKind:  \"test\",\n\t\t\tQueue: \"test\",\n\t\t\tState: rivertype.JobStateRunning,\n\t\t}\n\n\t\tcursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().After(JobListCursorFromJob(jobRow)))\n\t\trequire.Equal(t, jobRow.ID, cursor.id)\n\t\trequire.Equal(t, jobRow.Kind, cursor.kind)\n\t\trequire.Equal(t, jobRow.Queue, cursor.queue)\n\t\trequire.Zero(t, cursor.time)\n\t})\n\n\tfor i, state := range []rivertype.JobState{\n\t\trivertype.JobStateAvailable,\n\t\trivertype.JobStateRetryable,\n\t\trivertype.JobStateScheduled,\n\t} {\n\t\ti, state := i, state\n\n\t\tt.Run(fmt.Sprintf(\"OrderByTimeScheduledAtUsedFor%sJob\", state), func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\tnow := time.Now().UTC()\n\t\t\tjobRow := &rivertype.JobRow{\n\t\t\t\tCreatedAt:   now.Add(-11 * time.Second),\n\t\t\t\tID:          int64(i),\n\t\t\t\tKind:        \"test_kind\",\n\t\t\t\tQueue:       \"test_queue\",\n\t\t\t\tState:       state,\n\t\t\t\tScheduledAt: now.Add(-10 * time.Second),\n\t\t\t}\n\n\t\t\tcursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))\n\t\t\trequire.Equal(t, jobRow.ID, cursor.id)\n\t\t\trequire.Equal(t, jobRow.Kind, cursor.kind)\n\t\t\trequire.Equal(t, jobRow.Queue, cursor.queue)\n\t\t\trequire.Equal(t, jobRow.ScheduledAt, cursor.time)\n\t\t})\n\t}\n\n\tfor i, state := range []rivertype.JobState{\n\t\trivertype.JobStateCancelled,\n\t\trivertype.JobStateCompleted,\n\t\trivertype.JobStateDiscarded,\n\t} {\n\t\ti, state := i, state\n\n\t\tt.Run(fmt.Sprintf(\"OrderByTimeFinalizedAtUsedFor%sJob\", state), func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\tnow := time.Now().UTC()\n\t\t\tjobRow := &rivertype.JobRow{\n\t\t\t\tAttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second)),\n\t\t\t\tCreatedAt:   now.Add(-11 * time.Second),\n\t\t\t\tFinalizedAt: ptrutil.Ptr(now.Add(-1 * time.Second)),\n\t\t\t\tID:          int64(i),\n\t\t\t\tKind:        \"test_kind\",\n\t\t\t\tQueue:       \"test_queue\",\n\t\t\t\tState:       state,\n\t\t\t\tScheduledAt: now.Add(-10 * time.Second),\n\t\t\t}\n\n\t\t\tcursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))\n\t\t\trequire.Equal(t, jobRow.ID, cursor.id)\n\t\t\trequire.Equal(t, jobRow.Kind, cursor.kind)\n\t\t\trequire.Equal(t, jobRow.Queue, cursor.queue)\n\t\t\trequire.Equal(t, *jobRow.FinalizedAt, cursor.time)\n\t\t})\n\t}\n\n\tt.Run(\"OrderByTimeRunningJobUsesAttemptedAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tnow := time.Now().UTC()\n\t\tjobRow := &rivertype.JobRow{\n\t\t\tAttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second)),\n\t\t\tCreatedAt:   now.Add(-11 * time.Second),\n\t\t\tID:          4,\n\t\t\tKind:        \"test\",\n\t\t\tQueue:       \"test\",\n\t\t\tState:       rivertype.JobStateRunning,\n\t\t\tScheduledAt: now.Add(-10 * time.Second),\n\t\t}\n\n\t\tcursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))\n\t\trequire.Equal(t, jobRow.ID, cursor.id)\n\t\trequire.Equal(t, jobRow.Kind, cursor.kind)\n\t\trequire.Equal(t, jobRow.Queue, cursor.queue)\n\t\trequire.Equal(t, *jobRow.AttemptedAt, cursor.time)\n\t})\n\n\tt.Run(\"OrderByTimeUnknownJobStateUsesCreatedAt\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tnow := time.Now().UTC()\n\t\tjobRow := &rivertype.JobRow{\n\t\t\tCreatedAt:   now.Add(-11 * time.Second),\n\t\t\tID:          4,\n\t\t\tKind:        \"test\",\n\t\t\tQueue:       \"test\",\n\t\t\tState:       rivertype.JobState(\"unknown_fake_state\"),\n\t\t\tScheduledAt: now.Add(-10 * time.Second),\n\t\t}\n\n\t\tcursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))\n\t\trequire.Equal(t, jobRow.ID, cursor.id)\n\t\trequire.Equal(t, jobRow.Kind, cursor.kind)\n\t\trequire.Equal(t, jobRow.Queue, cursor.queue)\n\t\trequire.Equal(t, jobRow.CreatedAt, cursor.time)\n\t})\n}\n\nfunc Test_JobListCursor_MarshalJSON(t *testing.T) {\n\tt.Parallel()\n\n\tt.Run(\"CanMarshalAndUnmarshal\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tnow := time.Now().UTC()\n\t\tcursor := &JobListCursor{\n\t\t\tid:    4,\n\t\t\tkind:  \"test_kind\",\n\t\t\tqueue: \"test_queue\",\n\t\t\ttime:  now,\n\t\t}\n\n\t\ttext, err := json.Marshal(cursor)\n\t\trequire.NoError(t, err)\n\t\trequire.NotEqual(t, \"\", text)\n\n\t\tunmarshaledParams := &JobListCursor{}\n\t\trequire.NoError(t, json.Unmarshal(text, unmarshaledParams))\n\n\t\trequire.Equal(t, cursor, unmarshaledParams)\n\t})\n\n\tt.Run(\"ErrorsOnJobOnlyCursor\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tjobRow := &rivertype.JobRow{\n\t\t\tID:    4,\n\t\t\tKind:  \"test\",\n\t\t\tQueue: \"test\",\n\t\t\tState: rivertype.JobStateRunning,\n\t\t}\n\n\t\tcursor := JobListCursorFromJob(jobRow)\n\n\t\t_, err := json.Marshal(cursor)\n\t\trequire.EqualError(t, err, \"json: error calling MarshalText for type *river.JobListCursor: cursor initialized with only a job can't be marshaled; try a cursor from JobListResult instead\")\n\t})\n}\n"
        },
        {
          "name": "job_test.go",
          "type": "blob",
          "size": 0.5126953125,
          "content": "package river\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc TestUniqueOpts_isEmpty(t *testing.T) {\n\tt.Parallel()\n\n\trequire.True(t, (&UniqueOpts{}).isEmpty())\n\trequire.False(t, (&UniqueOpts{ByArgs: true}).isEmpty())\n\trequire.False(t, (&UniqueOpts{ByPeriod: 1 * time.Nanosecond}).isEmpty())\n\trequire.False(t, (&UniqueOpts{ByQueue: true}).isEmpty())\n\trequire.False(t, (&UniqueOpts{ByState: []rivertype.JobState{rivertype.JobStateAvailable}}).isEmpty())\n}\n"
        },
        {
          "name": "main_test.go",
          "type": "blob",
          "size": 0.16015625,
          "content": "package river\n\nimport (\n\t\"testing\"\n\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n)\n\nfunc TestMain(m *testing.M) {\n\triverinternaltest.WrapTestMain(m)\n}\n"
        },
        {
          "name": "middleware_defaults.go",
          "type": "blob",
          "size": 0.8525390625,
          "content": "package river\n\nimport (\n\t\"context\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// JobInsertMiddlewareDefaults is an embeddable struct that provides default\n// implementations for the rivertype.JobInsertMiddleware. Use of this struct is\n// recommended in case rivertype.JobInsertMiddleware is expanded in the future so that\n// existing code isn't unexpectedly broken during an upgrade.\ntype JobInsertMiddlewareDefaults struct{}\n\nfunc (d *JobInsertMiddlewareDefaults) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {\n\treturn doInner(ctx)\n}\n\ntype WorkerMiddlewareDefaults struct{}\n\nfunc (d *WorkerMiddlewareDefaults) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {\n\treturn doInner(ctx)\n}\n"
        },
        {
          "name": "middleware_defaults_test.go",
          "type": "blob",
          "size": 0.1953125,
          "content": "package river\n\nimport \"github.com/riverqueue/river/rivertype\"\n\nvar (\n\t_ rivertype.JobInsertMiddleware = &JobInsertMiddlewareDefaults{}\n\t_ rivertype.WorkerMiddleware    = &WorkerMiddlewareDefaults{}\n)\n"
        },
        {
          "name": "middleware_test.go",
          "type": "blob",
          "size": 1.1015625,
          "content": "package river\n\nimport (\n\t\"context\"\n\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\ntype overridableJobMiddleware struct {\n\tJobInsertMiddlewareDefaults\n\tWorkerMiddlewareDefaults\n\n\tinsertManyFunc func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error)\n\tworkFunc       func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error\n}\n\nfunc (m *overridableJobMiddleware) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {\n\tif m.insertManyFunc != nil {\n\t\treturn m.insertManyFunc(ctx, manyParams, doInner)\n\t}\n\treturn m.JobInsertMiddlewareDefaults.InsertMany(ctx, manyParams, doInner)\n}\n\nfunc (m *overridableJobMiddleware) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {\n\tif m.workFunc != nil {\n\t\treturn m.workFunc(ctx, job, doInner)\n\t}\n\treturn m.WorkerMiddlewareDefaults.Work(ctx, job, doInner)\n}\n"
        },
        {
          "name": "periodic_job.go",
          "type": "blob",
          "size": 7.8623046875,
          "content": "package river\n\nimport (\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/maintenance\"\n\t\"github.com/riverqueue/river/rivershared/util/sliceutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// PeriodicSchedule is a schedule for a periodic job. Periodic jobs should\n// generally have an interval of at least 1 minute, and never less than one\n// second.\ntype PeriodicSchedule interface {\n\t// Next returns the next time at which the job should be run given the\n\t// current time.\n\tNext(current time.Time) time.Time\n}\n\n// PeriodicJobConstructor is a function that gets called each time the paired\n// PeriodicSchedule is triggered.\n//\n// A constructor must never block. It may return nil to indicate that no job\n// should be inserted.\ntype PeriodicJobConstructor func() (JobArgs, *InsertOpts)\n\n// PeriodicJob is a configuration for a periodic job.\ntype PeriodicJob struct {\n\tconstructorFunc PeriodicJobConstructor\n\topts            *PeriodicJobOpts\n\tscheduleFunc    PeriodicSchedule\n}\n\n// PeriodicJobOpts are options for a periodic job.\ntype PeriodicJobOpts struct {\n\t// RunOnStart can be used to indicate that a periodic job should insert an\n\t// initial job as a new scheduler is started. This can be used as a hedge\n\t// for jobs with longer scheduled durations that may not get to expiry\n\t// before a new scheduler is elected.\n\t//\n\t// RunOnStart also applies when a new periodic job is added dynamically with\n\t// `PeriodicJobs().Add` or `PeriodicJobs().AddMany`. Jobs added this way\n\t// with RunOnStart set to true are inserted once, then continue with their\n\t// normal run schedule.\n\tRunOnStart bool\n}\n\n// NewPeriodicJob returns a new PeriodicJob given a schedule and a constructor\n// function.\n//\n// The schedule returns a time until the next time the periodic job should run.\n// The helper PeriodicInterval is available for jobs that should run on simple,\n// fixed intervals (e.g. every 15 minutes), and a custom schedule or third party\n// cron package can be used for more complex scheduling (see the cron example).\n// The constructor function is invoked each time a periodic job's schedule\n// elapses, returning job arguments to insert along with optional insertion\n// options.\n//\n// The periodic job scheduler is approximate and doesn't guarantee strong\n// durability. It's started by the elected leader in a River cluster, and each\n// periodic job is assigned an initial run time when that occurs. New run times\n// are scheduled each time a job's target run time is reached and a new job\n// inserted. However, each scheduler only retains in-memory state, so anytime a\n// process quits or a new leader is elected, the whole process starts over\n// without regard for the state of the last scheduler. The RunOnStart option\n// can be used as a hedge to make sure that jobs with long run durations are\n// guaranteed to occasionally run.\nfunc NewPeriodicJob(scheduleFunc PeriodicSchedule, constructorFunc PeriodicJobConstructor, opts *PeriodicJobOpts) *PeriodicJob {\n\treturn &PeriodicJob{\n\t\tconstructorFunc: constructorFunc,\n\t\topts:            opts,\n\t\tscheduleFunc:    scheduleFunc,\n\t}\n}\n\ntype periodicIntervalSchedule struct {\n\tinterval time.Duration\n}\n\n// PeriodicInterval returns a simple PeriodicSchedule that runs at the given\n// interval.\nfunc PeriodicInterval(interval time.Duration) PeriodicSchedule {\n\treturn &periodicIntervalSchedule{interval}\n}\n\nfunc (s *periodicIntervalSchedule) Next(t time.Time) time.Time {\n\treturn t.Add(s.interval)\n}\n\n// PeriodicJobBundle is a bundle of currently configured periodic jobs. It's\n// made accessible through Client, where new periodic jobs can be configured,\n// and old ones removed.\ntype PeriodicJobBundle struct {\n\tclientConfig        *Config\n\tperiodicJobEnqueuer *maintenance.PeriodicJobEnqueuer\n}\n\nfunc newPeriodicJobBundle(config *Config, periodicJobEnqueuer *maintenance.PeriodicJobEnqueuer) *PeriodicJobBundle {\n\treturn &PeriodicJobBundle{\n\t\tclientConfig:        config,\n\t\tperiodicJobEnqueuer: periodicJobEnqueuer,\n\t}\n}\n\n// Adds a new periodic job to the client. The job is queued immediately if\n// RunOnStart is enabled, and then scheduled normally.\n//\n// Returns a periodic job handle which can be used to subsequently remove the\n// job if desired.\n//\n// Adding or removing periodic jobs has no effect unless this client is elected\n// leader because only the leader enqueues periodic jobs. To make sure that a\n// new periodic job is fully enabled or disabled, it should be added or removed\n// from _every_ active River client across all processes.\nfunc (b *PeriodicJobBundle) Add(periodicJob *PeriodicJob) rivertype.PeriodicJobHandle {\n\treturn b.periodicJobEnqueuer.Add(b.toInternal(periodicJob))\n}\n\n// AddMany adds many new periodic jobs to the client. The jobs are queued\n// immediately if their RunOnStart is enabled, and then scheduled normally.\n//\n// Returns a periodic job handle which can be used to subsequently remove the\n// job if desired.\n//\n// Adding or removing periodic jobs has no effect unless this client is elected\n// leader because only the leader enqueues periodic jobs. To make sure that a\n// new periodic job is fully enabled or disabled, it should be added or removed\n// from _every_ active River client across all processes.\nfunc (b *PeriodicJobBundle) AddMany(periodicJobs []*PeriodicJob) []rivertype.PeriodicJobHandle {\n\treturn b.periodicJobEnqueuer.AddMany(sliceutil.Map(periodicJobs, b.toInternal))\n}\n\n// Clear clears all periodic jobs, cancelling all scheduled runs.\n//\n// Adding or removing periodic jobs has no effect unless this client is elected\n// leader because only the leader enqueues periodic jobs. To make sure that a\n// new periodic job is fully enabled or disabled, it should be added or removed\n// from _every_ active River client across all processes.\nfunc (b *PeriodicJobBundle) Clear() {\n\tb.periodicJobEnqueuer.Clear()\n}\n\n// Remove removes a periodic job, cancelling all scheduled runs.\n//\n// Requires the use of the periodic job handle that was returned when the job\n// was added.\n//\n// Adding or removing periodic jobs has no effect unless this client is elected\n// leader because only the leader enqueues periodic jobs. To make sure that a\n// new periodic job is fully enabled or disabled, it should be added or removed\n// from _every_ active River client across all processes.\nfunc (b *PeriodicJobBundle) Remove(periodicJobHandle rivertype.PeriodicJobHandle) {\n\tb.periodicJobEnqueuer.Remove(periodicJobHandle)\n}\n\n// RemoveMany removes many periodic jobs, cancelling all scheduled runs.\n//\n// Requires the use of the periodic job handles that were returned when the jobs\n// were added.\n//\n// Adding or removing periodic jobs has no effect unless this client is elected\n// leader because only the leader enqueues periodic jobs. To make sure that a\n// new periodic job is fully enabled or disabled, it should be added or removed\n// from _every_ active River client across all processes.\nfunc (b *PeriodicJobBundle) RemoveMany(periodicJobHandles []rivertype.PeriodicJobHandle) {\n\tb.periodicJobEnqueuer.RemoveMany(periodicJobHandles)\n}\n\n// An empty set of periodic job opts used as a default when none are specified.\nvar periodicJobEmptyOpts PeriodicJobOpts //nolint:gochecknoglobals\n\n// There are two separate periodic job structs so that the top-level River\n// package can expose one while still containing most periodic job logic in a\n// subpackage. This function converts a top-level periodic job struct (used for\n// configuration) to an internal one.\nfunc (b *PeriodicJobBundle) toInternal(periodicJob *PeriodicJob) *maintenance.PeriodicJob {\n\topts := &periodicJobEmptyOpts\n\tif periodicJob.opts != nil {\n\t\topts = periodicJob.opts\n\t}\n\treturn &maintenance.PeriodicJob{\n\t\tConstructorFunc: func() (*rivertype.JobInsertParams, error) {\n\t\t\targs, options := periodicJob.constructorFunc()\n\t\t\tif args == nil {\n\t\t\t\treturn nil, maintenance.ErrNoJobToInsert\n\t\t\t}\n\t\t\treturn insertParamsFromConfigArgsAndOptions(&b.periodicJobEnqueuer.Archetype, b.clientConfig, args, options)\n\t\t},\n\t\tRunOnStart:   opts.RunOnStart,\n\t\tScheduleFunc: periodicJob.scheduleFunc.Next,\n\t}\n}\n"
        },
        {
          "name": "periodic_job_test.go",
          "type": "blob",
          "size": 2.1357421875,
          "content": "package river\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/maintenance\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n)\n\nfunc TestPeriodicJobBundle(t *testing.T) {\n\tt.Parallel()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*PeriodicJobBundle, *testBundle) { //nolint:unparam\n\t\tt.Helper()\n\n\t\tperiodicJobEnqueuer := maintenance.NewPeriodicJobEnqueuer(\n\t\t\triversharedtest.BaseServiceArchetype(t),\n\t\t\t&maintenance.PeriodicJobEnqueuerConfig{},\n\t\t\tnil,\n\t\t)\n\n\t\treturn newPeriodicJobBundle(newTestConfig(t, nil), periodicJobEnqueuer), &testBundle{}\n\t}\n\n\tt.Run(\"ConstructorFuncGeneratesNewArgsOnEachCall\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tperiodicJobBundle, _ := setup(t)\n\n\t\ttype TestJobArgs struct {\n\t\t\tJobArgsReflectKind[TestJobArgs]\n\t\t\tJobNum int `json:\"job_num\"`\n\t\t}\n\n\t\tvar jobNum int\n\n\t\tperiodicJob := NewPeriodicJob(\n\t\t\tPeriodicInterval(15*time.Minute),\n\t\t\tfunc() (JobArgs, *InsertOpts) {\n\t\t\t\tjobNum++\n\t\t\t\treturn TestJobArgs{JobNum: jobNum}, nil\n\t\t\t},\n\t\t\tnil,\n\t\t)\n\n\t\tinternalPeriodicJob := periodicJobBundle.toInternal(periodicJob)\n\n\t\tinsertParams1, err := internalPeriodicJob.ConstructorFunc()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, mustUnmarshalJSON[TestJobArgs](t, insertParams1.EncodedArgs).JobNum)\n\n\t\tinsertParams2, err := internalPeriodicJob.ConstructorFunc()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 2, mustUnmarshalJSON[TestJobArgs](t, insertParams2.EncodedArgs).JobNum)\n\t})\n\n\tt.Run(\"ReturningNilDoesntInsertNewJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tperiodicJobBundle, _ := setup(t)\n\n\t\tperiodicJob := NewPeriodicJob(\n\t\t\tPeriodicInterval(15*time.Minute),\n\t\t\tfunc() (JobArgs, *InsertOpts) {\n\t\t\t\t// Returning nil from the constructor function should not insert a new job.\n\t\t\t\treturn nil, nil\n\t\t\t},\n\t\t\tnil,\n\t\t)\n\n\t\tinternalPeriodicJob := periodicJobBundle.toInternal(periodicJob)\n\n\t\t_, err := internalPeriodicJob.ConstructorFunc()\n\t\trequire.ErrorIs(t, err, maintenance.ErrNoJobToInsert)\n\t})\n}\n\nfunc mustUnmarshalJSON[T any](t *testing.T, data []byte) *T {\n\tt.Helper()\n\n\tvar val T\n\terr := json.Unmarshal(data, &val)\n\trequire.NoError(t, err)\n\treturn &val\n}\n"
        },
        {
          "name": "plugin.go",
          "type": "blob",
          "size": 0.990234375,
          "content": "package river\n\nimport (\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/riverpilot\"\n\t\"github.com/riverqueue/river/rivershared/startstop\"\n)\n\n// A plugin API that drivers may implement to extend a River client. Driver\n// plugins may, for example, add additional maintenance services.\n//\n// This should be considered a River internal API and its stability is not\n// guaranteed. DO NOT USE.\ntype driverPlugin[TTx any] interface {\n\t// PluginInit initializes a plugin with an archetype and client. It's\n\t// invoked on Client.NewClient.\n\tPluginInit(archetype *baseservice.Archetype, client *Client[TTx])\n\n\t// PluginMaintenanceServices returns additional maintenance services (will\n\t// only run on an elected leader) for a River client.\n\tPluginMaintenanceServices() []startstop.Service\n\n\tPluginPilot() riverpilot.Pilot\n\n\t// PluginServices returns additional non-maintenance services (will run on\n\t// all clients) for a River client.\n\tPluginServices() []startstop.Service\n}\n"
        },
        {
          "name": "plugin_test.go",
          "type": "blob",
          "size": 2.7744140625,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/riverpilot\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/startstop\"\n)\n\nfunc TestClientDriverPlugin(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tpluginDriver *TestDriverWithPlugin\n\t}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tpluginDriver := newDriverWithPlugin(t, riverinternaltest.TestDB(ctx, t))\n\n\t\tclient, err := NewClient(pluginDriver, newTestConfig(t, nil))\n\t\trequire.NoError(t, err)\n\n\t\treturn client, &testBundle{\n\t\t\tpluginDriver: pluginDriver,\n\t\t}\n\t}\n\n\tt.Run(\"ServicesStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, bundle := setup(t)\n\n\t\tstartClient(ctx, t, client)\n\n\t\triversharedtest.WaitOrTimeout(t, startstop.WaitAllStartedC(\n\t\t\tbundle.pluginDriver.maintenanceService,\n\t\t\tbundle.pluginDriver.service,\n\t\t))\n\t})\n}\n\nvar _ driverPlugin[pgx.Tx] = &TestDriverWithPlugin{}\n\ntype TestDriverWithPlugin struct {\n\t*riverpgxv5.Driver\n\tinitCalled         bool\n\tmaintenanceService startstop.Service\n\tservice            startstop.Service\n}\n\nfunc newDriverWithPlugin(t *testing.T, dbPool *pgxpool.Pool) *TestDriverWithPlugin {\n\tt.Helper()\n\n\tnewService := func(name string) startstop.Service {\n\t\treturn startstop.StartStopFunc(func(ctx context.Context, shouldStart bool, started, stopped func()) error {\n\t\t\tif !shouldStart {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tgo func() {\n\t\t\t\tstarted()\n\t\t\t\tdefer stopped() // this defer should come first so it's last out\n\n\t\t\t\tt.Logf(\"Test service started: %s\", name)\n\n\t\t\t\t<-ctx.Done()\n\t\t\t}()\n\n\t\t\treturn nil\n\t\t})\n\t}\n\n\treturn &TestDriverWithPlugin{\n\t\tDriver:             riverpgxv5.New(dbPool),\n\t\tmaintenanceService: newService(\"maintenance service\"),\n\t\tservice:            newService(\"other service\"),\n\t}\n}\n\nfunc (d *TestDriverWithPlugin) PluginInit(archetype *baseservice.Archetype, client *Client[pgx.Tx]) {\n\td.initCalled = true\n}\n\nfunc (d *TestDriverWithPlugin) PluginMaintenanceServices() []startstop.Service {\n\tif !d.initCalled {\n\t\tpanic(\"expected PluginInit to be called before this function\")\n\t}\n\n\treturn []startstop.Service{d.maintenanceService}\n}\n\nfunc (d *TestDriverWithPlugin) PluginPilot() riverpilot.Pilot {\n\tif !d.initCalled {\n\t\tpanic(\"expected PluginInit to be called before this function\")\n\t}\n\n\treturn nil\n}\n\nfunc (d *TestDriverWithPlugin) PluginServices() []startstop.Service {\n\tif !d.initCalled {\n\t\tpanic(\"expected PluginInit to be called before this function\")\n\t}\n\n\treturn []startstop.Service{d.service}\n}\n"
        },
        {
          "name": "producer.go",
          "type": "blob",
          "size": 23.01171875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/notifier\"\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/util/chanutil\"\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/startstop\"\n\t\"github.com/riverqueue/river/rivershared/testsignal\"\n\t\"github.com/riverqueue/river/rivershared/util/randutil\"\n\t\"github.com/riverqueue/river/rivershared/util/serviceutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nconst (\n\tqueuePollIntervalDefault   = 2 * time.Second\n\tqueueReportIntervalDefault = 10 * time.Minute\n)\n\n// Test-only properties.\ntype producerTestSignals struct {\n\tDeletedExpiredQueueRecords testsignal.TestSignal[struct{}] // notifies when the producer deletes expired queue records\n\tPaused                     testsignal.TestSignal[struct{}] // notifies when the producer is paused\n\tPolledQueueConfig          testsignal.TestSignal[struct{}] // notifies when the producer polls for queue settings\n\tReportedQueueStatus        testsignal.TestSignal[struct{}] // notifies when the producer reports queue status\n\tResumed                    testsignal.TestSignal[struct{}] // notifies when the producer is resumed\n\tStartedExecutors           testsignal.TestSignal[struct{}] // notifies when runOnce finishes a pass\n}\n\nfunc (ts *producerTestSignals) Init() {\n\tts.DeletedExpiredQueueRecords.Init()\n\tts.Paused.Init()\n\tts.PolledQueueConfig.Init()\n\tts.ReportedQueueStatus.Init()\n\tts.Resumed.Init()\n\tts.StartedExecutors.Init()\n}\n\ntype producerConfig struct {\n\tClientID     string\n\tCompleter    jobcompleter.JobCompleter\n\tErrorHandler ErrorHandler\n\n\t// FetchCooldown is the minimum amount of time to wait between fetches of new\n\t// jobs. Jobs will only be fetched *at most* this often, but if no new jobs\n\t// are coming in via LISTEN/NOTIFY then fetches may be delayed as long as\n\t// FetchPollInterval.\n\tFetchCooldown time.Duration\n\n\t// FetchPollInterval is the amount of time between periodic fetches for new\n\t// jobs. Typically new jobs will be picked up ~immediately after insert via\n\t// LISTEN/NOTIFY, but this provides a fallback.\n\tFetchPollInterval time.Duration\n\n\tGlobalMiddleware []rivertype.WorkerMiddleware\n\tJobTimeout       time.Duration\n\tMaxWorkers       int\n\n\t// Notifier is a notifier for subscribing to new job inserts and job\n\t// control. If nil, the producer will operate in poll-only mode.\n\tNotifier *notifier.Notifier\n\n\tQueue string\n\t// QueueEventCallback gets called when a queue's config changes (such as\n\t// pausing or resuming) events can be emitted to subscriptions.\n\tQueueEventCallback func(event *Event)\n\n\t// QueuePollInterval is the amount of time between periodic checks for\n\t// queue setting changes. This is only used in poll-only mode (when no\n\t// notifier is provided).\n\tQueuePollInterval time.Duration\n\t// QueueReportInterval is the amount of time between periodic reports\n\t// of the queue status.\n\tQueueReportInterval time.Duration\n\tRetryPolicy         ClientRetryPolicy\n\tSchedulerInterval   time.Duration\n\tWorkers             *Workers\n}\n\nfunc (c *producerConfig) mustValidate() *producerConfig {\n\tif c.Completer == nil {\n\t\tpanic(\"producerConfig.Completer is required\")\n\t}\n\tif c.ClientID == \"\" {\n\t\tpanic(\"producerConfig.ClientID is required\")\n\t}\n\tif c.FetchCooldown <= 0 {\n\t\tpanic(\"producerConfig.FetchCooldown must be great than zero\")\n\t}\n\tif c.FetchPollInterval <= 0 {\n\t\tpanic(\"producerConfig.FetchPollInterval must be greater than zero\")\n\t}\n\tif c.JobTimeout < -1 {\n\t\tpanic(\"producerConfig.JobTimeout must be greater or equal to zero\")\n\t}\n\tif c.MaxWorkers == 0 {\n\t\tpanic(\"producerConfig.MaxWorkers is required\")\n\t}\n\tif c.Queue == \"\" {\n\t\tpanic(\"producerConfig.Queue is required\")\n\t}\n\tif c.QueuePollInterval == 0 {\n\t\tc.QueuePollInterval = queuePollIntervalDefault\n\t}\n\tif c.QueuePollInterval <= 0 {\n\t\tpanic(\"producerConfig.QueueSettingsPollInterval must be greater than zero\")\n\t}\n\tif c.QueueReportInterval == 0 {\n\t\tc.QueueReportInterval = queueReportIntervalDefault\n\t}\n\tif c.QueueReportInterval <= 0 {\n\t\tpanic(\"producerConfig.QueueSettingsReportInterval must be greater than zero\")\n\t}\n\tif c.RetryPolicy == nil {\n\t\tpanic(\"producerConfig.RetryPolicy is required\")\n\t}\n\tif c.SchedulerInterval == 0 {\n\t\tpanic(\"producerConfig.SchedulerInterval is required\")\n\t}\n\tif c.Workers == nil {\n\t\tpanic(\"producerConfig.Workers is required\")\n\t}\n\n\treturn c\n}\n\n// producer manages a fleet of Workers up to a maximum size. It periodically fetches jobs\n// from the adapter and dispatches them to Workers. It receives completed job results from Workers.\n//\n// The producer never fetches more jobs than the number of free Worker slots it\n// has available. This is not optimal for throughput compared to pre-fetching\n// extra jobs, but it is better for smaller job counts or slower jobs where even\n// distribution and minimizing execution latency is more important.\ntype producer struct {\n\tbaseservice.BaseService\n\tstartstop.BaseStartStop\n\n\t// Jobs which are currently being worked. Only used by main goroutine.\n\tactiveJobs map[int64]*jobExecutor\n\n\tcompleter    jobcompleter.JobCompleter\n\tconfig       *producerConfig\n\texec         riverdriver.Executor\n\terrorHandler ErrorHandler\n\tworkers      *Workers\n\n\t// Receives job IDs to cancel. Written by notifier goroutine, only read from\n\t// main goroutine.\n\tcancelCh chan int64\n\n\t// Set to true when the producer thinks it should trigger another fetch as\n\t// soon as slots are available. This is written and read by the main\n\t// goroutine.\n\tfetchWhenSlotsAreAvailable bool\n\n\t// Receives completed jobs from workers. Written by completed workers, only\n\t// read from main goroutine.\n\tjobResultCh chan *rivertype.JobRow\n\n\tjobTimeout time.Duration\n\n\t// An atomic count of the number of jobs actively being worked on. This is\n\t// written to by the main goroutine, but read by the dispatcher.\n\tnumJobsActive atomic.Int32\n\n\tnumJobsRan atomic.Uint64\n\tpaused     bool\n\t// Receives control messages from the notifier goroutine. Written by notifier\n\t// goroutine, only read from main goroutine.\n\tqueueControlCh chan *jobControlPayload\n\tretryPolicy    ClientRetryPolicy\n\ttestSignals    producerTestSignals\n}\n\nfunc newProducer(archetype *baseservice.Archetype, exec riverdriver.Executor, config *producerConfig) *producer {\n\tif archetype == nil {\n\t\tpanic(\"archetype is required\")\n\t}\n\tif exec == nil {\n\t\tpanic(\"exec is required\")\n\t}\n\n\treturn baseservice.Init(archetype, &producer{\n\t\tactiveJobs:     make(map[int64]*jobExecutor),\n\t\tcancelCh:       make(chan int64, 1000),\n\t\tcompleter:      config.Completer,\n\t\tconfig:         config.mustValidate(),\n\t\texec:           exec,\n\t\terrorHandler:   config.ErrorHandler,\n\t\tjobResultCh:    make(chan *rivertype.JobRow, config.MaxWorkers),\n\t\tjobTimeout:     config.JobTimeout,\n\t\tqueueControlCh: make(chan *jobControlPayload, 100),\n\t\tretryPolicy:    config.RetryPolicy,\n\t\tworkers:        config.Workers,\n\t})\n}\n\n// Start starts the producer. It backgrounds a goroutine which is stopped when\n// context is cancelled or Stop is invoked.\n//\n// This variant uses a single context as fetchCtx and workCtx, and is here to\n// implement startstop.Service so that the producer can be stored as a service\n// variable and used with various service utilities. StartWorkContext below\n// should be preferred for production use.\nfunc (p *producer) Start(ctx context.Context) error {\n\treturn p.StartWorkContext(ctx, ctx)\n}\n\nfunc (p *producer) Stop() {\n\tp.Logger.Debug(p.Name + \": Stopping\")\n\tp.BaseStartStop.Stop()\n\tp.Logger.Debug(p.Name + \": Stop returned\")\n}\n\n// Start starts the producer. It backgrounds a goroutine which is stopped when\n// context is cancelled or Stop is invoked.\n//\n// When fetchCtx is cancelled, no more jobs will be fetched; however, if a fetch\n// is already in progress, It will be allowed to complete and run any fetched\n// jobs. When workCtx is cancelled, any in-progress jobs will have their\n// contexts cancelled too.\nfunc (p *producer) StartWorkContext(fetchCtx, workCtx context.Context) error {\n\tfetchCtx, shouldStart, started, stopped := p.StartInit(fetchCtx)\n\tif !shouldStart {\n\t\treturn nil\n\t}\n\n\tqueue, err := func() (*rivertype.Queue, error) {\n\t\tctx, cancel := context.WithTimeout(fetchCtx, 10*time.Second)\n\t\tdefer cancel()\n\n\t\tp.Logger.DebugContext(ctx, p.Name+\": Fetching initial queue settings\", slog.String(\"queue\", p.config.Queue))\n\t\treturn p.exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{\n\t\t\tMetadata: []byte(\"{}\"),\n\t\t\tName:     p.config.Queue,\n\t\t})\n\t}()\n\tif err != nil {\n\t\tstopped()\n\t\tif errors.Is(err, startstop.ErrStop) || strings.HasSuffix(err.Error(), \"conn closed\") || fetchCtx.Err() != nil {\n\t\t\treturn nil //nolint:nilerr\n\t\t}\n\t\tp.Logger.ErrorContext(fetchCtx, p.Name+\": Error fetching initial queue settings\", slog.String(\"err\", err.Error()))\n\t\treturn err\n\t}\n\n\tinitiallyPaused := queue != nil && (queue.PausedAt != nil)\n\tp.paused = initiallyPaused\n\n\t// TODO: fetcher should have some jitter in it to avoid stampeding issues.\n\tfetchLimiter := chanutil.NewDebouncedChan(fetchCtx, p.config.FetchCooldown, true)\n\n\tvar (\n\t\tcontrolSub *notifier.Subscription\n\t\tinsertSub  *notifier.Subscription\n\t)\n\tif p.config.Notifier == nil {\n\t\tp.Logger.DebugContext(fetchCtx, p.Name+\": No notifier configured; starting in poll mode\", \"client_id\", p.config.ClientID)\n\n\t\tgo p.pollForSettingChanges(fetchCtx, initiallyPaused)\n\t} else {\n\t\tvar err error\n\n\t\thandleInsertNotification := func(topic notifier.NotificationTopic, payload string) {\n\t\t\tvar decoded insertPayload\n\t\t\tif err := json.Unmarshal([]byte(payload), &decoded); err != nil {\n\t\t\t\tp.Logger.ErrorContext(workCtx, p.Name+\": Failed to unmarshal insert notification payload\", slog.String(\"err\", err.Error()))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif decoded.Queue != p.config.Queue {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Received insert notification\", slog.String(\"queue\", decoded.Queue))\n\t\t\tfetchLimiter.Call()\n\t\t}\n\t\tinsertSub, err = p.config.Notifier.Listen(fetchCtx, notifier.NotificationTopicInsert, handleInsertNotification)\n\t\tif err != nil {\n\t\t\tstopped()\n\t\t\tif strings.HasSuffix(err.Error(), \"conn closed\") || errors.Is(err, context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\tcontrolSub, err = p.config.Notifier.Listen(fetchCtx, notifier.NotificationTopicControl, p.handleControlNotification(workCtx))\n\t\tif err != nil {\n\t\t\tstopped()\n\t\t\tif strings.HasSuffix(err.Error(), \"conn closed\") || errors.Is(err, context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n\n\tgo func() {\n\t\tstarted()\n\t\tdefer stopped() // this defer should come first so it's last out\n\n\t\tp.Logger.DebugContext(fetchCtx, p.Name+\": Run loop started\", slog.String(\"queue\", p.config.Queue), slog.Bool(\"paused\", p.paused))\n\t\tdefer func() {\n\t\t\tp.Logger.DebugContext(fetchCtx, p.Name+\": Run loop stopped\", slog.String(\"queue\", p.config.Queue), slog.Uint64(\"num_completed_jobs\", p.numJobsRan.Load()))\n\t\t}()\n\n\t\tif insertSub != nil {\n\t\t\tdefer insertSub.Unlisten(fetchCtx)\n\t\t}\n\n\t\tif controlSub != nil {\n\t\t\tdefer controlSub.Unlisten(fetchCtx)\n\t\t}\n\n\t\tgo p.heartbeatLogLoop(fetchCtx)\n\t\tgo p.reportQueueStatusLoop(fetchCtx)\n\t\tp.fetchAndRunLoop(fetchCtx, workCtx, fetchLimiter)\n\n\t\tp.executorShutdownLoop()\n\t}()\n\n\treturn nil\n}\n\ntype controlAction string\n\nconst (\n\tcontrolActionCancel controlAction = \"cancel\"\n\tcontrolActionPause  controlAction = \"pause\"\n\tcontrolActionResume controlAction = \"resume\"\n)\n\ntype jobControlPayload struct {\n\tAction controlAction `json:\"action\"`\n\tJobID  int64         `json:\"job_id\"`\n\tQueue  string        `json:\"queue\"`\n}\n\ntype insertPayload struct {\n\tQueue string `json:\"queue\"`\n}\n\nfunc (p *producer) handleControlNotification(workCtx context.Context) func(notifier.NotificationTopic, string) {\n\treturn func(topic notifier.NotificationTopic, payload string) {\n\t\tvar decoded jobControlPayload\n\t\tif err := json.Unmarshal([]byte(payload), &decoded); err != nil {\n\t\t\tp.Logger.ErrorContext(workCtx, p.Name+\": Failed to unmarshal job control notification payload\", slog.String(\"err\", err.Error()))\n\t\t\treturn\n\t\t}\n\n\t\tswitch decoded.Action {\n\t\tcase controlActionPause, controlActionResume:\n\t\t\tif decoded.Queue != rivercommon.AllQueuesString && decoded.Queue != p.config.Queue {\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Queue control notification for other queue\", slog.String(\"action\", string(decoded.Action)))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-workCtx.Done():\n\t\t\tcase p.queueControlCh <- &decoded:\n\t\t\tdefault:\n\t\t\t\tp.Logger.WarnContext(workCtx, p.Name+\": Queue control notification dropped due to full buffer\", slog.String(\"action\", string(decoded.Action)))\n\t\t\t}\n\t\tcase controlActionCancel:\n\t\t\tif decoded.Queue != p.config.Queue {\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Received job cancel notification for other queue\",\n\t\t\t\t\tslog.String(\"action\", string(decoded.Action)),\n\t\t\t\t\tslog.Int64(\"job_id\", decoded.JobID),\n\t\t\t\t\tslog.String(\"queue\", decoded.Queue),\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-workCtx.Done():\n\t\t\tcase p.cancelCh <- decoded.JobID:\n\t\t\tdefault:\n\t\t\t\tp.Logger.WarnContext(workCtx, p.Name+\": Job cancel notification dropped due to full buffer\", slog.Int64(\"job_id\", decoded.JobID))\n\t\t\t}\n\t\tdefault:\n\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Received job control notification with unknown action\",\n\t\t\t\tslog.String(\"action\", string(decoded.Action)),\n\t\t\t\tslog.Int64(\"job_id\", decoded.JobID),\n\t\t\t\tslog.String(\"queue\", decoded.Queue),\n\t\t\t)\n\t\t}\n\t}\n}\n\nfunc (p *producer) fetchAndRunLoop(fetchCtx, workCtx context.Context, fetchLimiter *chanutil.DebouncedChan) {\n\t// Prime the fetchLimiter so we can make an initial fetch without waiting for\n\t// an insert notification or a fetch poll.\n\tfetchLimiter.Call()\n\n\tfetchPollTimer := time.NewTimer(p.config.FetchPollInterval)\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-fetchCtx.Done():\n\t\t\t\t// Stop fetch timer so no more fetches are triggered.\n\t\t\t\tif !fetchPollTimer.Stop() {\n\t\t\t\t\t<-fetchPollTimer.C\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\tcase <-fetchPollTimer.C:\n\t\t\t\tfetchLimiter.Call()\n\t\t\t\tfetchPollTimer.Reset(p.config.FetchPollInterval)\n\t\t\t}\n\t\t}\n\t}()\n\n\tfetchResultCh := make(chan producerFetchResult)\n\tfor {\n\t\tselect {\n\t\tcase <-fetchCtx.Done():\n\t\t\treturn\n\t\tcase msg := <-p.queueControlCh:\n\t\t\tswitch msg.Action {\n\t\t\tcase controlActionPause:\n\t\t\t\tif p.paused {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tp.paused = true\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Paused\", slog.String(\"queue\", p.config.Queue), slog.String(\"queue_in_message\", msg.Queue))\n\t\t\t\tp.testSignals.Paused.Signal(struct{}{})\n\t\t\t\tif p.config.QueueEventCallback != nil {\n\t\t\t\t\tp.config.QueueEventCallback(&Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: p.config.Queue}})\n\t\t\t\t}\n\t\t\tcase controlActionResume:\n\t\t\t\tif !p.paused {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tp.paused = false\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Resumed\", slog.String(\"queue\", p.config.Queue), slog.String(\"queue_in_message\", msg.Queue))\n\t\t\t\tp.testSignals.Resumed.Signal(struct{}{})\n\t\t\t\tif p.config.QueueEventCallback != nil {\n\t\t\t\t\tp.config.QueueEventCallback(&Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: p.config.Queue}})\n\t\t\t\t}\n\t\t\tcase controlActionCancel:\n\t\t\t\t// Separate this case to make linter happy:\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Unhandled queue control action\", \"action\", msg.Action)\n\t\t\tdefault:\n\t\t\t\tp.Logger.DebugContext(workCtx, p.Name+\": Unknown queue control action\", \"action\", msg.Action)\n\t\t\t}\n\t\tcase jobID := <-p.cancelCh:\n\t\t\tp.maybeCancelJob(jobID)\n\t\tcase <-fetchLimiter.C():\n\t\t\tif p.paused {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.innerFetchLoop(workCtx, fetchResultCh)\n\t\t\t// Ensure we can't start another fetch when fetchCtx is done, even if\n\t\t\t// the fetchLimiter is also ready to fire:\n\t\t\tselect {\n\t\t\tcase <-fetchCtx.Done():\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\t\tcase result := <-p.jobResultCh:\n\t\t\tp.removeActiveJob(result.ID)\n\t\t\tif p.fetchWhenSlotsAreAvailable {\n\t\t\t\t// If we missed a fetch because all worker slots were full, or if we\n\t\t\t\t// fetched the maximum number of jobs on the last attempt, get a little\n\t\t\t\t// more aggressive triggering the fetch limiter now that we have a slot\n\t\t\t\t// available.\n\t\t\t\tp.fetchWhenSlotsAreAvailable = false\n\t\t\t\tfetchLimiter.Call()\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (p *producer) innerFetchLoop(workCtx context.Context, fetchResultCh chan producerFetchResult) {\n\tlimit := p.maxJobsToFetch()\n\tif limit <= 0 {\n\t\t// We have no slots for new jobs, so don't bother fetching. However, since\n\t\t// we knew it was time to fetch, we keep track of what happened so we can\n\t\t// trigger another fetch as soon as we have open slots.\n\t\tp.fetchWhenSlotsAreAvailable = true\n\t\treturn\n\t}\n\n\tgo p.dispatchWork(workCtx, limit, fetchResultCh)\n\n\tfor {\n\t\tselect {\n\t\tcase result := <-fetchResultCh:\n\t\t\tif result.err != nil {\n\t\t\t\tp.Logger.ErrorContext(workCtx, p.Name+\": Error fetching jobs\", slog.String(\"err\", result.err.Error()))\n\t\t\t} else if len(result.jobs) > 0 {\n\t\t\t\tp.startNewExecutors(workCtx, result.jobs)\n\n\t\t\t\tif len(result.jobs) == limit {\n\t\t\t\t\t// Fetch returned the maximum number of jobs that were requested,\n\t\t\t\t\t// implying there may be more in the queue. Trigger another fetch when\n\t\t\t\t\t// slots are available.\n\t\t\t\t\tp.fetchWhenSlotsAreAvailable = true\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\tcase result := <-p.jobResultCh:\n\t\t\tp.removeActiveJob(result.ID)\n\t\tcase jobID := <-p.cancelCh:\n\t\t\tp.maybeCancelJob(jobID)\n\t\t}\n\t}\n}\n\nfunc (p *producer) executorShutdownLoop() {\n\t// No more jobs will be fetched or executed. However, we must wait for all\n\t// in-progress jobs to complete.\n\tfor {\n\t\tif len(p.activeJobs) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tresult := <-p.jobResultCh\n\t\tp.removeActiveJob(result.ID)\n\t}\n}\n\nfunc (p *producer) addActiveJob(id int64, executor *jobExecutor) {\n\tp.numJobsActive.Add(1)\n\tp.activeJobs[id] = executor\n}\n\nfunc (p *producer) removeActiveJob(id int64) {\n\tdelete(p.activeJobs, id)\n\tp.numJobsActive.Add(-1)\n\tp.numJobsRan.Add(1)\n}\n\nfunc (p *producer) maybeCancelJob(id int64) {\n\texecutor, ok := p.activeJobs[id]\n\tif !ok {\n\t\treturn\n\t}\n\texecutor.Cancel()\n}\n\nfunc (p *producer) dispatchWork(workCtx context.Context, count int, fetchResultCh chan<- producerFetchResult) {\n\t// This intentionally removes any deadlines or cancellation from the parent\n\t// context because we don't want it to get cancelled if the producer is asked\n\t// to shut down. In that situation, we want to finish fetching any jobs we are\n\t// in the midst of fetching, work them, and then stop. Otherwise we'd have a\n\t// risk of shutting down when we had already fetched jobs in the database,\n\t// leaving those jobs stranded. We'd then potentially have to release them\n\t// back to the queue.\n\tjobs, err := p.exec.JobGetAvailable(context.WithoutCancel(workCtx), &riverdriver.JobGetAvailableParams{\n\t\tAttemptedBy: p.config.ClientID,\n\t\tMax:         count,\n\t\tQueue:       p.config.Queue,\n\t})\n\tif err != nil {\n\t\tfetchResultCh <- producerFetchResult{err: err}\n\t\treturn\n\t}\n\tfetchResultCh <- producerFetchResult{jobs: jobs}\n}\n\n// Periodically logs an informational log line giving some insight into the\n// current state of the producer.\nfunc (p *producer) heartbeatLogLoop(ctx context.Context) {\n\tticker := time.NewTicker(5 * time.Second)\n\tdefer ticker.Stop()\n\ttype jobCount struct {\n\t\tran    uint64\n\t\tactive int\n\t}\n\tvar prevCount jobCount\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tcurCount := jobCount{ran: p.numJobsRan.Load(), active: int(p.numJobsActive.Load())}\n\t\t\tif curCount != prevCount {\n\t\t\t\tp.Logger.InfoContext(ctx, p.Name+\": Producer job counts\",\n\t\t\t\t\tslog.Uint64(\"num_completed_jobs\", curCount.ran),\n\t\t\t\t\tslog.Int(\"num_jobs_running\", curCount.active),\n\t\t\t\t\tslog.String(\"queue\", p.config.Queue),\n\t\t\t\t)\n\t\t\t}\n\t\t\tprevCount = curCount\n\t\t}\n\t}\n}\n\nfunc (p *producer) startNewExecutors(workCtx context.Context, jobs []*rivertype.JobRow) {\n\tfor _, job := range jobs {\n\t\tworkInfo, ok := p.workers.workersMap[job.Kind]\n\n\t\tvar workUnit workunit.WorkUnit\n\t\tif ok {\n\t\t\tworkUnit = workInfo.workUnitFactory.MakeUnit(job)\n\t\t}\n\n\t\t// jobCancel will always be called by the executor to prevent leaks.\n\t\tjobCtx, jobCancel := context.WithCancelCause(workCtx)\n\n\t\texecutor := baseservice.Init(&p.Archetype, &jobExecutor{\n\t\t\tCancelFunc:             jobCancel,\n\t\t\tClientJobTimeout:       p.jobTimeout,\n\t\t\tClientRetryPolicy:      p.retryPolicy,\n\t\t\tCompleter:              p.completer,\n\t\t\tErrorHandler:           p.errorHandler,\n\t\t\tInformProducerDoneFunc: p.handleWorkerDone,\n\t\t\tGlobalMiddleware:       p.config.GlobalMiddleware,\n\t\t\tJobRow:                 job,\n\t\t\tSchedulerInterval:      p.config.SchedulerInterval,\n\t\t\tWorkUnit:               workUnit,\n\t\t})\n\t\tp.addActiveJob(job.ID, executor)\n\n\t\tgo executor.Execute(jobCtx)\n\t}\n\n\tp.Logger.DebugContext(workCtx, p.Name+\": Distributed batch of jobs to executors\", \"num_jobs\", len(jobs))\n\n\tp.testSignals.StartedExecutors.Signal(struct{}{})\n}\n\nfunc (p *producer) maxJobsToFetch() int {\n\treturn p.config.MaxWorkers - int(p.numJobsActive.Load())\n}\n\nfunc (p *producer) handleWorkerDone(job *rivertype.JobRow) {\n\tp.jobResultCh <- job\n}\n\nfunc (p *producer) pollForSettingChanges(ctx context.Context, lastPaused bool) {\n\tticker := time.NewTicker(p.config.QueuePollInterval)\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tupdatedQueue, err := p.fetchQueueSettings(ctx)\n\t\t\tif err != nil {\n\t\t\t\tp.Logger.ErrorContext(ctx, p.Name+\": Error fetching queue settings\", slog.String(\"err\", err.Error()))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tshouldBePaused := (updatedQueue.PausedAt != nil)\n\t\t\tif lastPaused != shouldBePaused {\n\t\t\t\taction := controlActionPause\n\t\t\t\tif !shouldBePaused {\n\t\t\t\t\taction = controlActionResume\n\t\t\t\t}\n\t\t\t\tpayload := &jobControlPayload{\n\t\t\t\t\tAction: action,\n\t\t\t\t\tQueue:  p.config.Queue,\n\t\t\t\t}\n\t\t\t\tp.Logger.DebugContext(ctx, p.Name+\": Queue control state changed from polling\",\n\t\t\t\t\tslog.String(\"queue\", p.config.Queue),\n\t\t\t\t\tslog.String(\"action\", string(action)),\n\t\t\t\t\tslog.Bool(\"paused\", shouldBePaused),\n\t\t\t\t)\n\n\t\t\t\tselect {\n\t\t\t\tcase p.queueControlCh <- payload:\n\t\t\t\t\tlastPaused = shouldBePaused\n\t\t\t\tdefault:\n\t\t\t\t\tp.Logger.WarnContext(ctx, p.Name+\": Queue control notification dropped due to full buffer\", slog.String(\"action\", string(action)))\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.testSignals.PolledQueueConfig.Signal(struct{}{})\n\t\t}\n\t}\n}\n\nfunc (p *producer) fetchQueueSettings(ctx context.Context) (*rivertype.Queue, error) {\n\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\tdefer cancel()\n\n\treturn p.exec.QueueGet(ctx, p.config.Queue)\n}\n\nfunc (p *producer) reportQueueStatusLoop(ctx context.Context) {\n\tserviceutil.CancellableSleep(ctx, randutil.DurationBetween(0, time.Second))\n\treportTicker := time.NewTicker(p.config.QueueReportInterval)\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treportTicker.Stop()\n\t\t\treturn\n\t\tcase <-reportTicker.C:\n\t\t\tp.reportQueueStatusOnce(ctx)\n\t\t}\n\t}\n}\n\nfunc (p *producer) reportQueueStatusOnce(ctx context.Context) {\n\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\tdefer cancel()\n\n\tp.Logger.DebugContext(ctx, p.Name+\": Reporting queue status\", slog.String(\"queue\", p.config.Queue))\n\t_, err := p.exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{\n\t\tMetadata: []byte(\"{}\"),\n\t\tName:     p.config.Queue,\n\t})\n\tif err != nil && errors.Is(context.Cause(ctx), startstop.ErrStop) {\n\t\treturn\n\t}\n\tif err != nil {\n\t\tp.Logger.ErrorContext(ctx, p.Name+\": Queue status update, error updating in database\", slog.String(\"err\", err.Error()))\n\t\treturn\n\t}\n\tp.testSignals.ReportedQueueStatus.Signal(struct{}{})\n}\n\ntype producerFetchResult struct {\n\tjobs []*rivertype.JobRow\n\terr  error\n}\n"
        },
        {
          "name": "producer_test.go",
          "type": "blob",
          "size": 17.7294921875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"slices\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/maintenance\"\n\t\"github.com/riverqueue/river/internal/notifier\"\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest/sharedtx\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/startstoptest\"\n\t\"github.com/riverqueue/river/rivershared/testfactory\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nconst testClientID = \"test-client-id\"\n\nfunc Test_Producer_CanSafelyCompleteJobsWhileFetchingNewOnes(t *testing.T) {\n\t// We have encountered previous data races with the list of active jobs on\n\t// Producer because we need to know the count of active jobs in order to\n\t// determine how many we can fetch for the next batch, while we're managing\n\t// the map of active jobs in a different goroutine.\n\t//\n\t// This test attempts to exercise that race condition so that the race\n\t// detector can tell us if we're protected against it.\n\tt.Parallel()\n\n\tctx := context.Background()\n\trequire := require.New(t)\n\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\tconst maxJobCount = 10000\n\t// This doesn't strictly mean that there are no more jobs left to process,\n\t// merely that the final job we inserted is now being processed, which is\n\t// close enough for our purposes here.\n\tlastJobRun := make(chan struct{})\n\n\tarchetype := riversharedtest.BaseServiceArchetype(t)\n\n\tconfig := newTestConfig(t, nil)\n\tdbDriver := riverpgxv5.New(dbPool)\n\texec := dbDriver.GetExecutor()\n\tlistener := dbDriver.GetListener()\n\n\tsubscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 100)\n\tt.Cleanup(riverinternaltest.DiscardContinuously(subscribeCh))\n\n\tcompleter := jobcompleter.NewInlineCompleter(archetype, exec, subscribeCh)\n\tt.Cleanup(completer.Stop)\n\n\ttype WithJobNumArgs struct {\n\t\tJobArgsReflectKind[WithJobNumArgs]\n\t\tJobNum int `json:\"job_num\"`\n\t}\n\n\tworkers := NewWorkers()\n\tAddWorker(workers, WorkFunc(func(ctx context.Context, job *Job[WithJobNumArgs]) error {\n\t\tvar jobArgs WithJobNumArgs\n\t\trequire.NoError(json.Unmarshal(job.EncodedArgs, &jobArgs))\n\n\t\tif jobArgs.JobNum == maxJobCount-1 {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\tcase lastJobRun <- struct{}{}:\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}))\n\n\tnotifier := notifier.New(archetype, listener)\n\n\tproducer := newProducer(archetype, exec, &producerConfig{\n\t\tClientID:     testClientID,\n\t\tCompleter:    completer,\n\t\tErrorHandler: newTestErrorHandler(),\n\t\t// Fetch constantly to more aggressively trigger the potential data race:\n\t\tFetchCooldown:       time.Millisecond,\n\t\tFetchPollInterval:   time.Millisecond,\n\t\tJobTimeout:          JobTimeoutDefault,\n\t\tMaxWorkers:          1000,\n\t\tNotifier:            notifier,\n\t\tQueue:               rivercommon.QueueDefault,\n\t\tQueuePollInterval:   queuePollIntervalDefault,\n\t\tQueueReportInterval: queueReportIntervalDefault,\n\t\tRetryPolicy:         &DefaultClientRetryPolicy{},\n\t\tSchedulerInterval:   maintenance.JobSchedulerIntervalDefault,\n\t\tWorkers:             workers,\n\t})\n\n\tparams := make([]*riverdriver.JobInsertFastParams, maxJobCount)\n\tfor i := range params {\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, WithJobNumArgs{JobNum: i}, nil)\n\t\trequire.NoError(err)\n\n\t\tparams[i] = (*riverdriver.JobInsertFastParams)(insertParams)\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 20*time.Second)\n\tt.Cleanup(cancel)\n\n\tgo func() {\n\t\t// The producer should never exceed its MaxWorkerCount. If it does, panic so\n\t\t// we can get a trace.\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\t\t\tnumActiveJobs := producer.numJobsActive.Load()\n\t\t\tif int(numActiveJobs) > producer.config.MaxWorkers {\n\t\t\t\tpanic(fmt.Sprintf(\"producer exceeded MaxWorkerCount=%d, actual count=%d\", producer.config.MaxWorkers, numActiveJobs))\n\t\t\t}\n\t\t}\n\t}()\n\n\t_, err := exec.JobInsertFastMany(ctx, params)\n\trequire.NoError(err)\n\n\trequire.NoError(producer.StartWorkContext(ctx, ctx))\n\tt.Cleanup(producer.Stop)\n\n\tselect {\n\tcase <-lastJobRun:\n\t\tt.Logf(\"Last job reported in; cancelling context\")\n\t\tcancel()\n\tcase <-ctx.Done():\n\t\tt.Error(\"timed out waiting for last job to run\")\n\t}\n}\n\nfunc TestProducer_PollOnly(t *testing.T) {\n\tt.Parallel()\n\n\ttestProducer(t, func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated) {\n\t\tt.Helper()\n\n\t\tvar (\n\t\t\tarchetype = riversharedtest.BaseServiceArchetype(t)\n\t\t\tdriver    = riverpgxv5.New(nil)\n\t\t\ttx        = riverinternaltest.TestTx(ctx, t)\n\t\t)\n\n\t\t// Wrap with a shared transaction because the producer fetching jobs may\n\t\t// conflict with jobs being inserted in test cases.\n\t\ttx = sharedtx.NewSharedTx(tx)\n\n\t\tvar (\n\t\t\texec       = driver.UnwrapExecutor(tx)\n\t\t\tjobUpdates = make(chan []jobcompleter.CompleterJobUpdated, 10)\n\t\t)\n\n\t\tcompleter := jobcompleter.NewInlineCompleter(archetype, exec, jobUpdates)\n\t\t{\n\t\t\trequire.NoError(t, completer.Start(ctx))\n\t\t\tt.Cleanup(completer.Stop)\n\t\t}\n\n\t\treturn newProducer(archetype, exec, &producerConfig{\n\t\t\tClientID:            testClientID,\n\t\t\tCompleter:           completer,\n\t\t\tErrorHandler:        newTestErrorHandler(),\n\t\t\tFetchCooldown:       FetchCooldownDefault,\n\t\t\tFetchPollInterval:   50 * time.Millisecond, // more aggressive than normal because we have no notifier\n\t\t\tJobTimeout:          JobTimeoutDefault,\n\t\t\tMaxWorkers:          1_000,\n\t\t\tNotifier:            nil, // no notifier\n\t\t\tQueue:               rivercommon.QueueDefault,\n\t\t\tQueuePollInterval:   queuePollIntervalDefault,\n\t\t\tQueueReportInterval: queueReportIntervalDefault,\n\t\t\tRetryPolicy:         &DefaultClientRetryPolicy{},\n\t\t\tSchedulerInterval:   riverinternaltest.SchedulerShortInterval,\n\t\t\tWorkers:             NewWorkers(),\n\t\t}), jobUpdates\n\t})\n}\n\nfunc TestProducer_WithNotifier(t *testing.T) {\n\tt.Parallel()\n\n\ttestProducer(t, func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated) {\n\t\tt.Helper()\n\n\t\tvar (\n\t\t\tarchetype  = riversharedtest.BaseServiceArchetype(t)\n\t\t\tdbPool     = riverinternaltest.TestDB(ctx, t)\n\t\t\tdriver     = riverpgxv5.New(dbPool)\n\t\t\texec       = driver.GetExecutor()\n\t\t\tjobUpdates = make(chan []jobcompleter.CompleterJobUpdated, 10)\n\t\t\tlistener   = driver.GetListener()\n\t\t)\n\n\t\tcompleter := jobcompleter.NewInlineCompleter(archetype, exec, jobUpdates)\n\t\t{\n\t\t\trequire.NoError(t, completer.Start(ctx))\n\t\t\tt.Cleanup(completer.Stop)\n\t\t}\n\n\t\tnotifier := notifier.New(archetype, listener)\n\t\t{\n\t\t\trequire.NoError(t, notifier.Start(ctx))\n\t\t\tt.Cleanup(notifier.Stop)\n\t\t}\n\n\t\treturn newProducer(archetype, exec, &producerConfig{\n\t\t\tClientID:            testClientID,\n\t\t\tCompleter:           completer,\n\t\t\tErrorHandler:        newTestErrorHandler(),\n\t\t\tFetchCooldown:       FetchCooldownDefault,\n\t\t\tFetchPollInterval:   50 * time.Millisecond, // more aggressive than normal so in case we miss the event, tests still pass quickly\n\t\t\tJobTimeout:          JobTimeoutDefault,\n\t\t\tMaxWorkers:          1_000,\n\t\t\tNotifier:            notifier,\n\t\t\tQueue:               rivercommon.QueueDefault,\n\t\t\tQueuePollInterval:   queuePollIntervalDefault,\n\t\t\tQueueReportInterval: queueReportIntervalDefault,\n\t\t\tRetryPolicy:         &DefaultClientRetryPolicy{},\n\t\t\tSchedulerInterval:   riverinternaltest.SchedulerShortInterval,\n\t\t\tWorkers:             NewWorkers(),\n\t\t}), jobUpdates\n\t})\n}\n\nfunc testProducer(t *testing.T, makeProducer func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated)) {\n\tt.Helper()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\tarchetype       *baseservice.Archetype\n\t\tcompleter       jobcompleter.JobCompleter\n\t\tconfig          *Config\n\t\texec            riverdriver.Executor\n\t\tjobUpdates      chan jobcompleter.CompleterJobUpdated\n\t\ttimeBeforeStart time.Time\n\t\tworkers         *Workers\n\t}\n\n\tsetup := func(t *testing.T) (*producer, *testBundle) {\n\t\tt.Helper()\n\n\t\ttimeBeforeStart := time.Now().UTC()\n\n\t\tproducer, jobUpdates := makeProducer(ctx, t)\n\t\tproducer.testSignals.Init()\n\t\tconfig := newTestConfig(t, nil)\n\n\t\tjobUpdatesFlattened := make(chan jobcompleter.CompleterJobUpdated, 10)\n\t\tgo func() {\n\t\t\tfor updates := range jobUpdates {\n\t\t\t\tfor _, update := range updates {\n\t\t\t\t\tjobUpdatesFlattened <- update\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\treturn producer, &testBundle{\n\t\t\tarchetype:       &producer.Archetype,\n\t\t\tcompleter:       producer.completer,\n\t\t\tconfig:          config,\n\t\t\texec:            producer.exec,\n\t\t\tjobUpdates:      jobUpdatesFlattened,\n\t\t\ttimeBeforeStart: timeBeforeStart,\n\t\t\tworkers:         producer.workers,\n\t\t}\n\t}\n\n\tmustInsert := func(ctx context.Context, t *testing.T, bundle *testBundle, args JobArgs) {\n\t\tt.Helper()\n\n\t\tinsertParams, err := insertParamsFromConfigArgsAndOptions(bundle.archetype, bundle.config, args, nil)\n\t\trequire.NoError(t, err)\n\t\tif insertParams.ScheduledAt == nil {\n\t\t\t// Without this, newly inserted jobs will pick up a scheduled_at time\n\t\t\t// that's the current Go time at the time of insertion. If the test is\n\t\t\t// using a transaction, this will be after the `now()` time in the\n\t\t\t// transaction that gets used by default in `JobGetAvailable`, so new jobs\n\t\t\t// won't be visible.\n\t\t\t//\n\t\t\t// To work around this, set all inserted jobs to a time before the start\n\t\t\t// of the test to ensure they're visible.\n\t\t\tinsertParams.ScheduledAt = &bundle.timeBeforeStart\n\t\t}\n\n\t\t_, err = bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})\n\t\trequire.NoError(t, err)\n\t}\n\n\tstartProducer := func(t *testing.T, fetchCtx, workCtx context.Context, producer *producer) {\n\t\tt.Helper()\n\n\t\trequire.NoError(t, producer.StartWorkContext(fetchCtx, workCtx))\n\t\tt.Cleanup(producer.Stop)\n\t}\n\n\tt.Run(\"NoOp\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, _ := setup(t)\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\t})\n\n\tt.Run(\"SimpleJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tAddWorker(bundle.workers, &noOpWorker{})\n\n\t\tmustInsert(ctx, t, bundle, &noOpArgs{})\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\tupdate := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, update.Job.State)\n\t})\n\n\tt.Run(\"RegistersQueueStatus\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tproducer.config.QueueReportInterval = 50 * time.Millisecond\n\n\t\tnow := time.Now().UTC()\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\tqueue, err := bundle.exec.QueueGet(ctx, rivercommon.QueueDefault)\n\t\trequire.NoError(t, err)\n\t\trequire.WithinDuration(t, now, queue.CreatedAt, 2*time.Second)\n\t\trequire.Equal(t, []byte(\"{}\"), queue.Metadata)\n\t\trequire.Equal(t, rivercommon.QueueDefault, queue.Name)\n\t\trequire.WithinDuration(t, now, queue.UpdatedAt, 2*time.Second)\n\t\trequire.Equal(t, queue.CreatedAt, queue.UpdatedAt)\n\n\t\t// Queue status should be updated quickly:\n\t\tproducer.testSignals.ReportedQueueStatus.WaitOrTimeout()\n\t})\n\n\tt.Run(\"UnknownJobKind\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tAddWorker(bundle.workers, &noOpWorker{})\n\n\t\tmustInsert(ctx, t, bundle, &noOpArgs{})\n\t\tmustInsert(ctx, t, bundle, &callbackArgs{}) // not registered\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\tupdates := riversharedtest.WaitOrTimeoutN(t, bundle.jobUpdates, 2)\n\n\t\t// Print updated jobs for debugging.\n\t\tfor _, update := range updates {\n\t\t\tt.Logf(\"Job: %+v\", update.Job)\n\t\t}\n\n\t\t// Order jobs come back in is not guaranteed, which is why this is\n\t\t// written somewhat strangely.\n\t\tfindJob := func(kind string) *rivertype.JobRow {\n\t\t\tindex := slices.IndexFunc(updates, func(u jobcompleter.CompleterJobUpdated) bool { return u.Job.Kind == kind })\n\t\t\trequire.NotEqualf(t, -1, index, \"Job update not found for kind: %s\", kind)\n\t\t\treturn updates[index].Job\n\t\t}\n\n\t\t{\n\t\t\tjob := findJob((&callbackArgs{}).Kind())\n\t\t\trequire.Equal(t, rivertype.JobStateRetryable, job.State)\n\t\t\trequire.Equal(t, (&UnknownJobKindError{Kind: (&callbackArgs{}).Kind()}).Error(), job.Errors[0].Error)\n\t\t}\n\t\t{\n\t\t\tjob := findJob((&noOpArgs{}).Kind())\n\t\t\trequire.Equal(t, rivertype.JobStateCompleted, job.State)\n\t\t}\n\t})\n\n\tt.Run(\"CancelledWorkContextCancelsJob\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tAddWorker(bundle.workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tproducer.Logger.InfoContext(ctx, \"Job started\")\n\t\t\t<-ctx.Done()\n\t\t\tproducer.Logger.InfoContext(ctx, \"Job stopped after context cancelled\")\n\t\t\treturn ctx.Err()\n\t\t}))\n\n\t\tworkCtx, workCancel := context.WithCancel(ctx)\n\t\tdefer workCancel()\n\n\t\tmustInsert(ctx, t, bundle, &JobArgs{})\n\n\t\tstartProducer(t, ctx, workCtx, producer)\n\n\t\tworkCancel()\n\n\t\tupdate := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)\n\t\trequire.Equal(t, rivertype.JobStateRetryable, update.Job.State)\n\t})\n\n\tt.Run(\"MaxWorkers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tconst (\n\t\t\tmaxWorkers = 5\n\t\t\tnumJobs    = 10\n\t\t)\n\n\t\tproducer, bundle := setup(t)\n\t\tproducer.config.MaxWorkers = maxWorkers\n\n\t\ttype JobArgs struct {\n\t\t\tJobArgsReflectKind[JobArgs]\n\t\t}\n\n\t\tunpauseWorkers := make(chan struct{})\n\t\tdefer close(unpauseWorkers)\n\n\t\tAddWorker(bundle.workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {\n\t\t\tt.Logf(\"Job paused\")\n\t\t\t<-unpauseWorkers\n\t\t\tt.Logf(\"Job unpaused\")\n\t\t\treturn ctx.Err()\n\t\t}))\n\n\t\tfor i := 0; i < numJobs; i++ {\n\t\t\tmustInsert(ctx, t, bundle, &JobArgs{})\n\t\t}\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\tproducer.testSignals.StartedExecutors.WaitOrTimeout()\n\n\t\t// Jobs are still paused as we fetch updated job states.\n\t\tupdatedJobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(&JobArgs{}).Kind()})\n\t\trequire.NoError(t, err)\n\n\t\tjobStateCounts := make(map[rivertype.JobState]int)\n\n\t\tfor _, updatedJob := range updatedJobs {\n\t\t\tjobStateCounts[updatedJob.State]++\n\t\t}\n\n\t\trequire.Equal(t, maxWorkers, jobStateCounts[rivertype.JobStateRunning])\n\t\trequire.Equal(t, numJobs-maxWorkers, jobStateCounts[rivertype.JobStateAvailable])\n\n\t\trequire.Equal(t, maxWorkers, int(producer.numJobsActive.Load()))\n\t\trequire.Zero(t, producer.maxJobsToFetch()) // zero because all slots are occupied\n\t})\n\n\tt.Run(\"StartStopStress\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, _ := setup(t)\n\t\tproducer.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress\n\t\tproducer.testSignals = producerTestSignals{}    // deinit so channels don't fill\n\n\t\tstartstoptest.Stress(ctx, t, producer)\n\t})\n\n\tt.Run(\"QueuePausedBeforeStart\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tAddWorker(bundle.workers, &noOpWorker{})\n\n\t\ttestfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{\n\t\t\tName:     ptrutil.Ptr(rivercommon.QueueDefault),\n\t\t\tPausedAt: ptrutil.Ptr(time.Now()),\n\t\t})\n\n\t\tmustInsert(ctx, t, bundle, &noOpArgs{})\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\tselect {\n\t\tcase update := <-bundle.jobUpdates:\n\t\t\tt.Fatalf(\"Unexpected job update: job=%+v stats=%+v\", update.Job, update.JobStats)\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\t})\n\n\ttestQueuePause := func(t *testing.T, queueNameToPause string) {\n\t\tt.Helper()\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tproducer.config.QueuePollInterval = 50 * time.Millisecond\n\t\tAddWorker(bundle.workers, &noOpWorker{})\n\n\t\tmustInsert(ctx, t, bundle, &noOpArgs{})\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\t// First job should be executed immediately while resumed:\n\t\tupdate := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, update.Job.State)\n\n\t\t// Pause the queue and wait for confirmation:\n\t\trequire.NoError(t, bundle.exec.QueuePause(ctx, queueNameToPause))\n\t\tif producer.config.Notifier != nil {\n\t\t\t// also emit notification:\n\t\t\temitQueueNotification(t, ctx, bundle.exec, queueNameToPause, \"pause\")\n\t\t}\n\t\tproducer.testSignals.Paused.WaitOrTimeout()\n\n\t\t// Job should not be executed while paused:\n\t\tmustInsert(ctx, t, bundle, &noOpArgs{})\n\n\t\tselect {\n\t\tcase update := <-bundle.jobUpdates:\n\t\t\tt.Fatalf(\"Unexpected job update: %+v\", update)\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\n\t\t// Resume the queue and wait for confirmation:\n\t\trequire.NoError(t, bundle.exec.QueueResume(ctx, queueNameToPause))\n\t\tif producer.config.Notifier != nil {\n\t\t\t// also emit notification:\n\t\t\temitQueueNotification(t, ctx, bundle.exec, queueNameToPause, \"resume\")\n\t\t}\n\t\tproducer.testSignals.Resumed.WaitOrTimeout()\n\n\t\t// Now the 2nd job should execute:\n\t\tupdate = riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, update.Job.State)\n\t}\n\n\tt.Run(\"QueuePausedDuringOperation\", func(t *testing.T) {\n\t\ttestQueuePause(t, rivercommon.QueueDefault)\n\t})\n\n\tt.Run(\"QueuePausedAndResumedDuringOperationUsing*\", func(t *testing.T) {\n\t\ttestQueuePause(t, rivercommon.AllQueuesString)\n\t})\n\n\tt.Run(\"QueueDeletedFromRiverQueueTableDuringOperation\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tproducer, bundle := setup(t)\n\t\tproducer.config.QueuePollInterval = time.Second\n\t\tproducer.config.QueueReportInterval = time.Second\n\n\t\tstartProducer(t, ctx, ctx, producer)\n\n\t\t// Delete the queue by using a future-dated horizon:\n\t\t_, err := bundle.exec.QueueDeleteExpired(ctx, &riverdriver.QueueDeleteExpiredParams{\n\t\t\tMax:              100,\n\t\t\tUpdatedAtHorizon: time.Now().Add(time.Minute),\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tproducer.testSignals.ReportedQueueStatus.WaitOrTimeout()\n\t\tif producer.config.Notifier == nil {\n\t\t\tproducer.testSignals.PolledQueueConfig.WaitOrTimeout()\n\t\t}\n\t})\n}\n\nfunc emitQueueNotification(t *testing.T, ctx context.Context, exec riverdriver.Executor, queue, action string) {\n\tt.Helper()\n\terr := exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{\n\t\tTopic: string(notifier.NotificationTopicControl),\n\t\tPayload: []string{\n\t\t\tfmt.Sprintf(`{\"queue\":\"%s\",\"action\":\"%s\"}`, queue, action),\n\t\t},\n\t})\n\trequire.NoError(t, err)\n}\n"
        },
        {
          "name": "queue_list_params.go",
          "type": "blob",
          "size": 1.0556640625,
          "content": "package river\n\n// QueueListParams specifies the parameters for a QueueList query. It must be\n// initialized with NewQueueListParams. Params can be built by chaining methods\n// on the QueueListParams object:\n//\n//\tparams := NewQueueListParams().First(100)\ntype QueueListParams struct {\n\tpaginationCount int32\n}\n\n// NewQueueListParams creates a new QueueListParams to return available jobs\n// sorted by time in ascending order, returning 100 jobs at most.\nfunc NewQueueListParams() *QueueListParams {\n\treturn &QueueListParams{\n\t\tpaginationCount: 100,\n\t}\n}\n\nfunc (p *QueueListParams) copy() *QueueListParams {\n\treturn &QueueListParams{\n\t\tpaginationCount: p.paginationCount,\n\t}\n}\n\n// First returns an updated filter set that will only return the first count\n// queues.\n//\n// Count must be between 1 and 10000, inclusive, or this will panic.\nfunc (p *QueueListParams) First(count int) *QueueListParams {\n\tif count <= 0 {\n\t\tpanic(\"count must be > 0\")\n\t}\n\tif count > 10000 {\n\t\tpanic(\"count must be <= 10000\")\n\t}\n\tresult := p.copy()\n\tresult.paginationCount = int32(count)\n\treturn result\n}\n"
        },
        {
          "name": "queue_pause_opts.go",
          "type": "blob",
          "size": 0.1142578125,
          "content": "package river\n\n// QueuePauseOpts are optional settings for pausing or resuming a queue.\ntype QueuePauseOpts struct{}\n"
        },
        {
          "name": "retry_policy.go",
          "type": "blob",
          "size": 4.0546875,
          "content": "package river\n\nimport (\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/rivershared/util/timeutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// ClientRetryPolicy is an interface that can be implemented to provide a retry\n// policy for how River deals with failed jobs at the client level (when a\n// worker does not define an override for `NextRetry`). Jobs are scheduled to be\n// retried in the future up until they've reached the job's max attempts, at\n// which pointed they're set as discarded.\n//\n// The ClientRetryPolicy does not have access to generics and operates on the\n// raw JobRow struct with encoded args.\ntype ClientRetryPolicy interface {\n\t// NextRetry calculates when the next retry for a failed job should take place\n\t// given when it was last attempted and its number of attempts, or any other\n\t// of the job's properties a user-configured retry policy might want to\n\t// consider.\n\tNextRetry(job *rivertype.JobRow) time.Time\n}\n\n// River's default retry policy.\ntype DefaultClientRetryPolicy struct {\n\ttimeNowFunc func() time.Time\n}\n\n// NextRetry gets the next retry given for the given job, accounting for when it\n// was last attempted and what attempt number that was. Reschedules using a\n// basic exponential backoff of `ATTEMPT^4`, so after the first failure a new\n// try will be scheduled in 1 seconds, 16 seconds after the second, 1 minute and\n// 21 seconds after the third, etc.\n//\n// In order to avoid penalizing jobs that are snoozed, the number of errors is\n// used instead of the attempt count. This means that snoozing a job (even\n// repeatedly) will not lead to a future error having a longer than expected\n// retry delay.\n//\n// At degenerately high retry counts (>= 310) the policy starts adding the\n// equivalent of the maximum of time.Duration to each retry, about 292 years.\n// The schedule is no longer exponential past this point.\nfunc (p *DefaultClientRetryPolicy) NextRetry(job *rivertype.JobRow) time.Time {\n\t// For the purposes of calculating the backoff, we can look solely at the\n\t// number of errors. If we were to use the raw attempt count, this would be\n\t// incremented and influenced by snoozes. However the use case for snoozing is\n\t// to try again later *without* counting as an error.\n\t//\n\t// Note that we explicitly add 1 here, because the error hasn't been appended\n\t// yet at the time this is called (that happens in the completer).\n\terrorCount := len(job.Errors) + 1\n\n\treturn p.timeNowUTC().Add(timeutil.SecondsAsDuration(p.retrySeconds(errorCount)))\n}\n\nfunc (p *DefaultClientRetryPolicy) timeNowUTC() time.Time {\n\tif p.timeNowFunc != nil {\n\t\treturn p.timeNowFunc()\n\t}\n\n\treturn time.Now().UTC()\n}\n\n// The maximum value of a duration before it overflows. About 292 years.\nconst maxDuration time.Duration = 1<<63 - 1\n\n// Same as the above, but changed to a float represented in seconds.\nvar maxDurationSeconds = maxDuration.Seconds() //nolint:gochecknoglobals\n\n// Gets a number of retry seconds for the given attempt, random jitter included.\nfunc (p *DefaultClientRetryPolicy) retrySeconds(attempt int) float64 {\n\tretrySeconds := p.retrySecondsWithoutJitter(attempt)\n\n\t// After hitting maximum retry durations jitter is no longer applied because\n\t// it might overflow time.Duration. That's okay though because so much\n\t// jitter will already have been applied up to this point (jitter measured\n\t// in decades) that jobs will no longer run anywhere near contemporaneously\n\t// unless there's been considerable manual intervention.\n\tif retrySeconds == maxDurationSeconds {\n\t\treturn maxDurationSeconds\n\t}\n\n\t// Jitter number of seconds +/- 10%.\n\tretrySeconds += retrySeconds * (rand.Float64()*0.2 - 0.1)\n\n\treturn retrySeconds\n}\n\n// Gets a base number of retry seconds for the given attempt, jitter excluded.\n// If the number of seconds returned would overflow time.Duration if it were to\n// be made one, returns the maximum number of seconds that can fit in a\n// time.Duration instead, approximately 292 years.\nfunc (p *DefaultClientRetryPolicy) retrySecondsWithoutJitter(attempt int) float64 {\n\tretrySeconds := math.Pow(float64(attempt), 4)\n\treturn min(retrySeconds, maxDurationSeconds)\n}\n"
        },
        {
          "name": "retry_policy_test.go",
          "type": "blob",
          "size": 6.810546875,
          "content": "package river\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/rivercommon\"\n\t\"github.com/riverqueue/river/rivershared/util/timeutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// Just proves that DefaultRetryPolicy implements the RetryPolicy interface.\nvar _ ClientRetryPolicy = &DefaultClientRetryPolicy{}\n\nfunc TestDefaultClientRetryPolicy_NextRetry(t *testing.T) {\n\tt.Parallel()\n\n\ttype testBundle struct {\n\t\tnow time.Time\n\t}\n\n\tsetup := func(t *testing.T) (*DefaultClientRetryPolicy, *testBundle) {\n\t\tt.Helper()\n\n\t\tvar (\n\t\t\tnow         = time.Now().UTC()\n\t\t\ttimeNowFunc = func() time.Time { return now }\n\t\t)\n\n\t\treturn &DefaultClientRetryPolicy{timeNowFunc: timeNowFunc}, &testBundle{\n\t\t\tnow: now,\n\t\t}\n\t}\n\n\tt.Run(\"Schedule\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tretryPolicy, bundle := setup(t)\n\n\t\tfor attempt := 1; attempt < 10; attempt++ {\n\t\t\tretrySecondsWithoutJitter := retryPolicy.retrySecondsWithoutJitter(attempt)\n\t\t\tallowedDelta := timeutil.SecondsAsDuration(retrySecondsWithoutJitter * 0.2)\n\n\t\t\tnextRetryAt := retryPolicy.NextRetry(&rivertype.JobRow{\n\t\t\t\tAttempt:     attempt,\n\t\t\t\tAttemptedAt: &bundle.now,\n\t\t\t\tErrors:      make([]rivertype.AttemptError, attempt-1),\n\t\t\t})\n\t\t\trequire.WithinDuration(t, bundle.now.Add(timeutil.SecondsAsDuration(retrySecondsWithoutJitter)), nextRetryAt, allowedDelta)\n\t\t}\n\t})\n\n\tt.Run(\"MaxRetryDuration\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tretryPolicy, bundle := setup(t)\n\n\t\tmaxRetryDuration := timeutil.SecondsAsDuration(maxDurationSeconds)\n\n\t\t// First time the maximum will be hit.\n\t\trequire.Equal(t,\n\t\t\tbundle.now.Add(maxRetryDuration),\n\t\t\tretryPolicy.NextRetry(&rivertype.JobRow{\n\t\t\t\tAttempt:     310,\n\t\t\t\tAttemptedAt: &bundle.now,\n\t\t\t\tErrors:      make([]rivertype.AttemptError, 310-1),\n\t\t\t}),\n\t\t)\n\n\t\t// And will be hit on all subsequent attempts as well.\n\t\trequire.Equal(t,\n\t\t\tbundle.now.Add(maxRetryDuration),\n\t\t\tretryPolicy.NextRetry(&rivertype.JobRow{\n\t\t\t\tAttempt:     1_000,\n\t\t\t\tAttemptedAt: &bundle.now,\n\t\t\t\tErrors:      make([]rivertype.AttemptError, 1_000-1),\n\t\t\t}),\n\t\t)\n\t})\n}\n\nfunc TestDefaultRetryPolicy_retrySeconds(t *testing.T) {\n\tt.Parallel()\n\n\tretryPolicy := &DefaultClientRetryPolicy{}\n\n\tfor attempt := 1; attempt < rivercommon.MaxAttemptsDefault; attempt++ {\n\t\tretrySecondsWithoutJitter := retryPolicy.retrySecondsWithoutJitter(attempt)\n\n\t\t// Jitter is number of seconds +/- 10%.\n\t\tretrySecondsMin := timeutil.SecondsAsDuration(retrySecondsWithoutJitter - retrySecondsWithoutJitter*0.1)\n\t\tretrySecondsMax := timeutil.SecondsAsDuration(retrySecondsWithoutJitter + retrySecondsWithoutJitter*0.1)\n\n\t\t// Run a number of times just to make sure we never generate a number\n\t\t// outside of the expected bounds.\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tretryDuration := timeutil.SecondsAsDuration(retryPolicy.retrySeconds(attempt))\n\n\t\t\trequire.GreaterOrEqual(t, retryDuration, retrySecondsMin)\n\t\t\trequire.Less(t, retryDuration, retrySecondsMax)\n\t\t}\n\t}\n}\n\n// This is mostly to give a feeling for what the retry schedule looks like with\n// real values.\nfunc TestDefaultRetryPolicy_retrySecondsWithoutJitter(t *testing.T) {\n\tt.Parallel()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*DefaultClientRetryPolicy, *testBundle) { //nolint:unparam\n\t\tt.Helper()\n\n\t\treturn &DefaultClientRetryPolicy{}, &testBundle{}\n\t}\n\n\tt.Run(\"Schedule\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tretryPolicy, _ := setup(t)\n\n\t\tday := 24 * time.Hour\n\n\t\ttestCases := []struct {\n\t\t\tattempt       int\n\t\t\texpectedRetry time.Duration\n\t\t}{\n\t\t\t{attempt: 1, expectedRetry: 1 * time.Second},\n\t\t\t{attempt: 2, expectedRetry: 16 * time.Second},\n\t\t\t{attempt: 3, expectedRetry: 1*time.Minute + 21*time.Second},\n\t\t\t{attempt: 4, expectedRetry: 4*time.Minute + 16*time.Second},\n\t\t\t{attempt: 5, expectedRetry: 10*time.Minute + 25*time.Second},\n\t\t\t{attempt: 6, expectedRetry: 21*time.Minute + 36*time.Second},\n\t\t\t{attempt: 7, expectedRetry: 40*time.Minute + 1*time.Second},\n\t\t\t{attempt: 8, expectedRetry: 1*time.Hour + 8*time.Minute + 16*time.Second},\n\t\t\t{attempt: 9, expectedRetry: 1*time.Hour + 49*time.Minute + 21*time.Second},\n\t\t\t{attempt: 10, expectedRetry: 2*time.Hour + 46*time.Minute + 40*time.Second},\n\t\t\t{attempt: 11, expectedRetry: 4*time.Hour + 4*time.Minute + 1*time.Second},\n\t\t\t{attempt: 12, expectedRetry: 5*time.Hour + 45*time.Minute + 36*time.Second},\n\t\t\t{attempt: 13, expectedRetry: 7*time.Hour + 56*time.Minute + 1*time.Second},\n\t\t\t{attempt: 14, expectedRetry: 10*time.Hour + 40*time.Minute + 16*time.Second},\n\t\t\t{attempt: 15, expectedRetry: 14*time.Hour + 3*time.Minute + 45*time.Second},\n\t\t\t{attempt: 16, expectedRetry: 18*time.Hour + 12*time.Minute + 16*time.Second},\n\t\t\t{attempt: 17, expectedRetry: 23*time.Hour + 12*time.Minute + 1*time.Second},\n\t\t\t{attempt: 18, expectedRetry: 1*day + 5*time.Hour + 9*time.Minute + 36*time.Second},\n\t\t\t{attempt: 19, expectedRetry: 1*day + 12*time.Hour + 12*time.Minute + 1*time.Second},\n\t\t\t{attempt: 20, expectedRetry: 1*day + 20*time.Hour + 26*time.Minute + 40*time.Second},\n\t\t\t{attempt: 21, expectedRetry: 2*day + 6*time.Hour + 1*time.Minute + 21*time.Second},\n\t\t\t{attempt: 22, expectedRetry: 2*day + 17*time.Hour + 4*time.Minute + 16*time.Second},\n\t\t\t{attempt: 23, expectedRetry: 3*day + 5*time.Hour + 44*time.Minute + 1*time.Second},\n\t\t\t{attempt: 24, expectedRetry: 3*day + 20*time.Hour + 9*time.Minute + 36*time.Second},\n\t\t}\n\t\tfor _, tt := range testCases {\n\t\t\ttt := tt\n\t\t\tt.Run(fmt.Sprintf(\"Attempt_%02d\", tt.attempt), func(t *testing.T) {\n\t\t\t\tt.Parallel()\n\n\t\t\t\trequire.Equal(t,\n\t\t\t\t\ttt.expectedRetry,\n\t\t\t\t\ttime.Duration(retryPolicy.retrySecondsWithoutJitter(tt.attempt))*time.Second)\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"MaxDurationSeconds\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tretryPolicy, _ := setup(t)\n\n\t\trequire.NotEqual(t, time.Duration(maxDurationSeconds)*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(309))*time.Second)\n\n\t\t// Attempt number 310 hits the ceiling, and we'll always hit it from thence on.\n\t\trequire.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(310))*time.Second)\n\t\trequire.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(311))*time.Second)\n\t\trequire.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(1_000))*time.Second)\n\t\trequire.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(1_000_000))*time.Second)\n\t})\n}\n\nfunc TestDefaultRetryPolicy_stress(t *testing.T) {\n\tt.Parallel()\n\n\tvar wg sync.WaitGroup\n\tretryPolicy := &DefaultClientRetryPolicy{}\n\n\t// Hit the source with a bunch of goroutines to help suss out any problems\n\t// with concurrent safety (when combined with `-race`).\n\tfor i := 0; i < 10; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tfor j := 0; j < 100; j++ {\n\t\t\t\t_ = retryPolicy.retrySeconds(7)\n\t\t\t}\n\t\t\twg.Done()\n\t\t}()\n\t}\n\n\twg.Wait()\n}\n"
        },
        {
          "name": "riverdriver",
          "type": "tree",
          "content": null
        },
        {
          "name": "rivermigrate",
          "type": "tree",
          "content": null
        },
        {
          "name": "rivershared",
          "type": "tree",
          "content": null
        },
        {
          "name": "rivertest",
          "type": "tree",
          "content": null
        },
        {
          "name": "rivertype",
          "type": "tree",
          "content": null
        },
        {
          "name": "subscription_manager.go",
          "type": "blob",
          "size": 7.572265625,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/jobstats\"\n\t\"github.com/riverqueue/river/rivershared/baseservice\"\n\t\"github.com/riverqueue/river/rivershared/startstop\"\n\t\"github.com/riverqueue/river/rivershared/util/sliceutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\ntype subscriptionManager struct {\n\tbaseservice.BaseService\n\tstartstop.BaseStartStop\n\n\tsubscribeCh <-chan []jobcompleter.CompleterJobUpdated\n\n\tstatsMu        sync.Mutex // protects stats fields\n\tstatsAggregate jobstats.JobStatistics\n\tstatsNumJobs   int\n\n\tmu               sync.Mutex // protects subscription fields\n\tsubscriptions    map[int]*eventSubscription\n\tsubscriptionsSeq int // used for generating simple IDs\n}\n\nfunc newSubscriptionManager(archetype *baseservice.Archetype, subscribeCh <-chan []jobcompleter.CompleterJobUpdated) *subscriptionManager {\n\treturn baseservice.Init(archetype, &subscriptionManager{\n\t\tsubscribeCh:   subscribeCh,\n\t\tsubscriptions: make(map[int]*eventSubscription),\n\t})\n}\n\n// ResetSubscribeChan is used to change the channel that the subscription\n// manager listens on. It must only be called when the subscription manager is\n// stopped.\nfunc (sm *subscriptionManager) ResetSubscribeChan(subscribeCh <-chan []jobcompleter.CompleterJobUpdated) {\n\tsm.subscribeCh = subscribeCh\n}\n\nfunc (sm *subscriptionManager) Start(ctx context.Context) error {\n\tctx, shouldStart, started, stopped := sm.StartInit(ctx)\n\tif !shouldStart {\n\t\treturn nil\n\t}\n\n\tgo func() {\n\t\tstarted()\n\t\tdefer stopped() // this defer should come first so it's last out\n\n\t\tsm.Logger.DebugContext(ctx, sm.Name+\": Run loop started\")\n\t\tdefer sm.Logger.DebugContext(ctx, sm.Name+\": Run loop stopped\")\n\n\t\t// On shutdown, close and remove all active subscriptions.\n\t\tdefer func() {\n\t\t\tsm.mu.Lock()\n\t\t\tdefer sm.mu.Unlock()\n\n\t\t\tfor subID, sub := range sm.subscriptions {\n\t\t\t\tclose(sub.Chan)\n\t\t\t\tdelete(sm.subscriptions, subID)\n\t\t\t}\n\t\t}()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\t// Distribute remaining subscriptions until the channel is\n\t\t\t\t// closed. This does make the subscription manager a little\n\t\t\t\t// problematic in that it requires the subscription channel to\n\t\t\t\t// be closed before it will fully stop. This always happens in\n\t\t\t\t// the case of a real client by virtue of the completer always\n\t\t\t\t// stopping at the same time as the subscription manager, but\n\t\t\t\t// one has to be careful in tests.\n\t\t\t\tsm.Logger.DebugContext(ctx, sm.Name+\": Stopping; distributing subscriptions until channel is closed\")\n\t\t\t\tfor updates := range sm.subscribeCh {\n\t\t\t\t\tsm.distributeJobUpdates(updates)\n\t\t\t\t}\n\n\t\t\t\treturn\n\n\t\t\tcase updates := <-sm.subscribeCh:\n\t\t\t\tsm.distributeJobUpdates(updates)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\nfunc (sm *subscriptionManager) logStats(ctx context.Context, svcName string) {\n\tsm.statsMu.Lock()\n\tdefer sm.statsMu.Unlock()\n\n\tsm.Logger.DebugContext(ctx, svcName+\": Job stats (since last stats line)\",\n\t\t\"num_jobs_run\", sm.statsNumJobs,\n\t\t\"average_complete_duration\", sm.safeDurationAverage(sm.statsAggregate.CompleteDuration, sm.statsNumJobs),\n\t\t\"average_queue_wait_duration\", sm.safeDurationAverage(sm.statsAggregate.QueueWaitDuration, sm.statsNumJobs),\n\t\t\"average_run_duration\", sm.safeDurationAverage(sm.statsAggregate.RunDuration, sm.statsNumJobs))\n\n\tsm.statsAggregate = jobstats.JobStatistics{}\n\tsm.statsNumJobs = 0\n}\n\n// Handles a potential divide by zero.\nfunc (sm *subscriptionManager) safeDurationAverage(d time.Duration, n int) time.Duration {\n\tif n == 0 {\n\t\treturn 0\n\t}\n\treturn d / time.Duration(n)\n}\n\n// Receives updates from the completer and prompts the client to update\n// statistics and distribute jobs into any listening subscriber channels.\n// (Subscriber channels are non-blocking so this should be quite fast.)\nfunc (sm *subscriptionManager) distributeJobUpdates(updates []jobcompleter.CompleterJobUpdated) {\n\tfunc() {\n\t\tsm.statsMu.Lock()\n\t\tdefer sm.statsMu.Unlock()\n\n\t\tfor _, update := range updates {\n\t\t\tstats := update.JobStats\n\t\t\tsm.statsAggregate.CompleteDuration += stats.CompleteDuration\n\t\t\tsm.statsAggregate.QueueWaitDuration += stats.QueueWaitDuration\n\t\t\tsm.statsAggregate.RunDuration += stats.RunDuration\n\t\t\tsm.statsNumJobs++\n\t\t}\n\t}()\n\n\tsm.mu.Lock()\n\tdefer sm.mu.Unlock()\n\n\t// Quick path so we don't need to allocate anything if no one is listening.\n\tif len(sm.subscriptions) < 1 {\n\t\treturn\n\t}\n\n\tfor _, update := range updates {\n\t\tsm.distributeJobEvent(update.Job, jobStatisticsFromInternal(update.JobStats))\n\t}\n}\n\n// Distribute a single event into any listening subscriber channels.\n//\n// Job events should specify the job and stats, while queue events should only specify\n// the queue.\n//\n// MUST be called with sm.mu already held.\nfunc (sm *subscriptionManager) distributeJobEvent(job *rivertype.JobRow, stats *JobStatistics) {\n\tvar event *Event\n\tswitch job.State {\n\tcase rivertype.JobStateCancelled:\n\t\tevent = &Event{Kind: EventKindJobCancelled, Job: job, JobStats: stats}\n\tcase rivertype.JobStateCompleted:\n\t\tevent = &Event{Kind: EventKindJobCompleted, Job: job, JobStats: stats}\n\tcase rivertype.JobStateScheduled:\n\t\tevent = &Event{Kind: EventKindJobSnoozed, Job: job, JobStats: stats}\n\tcase rivertype.JobStateAvailable, rivertype.JobStateDiscarded, rivertype.JobStateRetryable, rivertype.JobStateRunning:\n\t\tevent = &Event{Kind: EventKindJobFailed, Job: job, JobStats: stats}\n\tcase rivertype.JobStatePending:\n\t\tpanic(\"completion subscriber unexpectedly received job in pending state, river bug\")\n\tdefault:\n\t\t// linter exhaustive rule prevents this from being reached\n\t\tpanic(\"unreachable state to distribute, river bug\")\n\t}\n\n\t// All subscription channels are non-blocking so this is always fast and\n\t// there's no risk of falling behind what producers are sending.\n\tfor _, sub := range sm.subscriptions {\n\t\tif sub.ListensFor(event.Kind) {\n\t\t\t// TODO: THIS IS UNSAFE AND WILL LEAD TO DROPPED EVENTS.\n\t\t\t//\n\t\t\t// We are allocating subscriber channels with a fixed size of 1000, but\n\t\t\t// potentially processing job events in batches of 5000 (batch completer\n\t\t\t// max batch size). It's probably not possible for the subscriber to keep\n\t\t\t// up with these bursts.\n\t\t\tselect {\n\t\t\tcase sub.Chan <- event:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (sm *subscriptionManager) distributeQueueEvent(event *Event) {\n\tsm.mu.Lock()\n\tdefer sm.mu.Unlock()\n\n\t// All subscription channels are non-blocking so this is always fast and\n\t// there's no risk of falling behind what producers are sending.\n\tfor _, sub := range sm.subscriptions {\n\t\tif sub.ListensFor(event.Kind) {\n\t\t\tselect {\n\t\t\tcase sub.Chan <- event:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Special internal variant that lets us inject an overridden size.\nfunc (sm *subscriptionManager) SubscribeConfig(config *SubscribeConfig) (<-chan *Event, func()) {\n\tif config.ChanSize < 0 {\n\t\tpanic(\"SubscribeConfig.ChanSize must be greater or equal to 1\")\n\t}\n\tif config.ChanSize == 0 {\n\t\tconfig.ChanSize = subscribeChanSizeDefault\n\t}\n\n\tfor _, kind := range config.Kinds {\n\t\tif _, ok := allKinds[kind]; !ok {\n\t\t\tpanic(fmt.Errorf(\"unknown event kind: %s\", kind))\n\t\t}\n\t}\n\n\tsubChan := make(chan *Event, config.ChanSize)\n\n\tsm.mu.Lock()\n\tdefer sm.mu.Unlock()\n\n\t// Just gives us an easy way of removing the subscription again later.\n\tsubID := sm.subscriptionsSeq\n\tsm.subscriptionsSeq++\n\n\tsm.subscriptions[subID] = &eventSubscription{\n\t\tChan:  subChan,\n\t\tKinds: sliceutil.KeyBy(config.Kinds, func(k EventKind) (EventKind, struct{}) { return k, struct{}{} }),\n\t}\n\n\tcancel := func() {\n\t\tsm.mu.Lock()\n\t\tdefer sm.mu.Unlock()\n\n\t\t// May no longer be present in case this was called after a stop.\n\t\tsub, ok := sm.subscriptions[subID]\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\n\t\tclose(sub.Chan)\n\n\t\tdelete(sm.subscriptions, subID)\n\t}\n\n\treturn subChan, cancel\n}\n"
        },
        {
          "name": "subscription_manager_test.go",
          "type": "blob",
          "size": 4.8935546875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/jobcompleter\"\n\t\"github.com/riverqueue/river/internal/jobstats\"\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/riverdriver\"\n\t\"github.com/riverqueue/river/riverdriver/riverpgxv5\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n\t\"github.com/riverqueue/river/rivershared/startstoptest\"\n\t\"github.com/riverqueue/river/rivershared/testfactory\"\n\t\"github.com/riverqueue/river/rivershared/util/ptrutil\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\nfunc Test_SubscriptionManager(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct {\n\t\texec        riverdriver.Executor\n\t\tsubscribeCh chan []jobcompleter.CompleterJobUpdated\n\t\ttx          pgx.Tx\n\t}\n\n\tsetup := func(t *testing.T) (*subscriptionManager, *testBundle) {\n\t\tt.Helper()\n\n\t\ttx := riverinternaltest.TestTx(ctx, t)\n\t\texec := riverpgxv5.New(nil).UnwrapExecutor(tx)\n\n\t\tsubscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 1)\n\t\tmanager := newSubscriptionManager(riversharedtest.BaseServiceArchetype(t), subscribeCh)\n\n\t\trequire.NoError(t, manager.Start(ctx))\n\t\tt.Cleanup(manager.Stop)\n\n\t\treturn manager, &testBundle{\n\t\t\texec:        exec,\n\t\t\tsubscribeCh: subscribeCh,\n\t\t\ttx:          tx,\n\t\t}\n\t}\n\n\tt.Run(\"DistributesRequestedEventsToSubscribers\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tmanager, bundle := setup(t)\n\t\tt.Cleanup(func() { close(bundle.subscribeCh) })\n\n\t\tsub, cancelSub := manager.SubscribeConfig(&SubscribeConfig{ChanSize: 10, Kinds: []EventKind{EventKindJobCompleted, EventKindJobSnoozed}})\n\t\tt.Cleanup(cancelSub)\n\n\t\t// Send some events\n\t\tjob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(time.Now())})\n\t\tjob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(time.Now())})\n\t\tjob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable)})\n\t\tjob4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled)})\n\n\t\tmakeStats := func(complete, wait, run time.Duration) *jobstats.JobStatistics {\n\t\t\treturn &jobstats.JobStatistics{\n\t\t\t\tCompleteDuration:  complete,\n\t\t\t\tQueueWaitDuration: wait,\n\t\t\t\tRunDuration:       run,\n\t\t\t}\n\t\t}\n\n\t\tbundle.subscribeCh <- []jobcompleter.CompleterJobUpdated{\n\t\t\t{Job: job1, JobStats: makeStats(101, 102, 103)}, // completed, should be sent\n\t\t\t{Job: job2, JobStats: makeStats(201, 202, 203)}, // cancelled, should be skipped\n\t\t}\n\t\tbundle.subscribeCh <- []jobcompleter.CompleterJobUpdated{\n\t\t\t{Job: job3, JobStats: makeStats(301, 302, 303)}, // retryable, should be skipped\n\t\t\t{Job: job4, JobStats: makeStats(401, 402, 403)}, // snoozed/scheduled, should be sent\n\t\t}\n\n\t\treceived := riversharedtest.WaitOrTimeoutN(t, sub, 2)\n\t\trequire.Equal(t, job1.ID, received[0].Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateCompleted, received[0].Job.State)\n\t\trequire.Equal(t, time.Duration(101), received[0].JobStats.CompleteDuration)\n\t\trequire.Equal(t, time.Duration(102), received[0].JobStats.QueueWaitDuration)\n\t\trequire.Equal(t, time.Duration(103), received[0].JobStats.RunDuration)\n\t\trequire.Equal(t, job4.ID, received[1].Job.ID)\n\t\trequire.Equal(t, rivertype.JobStateScheduled, received[1].Job.State)\n\t\trequire.Equal(t, time.Duration(401), received[1].JobStats.CompleteDuration)\n\t\trequire.Equal(t, time.Duration(402), received[1].JobStats.QueueWaitDuration)\n\t\trequire.Equal(t, time.Duration(403), received[1].JobStats.RunDuration)\n\n\t\tcancelSub()\n\t\tselect {\n\t\tcase value, stillOpen := <-sub:\n\t\t\trequire.False(t, stillOpen, \"subscription channel should be closed\")\n\t\t\trequire.Nil(t, value, \"subscription channel should be closed\")\n\t\tdefault:\n\t\t\trequire.Fail(t, \"subscription channel should have been closed\")\n\t\t}\n\t})\n\n\tt.Run(\"StartStopRepeatedly\", func(t *testing.T) {\n\t\t// This service does not use the typical `startstoptest.Stress()` test\n\t\t// because there are some additional steps required after a `Stop` for the\n\t\t// subsequent `Start` to succeed. It's also not friendly for multiple\n\t\t// concurrent calls to `Start` and `Stop`, but this is fine because the only\n\t\t// usage within `Client` is already protected by a mutex.\n\t\tt.Parallel()\n\n\t\tmanager, bundle := setup(t)\n\n\t\tsubscribeCh := bundle.subscribeCh\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tclose(subscribeCh)\n\t\t\tmanager.Stop()\n\n\t\t\tsubscribeCh = make(chan []jobcompleter.CompleterJobUpdated, 1)\n\t\t\tmanager.ResetSubscribeChan(subscribeCh)\n\n\t\t\trequire.NoError(t, manager.Start(ctx))\n\t\t}\n\t\tclose(subscribeCh)\n\t})\n\n\tt.Run(\"StartStopStress\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tsvc, bundle := setup(t)\n\n\t\t// Close the subscription channel in advance so that stops can leave\n\t\t// successfully.\n\t\tclose(bundle.subscribeCh)\n\n\t\tstartstoptest.Stress(ctx, t, svc)\n\t})\n}\n"
        },
        {
          "name": "work_unit_wrapper.go",
          "type": "blob",
          "size": 1.19140625,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// workUnitFactoryWrapper wraps a Worker to implement workUnitFactory.\ntype workUnitFactoryWrapper[T JobArgs] struct {\n\tworker Worker[T]\n}\n\nfunc (w *workUnitFactoryWrapper[T]) MakeUnit(jobRow *rivertype.JobRow) workunit.WorkUnit {\n\treturn &wrapperWorkUnit[T]{jobRow: jobRow, worker: w.worker}\n}\n\n// wrapperWorkUnit implements workUnit for a job and Worker.\ntype wrapperWorkUnit[T JobArgs] struct {\n\tjob    *Job[T] // not set until after UnmarshalJob is invoked\n\tjobRow *rivertype.JobRow\n\tworker Worker[T]\n}\n\nfunc (w *wrapperWorkUnit[T]) NextRetry() time.Time           { return w.worker.NextRetry(w.job) }\nfunc (w *wrapperWorkUnit[T]) Timeout() time.Duration         { return w.worker.Timeout(w.job) }\nfunc (w *wrapperWorkUnit[T]) Work(ctx context.Context) error { return w.worker.Work(ctx, w.job) }\n\nfunc (w *wrapperWorkUnit[T]) Middleware() []rivertype.WorkerMiddleware {\n\treturn w.worker.Middleware(w.job)\n}\n\nfunc (w *wrapperWorkUnit[T]) UnmarshalJob() error {\n\tw.job = &Job[T]{\n\t\tJobRow: w.jobRow,\n\t}\n\n\treturn json.Unmarshal(w.jobRow.EncodedArgs, &w.job.Args)\n}\n"
        },
        {
          "name": "worker.go",
          "type": "blob",
          "size": 6.5888671875,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/riverqueue/river/internal/workunit\"\n\t\"github.com/riverqueue/river/rivertype\"\n)\n\n// Worker is an interface that can perform a job with args of type T. A typical\n// implementation will be a JSON-serializable `JobArgs` struct that implements\n// `Kind()`, along with a Worker that embeds WorkerDefaults and implements `Work()`.\n// Workers may optionally override other methods to provide job-specific\n// configuration for all jobs of that type:\n//\n//\ttype SleepArgs struct {\n//\t\tDuration time.Duration `json:\"duration\"`\n//\t}\n//\n//\tfunc (SleepArgs) Kind() string { return \"sleep\" }\n//\n//\ttype SleepWorker struct {\n//\t\tWorkerDefaults[SleepArgs]\n//\t}\n//\n//\tfunc (w *SleepWorker) Work(ctx context.Context, job *Job[SleepArgs]) error {\n//\t\tselect {\n//\t\tcase <-ctx.Done():\n//\t\t\treturn ctx.Err()\n//\t\tcase <-time.After(job.Args.Duration):\n//\t\t\treturn nil\n//\t\t}\n//\t}\n//\n// In addition to fulfilling the Worker interface, workers must be registered\n// with the client using the AddWorker function.\ntype Worker[T JobArgs] interface {\n\t// Middleware returns the type-specific middleware for this job.\n\tMiddleware(job *Job[T]) []rivertype.WorkerMiddleware\n\n\t// NextRetry calculates when the next retry for a failed job should take\n\t// place given when it was last attempted and its number of attempts, or any\n\t// other of the job's properties a user-configured retry policy might want\n\t// to consider.\n\t//\n\t// Note that this method on a worker overrides any client-level retry policy.\n\t// To use the client-level retry policy, return an empty `time.Time{}` or\n\t// include WorkerDefaults to do this for you.\n\tNextRetry(job *Job[T]) time.Time\n\n\t// Timeout is the maximum amount of time the job is allowed to run before\n\t// its context is cancelled. A timeout of zero (the default) means the job\n\t// will inherit the Client-level timeout. A timeout of -1 means the job's\n\t// context will never time out.\n\tTimeout(job *Job[T]) time.Duration\n\n\t// Work performs the job and returns an error if the job failed. The context\n\t// will be configured with a timeout according to the worker settings and may\n\t// be cancelled for other reasons.\n\t//\n\t// If no error is returned, the job is assumed to have succeeded and will be\n\t// marked completed.\n\t//\n\t// It is important for any worker to respect context cancellation to enable\n\t// the client to respond to shutdown requests; there is no way to cancel a\n\t// running job that does not respect context cancellation, other than\n\t// terminating the process.\n\tWork(ctx context.Context, job *Job[T]) error\n}\n\n// WorkerDefaults is an empty struct that can be embedded in your worker\n// struct to make it fulfill the Worker interface with default values.\ntype WorkerDefaults[T JobArgs] struct{}\n\nfunc (w WorkerDefaults[T]) Middleware(*Job[T]) []rivertype.WorkerMiddleware { return nil }\n\n// NextRetry returns an empty time.Time{} to avoid setting any job or\n// Worker-specific overrides on the next retry time. This means that the\n// Client-level retry policy schedule will be used instead.\nfunc (w WorkerDefaults[T]) NextRetry(*Job[T]) time.Time { return time.Time{} }\n\n// Timeout returns the job-specific timeout. Override this method to set a\n// job-specific timeout, otherwise the Client-level timeout will be applied.\nfunc (w WorkerDefaults[T]) Timeout(*Job[T]) time.Duration { return 0 }\n\n// AddWorker registers a Worker on the provided Workers bundle. Each Worker must\n// be registered so that the Client knows it should handle a specific kind of\n// job (as returned by its `Kind()` method).\n//\n// Use by explicitly specifying a JobArgs type and then passing an instance of a\n// worker for the same type:\n//\n//\triver.AddWorker(workers, &SortWorker{})\n//\n// Note that AddWorker can panic in some situations, such as if the worker is\n// already registered or if its configuration is otherwise invalid. This default\n// probably makes sense for most applications because you wouldn't want to start\n// an application with invalid hardcoded runtime configuration. If you want to\n// avoid panics, use AddWorkerSafely instead.\nfunc AddWorker[T JobArgs](workers *Workers, worker Worker[T]) {\n\tif err := AddWorkerSafely[T](workers, worker); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// AddWorkerSafely registers a worker on the provided Workers bundle. Unlike AddWorker,\n// AddWorkerSafely does not panic and instead returns an error if the worker\n// is already registered or if its configuration is invalid.\n//\n// Use by explicitly specifying a JobArgs type and then passing an instance of a\n// worker for the same type:\n//\n//\triver.AddWorkerSafely[SortArgs](workers, &SortWorker{}).\nfunc AddWorkerSafely[T JobArgs](workers *Workers, worker Worker[T]) error {\n\tvar jobArgs T\n\treturn workers.add(jobArgs, &workUnitFactoryWrapper[T]{worker: worker})\n}\n\n// Workers is a list of available job workers. A Worker must be registered for\n// each type of Job to be handled.\n//\n// Use the top-level AddWorker function combined with a Workers to register a\n// worker.\ntype Workers struct {\n\tworkersMap map[string]workerInfo // job kind -> worker info\n}\n\n// workerInfo bundles information about a registered worker for later lookup\n// in a Workers bundle.\ntype workerInfo struct {\n\tjobArgs         JobArgs\n\tworkUnitFactory workunit.WorkUnitFactory\n}\n\n// NewWorkers initializes a new registry of available job workers.\n//\n// Use the top-level AddWorker function combined with a Workers registry to\n// register each available worker.\nfunc NewWorkers() *Workers {\n\treturn &Workers{\n\t\tworkersMap: make(map[string]workerInfo),\n\t}\n}\n\nfunc (w Workers) add(jobArgs JobArgs, workUnitFactory workunit.WorkUnitFactory) error {\n\tkind := jobArgs.Kind()\n\n\tif _, ok := w.workersMap[kind]; ok {\n\t\treturn fmt.Errorf(\"worker for kind %q is already registered\", kind)\n\t}\n\n\tw.workersMap[kind] = workerInfo{\n\t\tjobArgs:         jobArgs,\n\t\tworkUnitFactory: workUnitFactory,\n\t}\n\n\treturn nil\n}\n\n// workFunc implements JobArgs and is used to wrap a function given to WorkFunc.\ntype workFunc[T JobArgs] struct {\n\tWorkerDefaults[T]\n\tkind string\n\tf    func(context.Context, *Job[T]) error\n}\n\nfunc (wf *workFunc[T]) Kind() string {\n\treturn wf.kind\n}\n\nfunc (wf *workFunc[T]) Work(ctx context.Context, job *Job[T]) error {\n\treturn wf.f(ctx, job)\n}\n\n// WorkFunc wraps a function to implement the Worker interface. A job args\n// struct implementing JobArgs will still be required to specify a Kind.\n//\n// For example:\n//\n//\triver.AddWorker(workers, river.WorkFunc(func(ctx context.Context, job *river.Job[WorkFuncArgs]) error {\n//\t\tfmt.Printf(\"Message: %s\", job.Args.Message)\n//\t\treturn nil\n//\t}))\nfunc WorkFunc[T JobArgs](f func(context.Context, *Job[T]) error) Worker[T] {\n\treturn &workFunc[T]{f: f, kind: (*new(T)).Kind()}\n}\n"
        },
        {
          "name": "worker_test.go",
          "type": "blob",
          "size": 3.484375,
          "content": "package river\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/riverqueue/river/internal/riverinternaltest\"\n\t\"github.com/riverqueue/river/rivershared/riversharedtest\"\n)\n\nfunc TestWork(t *testing.T) {\n\tt.Parallel()\n\n\tworkers := NewWorkers()\n\n\tAddWorker(workers, &noOpWorker{})\n\trequire.Contains(t, workers.workersMap, (noOpArgs{}).Kind())\n\n\trequire.PanicsWithError(t, `worker for kind \"noOp\" is already registered`, func() {\n\t\tAddWorker(workers, &noOpWorker{})\n\t})\n\n\tfn := func(ctx context.Context, job *Job[callbackArgs]) error { return nil }\n\tch := callbackWorker{fn: fn}\n\n\t// function worker\n\tAddWorker(workers, &ch)\n\trequire.Contains(t, workers.workersMap, (callbackArgs{}).Kind())\n}\n\ntype configurableArgs struct {\n\tuniqueOpts UniqueOpts\n}\n\nfunc (a configurableArgs) Kind() string { return \"configurable\" }\nfunc (a configurableArgs) InsertOpts() InsertOpts {\n\treturn InsertOpts{UniqueOpts: a.uniqueOpts}\n}\n\ntype configurableWorker struct {\n\tWorkerDefaults[configurableArgs]\n}\n\nfunc (w *configurableWorker) Work(ctx context.Context, job *Job[configurableArgs]) error {\n\treturn nil\n}\n\nfunc TestWorkers_add(t *testing.T) {\n\tt.Parallel()\n\n\tworkers := NewWorkers()\n\n\terr := workers.add(noOpArgs{}, &workUnitFactoryWrapper[noOpArgs]{worker: &noOpWorker{}})\n\trequire.NoError(t, err)\n\n\t// Different worker kind.\n\terr = workers.add(configurableArgs{}, &workUnitFactoryWrapper[configurableArgs]{worker: &configurableWorker{}})\n\trequire.NoError(t, err)\n\n\terr = workers.add(noOpArgs{}, &workUnitFactoryWrapper[noOpArgs]{worker: &noOpWorker{}})\n\trequire.EqualError(t, err, `worker for kind \"noOp\" is already registered`)\n}\n\ntype WorkFuncArgs struct{}\n\nfunc (WorkFuncArgs) Kind() string { return \"work_func\" }\n\ntype StructWithFunc struct {\n\tWorkChan chan struct{}\n}\n\nfunc (s *StructWithFunc) Work(ctx context.Context, job *Job[WorkFuncArgs]) error {\n\ts.WorkChan <- struct{}{}\n\treturn nil\n}\n\nfunc TestWorkFunc(t *testing.T) {\n\tt.Parallel()\n\n\tctx := context.Background()\n\n\ttype testBundle struct{}\n\n\tsetup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {\n\t\tt.Helper()\n\n\t\tdbPool := riverinternaltest.TestDB(ctx, t)\n\n\t\tclient := newTestClient(t, dbPool, newTestConfig(t, nil))\n\t\tstartClient(ctx, t, client)\n\n\t\treturn client, &testBundle{}\n\t}\n\n\tt.Run(\"RoundTrip\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tworkChan := make(chan struct{})\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[WorkFuncArgs]) error {\n\t\t\tworkChan <- struct{}{}\n\t\t\treturn nil\n\t\t}))\n\n\t\t_, err := client.Insert(ctx, &WorkFuncArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, workChan)\n\t})\n\n\tt.Run(\"StructFunction\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\tstructWithFunc := &StructWithFunc{\n\t\t\tWorkChan: make(chan struct{}),\n\t\t}\n\n\t\tAddWorker(client.config.Workers, WorkFunc(structWithFunc.Work))\n\n\t\t_, err := client.Insert(ctx, &WorkFuncArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, structWithFunc.WorkChan)\n\t})\n\n\tt.Run(\"JobArgsReflectKind\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tclient, _ := setup(t)\n\n\t\ttype InFuncWorkFuncArgs struct {\n\t\t\tJobArgsReflectKind[InFuncWorkFuncArgs]\n\t\t}\n\n\t\tworkChan := make(chan struct{})\n\t\tAddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[InFuncWorkFuncArgs]) error {\n\t\t\tworkChan <- struct{}{}\n\t\t\treturn nil\n\t\t}))\n\n\t\t_, err := client.Insert(ctx, &InFuncWorkFuncArgs{}, nil)\n\t\trequire.NoError(t, err)\n\n\t\triversharedtest.WaitOrTimeout(t, workChan)\n\t})\n}\n"
        }
      ]
    }
  ]
}