{
  "metadata": {
    "timestamp": 1736567072891,
    "page": 658,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "go-mgo/mgo",
      "stars": 2734,
      "defaultBranch": "v2-unstable",
      "files": [
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.021484375,
          "content": "language: go\n\ngo_import_path: gopkg.in/mgo.v2\n\naddons:\n    apt:\n        packages:\n\nenv:\n    global:\n        - BUCKET=https://niemeyer.s3.amazonaws.com\n    matrix:\n        - GO=1.4.1 MONGODB=x86_64-2.2.7\n        - GO=1.4.1 MONGODB=x86_64-2.4.14\n        - GO=1.4.1 MONGODB=x86_64-2.6.11\n        - GO=1.4.1 MONGODB=x86_64-3.0.9\n        - GO=1.4.1 MONGODB=x86_64-3.2.3-nojournal\n        - GO=1.5.3 MONGODB=x86_64-3.0.9\n        - GO=1.6   MONGODB=x86_64-3.0.9\n\ninstall:\n    - eval \"$(gimme $GO)\"\n\n    - wget $BUCKET/mongodb-linux-$MONGODB.tgz\n    - tar xzvf mongodb-linux-$MONGODB.tgz\n    - export PATH=$PWD/mongodb-linux-$MONGODB/bin:$PATH\n\n    - wget $BUCKET/daemontools.tar.gz\n    - tar xzvf daemontools.tar.gz\n    - export PATH=$PWD/daemontools:$PATH\n\n    - go get gopkg.in/check.v1\n    - go get gopkg.in/yaml.v2\n    - go get gopkg.in/tomb.v2\n\nbefore_script:\n    - export NOIPV6=1\n    - make startdb\n\nscript:\n    - (cd bson && go test -check.v)\n    - go test -check.v -fast\n    - (cd txn && go test -check.v)\n    - make stopdb\n\n# vim:sw=4:ts=4:et\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.33203125,
          "content": "mgo - MongoDB driver for Go\n\nCopyright (c) 2010-2013 - Gustavo Niemeyer <gustavo@niemeyer.net>\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met: \n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer. \n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution. \n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.0654296875,
          "content": "startdb:\n\t@harness/setup.sh start\n\nstopdb:\n\t@harness/setup.sh stop\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.091796875,
          "content": "THIS IS UNMAINTAINED\n--------------------\n\nSeven years after creating the mgo driver, I'm formally pausing my work on its maintenance.\nThere are multiple reasons for that, but the main ones are that I've stopped using MongoDB\nfor any new projects, and supporting its good community was taking too much of my\npersonal time without a relevant benefit for those around me.\n\nMoving forward I would suggest you to look at one of these options:\n\n  * [globalsign/mgo](https://github.com/globalsign/mgo) - Community supported fork of mgo.\n  * [BoltDB](https://github.com/coreos/bbolt) - Single file in-memory document database for Go.\n  * [Badger](https://github.com/dgraph-io/badger) - Fast in-memory document database for Go.\n  * [DGraph](https://github.com/dgraph-io/dgraph) - Distributed graph database on top of Badger.\n  * [lib/pq](https://github.com/lib/pq) - PostgreSQL driver in pure Go.\n\nFor technical questions related to mgo, [Stack Overflow](https://stackoverflow.com/questions/tagged/mgo)\nis the best place to continue obtaining support from the community.\n\nFor personal contact, gustavo at http://niemeyer.net.\n"
        },
        {
          "name": "auth.go",
          "type": "blob",
          "size": 12.1376953125,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"crypto/md5\"\n\t\"crypto/sha1\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n\t\"gopkg.in/mgo.v2/internal/scram\"\n)\n\ntype authCmd struct {\n\tAuthenticate int\n\n\tNonce string\n\tUser  string\n\tKey   string\n}\n\ntype startSaslCmd struct {\n\tStartSASL int `bson:\"startSasl\"`\n}\n\ntype authResult struct {\n\tErrMsg string\n\tOk     bool\n}\n\ntype getNonceCmd struct {\n\tGetNonce int\n}\n\ntype getNonceResult struct {\n\tNonce string\n\tErr   string \"$err\"\n\tCode  int\n}\n\ntype logoutCmd struct {\n\tLogout int\n}\n\ntype saslCmd struct {\n\tStart          int    `bson:\"saslStart,omitempty\"`\n\tContinue       int    `bson:\"saslContinue,omitempty\"`\n\tConversationId int    `bson:\"conversationId,omitempty\"`\n\tMechanism      string `bson:\"mechanism,omitempty\"`\n\tPayload        []byte\n}\n\ntype saslResult struct {\n\tOk    bool `bson:\"ok\"`\n\tNotOk bool `bson:\"code\"` // Server <= 2.3.2 returns ok=1 & code>0 on errors (WTF?)\n\tDone  bool\n\n\tConversationId int `bson:\"conversationId\"`\n\tPayload        []byte\n\tErrMsg         string\n}\n\ntype saslStepper interface {\n\tStep(serverData []byte) (clientData []byte, done bool, err error)\n\tClose()\n}\n\nfunc (socket *mongoSocket) getNonce() (nonce string, err error) {\n\tsocket.Lock()\n\tfor socket.cachedNonce == \"\" && socket.dead == nil {\n\t\tdebugf(\"Socket %p to %s: waiting for nonce\", socket, socket.addr)\n\t\tsocket.gotNonce.Wait()\n\t}\n\tif socket.cachedNonce == \"mongos\" {\n\t\tsocket.Unlock()\n\t\treturn \"\", errors.New(\"Can't authenticate with mongos; see http://j.mp/mongos-auth\")\n\t}\n\tdebugf(\"Socket %p to %s: got nonce\", socket, socket.addr)\n\tnonce, err = socket.cachedNonce, socket.dead\n\tsocket.cachedNonce = \"\"\n\tsocket.Unlock()\n\tif err != nil {\n\t\tnonce = \"\"\n\t}\n\treturn\n}\n\nfunc (socket *mongoSocket) resetNonce() {\n\tdebugf(\"Socket %p to %s: requesting a new nonce\", socket, socket.addr)\n\top := &queryOp{}\n\top.query = &getNonceCmd{GetNonce: 1}\n\top.collection = \"admin.$cmd\"\n\top.limit = -1\n\top.replyFunc = func(err error, reply *replyOp, docNum int, docData []byte) {\n\t\tif err != nil {\n\t\t\tsocket.kill(errors.New(\"getNonce: \"+err.Error()), true)\n\t\t\treturn\n\t\t}\n\t\tresult := &getNonceResult{}\n\t\terr = bson.Unmarshal(docData, &result)\n\t\tif err != nil {\n\t\t\tsocket.kill(errors.New(\"Failed to unmarshal nonce: \"+err.Error()), true)\n\t\t\treturn\n\t\t}\n\t\tdebugf(\"Socket %p to %s: nonce unmarshalled: %#v\", socket, socket.addr, result)\n\t\tif result.Code == 13390 {\n\t\t\t// mongos doesn't yet support auth (see http://j.mp/mongos-auth)\n\t\t\tresult.Nonce = \"mongos\"\n\t\t} else if result.Nonce == \"\" {\n\t\t\tvar msg string\n\t\t\tif result.Err != \"\" {\n\t\t\t\tmsg = fmt.Sprintf(\"Got an empty nonce: %s (%d)\", result.Err, result.Code)\n\t\t\t} else {\n\t\t\t\tmsg = \"Got an empty nonce\"\n\t\t\t}\n\t\t\tsocket.kill(errors.New(msg), true)\n\t\t\treturn\n\t\t}\n\t\tsocket.Lock()\n\t\tif socket.cachedNonce != \"\" {\n\t\t\tsocket.Unlock()\n\t\t\tpanic(\"resetNonce: nonce already cached\")\n\t\t}\n\t\tsocket.cachedNonce = result.Nonce\n\t\tsocket.gotNonce.Signal()\n\t\tsocket.Unlock()\n\t}\n\terr := socket.Query(op)\n\tif err != nil {\n\t\tsocket.kill(errors.New(\"resetNonce: \"+err.Error()), true)\n\t}\n}\n\nfunc (socket *mongoSocket) Login(cred Credential) error {\n\tsocket.Lock()\n\tif cred.Mechanism == \"\" && socket.serverInfo.MaxWireVersion >= 3 {\n\t\tcred.Mechanism = \"SCRAM-SHA-1\"\n\t}\n\tfor _, sockCred := range socket.creds {\n\t\tif sockCred == cred {\n\t\t\tdebugf(\"Socket %p to %s: login: db=%q user=%q (already logged in)\", socket, socket.addr, cred.Source, cred.Username)\n\t\t\tsocket.Unlock()\n\t\t\treturn nil\n\t\t}\n\t}\n\tif socket.dropLogout(cred) {\n\t\tdebugf(\"Socket %p to %s: login: db=%q user=%q (cached)\", socket, socket.addr, cred.Source, cred.Username)\n\t\tsocket.creds = append(socket.creds, cred)\n\t\tsocket.Unlock()\n\t\treturn nil\n\t}\n\tsocket.Unlock()\n\n\tdebugf(\"Socket %p to %s: login: db=%q user=%q\", socket, socket.addr, cred.Source, cred.Username)\n\n\tvar err error\n\tswitch cred.Mechanism {\n\tcase \"\", \"MONGODB-CR\", \"MONGO-CR\": // Name changed to MONGODB-CR in SERVER-8501.\n\t\terr = socket.loginClassic(cred)\n\tcase \"PLAIN\":\n\t\terr = socket.loginPlain(cred)\n\tcase \"MONGODB-X509\":\n\t\terr = socket.loginX509(cred)\n\tdefault:\n\t\t// Try SASL for everything else, if it is available.\n\t\terr = socket.loginSASL(cred)\n\t}\n\n\tif err != nil {\n\t\tdebugf(\"Socket %p to %s: login error: %s\", socket, socket.addr, err)\n\t} else {\n\t\tdebugf(\"Socket %p to %s: login successful\", socket, socket.addr)\n\t}\n\treturn err\n}\n\nfunc (socket *mongoSocket) loginClassic(cred Credential) error {\n\t// Note that this only works properly because this function is\n\t// synchronous, which means the nonce won't get reset while we're\n\t// using it and any other login requests will block waiting for a\n\t// new nonce provided in the defer call below.\n\tnonce, err := socket.getNonce()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer socket.resetNonce()\n\n\tpsum := md5.New()\n\tpsum.Write([]byte(cred.Username + \":mongo:\" + cred.Password))\n\n\tksum := md5.New()\n\tksum.Write([]byte(nonce + cred.Username))\n\tksum.Write([]byte(hex.EncodeToString(psum.Sum(nil))))\n\n\tkey := hex.EncodeToString(ksum.Sum(nil))\n\n\tcmd := authCmd{Authenticate: 1, User: cred.Username, Nonce: nonce, Key: key}\n\tres := authResult{}\n\treturn socket.loginRun(cred.Source, &cmd, &res, func() error {\n\t\tif !res.Ok {\n\t\t\treturn errors.New(res.ErrMsg)\n\t\t}\n\t\tsocket.Lock()\n\t\tsocket.dropAuth(cred.Source)\n\t\tsocket.creds = append(socket.creds, cred)\n\t\tsocket.Unlock()\n\t\treturn nil\n\t})\n}\n\ntype authX509Cmd struct {\n\tAuthenticate int\n\tUser         string\n\tMechanism    string\n}\n\nfunc (socket *mongoSocket) loginX509(cred Credential) error {\n\tcmd := authX509Cmd{Authenticate: 1, User: cred.Username, Mechanism: \"MONGODB-X509\"}\n\tres := authResult{}\n\treturn socket.loginRun(cred.Source, &cmd, &res, func() error {\n\t\tif !res.Ok {\n\t\t\treturn errors.New(res.ErrMsg)\n\t\t}\n\t\tsocket.Lock()\n\t\tsocket.dropAuth(cred.Source)\n\t\tsocket.creds = append(socket.creds, cred)\n\t\tsocket.Unlock()\n\t\treturn nil\n\t})\n}\n\nfunc (socket *mongoSocket) loginPlain(cred Credential) error {\n\tcmd := saslCmd{Start: 1, Mechanism: \"PLAIN\", Payload: []byte(\"\\x00\" + cred.Username + \"\\x00\" + cred.Password)}\n\tres := authResult{}\n\treturn socket.loginRun(cred.Source, &cmd, &res, func() error {\n\t\tif !res.Ok {\n\t\t\treturn errors.New(res.ErrMsg)\n\t\t}\n\t\tsocket.Lock()\n\t\tsocket.dropAuth(cred.Source)\n\t\tsocket.creds = append(socket.creds, cred)\n\t\tsocket.Unlock()\n\t\treturn nil\n\t})\n}\n\nfunc (socket *mongoSocket) loginSASL(cred Credential) error {\n\tvar sasl saslStepper\n\tvar err error\n\tif cred.Mechanism == \"SCRAM-SHA-1\" {\n\t\t// SCRAM is handled without external libraries.\n\t\tsasl = saslNewScram(cred)\n\t} else if len(cred.ServiceHost) > 0 {\n\t\tsasl, err = saslNew(cred, cred.ServiceHost)\n\t} else {\n\t\tsasl, err = saslNew(cred, socket.Server().Addr)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer sasl.Close()\n\n\t// The goal of this logic is to carry a locked socket until the\n\t// local SASL step confirms the auth is valid; the socket needs to be\n\t// locked so that concurrent action doesn't leave the socket in an\n\t// auth state that doesn't reflect the operations that took place.\n\t// As a simple case, imagine inverting login=>logout to logout=>login.\n\t//\n\t// The logic below works because the lock func isn't called concurrently.\n\tlocked := false\n\tlock := func(b bool) {\n\t\tif locked != b {\n\t\t\tlocked = b\n\t\t\tif b {\n\t\t\t\tsocket.Lock()\n\t\t\t} else {\n\t\t\t\tsocket.Unlock()\n\t\t\t}\n\t\t}\n\t}\n\n\tlock(true)\n\tdefer lock(false)\n\n\tstart := 1\n\tcmd := saslCmd{}\n\tres := saslResult{}\n\tfor {\n\t\tpayload, done, err := sasl.Step(res.Payload)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif done && res.Done {\n\t\t\tsocket.dropAuth(cred.Source)\n\t\t\tsocket.creds = append(socket.creds, cred)\n\t\t\tbreak\n\t\t}\n\t\tlock(false)\n\n\t\tcmd = saslCmd{\n\t\t\tStart:          start,\n\t\t\tContinue:       1 - start,\n\t\t\tConversationId: res.ConversationId,\n\t\t\tMechanism:      cred.Mechanism,\n\t\t\tPayload:        payload,\n\t\t}\n\t\tstart = 0\n\t\terr = socket.loginRun(cred.Source, &cmd, &res, func() error {\n\t\t\t// See the comment on lock for why this is necessary.\n\t\t\tlock(true)\n\t\t\tif !res.Ok || res.NotOk {\n\t\t\t\treturn fmt.Errorf(\"server returned error on SASL authentication step: %s\", res.ErrMsg)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif done && res.Done {\n\t\t\tsocket.dropAuth(cred.Source)\n\t\t\tsocket.creds = append(socket.creds, cred)\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc saslNewScram(cred Credential) *saslScram {\n\tcredsum := md5.New()\n\tcredsum.Write([]byte(cred.Username + \":mongo:\" + cred.Password))\n\tclient := scram.NewClient(sha1.New, cred.Username, hex.EncodeToString(credsum.Sum(nil)))\n\treturn &saslScram{cred: cred, client: client}\n}\n\ntype saslScram struct {\n\tcred   Credential\n\tclient *scram.Client\n}\n\nfunc (s *saslScram) Close() {}\n\nfunc (s *saslScram) Step(serverData []byte) (clientData []byte, done bool, err error) {\n\tmore := s.client.Step(serverData)\n\treturn s.client.Out(), !more, s.client.Err()\n}\n\nfunc (socket *mongoSocket) loginRun(db string, query, result interface{}, f func() error) error {\n\tvar mutex sync.Mutex\n\tvar replyErr error\n\tmutex.Lock()\n\n\top := queryOp{}\n\top.query = query\n\top.collection = db + \".$cmd\"\n\top.limit = -1\n\top.replyFunc = func(err error, reply *replyOp, docNum int, docData []byte) {\n\t\tdefer mutex.Unlock()\n\n\t\tif err != nil {\n\t\t\treplyErr = err\n\t\t\treturn\n\t\t}\n\n\t\terr = bson.Unmarshal(docData, result)\n\t\tif err != nil {\n\t\t\treplyErr = err\n\t\t} else {\n\t\t\t// Must handle this within the read loop for the socket, so\n\t\t\t// that concurrent login requests are properly ordered.\n\t\t\treplyErr = f()\n\t\t}\n\t}\n\n\terr := socket.Query(&op)\n\tif err != nil {\n\t\treturn err\n\t}\n\tmutex.Lock() // Wait.\n\treturn replyErr\n}\n\nfunc (socket *mongoSocket) Logout(db string) {\n\tsocket.Lock()\n\tcred, found := socket.dropAuth(db)\n\tif found {\n\t\tdebugf(\"Socket %p to %s: logout: db=%q (flagged)\", socket, socket.addr, db)\n\t\tsocket.logout = append(socket.logout, cred)\n\t}\n\tsocket.Unlock()\n}\n\nfunc (socket *mongoSocket) LogoutAll() {\n\tsocket.Lock()\n\tif l := len(socket.creds); l > 0 {\n\t\tdebugf(\"Socket %p to %s: logout all (flagged %d)\", socket, socket.addr, l)\n\t\tsocket.logout = append(socket.logout, socket.creds...)\n\t\tsocket.creds = socket.creds[0:0]\n\t}\n\tsocket.Unlock()\n}\n\nfunc (socket *mongoSocket) flushLogout() (ops []interface{}) {\n\tsocket.Lock()\n\tif l := len(socket.logout); l > 0 {\n\t\tdebugf(\"Socket %p to %s: logout all (flushing %d)\", socket, socket.addr, l)\n\t\tfor i := 0; i != l; i++ {\n\t\t\top := queryOp{}\n\t\t\top.query = &logoutCmd{1}\n\t\t\top.collection = socket.logout[i].Source + \".$cmd\"\n\t\t\top.limit = -1\n\t\t\tops = append(ops, &op)\n\t\t}\n\t\tsocket.logout = socket.logout[0:0]\n\t}\n\tsocket.Unlock()\n\treturn\n}\n\nfunc (socket *mongoSocket) dropAuth(db string) (cred Credential, found bool) {\n\tfor i, sockCred := range socket.creds {\n\t\tif sockCred.Source == db {\n\t\t\tcopy(socket.creds[i:], socket.creds[i+1:])\n\t\t\tsocket.creds = socket.creds[:len(socket.creds)-1]\n\t\t\treturn sockCred, true\n\t\t}\n\t}\n\treturn cred, false\n}\n\nfunc (socket *mongoSocket) dropLogout(cred Credential) (found bool) {\n\tfor i, sockCred := range socket.logout {\n\t\tif sockCred == cred {\n\t\t\tcopy(socket.logout[i:], socket.logout[i+1:])\n\t\t\tsocket.logout = socket.logout[:len(socket.logout)-1]\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n"
        },
        {
          "name": "auth_test.go",
          "type": "blob",
          "size": 31.0673828125,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t\"crypto/tls\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"sync\"\n\t\"time\"\n\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n)\n\nfunc (s *S) TestAuthLoginDatabase(c *C) {\n\t// Test both with a normal database and with an authenticated shard.\n\tfor _, addr := range []string{\"localhost:40002\", \"localhost:40203\"} {\n\t\tsession, err := mgo.Dial(addr)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n\n\t\tadmindb := session.DB(\"admin\")\n\n\t\terr = admindb.Login(\"root\", \"wrong\")\n\t\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n\n\t\terr = admindb.Login(\"root\", \"rapadura\")\n\t\tc.Assert(err, IsNil)\n\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, IsNil)\n\t}\n}\n\nfunc (s *S) TestAuthLoginSession(c *C) {\n\t// Test both with a normal database and with an authenticated shard.\n\tfor _, addr := range []string{\"localhost:40002\", \"localhost:40203\"} {\n\t\tsession, err := mgo.Dial(addr)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n\n\t\tcred := mgo.Credential{\n\t\t\tUsername: \"root\",\n\t\t\tPassword: \"wrong\",\n\t\t}\n\t\terr = session.Login(&cred)\n\t\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n\n\t\tcred.Password = \"rapadura\"\n\n\t\terr = session.Login(&cred)\n\t\tc.Assert(err, IsNil)\n\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, IsNil)\n\t}\n}\n\nfunc (s *S) TestAuthLoginLogout(c *C) {\n\t// Test both with a normal database and with an authenticated shard.\n\tfor _, addr := range []string{\"localhost:40002\", \"localhost:40203\"} {\n\t\tsession, err := mgo.Dial(addr)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tadmindb := session.DB(\"admin\")\n\t\terr = admindb.Login(\"root\", \"rapadura\")\n\t\tc.Assert(err, IsNil)\n\n\t\tadmindb.Logout()\n\n\t\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n\n\t\t// Must have dropped auth from the session too.\n\t\tsession = session.Copy()\n\t\tdefer session.Close()\n\n\t\tcoll = session.DB(\"mydb\").C(\"mycoll\")\n\t\terr = coll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n\t}\n}\n\nfunc (s *S) TestAuthLoginLogoutAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tsession.LogoutAll()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n\n\t// Must have dropped auth from the session too.\n\tsession = session.Copy()\n\tdefer session.Close()\n\n\tcoll = session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n}\n\nfunc (s *S) TestAuthUpsertUserErrors(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\n\terr = mydb.UpsertUser(&mgo.User{})\n\tc.Assert(err, ErrorMatches, \"user has no Username\")\n\n\terr = mydb.UpsertUser(&mgo.User{Username: \"user\", Password: \"pass\", UserSource: \"source\"})\n\tc.Assert(err, ErrorMatches, \"user has both Password/PasswordHash and UserSource set\")\n\n\terr = mydb.UpsertUser(&mgo.User{Username: \"user\", Password: \"pass\", OtherDBRoles: map[string][]mgo.Role{\"db\": nil}})\n\tc.Assert(err, ErrorMatches, \"user with OtherDBRoles is only supported in the admin or \\\\$external databases\")\n}\n\nfunc (s *S) TestAuthUpsertUser(c *C) {\n\tif !s.versionAtLeast(2, 4) {\n\t\tc.Skip(\"UpsertUser only works on 2.4+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\n\truser := &mgo.User{\n\t\tUsername: \"myruser\",\n\t\tPassword: \"mypass\",\n\t\tRoles:    []mgo.Role{mgo.RoleRead},\n\t}\n\trwuser := &mgo.User{\n\t\tUsername: \"myrwuser\",\n\t\tPassword: \"mypass\",\n\t\tRoles:    []mgo.Role{mgo.RoleReadWrite},\n\t}\n\n\terr = mydb.UpsertUser(ruser)\n\tc.Assert(err, IsNil)\n\terr = mydb.UpsertUser(rwuser)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myruser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\tadmindb.Logout()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\terr = mydb.Login(\"myrwuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tmyotherdb := session.DB(\"myotherdb\")\n\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\t// Test UserSource.\n\trwuserother := &mgo.User{\n\t\tUsername:   \"myrwuser\",\n\t\tUserSource: \"mydb\",\n\t\tRoles:      []mgo.Role{mgo.RoleRead},\n\t}\n\n\terr = myotherdb.UpsertUser(rwuserother)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(err, ErrorMatches, `MongoDB 2.6\\+ does not support the UserSource setting`)\n\t\treturn\n\t}\n\tc.Assert(err, IsNil)\n\n\tadmindb.Logout()\n\n\t// Test indirection via UserSource: we can't write to it, because\n\t// the roles for myrwuser are different there.\n\tothercoll := myotherdb.C(\"myothercoll\")\n\terr = othercoll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\t// Reading works, though.\n\terr = othercoll.Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\t// Can't login directly into the database using UserSource, though.\n\terr = myotherdb.Login(\"myrwuser\", \"mypass\")\n\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n}\n\nfunc (s *S) TestAuthUpsertUserOtherDBRoles(c *C) {\n\tif !s.versionAtLeast(2, 4) {\n\t\tc.Skip(\"UpsertUser only works on 2.4+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\truser := &mgo.User{\n\t\tUsername:     \"myruser\",\n\t\tPassword:     \"mypass\",\n\t\tOtherDBRoles: map[string][]mgo.Role{\"mydb\": []mgo.Role{mgo.RoleRead}},\n\t}\n\n\terr = admindb.UpsertUser(ruser)\n\tc.Assert(err, IsNil)\n\tdefer admindb.RemoveUser(\"myruser\")\n\n\tadmindb.Logout()\n\terr = admindb.Login(\"myruser\", \"mypass\")\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\terr = coll.Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestAuthUpsertUserUpdates(c *C) {\n\tif !s.versionAtLeast(2, 4) {\n\t\tc.Skip(\"UpsertUser only works on 2.4+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\n\t// Insert a user that can read.\n\tuser := &mgo.User{\n\t\tUsername: \"myruser\",\n\t\tPassword: \"mypass\",\n\t\tRoles:    []mgo.Role{mgo.RoleRead},\n\t}\n\terr = mydb.UpsertUser(user)\n\tc.Assert(err, IsNil)\n\n\t// Now update the user password.\n\tuser = &mgo.User{\n\t\tUsername: \"myruser\",\n\t\tPassword: \"mynewpass\",\n\t}\n\terr = mydb.UpsertUser(user)\n\tc.Assert(err, IsNil)\n\n\t// Login with the new user.\n\tusession, err := mgo.Dial(\"myruser:mynewpass@localhost:40002/mydb\")\n\tc.Assert(err, IsNil)\n\tdefer usession.Close()\n\n\t// Can read, but not write.\n\terr = usession.DB(\"mydb\").C(\"mycoll\").Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\terr = usession.DB(\"mydb\").C(\"mycoll\").Insert(M{\"ok\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\t// Update the user role.\n\tuser = &mgo.User{\n\t\tUsername: \"myruser\",\n\t\tRoles:    []mgo.Role{mgo.RoleReadWrite},\n\t}\n\terr = mydb.UpsertUser(user)\n\tc.Assert(err, IsNil)\n\n\t// Dial again to ensure the password hasn't changed.\n\tusession, err = mgo.Dial(\"myruser:mynewpass@localhost:40002/mydb\")\n\tc.Assert(err, IsNil)\n\tdefer usession.Close()\n\n\t// Now it can write.\n\terr = usession.DB(\"mydb\").C(\"mycoll\").Insert(M{\"ok\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthAddUser(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myruser\", \"mypass\", true)\n\tc.Assert(err, IsNil)\n\terr = mydb.AddUser(\"mywuser\", \"mypass\", false)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myruser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\tadmindb.Logout()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\terr = mydb.Login(\"mywuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthAddUserReplaces(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myuser\", \"myoldpass\", false)\n\tc.Assert(err, IsNil)\n\terr = mydb.AddUser(\"myuser\", \"mynewpass\", true)\n\tc.Assert(err, IsNil)\n\n\tadmindb.Logout()\n\n\terr = mydb.Login(\"myuser\", \"myoldpass\")\n\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n\terr = mydb.Login(\"myuser\", \"mynewpass\")\n\tc.Assert(err, IsNil)\n\n\t// ReadOnly flag was changed too.\n\terr = mydb.C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n}\n\nfunc (s *S) TestAuthRemoveUser(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myuser\", \"mypass\", true)\n\tc.Assert(err, IsNil)\n\terr = mydb.RemoveUser(\"myuser\")\n\tc.Assert(err, IsNil)\n\terr = mydb.RemoveUser(\"myuser\")\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = mydb.Login(\"myuser\", \"mypass\")\n\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n}\n\nfunc (s *S) TestAuthLoginTwiceDoesNothing(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\toldStats := mgo.GetStats()\n\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tnewStats := mgo.GetStats()\n\tc.Assert(newStats.SentOps, Equals, oldStats.SentOps)\n}\n\nfunc (s *S) TestAuthLoginLogoutLoginDoesNothing(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\toldStats := mgo.GetStats()\n\n\tadmindb.Logout()\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tnewStats := mgo.GetStats()\n\tc.Assert(newStats.SentOps, Equals, oldStats.SentOps)\n}\n\nfunc (s *S) TestAuthLoginSwitchUser(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = admindb.Login(\"reader\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\t// Can't write.\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\t// But can read.\n\tresult := struct{ N int }{}\n\terr = coll.Find(nil).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 1)\n}\n\nfunc (s *S) TestAuthLoginChangePassword(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myuser\", \"myoldpass\", false)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myuser\", \"myoldpass\")\n\tc.Assert(err, IsNil)\n\n\terr = mydb.AddUser(\"myuser\", \"mynewpass\", true)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myuser\", \"mynewpass\")\n\tc.Assert(err, IsNil)\n\n\tadmindb.Logout()\n\n\t// The second login must be in effect, which means read-only.\n\terr = mydb.C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n}\n\nfunc (s *S) TestAuthLoginCachingWithSessionRefresh(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tsession.Refresh()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthLoginCachingWithSessionCopy(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tsession = session.Copy()\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthLoginCachingWithSessionClone(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tsession = session.Clone()\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthLoginCachingWithNewSession(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\tsession = session.New()\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|need to login|not authorized .*\")\n}\n\nfunc (s *S) TestAuthLoginCachingAcrossPool(c *C) {\n\t// Logins are cached even when the conenction goes back\n\t// into the pool.\n\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\t// Add another user to test the logout case at the same time.\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myuser\", \"mypass\", false)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\t// Logout root explicitly, to test both cases.\n\tadmindb.Logout()\n\n\t// Give socket back to pool.\n\tsession.Refresh()\n\n\t// Brand new session, should use socket from the pool.\n\tother := session.New()\n\tdefer other.Close()\n\n\toldStats := mgo.GetStats()\n\n\terr = other.DB(\"admin\").Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\terr = other.DB(\"mydb\").Login(\"myuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\t// Both logins were cached, so no ops.\n\tnewStats := mgo.GetStats()\n\tc.Assert(newStats.SentOps, Equals, oldStats.SentOps)\n\n\t// And they actually worked.\n\terr = other.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tother.DB(\"admin\").Logout()\n\n\terr = other.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthLoginCachingAcrossPoolWithLogout(c *C) {\n\t// Now verify that logouts are properly flushed if they\n\t// are not revalidated after leaving the pool.\n\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\t// Add another user to test the logout case at the same time.\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myuser\", \"mypass\", true)\n\tc.Assert(err, IsNil)\n\n\terr = mydb.Login(\"myuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\t// Just some data to query later.\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Give socket back to pool.\n\tsession.Refresh()\n\n\t// Brand new session, should use socket from the pool.\n\tother := session.New()\n\tdefer other.Close()\n\n\toldStats := mgo.GetStats()\n\n\terr = other.DB(\"mydb\").Login(\"myuser\", \"mypass\")\n\tc.Assert(err, IsNil)\n\n\t// Login was cached, so no ops.\n\tnewStats := mgo.GetStats()\n\tc.Assert(newStats.SentOps, Equals, oldStats.SentOps)\n\n\t// Can't write, since root has been implicitly logged out\n\t// when the collection went into the pool, and not revalidated.\n\terr = other.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\t// But can read due to the revalidated myuser login.\n\tresult := struct{ N int }{}\n\terr = other.DB(\"mydb\").C(\"mycoll\").Find(nil).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 1)\n}\n\nfunc (s *S) TestAuthEventual(c *C) {\n\t// Eventual sessions don't keep sockets around, so they are\n\t// an interesting test case.\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tadmindb := session.DB(\"admin\")\n\terr = admindb.Login(\"root\", \"rapadura\")\n\tc.Assert(err, IsNil)\n\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tvar wg sync.WaitGroup\n\twg.Add(20)\n\n\tfor i := 0; i != 10; i++ {\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tvar result struct{ N int }\n\t\t\terr := session.DB(\"mydb\").C(\"mycoll\").Find(nil).One(&result)\n\t\t\tc.Assert(err, IsNil)\n\t\t\tc.Assert(result.N, Equals, 1)\n\t\t}()\n\t}\n\n\tfor i := 0; i != 10; i++ {\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\terr := session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\t\t\tc.Assert(err, IsNil)\n\t\t}()\n\t}\n\n\twg.Wait()\n}\n\nfunc (s *S) TestAuthURL(c *C) {\n\tsession, err := mgo.Dial(\"mongodb://root:rapadura@localhost:40002/\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthURLWrongCredentials(c *C) {\n\tsession, err := mgo.Dial(\"mongodb://root:wrong@localhost:40002/\")\n\tif session != nil {\n\t\tsession.Close()\n\t}\n\tc.Assert(err, ErrorMatches, \"auth fail(s|ed)|.*Authentication failed.\")\n\tc.Assert(session, IsNil)\n}\n\nfunc (s *S) TestAuthURLWithNewSession(c *C) {\n\t// When authentication is in the URL, the new session will\n\t// actually carry it on as well, even if logged out explicitly.\n\tsession, err := mgo.Dial(\"mongodb://root:rapadura@localhost:40002/\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.DB(\"admin\").Logout()\n\n\t// Do it twice to ensure it passes the needed data on.\n\tsession = session.New()\n\tdefer session.Close()\n\tsession = session.New()\n\tdefer session.Close()\n\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestAuthURLWithDatabase(c *C) {\n\tsession, err := mgo.Dial(\"mongodb://root:rapadura@localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tmydb := session.DB(\"mydb\")\n\terr = mydb.AddUser(\"myruser\", \"mypass\", true)\n\tc.Assert(err, IsNil)\n\n\t// Test once with database, and once with source.\n\tfor i := 0; i < 2; i++ {\n\t\tvar url string\n\t\tif i == 0 {\n\t\t\turl = \"mongodb://myruser:mypass@localhost:40002/mydb\"\n\t\t} else {\n\t\t\turl = \"mongodb://myruser:mypass@localhost:40002/admin?authSource=mydb\"\n\t\t}\n\t\tusession, err := mgo.Dial(url)\n\t\tc.Assert(err, IsNil)\n\t\tdefer usession.Close()\n\n\t\tucoll := usession.DB(\"mydb\").C(\"mycoll\")\n\t\terr = ucoll.FindId(0).One(nil)\n\t\tc.Assert(err, Equals, mgo.ErrNotFound)\n\t\terr = ucoll.Insert(M{\"n\": 1})\n\t\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\t}\n}\n\nfunc (s *S) TestDefaultDatabase(c *C) {\n\ttests := []struct{ url, db string }{\n\t\t{\"mongodb://root:rapadura@localhost:40002\", \"test\"},\n\t\t{\"mongodb://root:rapadura@localhost:40002/admin\", \"admin\"},\n\t\t{\"mongodb://localhost:40001\", \"test\"},\n\t\t{\"mongodb://localhost:40001/\", \"test\"},\n\t\t{\"mongodb://localhost:40001/mydb\", \"mydb\"},\n\t}\n\n\tfor _, test := range tests {\n\t\tsession, err := mgo.Dial(test.url)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tc.Logf(\"test: %#v\", test)\n\t\tc.Assert(session.DB(\"\").Name, Equals, test.db)\n\n\t\tscopy := session.Copy()\n\t\tc.Check(scopy.DB(\"\").Name, Equals, test.db)\n\t\tscopy.Close()\n\t}\n}\n\nfunc (s *S) TestAuthDirect(c *C) {\n\t// Direct connections must work to the master and slaves.\n\tfor _, port := range []string{\"40031\", \"40032\", \"40033\"} {\n\t\turl := fmt.Sprintf(\"mongodb://root:rapadura@localhost:%s/?connect=direct\", port)\n\t\tsession, err := mgo.Dial(url)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tsession.SetMode(mgo.Monotonic, true)\n\n\t\tvar result struct{}\n\t\terr = session.DB(\"mydb\").C(\"mycoll\").Find(nil).One(&result)\n\t\tc.Assert(err, Equals, mgo.ErrNotFound)\n\t}\n}\n\nfunc (s *S) TestAuthDirectWithLogin(c *C) {\n\t// Direct connections must work to the master and slaves.\n\tfor _, port := range []string{\"40031\", \"40032\", \"40033\"} {\n\t\turl := fmt.Sprintf(\"mongodb://localhost:%s/?connect=direct\", port)\n\t\tsession, err := mgo.Dial(url)\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\n\t\tsession.SetMode(mgo.Monotonic, true)\n\t\tsession.SetSyncTimeout(3 * time.Second)\n\n\t\terr = session.DB(\"admin\").Login(\"root\", \"rapadura\")\n\t\tc.Assert(err, IsNil)\n\n\t\tvar result struct{}\n\t\terr = session.DB(\"mydb\").C(\"mycoll\").Find(nil).One(&result)\n\t\tc.Assert(err, Equals, mgo.ErrNotFound)\n\t}\n}\n\nfunc (s *S) TestAuthScramSha1Cred(c *C) {\n\tif !s.versionAtLeast(2, 7, 7) {\n\t\tc.Skip(\"SCRAM-SHA-1 tests depend on 2.7.7\")\n\t}\n\tcred := &mgo.Credential{\n\t\tUsername:  \"root\",\n\t\tPassword:  \"rapadura\",\n\t\tMechanism: \"SCRAM-SHA-1\",\n\t\tSource:    \"admin\",\n\t}\n\thost := \"localhost:40002\"\n\tc.Logf(\"Connecting to %s...\", host)\n\tsession, err := mgo.Dial(host)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tmycoll := session.DB(\"admin\").C(\"mycoll\")\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = mycoll.Find(nil).One(nil)\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\tc.Logf(\"Authenticating...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = mycoll.Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestAuthScramSha1URL(c *C) {\n\tif !s.versionAtLeast(2, 7, 7) {\n\t\tc.Skip(\"SCRAM-SHA-1 tests depend on 2.7.7\")\n\t}\n\thost := \"localhost:40002\"\n\tc.Logf(\"Connecting to %s...\", host)\n\tsession, err := mgo.Dial(fmt.Sprintf(\"root:rapadura@%s?authMechanism=SCRAM-SHA-1\", host))\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tmycoll := session.DB(\"admin\").C(\"mycoll\")\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = mycoll.Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestAuthX509Cred(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tbinfo, err := session.BuildInfo()\n\tc.Assert(err, IsNil)\n\tif binfo.OpenSSLVersion == \"\" {\n\t\tc.Skip(\"server does not support SSL\")\n\t}\n\n\tclientCertPEM, err := ioutil.ReadFile(\"harness/certs/client.pem\")\n\tc.Assert(err, IsNil)\n\n\tclientCert, err := tls.X509KeyPair(clientCertPEM, clientCertPEM)\n\tc.Assert(err, IsNil)\n\n\ttlsConfig := &tls.Config{\n\t\t// Isolating tests to client certs, don't care about server validation.\n\t\tInsecureSkipVerify: true,\n\t\tCertificates:       []tls.Certificate{clientCert},\n\t}\n\n\tvar host = \"localhost:40003\"\n\tc.Logf(\"Connecting to %s...\", host)\n\tsession, err = mgo.DialWithInfo(&mgo.DialInfo{\n\t\tAddrs: []string{host},\n\t\tDialServer: func(addr *mgo.ServerAddr) (net.Conn, error) {\n\t\t\treturn tls.Dial(\"tcp\", addr.String(), tlsConfig)\n\t\t},\n\t})\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\terr = session.Login(&mgo.Credential{Username: \"root\", Password: \"rapadura\"})\n\tc.Assert(err, IsNil)\n\n\t// This needs to be kept in sync with client.pem\n\tx509Subject := \"CN=localhost,OU=Client,O=MGO,L=MGO,ST=MGO,C=GO\"\n\n\texternalDB := session.DB(\"$external\")\n\tvar x509User mgo.User = mgo.User{\n\t\tUsername:     x509Subject,\n\t\tOtherDBRoles: map[string][]mgo.Role{\"admin\": []mgo.Role{mgo.RoleRoot}},\n\t}\n\terr = externalDB.UpsertUser(&x509User)\n\tc.Assert(err, IsNil)\n\n\tsession.LogoutAll()\n\n\tc.Logf(\"Connected! Ensuring authentication is required...\")\n\tnames, err := session.DatabaseNames()\n\tc.Assert(err, ErrorMatches, \"not authorized .*\")\n\n\tcred := &mgo.Credential{\n\t\tUsername:  x509Subject,\n\t\tMechanism: \"MONGODB-X509\",\n\t\tSource:    \"$external\",\n\t}\n\n\tc.Logf(\"Authenticating...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tnames, err = session.DatabaseNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(len(names) > 0, Equals, true)\n}\n\nvar (\n\tplainFlag = flag.String(\"plain\", \"\", \"Host to test PLAIN authentication against (depends on custom environment)\")\n\tplainUser = \"einstein\"\n\tplainPass = \"password\"\n)\n\nfunc (s *S) TestAuthPlainCred(c *C) {\n\tif *plainFlag == \"\" {\n\t\tc.Skip(\"no -plain\")\n\t}\n\tcred := &mgo.Credential{\n\t\tUsername:  plainUser,\n\t\tPassword:  plainPass,\n\t\tSource:    \"$external\",\n\t\tMechanism: \"PLAIN\",\n\t}\n\tc.Logf(\"Connecting to %s...\", *plainFlag)\n\tsession, err := mgo.Dial(*plainFlag)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\trecords := session.DB(\"records\").C(\"records\")\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = records.Find(nil).One(nil)\n\tc.Assert(err, ErrorMatches, \"unauthorized|not authorized .*\")\n\n\tc.Logf(\"Authenticating...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = records.Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestAuthPlainURL(c *C) {\n\tif *plainFlag == \"\" {\n\t\tc.Skip(\"no -plain\")\n\t}\n\tc.Logf(\"Connecting to %s...\", *plainFlag)\n\tsession, err := mgo.Dial(fmt.Sprintf(\"%s:%s@%s?authMechanism=PLAIN\", url.QueryEscape(plainUser), url.QueryEscape(plainPass), *plainFlag))\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\terr = session.DB(\"records\").C(\"records\").Find(nil).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nvar (\n\tkerberosFlag = flag.Bool(\"kerberos\", false, \"Test Kerberos authentication (depends on custom environment)\")\n\tkerberosHost = \"ldaptest.10gen.cc\"\n\tkerberosUser = \"drivers@LDAPTEST.10GEN.CC\"\n\n\twinKerberosPasswordEnv = \"MGO_KERBEROS_PASSWORD\"\n)\n\n// Kerberos has its own suite because it talks to a remote server\n// that is prepared to authenticate against a kerberos deployment.\ntype KerberosSuite struct{}\n\nvar _ = Suite(&KerberosSuite{})\n\nfunc (kerberosSuite *KerberosSuite) SetUpSuite(c *C) {\n\tmgo.SetDebug(true)\n\tmgo.SetStats(true)\n}\n\nfunc (kerberosSuite *KerberosSuite) TearDownSuite(c *C) {\n\tmgo.SetDebug(false)\n\tmgo.SetStats(false)\n}\n\nfunc (kerberosSuite *KerberosSuite) SetUpTest(c *C) {\n\tmgo.SetLogger((*cLogger)(c))\n\tmgo.ResetStats()\n}\n\nfunc (kerberosSuite *KerberosSuite) TearDownTest(c *C) {\n\tmgo.SetLogger(nil)\n}\n\nfunc (kerberosSuite *KerberosSuite) TestAuthKerberosCred(c *C) {\n\tif !*kerberosFlag {\n\t\tc.Skip(\"no -kerberos\")\n\t}\n\tcred := &mgo.Credential{\n\t\tUsername:  kerberosUser,\n\t\tMechanism: \"GSSAPI\",\n\t}\n\twindowsAppendPasswordToCredential(cred)\n\tc.Logf(\"Connecting to %s...\", kerberosHost)\n\tsession, err := mgo.Dial(kerberosHost)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tc.Logf(\"Connected! Testing the need for authentication...\")\n\tn, err := session.DB(\"kerberos\").C(\"test\").Find(M{}).Count()\n\tc.Assert(err, ErrorMatches, \".*authorized.*\")\n\n\tc.Logf(\"Authenticating...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tn, err = session.DB(\"kerberos\").C(\"test\").Find(M{}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n}\n\nfunc (kerberosSuite *KerberosSuite) TestAuthKerberosURL(c *C) {\n\tif !*kerberosFlag {\n\t\tc.Skip(\"no -kerberos\")\n\t}\n\tc.Logf(\"Connecting to %s...\", kerberosHost)\n\tconnectUri := url.QueryEscape(kerberosUser) + \"@\" + kerberosHost + \"?authMechanism=GSSAPI\"\n\tif runtime.GOOS == \"windows\" {\n\t\tconnectUri = url.QueryEscape(kerberosUser) + \":\" + url.QueryEscape(getWindowsKerberosPassword()) + \"@\" + kerberosHost + \"?authMechanism=GSSAPI\"\n\t}\n\tsession, err := mgo.Dial(connectUri)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tn, err := session.DB(\"kerberos\").C(\"test\").Find(M{}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n}\n\nfunc (kerberosSuite *KerberosSuite) TestAuthKerberosServiceName(c *C) {\n\tif !*kerberosFlag {\n\t\tc.Skip(\"no -kerberos\")\n\t}\n\n\twrongServiceName := \"wrong\"\n\trightServiceName := \"mongodb\"\n\n\tcred := &mgo.Credential{\n\t\tUsername:  kerberosUser,\n\t\tMechanism: \"GSSAPI\",\n\t\tService:   wrongServiceName,\n\t}\n\twindowsAppendPasswordToCredential(cred)\n\n\tc.Logf(\"Connecting to %s...\", kerberosHost)\n\tsession, err := mgo.Dial(kerberosHost)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tc.Logf(\"Authenticating with incorrect service name...\")\n\terr = session.Login(cred)\n\tc.Assert(err, ErrorMatches, \".*@LDAPTEST.10GEN.CC not found.*\")\n\n\tcred.Service = rightServiceName\n\tc.Logf(\"Authenticating with correct service name...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tn, err := session.DB(\"kerberos\").C(\"test\").Find(M{}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n}\n\nfunc (kerberosSuite *KerberosSuite) TestAuthKerberosServiceHost(c *C) {\n\tif !*kerberosFlag {\n\t\tc.Skip(\"no -kerberos\")\n\t}\n\n\twrongServiceHost := \"eggs.bacon.tk\"\n\trightServiceHost := kerberosHost\n\n\tcred := &mgo.Credential{\n\t\tUsername:    kerberosUser,\n\t\tMechanism:   \"GSSAPI\",\n\t\tServiceHost: wrongServiceHost,\n\t}\n\twindowsAppendPasswordToCredential(cred)\n\n\tc.Logf(\"Connecting to %s...\", kerberosHost)\n\tsession, err := mgo.Dial(kerberosHost)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tc.Logf(\"Authenticating with incorrect service host...\")\n\terr = session.Login(cred)\n\tc.Assert(err, ErrorMatches, \".*@LDAPTEST.10GEN.CC not found.*\")\n\n\tcred.ServiceHost = rightServiceHost\n\tc.Logf(\"Authenticating with correct service host...\")\n\terr = session.Login(cred)\n\tc.Assert(err, IsNil)\n\tc.Logf(\"Authenticated!\")\n\n\tn, err := session.DB(\"kerberos\").C(\"test\").Find(M{}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n}\n\n// No kinit on SSPI-style Kerberos, so we need to provide a password. In order\n// to avoid inlining password, require it to be set as an environment variable,\n// for instance: `SET MGO_KERBEROS_PASSWORD=this_isnt_the_password`\nfunc getWindowsKerberosPassword() string {\n\tpw := os.Getenv(winKerberosPasswordEnv)\n\tif pw == \"\" {\n\t\tpanic(fmt.Sprintf(\"Need to set %v environment variable to run Kerberos tests on Windows\", winKerberosPasswordEnv))\n\t}\n\treturn pw\n}\n\nfunc windowsAppendPasswordToCredential(cred *mgo.Credential) {\n\tif runtime.GOOS == \"windows\" {\n\t\tcred.Password = getWindowsKerberosPassword()\n\t}\n}\n"
        },
        {
          "name": "bson",
          "type": "tree",
          "content": null
        },
        {
          "name": "bulk.go",
          "type": "blob",
          "size": 9.7626953125,
          "content": "package mgo\n\nimport (\n\t\"bytes\"\n\t\"sort\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\n// Bulk represents an operation that can be prepared with several\n// orthogonal changes before being delivered to the server.\n//\n// MongoDB servers older than version 2.6 do not have proper support for bulk\n// operations, so the driver attempts to map its API as much as possible into\n// the functionality that works. In particular, in those releases updates and\n// removals are sent individually, and inserts are sent in bulk but have\n// suboptimal error reporting compared to more recent versions of the server.\n// See the documentation of BulkErrorCase for details on that.\n//\n// Relevant documentation:\n//\n//   http://blog.mongodb.org/post/84922794768/mongodbs-new-bulk-api\n//\ntype Bulk struct {\n\tc       *Collection\n\topcount int\n\tactions []bulkAction\n\tordered bool\n}\n\ntype bulkOp int\n\nconst (\n\tbulkInsert bulkOp = iota + 1\n\tbulkUpdate\n\tbulkUpdateAll\n\tbulkRemove\n)\n\ntype bulkAction struct {\n\top   bulkOp\n\tdocs []interface{}\n\tidxs []int\n}\n\ntype bulkUpdateOp []interface{}\ntype bulkDeleteOp []interface{}\n\n// BulkResult holds the results for a bulk operation.\ntype BulkResult struct {\n\tMatched  int\n\tModified int // Available only for MongoDB 2.6+\n\n\t// Be conservative while we understand exactly how to report these\n\t// results in a useful and convenient way, and also how to emulate\n\t// them with prior servers.\n\tprivate bool\n}\n\n// BulkError holds an error returned from running a Bulk operation.\n// Individual errors may be obtained and inspected via the Cases method.\ntype BulkError struct {\n\tecases []BulkErrorCase\n}\n\nfunc (e *BulkError) Error() string {\n\tif len(e.ecases) == 0 {\n\t\treturn \"invalid BulkError instance: no errors\"\n\t}\n\tif len(e.ecases) == 1 {\n\t\treturn e.ecases[0].Err.Error()\n\t}\n\tmsgs := make([]string, 0, len(e.ecases))\n\tseen := make(map[string]bool)\n\tfor _, ecase := range e.ecases {\n\t\tmsg := ecase.Err.Error()\n\t\tif !seen[msg] {\n\t\t\tseen[msg] = true\n\t\t\tmsgs = append(msgs, msg)\n\t\t}\n\t}\n\tif len(msgs) == 1 {\n\t\treturn msgs[0]\n\t}\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"multiple errors in bulk operation:\\n\")\n\tfor _, msg := range msgs {\n\t\tbuf.WriteString(\"  - \")\n\t\tbuf.WriteString(msg)\n\t\tbuf.WriteByte('\\n')\n\t}\n\treturn buf.String()\n}\n\ntype bulkErrorCases []BulkErrorCase\n\nfunc (slice bulkErrorCases) Len() int           { return len(slice) }\nfunc (slice bulkErrorCases) Less(i, j int) bool { return slice[i].Index < slice[j].Index }\nfunc (slice bulkErrorCases) Swap(i, j int)      { slice[i], slice[j] = slice[j], slice[i] }\n\n// BulkErrorCase holds an individual error found while attempting a single change\n// within a bulk operation, and the position in which it was enqueued.\n//\n// MongoDB servers older than version 2.6 do not have proper support for bulk\n// operations, so the driver attempts to map its API as much as possible into\n// the functionality that works. In particular, only the last error is reported\n// for bulk inserts and without any positional information, so the Index\n// field is set to -1 in these cases.\ntype BulkErrorCase struct {\n\tIndex int // Position of operation that failed, or -1 if unknown.\n\tErr   error\n}\n\n// Cases returns all individual errors found while attempting the requested changes.\n//\n// See the documentation of BulkErrorCase for limitations in older MongoDB releases.\nfunc (e *BulkError) Cases() []BulkErrorCase {\n\treturn e.ecases\n}\n\n// Bulk returns a value to prepare the execution of a bulk operation.\nfunc (c *Collection) Bulk() *Bulk {\n\treturn &Bulk{c: c, ordered: true}\n}\n\n// Unordered puts the bulk operation in unordered mode.\n//\n// In unordered mode the indvidual operations may be sent\n// out of order, which means latter operations may proceed\n// even if prior ones have failed.\nfunc (b *Bulk) Unordered() {\n\tb.ordered = false\n}\n\nfunc (b *Bulk) action(op bulkOp, opcount int) *bulkAction {\n\tvar action *bulkAction\n\tif len(b.actions) > 0 && b.actions[len(b.actions)-1].op == op {\n\t\taction = &b.actions[len(b.actions)-1]\n\t} else if !b.ordered {\n\t\tfor i := range b.actions {\n\t\t\tif b.actions[i].op == op {\n\t\t\t\taction = &b.actions[i]\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif action == nil {\n\t\tb.actions = append(b.actions, bulkAction{op: op})\n\t\taction = &b.actions[len(b.actions)-1]\n\t}\n\tfor i := 0; i < opcount; i++ {\n\t\taction.idxs = append(action.idxs, b.opcount)\n\t\tb.opcount++\n\t}\n\treturn action\n}\n\n// Insert queues up the provided documents for insertion.\nfunc (b *Bulk) Insert(docs ...interface{}) {\n\taction := b.action(bulkInsert, len(docs))\n\taction.docs = append(action.docs, docs...)\n}\n\n// Remove queues up the provided selectors for removing matching documents.\n// Each selector will remove only a single matching document.\nfunc (b *Bulk) Remove(selectors ...interface{}) {\n\taction := b.action(bulkRemove, len(selectors))\n\tfor _, selector := range selectors {\n\t\tif selector == nil {\n\t\t\tselector = bson.D{}\n\t\t}\n\t\taction.docs = append(action.docs, &deleteOp{\n\t\t\tCollection: b.c.FullName,\n\t\t\tSelector:   selector,\n\t\t\tFlags:      1,\n\t\t\tLimit:      1,\n\t\t})\n\t}\n}\n\n// RemoveAll queues up the provided selectors for removing all matching documents.\n// Each selector will remove all matching documents.\nfunc (b *Bulk) RemoveAll(selectors ...interface{}) {\n\taction := b.action(bulkRemove, len(selectors))\n\tfor _, selector := range selectors {\n\t\tif selector == nil {\n\t\t\tselector = bson.D{}\n\t\t}\n\t\taction.docs = append(action.docs, &deleteOp{\n\t\t\tCollection: b.c.FullName,\n\t\t\tSelector:   selector,\n\t\t\tFlags:      0,\n\t\t\tLimit:      0,\n\t\t})\n\t}\n}\n\n// Update queues up the provided pairs of updating instructions.\n// The first element of each pair selects which documents must be\n// updated, and the second element defines how to update it.\n// Each pair matches exactly one document for updating at most.\nfunc (b *Bulk) Update(pairs ...interface{}) {\n\tif len(pairs)%2 != 0 {\n\t\tpanic(\"Bulk.Update requires an even number of parameters\")\n\t}\n\taction := b.action(bulkUpdate, len(pairs)/2)\n\tfor i := 0; i < len(pairs); i += 2 {\n\t\tselector := pairs[i]\n\t\tif selector == nil {\n\t\t\tselector = bson.D{}\n\t\t}\n\t\taction.docs = append(action.docs, &updateOp{\n\t\t\tCollection: b.c.FullName,\n\t\t\tSelector:   selector,\n\t\t\tUpdate:     pairs[i+1],\n\t\t})\n\t}\n}\n\n// UpdateAll queues up the provided pairs of updating instructions.\n// The first element of each pair selects which documents must be\n// updated, and the second element defines how to update it.\n// Each pair updates all documents matching the selector.\nfunc (b *Bulk) UpdateAll(pairs ...interface{}) {\n\tif len(pairs)%2 != 0 {\n\t\tpanic(\"Bulk.UpdateAll requires an even number of parameters\")\n\t}\n\taction := b.action(bulkUpdate, len(pairs)/2)\n\tfor i := 0; i < len(pairs); i += 2 {\n\t\tselector := pairs[i]\n\t\tif selector == nil {\n\t\t\tselector = bson.D{}\n\t\t}\n\t\taction.docs = append(action.docs, &updateOp{\n\t\t\tCollection: b.c.FullName,\n\t\t\tSelector:   selector,\n\t\t\tUpdate:     pairs[i+1],\n\t\t\tFlags:      2,\n\t\t\tMulti:      true,\n\t\t})\n\t}\n}\n\n// Upsert queues up the provided pairs of upserting instructions.\n// The first element of each pair selects which documents must be\n// updated, and the second element defines how to update it.\n// Each pair matches exactly one document for updating at most.\nfunc (b *Bulk) Upsert(pairs ...interface{}) {\n\tif len(pairs)%2 != 0 {\n\t\tpanic(\"Bulk.Update requires an even number of parameters\")\n\t}\n\taction := b.action(bulkUpdate, len(pairs)/2)\n\tfor i := 0; i < len(pairs); i += 2 {\n\t\tselector := pairs[i]\n\t\tif selector == nil {\n\t\t\tselector = bson.D{}\n\t\t}\n\t\taction.docs = append(action.docs, &updateOp{\n\t\t\tCollection: b.c.FullName,\n\t\t\tSelector:   selector,\n\t\t\tUpdate:     pairs[i+1],\n\t\t\tFlags:      1,\n\t\t\tUpsert:     true,\n\t\t})\n\t}\n}\n\n// Run runs all the operations queued up.\n//\n// If an error is reported on an unordered bulk operation, the error value may\n// be an aggregation of all issues observed. As an exception to that, Insert\n// operations running on MongoDB versions prior to 2.6 will report the last\n// error only due to a limitation in the wire protocol.\nfunc (b *Bulk) Run() (*BulkResult, error) {\n\tvar result BulkResult\n\tvar berr BulkError\n\tvar failed bool\n\tfor i := range b.actions {\n\t\taction := &b.actions[i]\n\t\tvar ok bool\n\t\tswitch action.op {\n\t\tcase bulkInsert:\n\t\t\tok = b.runInsert(action, &result, &berr)\n\t\tcase bulkUpdate:\n\t\t\tok = b.runUpdate(action, &result, &berr)\n\t\tcase bulkRemove:\n\t\t\tok = b.runRemove(action, &result, &berr)\n\t\tdefault:\n\t\t\tpanic(\"unknown bulk operation\")\n\t\t}\n\t\tif !ok {\n\t\t\tfailed = true\n\t\t\tif b.ordered {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif failed {\n\t\tsort.Sort(bulkErrorCases(berr.ecases))\n\t\treturn nil, &berr\n\t}\n\treturn &result, nil\n}\n\nfunc (b *Bulk) runInsert(action *bulkAction, result *BulkResult, berr *BulkError) bool {\n\top := &insertOp{b.c.FullName, action.docs, 0}\n\tif !b.ordered {\n\t\top.flags = 1 // ContinueOnError\n\t}\n\tlerr, err := b.c.writeOp(op, b.ordered)\n\treturn b.checkSuccess(action, berr, lerr, err)\n}\n\nfunc (b *Bulk) runUpdate(action *bulkAction, result *BulkResult, berr *BulkError) bool {\n\tlerr, err := b.c.writeOp(bulkUpdateOp(action.docs), b.ordered)\n\tif lerr != nil {\n\t\tresult.Matched += lerr.N\n\t\tresult.Modified += lerr.modified\n\t}\n\treturn b.checkSuccess(action, berr, lerr, err)\n}\n\nfunc (b *Bulk) runRemove(action *bulkAction, result *BulkResult, berr *BulkError) bool {\n\tlerr, err := b.c.writeOp(bulkDeleteOp(action.docs), b.ordered)\n\tif lerr != nil {\n\t\tresult.Matched += lerr.N\n\t\tresult.Modified += lerr.modified\n\t}\n\treturn b.checkSuccess(action, berr, lerr, err)\n}\n\nfunc (b *Bulk) checkSuccess(action *bulkAction, berr *BulkError, lerr *LastError, err error) bool {\n\tif lerr != nil && len(lerr.ecases) > 0 {\n\t\tfor i := 0; i < len(lerr.ecases); i++ {\n\t\t\t// Map back from the local error index into the visible one.\n\t\t\tecase := lerr.ecases[i]\n\t\t\tidx := ecase.Index\n\t\t\tif idx >= 0 {\n\t\t\t\tidx = action.idxs[idx]\n\t\t\t}\n\t\t\tberr.ecases = append(berr.ecases, BulkErrorCase{idx, ecase.Err})\n\t\t}\n\t\treturn false\n\t} else if err != nil {\n\t\tfor i := 0; i < len(action.idxs); i++ {\n\t\t\tberr.ecases = append(berr.ecases, BulkErrorCase{action.idxs[i], err})\n\t\t}\n\t\treturn false\n\t}\n\treturn true\n}\n"
        },
        {
          "name": "bulk_test.go",
          "type": "blob",
          "size": 13.7353515625,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2015 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n)\n\nfunc (s *S) TestBulkInsert(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tbulk := coll.Bulk()\n\tbulk.Insert(M{\"n\": 1})\n\tbulk.Insert(M{\"n\": 2}, M{\"n\": 3})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r, FitsTypeOf, &mgo.BulkResult{})\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{1}, {2}, {3}})\n}\n\nfunc (s *S) TestBulkInsertError(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tbulk := coll.Bulk()\n\tbulk.Insert(M{\"_id\": 1}, M{\"_id\": 2}, M{\"_id\": 2}, M{\"_id\": 3})\n\t_, err = bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*duplicate key.*\")\n\tc.Assert(mgo.IsDup(err), Equals, true)\n\n\ttype doc struct {\n\t\tN int `_id`\n\t}\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"_id\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{1}, {2}})\n}\n\nfunc (s *S) TestBulkInsertErrorUnordered(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Insert(M{\"_id\": 1}, M{\"_id\": 2}, M{\"_id\": 2}, M{\"_id\": 3})\n\t_, err = bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*duplicate key.*\")\n\n\ttype doc struct {\n\t\tN int `_id`\n\t}\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"_id\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{1}, {2}, {3}})\n}\n\nfunc (s *S) TestBulkInsertErrorUnorderedSplitBatch(c *C) {\n\t// The server has a batch limit of 1000 documents when using write commands.\n\t// This artificial limit did not exist with the old wire protocol, so to\n\t// avoid compatibility issues the implementation internally split batches\n\t// into the proper size and delivers them one by one. This test ensures that\n\t// the behavior of unordered (that is, continue on error) remains correct\n\t// when errors happen and there are batches left.\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\n\tconst total = 4096\n\ttype doc struct {\n\t\tId int `_id`\n\t}\n\tdocs := make([]interface{}, total)\n\tfor i := 0; i < total; i++ {\n\t\tdocs[i] = doc{i}\n\t}\n\tdocs[1] = doc{0}\n\tbulk.Insert(docs...)\n\t_, err = bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*duplicate key.*\")\n\n\tn, err := coll.Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, total-1)\n\n\tvar res doc\n\terr = coll.FindId(1500).One(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res.Id, Equals, 1500)\n}\n\nfunc (s *S) TestBulkErrorString(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\t// If it's just the same string multiple times, join it into a single message.\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Insert(M{\"_id\": 1}, M{\"_id\": 2}, M{\"_id\": 2})\n\t_, err = bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*duplicate key.*\")\n\tc.Assert(err, Not(ErrorMatches), \".*duplicate key.*duplicate key\")\n\tc.Assert(mgo.IsDup(err), Equals, true)\n\n\t// With matching errors but different messages, present them all.\n\tbulk = coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Insert(M{\"_id\": \"dupone\"}, M{\"_id\": \"dupone\"}, M{\"_id\": \"duptwo\"}, M{\"_id\": \"duptwo\"})\n\t_, err = bulk.Run()\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(err, ErrorMatches, \"multiple errors in bulk operation:\\n(  - .*duplicate.*\\n){2}$\")\n\t\tc.Assert(err, ErrorMatches, \"(?s).*dupone.*\")\n\t\tc.Assert(err, ErrorMatches, \"(?s).*duptwo.*\")\n\t} else {\n\t\t// Wire protocol query doesn't return all errors.\n\t\tc.Assert(err, ErrorMatches, \".*duplicate.*\")\n\t}\n\tc.Assert(mgo.IsDup(err), Equals, true)\n\n\t// With mixed errors, present them all.\n\tbulk = coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Insert(M{\"_id\": 1}, M{\"_id\": []int{2}})\n\t_, err = bulk.Run()\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(err, ErrorMatches, \"multiple errors in bulk operation:\\n  - .*duplicate.*\\n  - .*array.*\\n$\")\n\t} else {\n\t\t// Wire protocol query doesn't return all errors.\n\t\tc.Assert(err, ErrorMatches, \".*array.*\")\n\t}\n\tc.Assert(mgo.IsDup(err), Equals, false)\n}\n\nfunc (s *S) TestBulkErrorCases_2_6(c *C) {\n\tif !s.versionAtLeast(2, 6) {\n\t\tc.Skip(\"2.4- has poor bulk reporting\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\n\t// There's a limit of 1000 operations per command, so\n\t// this forces the more complex indexing logic to act.\n\tfor i := 0; i < 1010; i++ {\n\t\tswitch i {\n\t\tcase 3, 14:\n\t\t\tbulk.Insert(M{\"_id\": \"dupone\"})\n\t\tcase 5, 106:\n\t\t\tbulk.Update(M{\"_id\": i - 1}, M{\"$set\": M{\"_id\": 4}})\n\t\tcase 7, 1008:\n\t\t\tbulk.Insert(M{\"_id\": \"duptwo\"})\n\t\tdefault:\n\t\t\tbulk.Insert(M{\"_id\": i})\n\t\t}\n\t}\n\n\t_, err = bulk.Run()\n\tecases := err.(*mgo.BulkError).Cases()\n\n\tc.Check(ecases[0].Err, ErrorMatches, \".*duplicate.*dupone.*\")\n\tc.Check(ecases[0].Index, Equals, 14)\n\tc.Check(ecases[1].Err, ErrorMatches, \".*update.*_id.*\")\n\tc.Check(ecases[1].Index, Equals, 106)\n\tc.Check(ecases[2].Err, ErrorMatches, \".*duplicate.*duptwo.*\")\n\tc.Check(ecases[2].Index, Equals, 1008)\n}\n\nfunc (s *S) TestBulkErrorCases_2_4(c *C) {\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Skip(\"2.6+ has better reporting\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\n\t// There's a limit of 1000 operations per command, so\n\t// this forces the more complex indexing logic to act.\n\tfor i := 0; i < 1010; i++ {\n\t\tswitch i {\n\t\tcase 3, 14:\n\t\t\tbulk.Insert(M{\"_id\": \"dupone\"})\n\t\tcase 5:\n\t\t\tbulk.Update(M{\"_id\": i - 1}, M{\"$set\": M{\"n\": 4}})\n\t\tcase 106:\n\t\t\tbulk.Update(M{\"_id\": i - 1}, M{\"$bogus\": M{\"n\": 4}})\n\t\tcase 7, 1008:\n\t\t\tbulk.Insert(M{\"_id\": \"duptwo\"})\n\t\tdefault:\n\t\t\tbulk.Insert(M{\"_id\": i})\n\t\t}\n\t}\n\n\t_, err = bulk.Run()\n\tecases := err.(*mgo.BulkError).Cases()\n\n\tc.Check(ecases[0].Err, ErrorMatches, \".*duplicate.*duptwo.*\")\n\tc.Check(ecases[0].Index, Equals, -1)\n\tc.Check(ecases[1].Err, ErrorMatches, `.*\\$bogus.*`)\n\tc.Check(ecases[1].Index, Equals, 106)\n}\n\nfunc (s *S) TestBulkErrorCasesOrdered(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tbulk := coll.Bulk()\n\n\t// There's a limit of 1000 operations per command, so\n\t// this forces the more complex indexing logic to act.\n\tfor i := 0; i < 20; i++ {\n\t\tswitch i {\n\t\tcase 3, 14:\n\t\t\tbulk.Insert(M{\"_id\": \"dupone\"})\n\t\tcase 7, 17:\n\t\t\tbulk.Insert(M{\"_id\": \"duptwo\"})\n\t\tdefault:\n\t\t\tbulk.Insert(M{\"_id\": i})\n\t\t}\n\t}\n\n\t_, err = bulk.Run()\n\tecases := err.(*mgo.BulkError).Cases()\n\n\tc.Check(ecases[0].Err, ErrorMatches, \".*duplicate.*dupone.*\")\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Check(ecases[0].Index, Equals, 14)\n\t} else {\n\t\tc.Check(ecases[0].Index, Equals, -1)\n\t}\n\tc.Check(ecases, HasLen, 1)\n}\n\nfunc (s *S) TestBulkUpdate(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.Update(M{\"n\": 1}, M{\"$set\": M{\"n\": 1}})\n\tbulk.Update(M{\"n\": 2}, M{\"$set\": M{\"n\": 20}})\n\tbulk.Update(M{\"n\": 5}, M{\"$set\": M{\"n\": 50}}) // Won't match.\n\tbulk.Update(M{\"n\": 1}, M{\"$set\": M{\"n\": 10}}, M{\"n\": 3}, M{\"$set\": M{\"n\": 30}})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r.Matched, Equals, 4)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(r.Modified, Equals, 3)\n\t}\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{10}, {20}, {30}})\n}\n\nfunc (s *S) TestBulkUpdateError(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.Update(\n\t\tM{\"n\": 1}, M{\"$set\": M{\"n\": 10}},\n\t\tM{\"n\": 2}, M{\"$set\": M{\"n\": 20, \"_id\": 20}},\n\t\tM{\"n\": 3}, M{\"$set\": M{\"n\": 30}},\n\t)\n\tr, err := bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*_id.*\")\n\tc.Assert(r, FitsTypeOf, &mgo.BulkResult{})\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{2}, {3}, {10}})\n}\n\nfunc (s *S) TestBulkUpdateErrorUnordered(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Update(\n\t\tM{\"n\": 1}, M{\"$set\": M{\"n\": 10}},\n\t\tM{\"n\": 2}, M{\"$set\": M{\"n\": 20, \"_id\": 20}},\n\t\tM{\"n\": 3}, M{\"$set\": M{\"n\": 30}},\n\t)\n\tr, err := bulk.Run()\n\tc.Assert(err, ErrorMatches, \".*_id.*\")\n\tc.Assert(r, FitsTypeOf, &mgo.BulkResult{})\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{2}, {10}, {30}})\n}\n\nfunc (s *S) TestBulkUpdateAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.UpdateAll(M{\"n\": 1}, M{\"$set\": M{\"n\": 10}})\n\tbulk.UpdateAll(M{\"n\": 2}, M{\"$set\": M{\"n\": 2}})  // Won't change.\n\tbulk.UpdateAll(M{\"n\": 5}, M{\"$set\": M{\"n\": 50}}) // Won't match.\n\tbulk.UpdateAll(M{}, M{\"$inc\": M{\"n\": 1}}, M{\"n\": 11}, M{\"$set\": M{\"n\": 5}})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r.Matched, Equals, 6)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(r.Modified, Equals, 5)\n\t}\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{3}, {4}, {5}})\n}\n\nfunc (s *S) TestBulkMixedUnordered(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\t// Abuse undefined behavior to ensure the desired implementation is in place.\n\tbulk := coll.Bulk()\n\tbulk.Unordered()\n\tbulk.Insert(M{\"n\": 1})\n\tbulk.Update(M{\"n\": 2}, M{\"$inc\": M{\"n\": 1}})\n\tbulk.Insert(M{\"n\": 2})\n\tbulk.Update(M{\"n\": 3}, M{\"$inc\": M{\"n\": 1}})\n\tbulk.Update(M{\"n\": 1}, M{\"$inc\": M{\"n\": 1}})\n\tbulk.Insert(M{\"n\": 3})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r.Matched, Equals, 3)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(r.Modified, Equals, 3)\n\t}\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{2}, {3}, {4}})\n}\n\nfunc (s *S) TestBulkUpsert(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.Upsert(M{\"n\": 2}, M{\"$set\": M{\"n\": 20}})\n\tbulk.Upsert(M{\"n\": 4}, M{\"$set\": M{\"n\": 40}}, M{\"n\": 3}, M{\"$set\": M{\"n\": 30}})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r, FitsTypeOf, &mgo.BulkResult{})\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{1}, {20}, {30}, {40}})\n}\n\nfunc (s *S) TestBulkRemove(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3}, M{\"n\": 4}, M{\"n\": 4})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.Remove(M{\"n\": 1})\n\tbulk.Remove(M{\"n\": 2}, M{\"n\": 4})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r.Matched, Equals, 3)\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{3}, {4}})\n}\n\nfunc (s *S) TestBulkRemoveAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1}, M{\"n\": 2}, M{\"n\": 3}, M{\"n\": 4}, M{\"n\": 4})\n\tc.Assert(err, IsNil)\n\n\tbulk := coll.Bulk()\n\tbulk.RemoveAll(M{\"n\": 1})\n\tbulk.RemoveAll(M{\"n\": 2}, M{\"n\": 4})\n\tr, err := bulk.Run()\n\tc.Assert(err, IsNil)\n\tc.Assert(r.Matched, Equals, 4)\n\n\ttype doc struct{ N int }\n\tvar res []doc\n\terr = coll.Find(nil).Sort(\"n\").All(&res)\n\tc.Assert(err, IsNil)\n\tc.Assert(res, DeepEquals, []doc{{3}})\n}\n"
        },
        {
          "name": "cluster.go",
          "type": "blob",
          "size": 18.8603515625,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\n// ---------------------------------------------------------------------------\n// Mongo cluster encapsulation.\n//\n// A cluster enables the communication with one or more servers participating\n// in a mongo cluster.  This works with individual servers, a replica set,\n// a replica pair, one or multiple mongos routers, etc.\n\ntype mongoCluster struct {\n\tsync.RWMutex\n\tserverSynced sync.Cond\n\tuserSeeds    []string\n\tdynaSeeds    []string\n\tservers      mongoServers\n\tmasters      mongoServers\n\treferences   int\n\tsyncing      bool\n\tdirect       bool\n\tfailFast     bool\n\tsyncCount    uint\n\tsetName      string\n\tcachedIndex  map[string]bool\n\tsync         chan bool\n\tdial         dialer\n}\n\nfunc newCluster(userSeeds []string, direct, failFast bool, dial dialer, setName string) *mongoCluster {\n\tcluster := &mongoCluster{\n\t\tuserSeeds:  userSeeds,\n\t\treferences: 1,\n\t\tdirect:     direct,\n\t\tfailFast:   failFast,\n\t\tdial:       dial,\n\t\tsetName:    setName,\n\t}\n\tcluster.serverSynced.L = cluster.RWMutex.RLocker()\n\tcluster.sync = make(chan bool, 1)\n\tstats.cluster(+1)\n\tgo cluster.syncServersLoop()\n\treturn cluster\n}\n\n// Acquire increases the reference count for the cluster.\nfunc (cluster *mongoCluster) Acquire() {\n\tcluster.Lock()\n\tcluster.references++\n\tdebugf(\"Cluster %p acquired (refs=%d)\", cluster, cluster.references)\n\tcluster.Unlock()\n}\n\n// Release decreases the reference count for the cluster. Once\n// it reaches zero, all servers will be closed.\nfunc (cluster *mongoCluster) Release() {\n\tcluster.Lock()\n\tif cluster.references == 0 {\n\t\tpanic(\"cluster.Release() with references == 0\")\n\t}\n\tcluster.references--\n\tdebugf(\"Cluster %p released (refs=%d)\", cluster, cluster.references)\n\tif cluster.references == 0 {\n\t\tfor _, server := range cluster.servers.Slice() {\n\t\t\tserver.Close()\n\t\t}\n\t\t// Wake up the sync loop so it can die.\n\t\tcluster.syncServers()\n\t\tstats.cluster(-1)\n\t}\n\tcluster.Unlock()\n}\n\nfunc (cluster *mongoCluster) LiveServers() (servers []string) {\n\tcluster.RLock()\n\tfor _, serv := range cluster.servers.Slice() {\n\t\tservers = append(servers, serv.Addr)\n\t}\n\tcluster.RUnlock()\n\treturn servers\n}\n\nfunc (cluster *mongoCluster) removeServer(server *mongoServer) {\n\tcluster.Lock()\n\tcluster.masters.Remove(server)\n\tother := cluster.servers.Remove(server)\n\tcluster.Unlock()\n\tif other != nil {\n\t\tother.Close()\n\t\tlog(\"Removed server \", server.Addr, \" from cluster.\")\n\t}\n\tserver.Close()\n}\n\ntype isMasterResult struct {\n\tIsMaster       bool\n\tSecondary      bool\n\tPrimary        string\n\tHosts          []string\n\tPassives       []string\n\tTags           bson.D\n\tMsg            string\n\tSetName        string `bson:\"setName\"`\n\tMaxWireVersion int    `bson:\"maxWireVersion\"`\n}\n\nfunc (cluster *mongoCluster) isMaster(socket *mongoSocket, result *isMasterResult) error {\n\t// Monotonic let's it talk to a slave and still hold the socket.\n\tsession := newSession(Monotonic, cluster, 10*time.Second)\n\tsession.setSocket(socket)\n\terr := session.Run(\"ismaster\", result)\n\tsession.Close()\n\treturn err\n}\n\ntype possibleTimeout interface {\n\tTimeout() bool\n}\n\nvar syncSocketTimeout = 5 * time.Second\n\nfunc (cluster *mongoCluster) syncServer(server *mongoServer) (info *mongoServerInfo, hosts []string, err error) {\n\tvar syncTimeout time.Duration\n\tif raceDetector {\n\t\t// This variable is only ever touched by tests.\n\t\tglobalMutex.Lock()\n\t\tsyncTimeout = syncSocketTimeout\n\t\tglobalMutex.Unlock()\n\t} else {\n\t\tsyncTimeout = syncSocketTimeout\n\t}\n\n\taddr := server.Addr\n\tlog(\"SYNC Processing \", addr, \"...\")\n\n\t// Retry a few times to avoid knocking a server down for a hiccup.\n\tvar result isMasterResult\n\tvar tryerr error\n\tfor retry := 0; ; retry++ {\n\t\tif retry == 3 || retry == 1 && cluster.failFast {\n\t\t\treturn nil, nil, tryerr\n\t\t}\n\t\tif retry > 0 {\n\t\t\t// Don't abuse the server needlessly if there's something actually wrong.\n\t\t\tif err, ok := tryerr.(possibleTimeout); ok && err.Timeout() {\n\t\t\t\t// Give a chance for waiters to timeout as well.\n\t\t\t\tcluster.serverSynced.Broadcast()\n\t\t\t}\n\t\t\ttime.Sleep(syncShortDelay)\n\t\t}\n\n\t\t// It's not clear what would be a good timeout here. Is it\n\t\t// better to wait longer or to retry?\n\t\tsocket, _, err := server.AcquireSocket(0, syncTimeout)\n\t\tif err != nil {\n\t\t\ttryerr = err\n\t\t\tlogf(\"SYNC Failed to get socket to %s: %v\", addr, err)\n\t\t\tcontinue\n\t\t}\n\t\terr = cluster.isMaster(socket, &result)\n\t\tsocket.Release()\n\t\tif err != nil {\n\t\t\ttryerr = err\n\t\t\tlogf(\"SYNC Command 'ismaster' to %s failed: %v\", addr, err)\n\t\t\tcontinue\n\t\t}\n\t\tdebugf(\"SYNC Result of 'ismaster' from %s: %#v\", addr, result)\n\t\tbreak\n\t}\n\n\tif cluster.setName != \"\" && result.SetName != cluster.setName {\n\t\tlogf(\"SYNC Server %s is not a member of replica set %q\", addr, cluster.setName)\n\t\treturn nil, nil, fmt.Errorf(\"server %s is not a member of replica set %q\", addr, cluster.setName)\n\t}\n\n\tif result.IsMaster {\n\t\tdebugf(\"SYNC %s is a master.\", addr)\n\t\tif !server.info.Master {\n\t\t\t// Made an incorrect assumption above, so fix stats.\n\t\t\tstats.conn(-1, false)\n\t\t\tstats.conn(+1, true)\n\t\t}\n\t} else if result.Secondary {\n\t\tdebugf(\"SYNC %s is a slave.\", addr)\n\t} else if cluster.direct {\n\t\tlogf(\"SYNC %s in unknown state. Pretending it's a slave due to direct connection.\", addr)\n\t} else {\n\t\tlogf(\"SYNC %s is neither a master nor a slave.\", addr)\n\t\t// Let stats track it as whatever was known before.\n\t\treturn nil, nil, errors.New(addr + \" is not a master nor slave\")\n\t}\n\n\tinfo = &mongoServerInfo{\n\t\tMaster:         result.IsMaster,\n\t\tMongos:         result.Msg == \"isdbgrid\",\n\t\tTags:           result.Tags,\n\t\tSetName:        result.SetName,\n\t\tMaxWireVersion: result.MaxWireVersion,\n\t}\n\n\thosts = make([]string, 0, 1+len(result.Hosts)+len(result.Passives))\n\tif result.Primary != \"\" {\n\t\t// First in the list to speed up master discovery.\n\t\thosts = append(hosts, result.Primary)\n\t}\n\thosts = append(hosts, result.Hosts...)\n\thosts = append(hosts, result.Passives...)\n\n\tdebugf(\"SYNC %s knows about the following peers: %#v\", addr, hosts)\n\treturn info, hosts, nil\n}\n\ntype syncKind bool\n\nconst (\n\tcompleteSync syncKind = true\n\tpartialSync  syncKind = false\n)\n\nfunc (cluster *mongoCluster) addServer(server *mongoServer, info *mongoServerInfo, syncKind syncKind) {\n\tcluster.Lock()\n\tcurrent := cluster.servers.Search(server.ResolvedAddr)\n\tif current == nil {\n\t\tif syncKind == partialSync {\n\t\t\tcluster.Unlock()\n\t\t\tserver.Close()\n\t\t\tlog(\"SYNC Discarding unknown server \", server.Addr, \" due to partial sync.\")\n\t\t\treturn\n\t\t}\n\t\tcluster.servers.Add(server)\n\t\tif info.Master {\n\t\t\tcluster.masters.Add(server)\n\t\t\tlog(\"SYNC Adding \", server.Addr, \" to cluster as a master.\")\n\t\t} else {\n\t\t\tlog(\"SYNC Adding \", server.Addr, \" to cluster as a slave.\")\n\t\t}\n\t} else {\n\t\tif server != current {\n\t\t\tpanic(\"addServer attempting to add duplicated server\")\n\t\t}\n\t\tif server.Info().Master != info.Master {\n\t\t\tif info.Master {\n\t\t\t\tlog(\"SYNC Server \", server.Addr, \" is now a master.\")\n\t\t\t\tcluster.masters.Add(server)\n\t\t\t} else {\n\t\t\t\tlog(\"SYNC Server \", server.Addr, \" is now a slave.\")\n\t\t\t\tcluster.masters.Remove(server)\n\t\t\t}\n\t\t}\n\t}\n\tserver.SetInfo(info)\n\tdebugf(\"SYNC Broadcasting availability of server %s\", server.Addr)\n\tcluster.serverSynced.Broadcast()\n\tcluster.Unlock()\n}\n\nfunc (cluster *mongoCluster) getKnownAddrs() []string {\n\tcluster.RLock()\n\tmax := len(cluster.userSeeds) + len(cluster.dynaSeeds) + cluster.servers.Len()\n\tseen := make(map[string]bool, max)\n\tknown := make([]string, 0, max)\n\n\tadd := func(addr string) {\n\t\tif _, found := seen[addr]; !found {\n\t\t\tseen[addr] = true\n\t\t\tknown = append(known, addr)\n\t\t}\n\t}\n\n\tfor _, addr := range cluster.userSeeds {\n\t\tadd(addr)\n\t}\n\tfor _, addr := range cluster.dynaSeeds {\n\t\tadd(addr)\n\t}\n\tfor _, serv := range cluster.servers.Slice() {\n\t\tadd(serv.Addr)\n\t}\n\tcluster.RUnlock()\n\n\treturn known\n}\n\n// syncServers injects a value into the cluster.sync channel to force\n// an iteration of the syncServersLoop function.\nfunc (cluster *mongoCluster) syncServers() {\n\tselect {\n\tcase cluster.sync <- true:\n\tdefault:\n\t}\n}\n\n// How long to wait for a checkup of the cluster topology if nothing\n// else kicks a synchronization before that.\nconst syncServersDelay = 30 * time.Second\nconst syncShortDelay = 500 * time.Millisecond\n\n// syncServersLoop loops while the cluster is alive to keep its idea of\n// the server topology up-to-date. It must be called just once from\n// newCluster.  The loop iterates once syncServersDelay has passed, or\n// if somebody injects a value into the cluster.sync channel to force a\n// synchronization.  A loop iteration will contact all servers in\n// parallel, ask them about known peers and their own role within the\n// cluster, and then attempt to do the same with all the peers\n// retrieved.\nfunc (cluster *mongoCluster) syncServersLoop() {\n\tfor {\n\t\tdebugf(\"SYNC Cluster %p is starting a sync loop iteration.\", cluster)\n\n\t\tcluster.Lock()\n\t\tif cluster.references == 0 {\n\t\t\tcluster.Unlock()\n\t\t\tbreak\n\t\t}\n\t\tcluster.references++ // Keep alive while syncing.\n\t\tdirect := cluster.direct\n\t\tcluster.Unlock()\n\n\t\tcluster.syncServersIteration(direct)\n\n\t\t// We just synchronized, so consume any outstanding requests.\n\t\tselect {\n\t\tcase <-cluster.sync:\n\t\tdefault:\n\t\t}\n\n\t\tcluster.Release()\n\n\t\t// Hold off before allowing another sync. No point in\n\t\t// burning CPU looking for down servers.\n\t\tif !cluster.failFast {\n\t\t\ttime.Sleep(syncShortDelay)\n\t\t}\n\n\t\tcluster.Lock()\n\t\tif cluster.references == 0 {\n\t\t\tcluster.Unlock()\n\t\t\tbreak\n\t\t}\n\t\tcluster.syncCount++\n\t\t// Poke all waiters so they have a chance to timeout or\n\t\t// restart syncing if they wish to.\n\t\tcluster.serverSynced.Broadcast()\n\t\t// Check if we have to restart immediately either way.\n\t\trestart := !direct && cluster.masters.Empty() || cluster.servers.Empty()\n\t\tcluster.Unlock()\n\n\t\tif restart {\n\t\t\tlog(\"SYNC No masters found. Will synchronize again.\")\n\t\t\ttime.Sleep(syncShortDelay)\n\t\t\tcontinue\n\t\t}\n\n\t\tdebugf(\"SYNC Cluster %p waiting for next requested or scheduled sync.\", cluster)\n\n\t\t// Hold off until somebody explicitly requests a synchronization\n\t\t// or it's time to check for a cluster topology change again.\n\t\tselect {\n\t\tcase <-cluster.sync:\n\t\tcase <-time.After(syncServersDelay):\n\t\t}\n\t}\n\tdebugf(\"SYNC Cluster %p is stopping its sync loop.\", cluster)\n}\n\nfunc (cluster *mongoCluster) server(addr string, tcpaddr *net.TCPAddr) *mongoServer {\n\tcluster.RLock()\n\tserver := cluster.servers.Search(tcpaddr.String())\n\tcluster.RUnlock()\n\tif server != nil {\n\t\treturn server\n\t}\n\treturn newServer(addr, tcpaddr, cluster.sync, cluster.dial)\n}\n\nfunc resolveAddr(addr string) (*net.TCPAddr, error) {\n\t// Simple cases that do not need actual resolution. Works with IPv4 and v6.\n\tif host, port, err := net.SplitHostPort(addr); err == nil {\n\t\tif port, _ := strconv.Atoi(port); port > 0 {\n\t\t\tzone := \"\"\n\t\t\tif i := strings.LastIndex(host, \"%\"); i >= 0 {\n\t\t\t\tzone = host[i+1:]\n\t\t\t\thost = host[:i]\n\t\t\t}\n\t\t\tip := net.ParseIP(host)\n\t\t\tif ip != nil {\n\t\t\t\treturn &net.TCPAddr{IP: ip, Port: port, Zone: zone}, nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Attempt to resolve IPv4 and v6 concurrently.\n\taddrChan := make(chan *net.TCPAddr, 2)\n\tfor _, network := range []string{\"udp4\", \"udp6\"} {\n\t\tnetwork := network\n\t\tgo func() {\n\t\t\t// The unfortunate UDP dialing hack allows having a timeout on address resolution.\n\t\t\tconn, err := net.DialTimeout(network, addr, 10*time.Second)\n\t\t\tif err != nil {\n\t\t\t\taddrChan <- nil\n\t\t\t} else {\n\t\t\t\taddrChan <- (*net.TCPAddr)(conn.RemoteAddr().(*net.UDPAddr))\n\t\t\t\tconn.Close()\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Wait for the result of IPv4 and v6 resolution. Use IPv4 if available.\n\ttcpaddr := <-addrChan\n\tif tcpaddr == nil || len(tcpaddr.IP) != 4 {\n\t\tvar timeout <-chan time.Time\n\t\tif tcpaddr != nil {\n\t\t\t// Don't wait too long if an IPv6 address is known.\n\t\t\ttimeout = time.After(50 * time.Millisecond)\n\t\t}\n\t\tselect {\n\t\tcase <-timeout:\n\t\tcase tcpaddr2 := <-addrChan:\n\t\t\tif tcpaddr == nil || tcpaddr2 != nil {\n\t\t\t\t// It's an IPv4 address or the only known address. Use it.\n\t\t\t\ttcpaddr = tcpaddr2\n\t\t\t}\n\t\t}\n\t}\n\n\tif tcpaddr == nil {\n\t\tlog(\"SYNC Failed to resolve server address: \", addr)\n\t\treturn nil, errors.New(\"failed to resolve server address: \" + addr)\n\t}\n\tif tcpaddr.String() != addr {\n\t\tdebug(\"SYNC Address \", addr, \" resolved as \", tcpaddr.String())\n\t}\n\treturn tcpaddr, nil\n}\n\ntype pendingAdd struct {\n\tserver *mongoServer\n\tinfo   *mongoServerInfo\n}\n\nfunc (cluster *mongoCluster) syncServersIteration(direct bool) {\n\tlog(\"SYNC Starting full topology synchronization...\")\n\n\tvar wg sync.WaitGroup\n\tvar m sync.Mutex\n\tnotYetAdded := make(map[string]pendingAdd)\n\taddIfFound := make(map[string]bool)\n\tseen := make(map[string]bool)\n\tsyncKind := partialSync\n\n\tvar spawnSync func(addr string, byMaster bool)\n\tspawnSync = func(addr string, byMaster bool) {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\n\t\t\ttcpaddr, err := resolveAddr(addr)\n\t\t\tif err != nil {\n\t\t\t\tlog(\"SYNC Failed to start sync of \", addr, \": \", err.Error())\n\t\t\t\treturn\n\t\t\t}\n\t\t\tresolvedAddr := tcpaddr.String()\n\n\t\t\tm.Lock()\n\t\t\tif byMaster {\n\t\t\t\tif pending, ok := notYetAdded[resolvedAddr]; ok {\n\t\t\t\t\tdelete(notYetAdded, resolvedAddr)\n\t\t\t\t\tm.Unlock()\n\t\t\t\t\tcluster.addServer(pending.server, pending.info, completeSync)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\taddIfFound[resolvedAddr] = true\n\t\t\t}\n\t\t\tif seen[resolvedAddr] {\n\t\t\t\tm.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tseen[resolvedAddr] = true\n\t\t\tm.Unlock()\n\n\t\t\tserver := cluster.server(addr, tcpaddr)\n\t\t\tinfo, hosts, err := cluster.syncServer(server)\n\t\t\tif err != nil {\n\t\t\t\tcluster.removeServer(server)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tm.Lock()\n\t\t\tadd := direct || info.Master || addIfFound[resolvedAddr]\n\t\t\tif add {\n\t\t\t\tsyncKind = completeSync\n\t\t\t} else {\n\t\t\t\tnotYetAdded[resolvedAddr] = pendingAdd{server, info}\n\t\t\t}\n\t\t\tm.Unlock()\n\t\t\tif add {\n\t\t\t\tcluster.addServer(server, info, completeSync)\n\t\t\t}\n\t\t\tif !direct {\n\t\t\t\tfor _, addr := range hosts {\n\t\t\t\t\tspawnSync(addr, info.Master)\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\n\tknownAddrs := cluster.getKnownAddrs()\n\tfor _, addr := range knownAddrs {\n\t\tspawnSync(addr, false)\n\t}\n\twg.Wait()\n\n\tif syncKind == completeSync {\n\t\tlogf(\"SYNC Synchronization was complete (got data from primary).\")\n\t\tfor _, pending := range notYetAdded {\n\t\t\tcluster.removeServer(pending.server)\n\t\t}\n\t} else {\n\t\tlogf(\"SYNC Synchronization was partial (cannot talk to primary).\")\n\t\tfor _, pending := range notYetAdded {\n\t\t\tcluster.addServer(pending.server, pending.info, partialSync)\n\t\t}\n\t}\n\n\tcluster.Lock()\n\tmastersLen := cluster.masters.Len()\n\tlogf(\"SYNC Synchronization completed: %d master(s) and %d slave(s) alive.\", mastersLen, cluster.servers.Len()-mastersLen)\n\n\t// Update dynamic seeds, but only if we have any good servers. Otherwise,\n\t// leave them alone for better chances of a successful sync in the future.\n\tif syncKind == completeSync {\n\t\tdynaSeeds := make([]string, cluster.servers.Len())\n\t\tfor i, server := range cluster.servers.Slice() {\n\t\t\tdynaSeeds[i] = server.Addr\n\t\t}\n\t\tcluster.dynaSeeds = dynaSeeds\n\t\tdebugf(\"SYNC New dynamic seeds: %#v\\n\", dynaSeeds)\n\t}\n\tcluster.Unlock()\n}\n\n// AcquireSocket returns a socket to a server in the cluster.  If slaveOk is\n// true, it will attempt to return a socket to a slave server.  If it is\n// false, the socket will necessarily be to a master server.\nfunc (cluster *mongoCluster) AcquireSocket(mode Mode, slaveOk bool, syncTimeout time.Duration, socketTimeout time.Duration, serverTags []bson.D, poolLimit int) (s *mongoSocket, err error) {\n\tvar started time.Time\n\tvar syncCount uint\n\twarnedLimit := false\n\tfor {\n\t\tcluster.RLock()\n\t\tfor {\n\t\t\tmastersLen := cluster.masters.Len()\n\t\t\tslavesLen := cluster.servers.Len() - mastersLen\n\t\t\tdebugf(\"Cluster has %d known masters and %d known slaves.\", mastersLen, slavesLen)\n\t\t\tif mastersLen > 0 && !(slaveOk && mode == Secondary) || slavesLen > 0 && slaveOk {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif mastersLen > 0 && mode == Secondary && cluster.masters.HasMongos() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif started.IsZero() {\n\t\t\t\t// Initialize after fast path above.\n\t\t\t\tstarted = time.Now()\n\t\t\t\tsyncCount = cluster.syncCount\n\t\t\t} else if syncTimeout != 0 && started.Before(time.Now().Add(-syncTimeout)) || cluster.failFast && cluster.syncCount != syncCount {\n\t\t\t\tcluster.RUnlock()\n\t\t\t\treturn nil, errors.New(\"no reachable servers\")\n\t\t\t}\n\t\t\tlog(\"Waiting for servers to synchronize...\")\n\t\t\tcluster.syncServers()\n\n\t\t\t// Remember: this will release and reacquire the lock.\n\t\t\tcluster.serverSynced.Wait()\n\t\t}\n\n\t\tvar server *mongoServer\n\t\tif slaveOk {\n\t\t\tserver = cluster.servers.BestFit(mode, serverTags)\n\t\t} else {\n\t\t\tserver = cluster.masters.BestFit(mode, nil)\n\t\t}\n\t\tcluster.RUnlock()\n\n\t\tif server == nil {\n\t\t\t// Must have failed the requested tags. Sleep to avoid spinning.\n\t\t\ttime.Sleep(1e8)\n\t\t\tcontinue\n\t\t}\n\n\t\ts, abended, err := server.AcquireSocket(poolLimit, socketTimeout)\n\t\tif err == errPoolLimit {\n\t\t\tif !warnedLimit {\n\t\t\t\twarnedLimit = true\n\t\t\t\tlog(\"WARNING: Per-server connection limit reached.\")\n\t\t\t}\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\tcluster.removeServer(server)\n\t\t\tcluster.syncServers()\n\t\t\tcontinue\n\t\t}\n\t\tif abended && !slaveOk {\n\t\t\tvar result isMasterResult\n\t\t\terr := cluster.isMaster(s, &result)\n\t\t\tif err != nil || !result.IsMaster {\n\t\t\t\tlogf(\"Cannot confirm server %s as master (%v)\", server.Addr, err)\n\t\t\t\ts.Release()\n\t\t\t\tcluster.syncServers()\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\treturn s, nil\n\t}\n\tpanic(\"unreached\")\n}\n\nfunc (cluster *mongoCluster) CacheIndex(cacheKey string, exists bool) {\n\tcluster.Lock()\n\tif cluster.cachedIndex == nil {\n\t\tcluster.cachedIndex = make(map[string]bool)\n\t}\n\tif exists {\n\t\tcluster.cachedIndex[cacheKey] = true\n\t} else {\n\t\tdelete(cluster.cachedIndex, cacheKey)\n\t}\n\tcluster.Unlock()\n}\n\nfunc (cluster *mongoCluster) HasCachedIndex(cacheKey string) (result bool) {\n\tcluster.RLock()\n\tif cluster.cachedIndex != nil {\n\t\tresult = cluster.cachedIndex[cacheKey]\n\t}\n\tcluster.RUnlock()\n\treturn\n}\n\nfunc (cluster *mongoCluster) ResetIndexCache() {\n\tcluster.Lock()\n\tcluster.cachedIndex = make(map[string]bool)\n\tcluster.Unlock()\n}\n"
        },
        {
          "name": "cluster_test.go",
          "type": "blob",
          "size": 53.51953125,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\nfunc (s *S) TestNewSession(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Do a dummy operation to wait for connection.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Tweak safety and query settings to ensure other has copied those.\n\tsession.SetSafe(nil)\n\tsession.SetBatch(-1)\n\tother := session.New()\n\tdefer other.Close()\n\tsession.SetSafe(&mgo.Safe{})\n\n\t// Clone was copied while session was unsafe, so no errors.\n\totherColl := other.DB(\"mydb\").C(\"mycoll\")\n\terr = otherColl.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Original session was made safe again.\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, NotNil)\n\n\t// With New(), each session has its own socket now.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 2)\n\tc.Assert(stats.SocketsInUse, Equals, 2)\n\n\t// Ensure query parameters were cloned.\n\terr = otherColl.Insert(M{\"_id\": 2})\n\tc.Assert(err, IsNil)\n\n\t// Ping the database to ensure the nonce has been received already.\n\tc.Assert(other.Ping(), IsNil)\n\n\tmgo.ResetStats()\n\n\titer := otherColl.Find(M{}).Iter()\n\tc.Assert(err, IsNil)\n\n\tm := M{}\n\tok := iter.Next(m)\n\tc.Assert(ok, Equals, true)\n\terr = iter.Close()\n\tc.Assert(err, IsNil)\n\n\t// If Batch(-1) is in effect, a single document must have been received.\n\tstats = mgo.GetStats()\n\tc.Assert(stats.ReceivedDocs, Equals, 1)\n}\n\nfunc (s *S) TestCloneSession(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Do a dummy operation to wait for connection.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Tweak safety and query settings to ensure clone is copying those.\n\tsession.SetSafe(nil)\n\tsession.SetBatch(-1)\n\tclone := session.Clone()\n\tdefer clone.Close()\n\tsession.SetSafe(&mgo.Safe{})\n\n\t// Clone was copied while session was unsafe, so no errors.\n\tcloneColl := clone.DB(\"mydb\").C(\"mycoll\")\n\terr = cloneColl.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Original session was made safe again.\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, NotNil)\n\n\t// With Clone(), same socket is shared between sessions now.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\tc.Assert(stats.SocketRefs, Equals, 2)\n\n\t// Refreshing one of them should let the original socket go,\n\t// while preserving the safety settings.\n\tclone.Refresh()\n\terr = cloneColl.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Must have used another connection now.\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 2)\n\tc.Assert(stats.SocketRefs, Equals, 2)\n\n\t// Ensure query parameters were cloned.\n\terr = cloneColl.Insert(M{\"_id\": 2})\n\tc.Assert(err, IsNil)\n\n\t// Ping the database to ensure the nonce has been received already.\n\tc.Assert(clone.Ping(), IsNil)\n\n\tmgo.ResetStats()\n\n\titer := cloneColl.Find(M{}).Iter()\n\tc.Assert(err, IsNil)\n\n\tm := M{}\n\tok := iter.Next(m)\n\tc.Assert(ok, Equals, true)\n\terr = iter.Close()\n\tc.Assert(err, IsNil)\n\n\t// If Batch(-1) is in effect, a single document must have been received.\n\tstats = mgo.GetStats()\n\tc.Assert(stats.ReceivedDocs, Equals, 1)\n}\n\nfunc (s *S) TestModeStrong(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, false)\n\tsession.SetMode(mgo.Strong, false)\n\n\tc.Assert(session.Mode(), Equals, mgo.Strong)\n\n\tresult := M{}\n\tcmd := session.DB(\"admin\").C(\"$cmd\")\n\terr = cmd.Find(M{\"ismaster\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, true)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 1)\n\tc.Assert(stats.SlaveConns, Equals, 2)\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\tsession.SetMode(mgo.Strong, true)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestModeMonotonic(c *C) {\n\t// Must necessarily connect to a slave, otherwise the\n\t// master connection will be available first.\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, false)\n\n\tc.Assert(session.Mode(), Equals, mgo.Monotonic)\n\n\tvar result struct{ IsMaster bool }\n\tcmd := session.DB(\"admin\").C(\"$cmd\")\n\terr = cmd.Find(M{\"ismaster\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.IsMaster, Equals, false)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = cmd.Find(M{\"ismaster\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.IsMaster, Equals, true)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 1)\n\tc.Assert(stats.SlaveConns, Equals, 2)\n\tc.Assert(stats.SocketsInUse, Equals, 2)\n\n\tsession.SetMode(mgo.Monotonic, true)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestModeMonotonicAfterStrong(c *C) {\n\t// Test that a strong session shifting to a monotonic\n\t// one preserves the socket untouched.\n\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Insert something to force a connection to the master.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\tsession.SetMode(mgo.Monotonic, false)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\t// Master socket should still be reserved.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\t// Confirm it's the master even though it's Monotonic by now.\n\tresult := M{}\n\tcmd := session.DB(\"admin\").C(\"$cmd\")\n\terr = cmd.Find(M{\"ismaster\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, true)\n}\n\nfunc (s *S) TestModeStrongAfterMonotonic(c *C) {\n\t// Test that shifting from Monotonic to Strong while\n\t// using a slave socket will keep the socket reserved\n\t// until the master socket is necessary, so that no\n\t// switch over occurs unless it's actually necessary.\n\n\t// Must necessarily connect to a slave, otherwise the\n\t// master connection will be available first.\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, false)\n\n\t// Ensure we're talking to a slave, and reserve the socket.\n\tresult := M{}\n\terr = session.Run(\"ismaster\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, false)\n\n\t// Switch to a Strong session.\n\tsession.SetMode(mgo.Strong, false)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\t// Slave socket should still be reserved.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\t// But any operation will switch it to the master.\n\tresult = M{}\n\terr = session.Run(\"ismaster\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, true)\n}\n\nfunc (s *S) TestModeMonotonicWriteOnIteration(c *C) {\n\t// Must necessarily connect to a slave, otherwise the\n\t// master connection will be available first.\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, false)\n\n\tc.Assert(session.Mode(), Equals, mgo.Monotonic)\n\n\tcoll1 := session.DB(\"mydb\").C(\"mycoll1\")\n\tcoll2 := session.DB(\"mydb\").C(\"mycoll2\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll1.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\t// Release master so we can grab a slave again.\n\tsession.Refresh()\n\n\t// Wait until synchronization is done.\n\tfor {\n\t\tn, err := coll1.Count()\n\t\tc.Assert(err, IsNil)\n\t\tif n == len(ns) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\titer := coll1.Find(nil).Batch(2).Iter()\n\ti := 0\n\tm := M{}\n\tfor iter.Next(&m) {\n\t\ti++\n\t\tif i > 3 {\n\t\t\terr := coll2.Insert(M{\"n\": 47 + i})\n\t\t\tc.Assert(err, IsNil)\n\t\t}\n\t}\n\tc.Assert(i, Equals, len(ns))\n}\n\nfunc (s *S) TestModeEventual(c *C) {\n\t// Must necessarily connect to a slave, otherwise the\n\t// master connection will be available first.\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Eventual, false)\n\n\tc.Assert(session.Mode(), Equals, mgo.Eventual)\n\n\tresult := M{}\n\terr = session.Run(\"ismaster\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, false)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\tresult = M{}\n\terr = session.Run(\"ismaster\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, false)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 1)\n\tc.Assert(stats.SlaveConns, Equals, 2)\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestModeEventualAfterStrong(c *C) {\n\t// Test that a strong session shifting to an eventual\n\t// one preserves the socket untouched.\n\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Insert something to force a connection to the master.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\tsession.SetMode(mgo.Eventual, false)\n\n\t// Wait since the sync also uses sockets.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\t// Master socket should still be reserved.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\t// Confirm it's the master even though it's Eventual by now.\n\tresult := M{}\n\tcmd := session.DB(\"admin\").C(\"$cmd\")\n\terr = cmd.Find(M{\"ismaster\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"ismaster\"], Equals, true)\n\n\tsession.SetMode(mgo.Eventual, true)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestModeStrongFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// With strong consistency, this will open a socket to the master.\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\n\t// Kill the master.\n\thost := result.Host\n\ts.Stop(host)\n\n\t// This must fail, since the connection was broken.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\t// With strong consistency, it fails again until reset.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\tsession.Refresh()\n\n\t// Now we should be able to talk to the new master.\n\t// Increase the timeout since this may take quite a while.\n\tsession.SetSyncTimeout(3 * time.Minute)\n\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Host, Not(Equals), host)\n\n\t// Insert some data to confirm it's indeed a master.\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 42})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestModePrimaryHiccup(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// With strong consistency, this will open a socket to the master.\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\n\t// Establish a few extra sessions to create spare sockets to\n\t// the master. This increases a bit the chances of getting an\n\t// incorrect cached socket.\n\tvar sessions []*mgo.Session\n\tfor i := 0; i < 20; i++ {\n\t\tsessions = append(sessions, session.Copy())\n\t\terr = sessions[len(sessions)-1].Run(\"serverStatus\", result)\n\t\tc.Assert(err, IsNil)\n\t}\n\tfor i := range sessions {\n\t\tsessions[i].Close()\n\t}\n\n\t// Kill the master, but bring it back immediatelly.\n\thost := result.Host\n\ts.Stop(host)\n\ts.StartAll()\n\n\t// This must fail, since the connection was broken.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\t// With strong consistency, it fails again until reset.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\tsession.Refresh()\n\n\t// Now we should be able to talk to the new master.\n\t// Increase the timeout since this may take quite a while.\n\tsession.SetSyncTimeout(3 * time.Minute)\n\n\t// Insert some data to confirm it's indeed a master.\n\terr = session.DB(\"mydb\").C(\"mycoll\").Insert(M{\"n\": 42})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestModeMonotonicFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, true)\n\n\t// Insert something to force a switch to the master.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait a bit for this to be synchronized to slaves.\n\ttime.Sleep(3 * time.Second)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\n\t// Kill the master.\n\thost := result.Host\n\ts.Stop(host)\n\n\t// This must fail, since the connection was broken.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\t// With monotonic consistency, it fails again until reset.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\tsession.Refresh()\n\n\t// Now we should be able to talk to the new master.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Host, Not(Equals), host)\n}\n\nfunc (s *S) TestModeMonotonicWithSlaveFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tssresult := &struct{ Host string }{}\n\timresult := &struct{ IsMaster bool }{}\n\n\t// Figure the master while still using the strong session.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tmaster := ssresult.Host\n\tc.Assert(imresult.IsMaster, Equals, true, Commentf(\"%s is not the master\", master))\n\n\t// Create new monotonic session with an explicit address to ensure\n\t// a slave is synchronized before the master, otherwise a connection\n\t// with the master may be used below for lack of other options.\n\tvar addr string\n\tswitch {\n\tcase strings.HasSuffix(ssresult.Host, \":40021\"):\n\t\taddr = \"localhost:40022\"\n\tcase strings.HasSuffix(ssresult.Host, \":40022\"):\n\t\taddr = \"localhost:40021\"\n\tcase strings.HasSuffix(ssresult.Host, \":40023\"):\n\t\taddr = \"localhost:40021\"\n\tdefault:\n\t\tc.Fatal(\"Unknown host: \", ssresult.Host)\n\t}\n\n\tsession, err = mgo.Dial(addr)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, true)\n\n\t// Check the address of the socket associated with the monotonic session.\n\tc.Log(\"Running serverStatus and isMaster with monotonic session\")\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tslave := ssresult.Host\n\tc.Assert(imresult.IsMaster, Equals, false, Commentf(\"%s is not a slave\", slave))\n\n\tc.Assert(master, Not(Equals), slave)\n\n\t// Kill the master.\n\ts.Stop(master)\n\n\t// Session must still be good, since we were talking to a slave.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\n\tc.Assert(ssresult.Host, Equals, slave,\n\t\tCommentf(\"Monotonic session moved from %s to %s\", slave, ssresult.Host))\n\n\t// If we try to insert something, it'll have to hold until the new\n\t// master is available to move the connection, and work correctly.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Must now be talking to the new master.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tc.Assert(imresult.IsMaster, Equals, true, Commentf(\"%s is not the master\", master))\n\n\t// ... which is not the old one, since it's still dead.\n\tc.Assert(ssresult.Host, Not(Equals), master)\n}\n\nfunc (s *S) TestModeEventualFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tmaster := result.Host\n\n\tsession.SetMode(mgo.Eventual, true)\n\n\t// Should connect to the master when needed.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait a bit for this to be synchronized to slaves.\n\ttime.Sleep(3 * time.Second)\n\n\t// Kill the master.\n\ts.Stop(master)\n\n\t// Should still work, with the new master now.\n\tcoll = session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Host, Not(Equals), master)\n}\n\nfunc (s *S) TestModeSecondaryJustPrimary(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Secondary, true)\n\n\terr = session.Ping()\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n}\n\nfunc (s *S) TestModeSecondaryPreferredJustPrimary(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.SecondaryPreferred, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestModeSecondaryPreferredFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Ensure secondaries are available for being picked up.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\ttime.Sleep(5e8)\n\t}\n\n\tsession.SetMode(mgo.SecondaryPreferred, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Not(Equals), \"rs1a\")\n\tsecondary := result.Host\n\n\t// Should connect to the primary when needed.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait a bit for this to be synchronized to slaves.\n\ttime.Sleep(3 * time.Second)\n\n\t// Kill the primary.\n\ts.Stop(\"localhost:40011\")\n\n\t// It can still talk to the selected secondary.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Host, Equals, secondary)\n\n\t// But cannot speak to the primary until reset.\n\tcoll = session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, Equals, io.EOF)\n\n\tsession.Refresh()\n\n\t// Can still talk to a secondary.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Not(Equals), \"rs1a\")\n\n\ts.StartAll()\n\n\t// Should now be able to talk to the primary again.\n\tcoll = session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestModePrimaryPreferredFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.PrimaryPreferred, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Equals, \"rs1a\")\n\n\t// Kill the primary.\n\ts.Stop(\"localhost:40011\")\n\n\t// Should now fail as there was a primary socket in use already.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\t// Refresh so the reserved primary socket goes away.\n\tsession.Refresh()\n\n\t// Should be able to talk to the secondary.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\n\ts.StartAll()\n\n\t// Should wait for the new primary to become available.\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// And should use the new primary in general, as it is preferred.\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Equals, \"rs1a\")\n}\n\nfunc (s *S) TestModePrimaryFallover(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetSyncTimeout(3 * time.Second)\n\n\tsession.SetMode(mgo.Primary, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Equals, \"rs1a\")\n\n\t// Kill the primary.\n\ts.Stop(\"localhost:40011\")\n\n\tsession.Refresh()\n\n\terr = session.Ping()\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n}\n\nfunc (s *S) TestModeSecondary(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Secondary, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(supvName(result.Host), Not(Equals), \"rs1a\")\n\tsecondary := result.Host\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Host, Equals, secondary)\n}\n\nfunc (s *S) TestPreserveSocketCountOnSync(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tstats := mgo.GetStats()\n\tfor stats.SocketsAlive != 3 {\n\t\tc.Logf(\"Waiting for all connections to be established (sockets alive currently %d)...\", stats.SocketsAlive)\n\t\tstats = mgo.GetStats()\n\t\ttime.Sleep(5e8)\n\t}\n\n\tc.Assert(stats.SocketsAlive, Equals, 3)\n\n\t// Kill the master (with rs1, 'a' is always the master).\n\ts.Stop(\"localhost:40011\")\n\n\t// Wait for the logic to run for a bit and bring it back.\n\tstartedAll := make(chan bool)\n\tgo func() {\n\t\ttime.Sleep(5e9)\n\t\ts.StartAll()\n\t\tstartedAll <- true\n\t}()\n\n\t// Do not allow the test to return before the goroutine above is done.\n\tdefer func() {\n\t\t<-startedAll\n\t}()\n\n\t// Do an action to kick the resync logic in, and also to\n\t// wait until the cluster recognizes the server is back.\n\tresult := struct{ Ok bool }{}\n\terr = session.Run(\"getLastError\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, true)\n\n\tfor i := 0; i != 20; i++ {\n\t\tstats = mgo.GetStats()\n\t\tif stats.SocketsAlive == 3 {\n\t\t\tbreak\n\t\t}\n\t\tc.Logf(\"Waiting for 3 sockets alive, have %d\", stats.SocketsAlive)\n\t\ttime.Sleep(5e8)\n\t}\n\n\t// Ensure the number of sockets is preserved after syncing.\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsAlive, Equals, 3)\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\tc.Assert(stats.SocketRefs, Equals, 1)\n}\n\n// Connect to the master of a deployment with a single server,\n// run an insert, and then ensure the insert worked and that a\n// single connection was established.\nfunc (s *S) TestTopologySyncWithSingleMaster(c *C) {\n\t// Use hostname here rather than IP, to make things trickier.\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1, \"b\": 2})\n\tc.Assert(err, IsNil)\n\n\t// One connection used for discovery. Master socket recycled for\n\t// insert. Socket is reserved after insert.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 1)\n\tc.Assert(stats.SlaveConns, Equals, 0)\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\t// Refresh session and socket must be released.\n\tsession.Refresh()\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestTopologySyncWithSlaveSeed(c *C) {\n\t// That's supposed to be a slave. Must run discovery\n\t// and find out master to insert successfully.\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.Insert(M{\"a\": 1, \"b\": 2})\n\n\tresult := struct{ Ok bool }{}\n\terr = session.Run(\"getLastError\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, true)\n\n\t// One connection to each during discovery. Master\n\t// socket recycled for insert.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.MasterConns, Equals, 1)\n\tc.Assert(stats.SlaveConns, Equals, 2)\n\n\t// Only one socket reference alive, in the master socket owned\n\t// by the above session.\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\n\t// Refresh it, and it must be gone.\n\tsession.Refresh()\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestSyncTimeout(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\ts.Stop(\"localhost:40001\")\n\n\ttimeout := 3 * time.Second\n\tsession.SetSyncTimeout(timeout)\n\tstarted := time.Now()\n\n\t// Do something.\n\tresult := struct{ Ok bool }{}\n\terr = session.Run(\"getLastError\", &result)\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\tc.Assert(started.After(time.Now().Add(-timeout*2)), Equals, true)\n}\n\nfunc (s *S) TestDialWithTimeout(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\ttimeout := 2 * time.Second\n\tstarted := time.Now()\n\n\t// 40009 isn't used by the test servers.\n\tsession, err := mgo.DialWithTimeout(\"localhost:40009\", timeout)\n\tif session != nil {\n\t\tsession.Close()\n\t}\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\tc.Assert(session, IsNil)\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\tc.Assert(started.After(time.Now().Add(-timeout*2)), Equals, true)\n}\n\nfunc (s *S) TestSocketTimeout(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\ts.Freeze(\"localhost:40001\")\n\n\ttimeout := 3 * time.Second\n\tsession.SetSocketTimeout(timeout)\n\tstarted := time.Now()\n\n\t// Do something.\n\tresult := struct{ Ok bool }{}\n\terr = session.Run(\"getLastError\", &result)\n\tc.Assert(err, ErrorMatches, \".*: i/o timeout\")\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\tc.Assert(started.After(time.Now().Add(-timeout*2)), Equals, true)\n}\n\nfunc (s *S) TestSocketTimeoutOnDial(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\ttimeout := 1 * time.Second\n\n\tdefer mgo.HackSyncSocketTimeout(timeout)()\n\n\ts.Freeze(\"localhost:40001\")\n\n\tstarted := time.Now()\n\n\tsession, err := mgo.DialWithTimeout(\"localhost:40001\", timeout)\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\tc.Assert(session, IsNil)\n\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\tc.Assert(started.After(time.Now().Add(-20*time.Second)), Equals, true)\n}\n\nfunc (s *S) TestSocketTimeoutOnInactiveSocket(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\ttimeout := 2 * time.Second\n\tsession.SetSocketTimeout(timeout)\n\n\t// Do something that relies on the timeout and works.\n\tc.Assert(session.Ping(), IsNil)\n\n\t// Freeze and wait for the timeout to go by.\n\ts.Freeze(\"localhost:40001\")\n\ttime.Sleep(timeout + 500*time.Millisecond)\n\ts.Thaw(\"localhost:40001\")\n\n\t// Do something again. The timeout above should not have killed\n\t// the socket as there was nothing to be done.\n\tc.Assert(session.Ping(), IsNil)\n}\n\nfunc (s *S) TestDialWithReplicaSetName(c *C) {\n\tseedLists := [][]string{\n\t\t// rs1 primary and rs2 primary\n\t\t[]string{\"localhost:40011\", \"localhost:40021\"},\n\t\t// rs1 primary and rs2 secondary\n\t\t[]string{\"localhost:40011\", \"localhost:40022\"},\n\t\t// rs1 secondary and rs2 primary\n\t\t[]string{\"localhost:40012\", \"localhost:40021\"},\n\t\t// rs1 secondary and rs2 secondary\n\t\t[]string{\"localhost:40012\", \"localhost:40022\"},\n\t}\n\n\trs2Members := []string{\":40021\", \":40022\", \":40023\"}\n\n\tverifySyncedServers := func(session *mgo.Session, numServers int) {\n\t\t// wait for the server(s) to be synced\n\t\tfor len(session.LiveServers()) != numServers {\n\t\t\tc.Log(\"Waiting for cluster sync to finish...\")\n\t\t\ttime.Sleep(5e8)\n\t\t}\n\n\t\t// ensure none of the rs2 set members are communicated with\n\t\tfor _, addr := range session.LiveServers() {\n\t\t\tfor _, rs2Member := range rs2Members {\n\t\t\t\tc.Assert(strings.HasSuffix(addr, rs2Member), Equals, false)\n\t\t\t}\n\t\t}\n\t}\n\n\t// only communication with rs1 members is expected\n\tfor _, seedList := range seedLists {\n\t\tinfo := mgo.DialInfo{\n\t\t\tAddrs:          seedList,\n\t\t\tTimeout:        5 * time.Second,\n\t\t\tReplicaSetName: \"rs1\",\n\t\t}\n\n\t\tsession, err := mgo.DialWithInfo(&info)\n\t\tc.Assert(err, IsNil)\n\t\tverifySyncedServers(session, 3)\n\t\tsession.Close()\n\n\t\tinfo.Direct = true\n\t\tsession, err = mgo.DialWithInfo(&info)\n\t\tc.Assert(err, IsNil)\n\t\tverifySyncedServers(session, 1)\n\t\tsession.Close()\n\n\t\tconnectionUrl := fmt.Sprintf(\"mongodb://%v/?replicaSet=rs1\", strings.Join(seedList, \",\"))\n\t\tsession, err = mgo.Dial(connectionUrl)\n\t\tc.Assert(err, IsNil)\n\t\tverifySyncedServers(session, 3)\n\t\tsession.Close()\n\n\t\tconnectionUrl += \"&connect=direct\"\n\t\tsession, err = mgo.Dial(connectionUrl)\n\t\tc.Assert(err, IsNil)\n\t\tverifySyncedServers(session, 1)\n\t\tsession.Close()\n\t}\n\n}\n\nfunc (s *S) TestDirect(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40012?connect=direct\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// We know that server is a slave.\n\tsession.SetMode(mgo.Monotonic, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(strings.HasSuffix(result.Host, \":40012\"), Equals, true)\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SocketsAlive, Equals, 1)\n\tc.Assert(stats.SocketsInUse, Equals, 1)\n\tc.Assert(stats.SocketRefs, Equals, 1)\n\n\t// We've got no master, so it'll timeout.\n\tsession.SetSyncTimeout(5e8 * time.Nanosecond)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"test\": 1})\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\n\t// Writing to the local database is okay.\n\tcoll = session.DB(\"local\").C(\"mycoll\")\n\tdefer coll.RemoveAll(nil)\n\tid := bson.NewObjectId()\n\terr = coll.Insert(M{\"_id\": id})\n\tc.Assert(err, IsNil)\n\n\t// Data was stored in the right server.\n\tn, err := coll.Find(M{\"_id\": id}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n\n\t// Server hasn't changed.\n\tresult.Host = \"\"\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(strings.HasSuffix(result.Host, \":40012\"), Equals, true)\n}\n\nfunc (s *S) TestDirectToUnknownStateMember(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40041?connect=direct\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Monotonic, true)\n\n\tresult := &struct{ Host string }{}\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(strings.HasSuffix(result.Host, \":40041\"), Equals, true)\n\n\t// We've got no master, so it'll timeout.\n\tsession.SetSyncTimeout(5e8 * time.Nanosecond)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"test\": 1})\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\n\t// Slave is still reachable.\n\tresult.Host = \"\"\n\terr = session.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(strings.HasSuffix(result.Host, \":40041\"), Equals, true)\n}\n\nfunc (s *S) TestFailFast(c *C) {\n\tinfo := mgo.DialInfo{\n\t\tAddrs:    []string{\"localhost:99999\"},\n\t\tTimeout:  5 * time.Second,\n\t\tFailFast: true,\n\t}\n\n\tstarted := time.Now()\n\n\t_, err := mgo.DialWithInfo(&info)\n\tc.Assert(err, ErrorMatches, \"no reachable servers\")\n\n\tc.Assert(started.After(time.Now().Add(-time.Second)), Equals, true)\n}\n\nfunc (s *S) countQueries(c *C, server string) (n int) {\n\tdefer func() { c.Logf(\"Queries for %q: %d\", server, n) }()\n\tsession, err := mgo.Dial(server + \"?connect=direct\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tsession.SetMode(mgo.Monotonic, true)\n\tvar result struct {\n\t\tOpCounters struct {\n\t\t\tQuery int\n\t\t}\n\t\tMetrics struct {\n\t\t\tCommands struct{ Find struct{ Total int } }\n\t\t}\n\t}\n\terr = session.Run(\"serverStatus\", &result)\n\tc.Assert(err, IsNil)\n\tif s.versionAtLeast(3, 2) {\n\t\treturn result.Metrics.Commands.Find.Total\n\t}\n\treturn result.OpCounters.Query\n}\n\nfunc (s *S) countCommands(c *C, server, commandName string) (n int) {\n\tdefer func() { c.Logf(\"Queries for %q: %d\", server, n) }()\n\tsession, err := mgo.Dial(server + \"?connect=direct\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tsession.SetMode(mgo.Monotonic, true)\n\tvar result struct {\n\t\tMetrics struct {\n\t\t\tCommands map[string]struct{ Total int }\n\t\t}\n\t}\n\terr = session.Run(\"serverStatus\", &result)\n\tc.Assert(err, IsNil)\n\treturn result.Metrics.Commands[commandName].Total\n}\n\nfunc (s *S) TestMonotonicSlaveOkFlagWithMongos(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tssresult := &struct{ Host string }{}\n\timresult := &struct{ IsMaster bool }{}\n\n\t// Figure the master while still using the strong session.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tmaster := ssresult.Host\n\tc.Assert(imresult.IsMaster, Equals, true, Commentf(\"%s is not the master\", master))\n\n\t// Ensure mongos is aware about the current topology.\n\ts.Stop(\":40201\")\n\ts.StartAll()\n\n\tmongos, err := mgo.Dial(\"localhost:40202\")\n\tc.Assert(err, IsNil)\n\tdefer mongos.Close()\n\n\t// Insert some data as otherwise 3.2+ doesn't seem to run the query at all.\n\terr = mongos.DB(\"mydb\").C(\"mycoll\").Insert(bson.M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait until all servers see the data.\n\tfor _, addr := range []string{\"localhost:40021\", \"localhost:40022\", \"localhost:40023\"} {\n\t\tsession, err := mgo.Dial(addr + \"?connect=direct\")\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\t\tsession.SetMode(mgo.Monotonic, true)\n\t\tfor i := 300; i >= 0; i-- {\n\t\t\tn, err := session.DB(\"mydb\").C(\"mycoll\").Find(nil).Count()\n\t\t\tc.Assert(err, IsNil)\n\t\t\tif n == 1 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif i == 0 {\n\t\t\t\tc.Fatalf(\"Inserted data never reached \" + addr)\n\t\t\t}\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t}\n\t}\n\n\t// Collect op counters for everyone.\n\tq21a := s.countQueries(c, \"localhost:40021\")\n\tq22a := s.countQueries(c, \"localhost:40022\")\n\tq23a := s.countQueries(c, \"localhost:40023\")\n\n\t// Do a SlaveOk query through MongoS\n\n\tmongos.SetMode(mgo.Monotonic, true)\n\n\tcoll := mongos.DB(\"mydb\").C(\"mycoll\")\n\tvar result struct{ N int }\n\tfor i := 0; i != 5; i++ {\n\t\terr = coll.Find(nil).One(&result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(result.N, Equals, 1)\n\t}\n\n\t// Collect op counters for everyone again.\n\tq21b := s.countQueries(c, \"localhost:40021\")\n\tq22b := s.countQueries(c, \"localhost:40022\")\n\tq23b := s.countQueries(c, \"localhost:40023\")\n\n\tvar masterDelta, slaveDelta int\n\tswitch hostPort(master) {\n\tcase \"40021\":\n\t\tmasterDelta = q21b - q21a\n\t\tslaveDelta = (q22b - q22a) + (q23b - q23a)\n\tcase \"40022\":\n\t\tmasterDelta = q22b - q22a\n\t\tslaveDelta = (q21b - q21a) + (q23b - q23a)\n\tcase \"40023\":\n\t\tmasterDelta = q23b - q23a\n\t\tslaveDelta = (q21b - q21a) + (q22b - q22a)\n\tdefault:\n\t\tc.Fatal(\"Uh?\")\n\t}\n\n\tc.Check(masterDelta, Equals, 0) // Just the counting itself.\n\tc.Check(slaveDelta, Equals, 5)  // The counting for both, plus 5 queries above.\n}\n\nfunc (s *S) TestSecondaryModeWithMongos(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tssresult := &struct{ Host string }{}\n\timresult := &struct{ IsMaster bool }{}\n\n\t// Figure the master while still using the strong session.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tmaster := ssresult.Host\n\tc.Assert(imresult.IsMaster, Equals, true, Commentf(\"%s is not the master\", master))\n\n\t// Ensure mongos is aware about the current topology.\n\ts.Stop(\":40201\")\n\ts.StartAll()\n\n\tmongos, err := mgo.Dial(\"localhost:40202\")\n\tc.Assert(err, IsNil)\n\tdefer mongos.Close()\n\n\tmongos.SetSyncTimeout(5 * time.Second)\n\n\t// Insert some data as otherwise 3.2+ doesn't seem to run the query at all.\n\terr = mongos.DB(\"mydb\").C(\"mycoll\").Insert(bson.M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Wait until all servers see the data.\n\tfor _, addr := range []string{\"localhost:40021\", \"localhost:40022\", \"localhost:40023\"} {\n\t\tsession, err := mgo.Dial(addr + \"?connect=direct\")\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\t\tsession.SetMode(mgo.Monotonic, true)\n\t\tfor i := 300; i >= 0; i-- {\n\t\t\tn, err := session.DB(\"mydb\").C(\"mycoll\").Find(nil).Count()\n\t\t\tc.Assert(err, IsNil)\n\t\t\tif n == 1 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif i == 0 {\n\t\t\t\tc.Fatalf(\"Inserted data never reached \" + addr)\n\t\t\t}\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t}\n\t}\n\n\t// Collect op counters for everyone.\n\tq21a := s.countQueries(c, \"localhost:40021\")\n\tq22a := s.countQueries(c, \"localhost:40022\")\n\tq23a := s.countQueries(c, \"localhost:40023\")\n\n\t// Do a Secondary query through MongoS\n\n\tmongos.SetMode(mgo.Secondary, true)\n\n\tcoll := mongos.DB(\"mydb\").C(\"mycoll\")\n\tvar result struct{ N int }\n\tfor i := 0; i != 5; i++ {\n\t\terr = coll.Find(nil).One(&result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(result.N, Equals, 1)\n\t}\n\n\t// Collect op counters for everyone again.\n\tq21b := s.countQueries(c, \"localhost:40021\")\n\tq22b := s.countQueries(c, \"localhost:40022\")\n\tq23b := s.countQueries(c, \"localhost:40023\")\n\n\tvar masterDelta, slaveDelta int\n\tswitch hostPort(master) {\n\tcase \"40021\":\n\t\tmasterDelta = q21b - q21a\n\t\tslaveDelta = (q22b - q22a) + (q23b - q23a)\n\tcase \"40022\":\n\t\tmasterDelta = q22b - q22a\n\t\tslaveDelta = (q21b - q21a) + (q23b - q23a)\n\tcase \"40023\":\n\t\tmasterDelta = q23b - q23a\n\t\tslaveDelta = (q21b - q21a) + (q22b - q22a)\n\tdefault:\n\t\tc.Fatal(\"Uh?\")\n\t}\n\n\tc.Check(masterDelta, Equals, 0) // Just the counting itself.\n\tc.Check(slaveDelta, Equals, 5)  // The counting for both, plus 5 queries above.\n}\n\nfunc (s *S) TestSecondaryModeWithMongosInsert(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40202\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Secondary, true)\n\tsession.SetSyncTimeout(4 * time.Second)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\tvar result struct{ A int }\n\tcoll.Find(nil).One(&result)\n\tc.Assert(result.A, Equals, 1)\n}\n\n\nfunc (s *S) TestRemovalOfClusterMember(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tmaster, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer master.Close()\n\n\t// Wait for cluster to fully sync up.\n\tfor i := 0; i < 10; i++ {\n\t\tif len(master.LiveServers()) == 3 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(5e8)\n\t}\n\tif len(master.LiveServers()) != 3 {\n\t\tc.Fatalf(\"Test started with bad cluster state: %v\", master.LiveServers())\n\t}\n\n\tresult := &struct {\n\t\tIsMaster bool\n\t\tMe       string\n\t}{}\n\tslave := master.Copy()\n\tslave.SetMode(mgo.Monotonic, true) // Monotonic can hold a non-master socket persistently.\n\terr = slave.Run(\"isMaster\", result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.IsMaster, Equals, false)\n\tslaveAddr := result.Me\n\n\tdefer func() {\n\t\tconfig := map[string]string{\n\t\t\t\"40021\": `{_id: 1, host: \"127.0.0.1:40021\", priority: 1, tags: {rs2: \"a\"}}`,\n\t\t\t\"40022\": `{_id: 2, host: \"127.0.0.1:40022\", priority: 0, tags: {rs2: \"b\"}}`,\n\t\t\t\"40023\": `{_id: 3, host: \"127.0.0.1:40023\", priority: 0, tags: {rs2: \"c\"}}`,\n\t\t}\n\t\tmaster.Refresh()\n\t\tmaster.Run(bson.D{{\"$eval\", `rs.add(` + config[hostPort(slaveAddr)] + `)`}}, nil)\n\t\tmaster.Close()\n\t\tslave.Close()\n\n\t\t// Ensure suite syncs up with the changes before next test.\n\t\ts.Stop(\":40201\")\n\t\ts.StartAll()\n\t\ttime.Sleep(8 * time.Second)\n\t\t// TODO Find a better way to find out when mongos is fully aware that all\n\t\t// servers are up. Without that follow up tests that depend on mongos will\n\t\t// break due to their expectation of things being in a working state.\n\t}()\n\n\tc.Logf(\"========== Removing slave: %s ==========\", slaveAddr)\n\n\tmaster.Run(bson.D{{\"$eval\", `rs.remove(\"` + slaveAddr + `\")`}}, nil)\n\n\tmaster.Refresh()\n\n\t// Give the cluster a moment to catch up by doing a roundtrip to the master.\n\terr = master.Ping()\n\tc.Assert(err, IsNil)\n\n\ttime.Sleep(3e9)\n\n\t// This must fail since the slave has been taken off the cluster.\n\terr = slave.Ping()\n\tc.Assert(err, NotNil)\n\n\tfor i := 0; i < 15; i++ {\n\t\tif len(master.LiveServers()) == 2 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(time.Second)\n\t}\n\tlive := master.LiveServers()\n\tif len(live) != 2 {\n\t\tc.Errorf(\"Removed server still considered live: %#s\", live)\n\t}\n\n\tc.Log(\"========== Test succeeded. ==========\")\n}\n\nfunc (s *S) TestPoolLimitSimple(c *C) {\n\tfor test := 0; test < 2; test++ {\n\t\tvar session *mgo.Session\n\t\tvar err error\n\t\tif test == 0 {\n\t\t\tsession, err = mgo.Dial(\"localhost:40001\")\n\t\t\tc.Assert(err, IsNil)\n\t\t\tsession.SetPoolLimit(1)\n\t\t} else {\n\t\t\tsession, err = mgo.Dial(\"localhost:40001?maxPoolSize=1\")\n\t\t\tc.Assert(err, IsNil)\n\t\t}\n\t\tdefer session.Close()\n\n\t\t// Put one socket in use.\n\t\tc.Assert(session.Ping(), IsNil)\n\n\t\tdone := make(chan time.Duration)\n\n\t\t// Now block trying to get another one due to the pool limit.\n\t\tgo func() {\n\t\t\tcopy := session.Copy()\n\t\t\tdefer copy.Close()\n\t\t\tstarted := time.Now()\n\t\t\tc.Check(copy.Ping(), IsNil)\n\t\t\tdone <- time.Now().Sub(started)\n\t\t}()\n\n\t\ttime.Sleep(300 * time.Millisecond)\n\n\t\t// Put the one socket back in the pool, freeing it for the copy.\n\t\tsession.Refresh()\n\t\tdelay := <-done\n\t\tc.Assert(delay > 300*time.Millisecond, Equals, true, Commentf(\"Delay: %s\", delay))\n\t}\n}\n\nfunc (s *S) TestPoolLimitMany(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tstats := mgo.GetStats()\n\tfor stats.SocketsAlive != 3 {\n\t\tc.Logf(\"Waiting for all connections to be established (sockets alive currently %d)...\", stats.SocketsAlive)\n\t\tstats = mgo.GetStats()\n\t\ttime.Sleep(5e8)\n\t}\n\n\tconst poolLimit = 64\n\tsession.SetPoolLimit(poolLimit)\n\n\t// Consume the whole limit for the master.\n\tvar master []*mgo.Session\n\tfor i := 0; i < poolLimit; i++ {\n\t\ts := session.Copy()\n\t\tdefer s.Close()\n\t\tc.Assert(s.Ping(), IsNil)\n\t\tmaster = append(master, s)\n\t}\n\n\tbefore := time.Now()\n\tgo func() {\n\t\ttime.Sleep(3e9)\n\t\tmaster[0].Refresh()\n\t}()\n\n\t// Then, a single ping must block, since it would need another\n\t// connection to the master, over the limit. Once the goroutine\n\t// above releases its socket, it should move on.\n\tsession.Ping()\n\tdelay := time.Now().Sub(before)\n\tc.Assert(delay > 3e9, Equals, true)\n\tc.Assert(delay < 6e9, Equals, true)\n}\n\nfunc (s *S) TestSetModeEventualIterBug(c *C) {\n\tsession1, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session1.Close()\n\n\tsession1.SetMode(mgo.Eventual, false)\n\n\tcoll1 := session1.DB(\"mydb\").C(\"mycoll\")\n\n\tconst N = 100\n\tfor i := 0; i < N; i++ {\n\t\terr = coll1.Insert(M{\"_id\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tc.Logf(\"Waiting until secondary syncs\")\n\tfor {\n\t\tn, err := coll1.Count()\n\t\tc.Assert(err, IsNil)\n\t\tif n == N {\n\t\t\tc.Logf(\"Found all\")\n\t\t\tbreak\n\t\t}\n\t}\n\n\tsession2, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session2.Close()\n\n\tsession2.SetMode(mgo.Eventual, false)\n\n\tcoll2 := session2.DB(\"mydb\").C(\"mycoll\")\n\n\ti := 0\n\titer := coll2.Find(nil).Batch(10).Iter()\n\tvar result struct{}\n\tfor iter.Next(&result) {\n\t\ti++\n\t}\n\tc.Assert(iter.Close(), Equals, nil)\n\tc.Assert(i, Equals, N)\n}\n\nfunc (s *S) TestCustomDialOld(c *C) {\n\tdials := make(chan bool, 16)\n\tdial := func(addr net.Addr) (net.Conn, error) {\n\t\ttcpaddr, ok := addr.(*net.TCPAddr)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"unexpected address type: %T\", addr)\n\t\t}\n\t\tdials <- true\n\t\treturn net.DialTCP(\"tcp\", nil, tcpaddr)\n\t}\n\tinfo := mgo.DialInfo{\n\t\tAddrs: []string{\"localhost:40012\"},\n\t\tDial:  dial,\n\t}\n\n\t// Use hostname here rather than IP, to make things trickier.\n\tsession, err := mgo.DialWithInfo(&info)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tconst N = 3\n\tfor i := 0; i < N; i++ {\n\t\tselect {\n\t\tcase <-dials:\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tc.Fatalf(\"expected %d dials, got %d\", N, i)\n\t\t}\n\t}\n\tselect {\n\tcase <-dials:\n\t\tc.Fatalf(\"got more dials than expected\")\n\tcase <-time.After(100 * time.Millisecond):\n\t}\n}\n\nfunc (s *S) TestCustomDialNew(c *C) {\n\tdials := make(chan bool, 16)\n\tdial := func(addr *mgo.ServerAddr) (net.Conn, error) {\n\t\tdials <- true\n\t\tif addr.TCPAddr().Port == 40012 {\n\t\t\tc.Check(addr.String(), Equals, \"localhost:40012\")\n\t\t}\n\t\treturn net.DialTCP(\"tcp\", nil, addr.TCPAddr())\n\t}\n\tinfo := mgo.DialInfo{\n\t\tAddrs:      []string{\"localhost:40012\"},\n\t\tDialServer: dial,\n\t}\n\n\t// Use hostname here rather than IP, to make things trickier.\n\tsession, err := mgo.DialWithInfo(&info)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tconst N = 3\n\tfor i := 0; i < N; i++ {\n\t\tselect {\n\t\tcase <-dials:\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tc.Fatalf(\"expected %d dials, got %d\", N, i)\n\t\t}\n\t}\n\tselect {\n\tcase <-dials:\n\t\tc.Fatalf(\"got more dials than expected\")\n\tcase <-time.After(100 * time.Millisecond):\n\t}\n}\n\nfunc (s *S) TestPrimaryShutdownOnAuthShard(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\t// Dial the shard.\n\tsession, err := mgo.Dial(\"localhost:40203\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Login and insert something to make it more realistic.\n\tsession.DB(\"admin\").Login(\"root\", \"rapadura\")\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(bson.M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Dial the replica set to figure the master out.\n\trs, err := mgo.Dial(\"root:rapadura@localhost:40031\")\n\tc.Assert(err, IsNil)\n\tdefer rs.Close()\n\n\t// With strong consistency, this will open a socket to the master.\n\tresult := &struct{ Host string }{}\n\terr = rs.Run(\"serverStatus\", result)\n\tc.Assert(err, IsNil)\n\n\t// Kill the master.\n\thost := result.Host\n\ts.Stop(host)\n\n\t// This must fail, since the connection was broken.\n\terr = rs.Run(\"serverStatus\", result)\n\tc.Assert(err, Equals, io.EOF)\n\n\t// This won't work because the master just died.\n\terr = coll.Insert(bson.M{\"n\": 2})\n\tc.Assert(err, NotNil)\n\n\t// Refresh session and wait for re-election.\n\tsession.Refresh()\n\tfor i := 0; i < 60; i++ {\n\t\terr = coll.Insert(bson.M{\"n\": 3})\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tc.Logf(\"Waiting for replica set to elect a new master. Last error: %v\", err)\n\t\ttime.Sleep(500 * time.Millisecond)\n\t}\n\tc.Assert(err, IsNil)\n\n\tcount, err := coll.Count()\n\tc.Assert(count > 1, Equals, true)\n}\n\nfunc (s *S) TestNearestSecondary(c *C) {\n\tdefer mgo.HackPingDelay(300 * time.Millisecond)()\n\n\trs1a := \"127.0.0.1:40011\"\n\trs1b := \"127.0.0.1:40012\"\n\trs1c := \"127.0.0.1:40013\"\n\ts.Freeze(rs1b)\n\n\tsession, err := mgo.Dial(rs1a)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Wait for the sync up to run through the first couple of servers.\n\tfor len(session.LiveServers()) != 2 {\n\t\tc.Log(\"Waiting for two servers to be alive...\")\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\t// Extra delay to ensure the third server gets penalized.\n\ttime.Sleep(500 * time.Millisecond)\n\n\t// Release third server.\n\ts.Thaw(rs1b)\n\n\t// Wait for it to come up.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for all servers to be alive...\")\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tsession.SetMode(mgo.Monotonic, true)\n\tvar result struct{ Host string }\n\n\t// See which slave picks the line, several times to avoid chance.\n\tfor i := 0; i < 10; i++ {\n\t\tsession.Refresh()\n\t\terr = session.Run(\"serverStatus\", &result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(hostPort(result.Host), Equals, hostPort(rs1c))\n\t}\n\n\tif *fast {\n\t\t// Don't hold back for several seconds.\n\t\treturn\n\t}\n\n\t// Now hold the other server for long enough to penalize it.\n\ts.Freeze(rs1c)\n\ttime.Sleep(5 * time.Second)\n\ts.Thaw(rs1c)\n\n\t// Wait for the ping to be processed.\n\ttime.Sleep(500 * time.Millisecond)\n\n\t// Repeating the test should now pick the former server consistently.\n\tfor i := 0; i < 10; i++ {\n\t\tsession.Refresh()\n\t\terr = session.Run(\"serverStatus\", &result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(hostPort(result.Host), Equals, hostPort(rs1b))\n\t}\n}\n\nfunc (s *S) TestNearestServer(c *C) {\n\tdefer mgo.HackPingDelay(300 * time.Millisecond)()\n\n\trs1a := \"127.0.0.1:40011\"\n\trs1b := \"127.0.0.1:40012\"\n\trs1c := \"127.0.0.1:40013\"\n\n\tsession, err := mgo.Dial(rs1a)\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\ts.Freeze(rs1a)\n\ts.Freeze(rs1b)\n\n\t// Extra delay to ensure the first two servers get penalized.\n\ttime.Sleep(500 * time.Millisecond)\n\n\t// Release them.\n\ts.Thaw(rs1a)\n\ts.Thaw(rs1b)\n\n\t// Wait for everyone to come up.\n\tfor len(session.LiveServers()) != 3 {\n\t\tc.Log(\"Waiting for all servers to be alive...\")\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tsession.SetMode(mgo.Nearest, true)\n\tvar result struct{ Host string }\n\n\t// See which server picks the line, several times to avoid chance.\n\tfor i := 0; i < 10; i++ {\n\t\tsession.Refresh()\n\t\terr = session.Run(\"serverStatus\", &result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(hostPort(result.Host), Equals, hostPort(rs1c))\n\t}\n\n\tif *fast {\n\t\t// Don't hold back for several seconds.\n\t\treturn\n\t}\n\n\t// Now hold the two secondaries for long enough to penalize them.\n\ts.Freeze(rs1b)\n\ts.Freeze(rs1c)\n\ttime.Sleep(5 * time.Second)\n\ts.Thaw(rs1b)\n\ts.Thaw(rs1c)\n\n\t// Wait for the ping to be processed.\n\ttime.Sleep(500 * time.Millisecond)\n\n\t// Repeating the test should now pick the primary server consistently.\n\tfor i := 0; i < 10; i++ {\n\t\tsession.Refresh()\n\t\terr = session.Run(\"serverStatus\", &result)\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(hostPort(result.Host), Equals, hostPort(rs1a))\n\t}\n}\n\nfunc (s *S) TestConnectCloseConcurrency(c *C) {\n\trestore := mgo.HackPingDelay(500 * time.Millisecond)\n\tdefer restore()\n\tvar wg sync.WaitGroup\n\tconst n = 500\n\twg.Add(n)\n\tfor i := 0; i < n; i++ {\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tsession, err := mgo.Dial(\"localhost:40001\")\n\t\t\tif err != nil {\n\t\t\t\tc.Fatal(err)\n\t\t\t}\n\t\t\ttime.Sleep(1)\n\t\t\tsession.Close()\n\t\t}()\n\t}\n\twg.Wait()\n}\n\nfunc (s *S) TestSelectServers(c *C) {\n\tif !s.versionAtLeast(2, 2) {\n\t\tc.Skip(\"read preferences introduced in 2.2\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetMode(mgo.Eventual, true)\n\n\tvar result struct{ Host string }\n\n\tsession.Refresh()\n\tsession.SelectServers(bson.D{{\"rs1\", \"b\"}})\n\terr = session.Run(\"serverStatus\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(hostPort(result.Host), Equals, \"40012\")\n\n\tsession.Refresh()\n\tsession.SelectServers(bson.D{{\"rs1\", \"c\"}})\n\terr = session.Run(\"serverStatus\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(hostPort(result.Host), Equals, \"40013\")\n}\n\nfunc (s *S) TestSelectServersWithMongos(c *C) {\n\tif !s.versionAtLeast(2, 2) {\n\t\tc.Skip(\"read preferences introduced in 2.2\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40021\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tssresult := &struct{ Host string }{}\n\timresult := &struct{ IsMaster bool }{}\n\n\t// Figure the master while still using the strong session.\n\terr = session.Run(\"serverStatus\", ssresult)\n\tc.Assert(err, IsNil)\n\terr = session.Run(\"isMaster\", imresult)\n\tc.Assert(err, IsNil)\n\tmaster := ssresult.Host\n\tc.Assert(imresult.IsMaster, Equals, true, Commentf(\"%s is not the master\", master))\n\n\tvar slave1, slave2 string\n\tswitch hostPort(master) {\n\tcase \"40021\":\n\t\tslave1, slave2 = \"b\", \"c\"\n\tcase \"40022\":\n\t\tslave1, slave2 = \"a\", \"c\"\n\tcase \"40023\":\n\t\tslave1, slave2 = \"a\", \"b\"\n\t}\n\n\t// Collect op counters for everyone.\n\tq21a := s.countQueries(c, \"localhost:40021\")\n\tq22a := s.countQueries(c, \"localhost:40022\")\n\tq23a := s.countQueries(c, \"localhost:40023\")\n\n\t// Do a SlaveOk query through MongoS\n\tmongos, err := mgo.Dial(\"localhost:40202\")\n\tc.Assert(err, IsNil)\n\tdefer mongos.Close()\n\n\tmongos.SetMode(mgo.Monotonic, true)\n\n\tmongos.Refresh()\n\tmongos.SelectServers(bson.D{{\"rs2\", slave1}})\n\tcoll := mongos.DB(\"mydb\").C(\"mycoll\")\n\tresult := &struct{}{}\n\tfor i := 0; i != 5; i++ {\n\t\terr := coll.Find(nil).One(result)\n\t\tc.Assert(err, Equals, mgo.ErrNotFound)\n\t}\n\n\tmongos.Refresh()\n\tmongos.SelectServers(bson.D{{\"rs2\", slave2}})\n\tcoll = mongos.DB(\"mydb\").C(\"mycoll\")\n\tfor i := 0; i != 7; i++ {\n\t\terr := coll.Find(nil).One(result)\n\t\tc.Assert(err, Equals, mgo.ErrNotFound)\n\t}\n\n\t// Collect op counters for everyone again.\n\tq21b := s.countQueries(c, \"localhost:40021\")\n\tq22b := s.countQueries(c, \"localhost:40022\")\n\tq23b := s.countQueries(c, \"localhost:40023\")\n\n\tswitch hostPort(master) {\n\tcase \"40021\":\n\t\tc.Check(q21b-q21a, Equals, 0)\n\t\tc.Check(q22b-q22a, Equals, 5)\n\t\tc.Check(q23b-q23a, Equals, 7)\n\tcase \"40022\":\n\t\tc.Check(q21b-q21a, Equals, 5)\n\t\tc.Check(q22b-q22a, Equals, 0)\n\t\tc.Check(q23b-q23a, Equals, 7)\n\tcase \"40023\":\n\t\tc.Check(q21b-q21a, Equals, 5)\n\t\tc.Check(q22b-q22a, Equals, 7)\n\t\tc.Check(q23b-q23a, Equals, 0)\n\tdefault:\n\t\tc.Fatal(\"Uh?\")\n\t}\n}\n\nfunc (s *S) TestDoNotFallbackToMonotonic(c *C) {\n\t// There was a bug at some point that some functions were\n\t// falling back to Monotonic mode. This test ensures all listIndexes\n\t// commands go to the primary, as should happen since the session is\n\t// in Strong mode.\n\tif !s.versionAtLeast(3, 0) {\n\t\tc.Skip(\"command-counting logic depends on 3.0+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40012\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tfor i := 0; i < 15; i++ {\n\t\tq11a := s.countCommands(c, \"localhost:40011\", \"listIndexes\")\n\t\tq12a := s.countCommands(c, \"localhost:40012\", \"listIndexes\")\n\t\tq13a := s.countCommands(c, \"localhost:40013\", \"listIndexes\")\n\n\t\t_, err := session.DB(\"local\").C(\"system.indexes\").Indexes()\n\t\tc.Assert(err, IsNil)\n\n\t\tq11b := s.countCommands(c, \"localhost:40011\", \"listIndexes\")\n\t\tq12b := s.countCommands(c, \"localhost:40012\", \"listIndexes\")\n\t\tq13b := s.countCommands(c, \"localhost:40013\", \"listIndexes\")\n\n\t\tc.Assert(q11b, Equals, q11a+1)\n\t\tc.Assert(q12b, Equals, q12a)\n\t\tc.Assert(q13b, Equals, q13a)\n\t}\n}\n"
        },
        {
          "name": "dbtest",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 1.326171875,
          "content": "// Package mgo offers a rich MongoDB driver for Go.\n//\n// #########################################################\n//\n// THIS DRIVER IS UNMAINTAINED! See here for details:\n//\n// https://github.com/go-mgo/mgo/blob/v2-unstable/README.md\n//\n// #########################################################\n//\n// Usage of the driver revolves around the concept of sessions.  To\n// get started, obtain a session using the Dial function:\n//\n//     session, err := mgo.Dial(url)\n//\n// This will establish one or more connections with the cluster of\n// servers defined by the url parameter.  From then on, the cluster\n// may be queried with multiple consistency rules (see SetMode) and\n// documents retrieved with statements such as:\n//\n//     c := session.DB(database).C(collection)\n//     err := c.Find(query).One(&result)\n//\n// New sessions are typically created by calling session.Copy on the\n// initial session obtained at dial time. These new sessions will share\n// the same cluster information and connection pool, and may be easily\n// handed into other methods and functions for organizing logic.\n// Every session created must have its Close method called at the end\n// of its life time, so its resources may be put back in the pool or\n// collected, depending on the case.\n//\n// For more details, see the documentation for the types and methods.\n//\npackage mgo\n"
        },
        {
          "name": "export_test.go",
          "type": "blob",
          "size": 0.572265625,
          "content": "package mgo\n\nimport (\n\t\"time\"\n)\n\nfunc HackPingDelay(newDelay time.Duration) (restore func()) {\n\tglobalMutex.Lock()\n\tdefer globalMutex.Unlock()\n\n\toldDelay := pingDelay\n\trestore = func() {\n\t\tglobalMutex.Lock()\n\t\tpingDelay = oldDelay\n\t\tglobalMutex.Unlock()\n\t}\n\tpingDelay = newDelay\n\treturn\n}\n\nfunc HackSyncSocketTimeout(newTimeout time.Duration) (restore func()) {\n\tglobalMutex.Lock()\n\tdefer globalMutex.Unlock()\n\n\toldTimeout := syncSocketTimeout\n\trestore = func() {\n\t\tglobalMutex.Lock()\n\t\tsyncSocketTimeout = oldTimeout\n\t\tglobalMutex.Unlock()\n\t}\n\tsyncSocketTimeout = newTimeout\n\treturn\n}\n"
        },
        {
          "name": "gridfs.go",
          "type": "blob",
          "size": 20.7099609375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"hash\"\n\t\"io\"\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\ntype GridFS struct {\n\tFiles  *Collection\n\tChunks *Collection\n}\n\ntype gfsFileMode int\n\nconst (\n\tgfsClosed  gfsFileMode = 0\n\tgfsReading gfsFileMode = 1\n\tgfsWriting gfsFileMode = 2\n)\n\ntype GridFile struct {\n\tm    sync.Mutex\n\tc    sync.Cond\n\tgfs  *GridFS\n\tmode gfsFileMode\n\terr  error\n\n\tchunk  int\n\toffset int64\n\n\twpending int\n\twbuf     []byte\n\twsum     hash.Hash\n\n\trbuf   []byte\n\trcache *gfsCachedChunk\n\n\tdoc gfsFile\n}\n\ntype gfsFile struct {\n\tId          interface{} \"_id\"\n\tChunkSize   int         \"chunkSize\"\n\tUploadDate  time.Time   \"uploadDate\"\n\tLength      int64       \",minsize\"\n\tMD5         string\n\tFilename    string    \",omitempty\"\n\tContentType string    \"contentType,omitempty\"\n\tMetadata    *bson.Raw \",omitempty\"\n}\n\ntype gfsChunk struct {\n\tId      interface{} \"_id\"\n\tFilesId interface{} \"files_id\"\n\tN       int\n\tData    []byte\n}\n\ntype gfsCachedChunk struct {\n\twait sync.Mutex\n\tn    int\n\tdata []byte\n\terr  error\n}\n\nfunc newGridFS(db *Database, prefix string) *GridFS {\n\treturn &GridFS{db.C(prefix + \".files\"), db.C(prefix + \".chunks\")}\n}\n\nfunc (gfs *GridFS) newFile() *GridFile {\n\tfile := &GridFile{gfs: gfs}\n\tfile.c.L = &file.m\n\t//runtime.SetFinalizer(file, finalizeFile)\n\treturn file\n}\n\nfunc finalizeFile(file *GridFile) {\n\tfile.Close()\n}\n\n// Create creates a new file with the provided name in the GridFS.  If the file\n// name already exists, a new version will be inserted with an up-to-date\n// uploadDate that will cause it to be atomically visible to the Open and\n// OpenId methods.  If the file name is not important, an empty name may be\n// provided and the file Id used instead.\n//\n// It's important to Close files whether they are being written to\n// or read from, and to check the err result to ensure the operation\n// completed successfully.\n//\n// A simple example inserting a new file:\n//\n//     func check(err error) {\n//         if err != nil {\n//             panic(err.String())\n//         }\n//     }\n//     file, err := db.GridFS(\"fs\").Create(\"myfile.txt\")\n//     check(err)\n//     n, err := file.Write([]byte(\"Hello world!\"))\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//     fmt.Printf(\"%d bytes written\\n\", n)\n//\n// The io.Writer interface is implemented by *GridFile and may be used to\n// help on the file creation.  For example:\n//\n//     file, err := db.GridFS(\"fs\").Create(\"myfile.txt\")\n//     check(err)\n//     messages, err := os.Open(\"/var/log/messages\")\n//     check(err)\n//     defer messages.Close()\n//     err = io.Copy(file, messages)\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//\nfunc (gfs *GridFS) Create(name string) (file *GridFile, err error) {\n\tfile = gfs.newFile()\n\tfile.mode = gfsWriting\n\tfile.wsum = md5.New()\n\tfile.doc = gfsFile{Id: bson.NewObjectId(), ChunkSize: 255 * 1024, Filename: name}\n\treturn\n}\n\n// OpenId returns the file with the provided id, for reading.\n// If the file isn't found, err will be set to mgo.ErrNotFound.\n//\n// It's important to Close files whether they are being written to\n// or read from, and to check the err result to ensure the operation\n// completed successfully.\n//\n// The following example will print the first 8192 bytes from the file:\n//\n//     func check(err error) {\n//         if err != nil {\n//             panic(err.String())\n//         }\n//     }\n//     file, err := db.GridFS(\"fs\").OpenId(objid)\n//     check(err)\n//     b := make([]byte, 8192)\n//     n, err := file.Read(b)\n//     check(err)\n//     fmt.Println(string(b))\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//     fmt.Printf(\"%d bytes read\\n\", n)\n//\n// The io.Reader interface is implemented by *GridFile and may be used to\n// deal with it.  As an example, the following snippet will dump the whole\n// file into the standard output:\n//\n//     file, err := db.GridFS(\"fs\").OpenId(objid)\n//     check(err)\n//     err = io.Copy(os.Stdout, file)\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//\nfunc (gfs *GridFS) OpenId(id interface{}) (file *GridFile, err error) {\n\tvar doc gfsFile\n\terr = gfs.Files.Find(bson.M{\"_id\": id}).One(&doc)\n\tif err != nil {\n\t\treturn\n\t}\n\tfile = gfs.newFile()\n\tfile.mode = gfsReading\n\tfile.doc = doc\n\treturn\n}\n\n// Open returns the most recently uploaded file with the provided\n// name, for reading. If the file isn't found, err will be set\n// to mgo.ErrNotFound.\n//\n// It's important to Close files whether they are being written to\n// or read from, and to check the err result to ensure the operation\n// completed successfully.\n//\n// The following example will print the first 8192 bytes from the file:\n//\n//     file, err := db.GridFS(\"fs\").Open(\"myfile.txt\")\n//     check(err)\n//     b := make([]byte, 8192)\n//     n, err := file.Read(b)\n//     check(err)\n//     fmt.Println(string(b))\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//     fmt.Printf(\"%d bytes read\\n\", n)\n//\n// The io.Reader interface is implemented by *GridFile and may be used to\n// deal with it.  As an example, the following snippet will dump the whole\n// file into the standard output:\n//\n//     file, err := db.GridFS(\"fs\").Open(\"myfile.txt\")\n//     check(err)\n//     err = io.Copy(os.Stdout, file)\n//     check(err)\n//     err = file.Close()\n//     check(err)\n//\nfunc (gfs *GridFS) Open(name string) (file *GridFile, err error) {\n\tvar doc gfsFile\n\terr = gfs.Files.Find(bson.M{\"filename\": name}).Sort(\"-uploadDate\").One(&doc)\n\tif err != nil {\n\t\treturn\n\t}\n\tfile = gfs.newFile()\n\tfile.mode = gfsReading\n\tfile.doc = doc\n\treturn\n}\n\n// OpenNext opens the next file from iter for reading, sets *file to it,\n// and returns true on the success case. If no more documents are available\n// on iter or an error occurred, *file is set to nil and the result is false.\n// Errors will be available via iter.Err().\n//\n// The iter parameter must be an iterator on the GridFS files collection.\n// Using the GridFS.Find method is an easy way to obtain such an iterator,\n// but any iterator on the collection will work.\n//\n// If the provided *file is non-nil, OpenNext will close it before attempting\n// to iterate to the next element. This means that in a loop one only\n// has to worry about closing files when breaking out of the loop early\n// (break, return, or panic).\n//\n// For example:\n//\n//     gfs := db.GridFS(\"fs\")\n//     query := gfs.Find(nil).Sort(\"filename\")\n//     iter := query.Iter()\n//     var f *mgo.GridFile\n//     for gfs.OpenNext(iter, &f) {\n//         fmt.Printf(\"Filename: %s\\n\", f.Name())\n//     }\n//     if iter.Close() != nil {\n//         panic(iter.Close())\n//     }\n//\nfunc (gfs *GridFS) OpenNext(iter *Iter, file **GridFile) bool {\n\tif *file != nil {\n\t\t// Ignoring the error here shouldn't be a big deal\n\t\t// as we're reading the file and the loop iteration\n\t\t// for this file is finished.\n\t\t_ = (*file).Close()\n\t}\n\tvar doc gfsFile\n\tif !iter.Next(&doc) {\n\t\t*file = nil\n\t\treturn false\n\t}\n\tf := gfs.newFile()\n\tf.mode = gfsReading\n\tf.doc = doc\n\t*file = f\n\treturn true\n}\n\n// Find runs query on GridFS's files collection and returns\n// the resulting Query.\n//\n// This logic:\n//\n//     gfs := db.GridFS(\"fs\")\n//     iter := gfs.Find(nil).Iter()\n//\n// Is equivalent to:\n//\n//     files := db.C(\"fs\" + \".files\")\n//     iter := files.Find(nil).Iter()\n//\nfunc (gfs *GridFS) Find(query interface{}) *Query {\n\treturn gfs.Files.Find(query)\n}\n\n// RemoveId deletes the file with the provided id from the GridFS.\nfunc (gfs *GridFS) RemoveId(id interface{}) error {\n\terr := gfs.Files.Remove(bson.M{\"_id\": id})\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = gfs.Chunks.RemoveAll(bson.D{{\"files_id\", id}})\n\treturn err\n}\n\ntype gfsDocId struct {\n\tId interface{} \"_id\"\n}\n\n// Remove deletes all files with the provided name from the GridFS.\nfunc (gfs *GridFS) Remove(name string) (err error) {\n\titer := gfs.Files.Find(bson.M{\"filename\": name}).Select(bson.M{\"_id\": 1}).Iter()\n\tvar doc gfsDocId\n\tfor iter.Next(&doc) {\n\t\tif e := gfs.RemoveId(doc.Id); e != nil {\n\t\t\terr = e\n\t\t}\n\t}\n\tif err == nil {\n\t\terr = iter.Close()\n\t}\n\treturn err\n}\n\nfunc (file *GridFile) assertMode(mode gfsFileMode) {\n\tswitch file.mode {\n\tcase mode:\n\t\treturn\n\tcase gfsWriting:\n\t\tpanic(\"GridFile is open for writing\")\n\tcase gfsReading:\n\t\tpanic(\"GridFile is open for reading\")\n\tcase gfsClosed:\n\t\tpanic(\"GridFile is closed\")\n\tdefault:\n\t\tpanic(\"internal error: missing GridFile mode\")\n\t}\n}\n\n// SetChunkSize sets size of saved chunks.  Once the file is written to, it\n// will be split in blocks of that size and each block saved into an\n// independent chunk document.  The default chunk size is 255kb.\n//\n// It is a runtime error to call this function once the file has started\n// being written to.\nfunc (file *GridFile) SetChunkSize(bytes int) {\n\tfile.assertMode(gfsWriting)\n\tdebugf(\"GridFile %p: setting chunk size to %d\", file, bytes)\n\tfile.m.Lock()\n\tfile.doc.ChunkSize = bytes\n\tfile.m.Unlock()\n}\n\n// Id returns the current file Id.\nfunc (file *GridFile) Id() interface{} {\n\treturn file.doc.Id\n}\n\n// SetId changes the current file Id.\n//\n// It is a runtime error to call this function once the file has started\n// being written to, or when the file is not open for writing.\nfunc (file *GridFile) SetId(id interface{}) {\n\tfile.assertMode(gfsWriting)\n\tfile.m.Lock()\n\tfile.doc.Id = id\n\tfile.m.Unlock()\n}\n\n// Name returns the optional file name.  An empty string will be returned\n// in case it is unset.\nfunc (file *GridFile) Name() string {\n\treturn file.doc.Filename\n}\n\n// SetName changes the optional file name.  An empty string may be used to\n// unset it.\n//\n// It is a runtime error to call this function when the file is not open\n// for writing.\nfunc (file *GridFile) SetName(name string) {\n\tfile.assertMode(gfsWriting)\n\tfile.m.Lock()\n\tfile.doc.Filename = name\n\tfile.m.Unlock()\n}\n\n// ContentType returns the optional file content type.  An empty string will be\n// returned in case it is unset.\nfunc (file *GridFile) ContentType() string {\n\treturn file.doc.ContentType\n}\n\n// ContentType changes the optional file content type.  An empty string may be\n// used to unset it.\n//\n// It is a runtime error to call this function when the file is not open\n// for writing.\nfunc (file *GridFile) SetContentType(ctype string) {\n\tfile.assertMode(gfsWriting)\n\tfile.m.Lock()\n\tfile.doc.ContentType = ctype\n\tfile.m.Unlock()\n}\n\n// GetMeta unmarshals the optional \"metadata\" field associated with the\n// file into the result parameter. The meaning of keys under that field\n// is user-defined. For example:\n//\n//     result := struct{ INode int }{}\n//     err = file.GetMeta(&result)\n//     if err != nil {\n//         panic(err.String())\n//     }\n//     fmt.Printf(\"inode: %d\\n\", result.INode)\n//\nfunc (file *GridFile) GetMeta(result interface{}) (err error) {\n\tfile.m.Lock()\n\tif file.doc.Metadata != nil {\n\t\terr = bson.Unmarshal(file.doc.Metadata.Data, result)\n\t}\n\tfile.m.Unlock()\n\treturn\n}\n\n// SetMeta changes the optional \"metadata\" field associated with the\n// file. The meaning of keys under that field is user-defined.\n// For example:\n//\n//     file.SetMeta(bson.M{\"inode\": inode})\n//\n// It is a runtime error to call this function when the file is not open\n// for writing.\nfunc (file *GridFile) SetMeta(metadata interface{}) {\n\tfile.assertMode(gfsWriting)\n\tdata, err := bson.Marshal(metadata)\n\tfile.m.Lock()\n\tif err != nil && file.err == nil {\n\t\tfile.err = err\n\t} else {\n\t\tfile.doc.Metadata = &bson.Raw{Data: data}\n\t}\n\tfile.m.Unlock()\n}\n\n// Size returns the file size in bytes.\nfunc (file *GridFile) Size() (bytes int64) {\n\tfile.m.Lock()\n\tbytes = file.doc.Length\n\tfile.m.Unlock()\n\treturn\n}\n\n// MD5 returns the file MD5 as a hex-encoded string.\nfunc (file *GridFile) MD5() (md5 string) {\n\treturn file.doc.MD5\n}\n\n// UploadDate returns the file upload time.\nfunc (file *GridFile) UploadDate() time.Time {\n\treturn file.doc.UploadDate\n}\n\n// SetUploadDate changes the file upload time.\n//\n// It is a runtime error to call this function when the file is not open\n// for writing.\nfunc (file *GridFile) SetUploadDate(t time.Time) {\n\tfile.assertMode(gfsWriting)\n\tfile.m.Lock()\n\tfile.doc.UploadDate = t\n\tfile.m.Unlock()\n}\n\n// Close flushes any pending changes in case the file is being written\n// to, waits for any background operations to finish, and closes the file.\n//\n// It's important to Close files whether they are being written to\n// or read from, and to check the err result to ensure the operation\n// completed successfully.\nfunc (file *GridFile) Close() (err error) {\n\tfile.m.Lock()\n\tdefer file.m.Unlock()\n\tif file.mode == gfsWriting {\n\t\tif len(file.wbuf) > 0 && file.err == nil {\n\t\t\tfile.insertChunk(file.wbuf)\n\t\t\tfile.wbuf = file.wbuf[0:0]\n\t\t}\n\t\tfile.completeWrite()\n\t} else if file.mode == gfsReading && file.rcache != nil {\n\t\tfile.rcache.wait.Lock()\n\t\tfile.rcache = nil\n\t}\n\tfile.mode = gfsClosed\n\tdebugf(\"GridFile %p: closed\", file)\n\treturn file.err\n}\n\nfunc (file *GridFile) completeWrite() {\n\tfor file.wpending > 0 {\n\t\tdebugf(\"GridFile %p: waiting for %d pending chunks to complete file write\", file, file.wpending)\n\t\tfile.c.Wait()\n\t}\n\tif file.err == nil {\n\t\thexsum := hex.EncodeToString(file.wsum.Sum(nil))\n\t\tif file.doc.UploadDate.IsZero() {\n\t\t\tfile.doc.UploadDate = bson.Now()\n\t\t}\n\t\tfile.doc.MD5 = hexsum\n\t\tfile.err = file.gfs.Files.Insert(file.doc)\n\t}\n\tif file.err != nil {\n\t\tfile.gfs.Chunks.RemoveAll(bson.D{{\"files_id\", file.doc.Id}})\n\t}\n\tif file.err == nil {\n\t\tindex := Index{\n\t\t\tKey:    []string{\"files_id\", \"n\"},\n\t\t\tUnique: true,\n\t\t}\n\t\tfile.err = file.gfs.Chunks.EnsureIndex(index)\n\t}\n}\n\n// Abort cancels an in-progress write, preventing the file from being\n// automically created and ensuring previously written chunks are\n// removed when the file is closed.\n//\n// It is a runtime error to call Abort when the file was not opened\n// for writing.\nfunc (file *GridFile) Abort() {\n\tif file.mode != gfsWriting {\n\t\tpanic(\"file.Abort must be called on file opened for writing\")\n\t}\n\tfile.err = errors.New(\"write aborted\")\n}\n\n// Write writes the provided data to the file and returns the\n// number of bytes written and an error in case something\n// wrong happened.\n//\n// The file will internally cache the data so that all but the last\n// chunk sent to the database have the size defined by SetChunkSize.\n// This also means that errors may be deferred until a future call\n// to Write or Close.\n//\n// The parameters and behavior of this function turn the file\n// into an io.Writer.\nfunc (file *GridFile) Write(data []byte) (n int, err error) {\n\tfile.assertMode(gfsWriting)\n\tfile.m.Lock()\n\tdebugf(\"GridFile %p: writing %d bytes\", file, len(data))\n\tdefer file.m.Unlock()\n\n\tif file.err != nil {\n\t\treturn 0, file.err\n\t}\n\n\tn = len(data)\n\tfile.doc.Length += int64(n)\n\tchunkSize := file.doc.ChunkSize\n\n\tif len(file.wbuf)+len(data) < chunkSize {\n\t\tfile.wbuf = append(file.wbuf, data...)\n\t\treturn\n\t}\n\n\t// First, flush file.wbuf complementing with data.\n\tif len(file.wbuf) > 0 {\n\t\tmissing := chunkSize - len(file.wbuf)\n\t\tif missing > len(data) {\n\t\t\tmissing = len(data)\n\t\t}\n\t\tfile.wbuf = append(file.wbuf, data[:missing]...)\n\t\tdata = data[missing:]\n\t\tfile.insertChunk(file.wbuf)\n\t\tfile.wbuf = file.wbuf[0:0]\n\t}\n\n\t// Then, flush all chunks from data without copying.\n\tfor len(data) > chunkSize {\n\t\tsize := chunkSize\n\t\tif size > len(data) {\n\t\t\tsize = len(data)\n\t\t}\n\t\tfile.insertChunk(data[:size])\n\t\tdata = data[size:]\n\t}\n\n\t// And append the rest for a future call.\n\tfile.wbuf = append(file.wbuf, data...)\n\n\treturn n, file.err\n}\n\nfunc (file *GridFile) insertChunk(data []byte) {\n\tn := file.chunk\n\tfile.chunk++\n\tdebugf(\"GridFile %p: adding to checksum: %q\", file, string(data))\n\tfile.wsum.Write(data)\n\n\tfor file.doc.ChunkSize*file.wpending >= 1024*1024 {\n\t\t// Hold on.. we got a MB pending.\n\t\tfile.c.Wait()\n\t\tif file.err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tfile.wpending++\n\n\tdebugf(\"GridFile %p: inserting chunk %d with %d bytes\", file, n, len(data))\n\n\t// We may not own the memory of data, so rather than\n\t// simply copying it, we'll marshal the document ahead of time.\n\tdata, err := bson.Marshal(gfsChunk{bson.NewObjectId(), file.doc.Id, n, data})\n\tif err != nil {\n\t\tfile.err = err\n\t\treturn\n\t}\n\n\tgo func() {\n\t\terr := file.gfs.Chunks.Insert(bson.Raw{Data: data})\n\t\tfile.m.Lock()\n\t\tfile.wpending--\n\t\tif err != nil && file.err == nil {\n\t\t\tfile.err = err\n\t\t}\n\t\tfile.c.Broadcast()\n\t\tfile.m.Unlock()\n\t}()\n}\n\n// Seek sets the offset for the next Read or Write on file to\n// offset, interpreted according to whence: 0 means relative to\n// the origin of the file, 1 means relative to the current offset,\n// and 2 means relative to the end. It returns the new offset and\n// an error, if any.\nfunc (file *GridFile) Seek(offset int64, whence int) (pos int64, err error) {\n\tfile.m.Lock()\n\tdebugf(\"GridFile %p: seeking for %s (whence=%d)\", file, offset, whence)\n\tdefer file.m.Unlock()\n\tswitch whence {\n\tcase os.SEEK_SET:\n\tcase os.SEEK_CUR:\n\t\toffset += file.offset\n\tcase os.SEEK_END:\n\t\toffset += file.doc.Length\n\tdefault:\n\t\tpanic(\"unsupported whence value\")\n\t}\n\tif offset > file.doc.Length {\n\t\treturn file.offset, errors.New(\"seek past end of file\")\n\t}\n\tif offset == file.doc.Length {\n\t\t// If we're seeking to the end of the file,\n\t\t// no need to read anything. This enables\n\t\t// a client to find the size of the file using only the\n\t\t// io.ReadSeeker interface with low overhead.\n\t\tfile.offset = offset\n\t\treturn file.offset, nil\n\t}\n\tchunk := int(offset / int64(file.doc.ChunkSize))\n\tif chunk+1 == file.chunk && offset >= file.offset {\n\t\tfile.rbuf = file.rbuf[int(offset-file.offset):]\n\t\tfile.offset = offset\n\t\treturn file.offset, nil\n\t}\n\tfile.offset = offset\n\tfile.chunk = chunk\n\tfile.rbuf = nil\n\tfile.rbuf, err = file.getChunk()\n\tif err == nil {\n\t\tfile.rbuf = file.rbuf[int(file.offset-int64(chunk)*int64(file.doc.ChunkSize)):]\n\t}\n\treturn file.offset, err\n}\n\n// Read reads into b the next available data from the file and\n// returns the number of bytes written and an error in case\n// something wrong happened.  At the end of the file, n will\n// be zero and err will be set to io.EOF.\n//\n// The parameters and behavior of this function turn the file\n// into an io.Reader.\nfunc (file *GridFile) Read(b []byte) (n int, err error) {\n\tfile.assertMode(gfsReading)\n\tfile.m.Lock()\n\tdebugf(\"GridFile %p: reading at offset %d into buffer of length %d\", file, file.offset, len(b))\n\tdefer file.m.Unlock()\n\tif file.offset == file.doc.Length {\n\t\treturn 0, io.EOF\n\t}\n\tfor err == nil {\n\t\ti := copy(b, file.rbuf)\n\t\tn += i\n\t\tfile.offset += int64(i)\n\t\tfile.rbuf = file.rbuf[i:]\n\t\tif i == len(b) || file.offset == file.doc.Length {\n\t\t\tbreak\n\t\t}\n\t\tb = b[i:]\n\t\tfile.rbuf, err = file.getChunk()\n\t}\n\treturn n, err\n}\n\nfunc (file *GridFile) getChunk() (data []byte, err error) {\n\tcache := file.rcache\n\tfile.rcache = nil\n\tif cache != nil && cache.n == file.chunk {\n\t\tdebugf(\"GridFile %p: Getting chunk %d from cache\", file, file.chunk)\n\t\tcache.wait.Lock()\n\t\tdata, err = cache.data, cache.err\n\t} else {\n\t\tdebugf(\"GridFile %p: Fetching chunk %d\", file, file.chunk)\n\t\tvar doc gfsChunk\n\t\terr = file.gfs.Chunks.Find(bson.D{{\"files_id\", file.doc.Id}, {\"n\", file.chunk}}).One(&doc)\n\t\tdata = doc.Data\n\t}\n\tfile.chunk++\n\tif int64(file.chunk)*int64(file.doc.ChunkSize) < file.doc.Length {\n\t\t// Read the next one in background.\n\t\tcache = &gfsCachedChunk{n: file.chunk}\n\t\tcache.wait.Lock()\n\t\tdebugf(\"GridFile %p: Scheduling chunk %d for background caching\", file, file.chunk)\n\t\t// Clone the session to avoid having it closed in between.\n\t\tchunks := file.gfs.Chunks\n\t\tsession := chunks.Database.Session.Clone()\n\t\tgo func(id interface{}, n int) {\n\t\t\tdefer session.Close()\n\t\t\tchunks = chunks.With(session)\n\t\t\tvar doc gfsChunk\n\t\t\tcache.err = chunks.Find(bson.D{{\"files_id\", id}, {\"n\", n}}).One(&doc)\n\t\t\tcache.data = doc.Data\n\t\t\tcache.wait.Unlock()\n\t\t}(file.doc.Id, file.chunk)\n\t\tfile.rcache = cache\n\t}\n\tdebugf(\"Returning err: %#v\", err)\n\treturn\n}\n"
        },
        {
          "name": "gridfs_test.go",
          "type": "blob",
          "size": 15.8466796875,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t\"io\"\n\t\"os\"\n\t\"time\"\n\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\nfunc (s *S) TestGridFSCreate(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tbefore := bson.Now()\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\n\tn, err := file.Write([]byte(\"some data\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 9)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\tafter := bson.Now()\n\n\t// Check the file information.\n\tresult := M{}\n\terr = db.C(\"fs.files\").Find(nil).One(result)\n\tc.Assert(err, IsNil)\n\n\tfileId, ok := result[\"_id\"].(bson.ObjectId)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(fileId.Valid(), Equals, true)\n\tresult[\"_id\"] = \"<id>\"\n\n\tud, ok := result[\"uploadDate\"].(time.Time)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(ud.After(before) && ud.Before(after), Equals, true)\n\tresult[\"uploadDate\"] = \"<timestamp>\"\n\n\texpected := M{\n\t\t\"_id\":        \"<id>\",\n\t\t\"length\":     9,\n\t\t\"chunkSize\":  255 * 1024,\n\t\t\"uploadDate\": \"<timestamp>\",\n\t\t\"md5\":        \"1e50210a0202497fb79bc38b6ade6c34\",\n\t}\n\tc.Assert(result, DeepEquals, expected)\n\n\t// Check the chunk.\n\tresult = M{}\n\terr = db.C(\"fs.chunks\").Find(nil).One(result)\n\tc.Assert(err, IsNil)\n\n\tchunkId, ok := result[\"_id\"].(bson.ObjectId)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(chunkId.Valid(), Equals, true)\n\tresult[\"_id\"] = \"<id>\"\n\n\texpected = M{\n\t\t\"_id\":      \"<id>\",\n\t\t\"files_id\": fileId,\n\t\t\"n\":        0,\n\t\t\"data\":     []byte(\"some data\"),\n\t}\n\tc.Assert(result, DeepEquals, expected)\n\n\t// Check that an index was created.\n\tindexes, err := db.C(\"fs.chunks\").Indexes()\n\tc.Assert(err, IsNil)\n\tc.Assert(len(indexes), Equals, 2)\n\tc.Assert(indexes[1].Key, DeepEquals, []string{\"files_id\", \"n\"})\n}\n\nfunc (s *S) TestGridFSFileDetails(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"myfile1.txt\")\n\tc.Assert(err, IsNil)\n\n\tn, err := file.Write([]byte(\"some\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 4)\n\n\tc.Assert(file.Size(), Equals, int64(4))\n\n\tn, err = file.Write([]byte(\" data\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 5)\n\n\tc.Assert(file.Size(), Equals, int64(9))\n\n\tid, _ := file.Id().(bson.ObjectId)\n\tc.Assert(id.Valid(), Equals, true)\n\tc.Assert(file.Name(), Equals, \"myfile1.txt\")\n\tc.Assert(file.ContentType(), Equals, \"\")\n\n\tvar info interface{}\n\terr = file.GetMeta(&info)\n\tc.Assert(err, IsNil)\n\tc.Assert(info, IsNil)\n\n\tfile.SetId(\"myid\")\n\tfile.SetName(\"myfile2.txt\")\n\tfile.SetContentType(\"text/plain\")\n\tfile.SetMeta(M{\"any\": \"thing\"})\n\n\tc.Assert(file.Id(), Equals, \"myid\")\n\tc.Assert(file.Name(), Equals, \"myfile2.txt\")\n\tc.Assert(file.ContentType(), Equals, \"text/plain\")\n\n\terr = file.GetMeta(&info)\n\tc.Assert(err, IsNil)\n\tc.Assert(info, DeepEquals, bson.M{\"any\": \"thing\"})\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\tc.Assert(file.MD5(), Equals, \"1e50210a0202497fb79bc38b6ade6c34\")\n\n\tud := file.UploadDate()\n\tnow := time.Now()\n\tc.Assert(ud.Before(now), Equals, true)\n\tc.Assert(ud.After(now.Add(-3*time.Second)), Equals, true)\n\n\tresult := M{}\n\terr = db.C(\"fs.files\").Find(nil).One(result)\n\tc.Assert(err, IsNil)\n\n\tresult[\"uploadDate\"] = \"<timestamp>\"\n\n\texpected := M{\n\t\t\"_id\":         \"myid\",\n\t\t\"length\":      9,\n\t\t\"chunkSize\":   255 * 1024,\n\t\t\"uploadDate\":  \"<timestamp>\",\n\t\t\"md5\":         \"1e50210a0202497fb79bc38b6ade6c34\",\n\t\t\"filename\":    \"myfile2.txt\",\n\t\t\"contentType\": \"text/plain\",\n\t\t\"metadata\":    M{\"any\": \"thing\"},\n\t}\n\tc.Assert(result, DeepEquals, expected)\n}\n\nfunc (s *S) TestGridFSSetUploadDate(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\n\tt := time.Date(2014, 1, 1, 1, 1, 1, 0, time.Local)\n\tfile.SetUploadDate(t)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\t// Check the file information.\n\tresult := M{}\n\terr = db.C(\"fs.files\").Find(nil).One(result)\n\tc.Assert(err, IsNil)\n\n\tud := result[\"uploadDate\"].(time.Time)\n\tif !ud.Equal(t) {\n\t\tc.Fatalf(\"want upload date %s, got %s\", t, ud)\n\t}\n}\n\nfunc (s *S) TestGridFSCreateWithChunking(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\n\tfile.SetChunkSize(5)\n\n\t// Smaller than the chunk size.\n\tn, err := file.Write([]byte(\"abc\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n\n\t// Boundary in the middle.\n\tn, err = file.Write([]byte(\"defg\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 4)\n\n\t// Boundary at the end.\n\tn, err = file.Write([]byte(\"hij\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n\n\t// Larger than the chunk size, with 3 chunks.\n\tn, err = file.Write([]byte(\"klmnopqrstuv\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 12)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\t// Check the file information.\n\tresult := M{}\n\terr = db.C(\"fs.files\").Find(nil).One(result)\n\tc.Assert(err, IsNil)\n\n\tfileId, _ := result[\"_id\"].(bson.ObjectId)\n\tc.Assert(fileId.Valid(), Equals, true)\n\tresult[\"_id\"] = \"<id>\"\n\tresult[\"uploadDate\"] = \"<timestamp>\"\n\n\texpected := M{\n\t\t\"_id\":        \"<id>\",\n\t\t\"length\":     22,\n\t\t\"chunkSize\":  5,\n\t\t\"uploadDate\": \"<timestamp>\",\n\t\t\"md5\":        \"44a66044834cbe55040089cabfc102d5\",\n\t}\n\tc.Assert(result, DeepEquals, expected)\n\n\t// Check the chunks.\n\titer := db.C(\"fs.chunks\").Find(nil).Sort(\"n\").Iter()\n\tdataChunks := []string{\"abcde\", \"fghij\", \"klmno\", \"pqrst\", \"uv\"}\n\tfor i := 0; ; i++ {\n\t\tresult = M{}\n\t\tif !iter.Next(result) {\n\t\t\tif i != 5 {\n\t\t\t\tc.Fatalf(\"Expected 5 chunks, got %d\", i)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tc.Assert(iter.Close(), IsNil)\n\n\t\tresult[\"_id\"] = \"<id>\"\n\n\t\texpected = M{\n\t\t\t\"_id\":      \"<id>\",\n\t\t\t\"files_id\": fileId,\n\t\t\t\"n\":        i,\n\t\t\t\"data\":     []byte(dataChunks[i]),\n\t\t}\n\t\tc.Assert(result, DeepEquals, expected)\n\t}\n}\n\nfunc (s *S) TestGridFSAbort(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\n\tfile.SetChunkSize(5)\n\n\tn, err := file.Write([]byte(\"some data\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 9)\n\n\tvar count int\n\tfor i := 0; i < 10; i++ {\n\t\tcount, err = db.C(\"fs.chunks\").Count()\n\t\tif count > 0 || err != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\tc.Assert(err, IsNil)\n\tc.Assert(count, Equals, 1)\n\n\tfile.Abort()\n\n\terr = file.Close()\n\tc.Assert(err, ErrorMatches, \"write aborted\")\n\n\tcount, err = db.C(\"fs.chunks\").Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(count, Equals, 0)\n}\n\nfunc (s *S) TestGridFSCloseConflict(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tdb.C(\"fs.files\").EnsureIndex(mgo.Index{Key: []string{\"filename\"}, Unique: true})\n\n\t// For a closing-time conflict\n\terr = db.C(\"fs.files\").Insert(M{\"filename\": \"foo.txt\"})\n\tc.Assert(err, IsNil)\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"foo.txt\")\n\tc.Assert(err, IsNil)\n\n\t_, err = file.Write([]byte(\"some data\"))\n\tc.Assert(err, IsNil)\n\n\terr = file.Close()\n\tc.Assert(mgo.IsDup(err), Equals, true)\n\n\tcount, err := db.C(\"fs.chunks\").Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(count, Equals, 0)\n}\n\nfunc (s *S) TestGridFSOpenNotFound(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.OpenId(\"non-existent\")\n\tc.Assert(err == mgo.ErrNotFound, Equals, true)\n\tc.Assert(file, IsNil)\n\n\tfile, err = gfs.Open(\"non-existent\")\n\tc.Assert(err == mgo.ErrNotFound, Equals, true)\n\tc.Assert(file, IsNil)\n}\n\nfunc (s *S) TestGridFSReadAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\tid := file.Id()\n\n\tfile.SetChunkSize(5)\n\n\tn, err := file.Write([]byte(\"abcdefghijklmnopqrstuv\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 22)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\tfile, err = gfs.OpenId(id)\n\tc.Assert(err, IsNil)\n\n\tb := make([]byte, 30)\n\tn, err = file.Read(b)\n\tc.Assert(n, Equals, 22)\n\tc.Assert(err, IsNil)\n\n\tn, err = file.Read(b)\n\tc.Assert(n, Equals, 0)\n\tc.Assert(err == io.EOF, Equals, true)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestGridFSReadChunking(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\n\tid := file.Id()\n\n\tfile.SetChunkSize(5)\n\n\tn, err := file.Write([]byte(\"abcdefghijklmnopqrstuv\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 22)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\tfile, err = gfs.OpenId(id)\n\tc.Assert(err, IsNil)\n\n\tb := make([]byte, 30)\n\n\t// Smaller than the chunk size.\n\tn, err = file.Read(b[:3])\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n\tc.Assert(b[:3], DeepEquals, []byte(\"abc\"))\n\n\t// Boundary in the middle.\n\tn, err = file.Read(b[:4])\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 4)\n\tc.Assert(b[:4], DeepEquals, []byte(\"defg\"))\n\n\t// Boundary at the end.\n\tn, err = file.Read(b[:3])\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n\tc.Assert(b[:3], DeepEquals, []byte(\"hij\"))\n\n\t// Larger than the chunk size, with 3 chunks.\n\tn, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 12)\n\tc.Assert(b[:12], DeepEquals, []byte(\"klmnopqrstuv\"))\n\n\tn, err = file.Read(b)\n\tc.Assert(n, Equals, 0)\n\tc.Assert(err == io.EOF, Equals, true)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestGridFSOpen(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'1'})\n\tfile.Close()\n\n\tfile, err = gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'2'})\n\tfile.Close()\n\n\tfile, err = gfs.Open(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tdefer file.Close()\n\n\tvar b [1]byte\n\n\t_, err = file.Read(b[:])\n\tc.Assert(err, IsNil)\n\tc.Assert(string(b[:]), Equals, \"2\")\n}\n\nfunc (s *S) TestGridFSSeek(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\tfile, err := gfs.Create(\"\")\n\tc.Assert(err, IsNil)\n\tid := file.Id()\n\n\tfile.SetChunkSize(5)\n\n\tn, err := file.Write([]byte(\"abcdefghijklmnopqrstuv\"))\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 22)\n\n\terr = file.Close()\n\tc.Assert(err, IsNil)\n\n\tb := make([]byte, 5)\n\n\tfile, err = gfs.OpenId(id)\n\tc.Assert(err, IsNil)\n\n\to, err := file.Seek(3, os.SEEK_SET)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(3))\n\t_, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(b, DeepEquals, []byte(\"defgh\"))\n\n\to, err = file.Seek(5, os.SEEK_CUR)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(13))\n\t_, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(b, DeepEquals, []byte(\"nopqr\"))\n\n\to, err = file.Seek(0, os.SEEK_END)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(22))\n\tn, err = file.Read(b)\n\tc.Assert(err, Equals, io.EOF)\n\tc.Assert(n, Equals, 0)\n\n\to, err = file.Seek(-10, os.SEEK_END)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(12))\n\t_, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(b, DeepEquals, []byte(\"mnopq\"))\n\n\to, err = file.Seek(8, os.SEEK_SET)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(8))\n\t_, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(b, DeepEquals, []byte(\"ijklm\"))\n\n\t// Trivial seek forward within same chunk. Already\n\t// got the data, shouldn't touch the database.\n\tsent := mgo.GetStats().SentOps\n\to, err = file.Seek(1, os.SEEK_CUR)\n\tc.Assert(err, IsNil)\n\tc.Assert(o, Equals, int64(14))\n\tc.Assert(mgo.GetStats().SentOps, Equals, sent)\n\t_, err = file.Read(b)\n\tc.Assert(err, IsNil)\n\tc.Assert(b, DeepEquals, []byte(\"opqrs\"))\n\n\t// Try seeking past end of file.\n\tfile.Seek(3, os.SEEK_SET)\n\to, err = file.Seek(23, os.SEEK_SET)\n\tc.Assert(err, ErrorMatches, \"seek past end of file\")\n\tc.Assert(o, Equals, int64(3))\n}\n\nfunc (s *S) TestGridFSRemoveId(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'1'})\n\tfile.Close()\n\n\tfile, err = gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'2'})\n\tid := file.Id()\n\tfile.Close()\n\n\terr = gfs.RemoveId(id)\n\tc.Assert(err, IsNil)\n\n\tfile, err = gfs.Open(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tdefer file.Close()\n\n\tvar b [1]byte\n\n\t_, err = file.Read(b[:])\n\tc.Assert(err, IsNil)\n\tc.Assert(string(b[:]), Equals, \"1\")\n\n\tn, err := db.C(\"fs.chunks\").Find(M{\"files_id\": id}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 0)\n}\n\nfunc (s *S) TestGridFSRemove(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'1'})\n\tfile.Close()\n\n\tfile, err = gfs.Create(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'2'})\n\tfile.Close()\n\n\terr = gfs.Remove(\"myfile.txt\")\n\tc.Assert(err, IsNil)\n\n\t_, err = gfs.Open(\"myfile.txt\")\n\tc.Assert(err == mgo.ErrNotFound, Equals, true)\n\n\tn, err := db.C(\"fs.chunks\").Find(nil).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 0)\n}\n\nfunc (s *S) TestGridFSOpenNext(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\n\tgfs := db.GridFS(\"fs\")\n\n\tfile, err := gfs.Create(\"myfile1.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'1'})\n\tfile.Close()\n\n\tfile, err = gfs.Create(\"myfile2.txt\")\n\tc.Assert(err, IsNil)\n\tfile.Write([]byte{'2'})\n\tfile.Close()\n\n\tvar f *mgo.GridFile\n\tvar b [1]byte\n\n\titer := gfs.Find(nil).Sort(\"-filename\").Iter()\n\n\tok := gfs.OpenNext(iter, &f)\n\tc.Assert(ok, Equals, true)\n\tc.Check(f.Name(), Equals, \"myfile2.txt\")\n\n\t_, err = f.Read(b[:])\n\tc.Assert(err, IsNil)\n\tc.Assert(string(b[:]), Equals, \"2\")\n\n\tok = gfs.OpenNext(iter, &f)\n\tc.Assert(ok, Equals, true)\n\tc.Check(f.Name(), Equals, \"myfile1.txt\")\n\n\t_, err = f.Read(b[:])\n\tc.Assert(err, IsNil)\n\tc.Assert(string(b[:]), Equals, \"1\")\n\n\tok = gfs.OpenNext(iter, &f)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(f, IsNil)\n\n\t// Do it again with a more restrictive query to make sure\n\t// it's actually taken into account.\n\titer = gfs.Find(bson.M{\"filename\": \"myfile1.txt\"}).Iter()\n\n\tok = gfs.OpenNext(iter, &f)\n\tc.Assert(ok, Equals, true)\n\tc.Check(f.Name(), Equals, \"myfile1.txt\")\n\n\tok = gfs.OpenNext(iter, &f)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(f, IsNil)\n}\n"
        },
        {
          "name": "harness",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "log.go",
          "type": "blob",
          "size": 3.8193359375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// ---------------------------------------------------------------------------\n// Logging integration.\n\n// Avoid importing the log type information unnecessarily.  There's a small cost\n// associated with using an interface rather than the type.  Depending on how\n// often the logger is plugged in, it would be worth using the type instead.\ntype log_Logger interface {\n\tOutput(calldepth int, s string) error\n}\n\nvar (\n\tglobalLogger log_Logger\n\tglobalDebug  bool\n\tglobalMutex  sync.Mutex\n)\n\n// RACE WARNING: There are known data races when logging, which are manually\n// silenced when the race detector is in use. These data races won't be\n// observed in typical use, because logging is supposed to be set up once when\n// the application starts. Having raceDetector as a constant, the compiler\n// should elide the locks altogether in actual use.\n\n// Specify the *log.Logger object where log messages should be sent to.\nfunc SetLogger(logger log_Logger) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tglobalLogger = logger\n}\n\n// Enable the delivery of debug messages to the logger.  Only meaningful\n// if a logger is also set.\nfunc SetDebug(debug bool) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tglobalDebug = debug\n}\n\nfunc log(v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprint(v...))\n\t}\n}\n\nfunc logln(v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprintln(v...))\n\t}\n}\n\nfunc logf(format string, v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprintf(format, v...))\n\t}\n}\n\nfunc debug(v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalDebug && globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprint(v...))\n\t}\n}\n\nfunc debugln(v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalDebug && globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprintln(v...))\n\t}\n}\n\nfunc debugf(format string, v ...interface{}) {\n\tif raceDetector {\n\t\tglobalMutex.Lock()\n\t\tdefer globalMutex.Unlock()\n\t}\n\tif globalDebug && globalLogger != nil {\n\t\tglobalLogger.Output(2, fmt.Sprintf(format, v...))\n\t}\n}\n"
        },
        {
          "name": "queue.go",
          "type": "blob",
          "size": 2.787109375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\ntype queue struct {\n\telems               []interface{}\n\tnelems, popi, pushi int\n}\n\nfunc (q *queue) Len() int {\n\treturn q.nelems\n}\n\nfunc (q *queue) Push(elem interface{}) {\n\t//debugf(\"Pushing(pushi=%d popi=%d cap=%d): %#v\\n\",\n\t//       q.pushi, q.popi, len(q.elems), elem)\n\tif q.nelems == len(q.elems) {\n\t\tq.expand()\n\t}\n\tq.elems[q.pushi] = elem\n\tq.nelems++\n\tq.pushi = (q.pushi + 1) % len(q.elems)\n\t//debugf(\" Pushed(pushi=%d popi=%d cap=%d): %#v\\n\",\n\t//       q.pushi, q.popi, len(q.elems), elem)\n}\n\nfunc (q *queue) Pop() (elem interface{}) {\n\t//debugf(\"Popping(pushi=%d popi=%d cap=%d)\\n\",\n\t//       q.pushi, q.popi, len(q.elems))\n\tif q.nelems == 0 {\n\t\treturn nil\n\t}\n\telem = q.elems[q.popi]\n\tq.elems[q.popi] = nil // Help GC.\n\tq.nelems--\n\tq.popi = (q.popi + 1) % len(q.elems)\n\t//debugf(\" Popped(pushi=%d popi=%d cap=%d): %#v\\n\",\n\t//       q.pushi, q.popi, len(q.elems), elem)\n\treturn elem\n}\n\nfunc (q *queue) expand() {\n\tcurcap := len(q.elems)\n\tvar newcap int\n\tif curcap == 0 {\n\t\tnewcap = 8\n\t} else if curcap < 1024 {\n\t\tnewcap = curcap * 2\n\t} else {\n\t\tnewcap = curcap + (curcap / 4)\n\t}\n\telems := make([]interface{}, newcap)\n\n\tif q.popi == 0 {\n\t\tcopy(elems, q.elems)\n\t\tq.pushi = curcap\n\t} else {\n\t\tnewpopi := newcap - (curcap - q.popi)\n\t\tcopy(elems, q.elems[:q.popi])\n\t\tcopy(elems[newpopi:], q.elems[q.popi:])\n\t\tq.popi = newpopi\n\t}\n\tfor i := range q.elems {\n\t\tq.elems[i] = nil // Help GC.\n\t}\n\tq.elems = elems\n}\n"
        },
        {
          "name": "queue_test.go",
          "type": "blob",
          "size": 2.7275390625,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t. \"gopkg.in/check.v1\"\n)\n\ntype QS struct{}\n\nvar _ = Suite(&QS{})\n\nfunc (s *QS) TestSequentialGrowth(c *C) {\n\tq := queue{}\n\tn := 2048\n\tfor i := 0; i != n; i++ {\n\t\tq.Push(i)\n\t}\n\tfor i := 0; i != n; i++ {\n\t\tc.Assert(q.Pop(), Equals, i)\n\t}\n}\n\nvar queueTestLists = [][]int{\n\t// {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\t{0, 1, 2, 3, 4, 5, 6, 7, 8, 9},\n\n\t// {8, 9, 10, 11, ... 2, 3, 4, 5, 6, 7}\n\t{0, 1, 2, 3, 4, 5, 6, 7, -1, -1, 8, 9, 10, 11},\n\n\t// {8, 9, 10, 11, ... 2, 3, 4, 5, 6, 7}\n\t{0, 1, 2, 3, -1, -1, 4, 5, 6, 7, 8, 9, 10, 11},\n\n\t// {0, 1, 2, 3, 4, 5, 6, 7, 8}\n\t{0, 1, 2, 3, 4, 5, 6, 7, 8,\n\t\t-1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t\t0, 1, 2, 3, 4, 5, 6, 7, 8},\n}\n\nfunc (s *QS) TestQueueTestLists(c *C) {\n\ttest := []int{}\n\ttesti := 0\n\treset := func() {\n\t\ttest = test[0:0]\n\t\ttesti = 0\n\t}\n\tpush := func(i int) {\n\t\ttest = append(test, i)\n\t}\n\tpop := func() (i int) {\n\t\tif testi == len(test) {\n\t\t\treturn -1\n\t\t}\n\t\ti = test[testi]\n\t\ttesti++\n\t\treturn\n\t}\n\n\tfor _, list := range queueTestLists {\n\t\treset()\n\t\tq := queue{}\n\t\tfor _, n := range list {\n\t\t\tif n == -1 {\n\t\t\t\tc.Assert(q.Pop(), Equals, pop(), Commentf(\"With list %#v\", list))\n\t\t\t} else {\n\t\t\t\tq.Push(n)\n\t\t\t\tpush(n)\n\t\t\t}\n\t\t}\n\n\t\tfor n := pop(); n != -1; n = pop() {\n\t\t\tc.Assert(q.Pop(), Equals, n, Commentf(\"With list %#v\", list))\n\t\t}\n\n\t\tc.Assert(q.Pop(), Equals, nil, Commentf(\"With list %#v\", list))\n\t}\n}\n"
        },
        {
          "name": "raceoff.go",
          "type": "blob",
          "size": 0.0556640625,
          "content": "// +build !race\n\npackage mgo\n\nconst raceDetector = false\n"
        },
        {
          "name": "raceon.go",
          "type": "blob",
          "size": 0.0537109375,
          "content": "// +build race\n\npackage mgo\n\nconst raceDetector = true\n"
        },
        {
          "name": "saslimpl.go",
          "type": "blob",
          "size": 0.21875,
          "content": "//+build sasl\n\npackage mgo\n\nimport (\n\t\"gopkg.in/mgo.v2/internal/sasl\"\n)\n\nfunc saslNew(cred Credential, host string) (saslStepper, error) {\n\treturn sasl.New(cred.Username, cred.Password, cred.Mechanism, cred.Service, host)\n}\n"
        },
        {
          "name": "saslstub.go",
          "type": "blob",
          "size": 0.189453125,
          "content": "//+build !sasl\n\npackage mgo\n\nimport (\n\t\"fmt\"\n)\n\nfunc saslNew(cred Credential, host string) (saslStepper, error) {\n\treturn nil, fmt.Errorf(\"SASL support not enabled during build (-tags sasl)\")\n}\n"
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 12.1484375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"errors\"\n\t\"net\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\n// ---------------------------------------------------------------------------\n// Mongo server encapsulation.\n\ntype mongoServer struct {\n\tsync.RWMutex\n\tAddr          string\n\tResolvedAddr  string\n\ttcpaddr       *net.TCPAddr\n\tunusedSockets []*mongoSocket\n\tliveSockets   []*mongoSocket\n\tclosed        bool\n\tabended       bool\n\tsync          chan bool\n\tdial          dialer\n\tpingValue     time.Duration\n\tpingIndex     int\n\tpingCount     uint32\n\tpingWindow    [6]time.Duration\n\tinfo          *mongoServerInfo\n}\n\ntype dialer struct {\n\told func(addr net.Addr) (net.Conn, error)\n\tnew func(addr *ServerAddr) (net.Conn, error)\n}\n\nfunc (dial dialer) isSet() bool {\n\treturn dial.old != nil || dial.new != nil\n}\n\ntype mongoServerInfo struct {\n\tMaster         bool\n\tMongos         bool\n\tTags           bson.D\n\tMaxWireVersion int\n\tSetName        string\n}\n\nvar defaultServerInfo mongoServerInfo\n\nfunc newServer(addr string, tcpaddr *net.TCPAddr, sync chan bool, dial dialer) *mongoServer {\n\tserver := &mongoServer{\n\t\tAddr:         addr,\n\t\tResolvedAddr: tcpaddr.String(),\n\t\ttcpaddr:      tcpaddr,\n\t\tsync:         sync,\n\t\tdial:         dial,\n\t\tinfo:         &defaultServerInfo,\n\t\tpingValue:    time.Hour, // Push it back before an actual ping.\n\t}\n\tgo server.pinger(true)\n\treturn server\n}\n\nvar errPoolLimit = errors.New(\"per-server connection limit reached\")\nvar errServerClosed = errors.New(\"server was closed\")\n\n// AcquireSocket returns a socket for communicating with the server.\n// This will attempt to reuse an old connection, if one is available. Otherwise,\n// it will establish a new one. The returned socket is owned by the call site,\n// and will return to the cache when the socket has its Release method called\n// the same number of times as AcquireSocket + Acquire were called for it.\n// If the poolLimit argument is greater than zero and the number of sockets in\n// use in this server is greater than the provided limit, errPoolLimit is\n// returned.\nfunc (server *mongoServer) AcquireSocket(poolLimit int, timeout time.Duration) (socket *mongoSocket, abended bool, err error) {\n\tfor {\n\t\tserver.Lock()\n\t\tabended = server.abended\n\t\tif server.closed {\n\t\t\tserver.Unlock()\n\t\t\treturn nil, abended, errServerClosed\n\t\t}\n\t\tn := len(server.unusedSockets)\n\t\tif poolLimit > 0 && len(server.liveSockets)-n >= poolLimit {\n\t\t\tserver.Unlock()\n\t\t\treturn nil, false, errPoolLimit\n\t\t}\n\t\tif n > 0 {\n\t\t\tsocket = server.unusedSockets[n-1]\n\t\t\tserver.unusedSockets[n-1] = nil // Help GC.\n\t\t\tserver.unusedSockets = server.unusedSockets[:n-1]\n\t\t\tinfo := server.info\n\t\t\tserver.Unlock()\n\t\t\terr = socket.InitialAcquire(info, timeout)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tserver.Unlock()\n\t\t\tsocket, err = server.Connect(timeout)\n\t\t\tif err == nil {\n\t\t\t\tserver.Lock()\n\t\t\t\t// We've waited for the Connect, see if we got\n\t\t\t\t// closed in the meantime\n\t\t\t\tif server.closed {\n\t\t\t\t\tserver.Unlock()\n\t\t\t\t\tsocket.Release()\n\t\t\t\t\tsocket.Close()\n\t\t\t\t\treturn nil, abended, errServerClosed\n\t\t\t\t}\n\t\t\t\tserver.liveSockets = append(server.liveSockets, socket)\n\t\t\t\tserver.Unlock()\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\tpanic(\"unreachable\")\n}\n\n// Connect establishes a new connection to the server. This should\n// generally be done through server.AcquireSocket().\nfunc (server *mongoServer) Connect(timeout time.Duration) (*mongoSocket, error) {\n\tserver.RLock()\n\tmaster := server.info.Master\n\tdial := server.dial\n\tserver.RUnlock()\n\n\tlogf(\"Establishing new connection to %s (timeout=%s)...\", server.Addr, timeout)\n\tvar conn net.Conn\n\tvar err error\n\tswitch {\n\tcase !dial.isSet():\n\t\t// Cannot do this because it lacks timeout support. :-(\n\t\t//conn, err = net.DialTCP(\"tcp\", nil, server.tcpaddr)\n\t\tconn, err = net.DialTimeout(\"tcp\", server.ResolvedAddr, timeout)\n\t\tif tcpconn, ok := conn.(*net.TCPConn); ok {\n\t\t\ttcpconn.SetKeepAlive(true)\n\t\t} else if err == nil {\n\t\t\tpanic(\"internal error: obtained TCP connection is not a *net.TCPConn!?\")\n\t\t}\n\tcase dial.old != nil:\n\t\tconn, err = dial.old(server.tcpaddr)\n\tcase dial.new != nil:\n\t\tconn, err = dial.new(&ServerAddr{server.Addr, server.tcpaddr})\n\tdefault:\n\t\tpanic(\"dialer is set, but both dial.old and dial.new are nil\")\n\t}\n\tif err != nil {\n\t\tlogf(\"Connection to %s failed: %v\", server.Addr, err.Error())\n\t\treturn nil, err\n\t}\n\tlogf(\"Connection to %s established.\", server.Addr)\n\n\tstats.conn(+1, master)\n\treturn newSocket(server, conn, timeout), nil\n}\n\n// Close forces closing all sockets that are alive, whether\n// they're currently in use or not.\nfunc (server *mongoServer) Close() {\n\tserver.Lock()\n\tserver.closed = true\n\tliveSockets := server.liveSockets\n\tunusedSockets := server.unusedSockets\n\tserver.liveSockets = nil\n\tserver.unusedSockets = nil\n\tserver.Unlock()\n\tlogf(\"Connections to %s closing (%d live sockets).\", server.Addr, len(liveSockets))\n\tfor i, s := range liveSockets {\n\t\ts.Close()\n\t\tliveSockets[i] = nil\n\t}\n\tfor i := range unusedSockets {\n\t\tunusedSockets[i] = nil\n\t}\n}\n\n// RecycleSocket puts socket back into the unused cache.\nfunc (server *mongoServer) RecycleSocket(socket *mongoSocket) {\n\tserver.Lock()\n\tif !server.closed {\n\t\tserver.unusedSockets = append(server.unusedSockets, socket)\n\t}\n\tserver.Unlock()\n}\n\nfunc removeSocket(sockets []*mongoSocket, socket *mongoSocket) []*mongoSocket {\n\tfor i, s := range sockets {\n\t\tif s == socket {\n\t\t\tcopy(sockets[i:], sockets[i+1:])\n\t\t\tn := len(sockets) - 1\n\t\t\tsockets[n] = nil\n\t\t\tsockets = sockets[:n]\n\t\t\tbreak\n\t\t}\n\t}\n\treturn sockets\n}\n\n// AbendSocket notifies the server that the given socket has terminated\n// abnormally, and thus should be discarded rather than cached.\nfunc (server *mongoServer) AbendSocket(socket *mongoSocket) {\n\tserver.Lock()\n\tserver.abended = true\n\tif server.closed {\n\t\tserver.Unlock()\n\t\treturn\n\t}\n\tserver.liveSockets = removeSocket(server.liveSockets, socket)\n\tserver.unusedSockets = removeSocket(server.unusedSockets, socket)\n\tserver.Unlock()\n\t// Maybe just a timeout, but suggest a cluster sync up just in case.\n\tselect {\n\tcase server.sync <- true:\n\tdefault:\n\t}\n}\n\nfunc (server *mongoServer) SetInfo(info *mongoServerInfo) {\n\tserver.Lock()\n\tserver.info = info\n\tserver.Unlock()\n}\n\nfunc (server *mongoServer) Info() *mongoServerInfo {\n\tserver.Lock()\n\tinfo := server.info\n\tserver.Unlock()\n\treturn info\n}\n\nfunc (server *mongoServer) hasTags(serverTags []bson.D) bool {\nNextTagSet:\n\tfor _, tags := range serverTags {\n\tNextReqTag:\n\t\tfor _, req := range tags {\n\t\t\tfor _, has := range server.info.Tags {\n\t\t\t\tif req.Name == has.Name {\n\t\t\t\t\tif req.Value == has.Value {\n\t\t\t\t\t\tcontinue NextReqTag\n\t\t\t\t\t}\n\t\t\t\t\tcontinue NextTagSet\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue NextTagSet\n\t\t}\n\t\treturn true\n\t}\n\treturn false\n}\n\nvar pingDelay = 15 * time.Second\n\nfunc (server *mongoServer) pinger(loop bool) {\n\tvar delay time.Duration\n\tif raceDetector {\n\t\t// This variable is only ever touched by tests.\n\t\tglobalMutex.Lock()\n\t\tdelay = pingDelay\n\t\tglobalMutex.Unlock()\n\t} else {\n\t\tdelay = pingDelay\n\t}\n\top := queryOp{\n\t\tcollection: \"admin.$cmd\",\n\t\tquery:      bson.D{{\"ping\", 1}},\n\t\tflags:      flagSlaveOk,\n\t\tlimit:      -1,\n\t}\n\tfor {\n\t\tif loop {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t\top := op\n\t\tsocket, _, err := server.AcquireSocket(0, delay)\n\t\tif err == nil {\n\t\t\tstart := time.Now()\n\t\t\t_, _ = socket.SimpleQuery(&op)\n\t\t\tdelay := time.Now().Sub(start)\n\n\t\t\tserver.pingWindow[server.pingIndex] = delay\n\t\t\tserver.pingIndex = (server.pingIndex + 1) % len(server.pingWindow)\n\t\t\tserver.pingCount++\n\t\t\tvar max time.Duration\n\t\t\tfor i := 0; i < len(server.pingWindow) && uint32(i) < server.pingCount; i++ {\n\t\t\t\tif server.pingWindow[i] > max {\n\t\t\t\t\tmax = server.pingWindow[i]\n\t\t\t\t}\n\t\t\t}\n\t\t\tsocket.Release()\n\t\t\tserver.Lock()\n\t\t\tif server.closed {\n\t\t\t\tloop = false\n\t\t\t}\n\t\t\tserver.pingValue = max\n\t\t\tserver.Unlock()\n\t\t\tlogf(\"Ping for %s is %d ms\", server.Addr, max/time.Millisecond)\n\t\t} else if err == errServerClosed {\n\t\t\treturn\n\t\t}\n\t\tif !loop {\n\t\t\treturn\n\t\t}\n\t}\n}\n\ntype mongoServerSlice []*mongoServer\n\nfunc (s mongoServerSlice) Len() int {\n\treturn len(s)\n}\n\nfunc (s mongoServerSlice) Less(i, j int) bool {\n\treturn s[i].ResolvedAddr < s[j].ResolvedAddr\n}\n\nfunc (s mongoServerSlice) Swap(i, j int) {\n\ts[i], s[j] = s[j], s[i]\n}\n\nfunc (s mongoServerSlice) Sort() {\n\tsort.Sort(s)\n}\n\nfunc (s mongoServerSlice) Search(resolvedAddr string) (i int, ok bool) {\n\tn := len(s)\n\ti = sort.Search(n, func(i int) bool {\n\t\treturn s[i].ResolvedAddr >= resolvedAddr\n\t})\n\treturn i, i != n && s[i].ResolvedAddr == resolvedAddr\n}\n\ntype mongoServers struct {\n\tslice mongoServerSlice\n}\n\nfunc (servers *mongoServers) Search(resolvedAddr string) (server *mongoServer) {\n\tif i, ok := servers.slice.Search(resolvedAddr); ok {\n\t\treturn servers.slice[i]\n\t}\n\treturn nil\n}\n\nfunc (servers *mongoServers) Add(server *mongoServer) {\n\tservers.slice = append(servers.slice, server)\n\tservers.slice.Sort()\n}\n\nfunc (servers *mongoServers) Remove(other *mongoServer) (server *mongoServer) {\n\tif i, found := servers.slice.Search(other.ResolvedAddr); found {\n\t\tserver = servers.slice[i]\n\t\tcopy(servers.slice[i:], servers.slice[i+1:])\n\t\tn := len(servers.slice) - 1\n\t\tservers.slice[n] = nil // Help GC.\n\t\tservers.slice = servers.slice[:n]\n\t}\n\treturn\n}\n\nfunc (servers *mongoServers) Slice() []*mongoServer {\n\treturn ([]*mongoServer)(servers.slice)\n}\n\nfunc (servers *mongoServers) Get(i int) *mongoServer {\n\treturn servers.slice[i]\n}\n\nfunc (servers *mongoServers) Len() int {\n\treturn len(servers.slice)\n}\n\nfunc (servers *mongoServers) Empty() bool {\n\treturn len(servers.slice) == 0\n}\n\nfunc (servers *mongoServers) HasMongos() bool {\n\tfor _, s := range servers.slice {\n\t\tif s.Info().Mongos {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// BestFit returns the best guess of what would be the most interesting\n// server to perform operations on at this point in time.\nfunc (servers *mongoServers) BestFit(mode Mode, serverTags []bson.D) *mongoServer {\n\tvar best *mongoServer\n\tfor _, next := range servers.slice {\n\t\tif best == nil {\n\t\t\tbest = next\n\t\t\tbest.RLock()\n\t\t\tif serverTags != nil && !next.info.Mongos && !best.hasTags(serverTags) {\n\t\t\t\tbest.RUnlock()\n\t\t\t\tbest = nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tnext.RLock()\n\t\tswap := false\n\t\tswitch {\n\t\tcase serverTags != nil && !next.info.Mongos && !next.hasTags(serverTags):\n\t\t\t// Must have requested tags.\n\t\tcase mode == Secondary && next.info.Master && !next.info.Mongos:\n\t\t\t// Must be a secondary or mongos.\n\t\tcase next.info.Master != best.info.Master && mode != Nearest:\n\t\t\t// Prefer slaves, unless the mode is PrimaryPreferred.\n\t\t\tswap = (mode == PrimaryPreferred) != best.info.Master\n\t\tcase absDuration(next.pingValue-best.pingValue) > 15*time.Millisecond:\n\t\t\t// Prefer nearest server.\n\t\t\tswap = next.pingValue < best.pingValue\n\t\tcase len(next.liveSockets)-len(next.unusedSockets) < len(best.liveSockets)-len(best.unusedSockets):\n\t\t\t// Prefer servers with less connections.\n\t\t\tswap = true\n\t\t}\n\t\tif swap {\n\t\t\tbest.RUnlock()\n\t\t\tbest = next\n\t\t} else {\n\t\t\tnext.RUnlock()\n\t\t}\n\t}\n\tif best != nil {\n\t\tbest.RUnlock()\n\t}\n\treturn best\n}\n\nfunc absDuration(d time.Duration) time.Duration {\n\tif d < 0 {\n\t\treturn -d\n\t}\n\treturn d\n}\n"
        },
        {
          "name": "session.go",
          "type": "blob",
          "size": 145.302734375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/url\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\ntype Mode int\n\nconst (\n\t// Relevant documentation on read preference modes:\n\t//\n\t//     http://docs.mongodb.org/manual/reference/read-preference/\n\t//\n\tPrimary            Mode = 2 // Default mode. All operations read from the current replica set primary.\n\tPrimaryPreferred   Mode = 3 // Read from the primary if available. Read from the secondary otherwise.\n\tSecondary          Mode = 4 // Read from one of the nearest secondary members of the replica set.\n\tSecondaryPreferred Mode = 5 // Read from one of the nearest secondaries if available. Read from primary otherwise.\n\tNearest            Mode = 6 // Read from one of the nearest members, irrespective of it being primary or secondary.\n\n\t// Read preference modes are specific to mgo:\n\tEventual  Mode = 0 // Same as Nearest, but may change servers between reads.\n\tMonotonic Mode = 1 // Same as SecondaryPreferred before first write. Same as Primary after first write.\n\tStrong    Mode = 2 // Same as Primary.\n)\n\n// mgo.v3: Drop Strong mode, suffix all modes with \"Mode\".\n\n// When changing the Session type, check if newSession and copySession\n// need to be updated too.\n\n// Session represents a communication session with the database.\n//\n// All Session methods are concurrency-safe and may be called from multiple\n// goroutines. In all session modes but Eventual, using the session from\n// multiple goroutines will cause them to share the same underlying socket.\n// See the documentation on Session.SetMode for more details.\ntype Session struct {\n\tm                sync.RWMutex\n\tcluster_         *mongoCluster\n\tslaveSocket      *mongoSocket\n\tmasterSocket     *mongoSocket\n\tslaveOk          bool\n\tconsistency      Mode\n\tqueryConfig      query\n\tsafeOp           *queryOp\n\tsyncTimeout      time.Duration\n\tsockTimeout      time.Duration\n\tdefaultdb        string\n\tsourcedb         string\n\tdialCred         *Credential\n\tcreds            []Credential\n\tpoolLimit        int\n\tbypassValidation bool\n}\n\ntype Database struct {\n\tSession *Session\n\tName    string\n}\n\ntype Collection struct {\n\tDatabase *Database\n\tName     string // \"collection\"\n\tFullName string // \"db.collection\"\n}\n\ntype Query struct {\n\tm       sync.Mutex\n\tsession *Session\n\tquery   // Enables default settings in session.\n}\n\ntype query struct {\n\top       queryOp\n\tprefetch float64\n\tlimit    int32\n}\n\ntype getLastError struct {\n\tCmdName  int         \"getLastError,omitempty\"\n\tW        interface{} \"w,omitempty\"\n\tWTimeout int         \"wtimeout,omitempty\"\n\tFSync    bool        \"fsync,omitempty\"\n\tJ        bool        \"j,omitempty\"\n}\n\ntype Iter struct {\n\tm              sync.Mutex\n\tgotReply       sync.Cond\n\tsession        *Session\n\tserver         *mongoServer\n\tdocData        queue\n\terr            error\n\top             getMoreOp\n\tprefetch       float64\n\tlimit          int32\n\tdocsToReceive  int\n\tdocsBeforeMore int\n\ttimeout        time.Duration\n\ttimedout       bool\n\tfindCmd        bool\n}\n\nvar (\n\tErrNotFound = errors.New(\"not found\")\n\tErrCursor   = errors.New(\"invalid cursor\")\n)\n\nconst (\n\tdefaultPrefetch  = 0.25\n\tmaxUpsertRetries = 5\n)\n\n// Dial establishes a new session to the cluster identified by the given seed\n// server(s). The session will enable communication with all of the servers in\n// the cluster, so the seed servers are used only to find out about the cluster\n// topology.\n//\n// Dial will timeout after 10 seconds if a server isn't reached. The returned\n// session will timeout operations after one minute by default if servers\n// aren't available. To customize the timeout, see DialWithTimeout,\n// SetSyncTimeout, and SetSocketTimeout.\n//\n// This method is generally called just once for a given cluster.  Further\n// sessions to the same cluster are then established using the New or Copy\n// methods on the obtained session. This will make them share the underlying\n// cluster, and manage the pool of connections appropriately.\n//\n// Once the session is not useful anymore, Close must be called to release the\n// resources appropriately.\n//\n// The seed servers must be provided in the following format:\n//\n//     [mongodb://][user:pass@]host1[:port1][,host2[:port2],...][/database][?options]\n//\n// For example, it may be as simple as:\n//\n//     localhost\n//\n// Or more involved like:\n//\n//     mongodb://myuser:mypass@localhost:40001,otherhost:40001/mydb\n//\n// If the port number is not provided for a server, it defaults to 27017.\n//\n// The username and password provided in the URL will be used to authenticate\n// into the database named after the slash at the end of the host names, or\n// into the \"admin\" database if none is provided.  The authentication information\n// will persist in sessions obtained through the New method as well.\n//\n// The following connection options are supported after the question mark:\n//\n//     connect=direct\n//\n//         Disables the automatic replica set server discovery logic, and\n//         forces the use of servers provided only (even if secondaries).\n//         Note that to talk to a secondary the consistency requirements\n//         must be relaxed to Monotonic or Eventual via SetMode.\n//\n//\n//     connect=replicaSet\n//\n//  \t   Discover replica sets automatically. Default connection behavior.\n//\n//\n//     replicaSet=<setname>\n//\n//         If specified will prevent the obtained session from communicating\n//         with any server which is not part of a replica set with the given name.\n//         The default is to communicate with any server specified or discovered\n//         via the servers contacted.\n//\n//\n//     authSource=<db>\n//\n//         Informs the database used to establish credentials and privileges\n//         with a MongoDB server. Defaults to the database name provided via\n//         the URL path, and \"admin\" if that's unset.\n//\n//\n//     authMechanism=<mechanism>\n//\n//        Defines the protocol for credential negotiation. Defaults to \"MONGODB-CR\",\n//        which is the default username/password challenge-response mechanism.\n//\n//\n//     gssapiServiceName=<name>\n//\n//        Defines the service name to use when authenticating with the GSSAPI\n//        mechanism. Defaults to \"mongodb\".\n//\n//\n//     maxPoolSize=<limit>\n//\n//        Defines the per-server socket pool limit. Defaults to 4096.\n//        See Session.SetPoolLimit for details.\n//\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/reference/connection-string/\n//\nfunc Dial(url string) (*Session, error) {\n\tsession, err := DialWithTimeout(url, 10*time.Second)\n\tif err == nil {\n\t\tsession.SetSyncTimeout(1 * time.Minute)\n\t\tsession.SetSocketTimeout(1 * time.Minute)\n\t}\n\treturn session, err\n}\n\n// DialWithTimeout works like Dial, but uses timeout as the amount of time to\n// wait for a server to respond when first connecting and also on follow up\n// operations in the session. If timeout is zero, the call may block\n// forever waiting for a connection to be made.\n//\n// See SetSyncTimeout for customizing the timeout for the session.\nfunc DialWithTimeout(url string, timeout time.Duration) (*Session, error) {\n\tinfo, err := ParseURL(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tinfo.Timeout = timeout\n\treturn DialWithInfo(info)\n}\n\n// ParseURL parses a MongoDB URL as accepted by the Dial function and returns\n// a value suitable for providing into DialWithInfo.\n//\n// See Dial for more details on the format of url.\nfunc ParseURL(url string) (*DialInfo, error) {\n\tuinfo, err := extractURL(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdirect := false\n\tmechanism := \"\"\n\tservice := \"\"\n\tsource := \"\"\n\tsetName := \"\"\n\tpoolLimit := 0\n\tfor k, v := range uinfo.options {\n\t\tswitch k {\n\t\tcase \"authSource\":\n\t\t\tsource = v\n\t\tcase \"authMechanism\":\n\t\t\tmechanism = v\n\t\tcase \"gssapiServiceName\":\n\t\t\tservice = v\n\t\tcase \"replicaSet\":\n\t\t\tsetName = v\n\t\tcase \"maxPoolSize\":\n\t\t\tpoolLimit, err = strconv.Atoi(v)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.New(\"bad value for maxPoolSize: \" + v)\n\t\t\t}\n\t\tcase \"connect\":\n\t\t\tif v == \"direct\" {\n\t\t\t\tdirect = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif v == \"replicaSet\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfallthrough\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"unsupported connection URL option: \" + k + \"=\" + v)\n\t\t}\n\t}\n\tinfo := DialInfo{\n\t\tAddrs:          uinfo.addrs,\n\t\tDirect:         direct,\n\t\tDatabase:       uinfo.db,\n\t\tUsername:       uinfo.user,\n\t\tPassword:       uinfo.pass,\n\t\tMechanism:      mechanism,\n\t\tService:        service,\n\t\tSource:         source,\n\t\tPoolLimit:      poolLimit,\n\t\tReplicaSetName: setName,\n\t}\n\treturn &info, nil\n}\n\n// DialInfo holds options for establishing a session with a MongoDB cluster.\n// To use a URL, see the Dial function.\ntype DialInfo struct {\n\t// Addrs holds the addresses for the seed servers.\n\tAddrs []string\n\n\t// Direct informs whether to establish connections only with the\n\t// specified seed servers, or to obtain information for the whole\n\t// cluster and establish connections with further servers too.\n\tDirect bool\n\n\t// Timeout is the amount of time to wait for a server to respond when\n\t// first connecting and on follow up operations in the session. If\n\t// timeout is zero, the call may block forever waiting for a connection\n\t// to be established. Timeout does not affect logic in DialServer.\n\tTimeout time.Duration\n\n\t// FailFast will cause connection and query attempts to fail faster when\n\t// the server is unavailable, instead of retrying until the configured\n\t// timeout period. Note that an unavailable server may silently drop\n\t// packets instead of rejecting them, in which case it's impossible to\n\t// distinguish it from a slow server, so the timeout stays relevant.\n\tFailFast bool\n\n\t// Database is the default database name used when the Session.DB method\n\t// is called with an empty name, and is also used during the initial\n\t// authentication if Source is unset.\n\tDatabase string\n\n\t// ReplicaSetName, if specified, will prevent the obtained session from\n\t// communicating with any server which is not part of a replica set\n\t// with the given name. The default is to communicate with any server\n\t// specified or discovered via the servers contacted.\n\tReplicaSetName string\n\n\t// Source is the database used to establish credentials and privileges\n\t// with a MongoDB server. Defaults to the value of Database, if that is\n\t// set, or \"admin\" otherwise.\n\tSource string\n\n\t// Service defines the service name to use when authenticating with the GSSAPI\n\t// mechanism. Defaults to \"mongodb\".\n\tService string\n\n\t// ServiceHost defines which hostname to use when authenticating\n\t// with the GSSAPI mechanism. If not specified, defaults to the MongoDB\n\t// server's address.\n\tServiceHost string\n\n\t// Mechanism defines the protocol for credential negotiation.\n\t// Defaults to \"MONGODB-CR\".\n\tMechanism string\n\n\t// Username and Password inform the credentials for the initial authentication\n\t// done on the database defined by the Source field. See Session.Login.\n\tUsername string\n\tPassword string\n\n\t// PoolLimit defines the per-server socket pool limit. Defaults to 4096.\n\t// See Session.SetPoolLimit for details.\n\tPoolLimit int\n\n\t// DialServer optionally specifies the dial function for establishing\n\t// connections with the MongoDB servers.\n\tDialServer func(addr *ServerAddr) (net.Conn, error)\n\n\t// WARNING: This field is obsolete. See DialServer above.\n\tDial func(addr net.Addr) (net.Conn, error)\n}\n\n// mgo.v3: Drop DialInfo.Dial.\n\n// ServerAddr represents the address for establishing a connection to an\n// individual MongoDB server.\ntype ServerAddr struct {\n\tstr string\n\ttcp *net.TCPAddr\n}\n\n// String returns the address that was provided for the server before resolution.\nfunc (addr *ServerAddr) String() string {\n\treturn addr.str\n}\n\n// TCPAddr returns the resolved TCP address for the server.\nfunc (addr *ServerAddr) TCPAddr() *net.TCPAddr {\n\treturn addr.tcp\n}\n\n// DialWithInfo establishes a new session to the cluster identified by info.\nfunc DialWithInfo(info *DialInfo) (*Session, error) {\n\taddrs := make([]string, len(info.Addrs))\n\tfor i, addr := range info.Addrs {\n\t\tp := strings.LastIndexAny(addr, \"]:\")\n\t\tif p == -1 || addr[p] != ':' {\n\t\t\t// XXX This is untested. The test suite doesn't use the standard port.\n\t\t\taddr += \":27017\"\n\t\t}\n\t\taddrs[i] = addr\n\t}\n\tcluster := newCluster(addrs, info.Direct, info.FailFast, dialer{info.Dial, info.DialServer}, info.ReplicaSetName)\n\tsession := newSession(Eventual, cluster, info.Timeout)\n\tsession.defaultdb = info.Database\n\tif session.defaultdb == \"\" {\n\t\tsession.defaultdb = \"test\"\n\t}\n\tsession.sourcedb = info.Source\n\tif session.sourcedb == \"\" {\n\t\tsession.sourcedb = info.Database\n\t\tif session.sourcedb == \"\" {\n\t\t\tsession.sourcedb = \"admin\"\n\t\t}\n\t}\n\tif info.Username != \"\" {\n\t\tsource := session.sourcedb\n\t\tif info.Source == \"\" &&\n\t\t\t(info.Mechanism == \"GSSAPI\" || info.Mechanism == \"PLAIN\" || info.Mechanism == \"MONGODB-X509\") {\n\t\t\tsource = \"$external\"\n\t\t}\n\t\tsession.dialCred = &Credential{\n\t\t\tUsername:    info.Username,\n\t\t\tPassword:    info.Password,\n\t\t\tMechanism:   info.Mechanism,\n\t\t\tService:     info.Service,\n\t\t\tServiceHost: info.ServiceHost,\n\t\t\tSource:      source,\n\t\t}\n\t\tsession.creds = []Credential{*session.dialCred}\n\t}\n\tif info.PoolLimit > 0 {\n\t\tsession.poolLimit = info.PoolLimit\n\t}\n\tcluster.Release()\n\n\t// People get confused when we return a session that is not actually\n\t// established to any servers yet (e.g. what if url was wrong). So,\n\t// ping the server to ensure there's someone there, and abort if it\n\t// fails.\n\tif err := session.Ping(); err != nil {\n\t\tsession.Close()\n\t\treturn nil, err\n\t}\n\tsession.SetMode(Strong, true)\n\treturn session, nil\n}\n\nfunc isOptSep(c rune) bool {\n\treturn c == ';' || c == '&'\n}\n\ntype urlInfo struct {\n\taddrs   []string\n\tuser    string\n\tpass    string\n\tdb      string\n\toptions map[string]string\n}\n\nfunc extractURL(s string) (*urlInfo, error) {\n\tif strings.HasPrefix(s, \"mongodb://\") {\n\t\ts = s[10:]\n\t}\n\tinfo := &urlInfo{options: make(map[string]string)}\n\tif c := strings.Index(s, \"?\"); c != -1 {\n\t\tfor _, pair := range strings.FieldsFunc(s[c+1:], isOptSep) {\n\t\t\tl := strings.SplitN(pair, \"=\", 2)\n\t\t\tif len(l) != 2 || l[0] == \"\" || l[1] == \"\" {\n\t\t\t\treturn nil, errors.New(\"connection option must be key=value: \" + pair)\n\t\t\t}\n\t\t\tinfo.options[l[0]] = l[1]\n\t\t}\n\t\ts = s[:c]\n\t}\n\tif c := strings.Index(s, \"@\"); c != -1 {\n\t\tpair := strings.SplitN(s[:c], \":\", 2)\n\t\tif len(pair) > 2 || pair[0] == \"\" {\n\t\t\treturn nil, errors.New(\"credentials must be provided as user:pass@host\")\n\t\t}\n\t\tvar err error\n\t\tinfo.user, err = url.QueryUnescape(pair[0])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"cannot unescape username in URL: %q\", pair[0])\n\t\t}\n\t\tif len(pair) > 1 {\n\t\t\tinfo.pass, err = url.QueryUnescape(pair[1])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"cannot unescape password in URL\")\n\t\t\t}\n\t\t}\n\t\ts = s[c+1:]\n\t}\n\tif c := strings.Index(s, \"/\"); c != -1 {\n\t\tinfo.db = s[c+1:]\n\t\ts = s[:c]\n\t}\n\tinfo.addrs = strings.Split(s, \",\")\n\treturn info, nil\n}\n\nfunc newSession(consistency Mode, cluster *mongoCluster, timeout time.Duration) (session *Session) {\n\tcluster.Acquire()\n\tsession = &Session{\n\t\tcluster_:    cluster,\n\t\tsyncTimeout: timeout,\n\t\tsockTimeout: timeout,\n\t\tpoolLimit:   4096,\n\t}\n\tdebugf(\"New session %p on cluster %p\", session, cluster)\n\tsession.SetMode(consistency, true)\n\tsession.SetSafe(&Safe{})\n\tsession.queryConfig.prefetch = defaultPrefetch\n\treturn session\n}\n\nfunc copySession(session *Session, keepCreds bool) (s *Session) {\n\tcluster := session.cluster()\n\tcluster.Acquire()\n\tif session.masterSocket != nil {\n\t\tsession.masterSocket.Acquire()\n\t}\n\tif session.slaveSocket != nil {\n\t\tsession.slaveSocket.Acquire()\n\t}\n\tvar creds []Credential\n\tif keepCreds {\n\t\tcreds = make([]Credential, len(session.creds))\n\t\tcopy(creds, session.creds)\n\t} else if session.dialCred != nil {\n\t\tcreds = []Credential{*session.dialCred}\n\t}\n\tscopy := *session\n\tscopy.m = sync.RWMutex{}\n\tscopy.creds = creds\n\ts = &scopy\n\tdebugf(\"New session %p on cluster %p (copy from %p)\", s, cluster, session)\n\treturn s\n}\n\n// LiveServers returns a list of server addresses which are\n// currently known to be alive.\nfunc (s *Session) LiveServers() (addrs []string) {\n\ts.m.RLock()\n\taddrs = s.cluster().LiveServers()\n\ts.m.RUnlock()\n\treturn addrs\n}\n\n// DB returns a value representing the named database. If name\n// is empty, the database name provided in the dialed URL is\n// used instead. If that is also empty, \"test\" is used as a\n// fallback in a way equivalent to the mongo shell.\n//\n// Creating this value is a very lightweight operation, and\n// involves no network communication.\nfunc (s *Session) DB(name string) *Database {\n\tif name == \"\" {\n\t\tname = s.defaultdb\n\t}\n\treturn &Database{s, name}\n}\n\n// C returns a value representing the named collection.\n//\n// Creating this value is a very lightweight operation, and\n// involves no network communication.\nfunc (db *Database) C(name string) *Collection {\n\treturn &Collection{db, name, db.Name + \".\" + name}\n}\n\n// With returns a copy of db that uses session s.\nfunc (db *Database) With(s *Session) *Database {\n\tnewdb := *db\n\tnewdb.Session = s\n\treturn &newdb\n}\n\n// With returns a copy of c that uses session s.\nfunc (c *Collection) With(s *Session) *Collection {\n\tnewdb := *c.Database\n\tnewdb.Session = s\n\tnewc := *c\n\tnewc.Database = &newdb\n\treturn &newc\n}\n\n// GridFS returns a GridFS value representing collections in db that\n// follow the standard GridFS specification.\n// The provided prefix (sometimes known as root) will determine which\n// collections to use, and is usually set to \"fs\" when there is a\n// single GridFS in the database.\n//\n// See the GridFS Create, Open, and OpenId methods for more details.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/GridFS\n//     http://www.mongodb.org/display/DOCS/GridFS+Tools\n//     http://www.mongodb.org/display/DOCS/GridFS+Specification\n//\nfunc (db *Database) GridFS(prefix string) *GridFS {\n\treturn newGridFS(db, prefix)\n}\n\n// Run issues the provided command on the db database and unmarshals\n// its result in the respective argument. The cmd argument may be either\n// a string with the command name itself, in which case an empty document of\n// the form bson.M{cmd: 1} will be used, or it may be a full command document.\n//\n// Note that MongoDB considers the first marshalled key as the command\n// name, so when providing a command with options, it's important to\n// use an ordering-preserving document, such as a struct value or an\n// instance of bson.D.  For instance:\n//\n//     db.Run(bson.D{{\"create\", \"mycollection\"}, {\"size\", 1024}})\n//\n// For privilleged commands typically run on the \"admin\" database, see\n// the Run method in the Session type.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Commands\n//     http://www.mongodb.org/display/DOCS/List+of+Database+CommandSkips\n//\nfunc (db *Database) Run(cmd interface{}, result interface{}) error {\n\tsocket, err := db.Session.acquireSocket(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer socket.Release()\n\n\t// This is an optimized form of db.C(\"$cmd\").Find(cmd).One(result).\n\treturn db.run(socket, cmd, result)\n}\n\n// Credential holds details to authenticate with a MongoDB server.\ntype Credential struct {\n\t// Username and Password hold the basic details for authentication.\n\t// Password is optional with some authentication mechanisms.\n\tUsername string\n\tPassword string\n\n\t// Source is the database used to establish credentials and privileges\n\t// with a MongoDB server. Defaults to the default database provided\n\t// during dial, or \"admin\" if that was unset.\n\tSource string\n\n\t// Service defines the service name to use when authenticating with the GSSAPI\n\t// mechanism. Defaults to \"mongodb\".\n\tService string\n\n\t// ServiceHost defines which hostname to use when authenticating\n\t// with the GSSAPI mechanism. If not specified, defaults to the MongoDB\n\t// server's address.\n\tServiceHost string\n\n\t// Mechanism defines the protocol for credential negotiation.\n\t// Defaults to \"MONGODB-CR\".\n\tMechanism string\n}\n\n// Login authenticates with MongoDB using the provided credential.  The\n// authentication is valid for the whole session and will stay valid until\n// Logout is explicitly called for the same database, or the session is\n// closed.\nfunc (db *Database) Login(user, pass string) error {\n\treturn db.Session.Login(&Credential{Username: user, Password: pass, Source: db.Name})\n}\n\n// Login authenticates with MongoDB using the provided credential.  The\n// authentication is valid for the whole session and will stay valid until\n// Logout is explicitly called for the same database, or the session is\n// closed.\nfunc (s *Session) Login(cred *Credential) error {\n\tsocket, err := s.acquireSocket(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer socket.Release()\n\n\tcredCopy := *cred\n\tif cred.Source == \"\" {\n\t\tif cred.Mechanism == \"GSSAPI\" {\n\t\t\tcredCopy.Source = \"$external\"\n\t\t} else {\n\t\t\tcredCopy.Source = s.sourcedb\n\t\t}\n\t}\n\terr = socket.Login(credCopy)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ts.m.Lock()\n\ts.creds = append(s.creds, credCopy)\n\ts.m.Unlock()\n\treturn nil\n}\n\nfunc (s *Session) socketLogin(socket *mongoSocket) error {\n\tfor _, cred := range s.creds {\n\t\tif err := socket.Login(cred); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Logout removes any established authentication credentials for the database.\nfunc (db *Database) Logout() {\n\tsession := db.Session\n\tdbname := db.Name\n\tsession.m.Lock()\n\tfound := false\n\tfor i, cred := range session.creds {\n\t\tif cred.Source == dbname {\n\t\t\tcopy(session.creds[i:], session.creds[i+1:])\n\t\t\tsession.creds = session.creds[:len(session.creds)-1]\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif found {\n\t\tif session.masterSocket != nil {\n\t\t\tsession.masterSocket.Logout(dbname)\n\t\t}\n\t\tif session.slaveSocket != nil {\n\t\t\tsession.slaveSocket.Logout(dbname)\n\t\t}\n\t}\n\tsession.m.Unlock()\n}\n\n// LogoutAll removes all established authentication credentials for the session.\nfunc (s *Session) LogoutAll() {\n\ts.m.Lock()\n\tfor _, cred := range s.creds {\n\t\tif s.masterSocket != nil {\n\t\t\ts.masterSocket.Logout(cred.Source)\n\t\t}\n\t\tif s.slaveSocket != nil {\n\t\t\ts.slaveSocket.Logout(cred.Source)\n\t\t}\n\t}\n\ts.creds = s.creds[0:0]\n\ts.m.Unlock()\n}\n\n// User represents a MongoDB user.\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/reference/privilege-documents/\n//     http://docs.mongodb.org/manual/reference/user-privileges/\n//\ntype User struct {\n\t// Username is how the user identifies itself to the system.\n\tUsername string `bson:\"user\"`\n\n\t// Password is the plaintext password for the user. If set,\n\t// the UpsertUser method will hash it into PasswordHash and\n\t// unset it before the user is added to the database.\n\tPassword string `bson:\",omitempty\"`\n\n\t// PasswordHash is the MD5 hash of Username+\":mongo:\"+Password.\n\tPasswordHash string `bson:\"pwd,omitempty\"`\n\n\t// CustomData holds arbitrary data admins decide to associate\n\t// with this user, such as the full name or employee id.\n\tCustomData interface{} `bson:\"customData,omitempty\"`\n\n\t// Roles indicates the set of roles the user will be provided.\n\t// See the Role constants.\n\tRoles []Role `bson:\"roles\"`\n\n\t// OtherDBRoles allows assigning roles in other databases from\n\t// user documents inserted in the admin database. This field\n\t// only works in the admin database.\n\tOtherDBRoles map[string][]Role `bson:\"otherDBRoles,omitempty\"`\n\n\t// UserSource indicates where to look for this user's credentials.\n\t// It may be set to a database name, or to \"$external\" for\n\t// consulting an external resource such as Kerberos. UserSource\n\t// must not be set if Password or PasswordHash are present.\n\t//\n\t// WARNING: This setting was only ever supported in MongoDB 2.4,\n\t// and is now obsolete.\n\tUserSource string `bson:\"userSource,omitempty\"`\n}\n\ntype Role string\n\nconst (\n\t// Relevant documentation:\n\t//\n\t//     http://docs.mongodb.org/manual/reference/user-privileges/\n\t//\n\tRoleRoot         Role = \"root\"\n\tRoleRead         Role = \"read\"\n\tRoleReadAny      Role = \"readAnyDatabase\"\n\tRoleReadWrite    Role = \"readWrite\"\n\tRoleReadWriteAny Role = \"readWriteAnyDatabase\"\n\tRoleDBAdmin      Role = \"dbAdmin\"\n\tRoleDBAdminAny   Role = \"dbAdminAnyDatabase\"\n\tRoleUserAdmin    Role = \"userAdmin\"\n\tRoleUserAdminAny Role = \"userAdminAnyDatabase\"\n\tRoleClusterAdmin Role = \"clusterAdmin\"\n)\n\n// UpsertUser updates the authentication credentials and the roles for\n// a MongoDB user within the db database. If the named user doesn't exist\n// it will be created.\n//\n// This method should only be used from MongoDB 2.4 and on. For older\n// MongoDB releases, use the obsolete AddUser method instead.\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/reference/user-privileges/\n//     http://docs.mongodb.org/manual/reference/privilege-documents/\n//\nfunc (db *Database) UpsertUser(user *User) error {\n\tif user.Username == \"\" {\n\t\treturn fmt.Errorf(\"user has no Username\")\n\t}\n\tif (user.Password != \"\" || user.PasswordHash != \"\") && user.UserSource != \"\" {\n\t\treturn fmt.Errorf(\"user has both Password/PasswordHash and UserSource set\")\n\t}\n\tif len(user.OtherDBRoles) > 0 && db.Name != \"admin\" && db.Name != \"$external\" {\n\t\treturn fmt.Errorf(\"user with OtherDBRoles is only supported in the admin or $external databases\")\n\t}\n\n\t// Attempt to run this using 2.6+ commands.\n\trundb := db\n\tif user.UserSource != \"\" {\n\t\t// Compatibility logic for the userSource field of MongoDB <= 2.4.X\n\t\trundb = db.Session.DB(user.UserSource)\n\t}\n\terr := rundb.runUserCmd(\"updateUser\", user)\n\t// retry with createUser when isAuthError in order to enable the \"localhost exception\"\n\tif isNotFound(err) || isAuthError(err) {\n\t\treturn rundb.runUserCmd(\"createUser\", user)\n\t}\n\tif !isNoCmd(err) {\n\t\treturn err\n\t}\n\n\t// Command does not exist. Fallback to pre-2.6 behavior.\n\tvar set, unset bson.D\n\tif user.Password != \"\" {\n\t\tpsum := md5.New()\n\t\tpsum.Write([]byte(user.Username + \":mongo:\" + user.Password))\n\t\tset = append(set, bson.DocElem{\"pwd\", hex.EncodeToString(psum.Sum(nil))})\n\t\tunset = append(unset, bson.DocElem{\"userSource\", 1})\n\t} else if user.PasswordHash != \"\" {\n\t\tset = append(set, bson.DocElem{\"pwd\", user.PasswordHash})\n\t\tunset = append(unset, bson.DocElem{\"userSource\", 1})\n\t}\n\tif user.UserSource != \"\" {\n\t\tset = append(set, bson.DocElem{\"userSource\", user.UserSource})\n\t\tunset = append(unset, bson.DocElem{\"pwd\", 1})\n\t}\n\tif user.Roles != nil || user.OtherDBRoles != nil {\n\t\tset = append(set, bson.DocElem{\"roles\", user.Roles})\n\t\tif len(user.OtherDBRoles) > 0 {\n\t\t\tset = append(set, bson.DocElem{\"otherDBRoles\", user.OtherDBRoles})\n\t\t} else {\n\t\t\tunset = append(unset, bson.DocElem{\"otherDBRoles\", 1})\n\t\t}\n\t}\n\tusers := db.C(\"system.users\")\n\terr = users.Update(bson.D{{\"user\", user.Username}}, bson.D{{\"$unset\", unset}, {\"$set\", set}})\n\tif err == ErrNotFound {\n\t\tset = append(set, bson.DocElem{\"user\", user.Username})\n\t\tif user.Roles == nil && user.OtherDBRoles == nil {\n\t\t\t// Roles must be sent, as it's the way MongoDB distinguishes\n\t\t\t// old-style documents from new-style documents in pre-2.6.\n\t\t\tset = append(set, bson.DocElem{\"roles\", user.Roles})\n\t\t}\n\t\terr = users.Insert(set)\n\t}\n\treturn err\n}\n\nfunc isNoCmd(err error) bool {\n\te, ok := err.(*QueryError)\n\treturn ok && (e.Code == 59 || e.Code == 13390 || strings.HasPrefix(e.Message, \"no such cmd:\"))\n}\n\nfunc isNotFound(err error) bool {\n\te, ok := err.(*QueryError)\n\treturn ok && e.Code == 11\n}\n\nfunc isAuthError(err error) bool {\n\te, ok := err.(*QueryError)\n\treturn ok && e.Code == 13\n}\n\nfunc (db *Database) runUserCmd(cmdName string, user *User) error {\n\tcmd := make(bson.D, 0, 16)\n\tcmd = append(cmd, bson.DocElem{cmdName, user.Username})\n\tif user.Password != \"\" {\n\t\tcmd = append(cmd, bson.DocElem{\"pwd\", user.Password})\n\t}\n\tvar roles []interface{}\n\tfor _, role := range user.Roles {\n\t\troles = append(roles, role)\n\t}\n\tfor db, dbroles := range user.OtherDBRoles {\n\t\tfor _, role := range dbroles {\n\t\t\troles = append(roles, bson.D{{\"role\", role}, {\"db\", db}})\n\t\t}\n\t}\n\tif roles != nil || user.Roles != nil || cmdName == \"createUser\" {\n\t\tcmd = append(cmd, bson.DocElem{\"roles\", roles})\n\t}\n\terr := db.Run(cmd, nil)\n\tif !isNoCmd(err) && user.UserSource != \"\" && (user.UserSource != \"$external\" || db.Name != \"$external\") {\n\t\treturn fmt.Errorf(\"MongoDB 2.6+ does not support the UserSource setting\")\n\t}\n\treturn err\n}\n\n// AddUser creates or updates the authentication credentials of user within\n// the db database.\n//\n// WARNING: This method is obsolete and should only be used with MongoDB 2.2\n// or earlier. For MongoDB 2.4 and on, use UpsertUser instead.\nfunc (db *Database) AddUser(username, password string, readOnly bool) error {\n\t// Try to emulate the old behavior on 2.6+\n\tuser := &User{Username: username, Password: password}\n\tif db.Name == \"admin\" {\n\t\tif readOnly {\n\t\t\tuser.Roles = []Role{RoleReadAny}\n\t\t} else {\n\t\t\tuser.Roles = []Role{RoleReadWriteAny}\n\t\t}\n\t} else {\n\t\tif readOnly {\n\t\t\tuser.Roles = []Role{RoleRead}\n\t\t} else {\n\t\t\tuser.Roles = []Role{RoleReadWrite}\n\t\t}\n\t}\n\terr := db.runUserCmd(\"updateUser\", user)\n\tif isNotFound(err) {\n\t\treturn db.runUserCmd(\"createUser\", user)\n\t}\n\tif !isNoCmd(err) {\n\t\treturn err\n\t}\n\n\t// Command doesn't exist. Fallback to pre-2.6 behavior.\n\tpsum := md5.New()\n\tpsum.Write([]byte(username + \":mongo:\" + password))\n\tdigest := hex.EncodeToString(psum.Sum(nil))\n\tc := db.C(\"system.users\")\n\t_, err = c.Upsert(bson.M{\"user\": username}, bson.M{\"$set\": bson.M{\"user\": username, \"pwd\": digest, \"readOnly\": readOnly}})\n\treturn err\n}\n\n// RemoveUser removes the authentication credentials of user from the database.\nfunc (db *Database) RemoveUser(user string) error {\n\terr := db.Run(bson.D{{\"dropUser\", user}}, nil)\n\tif isNoCmd(err) {\n\t\tusers := db.C(\"system.users\")\n\t\treturn users.Remove(bson.M{\"user\": user})\n\t}\n\tif isNotFound(err) {\n\t\treturn ErrNotFound\n\t}\n\treturn err\n}\n\ntype indexSpec struct {\n\tName, NS         string\n\tKey              bson.D\n\tUnique           bool    \",omitempty\"\n\tDropDups         bool    \"dropDups,omitempty\"\n\tBackground       bool    \",omitempty\"\n\tSparse           bool    \",omitempty\"\n\tBits             int     \",omitempty\"\n\tMin, Max         float64 \",omitempty\"\n\tBucketSize       float64 \"bucketSize,omitempty\"\n\tExpireAfter      int     \"expireAfterSeconds,omitempty\"\n\tWeights          bson.D  \",omitempty\"\n\tDefaultLanguage  string  \"default_language,omitempty\"\n\tLanguageOverride string  \"language_override,omitempty\"\n\tTextIndexVersion int     \"textIndexVersion,omitempty\"\n\n\tCollation *Collation \"collation,omitempty\"\n}\n\ntype Index struct {\n\tKey        []string // Index key fields; prefix name with dash (-) for descending order\n\tUnique     bool     // Prevent two documents from having the same index key\n\tDropDups   bool     // Drop documents with the same index key as a previously indexed one\n\tBackground bool     // Build index in background and return immediately\n\tSparse     bool     // Only index documents containing the Key fields\n\n\t// If ExpireAfter is defined the server will periodically delete\n\t// documents with indexed time.Time older than the provided delta.\n\tExpireAfter time.Duration\n\n\t// Name holds the stored index name. On creation if this field is unset it is\n\t// computed by EnsureIndex based on the index key.\n\tName string\n\n\t// Properties for spatial indexes.\n\t//\n\t// Min and Max were improperly typed as int when they should have been\n\t// floats.  To preserve backwards compatibility they are still typed as\n\t// int and the following two fields enable reading and writing the same\n\t// fields as float numbers. In mgo.v3, these fields will be dropped and\n\t// Min/Max will become floats.\n\tMin, Max   int\n\tMinf, Maxf float64\n\tBucketSize float64\n\tBits       int\n\n\t// Properties for text indexes.\n\tDefaultLanguage  string\n\tLanguageOverride string\n\n\t// Weights defines the significance of provided fields relative to other\n\t// fields in a text index. The score for a given word in a document is derived\n\t// from the weighted sum of the frequency for each of the indexed fields in\n\t// that document. The default field weight is 1.\n\tWeights map[string]int\n\n\t// Collation defines the collation to use for the index.\n\tCollation *Collation\n}\n\ntype Collation struct {\n\n\t// Locale defines the collation locale.\n\tLocale string `bson:\"locale\"`\n\n\t// CaseLevel defines whether to turn case sensitivity on at strength 1 or 2.\n\tCaseLevel bool `bson:\"caseLevel,omitempty\"`\n\n\t// CaseFirst may be set to \"upper\" or \"lower\" to define whether\n\t// to have uppercase or lowercase items first. Default is \"off\".\n\tCaseFirst string `bson:\"caseFirst,omitempty\"`\n\n\t// Strength defines the priority of comparison properties, as follows:\n\t//\n\t//   1 (primary)    - Strongest level, denote difference between base characters\n\t//   2 (secondary)  - Accents in characters are considered secondary differences\n\t//   3 (tertiary)   - Upper and lower case differences in characters are\n\t//                    distinguished at the tertiary level\n\t//   4 (quaternary) - When punctuation is ignored at level 1-3, an additional\n\t//                    level can be used to distinguish words with and without\n\t//                    punctuation. Should only be used if ignoring punctuation\n\t//                    is required or when processing Japanese text.\n\t//   5 (identical)  - When all other levels are equal, the identical level is\n\t//                    used as a tiebreaker. The Unicode code point values of\n\t//                    the NFD form of each string are compared at this level,\n\t//                    just in case there is no difference at levels 1-4\n\t//\n\t// Strength defaults to 3.\n\tStrength int `bson:\"strength,omitempty\"`\n\n\t// NumericOrdering defines whether to order numbers based on numerical\n\t// order and not collation order.\n\tNumericOrdering bool `bson:\"numericOrdering,omitempty\"`\n\n\t// Alternate controls whether spaces and punctuation are considered base characters.\n\t// May be set to \"non-ignorable\" (spaces and punctuation considered base characters)\n\t// or \"shifted\" (spaces and punctuation not considered base characters, and only\n\t// distinguished at strength > 3). Defaults to \"non-ignorable\".\n\tAlternate string `bson:\"alternate,omitempty\"`\n\n\t// Backwards defines whether to have secondary differences considered in reverse order,\n\t// as done in the French language.\n\tBackwards bool `bson:\"backwards,omitempty\"`\n}\n\n// mgo.v3: Drop Minf and Maxf and transform Min and Max to floats.\n// mgo.v3: Drop DropDups as it's unsupported past 2.8.\n\ntype indexKeyInfo struct {\n\tname    string\n\tkey     bson.D\n\tweights bson.D\n}\n\nfunc parseIndexKey(key []string) (*indexKeyInfo, error) {\n\tvar keyInfo indexKeyInfo\n\tisText := false\n\tvar order interface{}\n\tfor _, field := range key {\n\t\traw := field\n\t\tif keyInfo.name != \"\" {\n\t\t\tkeyInfo.name += \"_\"\n\t\t}\n\t\tvar kind string\n\t\tif field != \"\" {\n\t\t\tif field[0] == '$' {\n\t\t\t\tif c := strings.Index(field, \":\"); c > 1 && c < len(field)-1 {\n\t\t\t\t\tkind = field[1:c]\n\t\t\t\t\tfield = field[c+1:]\n\t\t\t\t\tkeyInfo.name += field + \"_\" + kind\n\t\t\t\t} else {\n\t\t\t\t\tfield = \"\\x00\"\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch field[0] {\n\t\t\tcase 0:\n\t\t\t\t// Logic above failed. Reset and error.\n\t\t\t\tfield = \"\"\n\t\t\tcase '@':\n\t\t\t\torder = \"2d\"\n\t\t\t\tfield = field[1:]\n\t\t\t\t// The shell used to render this field as key_ instead of key_2d,\n\t\t\t\t// and mgo followed suit. This has been fixed in recent server\n\t\t\t\t// releases, and mgo followed as well.\n\t\t\t\tkeyInfo.name += field + \"_2d\"\n\t\t\tcase '-':\n\t\t\t\torder = -1\n\t\t\t\tfield = field[1:]\n\t\t\t\tkeyInfo.name += field + \"_-1\"\n\t\t\tcase '+':\n\t\t\t\tfield = field[1:]\n\t\t\t\tfallthrough\n\t\t\tdefault:\n\t\t\t\tif kind == \"\" {\n\t\t\t\t\torder = 1\n\t\t\t\t\tkeyInfo.name += field + \"_1\"\n\t\t\t\t} else {\n\t\t\t\t\torder = kind\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif field == \"\" || kind != \"\" && order != kind {\n\t\t\treturn nil, fmt.Errorf(`invalid index key: want \"[$<kind>:][-]<field name>\", got %q`, raw)\n\t\t}\n\t\tif kind == \"text\" {\n\t\t\tif !isText {\n\t\t\t\tkeyInfo.key = append(keyInfo.key, bson.DocElem{\"_fts\", \"text\"}, bson.DocElem{\"_ftsx\", 1})\n\t\t\t\tisText = true\n\t\t\t}\n\t\t\tkeyInfo.weights = append(keyInfo.weights, bson.DocElem{field, 1})\n\t\t} else {\n\t\t\tkeyInfo.key = append(keyInfo.key, bson.DocElem{field, order})\n\t\t}\n\t}\n\tif keyInfo.name == \"\" {\n\t\treturn nil, errors.New(\"invalid index key: no fields provided\")\n\t}\n\treturn &keyInfo, nil\n}\n\n// EnsureIndexKey ensures an index with the given key exists, creating it\n// if necessary.\n//\n// This example:\n//\n//     err := collection.EnsureIndexKey(\"a\", \"b\")\n//\n// Is equivalent to:\n//\n//     err := collection.EnsureIndex(mgo.Index{Key: []string{\"a\", \"b\"}})\n//\n// See the EnsureIndex method for more details.\nfunc (c *Collection) EnsureIndexKey(key ...string) error {\n\treturn c.EnsureIndex(Index{Key: key})\n}\n\n// EnsureIndex ensures an index with the given key exists, creating it with\n// the provided parameters if necessary. EnsureIndex does not modify a previously\n// existent index with a matching key. The old index must be dropped first instead.\n//\n// Once EnsureIndex returns successfully, following requests for the same index\n// will not contact the server unless Collection.DropIndex is used to drop the\n// same index, or Session.ResetIndexCache is called.\n//\n// For example:\n//\n//     index := Index{\n//         Key: []string{\"lastname\", \"firstname\"},\n//         Unique: true,\n//         DropDups: true,\n//         Background: true, // See notes.\n//         Sparse: true,\n//     }\n//     err := collection.EnsureIndex(index)\n//\n// The Key value determines which fields compose the index. The index ordering\n// will be ascending by default.  To obtain an index with a descending order,\n// the field name should be prefixed by a dash (e.g. []string{\"-time\"}). It can\n// also be optionally prefixed by an index kind, as in \"$text:summary\" or\n// \"$2d:-point\". The key string format is:\n//\n//     [$<kind>:][-]<field name>\n//\n// If the Unique field is true, the index must necessarily contain only a single\n// document per Key.  With DropDups set to true, documents with the same key\n// as a previously indexed one will be dropped rather than an error returned.\n//\n// If Background is true, other connections will be allowed to proceed using\n// the collection without the index while it's being built. Note that the\n// session executing EnsureIndex will be blocked for as long as it takes for\n// the index to be built.\n//\n// If Sparse is true, only documents containing the provided Key fields will be\n// included in the index.  When using a sparse index for sorting, only indexed\n// documents will be returned.\n//\n// If ExpireAfter is non-zero, the server will periodically scan the collection\n// and remove documents containing an indexed time.Time field with a value\n// older than ExpireAfter. See the documentation for details:\n//\n//     http://docs.mongodb.org/manual/tutorial/expire-data\n//\n// Other kinds of indexes are also supported through that API. Here is an example:\n//\n//     index := Index{\n//         Key: []string{\"$2d:loc\"},\n//         Bits: 26,\n//     }\n//     err := collection.EnsureIndex(index)\n//\n// The example above requests the creation of a \"2d\" index for the \"loc\" field.\n//\n// The 2D index bounds may be changed using the Min and Max attributes of the\n// Index value.  The default bound setting of (-180, 180) is suitable for\n// latitude/longitude pairs.\n//\n// The Bits parameter sets the precision of the 2D geohash values.  If not\n// provided, 26 bits are used, which is roughly equivalent to 1 foot of\n// precision for the default (-180, 180) index bounds.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Indexes\n//     http://www.mongodb.org/display/DOCS/Indexing+Advice+and+FAQ\n//     http://www.mongodb.org/display/DOCS/Indexing+as+a+Background+Operation\n//     http://www.mongodb.org/display/DOCS/Geospatial+Indexing\n//     http://www.mongodb.org/display/DOCS/Multikeys\n//\nfunc (c *Collection) EnsureIndex(index Index) error {\n\tkeyInfo, err := parseIndexKey(index.Key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsession := c.Database.Session\n\tcacheKey := c.FullName + \"\\x00\" + keyInfo.name\n\tif session.cluster().HasCachedIndex(cacheKey) {\n\t\treturn nil\n\t}\n\n\tspec := indexSpec{\n\t\tName:             keyInfo.name,\n\t\tNS:               c.FullName,\n\t\tKey:              keyInfo.key,\n\t\tUnique:           index.Unique,\n\t\tDropDups:         index.DropDups,\n\t\tBackground:       index.Background,\n\t\tSparse:           index.Sparse,\n\t\tBits:             index.Bits,\n\t\tMin:              index.Minf,\n\t\tMax:              index.Maxf,\n\t\tBucketSize:       index.BucketSize,\n\t\tExpireAfter:      int(index.ExpireAfter / time.Second),\n\t\tWeights:          keyInfo.weights,\n\t\tDefaultLanguage:  index.DefaultLanguage,\n\t\tLanguageOverride: index.LanguageOverride,\n\t\tCollation:        index.Collation,\n\t}\n\n\tif spec.Min == 0 && spec.Max == 0 {\n\t\tspec.Min = float64(index.Min)\n\t\tspec.Max = float64(index.Max)\n\t}\n\n\tif index.Name != \"\" {\n\t\tspec.Name = index.Name\n\t}\n\nNextField:\n\tfor name, weight := range index.Weights {\n\t\tfor i, elem := range spec.Weights {\n\t\t\tif elem.Name == name {\n\t\t\t\tspec.Weights[i].Value = weight\n\t\t\t\tcontinue NextField\n\t\t\t}\n\t\t}\n\t\tpanic(\"weight provided for field that is not part of index key: \" + name)\n\t}\n\n\tcloned := session.Clone()\n\tdefer cloned.Close()\n\tcloned.SetMode(Strong, false)\n\tcloned.EnsureSafe(&Safe{})\n\tdb := c.Database.With(cloned)\n\n\t// Try with a command first.\n\terr = db.Run(bson.D{{\"createIndexes\", c.Name}, {\"indexes\", []indexSpec{spec}}}, nil)\n\tif isNoCmd(err) {\n\t\t// Command not yet supported. Insert into the indexes collection instead.\n\t\terr = db.C(\"system.indexes\").Insert(&spec)\n\t}\n\tif err == nil {\n\t\tsession.cluster().CacheIndex(cacheKey, true)\n\t}\n\treturn err\n}\n\n// DropIndex drops the index with the provided key from the c collection.\n//\n// See EnsureIndex for details on the accepted key variants.\n//\n// For example:\n//\n//     err1 := collection.DropIndex(\"firstField\", \"-secondField\")\n//     err2 := collection.DropIndex(\"customIndexName\")\n//\nfunc (c *Collection) DropIndex(key ...string) error {\n\tkeyInfo, err := parseIndexKey(key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsession := c.Database.Session\n\tcacheKey := c.FullName + \"\\x00\" + keyInfo.name\n\tsession.cluster().CacheIndex(cacheKey, false)\n\n\tsession = session.Clone()\n\tdefer session.Close()\n\tsession.SetMode(Strong, false)\n\n\tdb := c.Database.With(session)\n\tresult := struct {\n\t\tErrMsg string\n\t\tOk     bool\n\t}{}\n\terr = db.Run(bson.D{{\"dropIndexes\", c.Name}, {\"index\", keyInfo.name}}, &result)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !result.Ok {\n\t\treturn errors.New(result.ErrMsg)\n\t}\n\treturn nil\n}\n\n// DropIndexName removes the index with the provided index name.\n//\n// For example:\n//\n//     err := collection.DropIndex(\"customIndexName\")\n//\nfunc (c *Collection) DropIndexName(name string) error {\n\tsession := c.Database.Session\n\n\tsession = session.Clone()\n\tdefer session.Close()\n\tsession.SetMode(Strong, false)\n\n\tc = c.With(session)\n\n\tindexes, err := c.Indexes()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar index Index\n\tfor _, idx := range indexes {\n\t\tif idx.Name == name {\n\t\t\tindex = idx\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif index.Name != \"\" {\n\t\tkeyInfo, err := parseIndexKey(index.Key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tcacheKey := c.FullName + \"\\x00\" + keyInfo.name\n\t\tsession.cluster().CacheIndex(cacheKey, false)\n\t}\n\n\tresult := struct {\n\t\tErrMsg string\n\t\tOk     bool\n\t}{}\n\terr = c.Database.Run(bson.D{{\"dropIndexes\", c.Name}, {\"index\", name}}, &result)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !result.Ok {\n\t\treturn errors.New(result.ErrMsg)\n\t}\n\treturn nil\n}\n\n// nonEventual returns a clone of session and ensures it is not Eventual.\n// This guarantees that the server that is used for queries may be reused\n// afterwards when a cursor is received.\nfunc (session *Session) nonEventual() *Session {\n\tcloned := session.Clone()\n\tif cloned.consistency == Eventual {\n\t\tcloned.SetMode(Monotonic, false)\n\t}\n\treturn cloned\n}\n\n// Indexes returns a list of all indexes for the collection.\n//\n// For example, this snippet would drop all available indexes:\n//\n//   indexes, err := collection.Indexes()\n//   if err != nil {\n//       return err\n//   }\n//   for _, index := range indexes {\n//       err = collection.DropIndex(index.Key...)\n//       if err != nil {\n//           return err\n//       }\n//   }\n//\n// See the EnsureIndex method for more details on indexes.\nfunc (c *Collection) Indexes() (indexes []Index, err error) {\n\tcloned := c.Database.Session.nonEventual()\n\tdefer cloned.Close()\n\n\tbatchSize := int(cloned.queryConfig.op.limit)\n\n\t// Try with a command.\n\tvar result struct {\n\t\tIndexes []bson.Raw\n\t\tCursor  cursorData\n\t}\n\tvar iter *Iter\n\terr = c.Database.With(cloned).Run(bson.D{{\"listIndexes\", c.Name}, {\"cursor\", bson.D{{\"batchSize\", batchSize}}}}, &result)\n\tif err == nil {\n\t\tfirstBatch := result.Indexes\n\t\tif firstBatch == nil {\n\t\t\tfirstBatch = result.Cursor.FirstBatch\n\t\t}\n\t\tns := strings.SplitN(result.Cursor.NS, \".\", 2)\n\t\tif len(ns) < 2 {\n\t\t\titer = c.With(cloned).NewIter(nil, firstBatch, result.Cursor.Id, nil)\n\t\t} else {\n\t\t\titer = cloned.DB(ns[0]).C(ns[1]).NewIter(nil, firstBatch, result.Cursor.Id, nil)\n\t\t}\n\t} else if isNoCmd(err) {\n\t\t// Command not yet supported. Query the database instead.\n\t\titer = c.Database.C(\"system.indexes\").Find(bson.M{\"ns\": c.FullName}).Iter()\n\t} else {\n\t\treturn nil, err\n\t}\n\n\tvar spec indexSpec\n\tfor iter.Next(&spec) {\n\t\tindexes = append(indexes, indexFromSpec(spec))\n\t}\n\tif err = iter.Close(); err != nil {\n\t\treturn nil, err\n\t}\n\tsort.Sort(indexSlice(indexes))\n\treturn indexes, nil\n}\n\nfunc indexFromSpec(spec indexSpec) Index {\n\tindex := Index{\n\t\tName:             spec.Name,\n\t\tKey:              simpleIndexKey(spec.Key),\n\t\tUnique:           spec.Unique,\n\t\tDropDups:         spec.DropDups,\n\t\tBackground:       spec.Background,\n\t\tSparse:           spec.Sparse,\n\t\tMinf:             spec.Min,\n\t\tMaxf:             spec.Max,\n\t\tBits:             spec.Bits,\n\t\tBucketSize:       spec.BucketSize,\n\t\tDefaultLanguage:  spec.DefaultLanguage,\n\t\tLanguageOverride: spec.LanguageOverride,\n\t\tExpireAfter:      time.Duration(spec.ExpireAfter) * time.Second,\n\t\tCollation:        spec.Collation,\n\t}\n\tif float64(int(spec.Min)) == spec.Min && float64(int(spec.Max)) == spec.Max {\n\t\tindex.Min = int(spec.Min)\n\t\tindex.Max = int(spec.Max)\n\t}\n\tif spec.TextIndexVersion > 0 {\n\t\tindex.Key = make([]string, len(spec.Weights))\n\t\tindex.Weights = make(map[string]int)\n\t\tfor i, elem := range spec.Weights {\n\t\t\tindex.Key[i] = \"$text:\" + elem.Name\n\t\t\tif w, ok := elem.Value.(int); ok {\n\t\t\t\tindex.Weights[elem.Name] = w\n\t\t\t}\n\t\t}\n\t}\n\treturn index\n}\n\ntype indexSlice []Index\n\nfunc (idxs indexSlice) Len() int           { return len(idxs) }\nfunc (idxs indexSlice) Less(i, j int) bool { return idxs[i].Name < idxs[j].Name }\nfunc (idxs indexSlice) Swap(i, j int)      { idxs[i], idxs[j] = idxs[j], idxs[i] }\n\nfunc simpleIndexKey(realKey bson.D) (key []string) {\n\tfor i := range realKey {\n\t\tvar vi int\n\t\tfield := realKey[i].Name\n\n\t\tswitch realKey[i].Value.(type) {\n\t\tcase int64:\n\t\t\tvf, _ := realKey[i].Value.(int64)\n\t\t\tvi = int(vf)\n\t\tcase float64:\n\t\t\tvf, _ := realKey[i].Value.(float64)\n\t\t\tvi = int(vf)\n\t\tcase string:\n\t\t\tif vs, ok := realKey[i].Value.(string); ok {\n\t\t\t\tkey = append(key, \"$\"+vs+\":\"+field)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase int:\n\t\t\tvi = realKey[i].Value.(int)\n\t\t}\n\n\t\tif vi == 1 {\n\t\t\tkey = append(key, field)\n\t\t\tcontinue\n\t\t}\n\t\tif vi == -1 {\n\t\t\tkey = append(key, \"-\"+field)\n\t\t\tcontinue\n\t\t}\n\t\tpanic(\"Got unknown index key type for field \" + field)\n\t}\n\treturn\n}\n\n// ResetIndexCache() clears the cache of previously ensured indexes.\n// Following requests to EnsureIndex will contact the server.\nfunc (s *Session) ResetIndexCache() {\n\ts.cluster().ResetIndexCache()\n}\n\n// New creates a new session with the same parameters as the original\n// session, including consistency, batch size, prefetching, safety mode,\n// etc. The returned session will use sockets from the pool, so there's\n// a chance that writes just performed in another session may not yet\n// be visible.\n//\n// Login information from the original session will not be copied over\n// into the new session unless it was provided through the initial URL\n// for the Dial function.\n//\n// See the Copy and Clone methods.\n//\nfunc (s *Session) New() *Session {\n\ts.m.Lock()\n\tscopy := copySession(s, false)\n\ts.m.Unlock()\n\tscopy.Refresh()\n\treturn scopy\n}\n\n// Copy works just like New, but preserves the exact authentication\n// information from the original session.\nfunc (s *Session) Copy() *Session {\n\ts.m.Lock()\n\tscopy := copySession(s, true)\n\ts.m.Unlock()\n\tscopy.Refresh()\n\treturn scopy\n}\n\n// Clone works just like Copy, but also reuses the same socket as the original\n// session, in case it had already reserved one due to its consistency\n// guarantees.  This behavior ensures that writes performed in the old session\n// are necessarily observed when using the new session, as long as it was a\n// strong or monotonic session.  That said, it also means that long operations\n// may cause other goroutines using the original session to wait.\nfunc (s *Session) Clone() *Session {\n\ts.m.Lock()\n\tscopy := copySession(s, true)\n\ts.m.Unlock()\n\treturn scopy\n}\n\n// Close terminates the session.  It's a runtime error to use a session\n// after it has been closed.\nfunc (s *Session) Close() {\n\ts.m.Lock()\n\tif s.cluster_ != nil {\n\t\tdebugf(\"Closing session %p\", s)\n\t\ts.unsetSocket()\n\t\ts.cluster_.Release()\n\t\ts.cluster_ = nil\n\t}\n\ts.m.Unlock()\n}\n\nfunc (s *Session) cluster() *mongoCluster {\n\tif s.cluster_ == nil {\n\t\tpanic(\"Session already closed\")\n\t}\n\treturn s.cluster_\n}\n\n// Refresh puts back any reserved sockets in use and restarts the consistency\n// guarantees according to the current consistency setting for the session.\nfunc (s *Session) Refresh() {\n\ts.m.Lock()\n\ts.slaveOk = s.consistency != Strong\n\ts.unsetSocket()\n\ts.m.Unlock()\n}\n\n// SetMode changes the consistency mode for the session.\n//\n// The default mode is Strong.\n//\n// In the Strong consistency mode reads and writes will always be made to\n// the primary server using a unique connection so that reads and writes are\n// fully consistent, ordered, and observing the most up-to-date data.\n// This offers the least benefits in terms of distributing load, but the\n// most guarantees.  See also Monotonic and Eventual.\n//\n// In the Monotonic consistency mode reads may not be entirely up-to-date,\n// but they will always see the history of changes moving forward, the data\n// read will be consistent across sequential queries in the same session,\n// and modifications made within the session will be observed in following\n// queries (read-your-writes).\n//\n// In practice, the Monotonic mode is obtained by performing initial reads\n// on a unique connection to an arbitrary secondary, if one is available,\n// and once the first write happens, the session connection is switched over\n// to the primary server.  This manages to distribute some of the reading\n// load with secondaries, while maintaining some useful guarantees.\n//\n// In the Eventual consistency mode reads will be made to any secondary in the\n// cluster, if one is available, and sequential reads will not necessarily\n// be made with the same connection.  This means that data may be observed\n// out of order.  Writes will of course be issued to the primary, but\n// independent writes in the same Eventual session may also be made with\n// independent connections, so there are also no guarantees in terms of\n// write ordering (no read-your-writes guarantees either).\n//\n// The Eventual mode is the fastest and most resource-friendly, but is\n// also the one offering the least guarantees about ordering of the data\n// read and written.\n//\n// If refresh is true, in addition to ensuring the session is in the given\n// consistency mode, the consistency guarantees will also be reset (e.g.\n// a Monotonic session will be allowed to read from secondaries again).\n// This is equivalent to calling the Refresh function.\n//\n// Shifting between Monotonic and Strong modes will keep a previously\n// reserved connection for the session unless refresh is true or the\n// connection is unsuitable (to a secondary server in a Strong session).\nfunc (s *Session) SetMode(consistency Mode, refresh bool) {\n\ts.m.Lock()\n\tdebugf(\"Session %p: setting mode %d with refresh=%v (master=%p, slave=%p)\", s, consistency, refresh, s.masterSocket, s.slaveSocket)\n\ts.consistency = consistency\n\tif refresh {\n\t\ts.slaveOk = s.consistency != Strong\n\t\ts.unsetSocket()\n\t} else if s.consistency == Strong {\n\t\ts.slaveOk = false\n\t} else if s.masterSocket == nil {\n\t\ts.slaveOk = true\n\t}\n\ts.m.Unlock()\n}\n\n// Mode returns the current consistency mode for the session.\nfunc (s *Session) Mode() Mode {\n\ts.m.RLock()\n\tmode := s.consistency\n\ts.m.RUnlock()\n\treturn mode\n}\n\n// SetSyncTimeout sets the amount of time an operation with this session\n// will wait before returning an error in case a connection to a usable\n// server can't be established. Set it to zero to wait forever. The\n// default value is 7 seconds.\nfunc (s *Session) SetSyncTimeout(d time.Duration) {\n\ts.m.Lock()\n\ts.syncTimeout = d\n\ts.m.Unlock()\n}\n\n// SetSocketTimeout sets the amount of time to wait for a non-responding\n// socket to the database before it is forcefully closed.\n//\n// The default timeout is 1 minute.\nfunc (s *Session) SetSocketTimeout(d time.Duration) {\n\ts.m.Lock()\n\ts.sockTimeout = d\n\tif s.masterSocket != nil {\n\t\ts.masterSocket.SetTimeout(d)\n\t}\n\tif s.slaveSocket != nil {\n\t\ts.slaveSocket.SetTimeout(d)\n\t}\n\ts.m.Unlock()\n}\n\n// SetCursorTimeout changes the standard timeout period that the server\n// enforces on created cursors. The only supported value right now is\n// 0, which disables the timeout. The standard server timeout is 10 minutes.\nfunc (s *Session) SetCursorTimeout(d time.Duration) {\n\ts.m.Lock()\n\tif d == 0 {\n\t\ts.queryConfig.op.flags |= flagNoCursorTimeout\n\t} else {\n\t\tpanic(\"SetCursorTimeout: only 0 (disable timeout) supported for now\")\n\t}\n\ts.m.Unlock()\n}\n\n// SetPoolLimit sets the maximum number of sockets in use in a single server\n// before this session will block waiting for a socket to be available.\n// The default limit is 4096.\n//\n// This limit must be set to cover more than any expected workload of the\n// application. It is a bad practice and an unsupported use case to use the\n// database driver to define the concurrency limit of an application. Prevent\n// such concurrency \"at the door\" instead, by properly restricting the amount\n// of used resources and number of goroutines before they are created.\nfunc (s *Session) SetPoolLimit(limit int) {\n\ts.m.Lock()\n\ts.poolLimit = limit\n\ts.m.Unlock()\n}\n\n// SetBypassValidation sets whether the server should bypass the registered\n// validation expressions executed when documents are inserted or modified,\n// in the interest of preserving invariants in the collection being modified.\n// The default is to not bypass, and thus to perform the validation\n// expressions registered for modified collections.\n//\n// Document validation was introuced in MongoDB 3.2.\n//\n// Relevant documentation:\n//\n//   https://docs.mongodb.org/manual/release-notes/3.2/#bypass-validation\n//\nfunc (s *Session) SetBypassValidation(bypass bool) {\n\ts.m.Lock()\n\ts.bypassValidation = bypass\n\ts.m.Unlock()\n}\n\n// SetBatch sets the default batch size used when fetching documents from the\n// database. It's possible to change this setting on a per-query basis as\n// well, using the Query.Batch method.\n//\n// The default batch size is defined by the database itself.  As of this\n// writing, MongoDB will use an initial size of min(100 docs, 4MB) on the\n// first batch, and 4MB on remaining ones.\nfunc (s *Session) SetBatch(n int) {\n\tif n == 1 {\n\t\t// Server interprets 1 as -1 and closes the cursor (!?)\n\t\tn = 2\n\t}\n\ts.m.Lock()\n\ts.queryConfig.op.limit = int32(n)\n\ts.m.Unlock()\n}\n\n// SetPrefetch sets the default point at which the next batch of results will be\n// requested.  When there are p*batch_size remaining documents cached in an\n// Iter, the next batch will be requested in background. For instance, when\n// using this:\n//\n//     session.SetBatch(200)\n//     session.SetPrefetch(0.25)\n//\n// and there are only 50 documents cached in the Iter to be processed, the\n// next batch of 200 will be requested. It's possible to change this setting on\n// a per-query basis as well, using the Prefetch method of Query.\n//\n// The default prefetch value is 0.25.\nfunc (s *Session) SetPrefetch(p float64) {\n\ts.m.Lock()\n\ts.queryConfig.prefetch = p\n\ts.m.Unlock()\n}\n\n// See SetSafe for details on the Safe type.\ntype Safe struct {\n\tW        int    // Min # of servers to ack before success\n\tWMode    string // Write mode for MongoDB 2.0+ (e.g. \"majority\")\n\tWTimeout int    // Milliseconds to wait for W before timing out\n\tFSync    bool   // Sync via the journal if present, or via data files sync otherwise\n\tJ        bool   // Sync via the journal if present\n}\n\n// Safe returns the current safety mode for the session.\nfunc (s *Session) Safe() (safe *Safe) {\n\ts.m.Lock()\n\tdefer s.m.Unlock()\n\tif s.safeOp != nil {\n\t\tcmd := s.safeOp.query.(*getLastError)\n\t\tsafe = &Safe{WTimeout: cmd.WTimeout, FSync: cmd.FSync, J: cmd.J}\n\t\tswitch w := cmd.W.(type) {\n\t\tcase string:\n\t\t\tsafe.WMode = w\n\t\tcase int:\n\t\t\tsafe.W = w\n\t\t}\n\t}\n\treturn\n}\n\n// SetSafe changes the session safety mode.\n//\n// If the safe parameter is nil, the session is put in unsafe mode, and writes\n// become fire-and-forget, without error checking.  The unsafe mode is faster\n// since operations won't hold on waiting for a confirmation.\n//\n// If the safe parameter is not nil, any changing query (insert, update, ...)\n// will be followed by a getLastError command with the specified parameters,\n// to ensure the request was correctly processed.\n//\n// The default is &Safe{}, meaning check for errors and use the default\n// behavior for all fields.\n//\n// The safe.W parameter determines how many servers should confirm a write\n// before the operation is considered successful.  If set to 0 or 1, the\n// command will return as soon as the primary is done with the request.\n// If safe.WTimeout is greater than zero, it determines how many milliseconds\n// to wait for the safe.W servers to respond before returning an error.\n//\n// Starting with MongoDB 2.0.0 the safe.WMode parameter can be used instead\n// of W to request for richer semantics. If set to \"majority\" the server will\n// wait for a majority of members from the replica set to respond before\n// returning. Custom modes may also be defined within the server to create\n// very detailed placement schemas. See the data awareness documentation in\n// the links below for more details (note that MongoDB internally reuses the\n// \"w\" field name for WMode).\n//\n// If safe.J is true, servers will block until write operations have been\n// committed to the journal. Cannot be used in combination with FSync. Prior\n// to MongoDB 2.6 this option was ignored if the server was running without\n// journaling. Starting with MongoDB 2.6 write operations will fail with an\n// exception if this option is used when the server is running without\n// journaling.\n//\n// If safe.FSync is true and the server is running without journaling, blocks\n// until the server has synced all data files to disk. If the server is running\n// with journaling, this acts the same as the J option, blocking until write\n// operations have been committed to the journal. Cannot be used in\n// combination with J.\n//\n// Since MongoDB 2.0.0, the safe.J option can also be used instead of FSync\n// to force the server to wait for a group commit in case journaling is\n// enabled. The option has no effect if the server has journaling disabled.\n//\n// For example, the following statement will make the session check for\n// errors, without imposing further constraints:\n//\n//     session.SetSafe(&mgo.Safe{})\n//\n// The following statement will force the server to wait for a majority of\n// members of a replica set to return (MongoDB 2.0+ only):\n//\n//     session.SetSafe(&mgo.Safe{WMode: \"majority\"})\n//\n// The following statement, on the other hand, ensures that at least two\n// servers have flushed the change to disk before confirming the success\n// of operations:\n//\n//     session.EnsureSafe(&mgo.Safe{W: 2, FSync: true})\n//\n// The following statement, on the other hand, disables the verification\n// of errors entirely:\n//\n//     session.SetSafe(nil)\n//\n// See also the EnsureSafe method.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/getLastError+Command\n//     http://www.mongodb.org/display/DOCS/Verifying+Propagation+of+Writes+with+getLastError\n//     http://www.mongodb.org/display/DOCS/Data+Center+Awareness\n//\nfunc (s *Session) SetSafe(safe *Safe) {\n\ts.m.Lock()\n\ts.safeOp = nil\n\ts.ensureSafe(safe)\n\ts.m.Unlock()\n}\n\n// EnsureSafe compares the provided safety parameters with the ones\n// currently in use by the session and picks the most conservative\n// choice for each setting.\n//\n// That is:\n//\n//     - safe.WMode is always used if set.\n//     - safe.W is used if larger than the current W and WMode is empty.\n//     - safe.FSync is always used if true.\n//     - safe.J is used if FSync is false.\n//     - safe.WTimeout is used if set and smaller than the current WTimeout.\n//\n// For example, the following statement will ensure the session is\n// at least checking for errors, without enforcing further constraints.\n// If a more conservative SetSafe or EnsureSafe call was previously done,\n// the following call will be ignored.\n//\n//     session.EnsureSafe(&mgo.Safe{})\n//\n// See also the SetSafe method for details on what each option means.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/getLastError+Command\n//     http://www.mongodb.org/display/DOCS/Verifying+Propagation+of+Writes+with+getLastError\n//     http://www.mongodb.org/display/DOCS/Data+Center+Awareness\n//\nfunc (s *Session) EnsureSafe(safe *Safe) {\n\ts.m.Lock()\n\ts.ensureSafe(safe)\n\ts.m.Unlock()\n}\n\nfunc (s *Session) ensureSafe(safe *Safe) {\n\tif safe == nil {\n\t\treturn\n\t}\n\n\tvar w interface{}\n\tif safe.WMode != \"\" {\n\t\tw = safe.WMode\n\t} else if safe.W > 0 {\n\t\tw = safe.W\n\t}\n\n\tvar cmd getLastError\n\tif s.safeOp == nil {\n\t\tcmd = getLastError{1, w, safe.WTimeout, safe.FSync, safe.J}\n\t} else {\n\t\t// Copy.  We don't want to mutate the existing query.\n\t\tcmd = *(s.safeOp.query.(*getLastError))\n\t\tif cmd.W == nil {\n\t\t\tcmd.W = w\n\t\t} else if safe.WMode != \"\" {\n\t\t\tcmd.W = safe.WMode\n\t\t} else if i, ok := cmd.W.(int); ok && safe.W > i {\n\t\t\tcmd.W = safe.W\n\t\t}\n\t\tif safe.WTimeout > 0 && safe.WTimeout < cmd.WTimeout {\n\t\t\tcmd.WTimeout = safe.WTimeout\n\t\t}\n\t\tif safe.FSync {\n\t\t\tcmd.FSync = true\n\t\t\tcmd.J = false\n\t\t} else if safe.J && !cmd.FSync {\n\t\t\tcmd.J = true\n\t\t}\n\t}\n\ts.safeOp = &queryOp{\n\t\tquery:      &cmd,\n\t\tcollection: \"admin.$cmd\",\n\t\tlimit:      -1,\n\t}\n}\n\n// Run issues the provided command on the \"admin\" database and\n// and unmarshals its result in the respective argument. The cmd\n// argument may be either a string with the command name itself, in\n// which case an empty document of the form bson.M{cmd: 1} will be used,\n// or it may be a full command document.\n//\n// Note that MongoDB considers the first marshalled key as the command\n// name, so when providing a command with options, it's important to\n// use an ordering-preserving document, such as a struct value or an\n// instance of bson.D.  For instance:\n//\n//     db.Run(bson.D{{\"create\", \"mycollection\"}, {\"size\", 1024}})\n//\n// For commands on arbitrary databases, see the Run method in\n// the Database type.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Commands\n//     http://www.mongodb.org/display/DOCS/List+of+Database+CommandSkips\n//\nfunc (s *Session) Run(cmd interface{}, result interface{}) error {\n\treturn s.DB(\"admin\").Run(cmd, result)\n}\n\n// SelectServers restricts communication to servers configured with the\n// given tags. For example, the following statement restricts servers\n// used for reading operations to those with both tag \"disk\" set to\n// \"ssd\" and tag \"rack\" set to 1:\n//\n//     session.SelectServers(bson.D{{\"disk\", \"ssd\"}, {\"rack\", 1}})\n//\n// Multiple sets of tags may be provided, in which case the used server\n// must match all tags within any one set.\n//\n// If a connection was previously assigned to the session due to the\n// current session mode (see Session.SetMode), the tag selection will\n// only be enforced after the session is refreshed.\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/tutorial/configure-replica-set-tag-sets\n//\nfunc (s *Session) SelectServers(tags ...bson.D) {\n\ts.m.Lock()\n\ts.queryConfig.op.serverTags = tags\n\ts.m.Unlock()\n}\n\n// Ping runs a trivial ping command just to get in touch with the server.\nfunc (s *Session) Ping() error {\n\treturn s.Run(\"ping\", nil)\n}\n\n// Fsync flushes in-memory writes to disk on the server the session\n// is established with. If async is true, the call returns immediately,\n// otherwise it returns after the flush has been made.\nfunc (s *Session) Fsync(async bool) error {\n\treturn s.Run(bson.D{{\"fsync\", 1}, {\"async\", async}}, nil)\n}\n\n// FsyncLock locks all writes in the specific server the session is\n// established with and returns. Any writes attempted to the server\n// after it is successfully locked will block until FsyncUnlock is\n// called for the same server.\n//\n// This method works on secondaries as well, preventing the oplog from\n// being flushed while the server is locked, but since only the server\n// connected to is locked, for locking specific secondaries it may be\n// necessary to establish a connection directly to the secondary (see\n// Dial's connect=direct option).\n//\n// As an important caveat, note that once a write is attempted and\n// blocks, follow up reads will block as well due to the way the\n// lock is internally implemented in the server. More details at:\n//\n//     https://jira.mongodb.org/browse/SERVER-4243\n//\n// FsyncLock is often used for performing consistent backups of\n// the database files on disk.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/fsync+Command\n//     http://www.mongodb.org/display/DOCS/Backups\n//\nfunc (s *Session) FsyncLock() error {\n\treturn s.Run(bson.D{{\"fsync\", 1}, {\"lock\", true}}, nil)\n}\n\n// FsyncUnlock releases the server for writes. See FsyncLock for details.\nfunc (s *Session) FsyncUnlock() error {\n\terr := s.Run(bson.D{{\"fsyncUnlock\", 1}}, nil)\n\tif isNoCmd(err) {\n\t\terr = s.DB(\"admin\").C(\"$cmd.sys.unlock\").Find(nil).One(nil) // WTF?\n\t}\n\treturn err\n}\n\n// Find prepares a query using the provided document.  The document may be a\n// map or a struct value capable of being marshalled with bson.  The map\n// may be a generic one using interface{} for its key and/or values, such as\n// bson.M, or it may be a properly typed map.  Providing nil as the document\n// is equivalent to providing an empty document such as bson.M{}.\n//\n// Further details of the query may be tweaked using the resulting Query value,\n// and then executed to retrieve results using methods such as One, For,\n// Iter, or Tail.\n//\n// In case the resulting document includes a field named $err or errmsg, which\n// are standard ways for MongoDB to return query errors, the returned err will\n// be set to a *QueryError value including the Err message and the Code.  In\n// those cases, the result argument is still unmarshalled into with the\n// received document so that any other custom values may be obtained if\n// desired.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Querying\n//     http://www.mongodb.org/display/DOCS/Advanced+Queries\n//\nfunc (c *Collection) Find(query interface{}) *Query {\n\tsession := c.Database.Session\n\tsession.m.RLock()\n\tq := &Query{session: session, query: session.queryConfig}\n\tsession.m.RUnlock()\n\tq.op.query = query\n\tq.op.collection = c.FullName\n\treturn q\n}\n\ntype repairCmd struct {\n\tRepairCursor string           `bson:\"repairCursor\"`\n\tCursor       *repairCmdCursor \",omitempty\"\n}\n\ntype repairCmdCursor struct {\n\tBatchSize int `bson:\"batchSize,omitempty\"`\n}\n\n// Repair returns an iterator that goes over all recovered documents in the\n// collection, in a best-effort manner. This is most useful when there are\n// damaged data files. Multiple copies of the same document may be returned\n// by the iterator.\n//\n// Repair is supported in MongoDB 2.7.8 and later.\nfunc (c *Collection) Repair() *Iter {\n\t// Clone session and set it to Monotonic mode so that the server\n\t// used for the query may be safely obtained afterwards, if\n\t// necessary for iteration when a cursor is received.\n\tsession := c.Database.Session\n\tcloned := session.nonEventual()\n\tdefer cloned.Close()\n\n\tbatchSize := int(cloned.queryConfig.op.limit)\n\n\tvar result struct{ Cursor cursorData }\n\n\tcmd := repairCmd{\n\t\tRepairCursor: c.Name,\n\t\tCursor:       &repairCmdCursor{batchSize},\n\t}\n\n\tclonedc := c.With(cloned)\n\terr := clonedc.Database.Run(cmd, &result)\n\treturn clonedc.NewIter(session, result.Cursor.FirstBatch, result.Cursor.Id, err)\n}\n\n// FindId is a convenience helper equivalent to:\n//\n//     query := collection.Find(bson.M{\"_id\": id})\n//\n// See the Find method for more details.\nfunc (c *Collection) FindId(id interface{}) *Query {\n\treturn c.Find(bson.D{{\"_id\", id}})\n}\n\ntype Pipe struct {\n\tsession    *Session\n\tcollection *Collection\n\tpipeline   interface{}\n\tallowDisk  bool\n\tbatchSize  int\n}\n\ntype pipeCmd struct {\n\tAggregate string\n\tPipeline  interface{}\n\tCursor    *pipeCmdCursor \",omitempty\"\n\tExplain   bool           \",omitempty\"\n\tAllowDisk bool           \"allowDiskUse,omitempty\"\n}\n\ntype pipeCmdCursor struct {\n\tBatchSize int `bson:\"batchSize,omitempty\"`\n}\n\n// Pipe prepares a pipeline to aggregate. The pipeline document\n// must be a slice built in terms of the aggregation framework language.\n//\n// For example:\n//\n//     pipe := collection.Pipe([]bson.M{{\"$match\": bson.M{\"name\": \"Otavio\"}}})\n//     iter := pipe.Iter()\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/reference/aggregation\n//     http://docs.mongodb.org/manual/applications/aggregation\n//     http://docs.mongodb.org/manual/tutorial/aggregation-examples\n//\nfunc (c *Collection) Pipe(pipeline interface{}) *Pipe {\n\tsession := c.Database.Session\n\tsession.m.RLock()\n\tbatchSize := int(session.queryConfig.op.limit)\n\tsession.m.RUnlock()\n\treturn &Pipe{\n\t\tsession:    session,\n\t\tcollection: c,\n\t\tpipeline:   pipeline,\n\t\tbatchSize:  batchSize,\n\t}\n}\n\n// Iter executes the pipeline and returns an iterator capable of going\n// over all the generated results.\nfunc (p *Pipe) Iter() *Iter {\n\t// Clone session and set it to Monotonic mode so that the server\n\t// used for the query may be safely obtained afterwards, if\n\t// necessary for iteration when a cursor is received.\n\tcloned := p.session.nonEventual()\n\tdefer cloned.Close()\n\tc := p.collection.With(cloned)\n\n\tvar result struct {\n\t\tResult []bson.Raw // 2.4, no cursors.\n\t\tCursor cursorData // 2.6+, with cursors.\n\t}\n\n\tcmd := pipeCmd{\n\t\tAggregate: c.Name,\n\t\tPipeline:  p.pipeline,\n\t\tAllowDisk: p.allowDisk,\n\t\tCursor:    &pipeCmdCursor{p.batchSize},\n\t}\n\terr := c.Database.Run(cmd, &result)\n\tif e, ok := err.(*QueryError); ok && e.Message == `unrecognized field \"cursor` {\n\t\tcmd.Cursor = nil\n\t\tcmd.AllowDisk = false\n\t\terr = c.Database.Run(cmd, &result)\n\t}\n\tfirstBatch := result.Result\n\tif firstBatch == nil {\n\t\tfirstBatch = result.Cursor.FirstBatch\n\t}\n\treturn c.NewIter(p.session, firstBatch, result.Cursor.Id, err)\n}\n\n// NewIter returns a newly created iterator with the provided parameters.\n// Using this method is not recommended unless the desired functionality\n// is not yet exposed via a more convenient interface (Find, Pipe, etc).\n//\n// The optional session parameter associates the lifetime of the returned\n// iterator to an arbitrary session. If nil, the iterator will be bound to\n// c's session.\n//\n// Documents in firstBatch will be individually provided by the returned\n// iterator before documents from cursorId are made available. If cursorId\n// is zero, only the documents in firstBatch are provided.\n//\n// If err is not nil, the iterator's Err method will report it after\n// exhausting documents in firstBatch.\n//\n// NewIter must be called right after the cursor id is obtained, and must not\n// be called on a collection in Eventual mode, because the cursor id is\n// associated with the specific server that returned it. The provided session\n// parameter may be in any mode or state, though.\n//\nfunc (c *Collection) NewIter(session *Session, firstBatch []bson.Raw, cursorId int64, err error) *Iter {\n\tvar server *mongoServer\n\tcsession := c.Database.Session\n\tcsession.m.RLock()\n\tsocket := csession.masterSocket\n\tif socket == nil {\n\t\tsocket = csession.slaveSocket\n\t}\n\tif socket != nil {\n\t\tserver = socket.Server()\n\t}\n\tcsession.m.RUnlock()\n\n\tif server == nil {\n\t\tif csession.Mode() == Eventual {\n\t\t\tpanic(\"Collection.NewIter called in Eventual mode\")\n\t\t}\n\t\tif err == nil {\n\t\t\terr = errors.New(\"server not available\")\n\t\t}\n\t}\n\n\tif session == nil {\n\t\tsession = csession\n\t}\n\n\titer := &Iter{\n\t\tsession: session,\n\t\tserver:  server,\n\t\ttimeout: -1,\n\t\terr:     err,\n\t}\n\titer.gotReply.L = &iter.m\n\tfor _, doc := range firstBatch {\n\t\titer.docData.Push(doc.Data)\n\t}\n\tif cursorId != 0 {\n\t\titer.op.cursorId = cursorId\n\t\titer.op.collection = c.FullName\n\t\titer.op.replyFunc = iter.replyFunc()\n\t}\n\treturn iter\n}\n\n// All works like Iter.All.\nfunc (p *Pipe) All(result interface{}) error {\n\treturn p.Iter().All(result)\n}\n\n// One executes the pipeline and unmarshals the first item from the\n// result set into the result parameter.\n// It returns ErrNotFound if no items are generated by the pipeline.\nfunc (p *Pipe) One(result interface{}) error {\n\titer := p.Iter()\n\tif iter.Next(result) {\n\t\treturn nil\n\t}\n\tif err := iter.Err(); err != nil {\n\t\treturn err\n\t}\n\treturn ErrNotFound\n}\n\n// Explain returns a number of details about how the MongoDB server would\n// execute the requested pipeline, such as the number of objects examined,\n// the number of times the read lock was yielded to allow writes to go in,\n// and so on.\n//\n// For example:\n//\n//     var m bson.M\n//     err := collection.Pipe(pipeline).Explain(&m)\n//     if err == nil {\n//         fmt.Printf(\"Explain: %#v\\n\", m)\n//     }\n//\nfunc (p *Pipe) Explain(result interface{}) error {\n\tc := p.collection\n\tcmd := pipeCmd{\n\t\tAggregate: c.Name,\n\t\tPipeline:  p.pipeline,\n\t\tAllowDisk: p.allowDisk,\n\t\tExplain:   true,\n\t}\n\treturn c.Database.Run(cmd, result)\n}\n\n// AllowDiskUse enables writing to the \"<dbpath>/_tmp\" server directory so\n// that aggregation pipelines do not have to be held entirely in memory.\nfunc (p *Pipe) AllowDiskUse() *Pipe {\n\tp.allowDisk = true\n\treturn p\n}\n\n// Batch sets the batch size used when fetching documents from the database.\n// It's possible to change this setting on a per-session basis as well, using\n// the Batch method of Session.\n//\n// The default batch size is defined by the database server.\nfunc (p *Pipe) Batch(n int) *Pipe {\n\tp.batchSize = n\n\treturn p\n}\n\n// mgo.v3: Use a single user-visible error type.\n\ntype LastError struct {\n\tErr             string\n\tCode, N, Waited int\n\tFSyncFiles      int `bson:\"fsyncFiles\"`\n\tWTimeout        bool\n\tUpdatedExisting bool        `bson:\"updatedExisting\"`\n\tUpsertedId      interface{} `bson:\"upserted\"`\n\n\tmodified int\n\tecases   []BulkErrorCase\n}\n\nfunc (err *LastError) Error() string {\n\treturn err.Err\n}\n\ntype queryError struct {\n\tErr           string \"$err\"\n\tErrMsg        string\n\tAssertion     string\n\tCode          int\n\tAssertionCode int \"assertionCode\"\n}\n\ntype QueryError struct {\n\tCode      int\n\tMessage   string\n\tAssertion bool\n}\n\nfunc (err *QueryError) Error() string {\n\treturn err.Message\n}\n\n// IsDup returns whether err informs of a duplicate key error because\n// a primary key index or a secondary unique index already has an entry\n// with the given value.\nfunc IsDup(err error) bool {\n\t// Besides being handy, helps with MongoDB bugs SERVER-7164 and SERVER-11493.\n\t// What follows makes me sad. Hopefully conventions will be more clear over time.\n\tswitch e := err.(type) {\n\tcase *LastError:\n\t\treturn e.Code == 11000 || e.Code == 11001 || e.Code == 12582 || e.Code == 16460 && strings.Contains(e.Err, \" E11000 \")\n\tcase *QueryError:\n\t\treturn e.Code == 11000 || e.Code == 11001 || e.Code == 12582\n\tcase *BulkError:\n\t\tfor _, ecase := range e.ecases {\n\t\t\tif !IsDup(ecase.Err) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Insert inserts one or more documents in the respective collection.  In\n// case the session is in safe mode (see the SetSafe method) and an error\n// happens while inserting the provided documents, the returned error will\n// be of type *LastError.\nfunc (c *Collection) Insert(docs ...interface{}) error {\n\t_, err := c.writeOp(&insertOp{c.FullName, docs, 0}, true)\n\treturn err\n}\n\n// Update finds a single document matching the provided selector document\n// and modifies it according to the update document.\n// If the session is in safe mode (see SetSafe) a ErrNotFound error is\n// returned if a document isn't found, or a value of type *LastError\n// when some other error is detected.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Updating\n//     http://www.mongodb.org/display/DOCS/Atomic+Operations\n//\nfunc (c *Collection) Update(selector interface{}, update interface{}) error {\n\tif selector == nil {\n\t\tselector = bson.D{}\n\t}\n\top := updateOp{\n\t\tCollection: c.FullName,\n\t\tSelector:   selector,\n\t\tUpdate:     update,\n\t}\n\tlerr, err := c.writeOp(&op, true)\n\tif err == nil && lerr != nil && !lerr.UpdatedExisting {\n\t\treturn ErrNotFound\n\t}\n\treturn err\n}\n\n// UpdateId is a convenience helper equivalent to:\n//\n//     err := collection.Update(bson.M{\"_id\": id}, update)\n//\n// See the Update method for more details.\nfunc (c *Collection) UpdateId(id interface{}, update interface{}) error {\n\treturn c.Update(bson.D{{\"_id\", id}}, update)\n}\n\n// ChangeInfo holds details about the outcome of an update operation.\ntype ChangeInfo struct {\n\t// Updated reports the number of existing documents modified.\n\t// Due to server limitations, this reports the same value as the Matched field when\n\t// talking to MongoDB <= 2.4 and on Upsert and Apply (findAndModify) operations.\n\tUpdated    int\n\tRemoved    int         // Number of documents removed\n\tMatched    int         // Number of documents matched but not necessarily changed\n\tUpsertedId interface{} // Upserted _id field, when not explicitly provided\n}\n\n// UpdateAll finds all documents matching the provided selector document\n// and modifies them according to the update document.\n// If the session is in safe mode (see SetSafe) details of the executed\n// operation are returned in info or an error of type *LastError when\n// some problem is detected. It is not an error for the update to not be\n// applied on any documents because the selector doesn't match.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Updating\n//     http://www.mongodb.org/display/DOCS/Atomic+Operations\n//\nfunc (c *Collection) UpdateAll(selector interface{}, update interface{}) (info *ChangeInfo, err error) {\n\tif selector == nil {\n\t\tselector = bson.D{}\n\t}\n\top := updateOp{\n\t\tCollection: c.FullName,\n\t\tSelector:   selector,\n\t\tUpdate:     update,\n\t\tFlags:      2,\n\t\tMulti:      true,\n\t}\n\tlerr, err := c.writeOp(&op, true)\n\tif err == nil && lerr != nil {\n\t\tinfo = &ChangeInfo{Updated: lerr.modified, Matched: lerr.N}\n\t}\n\treturn info, err\n}\n\n// Upsert finds a single document matching the provided selector document\n// and modifies it according to the update document.  If no document matching\n// the selector is found, the update document is applied to the selector\n// document and the result is inserted in the collection.\n// If the session is in safe mode (see SetSafe) details of the executed\n// operation are returned in info, or an error of type *LastError when\n// some problem is detected.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Updating\n//     http://www.mongodb.org/display/DOCS/Atomic+Operations\n//\nfunc (c *Collection) Upsert(selector interface{}, update interface{}) (info *ChangeInfo, err error) {\n\tif selector == nil {\n\t\tselector = bson.D{}\n\t}\n\top := updateOp{\n\t\tCollection: c.FullName,\n\t\tSelector:   selector,\n\t\tUpdate:     update,\n\t\tFlags:      1,\n\t\tUpsert:     true,\n\t}\n\tvar lerr *LastError\n\tfor i := 0; i < maxUpsertRetries; i++ {\n\t\tlerr, err = c.writeOp(&op, true)\n\t\t// Retry duplicate key errors on upserts.\n\t\t// https://docs.mongodb.com/v3.2/reference/method/db.collection.update/#use-unique-indexes\n\t\tif !IsDup(err) {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err == nil && lerr != nil {\n\t\tinfo = &ChangeInfo{}\n\t\tif lerr.UpdatedExisting {\n\t\t\tinfo.Matched = lerr.N\n\t\t\tinfo.Updated = lerr.modified\n\t\t} else {\n\t\t\tinfo.UpsertedId = lerr.UpsertedId\n\t\t}\n\t}\n\treturn info, err\n}\n\n// UpsertId is a convenience helper equivalent to:\n//\n//     info, err := collection.Upsert(bson.M{\"_id\": id}, update)\n//\n// See the Upsert method for more details.\nfunc (c *Collection) UpsertId(id interface{}, update interface{}) (info *ChangeInfo, err error) {\n\treturn c.Upsert(bson.D{{\"_id\", id}}, update)\n}\n\n// Remove finds a single document matching the provided selector document\n// and removes it from the database.\n// If the session is in safe mode (see SetSafe) a ErrNotFound error is\n// returned if a document isn't found, or a value of type *LastError\n// when some other error is detected.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Removing\n//\nfunc (c *Collection) Remove(selector interface{}) error {\n\tif selector == nil {\n\t\tselector = bson.D{}\n\t}\n\tlerr, err := c.writeOp(&deleteOp{c.FullName, selector, 1, 1}, true)\n\tif err == nil && lerr != nil && lerr.N == 0 {\n\t\treturn ErrNotFound\n\t}\n\treturn err\n}\n\n// RemoveId is a convenience helper equivalent to:\n//\n//     err := collection.Remove(bson.M{\"_id\": id})\n//\n// See the Remove method for more details.\nfunc (c *Collection) RemoveId(id interface{}) error {\n\treturn c.Remove(bson.D{{\"_id\", id}})\n}\n\n// RemoveAll finds all documents matching the provided selector document\n// and removes them from the database.  In case the session is in safe mode\n// (see the SetSafe method) and an error happens when attempting the change,\n// the returned error will be of type *LastError.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Removing\n//\nfunc (c *Collection) RemoveAll(selector interface{}) (info *ChangeInfo, err error) {\n\tif selector == nil {\n\t\tselector = bson.D{}\n\t}\n\tlerr, err := c.writeOp(&deleteOp{c.FullName, selector, 0, 0}, true)\n\tif err == nil && lerr != nil {\n\t\tinfo = &ChangeInfo{Removed: lerr.N, Matched: lerr.N}\n\t}\n\treturn info, err\n}\n\n// DropDatabase removes the entire database including all of its collections.\nfunc (db *Database) DropDatabase() error {\n\treturn db.Run(bson.D{{\"dropDatabase\", 1}}, nil)\n}\n\n// DropCollection removes the entire collection including all of its documents.\nfunc (c *Collection) DropCollection() error {\n\treturn c.Database.Run(bson.D{{\"drop\", c.Name}}, nil)\n}\n\n// The CollectionInfo type holds metadata about a collection.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/createCollection+Command\n//     http://www.mongodb.org/display/DOCS/Capped+Collections\n//\ntype CollectionInfo struct {\n\t// DisableIdIndex prevents the automatic creation of the index\n\t// on the _id field for the collection.\n\tDisableIdIndex bool\n\n\t// ForceIdIndex enforces the automatic creation of the index\n\t// on the _id field for the collection. Capped collections,\n\t// for example, do not have such an index by default.\n\tForceIdIndex bool\n\n\t// If Capped is true new documents will replace old ones when\n\t// the collection is full. MaxBytes must necessarily be set\n\t// to define the size when the collection wraps around.\n\t// MaxDocs optionally defines the number of documents when it\n\t// wraps, but MaxBytes still needs to be set.\n\tCapped   bool\n\tMaxBytes int\n\tMaxDocs  int\n\n\t// Validator contains a validation expression that defines which\n\t// documents should be considered valid for this collection.\n\tValidator interface{}\n\n\t// ValidationLevel may be set to \"strict\" (the default) to force\n\t// MongoDB to validate all documents on inserts and updates, to\n\t// \"moderate\" to apply the validation rules only to documents\n\t// that already fulfill the validation criteria, or to \"off\" for\n\t// disabling validation entirely.\n\tValidationLevel string\n\n\t// ValidationAction determines how MongoDB handles documents that\n\t// violate the validation rules. It may be set to \"error\" (the default)\n\t// to reject inserts or updates that violate the rules, or to \"warn\"\n\t// to log invalid operations but allow them to proceed.\n\tValidationAction string\n\n\t// StorageEngine allows specifying collection options for the\n\t// storage engine in use. The map keys must hold the storage engine\n\t// name for which options are being specified.\n\tStorageEngine interface{}\n}\n\n// Create explicitly creates the c collection with details of info.\n// MongoDB creates collections automatically on use, so this method\n// is only necessary when creating collection with non-default\n// characteristics, such as capped collections.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/createCollection+Command\n//     http://www.mongodb.org/display/DOCS/Capped+Collections\n//\nfunc (c *Collection) Create(info *CollectionInfo) error {\n\tcmd := make(bson.D, 0, 4)\n\tcmd = append(cmd, bson.DocElem{\"create\", c.Name})\n\tif info.Capped {\n\t\tif info.MaxBytes < 1 {\n\t\t\treturn fmt.Errorf(\"Collection.Create: with Capped, MaxBytes must also be set\")\n\t\t}\n\t\tcmd = append(cmd, bson.DocElem{\"capped\", true})\n\t\tcmd = append(cmd, bson.DocElem{\"size\", info.MaxBytes})\n\t\tif info.MaxDocs > 0 {\n\t\t\tcmd = append(cmd, bson.DocElem{\"max\", info.MaxDocs})\n\t\t}\n\t}\n\tif info.DisableIdIndex {\n\t\tcmd = append(cmd, bson.DocElem{\"autoIndexId\", false})\n\t}\n\tif info.ForceIdIndex {\n\t\tcmd = append(cmd, bson.DocElem{\"autoIndexId\", true})\n\t}\n\tif info.Validator != nil {\n\t\tcmd = append(cmd, bson.DocElem{\"validator\", info.Validator})\n\t}\n\tif info.ValidationLevel != \"\" {\n\t\tcmd = append(cmd, bson.DocElem{\"validationLevel\", info.ValidationLevel})\n\t}\n\tif info.ValidationAction != \"\" {\n\t\tcmd = append(cmd, bson.DocElem{\"validationAction\", info.ValidationAction})\n\t}\n\tif info.StorageEngine != nil {\n\t\tcmd = append(cmd, bson.DocElem{\"storageEngine\", info.StorageEngine})\n\t}\n\treturn c.Database.Run(cmd, nil)\n}\n\n// Batch sets the batch size used when fetching documents from the database.\n// It's possible to change this setting on a per-session basis as well, using\n// the Batch method of Session.\n\n// The default batch size is defined by the database itself.  As of this\n// writing, MongoDB will use an initial size of min(100 docs, 4MB) on the\n// first batch, and 4MB on remaining ones.\nfunc (q *Query) Batch(n int) *Query {\n\tif n == 1 {\n\t\t// Server interprets 1 as -1 and closes the cursor (!?)\n\t\tn = 2\n\t}\n\tq.m.Lock()\n\tq.op.limit = int32(n)\n\tq.m.Unlock()\n\treturn q\n}\n\n// Prefetch sets the point at which the next batch of results will be requested.\n// When there are p*batch_size remaining documents cached in an Iter, the next\n// batch will be requested in background. For instance, when using this:\n//\n//     query.Batch(200).Prefetch(0.25)\n//\n// and there are only 50 documents cached in the Iter to be processed, the\n// next batch of 200 will be requested. It's possible to change this setting on\n// a per-session basis as well, using the SetPrefetch method of Session.\n//\n// The default prefetch value is 0.25.\nfunc (q *Query) Prefetch(p float64) *Query {\n\tq.m.Lock()\n\tq.prefetch = p\n\tq.m.Unlock()\n\treturn q\n}\n\n// Skip skips over the n initial documents from the query results.  Note that\n// this only makes sense with capped collections where documents are naturally\n// ordered by insertion time, or with sorted results.\nfunc (q *Query) Skip(n int) *Query {\n\tq.m.Lock()\n\tq.op.skip = int32(n)\n\tq.m.Unlock()\n\treturn q\n}\n\n// Limit restricts the maximum number of documents retrieved to n, and also\n// changes the batch size to the same value.  Once n documents have been\n// returned by Next, the following call will return ErrNotFound.\nfunc (q *Query) Limit(n int) *Query {\n\tq.m.Lock()\n\tswitch {\n\tcase n == 1:\n\t\tq.limit = 1\n\t\tq.op.limit = -1\n\tcase n == math.MinInt32: // -MinInt32 == -MinInt32\n\t\tq.limit = math.MaxInt32\n\t\tq.op.limit = math.MinInt32 + 1\n\tcase n < 0:\n\t\tq.limit = int32(-n)\n\t\tq.op.limit = int32(n)\n\tdefault:\n\t\tq.limit = int32(n)\n\t\tq.op.limit = int32(n)\n\t}\n\tq.m.Unlock()\n\treturn q\n}\n\n// Select enables selecting which fields should be retrieved for the results\n// found. For example, the following query would only retrieve the name field:\n//\n//     err := collection.Find(nil).Select(bson.M{\"name\": 1}).One(&result)\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Retrieving+a+Subset+of+Fields\n//\nfunc (q *Query) Select(selector interface{}) *Query {\n\tq.m.Lock()\n\tq.op.selector = selector\n\tq.m.Unlock()\n\treturn q\n}\n\n// Sort asks the database to order returned documents according to the\n// provided field names. A field name may be prefixed by - (minus) for\n// it to be sorted in reverse order.\n//\n// For example:\n//\n//     query1 := collection.Find(nil).Sort(\"firstname\", \"lastname\")\n//     query2 := collection.Find(nil).Sort(\"-age\")\n//     query3 := collection.Find(nil).Sort(\"$natural\")\n//     query4 := collection.Find(nil).Select(bson.M{\"score\": bson.M{\"$meta\": \"textScore\"}}).Sort(\"$textScore:score\")\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Sorting+and+Natural+Order\n//\nfunc (q *Query) Sort(fields ...string) *Query {\n\tq.m.Lock()\n\tvar order bson.D\n\tfor _, field := range fields {\n\t\tn := 1\n\t\tvar kind string\n\t\tif field != \"\" {\n\t\t\tif field[0] == '$' {\n\t\t\t\tif c := strings.Index(field, \":\"); c > 1 && c < len(field)-1 {\n\t\t\t\t\tkind = field[1:c]\n\t\t\t\t\tfield = field[c+1:]\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch field[0] {\n\t\t\tcase '+':\n\t\t\t\tfield = field[1:]\n\t\t\tcase '-':\n\t\t\t\tn = -1\n\t\t\t\tfield = field[1:]\n\t\t\t}\n\t\t}\n\t\tif field == \"\" {\n\t\t\tpanic(\"Sort: empty field name\")\n\t\t}\n\t\tif kind == \"textScore\" {\n\t\t\torder = append(order, bson.DocElem{field, bson.M{\"$meta\": kind}})\n\t\t} else {\n\t\t\torder = append(order, bson.DocElem{field, n})\n\t\t}\n\t}\n\tq.op.options.OrderBy = order\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\treturn q\n}\n\n// Explain returns a number of details about how the MongoDB server would\n// execute the requested query, such as the number of objects examined,\n// the number of times the read lock was yielded to allow writes to go in,\n// and so on.\n//\n// For example:\n//\n//     m := bson.M{}\n//     err := collection.Find(bson.M{\"filename\": name}).Explain(m)\n//     if err == nil {\n//         fmt.Printf(\"Explain: %#v\\n\", m)\n//     }\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Optimization\n//     http://www.mongodb.org/display/DOCS/Query+Optimizer\n//\nfunc (q *Query) Explain(result interface{}) error {\n\tq.m.Lock()\n\tclone := &Query{session: q.session, query: q.query}\n\tq.m.Unlock()\n\tclone.op.options.Explain = true\n\tclone.op.hasOptions = true\n\tif clone.op.limit > 0 {\n\t\tclone.op.limit = -q.op.limit\n\t}\n\titer := clone.Iter()\n\tif iter.Next(result) {\n\t\treturn nil\n\t}\n\treturn iter.Close()\n}\n\n// TODO: Add Collection.Explain. See https://goo.gl/1MDlvz.\n\n// Hint will include an explicit \"hint\" in the query to force the server\n// to use a specified index, potentially improving performance in some\n// situations.  The provided parameters are the fields that compose the\n// key of the index to be used.  For details on how the indexKey may be\n// built, see the EnsureIndex method.\n//\n// For example:\n//\n//     query := collection.Find(bson.M{\"firstname\": \"Joe\", \"lastname\": \"Winter\"})\n//     query.Hint(\"lastname\", \"firstname\")\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Optimization\n//     http://www.mongodb.org/display/DOCS/Query+Optimizer\n//\nfunc (q *Query) Hint(indexKey ...string) *Query {\n\tq.m.Lock()\n\tkeyInfo, err := parseIndexKey(indexKey)\n\tq.op.options.Hint = keyInfo.key\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn q\n}\n\n// SetMaxScan constrains the query to stop after scanning the specified\n// number of documents.\n//\n// This modifier is generally used to prevent potentially long running\n// queries from disrupting performance by scanning through too much data.\nfunc (q *Query) SetMaxScan(n int) *Query {\n\tq.m.Lock()\n\tq.op.options.MaxScan = n\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\treturn q\n}\n\n// SetMaxTime constrains the query to stop after running for the specified time.\n//\n// When the time limit is reached MongoDB automatically cancels the query.\n// This can be used to efficiently prevent and identify unexpectedly slow queries.\n//\n// A few important notes about the mechanism enforcing this limit:\n//\n//  - Requests can block behind locking operations on the server, and that blocking\n//    time is not accounted for. In other words, the timer starts ticking only after\n//    the actual start of the query when it initially acquires the appropriate lock;\n//\n//  - Operations are interrupted only at interrupt points where an operation can be\n//    safely aborted – the total execution time may exceed the specified value;\n//\n//  - The limit can be applied to both CRUD operations and commands, but not all\n//    commands are interruptible;\n//\n//  - While iterating over results, computing follow up batches is included in the\n//    total time and the iteration continues until the alloted time is over, but\n//    network roundtrips are not taken into account for the limit.\n//\n//  - This limit does not override the inactive cursor timeout for idle cursors\n//    (default is 10 min).\n//\n// This mechanism was introduced in MongoDB 2.6.\n//\n// Relevant documentation:\n//\n//   http://blog.mongodb.org/post/83621787773/maxtimems-and-query-optimizer-introspection-in\n//\nfunc (q *Query) SetMaxTime(d time.Duration) *Query {\n\tq.m.Lock()\n\tq.op.options.MaxTimeMS = int(d / time.Millisecond)\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\treturn q\n}\n\n// Snapshot will force the performed query to make use of an available\n// index on the _id field to prevent the same document from being returned\n// more than once in a single iteration. This might happen without this\n// setting in situations when the document changes in size and thus has to\n// be moved while the iteration is running.\n//\n// Because snapshot mode traverses the _id index, it may not be used with\n// sorting or explicit hints. It also cannot use any other index for the\n// query.\n//\n// Even with snapshot mode, items inserted or deleted during the query may\n// or may not be returned; that is, this mode is not a true point-in-time\n// snapshot.\n//\n// The same effect of Snapshot may be obtained by using any unique index on\n// field(s) that will not be modified (best to use Hint explicitly too).\n// A non-unique index (such as creation time) may be made unique by\n// appending _id to the index when creating it.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/How+to+do+Snapshotted+Queries+in+the+Mongo+Database\n//\nfunc (q *Query) Snapshot() *Query {\n\tq.m.Lock()\n\tq.op.options.Snapshot = true\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\treturn q\n}\n\n// Comment adds a comment to the query to identify it in the database profiler output.\n//\n// Relevant documentation:\n//\n//     http://docs.mongodb.org/manual/reference/operator/meta/comment\n//     http://docs.mongodb.org/manual/reference/command/profile\n//     http://docs.mongodb.org/manual/administration/analyzing-mongodb-performance/#database-profiling\n//\nfunc (q *Query) Comment(comment string) *Query {\n\tq.m.Lock()\n\tq.op.options.Comment = comment\n\tq.op.hasOptions = true\n\tq.m.Unlock()\n\treturn q\n}\n\n// LogReplay enables an option that optimizes queries that are typically\n// made on the MongoDB oplog for replaying it. This is an internal\n// implementation aspect and most likely uninteresting for other uses.\n// It has seen at least one use case, though, so it's exposed via the API.\nfunc (q *Query) LogReplay() *Query {\n\tq.m.Lock()\n\tq.op.flags |= flagLogReplay\n\tq.m.Unlock()\n\treturn q\n}\n\nfunc checkQueryError(fullname string, d []byte) error {\n\tl := len(d)\n\tif l < 16 {\n\t\treturn nil\n\t}\n\tif d[5] == '$' && d[6] == 'e' && d[7] == 'r' && d[8] == 'r' && d[9] == '\\x00' && d[4] == '\\x02' {\n\t\tgoto Error\n\t}\n\tif len(fullname) < 5 || fullname[len(fullname)-5:] != \".$cmd\" {\n\t\treturn nil\n\t}\n\tfor i := 0; i+8 < l; i++ {\n\t\tif d[i] == '\\x02' && d[i+1] == 'e' && d[i+2] == 'r' && d[i+3] == 'r' && d[i+4] == 'm' && d[i+5] == 's' && d[i+6] == 'g' && d[i+7] == '\\x00' {\n\t\t\tgoto Error\n\t\t}\n\t}\n\treturn nil\n\nError:\n\tresult := &queryError{}\n\tbson.Unmarshal(d, result)\n\tif result.Err == \"\" && result.ErrMsg == \"\" {\n\t\treturn nil\n\t}\n\tif result.AssertionCode != 0 && result.Assertion != \"\" {\n\t\treturn &QueryError{Code: result.AssertionCode, Message: result.Assertion, Assertion: true}\n\t}\n\tif result.Err != \"\" {\n\t\treturn &QueryError{Code: result.Code, Message: result.Err}\n\t}\n\treturn &QueryError{Code: result.Code, Message: result.ErrMsg}\n}\n\n// One executes the query and unmarshals the first obtained document into the\n// result argument.  The result must be a struct or map value capable of being\n// unmarshalled into by gobson.  This function blocks until either a result\n// is available or an error happens.  For example:\n//\n//     err := collection.Find(bson.M{\"a\": 1}).One(&result)\n//\n// In case the resulting document includes a field named $err or errmsg, which\n// are standard ways for MongoDB to return query errors, the returned err will\n// be set to a *QueryError value including the Err message and the Code.  In\n// those cases, the result argument is still unmarshalled into with the\n// received document so that any other custom values may be obtained if\n// desired.\n//\nfunc (q *Query) One(result interface{}) (err error) {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op // Copy.\n\tq.m.Unlock()\n\n\tsocket, err := session.acquireSocket(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer socket.Release()\n\n\top.limit = -1\n\n\tsession.prepareQuery(&op)\n\n\texpectFindReply := prepareFindOp(socket, &op, 1)\n\n\tdata, err := socket.SimpleQuery(&op)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif data == nil {\n\t\treturn ErrNotFound\n\t}\n\tif expectFindReply {\n\t\tvar findReply struct {\n\t\t\tOk     bool\n\t\t\tCode   int\n\t\t\tErrmsg string\n\t\t\tCursor cursorData\n\t\t}\n\t\terr = bson.Unmarshal(data, &findReply)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !findReply.Ok && findReply.Errmsg != \"\" {\n\t\t\treturn &QueryError{Code: findReply.Code, Message: findReply.Errmsg}\n\t\t}\n\t\tif len(findReply.Cursor.FirstBatch) == 0 {\n\t\t\treturn ErrNotFound\n\t\t}\n\t\tdata = findReply.Cursor.FirstBatch[0].Data\n\t}\n\tif result != nil {\n\t\terr = bson.Unmarshal(data, result)\n\t\tif err == nil {\n\t\t\tdebugf(\"Query %p document unmarshaled: %#v\", q, result)\n\t\t} else {\n\t\t\tdebugf(\"Query %p document unmarshaling failed: %#v\", q, err)\n\t\t\treturn err\n\t\t}\n\t}\n\treturn checkQueryError(op.collection, data)\n}\n\n// prepareFindOp translates op from being an old-style wire protocol query into\n// a new-style find command if that's supported by the MongoDB server (3.2+).\n// It returns whether to expect a find command result or not. Note op may be\n// translated into an explain command, in which case the function returns false.\nfunc prepareFindOp(socket *mongoSocket, op *queryOp, limit int32) bool {\n\tif socket.ServerInfo().MaxWireVersion < 4 || op.collection == \"admin.$cmd\" {\n\t\treturn false\n\t}\n\n\tnameDot := strings.Index(op.collection, \".\")\n\tif nameDot < 0 {\n\t\tpanic(\"invalid query collection name: \" + op.collection)\n\t}\n\n\tfind := findCmd{\n\t\tCollection:  op.collection[nameDot+1:],\n\t\tFilter:      op.query,\n\t\tProjection:  op.selector,\n\t\tSort:        op.options.OrderBy,\n\t\tSkip:        op.skip,\n\t\tLimit:       limit,\n\t\tMaxTimeMS:   op.options.MaxTimeMS,\n\t\tMaxScan:     op.options.MaxScan,\n\t\tHint:        op.options.Hint,\n\t\tComment:     op.options.Comment,\n\t\tSnapshot:    op.options.Snapshot,\n\t\tOplogReplay: op.flags&flagLogReplay != 0,\n\t}\n\tif op.limit < 0 {\n\t\tfind.BatchSize = -op.limit\n\t\tfind.SingleBatch = true\n\t} else {\n\t\tfind.BatchSize = op.limit\n\t}\n\n\texplain := op.options.Explain\n\n\top.collection = op.collection[:nameDot] + \".$cmd\"\n\top.query = &find\n\top.skip = 0\n\top.limit = -1\n\top.options = queryWrapper{}\n\top.hasOptions = false\n\n\tif explain {\n\t\top.query = bson.D{{\"explain\", op.query}}\n\t\treturn false\n\t}\n\treturn true\n}\n\ntype cursorData struct {\n\tFirstBatch []bson.Raw \"firstBatch\"\n\tNextBatch  []bson.Raw \"nextBatch\"\n\tNS         string\n\tId         int64\n}\n\n// findCmd holds the command used for performing queries on MongoDB 3.2+.\n//\n// Relevant documentation:\n//\n//     https://docs.mongodb.org/master/reference/command/find/#dbcmd.find\n//\ntype findCmd struct {\n\tCollection          string      `bson:\"find\"`\n\tFilter              interface{} `bson:\"filter,omitempty\"`\n\tSort                interface{} `bson:\"sort,omitempty\"`\n\tProjection          interface{} `bson:\"projection,omitempty\"`\n\tHint                interface{} `bson:\"hint,omitempty\"`\n\tSkip                interface{} `bson:\"skip,omitempty\"`\n\tLimit               int32       `bson:\"limit,omitempty\"`\n\tBatchSize           int32       `bson:\"batchSize,omitempty\"`\n\tSingleBatch         bool        `bson:\"singleBatch,omitempty\"`\n\tComment             string      `bson:\"comment,omitempty\"`\n\tMaxScan             int         `bson:\"maxScan,omitempty\"`\n\tMaxTimeMS           int         `bson:\"maxTimeMS,omitempty\"`\n\tReadConcern         interface{} `bson:\"readConcern,omitempty\"`\n\tMax                 interface{} `bson:\"max,omitempty\"`\n\tMin                 interface{} `bson:\"min,omitempty\"`\n\tReturnKey           bool        `bson:\"returnKey,omitempty\"`\n\tShowRecordId        bool        `bson:\"showRecordId,omitempty\"`\n\tSnapshot            bool        `bson:\"snapshot,omitempty\"`\n\tTailable            bool        `bson:\"tailable,omitempty\"`\n\tAwaitData           bool        `bson:\"awaitData,omitempty\"`\n\tOplogReplay         bool        `bson:\"oplogReplay,omitempty\"`\n\tNoCursorTimeout     bool        `bson:\"noCursorTimeout,omitempty\"`\n\tAllowPartialResults bool        `bson:\"allowPartialResults,omitempty\"`\n}\n\n// getMoreCmd holds the command used for requesting more query results on MongoDB 3.2+.\n//\n// Relevant documentation:\n//\n//     https://docs.mongodb.org/master/reference/command/getMore/#dbcmd.getMore\n//\ntype getMoreCmd struct {\n\tCursorId   int64  `bson:\"getMore\"`\n\tCollection string `bson:\"collection\"`\n\tBatchSize  int32  `bson:\"batchSize,omitempty\"`\n\tMaxTimeMS  int64  `bson:\"maxTimeMS,omitempty\"`\n}\n\n// run duplicates the behavior of collection.Find(query).One(&result)\n// as performed by Database.Run, specializing the logic for running\n// database commands on a given socket.\nfunc (db *Database) run(socket *mongoSocket, cmd, result interface{}) (err error) {\n\t// Database.Run:\n\tif name, ok := cmd.(string); ok {\n\t\tcmd = bson.D{{name, 1}}\n\t}\n\n\t// Collection.Find:\n\tsession := db.Session\n\tsession.m.RLock()\n\top := session.queryConfig.op // Copy.\n\tsession.m.RUnlock()\n\top.query = cmd\n\top.collection = db.Name + \".$cmd\"\n\n\t// Query.One:\n\tsession.prepareQuery(&op)\n\top.limit = -1\n\n\tdata, err := socket.SimpleQuery(&op)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif data == nil {\n\t\treturn ErrNotFound\n\t}\n\tif result != nil {\n\t\terr = bson.Unmarshal(data, result)\n\t\tif err != nil {\n\t\t\tdebugf(\"Run command unmarshaling failed: %#v\", op, err)\n\t\t\treturn err\n\t\t}\n\t\tif globalDebug && globalLogger != nil {\n\t\t\tvar res bson.M\n\t\t\tbson.Unmarshal(data, &res)\n\t\t\tdebugf(\"Run command unmarshaled: %#v, result: %#v\", op, res)\n\t\t}\n\t}\n\treturn checkQueryError(op.collection, data)\n}\n\n// The DBRef type implements support for the database reference MongoDB\n// convention as supported by multiple drivers.  This convention enables\n// cross-referencing documents between collections and databases using\n// a structure which includes a collection name, a document id, and\n// optionally a database name.\n//\n// See the FindRef methods on Session and on Database.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Database+References\n//\ntype DBRef struct {\n\tCollection string      `bson:\"$ref\"`\n\tId         interface{} `bson:\"$id\"`\n\tDatabase   string      `bson:\"$db,omitempty\"`\n}\n\n// NOTE: Order of fields for DBRef above does matter, per documentation.\n\n// FindRef returns a query that looks for the document in the provided\n// reference. If the reference includes the DB field, the document will\n// be retrieved from the respective database.\n//\n// See also the DBRef type and the FindRef method on Session.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Database+References\n//\nfunc (db *Database) FindRef(ref *DBRef) *Query {\n\tvar c *Collection\n\tif ref.Database == \"\" {\n\t\tc = db.C(ref.Collection)\n\t} else {\n\t\tc = db.Session.DB(ref.Database).C(ref.Collection)\n\t}\n\treturn c.FindId(ref.Id)\n}\n\n// FindRef returns a query that looks for the document in the provided\n// reference. For a DBRef to be resolved correctly at the session level\n// it must necessarily have the optional DB field defined.\n//\n// See also the DBRef type and the FindRef method on Database.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Database+References\n//\nfunc (s *Session) FindRef(ref *DBRef) *Query {\n\tif ref.Database == \"\" {\n\t\tpanic(errors.New(fmt.Sprintf(\"Can't resolve database for %#v\", ref)))\n\t}\n\tc := s.DB(ref.Database).C(ref.Collection)\n\treturn c.FindId(ref.Id)\n}\n\n// CollectionNames returns the collection names present in the db database.\nfunc (db *Database) CollectionNames() (names []string, err error) {\n\t// Clone session and set it to Monotonic mode so that the server\n\t// used for the query may be safely obtained afterwards, if\n\t// necessary for iteration when a cursor is received.\n\tcloned := db.Session.nonEventual()\n\tdefer cloned.Close()\n\n\tbatchSize := int(cloned.queryConfig.op.limit)\n\n\t// Try with a command.\n\tvar result struct {\n\t\tCollections []bson.Raw\n\t\tCursor      cursorData\n\t}\n\terr = db.With(cloned).Run(bson.D{{\"listCollections\", 1}, {\"cursor\", bson.D{{\"batchSize\", batchSize}}}}, &result)\n\tif err == nil {\n\t\tfirstBatch := result.Collections\n\t\tif firstBatch == nil {\n\t\t\tfirstBatch = result.Cursor.FirstBatch\n\t\t}\n\t\tvar iter *Iter\n\t\tns := strings.SplitN(result.Cursor.NS, \".\", 2)\n\t\tif len(ns) < 2 {\n\t\t\titer = db.With(cloned).C(\"\").NewIter(nil, firstBatch, result.Cursor.Id, nil)\n\t\t} else {\n\t\t\titer = cloned.DB(ns[0]).C(ns[1]).NewIter(nil, firstBatch, result.Cursor.Id, nil)\n\t\t}\n\t\tvar coll struct{ Name string }\n\t\tfor iter.Next(&coll) {\n\t\t\tnames = append(names, coll.Name)\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsort.Strings(names)\n\t\treturn names, err\n\t}\n\tif err != nil && !isNoCmd(err) {\n\t\treturn nil, err\n\t}\n\n\t// Command not yet supported. Query the database instead.\n\tnameIndex := len(db.Name) + 1\n\titer := db.C(\"system.namespaces\").Find(nil).Iter()\n\tvar coll struct{ Name string }\n\tfor iter.Next(&coll) {\n\t\tif strings.Index(coll.Name, \"$\") < 0 || strings.Index(coll.Name, \".oplog.$\") >= 0 {\n\t\t\tnames = append(names, coll.Name[nameIndex:])\n\t\t}\n\t}\n\tif err := iter.Close(); err != nil {\n\t\treturn nil, err\n\t}\n\tsort.Strings(names)\n\treturn names, nil\n}\n\ntype dbNames struct {\n\tDatabases []struct {\n\t\tName  string\n\t\tEmpty bool\n\t}\n}\n\n// DatabaseNames returns the names of non-empty databases present in the cluster.\nfunc (s *Session) DatabaseNames() (names []string, err error) {\n\tvar result dbNames\n\terr = s.Run(\"listDatabases\", &result)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, db := range result.Databases {\n\t\tif !db.Empty {\n\t\t\tnames = append(names, db.Name)\n\t\t}\n\t}\n\tsort.Strings(names)\n\treturn names, nil\n}\n\n// Iter executes the query and returns an iterator capable of going over all\n// the results. Results will be returned in batches of configurable\n// size (see the Batch method) and more documents will be requested when a\n// configurable number of documents is iterated over (see the Prefetch method).\nfunc (q *Query) Iter() *Iter {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op\n\tprefetch := q.prefetch\n\tlimit := q.limit\n\tq.m.Unlock()\n\n\titer := &Iter{\n\t\tsession:  session,\n\t\tprefetch: prefetch,\n\t\tlimit:    limit,\n\t\ttimeout:  -1,\n\t}\n\titer.gotReply.L = &iter.m\n\titer.op.collection = op.collection\n\titer.op.limit = op.limit\n\titer.op.replyFunc = iter.replyFunc()\n\titer.docsToReceive++\n\n\tsocket, err := session.acquireSocket(true)\n\tif err != nil {\n\t\titer.err = err\n\t\treturn iter\n\t}\n\tdefer socket.Release()\n\n\tsession.prepareQuery(&op)\n\top.replyFunc = iter.op.replyFunc\n\n\tif prepareFindOp(socket, &op, limit) {\n\t\titer.findCmd = true\n\t}\n\n\titer.server = socket.Server()\n\terr = socket.Query(&op)\n\tif err != nil {\n\t\t// Must lock as the query is already out and it may call replyFunc.\n\t\titer.m.Lock()\n\t\titer.err = err\n\t\titer.m.Unlock()\n\t}\n\n\treturn iter\n}\n\n// Tail returns a tailable iterator. Unlike a normal iterator, a\n// tailable iterator may wait for new values to be inserted in the\n// collection once the end of the current result set is reached,\n// A tailable iterator may only be used with capped collections.\n//\n// The timeout parameter indicates how long Next will block waiting\n// for a result before timing out.  If set to -1, Next will not\n// timeout, and will continue waiting for a result for as long as\n// the cursor is valid and the session is not closed. If set to 0,\n// Next times out as soon as it reaches the end of the result set.\n// Otherwise, Next will wait for at least the given number of\n// seconds for a new document to be available before timing out.\n//\n// On timeouts, Next will unblock and return false, and the Timeout\n// method will return true if called. In these cases, Next may still\n// be called again on the same iterator to check if a new value is\n// available at the current cursor position, and again it will block\n// according to the specified timeoutSecs. If the cursor becomes\n// invalid, though, both Next and Timeout will return false and\n// the query must be restarted.\n//\n// The following example demonstrates timeout handling and query\n// restarting:\n//\n//    iter := collection.Find(nil).Sort(\"$natural\").Tail(5 * time.Second)\n//    for {\n//         for iter.Next(&result) {\n//             fmt.Println(result.Id)\n//             lastId = result.Id\n//         }\n//         if iter.Err() != nil {\n//             return iter.Close()\n//         }\n//         if iter.Timeout() {\n//             continue\n//         }\n//         query := collection.Find(bson.M{\"_id\": bson.M{\"$gt\": lastId}})\n//         iter = query.Sort(\"$natural\").Tail(5 * time.Second)\n//    }\n//    iter.Close()\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Tailable+Cursors\n//     http://www.mongodb.org/display/DOCS/Capped+Collections\n//     http://www.mongodb.org/display/DOCS/Sorting+and+Natural+Order\n//\nfunc (q *Query) Tail(timeout time.Duration) *Iter {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op\n\tprefetch := q.prefetch\n\tq.m.Unlock()\n\n\titer := &Iter{session: session, prefetch: prefetch}\n\titer.gotReply.L = &iter.m\n\titer.timeout = timeout\n\titer.op.collection = op.collection\n\titer.op.limit = op.limit\n\titer.op.replyFunc = iter.replyFunc()\n\titer.docsToReceive++\n\tsession.prepareQuery(&op)\n\top.replyFunc = iter.op.replyFunc\n\top.flags |= flagTailable | flagAwaitData\n\n\tsocket, err := session.acquireSocket(true)\n\tif err != nil {\n\t\titer.err = err\n\t} else {\n\t\titer.server = socket.Server()\n\t\terr = socket.Query(&op)\n\t\tif err != nil {\n\t\t\t// Must lock as the query is already out and it may call replyFunc.\n\t\t\titer.m.Lock()\n\t\t\titer.err = err\n\t\t\titer.m.Unlock()\n\t\t}\n\t\tsocket.Release()\n\t}\n\treturn iter\n}\n\nfunc (s *Session) prepareQuery(op *queryOp) {\n\ts.m.RLock()\n\top.mode = s.consistency\n\tif s.slaveOk {\n\t\top.flags |= flagSlaveOk\n\t}\n\ts.m.RUnlock()\n\treturn\n}\n\n// Err returns nil if no errors happened during iteration, or the actual\n// error otherwise.\n//\n// In case a resulting document included a field named $err or errmsg, which are\n// standard ways for MongoDB to report an improper query, the returned value has\n// a *QueryError type, and includes the Err message and the Code.\nfunc (iter *Iter) Err() error {\n\titer.m.Lock()\n\terr := iter.err\n\titer.m.Unlock()\n\tif err == ErrNotFound {\n\t\treturn nil\n\t}\n\treturn err\n}\n\n// Close kills the server cursor used by the iterator, if any, and returns\n// nil if no errors happened during iteration, or the actual error otherwise.\n//\n// Server cursors are automatically closed at the end of an iteration, which\n// means close will do nothing unless the iteration was interrupted before\n// the server finished sending results to the driver. If Close is not called\n// in such a situation, the cursor will remain available at the server until\n// the default cursor timeout period is reached. No further problems arise.\n//\n// Close is idempotent. That means it can be called repeatedly and will\n// return the same result every time.\n//\n// In case a resulting document included a field named $err or errmsg, which are\n// standard ways for MongoDB to report an improper query, the returned value has\n// a *QueryError type.\nfunc (iter *Iter) Close() error {\n\titer.m.Lock()\n\tcursorId := iter.op.cursorId\n\titer.op.cursorId = 0\n\terr := iter.err\n\titer.m.Unlock()\n\tif cursorId == 0 {\n\t\tif err == ErrNotFound {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tsocket, err := iter.acquireSocket()\n\tif err == nil {\n\t\t// TODO Batch kills.\n\t\terr = socket.Query(&killCursorsOp{[]int64{cursorId}})\n\t\tsocket.Release()\n\t}\n\n\titer.m.Lock()\n\tif err != nil && (iter.err == nil || iter.err == ErrNotFound) {\n\t\titer.err = err\n\t} else if iter.err != ErrNotFound {\n\t\terr = iter.err\n\t}\n\titer.m.Unlock()\n\treturn err\n}\n\n// Done returns true only if a follow up Next call is guaranteed\n// to return false.\n//\n// For an iterator created with Tail, Done may return false for\n// an iterator that has no more data. Otherwise it's guaranteed\n// to return false only if there is data or an error happened.\n//\n// Done may block waiting for a pending query to verify whether\n// more data is actually available or not.\nfunc (iter *Iter) Done() bool {\n\titer.m.Lock()\n\tdefer iter.m.Unlock()\n\n\tfor {\n\t\tif iter.docData.Len() > 0 {\n\t\t\treturn false\n\t\t}\n\t\tif iter.docsToReceive > 1 {\n\t\t\treturn true\n\t\t}\n\t\tif iter.docsToReceive > 0 {\n\t\t\titer.gotReply.Wait()\n\t\t\tcontinue\n\t\t}\n\t\treturn iter.op.cursorId == 0\n\t}\n}\n\n// Timeout returns true if Next returned false due to a timeout of\n// a tailable cursor. In those cases, Next may be called again to continue\n// the iteration at the previous cursor position.\nfunc (iter *Iter) Timeout() bool {\n\titer.m.Lock()\n\tresult := iter.timedout\n\titer.m.Unlock()\n\treturn result\n}\n\n// Next retrieves the next document from the result set, blocking if necessary.\n// This method will also automatically retrieve another batch of documents from\n// the server when the current one is exhausted, or before that in background\n// if pre-fetching is enabled (see the Query.Prefetch and Session.SetPrefetch\n// methods).\n//\n// Next returns true if a document was successfully unmarshalled onto result,\n// and false at the end of the result set or if an error happened.\n// When Next returns false, the Err method should be called to verify if\n// there was an error during iteration.\n//\n// For example:\n//\n//    iter := collection.Find(nil).Iter()\n//    for iter.Next(&result) {\n//        fmt.Printf(\"Result: %v\\n\", result.Id)\n//    }\n//    if err := iter.Close(); err != nil {\n//        return err\n//    }\n//\nfunc (iter *Iter) Next(result interface{}) bool {\n\titer.m.Lock()\n\titer.timedout = false\n\ttimeout := time.Time{}\n\tfor iter.err == nil && iter.docData.Len() == 0 && (iter.docsToReceive > 0 || iter.op.cursorId != 0) {\n\t\tif iter.docsToReceive == 0 {\n\t\t\tif iter.timeout >= 0 {\n\t\t\t\tif timeout.IsZero() {\n\t\t\t\t\ttimeout = time.Now().Add(iter.timeout)\n\t\t\t\t}\n\t\t\t\tif time.Now().After(timeout) {\n\t\t\t\t\titer.timedout = true\n\t\t\t\t\titer.m.Unlock()\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\titer.getMore()\n\t\t\tif iter.err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\titer.gotReply.Wait()\n\t}\n\n\t// Exhaust available data before reporting any errors.\n\tif docData, ok := iter.docData.Pop().([]byte); ok {\n\t\tclose := false\n\t\tif iter.limit > 0 {\n\t\t\titer.limit--\n\t\t\tif iter.limit == 0 {\n\t\t\t\tif iter.docData.Len() > 0 {\n\t\t\t\t\titer.m.Unlock()\n\t\t\t\t\tpanic(fmt.Errorf(\"data remains after limit exhausted: %d\", iter.docData.Len()))\n\t\t\t\t}\n\t\t\t\titer.err = ErrNotFound\n\t\t\t\tclose = true\n\t\t\t}\n\t\t}\n\t\tif iter.op.cursorId != 0 && iter.err == nil {\n\t\t\titer.docsBeforeMore--\n\t\t\tif iter.docsBeforeMore == -1 {\n\t\t\t\titer.getMore()\n\t\t\t}\n\t\t}\n\t\titer.m.Unlock()\n\n\t\tif close {\n\t\t\titer.Close()\n\t\t}\n\t\terr := bson.Unmarshal(docData, result)\n\t\tif err != nil {\n\t\t\tdebugf(\"Iter %p document unmarshaling failed: %#v\", iter, err)\n\t\t\titer.m.Lock()\n\t\t\tif iter.err == nil {\n\t\t\t\titer.err = err\n\t\t\t}\n\t\t\titer.m.Unlock()\n\t\t\treturn false\n\t\t}\n\t\tdebugf(\"Iter %p document unmarshaled: %#v\", iter, result)\n\t\t// XXX Only have to check first document for a query error?\n\t\terr = checkQueryError(iter.op.collection, docData)\n\t\tif err != nil {\n\t\t\titer.m.Lock()\n\t\t\tif iter.err == nil {\n\t\t\t\titer.err = err\n\t\t\t}\n\t\t\titer.m.Unlock()\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t} else if iter.err != nil {\n\t\tdebugf(\"Iter %p returning false: %s\", iter, iter.err)\n\t\titer.m.Unlock()\n\t\treturn false\n\t} else if iter.op.cursorId == 0 {\n\t\titer.err = ErrNotFound\n\t\tdebugf(\"Iter %p exhausted with cursor=0\", iter)\n\t\titer.m.Unlock()\n\t\treturn false\n\t}\n\n\tpanic(\"unreachable\")\n}\n\n// All retrieves all documents from the result set into the provided slice\n// and closes the iterator.\n//\n// The result argument must necessarily be the address for a slice. The slice\n// may be nil or previously allocated.\n//\n// WARNING: Obviously, All must not be used with result sets that may be\n// potentially large, since it may consume all memory until the system\n// crashes. Consider building the query with a Limit clause to ensure the\n// result size is bounded.\n//\n// For instance:\n//\n//    var result []struct{ Value int }\n//    iter := collection.Find(nil).Limit(100).Iter()\n//    err := iter.All(&result)\n//    if err != nil {\n//        return err\n//    }\n//\nfunc (iter *Iter) All(result interface{}) error {\n\tresultv := reflect.ValueOf(result)\n\tif resultv.Kind() != reflect.Ptr || resultv.Elem().Kind() != reflect.Slice {\n\t\tpanic(\"result argument must be a slice address\")\n\t}\n\tslicev := resultv.Elem()\n\tslicev = slicev.Slice(0, slicev.Cap())\n\telemt := slicev.Type().Elem()\n\ti := 0\n\tfor {\n\t\tif slicev.Len() == i {\n\t\t\telemp := reflect.New(elemt)\n\t\t\tif !iter.Next(elemp.Interface()) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tslicev = reflect.Append(slicev, elemp.Elem())\n\t\t\tslicev = slicev.Slice(0, slicev.Cap())\n\t\t} else {\n\t\t\tif !iter.Next(slicev.Index(i).Addr().Interface()) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\ti++\n\t}\n\tresultv.Elem().Set(slicev.Slice(0, i))\n\treturn iter.Close()\n}\n\n// All works like Iter.All.\nfunc (q *Query) All(result interface{}) error {\n\treturn q.Iter().All(result)\n}\n\n// The For method is obsolete and will be removed in a future release.\n// See Iter as an elegant replacement.\nfunc (q *Query) For(result interface{}, f func() error) error {\n\treturn q.Iter().For(result, f)\n}\n\n// The For method is obsolete and will be removed in a future release.\n// See Iter as an elegant replacement.\nfunc (iter *Iter) For(result interface{}, f func() error) (err error) {\n\tvalid := false\n\tv := reflect.ValueOf(result)\n\tif v.Kind() == reflect.Ptr {\n\t\tv = v.Elem()\n\t\tswitch v.Kind() {\n\t\tcase reflect.Map, reflect.Ptr, reflect.Interface, reflect.Slice:\n\t\t\tvalid = v.IsNil()\n\t\t}\n\t}\n\tif !valid {\n\t\tpanic(\"For needs a pointer to nil reference value.  See the documentation.\")\n\t}\n\tzero := reflect.Zero(v.Type())\n\tfor {\n\t\tv.Set(zero)\n\t\tif !iter.Next(result) {\n\t\t\tbreak\n\t\t}\n\t\terr = f()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn iter.Err()\n}\n\n// acquireSocket acquires a socket from the same server that the iterator\n// cursor was obtained from.\n//\n// WARNING: This method must not be called with iter.m locked. Acquiring the\n// socket depends on the cluster sync loop, and the cluster sync loop might\n// attempt actions which cause replyFunc to be called, inducing a deadlock.\nfunc (iter *Iter) acquireSocket() (*mongoSocket, error) {\n\tsocket, err := iter.session.acquireSocket(true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif socket.Server() != iter.server {\n\t\t// Socket server changed during iteration. This may happen\n\t\t// with Eventual sessions, if a Refresh is done, or if a\n\t\t// monotonic session gets a write and shifts from secondary\n\t\t// to primary. Our cursor is in a specific server, though.\n\t\titer.session.m.Lock()\n\t\tsockTimeout := iter.session.sockTimeout\n\t\titer.session.m.Unlock()\n\t\tsocket.Release()\n\t\tsocket, _, err = iter.server.AcquireSocket(0, sockTimeout)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\terr := iter.session.socketLogin(socket)\n\t\tif err != nil {\n\t\t\tsocket.Release()\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn socket, nil\n}\n\nfunc (iter *Iter) getMore() {\n\t// Increment now so that unlocking the iterator won't cause a\n\t// different goroutine to get here as well.\n\titer.docsToReceive++\n\titer.m.Unlock()\n\tsocket, err := iter.acquireSocket()\n\titer.m.Lock()\n\tif err != nil {\n\t\titer.err = err\n\t\treturn\n\t}\n\tdefer socket.Release()\n\n\tdebugf(\"Iter %p requesting more documents\", iter)\n\tif iter.limit > 0 {\n\t\t// The -1 below accounts for the fact docsToReceive was incremented above.\n\t\tlimit := iter.limit - int32(iter.docsToReceive-1) - int32(iter.docData.Len())\n\t\tif limit < iter.op.limit {\n\t\t\titer.op.limit = limit\n\t\t}\n\t}\n\tvar op interface{}\n\tif iter.findCmd {\n\t\top = iter.getMoreCmd()\n\t} else {\n\t\top = &iter.op\n\t}\n\tif err := socket.Query(op); err != nil {\n\t\titer.docsToReceive--\n\t\titer.err = err\n\t}\n}\n\nfunc (iter *Iter) getMoreCmd() *queryOp {\n\t// TODO: Define the query statically in the Iter type, next to getMoreOp.\n\tnameDot := strings.Index(iter.op.collection, \".\")\n\tif nameDot < 0 {\n\t\tpanic(\"invalid query collection name: \" + iter.op.collection)\n\t}\n\n\tgetMore := getMoreCmd{\n\t\tCursorId:   iter.op.cursorId,\n\t\tCollection: iter.op.collection[nameDot+1:],\n\t\tBatchSize:  iter.op.limit,\n\t}\n\n\tvar op queryOp\n\top.collection = iter.op.collection[:nameDot] + \".$cmd\"\n\top.query = &getMore\n\top.limit = -1\n\top.replyFunc = iter.op.replyFunc\n\treturn &op\n}\n\ntype countCmd struct {\n\tCount string\n\tQuery interface{}\n\tLimit int32 \",omitempty\"\n\tSkip  int32 \",omitempty\"\n}\n\n// Count returns the total number of documents in the result set.\nfunc (q *Query) Count() (n int, err error) {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op\n\tlimit := q.limit\n\tq.m.Unlock()\n\n\tc := strings.Index(op.collection, \".\")\n\tif c < 0 {\n\t\treturn 0, errors.New(\"Bad collection name: \" + op.collection)\n\t}\n\n\tdbname := op.collection[:c]\n\tcname := op.collection[c+1:]\n\tquery := op.query\n\tif query == nil {\n\t\tquery = bson.D{}\n\t}\n\tresult := struct{ N int }{}\n\terr = session.DB(dbname).Run(countCmd{cname, query, limit, op.skip}, &result)\n\treturn result.N, err\n}\n\n// Count returns the total number of documents in the collection.\nfunc (c *Collection) Count() (n int, err error) {\n\treturn c.Find(nil).Count()\n}\n\ntype distinctCmd struct {\n\tCollection string \"distinct\"\n\tKey        string\n\tQuery      interface{} \",omitempty\"\n}\n\n// Distinct unmarshals into result the list of distinct values for the given key.\n//\n// For example:\n//\n//     var result []int\n//     err := collection.Find(bson.M{\"gender\": \"F\"}).Distinct(\"age\", &result)\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/Aggregation\n//\nfunc (q *Query) Distinct(key string, result interface{}) error {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op // Copy.\n\tq.m.Unlock()\n\n\tc := strings.Index(op.collection, \".\")\n\tif c < 0 {\n\t\treturn errors.New(\"Bad collection name: \" + op.collection)\n\t}\n\n\tdbname := op.collection[:c]\n\tcname := op.collection[c+1:]\n\n\tvar doc struct{ Values bson.Raw }\n\terr := session.DB(dbname).Run(distinctCmd{cname, key, op.query}, &doc)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn doc.Values.Unmarshal(result)\n}\n\ntype mapReduceCmd struct {\n\tCollection string \"mapreduce\"\n\tMap        string \",omitempty\"\n\tReduce     string \",omitempty\"\n\tFinalize   string \",omitempty\"\n\tLimit      int32  \",omitempty\"\n\tOut        interface{}\n\tQuery      interface{} \",omitempty\"\n\tSort       interface{} \",omitempty\"\n\tScope      interface{} \",omitempty\"\n\tVerbose    bool        \",omitempty\"\n}\n\ntype mapReduceResult struct {\n\tResults    bson.Raw\n\tResult     bson.Raw\n\tTimeMillis int64 \"timeMillis\"\n\tCounts     struct{ Input, Emit, Output int }\n\tOk         bool\n\tErr        string\n\tTiming     *MapReduceTime\n}\n\ntype MapReduce struct {\n\tMap      string      // Map Javascript function code (required)\n\tReduce   string      // Reduce Javascript function code (required)\n\tFinalize string      // Finalize Javascript function code (optional)\n\tOut      interface{} // Output collection name or document. If nil, results are inlined into the result parameter.\n\tScope    interface{} // Optional global scope for Javascript functions\n\tVerbose  bool\n}\n\ntype MapReduceInfo struct {\n\tInputCount  int            // Number of documents mapped\n\tEmitCount   int            // Number of times reduce called emit\n\tOutputCount int            // Number of documents in resulting collection\n\tDatabase    string         // Output database, if results are not inlined\n\tCollection  string         // Output collection, if results are not inlined\n\tTime        int64          // Time to run the job, in nanoseconds\n\tVerboseTime *MapReduceTime // Only defined if Verbose was true\n}\n\ntype MapReduceTime struct {\n\tTotal    int64 // Total time, in nanoseconds\n\tMap      int64 \"mapTime\"  // Time within map function, in nanoseconds\n\tEmitLoop int64 \"emitLoop\" // Time within the emit/map loop, in nanoseconds\n}\n\n// MapReduce executes a map/reduce job for documents covered by the query.\n// That kind of job is suitable for very flexible bulk aggregation of data\n// performed at the server side via Javascript functions.\n//\n// Results from the job may be returned as a result of the query itself\n// through the result parameter in case they'll certainly fit in memory\n// and in a single document.  If there's the possibility that the amount\n// of data might be too large, results must be stored back in an alternative\n// collection or even a separate database, by setting the Out field of the\n// provided MapReduce job.  In that case, provide nil as the result parameter.\n//\n// These are some of the ways to set Out:\n//\n//     nil\n//         Inline results into the result parameter.\n//\n//     bson.M{\"replace\": \"mycollection\"}\n//         The output will be inserted into a collection which replaces any\n//         existing collection with the same name.\n//\n//     bson.M{\"merge\": \"mycollection\"}\n//         This option will merge new data into the old output collection. In\n//         other words, if the same key exists in both the result set and the\n//         old collection, the new key will overwrite the old one.\n//\n//     bson.M{\"reduce\": \"mycollection\"}\n//         If documents exist for a given key in the result set and in the old\n//         collection, then a reduce operation (using the specified reduce\n//         function) will be performed on the two values and the result will be\n//         written to the output collection. If a finalize function was\n//         provided, this will be run after the reduce as well.\n//\n//     bson.M{...., \"db\": \"mydb\"}\n//         Any of the above options can have the \"db\" key included for doing\n//         the respective action in a separate database.\n//\n// The following is a trivial example which will count the number of\n// occurrences of a field named n on each document in a collection, and\n// will return results inline:\n//\n//     job := &mgo.MapReduce{\n//             Map:      \"function() { emit(this.n, 1) }\",\n//             Reduce:   \"function(key, values) { return Array.sum(values) }\",\n//     }\n//     var result []struct { Id int \"_id\"; Value int }\n//     _, err := collection.Find(nil).MapReduce(job, &result)\n//     if err != nil {\n//         return err\n//     }\n//     for _, item := range result {\n//         fmt.Println(item.Value)\n//     }\n//\n// This function is compatible with MongoDB 1.7.4+.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/MapReduce\n//\nfunc (q *Query) MapReduce(job *MapReduce, result interface{}) (info *MapReduceInfo, err error) {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op // Copy.\n\tlimit := q.limit\n\tq.m.Unlock()\n\n\tc := strings.Index(op.collection, \".\")\n\tif c < 0 {\n\t\treturn nil, errors.New(\"Bad collection name: \" + op.collection)\n\t}\n\n\tdbname := op.collection[:c]\n\tcname := op.collection[c+1:]\n\n\tcmd := mapReduceCmd{\n\t\tCollection: cname,\n\t\tMap:        job.Map,\n\t\tReduce:     job.Reduce,\n\t\tFinalize:   job.Finalize,\n\t\tOut:        fixMROut(job.Out),\n\t\tScope:      job.Scope,\n\t\tVerbose:    job.Verbose,\n\t\tQuery:      op.query,\n\t\tSort:       op.options.OrderBy,\n\t\tLimit:      limit,\n\t}\n\n\tif cmd.Out == nil {\n\t\tcmd.Out = bson.D{{\"inline\", 1}}\n\t}\n\n\tvar doc mapReduceResult\n\terr = session.DB(dbname).Run(&cmd, &doc)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif doc.Err != \"\" {\n\t\treturn nil, errors.New(doc.Err)\n\t}\n\n\tinfo = &MapReduceInfo{\n\t\tInputCount:  doc.Counts.Input,\n\t\tEmitCount:   doc.Counts.Emit,\n\t\tOutputCount: doc.Counts.Output,\n\t\tTime:        doc.TimeMillis * 1e6,\n\t}\n\n\tif doc.Result.Kind == 0x02 {\n\t\terr = doc.Result.Unmarshal(&info.Collection)\n\t\tinfo.Database = dbname\n\t} else if doc.Result.Kind == 0x03 {\n\t\tvar v struct{ Collection, Db string }\n\t\terr = doc.Result.Unmarshal(&v)\n\t\tinfo.Collection = v.Collection\n\t\tinfo.Database = v.Db\n\t}\n\n\tif doc.Timing != nil {\n\t\tinfo.VerboseTime = doc.Timing\n\t\tinfo.VerboseTime.Total *= 1e6\n\t\tinfo.VerboseTime.Map *= 1e6\n\t\tinfo.VerboseTime.EmitLoop *= 1e6\n\t}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif result != nil {\n\t\treturn info, doc.Results.Unmarshal(result)\n\t}\n\treturn info, nil\n}\n\n// The \"out\" option in the MapReduce command must be ordered. This was\n// found after the implementation was accepting maps for a long time,\n// so rather than breaking the API, we'll fix the order if necessary.\n// Details about the order requirement may be seen in MongoDB's code:\n//\n//     http://goo.gl/L8jwJX\n//\nfunc fixMROut(out interface{}) interface{} {\n\toutv := reflect.ValueOf(out)\n\tif outv.Kind() != reflect.Map || outv.Type().Key() != reflect.TypeOf(\"\") {\n\t\treturn out\n\t}\n\touts := make(bson.D, outv.Len())\n\n\toutTypeIndex := -1\n\tfor i, k := range outv.MapKeys() {\n\t\tks := k.String()\n\t\touts[i].Name = ks\n\t\touts[i].Value = outv.MapIndex(k).Interface()\n\t\tswitch ks {\n\t\tcase \"normal\", \"replace\", \"merge\", \"reduce\", \"inline\":\n\t\t\toutTypeIndex = i\n\t\t}\n\t}\n\tif outTypeIndex > 0 {\n\t\touts[0], outs[outTypeIndex] = outs[outTypeIndex], outs[0]\n\t}\n\treturn outs\n}\n\n// Change holds fields for running a findAndModify MongoDB command via\n// the Query.Apply method.\ntype Change struct {\n\tUpdate    interface{} // The update document\n\tUpsert    bool        // Whether to insert in case the document isn't found\n\tRemove    bool        // Whether to remove the document found rather than updating\n\tReturnNew bool        // Should the modified document be returned rather than the old one\n}\n\ntype findModifyCmd struct {\n\tCollection                  string      \"findAndModify\"\n\tQuery, Update, Sort, Fields interface{} \",omitempty\"\n\tUpsert, Remove, New         bool        \",omitempty\"\n}\n\ntype valueResult struct {\n\tValue     bson.Raw\n\tLastError LastError \"lastErrorObject\"\n}\n\n// Apply runs the findAndModify MongoDB command, which allows updating, upserting\n// or removing a document matching a query and atomically returning either the old\n// version (the default) or the new version of the document (when ReturnNew is true).\n// If no objects are found Apply returns ErrNotFound.\n//\n// The Sort and Select query methods affect the result of Apply.  In case\n// multiple documents match the query, Sort enables selecting which document to\n// act upon by ordering it first.  Select enables retrieving only a selection\n// of fields of the new or old document.\n//\n// This simple example increments a counter and prints its new value:\n//\n//     change := mgo.Change{\n//             Update: bson.M{\"$inc\": bson.M{\"n\": 1}},\n//             ReturnNew: true,\n//     }\n//     info, err = col.Find(M{\"_id\": id}).Apply(change, &doc)\n//     fmt.Println(doc.N)\n//\n// This method depends on MongoDB >= 2.0 to work properly.\n//\n// Relevant documentation:\n//\n//     http://www.mongodb.org/display/DOCS/findAndModify+Command\n//     http://www.mongodb.org/display/DOCS/Updating\n//     http://www.mongodb.org/display/DOCS/Atomic+Operations\n//\nfunc (q *Query) Apply(change Change, result interface{}) (info *ChangeInfo, err error) {\n\tq.m.Lock()\n\tsession := q.session\n\top := q.op // Copy.\n\tq.m.Unlock()\n\n\tc := strings.Index(op.collection, \".\")\n\tif c < 0 {\n\t\treturn nil, errors.New(\"bad collection name: \" + op.collection)\n\t}\n\n\tdbname := op.collection[:c]\n\tcname := op.collection[c+1:]\n\n\tcmd := findModifyCmd{\n\t\tCollection: cname,\n\t\tUpdate:     change.Update,\n\t\tUpsert:     change.Upsert,\n\t\tRemove:     change.Remove,\n\t\tNew:        change.ReturnNew,\n\t\tQuery:      op.query,\n\t\tSort:       op.options.OrderBy,\n\t\tFields:     op.selector,\n\t}\n\n\tsession = session.Clone()\n\tdefer session.Close()\n\tsession.SetMode(Strong, false)\n\n\tvar doc valueResult\n\tfor i := 0; i < maxUpsertRetries; i++ {\n\t\terr = session.DB(dbname).Run(&cmd, &doc)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif change.Upsert && IsDup(err) && i+1 < maxUpsertRetries {\n\t\t\t// Retry duplicate key errors on upserts.\n\t\t\t// https://docs.mongodb.com/v3.2/reference/method/db.collection.update/#use-unique-indexes\n\t\t\tcontinue\n\t\t}\n\t\tif qerr, ok := err.(*QueryError); ok && qerr.Message == \"No matching object found\" {\n\t\t\treturn nil, ErrNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\tif doc.LastError.N == 0 {\n\t\treturn nil, ErrNotFound\n\t}\n\tif doc.Value.Kind != 0x0A && result != nil {\n\t\terr = doc.Value.Unmarshal(result)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tinfo = &ChangeInfo{}\n\tlerr := &doc.LastError\n\tif lerr.UpdatedExisting {\n\t\tinfo.Updated = lerr.N\n\t\tinfo.Matched = lerr.N\n\t} else if change.Remove {\n\t\tinfo.Removed = lerr.N\n\t\tinfo.Matched = lerr.N\n\t} else if change.Upsert {\n\t\tinfo.UpsertedId = lerr.UpsertedId\n\t}\n\treturn info, nil\n}\n\n// The BuildInfo type encapsulates details about the running MongoDB server.\n//\n// Note that the VersionArray field was introduced in MongoDB 2.0+, but it is\n// internally assembled from the Version information for previous versions.\n// In both cases, VersionArray is guaranteed to have at least 4 entries.\ntype BuildInfo struct {\n\tVersion        string\n\tVersionArray   []int  `bson:\"versionArray\"` // On MongoDB 2.0+; assembled from Version otherwise\n\tGitVersion     string `bson:\"gitVersion\"`\n\tOpenSSLVersion string `bson:\"OpenSSLVersion\"`\n\tSysInfo        string `bson:\"sysInfo\"` // Deprecated and empty on MongoDB 3.2+.\n\tBits           int\n\tDebug          bool\n\tMaxObjectSize  int `bson:\"maxBsonObjectSize\"`\n}\n\n// VersionAtLeast returns whether the BuildInfo version is greater than or\n// equal to the provided version number. If more than one number is\n// provided, numbers will be considered as major, minor, and so on.\nfunc (bi *BuildInfo) VersionAtLeast(version ...int) bool {\n\tfor i, vi := range version {\n\t\tif i == len(bi.VersionArray) {\n\t\t\treturn false\n\t\t}\n\t\tif bivi := bi.VersionArray[i]; bivi != vi {\n\t\t\treturn bivi >= vi\n\t\t}\n\t}\n\treturn true\n}\n\n// BuildInfo retrieves the version and other details about the\n// running MongoDB server.\nfunc (s *Session) BuildInfo() (info BuildInfo, err error) {\n\terr = s.Run(bson.D{{\"buildInfo\", \"1\"}}, &info)\n\tif len(info.VersionArray) == 0 {\n\t\tfor _, a := range strings.Split(info.Version, \".\") {\n\t\t\ti, err := strconv.Atoi(a)\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tinfo.VersionArray = append(info.VersionArray, i)\n\t\t}\n\t}\n\tfor len(info.VersionArray) < 4 {\n\t\tinfo.VersionArray = append(info.VersionArray, 0)\n\t}\n\tif i := strings.IndexByte(info.GitVersion, ' '); i >= 0 {\n\t\t// Strip off the \" modules: enterprise\" suffix. This is a _git version_.\n\t\t// That information may be moved to another field if people need it.\n\t\tinfo.GitVersion = info.GitVersion[:i]\n\t}\n\tif info.SysInfo == \"deprecated\" {\n\t\tinfo.SysInfo = \"\"\n\t}\n\treturn\n}\n\n// ---------------------------------------------------------------------------\n// Internal session handling helpers.\n\nfunc (s *Session) acquireSocket(slaveOk bool) (*mongoSocket, error) {\n\n\t// Read-only lock to check for previously reserved socket.\n\ts.m.RLock()\n\t// If there is a slave socket reserved and its use is acceptable, take it as long\n\t// as there isn't a master socket which would be preferred by the read preference mode.\n\tif s.slaveSocket != nil && s.slaveOk && slaveOk && (s.masterSocket == nil || s.consistency != PrimaryPreferred && s.consistency != Monotonic) {\n\t\tsocket := s.slaveSocket\n\t\tsocket.Acquire()\n\t\ts.m.RUnlock()\n\t\treturn socket, nil\n\t}\n\tif s.masterSocket != nil {\n\t\tsocket := s.masterSocket\n\t\tsocket.Acquire()\n\t\ts.m.RUnlock()\n\t\treturn socket, nil\n\t}\n\ts.m.RUnlock()\n\n\t// No go.  We may have to request a new socket and change the session,\n\t// so try again but with an exclusive lock now.\n\ts.m.Lock()\n\tdefer s.m.Unlock()\n\n\tif s.slaveSocket != nil && s.slaveOk && slaveOk && (s.masterSocket == nil || s.consistency != PrimaryPreferred && s.consistency != Monotonic) {\n\t\ts.slaveSocket.Acquire()\n\t\treturn s.slaveSocket, nil\n\t}\n\tif s.masterSocket != nil {\n\t\ts.masterSocket.Acquire()\n\t\treturn s.masterSocket, nil\n\t}\n\n\t// Still not good.  We need a new socket.\n\tsock, err := s.cluster().AcquireSocket(s.consistency, slaveOk && s.slaveOk, s.syncTimeout, s.sockTimeout, s.queryConfig.op.serverTags, s.poolLimit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Authenticate the new socket.\n\tif err = s.socketLogin(sock); err != nil {\n\t\tsock.Release()\n\t\treturn nil, err\n\t}\n\n\t// Keep track of the new socket, if necessary.\n\t// Note that, as a special case, if the Eventual session was\n\t// not refreshed (s.slaveSocket != nil), it means the developer\n\t// asked to preserve an existing reserved socket, so we'll\n\t// keep a master one around too before a Refresh happens.\n\tif s.consistency != Eventual || s.slaveSocket != nil {\n\t\ts.setSocket(sock)\n\t}\n\n\t// Switch over a Monotonic session to the master.\n\tif !slaveOk && s.consistency == Monotonic {\n\t\ts.slaveOk = false\n\t}\n\n\treturn sock, nil\n}\n\n// setSocket binds socket to this section.\nfunc (s *Session) setSocket(socket *mongoSocket) {\n\tinfo := socket.Acquire()\n\tif info.Master {\n\t\tif s.masterSocket != nil {\n\t\t\tpanic(\"setSocket(master) with existing master socket reserved\")\n\t\t}\n\t\ts.masterSocket = socket\n\t} else {\n\t\tif s.slaveSocket != nil {\n\t\t\tpanic(\"setSocket(slave) with existing slave socket reserved\")\n\t\t}\n\t\ts.slaveSocket = socket\n\t}\n}\n\n// unsetSocket releases any slave and/or master sockets reserved.\nfunc (s *Session) unsetSocket() {\n\tif s.masterSocket != nil {\n\t\ts.masterSocket.Release()\n\t}\n\tif s.slaveSocket != nil {\n\t\ts.slaveSocket.Release()\n\t}\n\ts.masterSocket = nil\n\ts.slaveSocket = nil\n}\n\nfunc (iter *Iter) replyFunc() replyFunc {\n\treturn func(err error, op *replyOp, docNum int, docData []byte) {\n\t\titer.m.Lock()\n\t\titer.docsToReceive--\n\t\tif err != nil {\n\t\t\titer.err = err\n\t\t\tdebugf(\"Iter %p received an error: %s\", iter, err.Error())\n\t\t} else if docNum == -1 {\n\t\t\tdebugf(\"Iter %p received no documents (cursor=%d).\", iter, op.cursorId)\n\t\t\tif op != nil && op.cursorId != 0 {\n\t\t\t\t// It's a tailable cursor.\n\t\t\t\titer.op.cursorId = op.cursorId\n\t\t\t} else if op != nil && op.cursorId == 0 && op.flags&1 == 1 {\n\t\t\t\t// Cursor likely timed out.\n\t\t\t\titer.err = ErrCursor\n\t\t\t} else {\n\t\t\t\titer.err = ErrNotFound\n\t\t\t}\n\t\t} else if iter.findCmd {\n\t\t\tdebugf(\"Iter %p received reply document %d/%d (cursor=%d)\", iter, docNum+1, int(op.replyDocs), op.cursorId)\n\t\t\tvar findReply struct {\n\t\t\t\tOk     bool\n\t\t\t\tCode   int\n\t\t\t\tErrmsg string\n\t\t\t\tCursor cursorData\n\t\t\t}\n\t\t\tif err := bson.Unmarshal(docData, &findReply); err != nil {\n\t\t\t\titer.err = err\n\t\t\t} else if !findReply.Ok && findReply.Errmsg != \"\" {\n\t\t\t\titer.err = &QueryError{Code: findReply.Code, Message: findReply.Errmsg}\n\t\t\t} else if len(findReply.Cursor.FirstBatch) == 0 && len(findReply.Cursor.NextBatch) == 0 {\n\t\t\t\titer.err = ErrNotFound\n\t\t\t} else {\n\t\t\t\tbatch := findReply.Cursor.FirstBatch\n\t\t\t\tif len(batch) == 0 {\n\t\t\t\t\tbatch = findReply.Cursor.NextBatch\n\t\t\t\t}\n\t\t\t\trdocs := len(batch)\n\t\t\t\tfor _, raw := range batch {\n\t\t\t\t\titer.docData.Push(raw.Data)\n\t\t\t\t}\n\t\t\t\titer.docsToReceive = 0\n\t\t\t\tdocsToProcess := iter.docData.Len()\n\t\t\t\tif iter.limit == 0 || int32(docsToProcess) < iter.limit {\n\t\t\t\t\titer.docsBeforeMore = docsToProcess - int(iter.prefetch*float64(rdocs))\n\t\t\t\t} else {\n\t\t\t\t\titer.docsBeforeMore = -1\n\t\t\t\t}\n\t\t\t\titer.op.cursorId = findReply.Cursor.Id\n\t\t\t}\n\t\t} else {\n\t\t\trdocs := int(op.replyDocs)\n\t\t\tif docNum == 0 {\n\t\t\t\titer.docsToReceive += rdocs - 1\n\t\t\t\tdocsToProcess := iter.docData.Len() + rdocs\n\t\t\t\tif iter.limit == 0 || int32(docsToProcess) < iter.limit {\n\t\t\t\t\titer.docsBeforeMore = docsToProcess - int(iter.prefetch*float64(rdocs))\n\t\t\t\t} else {\n\t\t\t\t\titer.docsBeforeMore = -1\n\t\t\t\t}\n\t\t\t\titer.op.cursorId = op.cursorId\n\t\t\t}\n\t\t\tdebugf(\"Iter %p received reply document %d/%d (cursor=%d)\", iter, docNum+1, rdocs, op.cursorId)\n\t\t\titer.docData.Push(docData)\n\t\t}\n\t\titer.gotReply.Broadcast()\n\t\titer.m.Unlock()\n\t}\n}\n\ntype writeCmdResult struct {\n\tOk        bool\n\tN         int\n\tNModified int `bson:\"nModified\"`\n\tUpserted  []struct {\n\t\tIndex int\n\t\tId    interface{} `_id`\n\t}\n\tConcernError writeConcernError `bson:\"writeConcernError\"`\n\tErrors       []writeCmdError   `bson:\"writeErrors\"`\n}\n\ntype writeConcernError struct {\n\tCode   int\n\tErrMsg string\n}\n\ntype writeCmdError struct {\n\tIndex  int\n\tCode   int\n\tErrMsg string\n}\n\nfunc (r *writeCmdResult) BulkErrorCases() []BulkErrorCase {\n\tecases := make([]BulkErrorCase, len(r.Errors))\n\tfor i, err := range r.Errors {\n\t\tecases[i] = BulkErrorCase{err.Index, &QueryError{Code: err.Code, Message: err.ErrMsg}}\n\t}\n\treturn ecases\n}\n\n// writeOp runs the given modifying operation, potentially followed up\n// by a getLastError command in case the session is in safe mode.  The\n// LastError result is made available in lerr, and if lerr.Err is set it\n// will also be returned as err.\nfunc (c *Collection) writeOp(op interface{}, ordered bool) (lerr *LastError, err error) {\n\ts := c.Database.Session\n\tsocket, err := s.acquireSocket(c.Database.Name == \"local\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer socket.Release()\n\n\ts.m.RLock()\n\tsafeOp := s.safeOp\n\tbypassValidation := s.bypassValidation\n\ts.m.RUnlock()\n\n\tif socket.ServerInfo().MaxWireVersion >= 2 {\n\t\t// Servers with a more recent write protocol benefit from write commands.\n\t\tif op, ok := op.(*insertOp); ok && len(op.documents) > 1000 {\n\t\t\tvar lerr LastError\n\n\t\t\t// Maximum batch size is 1000. Must split out in separate operations for compatibility.\n\t\t\tall := op.documents\n\t\t\tfor i := 0; i < len(all); i += 1000 {\n\t\t\t\tl := i + 1000\n\t\t\t\tif l > len(all) {\n\t\t\t\t\tl = len(all)\n\t\t\t\t}\n\t\t\t\top.documents = all[i:l]\n\t\t\t\toplerr, err := c.writeOpCommand(socket, safeOp, op, ordered, bypassValidation)\n\t\t\t\tlerr.N += oplerr.N\n\t\t\t\tlerr.modified += oplerr.modified\n\t\t\t\tif err != nil {\n\t\t\t\t\tfor ei := range oplerr.ecases {\n\t\t\t\t\t\toplerr.ecases[ei].Index += i\n\t\t\t\t\t}\n\t\t\t\t\tlerr.ecases = append(lerr.ecases, oplerr.ecases...)\n\t\t\t\t\tif op.flags&1 == 0 {\n\t\t\t\t\t\treturn &lerr, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(lerr.ecases) != 0 {\n\t\t\t\treturn &lerr, lerr.ecases[0].Err\n\t\t\t}\n\t\t\treturn &lerr, nil\n\t\t}\n\t\treturn c.writeOpCommand(socket, safeOp, op, ordered, bypassValidation)\n\t} else if updateOps, ok := op.(bulkUpdateOp); ok {\n\t\tvar lerr LastError\n\t\tfor i, updateOp := range updateOps {\n\t\t\toplerr, err := c.writeOpQuery(socket, safeOp, updateOp, ordered)\n\t\t\tlerr.N += oplerr.N\n\t\t\tlerr.modified += oplerr.modified\n\t\t\tif err != nil {\n\t\t\t\tlerr.ecases = append(lerr.ecases, BulkErrorCase{i, err})\n\t\t\t\tif ordered {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(lerr.ecases) != 0 {\n\t\t\treturn &lerr, lerr.ecases[0].Err\n\t\t}\n\t\treturn &lerr, nil\n\t} else if deleteOps, ok := op.(bulkDeleteOp); ok {\n\t\tvar lerr LastError\n\t\tfor i, deleteOp := range deleteOps {\n\t\t\toplerr, err := c.writeOpQuery(socket, safeOp, deleteOp, ordered)\n\t\t\tlerr.N += oplerr.N\n\t\t\tlerr.modified += oplerr.modified\n\t\t\tif err != nil {\n\t\t\t\tlerr.ecases = append(lerr.ecases, BulkErrorCase{i, err})\n\t\t\t\tif ordered {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(lerr.ecases) != 0 {\n\t\t\treturn &lerr, lerr.ecases[0].Err\n\t\t}\n\t\treturn &lerr, nil\n\t}\n\treturn c.writeOpQuery(socket, safeOp, op, ordered)\n}\n\nfunc (c *Collection) writeOpQuery(socket *mongoSocket, safeOp *queryOp, op interface{}, ordered bool) (lerr *LastError, err error) {\n\tif safeOp == nil {\n\t\treturn nil, socket.Query(op)\n\t}\n\n\tvar mutex sync.Mutex\n\tvar replyData []byte\n\tvar replyErr error\n\tmutex.Lock()\n\tquery := *safeOp // Copy the data.\n\tquery.collection = c.Database.Name + \".$cmd\"\n\tquery.replyFunc = func(err error, reply *replyOp, docNum int, docData []byte) {\n\t\treplyData = docData\n\t\treplyErr = err\n\t\tmutex.Unlock()\n\t}\n\terr = socket.Query(op, &query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmutex.Lock() // Wait.\n\tif replyErr != nil {\n\t\treturn nil, replyErr // XXX TESTME\n\t}\n\tif hasErrMsg(replyData) {\n\t\t// Looks like getLastError itself failed.\n\t\terr = checkQueryError(query.collection, replyData)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tresult := &LastError{}\n\tbson.Unmarshal(replyData, &result)\n\tdebugf(\"Result from writing query: %#v\", result)\n\tif result.Err != \"\" {\n\t\tresult.ecases = []BulkErrorCase{{Index: 0, Err: result}}\n\t\tif insert, ok := op.(*insertOp); ok && len(insert.documents) > 1 {\n\t\t\tresult.ecases[0].Index = -1\n\t\t}\n\t\treturn result, result\n\t}\n\t// With MongoDB <2.6 we don't know how many actually changed, so make it the same as matched.\n\tresult.modified = result.N\n\treturn result, nil\n}\n\nfunc (c *Collection) writeOpCommand(socket *mongoSocket, safeOp *queryOp, op interface{}, ordered, bypassValidation bool) (lerr *LastError, err error) {\n\tvar writeConcern interface{}\n\tif safeOp == nil {\n\t\twriteConcern = bson.D{{\"w\", 0}}\n\t} else {\n\t\twriteConcern = safeOp.query.(*getLastError)\n\t}\n\n\tvar cmd bson.D\n\tswitch op := op.(type) {\n\tcase *insertOp:\n\t\t// http://docs.mongodb.org/manual/reference/command/insert\n\t\tcmd = bson.D{\n\t\t\t{\"insert\", c.Name},\n\t\t\t{\"documents\", op.documents},\n\t\t\t{\"writeConcern\", writeConcern},\n\t\t\t{\"ordered\", op.flags&1 == 0},\n\t\t}\n\tcase *updateOp:\n\t\t// http://docs.mongodb.org/manual/reference/command/update\n\t\tcmd = bson.D{\n\t\t\t{\"update\", c.Name},\n\t\t\t{\"updates\", []interface{}{op}},\n\t\t\t{\"writeConcern\", writeConcern},\n\t\t\t{\"ordered\", ordered},\n\t\t}\n\tcase bulkUpdateOp:\n\t\t// http://docs.mongodb.org/manual/reference/command/update\n\t\tcmd = bson.D{\n\t\t\t{\"update\", c.Name},\n\t\t\t{\"updates\", op},\n\t\t\t{\"writeConcern\", writeConcern},\n\t\t\t{\"ordered\", ordered},\n\t\t}\n\tcase *deleteOp:\n\t\t// http://docs.mongodb.org/manual/reference/command/delete\n\t\tcmd = bson.D{\n\t\t\t{\"delete\", c.Name},\n\t\t\t{\"deletes\", []interface{}{op}},\n\t\t\t{\"writeConcern\", writeConcern},\n\t\t\t{\"ordered\", ordered},\n\t\t}\n\tcase bulkDeleteOp:\n\t\t// http://docs.mongodb.org/manual/reference/command/delete\n\t\tcmd = bson.D{\n\t\t\t{\"delete\", c.Name},\n\t\t\t{\"deletes\", op},\n\t\t\t{\"writeConcern\", writeConcern},\n\t\t\t{\"ordered\", ordered},\n\t\t}\n\t}\n\tif bypassValidation {\n\t\tcmd = append(cmd, bson.DocElem{\"bypassDocumentValidation\", true})\n\t}\n\n\tvar result writeCmdResult\n\terr = c.Database.run(socket, cmd, &result)\n\tdebugf(\"Write command result: %#v (err=%v)\", result, err)\n\tecases := result.BulkErrorCases()\n\tlerr = &LastError{\n\t\tUpdatedExisting: result.N > 0 && len(result.Upserted) == 0,\n\t\tN:               result.N,\n\n\t\tmodified: result.NModified,\n\t\tecases:   ecases,\n\t}\n\tif len(result.Upserted) > 0 {\n\t\tlerr.UpsertedId = result.Upserted[0].Id\n\t}\n\tif len(result.Errors) > 0 {\n\t\te := result.Errors[0]\n\t\tlerr.Code = e.Code\n\t\tlerr.Err = e.ErrMsg\n\t\terr = lerr\n\t} else if result.ConcernError.Code != 0 {\n\t\te := result.ConcernError\n\t\tlerr.Code = e.Code\n\t\tlerr.Err = e.ErrMsg\n\t\terr = lerr\n\t}\n\n\tif err == nil && safeOp == nil {\n\t\treturn nil, nil\n\t}\n\treturn lerr, err\n}\n\nfunc hasErrMsg(d []byte) bool {\n\tl := len(d)\n\tfor i := 0; i+8 < l; i++ {\n\t\tif d[i] == '\\x02' && d[i+1] == 'e' && d[i+2] == 'r' && d[i+3] == 'r' && d[i+4] == 'm' && d[i+5] == 's' && d[i+6] == 'g' && d[i+7] == '\\x00' {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n"
        },
        {
          "name": "session_test.go",
          "type": "blob",
          "size": 102.5615234375,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\nfunc (s *S) TestRunString(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tresult := struct{ Ok int }{}\n\terr = session.Run(\"ping\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, 1)\n}\n\nfunc (s *S) TestRunValue(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tresult := struct{ Ok int }{}\n\terr = session.Run(M{\"ping\": 1}, &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, 1)\n}\n\nfunc (s *S) TestPing(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Just ensure the nonce has been received.\n\tresult := struct{}{}\n\terr = session.Run(\"ping\", &result)\n\n\tmgo.ResetStats()\n\n\terr = session.Ping()\n\tc.Assert(err, IsNil)\n\n\t// Pretty boring.\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 1)\n\tc.Assert(stats.ReceivedOps, Equals, 1)\n}\n\nfunc (s *S) TestDialIPAddress(c *C) {\n\tsession, err := mgo.Dial(\"127.0.0.1:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tif os.Getenv(\"NOIPV6\") != \"1\" {\n\t\tsession, err = mgo.Dial(\"[::1%]:40001\")\n\t\tc.Assert(err, IsNil)\n\t\tdefer session.Close()\n\t}\n}\n\nfunc (s *S) TestURLSingle(c *C) {\n\tsession, err := mgo.Dial(\"mongodb://localhost:40001/\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tresult := struct{ Ok int }{}\n\terr = session.Run(\"ping\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, 1)\n}\n\nfunc (s *S) TestURLMany(c *C) {\n\tsession, err := mgo.Dial(\"mongodb://localhost:40011,localhost:40012/\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tresult := struct{ Ok int }{}\n\terr = session.Run(\"ping\", &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, 1)\n}\n\nfunc (s *S) TestURLParsing(c *C) {\n\turls := []string{\n\t\t\"localhost:40001?foo=1&bar=2\",\n\t\t\"localhost:40001?foo=1;bar=2\",\n\t}\n\tfor _, url := range urls {\n\t\tsession, err := mgo.Dial(url)\n\t\tif session != nil {\n\t\t\tsession.Close()\n\t\t}\n\t\tc.Assert(err, ErrorMatches, \"unsupported connection URL option: (foo=1|bar=2)\")\n\t}\n}\n\nfunc (s *S) TestInsertFindOne(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1, \"b\": 2})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 1, \"b\": 3})\n\tc.Assert(err, IsNil)\n\n\tresult := struct{ A, B int }{}\n\n\terr = coll.Find(M{\"a\": 1}).Sort(\"b\").One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.A, Equals, 1)\n\tc.Assert(result.B, Equals, 2)\n\n\terr = coll.Find(M{\"a\": 1}).Sort(\"-b\").One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.A, Equals, 1)\n\tc.Assert(result.B, Equals, 3)\n}\n\nfunc (s *S) TestInsertFindOneNil(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Find(nil).One(nil)\n\tc.Assert(err, ErrorMatches, \"unauthorized.*|not authorized.*\")\n}\n\nfunc (s *S) TestInsertFindOneMap(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1, \"b\": 2})\n\tc.Assert(err, IsNil)\n\tresult := make(M)\n\terr = coll.Find(M{\"a\": 1}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"a\"], Equals, 1)\n\tc.Assert(result[\"b\"], Equals, 2)\n}\n\nfunc (s *S) TestInsertFindAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"a\": 1, \"b\": 2})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 3, \"b\": 4})\n\tc.Assert(err, IsNil)\n\n\ttype R struct{ A, B int }\n\tvar result []R\n\n\tassertResult := func() {\n\t\tc.Assert(len(result), Equals, 2)\n\t\tc.Assert(result[0].A, Equals, 1)\n\t\tc.Assert(result[0].B, Equals, 2)\n\t\tc.Assert(result[1].A, Equals, 3)\n\t\tc.Assert(result[1].B, Equals, 4)\n\t}\n\n\t// nil slice\n\terr = coll.Find(nil).Sort(\"a\").All(&result)\n\tc.Assert(err, IsNil)\n\tassertResult()\n\n\t// Previously allocated slice\n\tallocd := make([]R, 5)\n\tresult = allocd\n\terr = coll.Find(nil).Sort(\"a\").All(&result)\n\tc.Assert(err, IsNil)\n\tassertResult()\n\n\t// Ensure result is backed by the originally allocated array\n\tc.Assert(&result[0], Equals, &allocd[0])\n\n\t// Non-pointer slice error\n\tf := func() { coll.Find(nil).All(result) }\n\tc.Assert(f, Panics, \"result argument must be a slice address\")\n\n\t// Non-slice error\n\tf = func() { coll.Find(nil).All(new(int)) }\n\tc.Assert(f, Panics, \"result argument must be a slice address\")\n}\n\nfunc (s *S) TestFindRef(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb1 := session.DB(\"db1\")\n\tdb1col1 := db1.C(\"col1\")\n\n\tdb2 := session.DB(\"db2\")\n\tdb2col1 := db2.C(\"col1\")\n\n\terr = db1col1.Insert(M{\"_id\": 1, \"n\": 1})\n\tc.Assert(err, IsNil)\n\terr = db1col1.Insert(M{\"_id\": 2, \"n\": 2})\n\tc.Assert(err, IsNil)\n\terr = db2col1.Insert(M{\"_id\": 2, \"n\": 3})\n\tc.Assert(err, IsNil)\n\n\tresult := struct{ N int }{}\n\n\tref1 := &mgo.DBRef{Collection: \"col1\", Id: 1}\n\tref2 := &mgo.DBRef{Collection: \"col1\", Id: 2, Database: \"db2\"}\n\n\terr = db1.FindRef(ref1).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 1)\n\n\terr = db1.FindRef(ref2).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 3)\n\n\terr = db2.FindRef(ref1).One(&result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = db2.FindRef(ref2).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 3)\n\n\terr = session.FindRef(ref2).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 3)\n\n\tf := func() { session.FindRef(ref1).One(&result) }\n\tc.Assert(f, PanicMatches, \"Can't resolve database for &mgo.DBRef{Collection:\\\"col1\\\", Id:1, Database:\\\"\\\"}\")\n}\n\nfunc (s *S) TestDatabaseAndCollectionNames(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb1 := session.DB(\"db1\")\n\tdb1col1 := db1.C(\"col1\")\n\tdb1col2 := db1.C(\"col2\")\n\n\tdb2 := session.DB(\"db2\")\n\tdb2col1 := db2.C(\"col3\")\n\n\terr = db1col1.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\terr = db1col2.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\terr = db2col1.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\tnames, err := session.DatabaseNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(filterDBs(names), DeepEquals, []string{\"db1\", \"db2\"})\n\n\t// Try to exercise cursor logic. 2.8.0-rc3 still ignores this.\n\tsession.SetBatch(2)\n\n\tnames, err = db1.CollectionNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(names, DeepEquals, []string{\"col1\", \"col2\", \"system.indexes\"})\n\n\tnames, err = db2.CollectionNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(names, DeepEquals, []string{\"col3\", \"system.indexes\"})\n}\n\nfunc (s *S) TestSelect(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.Insert(M{\"a\": 1, \"b\": 2})\n\n\tresult := struct{ A, B int }{}\n\n\terr = coll.Find(M{\"a\": 1}).Select(M{\"b\": 1}).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.A, Equals, 0)\n\tc.Assert(result.B, Equals, 2)\n}\n\nfunc (s *S) TestInlineMap(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tvar v, result1 struct {\n\t\tA int\n\t\tM map[string]int \",inline\"\n\t}\n\n\tv.A = 1\n\tv.M = map[string]int{\"b\": 2}\n\terr = coll.Insert(v)\n\tc.Assert(err, IsNil)\n\n\tnoId := M{\"_id\": 0}\n\n\terr = coll.Find(nil).Select(noId).One(&result1)\n\tc.Assert(err, IsNil)\n\tc.Assert(result1.A, Equals, 1)\n\tc.Assert(result1.M, DeepEquals, map[string]int{\"b\": 2})\n\n\tvar result2 M\n\terr = coll.Find(nil).Select(noId).One(&result2)\n\tc.Assert(err, IsNil)\n\tc.Assert(result2, DeepEquals, M{\"a\": 1, \"b\": 2})\n\n}\n\nfunc (s *S) TestUpdate(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"k\": n, \"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\t// No changes is a no-op and shouldn't return an error.\n\terr = coll.Update(M{\"k\": 42}, M{\"$set\": M{\"n\": 42}})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Update(M{\"k\": 42}, M{\"$inc\": M{\"n\": 1}})\n\tc.Assert(err, IsNil)\n\n\tresult := make(M)\n\terr = coll.Find(M{\"k\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 43)\n\n\terr = coll.Update(M{\"k\": 47}, M{\"k\": 47, \"n\": 47})\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.Find(M{\"k\": 47}).One(result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestUpdateId(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"_id\": n, \"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\terr = coll.UpdateId(42, M{\"$inc\": M{\"n\": 1}})\n\tc.Assert(err, IsNil)\n\n\tresult := make(M)\n\terr = coll.FindId(42).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 43)\n\n\terr = coll.UpdateId(47, M{\"k\": 47, \"n\": 47})\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.FindId(47).One(result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestUpdateNil(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"k\": 42, \"n\": 42})\n\tc.Assert(err, IsNil)\n\terr = coll.Update(nil, M{\"$inc\": M{\"n\": 1}})\n\tc.Assert(err, IsNil)\n\n\tresult := make(M)\n\terr = coll.Find(M{\"k\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 43)\n\n\terr = coll.Insert(M{\"k\": 45, \"n\": 45})\n\tc.Assert(err, IsNil)\n\t_, err = coll.UpdateAll(nil, M{\"$inc\": M{\"n\": 1}})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Find(M{\"k\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 44)\n\terr = coll.Find(M{\"k\": 45}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 46)\n}\n\nfunc (s *S) TestUpsert(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(bson.D{{\"k\", n}, {\"n\", n}})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tinfo, err := coll.Upsert(M{\"k\": 42}, bson.D{{\"k\", 42}, {\"n\", 24}})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 1)\n\tc.Assert(info.Matched, Equals, 1)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult := M{}\n\terr = coll.Find(M{\"k\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 24)\n\n\t// Match but do not change.\n\tinfo, err = coll.Upsert(M{\"k\": 42}, bson.D{{\"k\", 42}, {\"n\", 24}})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 1) // On 2.6+ this feels like a server mistake.\n\tc.Assert(info.Matched, Equals, 1)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\t// Insert with internally created id.\n\tinfo, err = coll.Upsert(M{\"k\": 47}, M{\"k\": 47, \"n\": 47})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Matched, Equals, 0)\n\tc.Assert(info.UpsertedId, NotNil)\n\n\terr = coll.Find(M{\"k\": 47}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 47)\n\n\tresult = M{}\n\terr = coll.Find(M{\"_id\": info.UpsertedId}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 47)\n\n\t// Insert with provided id.\n\tinfo, err = coll.Upsert(M{\"k\": 48}, M{\"k\": 48, \"n\": 48, \"_id\": 48})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Matched, Equals, 0)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(info.UpsertedId, Equals, 48)\n\t} else {\n\t\tc.Assert(info.UpsertedId, IsNil) // Unfortunate, but that's what Mongo gave us.\n\t}\n\n\terr = coll.Find(M{\"k\": 48}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 48)\n}\n\nfunc (s *S) TestUpsertId(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"_id\": n, \"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tinfo, err := coll.UpsertId(42, M{\"n\": 24})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 1)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult := M{}\n\terr = coll.FindId(42).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 24)\n\n\tinfo, err = coll.UpsertId(47, M{\"_id\": 47, \"n\": 47})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(info.UpsertedId, Equals, 47)\n\t} else {\n\t\tc.Assert(info.UpsertedId, IsNil)\n\t}\n\n\terr = coll.FindId(47).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 47)\n}\n\nfunc (s *S) TestUpdateAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"k\": n, \"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tinfo, err := coll.UpdateAll(M{\"k\": M{\"$gt\": 42}}, M{\"$unset\": M{\"missing\": 1}})\n\tc.Assert(err, IsNil)\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(info.Updated, Equals, 0)\n\t\tc.Assert(info.Matched, Equals, 4)\n\t} else {\n\t\tc.Assert(info.Updated, Equals, 4)\n\t\tc.Assert(info.Matched, Equals, 4)\n\t}\n\n\tinfo, err = coll.UpdateAll(M{\"k\": M{\"$gt\": 42}}, M{\"$inc\": M{\"n\": 1}})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 4)\n\tc.Assert(info.Matched, Equals, 4)\n\n\tresult := make(M)\n\terr = coll.Find(M{\"k\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 42)\n\n\terr = coll.Find(M{\"k\": 43}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 44)\n\n\terr = coll.Find(M{\"k\": 44}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 45)\n\n\tif !s.versionAtLeast(2, 6) {\n\t\t// 2.6 made this invalid.\n\t\tinfo, err = coll.UpdateAll(M{\"k\": 47}, M{\"k\": 47, \"n\": 47})\n\t\tc.Assert(err, Equals, nil)\n\t\tc.Assert(info.Updated, Equals, 0)\n\t}\n}\n\nfunc (s *S) TestRemove(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\terr = coll.Remove(M{\"n\": M{\"$gt\": 42}})\n\tc.Assert(err, IsNil)\n\n\tresult := &struct{ N int }{}\n\terr = coll.Find(M{\"n\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 42)\n\n\terr = coll.Find(M{\"n\": 43}).One(result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.Find(M{\"n\": 44}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 44)\n}\n\nfunc (s *S) TestRemoveId(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"_id\": 40}, M{\"_id\": 41}, M{\"_id\": 42})\n\tc.Assert(err, IsNil)\n\n\terr = coll.RemoveId(41)\n\tc.Assert(err, IsNil)\n\n\tc.Assert(coll.FindId(40).One(nil), IsNil)\n\tc.Assert(coll.FindId(41).One(nil), Equals, mgo.ErrNotFound)\n\tc.Assert(coll.FindId(42).One(nil), IsNil)\n}\n\nfunc (s *S) TestRemoveUnsafe(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetSafe(nil)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"_id\": 40}, M{\"_id\": 41}, M{\"_id\": 42})\n\tc.Assert(err, IsNil)\n\n\terr = coll.RemoveId(41)\n\tc.Assert(err, IsNil)\n\n\tc.Assert(coll.FindId(40).One(nil), IsNil)\n\tc.Assert(coll.FindId(41).One(nil), Equals, mgo.ErrNotFound)\n\tc.Assert(coll.FindId(42).One(nil), IsNil)\n}\n\nfunc (s *S) TestRemoveAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tinfo, err := coll.RemoveAll(M{\"n\": M{\"$gt\": 42}})\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Removed, Equals, 4)\n\tc.Assert(info.Matched, Equals, 4)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult := &struct{ N int }{}\n\terr = coll.Find(M{\"n\": 42}).One(result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 42)\n\n\terr = coll.Find(M{\"n\": 43}).One(result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.Find(M{\"n\": 44}).One(result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\tinfo, err = coll.RemoveAll(nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Removed, Equals, 3)\n\tc.Assert(info.Matched, Equals, 3)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tn, err := coll.Find(nil).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 0)\n}\n\nfunc (s *S) TestDropDatabase(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb1 := session.DB(\"db1\")\n\tdb1.C(\"col\").Insert(M{\"_id\": 1})\n\n\tdb2 := session.DB(\"db2\")\n\tdb2.C(\"col\").Insert(M{\"_id\": 1})\n\n\terr = db1.DropDatabase()\n\tc.Assert(err, IsNil)\n\n\tnames, err := session.DatabaseNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(filterDBs(names), DeepEquals, []string{\"db2\"})\n\n\terr = db2.DropDatabase()\n\tc.Assert(err, IsNil)\n\n\tnames, err = session.DatabaseNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(filterDBs(names), DeepEquals, []string{})\n}\n\nfunc filterDBs(dbs []string) []string {\n\tvar i int\n\tfor _, name := range dbs {\n\t\tswitch name {\n\t\tcase \"admin\", \"local\":\n\t\tdefault:\n\t\t\tdbs[i] = name\n\t\t\ti++\n\t\t}\n\t}\n\tif len(dbs) == 0 {\n\t\treturn []string{}\n\t}\n\treturn dbs[:i]\n}\n\nfunc (s *S) TestDropCollection(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"db1\")\n\tdb.C(\"col1\").Insert(M{\"_id\": 1})\n\tdb.C(\"col2\").Insert(M{\"_id\": 1})\n\n\terr = db.C(\"col1\").DropCollection()\n\tc.Assert(err, IsNil)\n\n\tnames, err := db.CollectionNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(names, DeepEquals, []string{\"col2\", \"system.indexes\"})\n\n\terr = db.C(\"col2\").DropCollection()\n\tc.Assert(err, IsNil)\n\n\tnames, err = db.CollectionNames()\n\tc.Assert(err, IsNil)\n\tc.Assert(names, DeepEquals, []string{\"system.indexes\"})\n}\n\nfunc (s *S) TestCreateCollectionCapped(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tinfo := &mgo.CollectionInfo{\n\t\tCapped:   true,\n\t\tMaxBytes: 1024,\n\t\tMaxDocs:  3,\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\n\tns := []int{1, 2, 3, 4, 5}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tn, err := coll.Find(nil).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n}\n\nfunc (s *S) TestCreateCollectionNoIndex(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tinfo := &mgo.CollectionInfo{\n\t\tDisableIdIndex: true,\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tindexes, err := coll.Indexes()\n\tc.Assert(indexes, HasLen, 0)\n}\n\nfunc (s *S) TestCreateCollectionForceIndex(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tinfo := &mgo.CollectionInfo{\n\t\tForceIdIndex: true,\n\t\tCapped:       true,\n\t\tMaxBytes:     1024,\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tindexes, err := coll.Indexes()\n\tc.Assert(indexes, HasLen, 1)\n}\n\nfunc (s *S) TestCreateCollectionValidator(c *C) {\n\tif !s.versionAtLeast(3, 2) {\n\t\tc.Skip(\"validation depends on MongoDB 3.2+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\tcoll := db.C(\"mycoll\")\n\n\t// Test Validator.\n\tinfo := &mgo.CollectionInfo{\n\t\tValidator: M{\"b\": M{\"$exists\": true}},\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, ErrorMatches, \"Document failed validation\")\n\terr = coll.DropCollection()\n\tc.Assert(err, IsNil)\n\n\t// Test ValidatorAction.\n\tinfo = &mgo.CollectionInfo{\n\t\tValidator:        M{\"b\": M{\"$exists\": true}},\n\t\tValidationAction: \"warn\",\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.DropCollection()\n\tc.Assert(err, IsNil)\n\n\t// Test ValidationLevel.\n\tinfo = &mgo.CollectionInfo{\n\t\tValidator:       M{\"a\": M{\"$exists\": true}},\n\t\tValidationLevel: \"moderate\",\n\t}\n\terr = coll.Create(info)\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\terr = db.Run(bson.D{{\"collMod\", \"mycoll\"}, {\"validator\", M{\"b\": M{\"$exists\": true}}}}, nil)\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 2})\n\tc.Assert(err, ErrorMatches, \"Document failed validation\")\n\terr = coll.Update(M{\"a\": 1}, M{\"c\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.DropCollection()\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestCreateCollectionStorageEngine(c *C) {\n\tif !s.versionAtLeast(3, 0) {\n\t\tc.Skip(\"storageEngine option depends on MongoDB 3.0+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\tcoll := db.C(\"mycoll\")\n\n\tinfo := &mgo.CollectionInfo{\n\t\tStorageEngine: M{\"test\": M{}},\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, ErrorMatches, \"test is not a registered storage engine for this server\")\n}\n\nfunc (s *S) TestIsDupValues(c *C) {\n\tc.Assert(mgo.IsDup(nil), Equals, false)\n\tc.Assert(mgo.IsDup(&mgo.LastError{Code: 1}), Equals, false)\n\tc.Assert(mgo.IsDup(&mgo.QueryError{Code: 1}), Equals, false)\n\tc.Assert(mgo.IsDup(&mgo.LastError{Code: 11000}), Equals, true)\n\tc.Assert(mgo.IsDup(&mgo.QueryError{Code: 11000}), Equals, true)\n\tc.Assert(mgo.IsDup(&mgo.LastError{Code: 11001}), Equals, true)\n\tc.Assert(mgo.IsDup(&mgo.QueryError{Code: 11001}), Equals, true)\n\tc.Assert(mgo.IsDup(&mgo.LastError{Code: 12582}), Equals, true)\n\tc.Assert(mgo.IsDup(&mgo.QueryError{Code: 12582}), Equals, true)\n\tlerr := &mgo.LastError{Code: 16460, Err: \"error inserting 1 documents to shard ... caused by :: E11000 duplicate key error index: ...\"}\n\tc.Assert(mgo.IsDup(lerr), Equals, true)\n}\n\nfunc (s *S) TestIsDupPrimary(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, ErrorMatches, \".*duplicate key error.*\")\n\tc.Assert(mgo.IsDup(err), Equals, true)\n}\n\nfunc (s *S) TestIsDupUnique(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tindex := mgo.Index{\n\t\tKey:    []string{\"a\", \"b\"},\n\t\tUnique: true,\n\t}\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndex(index)\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"a\": 1, \"b\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"a\": 1, \"b\": 1})\n\tc.Assert(err, ErrorMatches, \".*duplicate key error.*\")\n\tc.Assert(mgo.IsDup(err), Equals, true)\n}\n\nfunc (s *S) TestIsDupCapped(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tinfo := &mgo.CollectionInfo{\n\t\tForceIdIndex: true,\n\t\tCapped:       true,\n\t\tMaxBytes:     1024,\n\t}\n\terr = coll.Create(info)\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"_id\": 1})\n\t// The error was different for capped collections before 2.6.\n\tc.Assert(err, ErrorMatches, \".*duplicate key.*\")\n\t// The issue is reduced by using IsDup.\n\tc.Assert(mgo.IsDup(err), Equals, true)\n}\n\nfunc (s *S) TestIsDupFindAndModify(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"n\"}, Unique: true})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"n\": 2})\n\tc.Assert(err, IsNil)\n\t_, err = coll.Find(M{\"n\": 1}).Apply(mgo.Change{Update: M{\"$inc\": M{\"n\": 1}}}, bson.M{})\n\tc.Assert(err, ErrorMatches, \".*duplicate key error.*\")\n\tc.Assert(mgo.IsDup(err), Equals, true)\n}\n\nfunc (s *S) TestIsDupRetryUpsert(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(bson.M{\"_id\": 1, \"x\": 1})\n\tc.Assert(err, IsNil)\n\n\t_, err = coll.Upsert(bson.M{\"_id\": 1, \"x\": 2}, bson.M{\"$set\": bson.M{\"x\": 3}})\n\tc.Assert(mgo.IsDup(err), Equals, true)\n\n\t_, err = coll.Find(bson.M{\"_id\": 1, \"x\": 2}).Apply(mgo.Change{\n\t\tUpdate: bson.M{\"$set\": bson.M{\"x\": 3}},\n\t\tUpsert: true,\n\t}, nil)\n\tc.Assert(mgo.IsDup(err), Equals, true)\n}\n\nfunc (s *S) TestFindAndModify(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 42})\n\n\tsession.SetMode(mgo.Monotonic, true)\n\n\tresult := M{}\n\tinfo, err := coll.Find(M{\"n\": 42}).Apply(mgo.Change{Update: M{\"$inc\": M{\"n\": 1}}}, result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 42)\n\tc.Assert(info.Updated, Equals, 1)\n\tc.Assert(info.Matched, Equals, 1)\n\tc.Assert(info.Removed, Equals, 0)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\t// A nil result parameter should be acceptable.\n\tinfo, err = coll.Find(M{\"n\": 43}).Apply(mgo.Change{Update: M{\"$unset\": M{\"missing\": 1}}}, nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Updated, Equals, 1) // On 2.6+ this feels like a server mistake.\n\tc.Assert(info.Matched, Equals, 1)\n\tc.Assert(info.Removed, Equals, 0)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult = M{}\n\tinfo, err = coll.Find(M{\"n\": 43}).Apply(mgo.Change{Update: M{\"$inc\": M{\"n\": 1}}, ReturnNew: true}, result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 44)\n\tc.Assert(info.Updated, Equals, 1)\n\tc.Assert(info.Removed, Equals, 0)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult = M{}\n\tinfo, err = coll.Find(M{\"n\": 50}).Apply(mgo.Change{Upsert: true, Update: M{\"n\": 51, \"o\": 52}}, result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], IsNil)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Removed, Equals, 0)\n\tc.Assert(info.UpsertedId, NotNil)\n\n\tresult = M{}\n\tinfo, err = coll.Find(nil).Sort(\"-n\").Apply(mgo.Change{Update: M{\"$inc\": M{\"n\": 1}}, ReturnNew: true}, result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], Equals, 52)\n\tc.Assert(info.Updated, Equals, 1)\n\tc.Assert(info.Removed, Equals, 0)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult = M{}\n\tinfo, err = coll.Find(M{\"n\": 52}).Select(M{\"o\": 1}).Apply(mgo.Change{Remove: true}, result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result[\"n\"], IsNil)\n\tc.Assert(result[\"o\"], Equals, 52)\n\tc.Assert(info.Updated, Equals, 0)\n\tc.Assert(info.Removed, Equals, 1)\n\tc.Assert(info.UpsertedId, IsNil)\n\n\tresult = M{}\n\tinfo, err = coll.Find(M{\"n\": 60}).Apply(mgo.Change{Remove: true}, result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\tc.Assert(len(result), Equals, 0)\n\tc.Assert(info, IsNil)\n}\n\nfunc (s *S) TestFindAndModifyBug997828(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": \"not-a-number\"})\n\n\tresult := make(M)\n\t_, err = coll.Find(M{\"n\": \"not-a-number\"}).Apply(mgo.Change{Update: M{\"$inc\": M{\"n\": 1}}}, result)\n\tc.Assert(err, ErrorMatches, `(exception: )?Cannot apply \\$inc .*`)\n\tif s.versionAtLeast(2, 1) {\n\t\tqerr, _ := err.(*mgo.QueryError)\n\t\tc.Assert(qerr, NotNil, Commentf(\"err: %#v\", err))\n\t\tif s.versionAtLeast(2, 6) {\n\t\t\t// Oh, the dance of error codes. :-(\n\t\t\tc.Assert(qerr.Code, Equals, 16837)\n\t\t} else {\n\t\t\tc.Assert(qerr.Code, Equals, 10140)\n\t\t}\n\t} else {\n\t\tlerr, _ := err.(*mgo.LastError)\n\t\tc.Assert(lerr, NotNil, Commentf(\"err: %#v\", err))\n\t\tc.Assert(lerr.Code, Equals, 10140)\n\t}\n}\n\nfunc (s *S) TestFindAndModifyErrmsgDoc(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"errmsg\": \"an error\"})\n\n\tvar result M\n\t_, err = coll.Find(M{}).Apply(mgo.Change{Update: M{\"$set\": M{\"n\": 1}}}, &result)\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestCountCollection(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tn, err := coll.Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n}\n\nfunc (s *S) TestCountQuery(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tn, err := coll.Find(M{\"n\": M{\"$gt\": 40}}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 2)\n}\n\nfunc (s *S) TestCountQuerySorted(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tn, err := coll.Find(M{\"n\": M{\"$gt\": 40}}).Sort(\"n\").Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 2)\n}\n\nfunc (s *S) TestCountSkipLimit(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tn, err := coll.Find(nil).Skip(1).Limit(3).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 3)\n\n\tn, err = coll.Find(nil).Skip(1).Limit(5).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 4)\n}\n\nfunc (s *S) TestQueryExplain(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tm := M{}\n\tquery := coll.Find(nil).Limit(2)\n\terr = query.Explain(m)\n\tc.Assert(err, IsNil)\n\tif m[\"queryPlanner\"] != nil {\n\t\tc.Assert(m[\"executionStats\"].(M)[\"totalDocsExamined\"], Equals, 2)\n\t} else {\n\t\tc.Assert(m[\"cursor\"], Equals, \"BasicCursor\")\n\t\tc.Assert(m[\"nscanned\"], Equals, 2)\n\t\tc.Assert(m[\"n\"], Equals, 2)\n\t}\n\n\tn := 0\n\tvar result M\n\titer := query.Iter()\n\tfor iter.Next(&result) {\n\t\tn++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(n, Equals, 2)\n}\n\nfunc (s *S) TestQuerySetMaxScan(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tquery := coll.Find(nil).SetMaxScan(2)\n\tvar result []M\n\terr = query.All(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result, HasLen, 2)\n}\n\nfunc (s *S) TestQuerySetMaxTime(c *C) {\n\tif !s.versionAtLeast(2, 6) {\n\t\tc.Skip(\"SetMaxTime only supported in 2.6+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor i := 0; i < 1000; i++ {\n\t\terr := coll.Insert(M{\"n\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tquery := coll.Find(nil)\n\tquery.SetMaxTime(1 * time.Millisecond)\n\tquery.Batch(2)\n\tvar result []M\n\terr = query.All(&result)\n\tc.Assert(err, ErrorMatches, \"operation exceeded time limit\")\n}\n\nfunc (s *S) TestQueryHint(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.EnsureIndexKey(\"a\")\n\n\tm := M{}\n\terr = coll.Find(nil).Hint(\"a\").Explain(m)\n\tc.Assert(err, IsNil)\n\n\tif m[\"queryPlanner\"] != nil {\n\t\tm = m[\"queryPlanner\"].(M)\n\t\tm = m[\"winningPlan\"].(M)\n\t\tm = m[\"inputStage\"].(M)\n\t\tc.Assert(m[\"indexName\"], Equals, \"a_1\")\n\t} else {\n\t\tc.Assert(m[\"indexBounds\"], NotNil)\n\t\tc.Assert(m[\"indexBounds\"].(M)[\"a\"], NotNil)\n\t}\n}\n\nfunc (s *S) TestQueryComment(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdb := session.DB(\"mydb\")\n\tcoll := db.C(\"mycoll\")\n\n\terr = db.Run(bson.M{\"profile\": 2}, nil)\n\tc.Assert(err, IsNil)\n\n\tns := []int{40, 41, 42}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tquery := coll.Find(bson.M{\"n\": 41})\n\tquery.Comment(\"some comment\")\n\terr = query.One(nil)\n\tc.Assert(err, IsNil)\n\n\tquery = coll.Find(bson.M{\"n\": 41})\n\tquery.Comment(\"another comment\")\n\terr = query.One(nil)\n\tc.Assert(err, IsNil)\n\n\tcommentField := \"query.$comment\"\n\tnField := \"query.$query.n\"\n\tif s.versionAtLeast(3, 2) {\n\t\tcommentField = \"query.comment\"\n\t\tnField = \"query.filter.n\"\n\t}\n\tn, err := session.DB(\"mydb\").C(\"system.profile\").Find(bson.M{nField: 41, commentField: \"some comment\"}).Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, 1)\n}\n\nfunc (s *S) TestFindOneNotFound(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tresult := struct{ A, B int }{}\n\terr = coll.Find(M{\"a\": 1}).One(&result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\tc.Assert(err, ErrorMatches, \"not found\")\n\tc.Assert(err == mgo.ErrNotFound, Equals, true)\n}\n\nfunc (s *S) TestFindIterNotFound(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tresult := struct{ A, B int }{}\n\titer := coll.Find(M{\"a\": 1}).Iter()\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Err(), IsNil)\n}\n\nfunc (s *S) TestFindNil(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tresult := struct{ N int }{}\n\n\terr = coll.Find(nil).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 1)\n}\n\nfunc (s *S) TestFindId(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"_id\": 41, \"n\": 41})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"_id\": 42, \"n\": 42})\n\tc.Assert(err, IsNil)\n\n\tresult := struct{ N int }{}\n\n\terr = coll.FindId(42).One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.N, Equals, 42)\n}\n\nfunc (s *S) TestFindIterAll(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\titer := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2).Iter()\n\tresult := struct{ N int }{}\n\tfor i := 2; i < 7; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true, Commentf(\"err=%v\", err))\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 1 {\n\t\t\tstats := mgo.GetStats()\n\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t}\n\t}\n\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 3)     // 1*QUERY_OP + 2*GET_MORE_OP\n\tc.Assert(stats.ReceivedOps, Equals, 3) // and their REPLY_OPs.\n\tif s.versionAtLeast(3, 2) {\n\t\t// In 3.2+ responses come in batches inside the op reply docs.\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t} else {\n\t\tc.Assert(stats.ReceivedDocs, Equals, 5)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestFindIterTwiceWithSameQuery(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor i := 40; i != 47; i++ {\n\t\terr := coll.Insert(M{\"n\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tquery := coll.Find(M{}).Sort(\"n\")\n\n\titer1 := query.Skip(1).Iter()\n\titer2 := query.Skip(2).Iter()\n\n\tvar result struct{ N int }\n\tok := iter2.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(result.N, Equals, 42)\n\tok = iter1.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(result.N, Equals, 41)\n}\n\nfunc (s *S) TestFindIterWithoutResults(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.Insert(M{\"n\": 42})\n\n\titer := coll.Find(M{\"n\": 0}).Iter()\n\n\tresult := struct{ N int }{}\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(result.N, Equals, 0)\n}\n\nfunc (s *S) TestFindIterLimit(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Limit(3)\n\titer := query.Iter()\n\n\tresult := struct{ N int }{}\n\tfor i := 2; i < 5; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t}\n\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tif s.versionAtLeast(3, 2) {\n\t\t// Limit works properly in 3.2+, and results are batched in single doc.\n\t\tc.Assert(stats.SentOps, Equals, 1)     // 1*QUERY_OP\n\t\tc.Assert(stats.ReceivedOps, Equals, 1) // and its REPLY_OP\n\t\tc.Assert(stats.ReceivedDocs, Equals, 1)\n\t} else {\n\t\tc.Assert(stats.SentOps, Equals, 2)     // 1*QUERY_OP + 1*KILL_CURSORS_OP\n\t\tc.Assert(stats.ReceivedOps, Equals, 1) // and its REPLY_OP\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nvar cursorTimeout = flag.Bool(\"cursor-timeout\", false, \"Enable cursor timeout test\")\n\nfunc (s *S) TestFindIterCursorTimeout(c *C) {\n\tif !*cursorTimeout {\n\t\tc.Skip(\"-cursor-timeout\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\ttype Doc struct {\n\t\tId int \"_id\"\n\t}\n\n\tcoll := session.DB(\"test\").C(\"test\")\n\tcoll.Remove(nil)\n\tfor i := 0; i < 100; i++ {\n\t\terr = coll.Insert(Doc{i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tsession.SetBatch(1)\n\titer := coll.Find(nil).Iter()\n\tvar doc Doc\n\tif !iter.Next(&doc) {\n\t\tc.Fatalf(\"iterator failed to return any documents\")\n\t}\n\n\tfor i := 10; i > 0; i-- {\n\t\tc.Logf(\"Sleeping... %d minutes to go...\", i)\n\t\ttime.Sleep(1*time.Minute + 2*time.Second)\n\t}\n\n\t// Drain any existing documents that were fetched.\n\tif !iter.Next(&doc) {\n\t\tc.Fatalf(\"iterator with timed out cursor failed to return previously cached document\")\n\t}\n\tif iter.Next(&doc) {\n\t\tc.Fatalf(\"timed out cursor returned document\")\n\t}\n\n\tc.Assert(iter.Err(), Equals, mgo.ErrCursor)\n}\n\nfunc (s *S) TestTooManyItemsLimitBug(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tdefer runtime.GOMAXPROCS(runtime.GOMAXPROCS(runtime.NumCPU()))\n\n\tmgo.SetDebug(false)\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\twords := strings.Split(\"foo bar baz\", \" \")\n\tfor i := 0; i < 5; i++ {\n\t\twords = append(words, words...)\n\t}\n\tdoc := bson.D{{\"words\", words}}\n\tinserts := 10000\n\tlimit := 5000\n\titers := 0\n\tc.Assert(inserts > limit, Equals, true)\n\tfor i := 0; i < inserts; i++ {\n\t\terr := coll.Insert(&doc)\n\t\tc.Assert(err, IsNil)\n\t}\n\titer := coll.Find(nil).Limit(limit).Iter()\n\tfor iter.Next(&doc) {\n\t\tif iters%100 == 0 {\n\t\t\tc.Logf(\"Seen %d docments\", iters)\n\t\t}\n\t\titers++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(iters, Equals, limit)\n}\n\nfunc (s *S) TestBatchSizeZeroGetMore(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tdefer runtime.GOMAXPROCS(runtime.GOMAXPROCS(runtime.NumCPU()))\n\n\tmgo.SetDebug(false)\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\twords := strings.Split(\"foo bar baz\", \" \")\n\tfor i := 0; i < 5; i++ {\n\t\twords = append(words, words...)\n\t}\n\tdoc := bson.D{{\"words\", words}}\n\tinserts := 10000\n\titers := 0\n\tfor i := 0; i < inserts; i++ {\n\t\terr := coll.Insert(&doc)\n\t\tc.Assert(err, IsNil)\n\t}\n\titer := coll.Find(nil).Iter()\n\tfor iter.Next(&doc) {\n\t\tif iters%100 == 0 {\n\t\t\tc.Logf(\"Seen %d docments\", iters)\n\t\t}\n\t\titers++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc serverCursorsOpen(session *mgo.Session) int {\n\tvar result struct {\n\t\tCursors struct {\n\t\t\tTotalOpen int `bson:\"totalOpen\"`\n\t\t\tTimedOut  int `bson:\"timedOut\"`\n\t\t}\n\t}\n\terr := session.Run(\"serverStatus\", &result)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn result.Cursors.TotalOpen\n}\n\nfunc (s *S) TestFindIterLimitWithMore(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\t// Insane amounts of logging otherwise due to the\n\t// amount of data being shuffled.\n\tmgo.SetDebug(false)\n\tdefer mgo.SetDebug(true)\n\n\t// Should amount to more than 4MB bson payload,\n\t// the default limit per result chunk.\n\tconst total = 4096\n\tvar d struct{ A [1024]byte }\n\tdocs := make([]interface{}, total)\n\tfor i := 0; i < total; i++ {\n\t\tdocs[i] = &d\n\t}\n\terr = coll.Insert(docs...)\n\tc.Assert(err, IsNil)\n\n\tn, err := coll.Count()\n\tc.Assert(err, IsNil)\n\tc.Assert(n, Equals, total)\n\n\t// First, try restricting to a single chunk with a negative limit.\n\tnresults := 0\n\titer := coll.Find(nil).Limit(-total).Iter()\n\tvar discard struct{}\n\tfor iter.Next(&discard) {\n\t\tnresults++\n\t}\n\tif nresults < total/2 || nresults >= total {\n\t\tc.Fatalf(\"Bad result size with negative limit: %d\", nresults)\n\t}\n\n\tcursorsOpen := serverCursorsOpen(session)\n\n\t// Try again, with a positive limit. Should reach the end now,\n\t// using multiple chunks.\n\tnresults = 0\n\titer = coll.Find(nil).Limit(total).Iter()\n\tfor iter.Next(&discard) {\n\t\tnresults++\n\t}\n\tc.Assert(nresults, Equals, total)\n\n\t// Ensure the cursor used is properly killed.\n\tc.Assert(serverCursorsOpen(session), Equals, cursorsOpen)\n\n\t// Edge case, -MinInt == -MinInt.\n\tnresults = 0\n\titer = coll.Find(nil).Limit(math.MinInt32).Iter()\n\tfor iter.Next(&discard) {\n\t\tnresults++\n\t}\n\tif nresults < total/2 || nresults >= total {\n\t\tc.Fatalf(\"Bad result size with MinInt32 limit: %d\", nresults)\n\t}\n}\n\nfunc (s *S) TestFindIterLimitWithBatch(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\t// Ping the database to ensure the nonce has been received already.\n\tc.Assert(session.Ping(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Limit(3).Batch(2)\n\titer := query.Iter()\n\tresult := struct{ N int }{}\n\tfor i := 2; i < 5; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 {\n\t\t\tstats := mgo.GetStats()\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// In 3.2+ responses come in batches inside the op reply docs.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 1)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t\t}\n\t\t}\n\t}\n\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tif s.versionAtLeast(3, 2) {\n\t\t// In 3.2+ limit works properly even with multiple batches..\n\t\tc.Assert(stats.SentOps, Equals, 2)     // 1*QUERY_OP + 1*GET_MORE_OP\n\t\tc.Assert(stats.ReceivedOps, Equals, 2) // and its REPLY_OPs\n\n\t\t// In 3.2+ responses come in batches inside the op reply docs.\n\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t} else {\n\t\tc.Assert(stats.SentOps, Equals, 3)     // 1*QUERY_OP + 1*GET_MORE_OP + 1*KILL_CURSORS_OP\n\t\tc.Assert(stats.ReceivedOps, Equals, 2) // and its REPLY_OPs\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestFindIterSortWithBatch(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\t// Without this, the logic above breaks because Mongo refuses to\n\t// return a cursor with an in-memory sort.\n\tcoll.EnsureIndexKey(\"n\")\n\n\t// Ping the database to ensure the nonce has been received already.\n\tc.Assert(session.Ping(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$lte\": 44}}).Sort(\"-n\").Batch(2)\n\titer := query.Iter()\n\tns = []int{46, 45, 44, 43, 42, 41, 40}\n\tresult := struct{ N int }{}\n\tfor i := 2; i < len(ns); i++ {\n\t\tc.Logf(\"i=%d\", i)\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 {\n\t\t\tstats := mgo.GetStats()\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 1)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t\t}\n\t\t}\n\t}\n\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 3)     // 1*QUERY_OP + 2*GET_MORE_OP\n\tc.Assert(stats.ReceivedOps, Equals, 3) // and its REPLY_OPs\n\tif s.versionAtLeast(3, 2) {\n\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t} else {\n\t\tc.Assert(stats.ReceivedDocs, Equals, 5)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\n// Test tailable cursors in a situation where Next has to sleep to\n// respect the timeout requested on Tail.\nfunc (s *S) TestFindTailTimeoutWithSleep(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcresult := struct{ ErrMsg string }{}\n\n\tdb := session.DB(\"mydb\")\n\terr = db.Run(bson.D{{\"create\", \"mycoll\"}, {\"capped\", true}, {\"size\", 1024}}, &cresult)\n\tc.Assert(err, IsNil)\n\tc.Assert(cresult.ErrMsg, Equals, \"\")\n\tcoll := db.C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\ttimeout := 5 * time.Second\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2)\n\titer := query.Tail(timeout)\n\n\tn := len(ns)\n\tresult := struct{ N int }{}\n\tfor i := 2; i != n; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(iter.Err(), IsNil)\n\t\tc.Assert(iter.Timeout(), Equals, false)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 { // The batch boundary.\n\t\t\tstats := mgo.GetStats()\n\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t}\n\t}\n\n\tmgo.ResetStats()\n\n\t// The following call to Next will block.\n\tdone := make(chan bool)\n\tdefer func() { <-done }()\n\tgo func() {\n\t\t// The internal AwaitData timing of MongoDB is around 2 seconds,\n\t\t// so this should force mgo to sleep at least once by itself to\n\t\t// respect the requested timeout.\n\t\tc.Logf(\"[GOROUTINE] Starting and sleeping...\")\n\t\ttime.Sleep(timeout - 2*time.Second)\n\t\tc.Logf(\"[GOROUTINE] Woke up...\")\n\t\tsession := session.New()\n\t\tc.Logf(\"[GOROUTINE] Session created and will insert...\")\n\t\terr := coll.Insert(M{\"n\": 47})\n\t\tc.Logf(\"[GOROUTINE] Insert attempted, err=%v...\", err)\n\t\tsession.Close()\n\t\tc.Logf(\"[GOROUTINE] Session closed.\")\n\t\tc.Check(err, IsNil)\n\t\tdone <- true\n\t}()\n\n\tc.Log(\"Will wait for Next with N=47...\")\n\tok := iter.Next(&result)\n\tc.Log(\"Next unblocked...\")\n\tc.Assert(ok, Equals, true)\n\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, false)\n\tc.Assert(result.N, Equals, 47)\n\tc.Log(\"Got Next with N=47!\")\n\n\tc.Log(\"Will wait for a result which will never come...\")\n\n\tstarted := time.Now()\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, true)\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\n\tc.Log(\"Will now reuse the timed out tail cursor...\")\n\n\tcoll.Insert(M{\"n\": 48})\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, false)\n\tc.Assert(result.N, Equals, 48)\n}\n\n// Test tailable cursors in a situation where Next never gets to sleep once\n// to respect the timeout requested on Tail.\nfunc (s *S) TestFindTailTimeoutNoSleep(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcresult := struct{ ErrMsg string }{}\n\n\tdb := session.DB(\"mydb\")\n\terr = db.Run(bson.D{{\"create\", \"mycoll\"}, {\"capped\", true}, {\"size\", 1024}}, &cresult)\n\tc.Assert(err, IsNil)\n\tc.Assert(cresult.ErrMsg, Equals, \"\")\n\tcoll := db.C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\ttimeout := 1 * time.Second\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2)\n\titer := query.Tail(timeout)\n\n\tn := len(ns)\n\tresult := struct{ N int }{}\n\tfor i := 2; i != n; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(iter.Err(), IsNil)\n\t\tc.Assert(iter.Timeout(), Equals, false)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 { // The batch boundary.\n\t\t\tstats := mgo.GetStats()\n\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t}\n\t}\n\n\t// The following call to Next will block.\n\tgo func() {\n\t\t// The internal AwaitData timing of MongoDB is around 2 seconds,\n\t\t// so this item should arrive within the AwaitData threshold.\n\t\ttime.Sleep(500 * time.Millisecond)\n\t\tsession := session.New()\n\t\tdefer session.Close()\n\t\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\t\tcoll.Insert(M{\"n\": 47})\n\t}()\n\n\tc.Log(\"Will wait for Next with N=47...\")\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, false)\n\tc.Assert(result.N, Equals, 47)\n\tc.Log(\"Got Next with N=47!\")\n\n\tc.Log(\"Will wait for a result which will never come...\")\n\n\tstarted := time.Now()\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, true)\n\tc.Assert(started.Before(time.Now().Add(-timeout)), Equals, true)\n\n\tc.Log(\"Will now reuse the timed out tail cursor...\")\n\n\tcoll.Insert(M{\"n\": 48})\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, false)\n\tc.Assert(result.N, Equals, 48)\n}\n\n// Test tailable cursors in a situation where Next never gets to sleep once\n// to respect the timeout requested on Tail.\nfunc (s *S) TestFindTailNoTimeout(c *C) {\n\tif *fast {\n\t\tc.Skip(\"-fast\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcresult := struct{ ErrMsg string }{}\n\n\tdb := session.DB(\"mydb\")\n\terr = db.Run(bson.D{{\"create\", \"mycoll\"}, {\"capped\", true}, {\"size\", 1024}}, &cresult)\n\tc.Assert(err, IsNil)\n\tc.Assert(cresult.ErrMsg, Equals, \"\")\n\tcoll := db.C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2)\n\titer := query.Tail(-1)\n\tc.Assert(err, IsNil)\n\n\tn := len(ns)\n\tresult := struct{ N int }{}\n\tfor i := 2; i != n; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 { // The batch boundary.\n\t\t\tstats := mgo.GetStats()\n\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t}\n\t}\n\n\tmgo.ResetStats()\n\n\t// The following call to Next will block.\n\tgo func() {\n\t\ttime.Sleep(5e8)\n\t\tsession := session.New()\n\t\tdefer session.Close()\n\t\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\t\tcoll.Insert(M{\"n\": 47})\n\t}()\n\n\tc.Log(\"Will wait for Next with N=47...\")\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(iter.Timeout(), Equals, false)\n\tc.Assert(result.N, Equals, 47)\n\tc.Log(\"Got Next with N=47!\")\n\n\tc.Log(\"Will wait for a result which will never come...\")\n\n\tgotNext := make(chan bool)\n\tgo func() {\n\t\tok := iter.Next(&result)\n\t\tgotNext <- ok\n\t}()\n\n\tselect {\n\tcase ok := <-gotNext:\n\t\tc.Fatalf(\"Next returned: %v\", ok)\n\tcase <-time.After(3e9):\n\t\t// Good. Should still be sleeping at that point.\n\t}\n\n\t// Closing the session should cause Next to return.\n\tsession.Close()\n\n\tselect {\n\tcase ok := <-gotNext:\n\t\tc.Assert(ok, Equals, false)\n\t\tc.Assert(iter.Err(), ErrorMatches, \"Closed explicitly\")\n\t\tc.Assert(iter.Timeout(), Equals, false)\n\tcase <-time.After(1e9):\n\t\tc.Fatal(\"Closing the session did not unblock Next\")\n\t}\n}\n\nfunc (s *S) TestIterNextResetsResult(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{1, 2, 3}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\" + strconv.Itoa(n): n})\n\t}\n\n\tquery := coll.Find(nil).Sort(\"$natural\")\n\n\ti := 0\n\tvar sresult *struct{ N1, N2, N3 int }\n\titer := query.Iter()\n\tfor iter.Next(&sresult) {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(sresult.N1, Equals, 1)\n\t\t\tc.Assert(sresult.N2+sresult.N3, Equals, 0)\n\t\tcase 1:\n\t\t\tc.Assert(sresult.N2, Equals, 2)\n\t\t\tc.Assert(sresult.N1+sresult.N3, Equals, 0)\n\t\tcase 2:\n\t\t\tc.Assert(sresult.N3, Equals, 3)\n\t\t\tc.Assert(sresult.N1+sresult.N2, Equals, 0)\n\t\t}\n\t\ti++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n\n\ti = 0\n\tvar mresult M\n\titer = query.Iter()\n\tfor iter.Next(&mresult) {\n\t\tdelete(mresult, \"_id\")\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n1\": 1})\n\t\tcase 1:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n2\": 2})\n\t\tcase 2:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n3\": 3})\n\t\t}\n\t\ti++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n\n\ti = 0\n\tvar iresult interface{}\n\titer = query.Iter()\n\tfor iter.Next(&iresult) {\n\t\tmresult, ok := iresult.(bson.M)\n\t\tc.Assert(ok, Equals, true, Commentf(\"%#v\", iresult))\n\t\tdelete(mresult, \"_id\")\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n1\": 1})\n\t\tcase 1:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n2\": 2})\n\t\tcase 2:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n3\": 3})\n\t\t}\n\t\ti++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestFindForOnIter(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2)\n\titer := query.Iter()\n\n\ti := 2\n\tvar result *struct{ N int }\n\terr = iter.For(&result, func() error {\n\t\tc.Assert(i < 7, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 1 {\n\t\t\tstats := mgo.GetStats()\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 1)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t\t}\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 3)     // 1*QUERY_OP + 2*GET_MORE_OP\n\tc.Assert(stats.ReceivedOps, Equals, 3) // and their REPLY_OPs.\n\tif s.versionAtLeast(3, 2) {\n\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t} else {\n\t\tc.Assert(stats.ReceivedDocs, Equals, 5)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestFindFor(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tsession.Refresh() // Release socket.\n\n\tmgo.ResetStats()\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2)\n\n\ti := 2\n\tvar result *struct{ N int }\n\terr = query.For(&result, func() error {\n\t\tc.Assert(i < 7, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 1 {\n\t\t\tstats := mgo.GetStats()\n\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 1)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, 2)\n\t\t\t}\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, IsNil)\n\n\tsession.Refresh() // Release socket.\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 3)     // 1*QUERY_OP + 2*GET_MORE_OP\n\tc.Assert(stats.ReceivedOps, Equals, 3) // and their REPLY_OPs.\n\tif s.versionAtLeast(3, 2) {\n\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\tc.Assert(stats.ReceivedDocs, Equals, 3)\n\t} else {\n\t\tc.Assert(stats.ReceivedDocs, Equals, 5)\n\t}\n\tc.Assert(stats.SocketsInUse, Equals, 0)\n}\n\nfunc (s *S) TestFindForStopOnError(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tquery := coll.Find(M{\"n\": M{\"$gte\": 42}})\n\ti := 2\n\tvar result *struct{ N int }\n\terr = query.For(&result, func() error {\n\t\tc.Assert(i < 4, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t\tif i == 3 {\n\t\t\treturn fmt.Errorf(\"stop!\")\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, ErrorMatches, \"stop!\")\n}\n\nfunc (s *S) TestFindForResetsResult(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{1, 2, 3}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\" + strconv.Itoa(n): n})\n\t}\n\n\tquery := coll.Find(nil).Sort(\"$natural\")\n\n\ti := 0\n\tvar sresult *struct{ N1, N2, N3 int }\n\terr = query.For(&sresult, func() error {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(sresult.N1, Equals, 1)\n\t\t\tc.Assert(sresult.N2+sresult.N3, Equals, 0)\n\t\tcase 1:\n\t\t\tc.Assert(sresult.N2, Equals, 2)\n\t\t\tc.Assert(sresult.N1+sresult.N3, Equals, 0)\n\t\tcase 2:\n\t\t\tc.Assert(sresult.N3, Equals, 3)\n\t\t\tc.Assert(sresult.N1+sresult.N2, Equals, 0)\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, IsNil)\n\n\ti = 0\n\tvar mresult M\n\terr = query.For(&mresult, func() error {\n\t\tdelete(mresult, \"_id\")\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n1\": 1})\n\t\tcase 1:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n2\": 2})\n\t\tcase 2:\n\t\t\tc.Assert(mresult, DeepEquals, M{\"n3\": 3})\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, IsNil)\n\n\ti = 0\n\tvar iresult interface{}\n\terr = query.For(&iresult, func() error {\n\t\tmresult, ok := iresult.(bson.M)\n\t\tc.Assert(ok, Equals, true, Commentf(\"%#v\", iresult))\n\t\tdelete(mresult, \"_id\")\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n1\": 1})\n\t\tcase 1:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n2\": 2})\n\t\tcase 2:\n\t\t\tc.Assert(mresult, DeepEquals, bson.M{\"n3\": 3})\n\t\t}\n\t\ti++\n\t\treturn nil\n\t})\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestFindIterSnapshot(c *C) {\n\tif s.versionAtLeast(3, 2) {\n\t\tc.Skip(\"Broken in 3.2: https://jira.mongodb.org/browse/SERVER-21403\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Insane amounts of logging otherwise due to the\n\t// amount of data being shuffled.\n\tmgo.SetDebug(false)\n\tdefer mgo.SetDebug(true)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tvar a [1024000]byte\n\n\tfor n := 0; n < 10; n++ {\n\t\terr := coll.Insert(M{\"_id\": n, \"n\": n, \"a1\": &a})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tquery := coll.Find(M{\"n\": M{\"$gt\": -1}}).Batch(2).Prefetch(0)\n\tquery.Snapshot()\n\titer := query.Iter()\n\n\tseen := map[int]bool{}\n\tresult := struct {\n\t\tId int \"_id\"\n\t}{}\n\tfor iter.Next(&result) {\n\t\tif len(seen) == 2 {\n\t\t\t// Grow all entries so that they have to move.\n\t\t\t// Backwards so that the order is inverted.\n\t\t\tfor n := 10; n >= 0; n-- {\n\t\t\t\t_, err := coll.Upsert(M{\"_id\": n}, M{\"$set\": M{\"a2\": &a}})\n\t\t\t\tc.Assert(err, IsNil)\n\t\t\t}\n\t\t}\n\t\tif seen[result.Id] {\n\t\t\tc.Fatalf(\"seen duplicated key: %d\", result.Id)\n\t\t}\n\t\tseen[result.Id] = true\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestSort(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tcoll.Insert(M{\"a\": 1, \"b\": 1})\n\tcoll.Insert(M{\"a\": 2, \"b\": 2})\n\tcoll.Insert(M{\"a\": 2, \"b\": 1})\n\tcoll.Insert(M{\"a\": 0, \"b\": 1})\n\tcoll.Insert(M{\"a\": 2, \"b\": 0})\n\tcoll.Insert(M{\"a\": 0, \"b\": 2})\n\tcoll.Insert(M{\"a\": 1, \"b\": 2})\n\tcoll.Insert(M{\"a\": 0, \"b\": 0})\n\tcoll.Insert(M{\"a\": 1, \"b\": 0})\n\n\tquery := coll.Find(M{})\n\tquery.Sort(\"-a\") // Should be ignored.\n\tquery.Sort(\"-b\", \"a\")\n\titer := query.Iter()\n\n\tl := make([]int, 18)\n\tr := struct{ A, B int }{}\n\tfor i := 0; i != len(l); i += 2 {\n\t\tok := iter.Next(&r)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(err, IsNil)\n\t\tl[i] = r.A\n\t\tl[i+1] = r.B\n\t}\n\n\tc.Assert(l, DeepEquals, []int{0, 2, 1, 2, 2, 2, 0, 1, 1, 1, 2, 1, 0, 0, 1, 0, 2, 0})\n}\n\nfunc (s *S) TestSortWithBadArgs(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tf1 := func() { coll.Find(nil).Sort(\"\") }\n\tf2 := func() { coll.Find(nil).Sort(\"+\") }\n\tf3 := func() { coll.Find(nil).Sort(\"foo\", \"-\") }\n\n\tfor _, f := range []func(){f1, f2, f3} {\n\t\tc.Assert(f, PanicMatches, \"Sort: empty field name\")\n\t}\n}\n\nfunc (s *S) TestSortScoreText(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tif !s.versionAtLeast(2, 4) {\n\t\tc.Skip(\"Text search depends on 2.4+\")\n\t}\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndex(mgo.Index{\n\t\tKey: []string{\"$text:a\", \"$text:b\"},\n\t})\n\tmsg := \"text search not enabled\"\n\tif err != nil && strings.Contains(err.Error(), msg) {\n\t\tc.Skip(msg)\n\t}\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\n\t\t\"a\": \"none\",\n\t\t\"b\": \"twice: foo foo\",\n\t})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\n\t\t\"a\": \"just once: foo\",\n\t\t\"b\": \"none\",\n\t})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\n\t\t\"a\": \"many: foo foo foo\",\n\t\t\"b\": \"none\",\n\t})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\n\t\t\"a\": \"none\",\n\t\t\"b\": \"none\",\n\t\t\"c\": \"ignore: foo\",\n\t})\n\tc.Assert(err, IsNil)\n\n\tquery := coll.Find(M{\"$text\": M{\"$search\": \"foo\"}})\n\tquery.Select(M{\"score\": M{\"$meta\": \"textScore\"}})\n\tquery.Sort(\"$textScore:score\")\n\titer := query.Iter()\n\n\tvar r struct{ A, B string }\n\tvar results []string\n\tfor iter.Next(&r) {\n\t\tresults = append(results, r.A, r.B)\n\t}\n\n\tc.Assert(results, DeepEquals, []string{\n\t\t\"many: foo foo foo\", \"none\",\n\t\t\"none\", \"twice: foo foo\",\n\t\t\"just once: foo\", \"none\",\n\t})\n}\n\nfunc (s *S) TestPrefetching(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tconst total = 600\n\tconst batch = 100\n\tmgo.SetDebug(false)\n\tdocs := make([]interface{}, total)\n\tfor i := 0; i != total; i++ {\n\t\tdocs[i] = bson.D{{\"n\", i}}\n\t}\n\terr = coll.Insert(docs...)\n\tc.Assert(err, IsNil)\n\n\tfor testi := 0; testi < 5; testi++ {\n\t\tmgo.ResetStats()\n\n\t\tvar iter *mgo.Iter\n\t\tvar beforeMore int\n\n\t\tswitch testi {\n\t\tcase 0: // The default session value.\n\t\t\tsession.SetBatch(batch)\n\t\t\titer = coll.Find(M{}).Iter()\n\t\t\tbeforeMore = 75\n\n\t\tcase 2: // Changing the session value.\n\t\t\tsession.SetBatch(batch)\n\t\t\tsession.SetPrefetch(0.27)\n\t\t\titer = coll.Find(M{}).Iter()\n\t\t\tbeforeMore = 73\n\n\t\tcase 1: // Changing via query methods.\n\t\t\titer = coll.Find(M{}).Prefetch(0.27).Batch(batch).Iter()\n\t\t\tbeforeMore = 73\n\n\t\tcase 3: // With prefetch on first document.\n\t\t\titer = coll.Find(M{}).Prefetch(1.0).Batch(batch).Iter()\n\t\t\tbeforeMore = 0\n\n\t\tcase 4: // Without prefetch.\n\t\t\titer = coll.Find(M{}).Prefetch(0).Batch(batch).Iter()\n\t\t\tbeforeMore = 100\n\t\t}\n\n\t\tpings := 0\n\t\tfor batchi := 0; batchi < len(docs)/batch-1; batchi++ {\n\t\t\tc.Logf(\"Iterating over %d documents on batch %d\", beforeMore, batchi)\n\t\t\tvar result struct{ N int }\n\t\t\tfor i := 0; i < beforeMore; i++ {\n\t\t\t\tok := iter.Next(&result)\n\t\t\t\tc.Assert(ok, Equals, true, Commentf(\"iter.Err: %v\", iter.Err()))\n\t\t\t}\n\t\t\tbeforeMore = 99\n\t\t\tc.Logf(\"Done iterating.\")\n\n\t\t\tsession.Run(\"ping\", nil) // Roundtrip to settle down.\n\t\t\tpings++\n\n\t\t\tstats := mgo.GetStats()\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, (batchi+1)+pings)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, (batchi+1)*batch+pings)\n\t\t\t}\n\n\t\t\tc.Logf(\"Iterating over one more document on batch %d\", batchi)\n\t\t\tok := iter.Next(&result)\n\t\t\tc.Assert(ok, Equals, true, Commentf(\"iter.Err: %v\", iter.Err()))\n\t\t\tc.Logf(\"Done iterating.\")\n\n\t\t\tsession.Run(\"ping\", nil) // Roundtrip to settle down.\n\t\t\tpings++\n\n\t\t\tstats = mgo.GetStats()\n\t\t\tif s.versionAtLeast(3, 2) {\n\t\t\t\t// Find command in 3.2+ bundles batches in a single document.\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, (batchi+2)+pings)\n\t\t\t} else {\n\t\t\t\tc.Assert(stats.ReceivedDocs, Equals, (batchi+2)*batch+pings)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (s *S) TestSafeSetting(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Check the default\n\tsafe := session.Safe()\n\tc.Assert(safe.W, Equals, 0)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 0)\n\tc.Assert(safe.FSync, Equals, false)\n\tc.Assert(safe.J, Equals, false)\n\n\t// Tweak it\n\tsession.SetSafe(&mgo.Safe{W: 1, WTimeout: 2, FSync: true})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 1)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 2)\n\tc.Assert(safe.FSync, Equals, true)\n\tc.Assert(safe.J, Equals, false)\n\n\t// Reset it again.\n\tsession.SetSafe(&mgo.Safe{})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 0)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 0)\n\tc.Assert(safe.FSync, Equals, false)\n\tc.Assert(safe.J, Equals, false)\n\n\t// Ensure safety to something more conservative.\n\tsession.SetSafe(&mgo.Safe{W: 5, WTimeout: 6, J: true})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 5)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 6)\n\tc.Assert(safe.FSync, Equals, false)\n\tc.Assert(safe.J, Equals, true)\n\n\t// Ensure safety to something less conservative won't change it.\n\tsession.EnsureSafe(&mgo.Safe{W: 4, WTimeout: 7})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 5)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 6)\n\tc.Assert(safe.FSync, Equals, false)\n\tc.Assert(safe.J, Equals, true)\n\n\t// But to something more conservative will.\n\tsession.EnsureSafe(&mgo.Safe{W: 6, WTimeout: 4, FSync: true})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 6)\n\tc.Assert(safe.WMode, Equals, \"\")\n\tc.Assert(safe.WTimeout, Equals, 4)\n\tc.Assert(safe.FSync, Equals, true)\n\tc.Assert(safe.J, Equals, false)\n\n\t// Even more conservative.\n\tsession.EnsureSafe(&mgo.Safe{WMode: \"majority\", WTimeout: 2})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 0)\n\tc.Assert(safe.WMode, Equals, \"majority\")\n\tc.Assert(safe.WTimeout, Equals, 2)\n\tc.Assert(safe.FSync, Equals, true)\n\tc.Assert(safe.J, Equals, false)\n\n\t// WMode always overrides, whatever it is, but J doesn't.\n\tsession.EnsureSafe(&mgo.Safe{WMode: \"something\", J: true})\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 0)\n\tc.Assert(safe.WMode, Equals, \"something\")\n\tc.Assert(safe.WTimeout, Equals, 2)\n\tc.Assert(safe.FSync, Equals, true)\n\tc.Assert(safe.J, Equals, false)\n\n\t// EnsureSafe with nil does nothing.\n\tsession.EnsureSafe(nil)\n\tsafe = session.Safe()\n\tc.Assert(safe.W, Equals, 0)\n\tc.Assert(safe.WMode, Equals, \"something\")\n\tc.Assert(safe.WTimeout, Equals, 2)\n\tc.Assert(safe.FSync, Equals, true)\n\tc.Assert(safe.J, Equals, false)\n\n\t// Changing the safety of a cloned session doesn't touch the original.\n\tclone := session.Clone()\n\tdefer clone.Close()\n\tclone.EnsureSafe(&mgo.Safe{WMode: \"foo\"})\n\tsafe = session.Safe()\n\tc.Assert(safe.WMode, Equals, \"something\")\n}\n\nfunc (s *S) TestSafeInsert(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\t// Insert an element with a predefined key.\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\tmgo.ResetStats()\n\n\t// Session should be safe by default, so inserting it again must fail.\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, ErrorMatches, \".*E11000 duplicate.*\")\n\tc.Assert(err.(*mgo.LastError).Code, Equals, 11000)\n\n\t// It must have sent two operations (INSERT_OP + getLastError QUERY_OP)\n\tstats := mgo.GetStats()\n\n\tif s.versionAtLeast(2, 6) {\n\t\tc.Assert(stats.SentOps, Equals, 1)\n\t} else {\n\t\tc.Assert(stats.SentOps, Equals, 2)\n\t}\n\n\tmgo.ResetStats()\n\n\t// If we disable safety, though, it won't complain.\n\tsession.SetSafe(nil)\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Must have sent a single operation this time (just the INSERT_OP)\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 1)\n}\n\nfunc (s *S) TestSafeParameters(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40011\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\t// Tweak the safety parameters to something unachievable.\n\tsession.SetSafe(&mgo.Safe{W: 4, WTimeout: 100})\n\terr = coll.Insert(M{\"_id\": 1})\n\tc.Assert(err, ErrorMatches, \"timeout|timed out waiting for slaves|Not enough data-bearing nodes|waiting for replication timed out\") // :-(\n\tif !s.versionAtLeast(2, 6) {\n\t\t// 2.6 turned it into a query error.\n\t\tc.Assert(err.(*mgo.LastError).WTimeout, Equals, true)\n\t}\n}\n\nfunc (s *S) TestQueryErrorOne(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Find(M{\"a\": 1}).Select(M{\"a\": M{\"b\": 1}}).One(nil)\n\tc.Assert(err, ErrorMatches, \".*Unsupported projection option:.*\")\n\tc.Assert(err.(*mgo.QueryError).Message, Matches, \".*Unsupported projection option:.*\")\n\t// Oh, the dance of error codes. :-(\n\tif s.versionAtLeast(3, 2) {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 2)\n\t} else if s.versionAtLeast(2, 6) {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 17287)\n\t} else {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 13097)\n\t}\n}\n\nfunc (s *S) TestQueryErrorNext(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\titer := coll.Find(M{\"a\": 1}).Select(M{\"a\": M{\"b\": 1}}).Iter()\n\n\tvar result struct{}\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\n\terr = iter.Close()\n\tc.Assert(err, ErrorMatches, \".*Unsupported projection option:.*\")\n\tc.Assert(err.(*mgo.QueryError).Message, Matches, \".*Unsupported projection option:.*\")\n\t// Oh, the dance of error codes. :-(\n\tif s.versionAtLeast(3, 2) {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 2)\n\t} else if s.versionAtLeast(2, 6) {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 17287)\n\t} else {\n\t\tc.Assert(err.(*mgo.QueryError).Code, Equals, 13097)\n\t}\n\tc.Assert(iter.Err(), Equals, err)\n}\n\nvar indexTests = []struct {\n\tindex    mgo.Index\n\texpected M\n}{{\n\tmgo.Index{\n\t\tKey:        []string{\"a\"},\n\t\tBackground: true,\n\t},\n\tM{\n\t\t\"name\":       \"a_1\",\n\t\t\"key\":        M{\"a\": 1},\n\t\t\"ns\":         \"mydb.mycoll\",\n\t\t\"background\": true,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:      []string{\"a\", \"-b\"},\n\t\tUnique:   true,\n\t\tDropDups: true,\n\t},\n\tM{\n\t\t\"name\":     \"a_1_b_-1\",\n\t\t\"key\":      M{\"a\": 1, \"b\": -1},\n\t\t\"ns\":       \"mydb.mycoll\",\n\t\t\"unique\":   true,\n\t\t\"dropDups\": true,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:  []string{\"@loc_old\"}, // Obsolete\n\t\tMin:  -500,\n\t\tMax:  500,\n\t\tBits: 32,\n\t},\n\tM{\n\t\t\"name\": \"loc_old_2d\",\n\t\t\"key\":  M{\"loc_old\": \"2d\"},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t\t\"min\":  -500.0,\n\t\t\"max\":  500.0,\n\t\t\"bits\": 32,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:  []string{\"$2d:loc\"},\n\t\tMin:  -500,\n\t\tMax:  500,\n\t\tBits: 32,\n\t},\n\tM{\n\t\t\"name\": \"loc_2d\",\n\t\t\"key\":  M{\"loc\": \"2d\"},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t\t\"min\":  -500.0,\n\t\t\"max\":  500.0,\n\t\t\"bits\": 32,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:  []string{\"$2d:loc\"},\n\t\tMinf: -500.1,\n\t\tMaxf: 500.1,\n\t\tMin:  1, // Should be ignored\n\t\tMax:  2,\n\t\tBits: 32,\n\t},\n\tM{\n\t\t\"name\": \"loc_2d\",\n\t\t\"key\":  M{\"loc\": \"2d\"},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t\t\"min\":  -500.1,\n\t\t\"max\":  500.1,\n\t\t\"bits\": 32,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:        []string{\"$geoHaystack:loc\", \"type\"},\n\t\tBucketSize: 1,\n\t},\n\tM{\n\t\t\"name\":       \"loc_geoHaystack_type_1\",\n\t\t\"key\":        M{\"loc\": \"geoHaystack\", \"type\": 1},\n\t\t\"ns\":         \"mydb.mycoll\",\n\t\t\"bucketSize\": 1.0,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:     []string{\"$text:a\", \"$text:b\"},\n\t\tWeights: map[string]int{\"b\": 42},\n\t},\n\tM{\n\t\t\"name\":              \"a_text_b_text\",\n\t\t\"key\":               M{\"_fts\": \"text\", \"_ftsx\": 1},\n\t\t\"ns\":                \"mydb.mycoll\",\n\t\t\"weights\":           M{\"a\": 1, \"b\": 42},\n\t\t\"default_language\":  \"english\",\n\t\t\"language_override\": \"language\",\n\t\t\"textIndexVersion\":  2,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:              []string{\"$text:a\"},\n\t\tDefaultLanguage:  \"portuguese\",\n\t\tLanguageOverride: \"idioma\",\n\t},\n\tM{\n\t\t\"name\":              \"a_text\",\n\t\t\"key\":               M{\"_fts\": \"text\", \"_ftsx\": 1},\n\t\t\"ns\":                \"mydb.mycoll\",\n\t\t\"weights\":           M{\"a\": 1},\n\t\t\"default_language\":  \"portuguese\",\n\t\t\"language_override\": \"idioma\",\n\t\t\"textIndexVersion\":  2,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey: []string{\"$text:$**\"},\n\t},\n\tM{\n\t\t\"name\":              \"$**_text\",\n\t\t\"key\":               M{\"_fts\": \"text\", \"_ftsx\": 1},\n\t\t\"ns\":                \"mydb.mycoll\",\n\t\t\"weights\":           M{\"$**\": 1},\n\t\t\"default_language\":  \"english\",\n\t\t\"language_override\": \"language\",\n\t\t\"textIndexVersion\":  2,\n\t},\n}, {\n\tmgo.Index{\n\t\tKey:  []string{\"cn\"},\n\t\tName: \"CustomName\",\n\t},\n\tM{\n\t\t\"name\": \"CustomName\",\n\t\t\"key\":  M{\"cn\": 1},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t},\n}}\n\nfunc (s *S) TestEnsureIndex(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tidxs := session.DB(\"mydb\").C(\"system.indexes\")\n\n\tfor _, test := range indexTests {\n\t\tif !s.versionAtLeast(2, 4) && test.expected[\"textIndexVersion\"] != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\terr = coll.EnsureIndex(test.index)\n\t\tmsg := \"text search not enabled\"\n\t\tif err != nil && strings.Contains(err.Error(), msg) {\n\t\t\tcontinue\n\t\t}\n\t\tc.Assert(err, IsNil)\n\n\t\texpectedName := test.index.Name\n\t\tif expectedName == \"\" {\n\t\t\texpectedName, _ = test.expected[\"name\"].(string)\n\t\t}\n\n\t\tobtained := M{}\n\t\terr = idxs.Find(M{\"name\": expectedName}).One(obtained)\n\t\tc.Assert(err, IsNil)\n\n\t\tdelete(obtained, \"v\")\n\n\t\tif s.versionAtLeast(2, 7) {\n\t\t\t// Was deprecated in 2.6, and not being reported by 2.7+.\n\t\t\tdelete(test.expected, \"dropDups\")\n\t\t\ttest.index.DropDups = false\n\t\t}\n\t\tif s.versionAtLeast(3, 2) && test.expected[\"textIndexVersion\"] != nil {\n\t\t\ttest.expected[\"textIndexVersion\"] = 3\n\t\t}\n\n\t\tc.Assert(obtained, DeepEquals, test.expected)\n\n\t\t// The result of Indexes must match closely what was used to create the index.\n\t\tindexes, err := coll.Indexes()\n\t\tc.Assert(err, IsNil)\n\t\tc.Assert(indexes, HasLen, 2)\n\t\tgotIndex := indexes[0]\n\t\tif gotIndex.Name == \"_id_\" {\n\t\t\tgotIndex = indexes[1]\n\t\t}\n\t\twantIndex := test.index\n\t\tif wantIndex.Name == \"\" {\n\t\t\twantIndex.Name = gotIndex.Name\n\t\t}\n\t\tif strings.HasPrefix(wantIndex.Key[0], \"@\") {\n\t\t\twantIndex.Key[0] = \"$2d:\" + wantIndex.Key[0][1:]\n\t\t}\n\t\tif wantIndex.Minf == 0 && wantIndex.Maxf == 0 {\n\t\t\twantIndex.Minf = float64(wantIndex.Min)\n\t\t\twantIndex.Maxf = float64(wantIndex.Max)\n\t\t} else {\n\t\t\twantIndex.Min = gotIndex.Min\n\t\t\twantIndex.Max = gotIndex.Max\n\t\t}\n\t\tif wantIndex.DefaultLanguage == \"\" {\n\t\t\twantIndex.DefaultLanguage = gotIndex.DefaultLanguage\n\t\t}\n\t\tif wantIndex.LanguageOverride == \"\" {\n\t\t\twantIndex.LanguageOverride = gotIndex.LanguageOverride\n\t\t}\n\t\tfor name, _ := range gotIndex.Weights {\n\t\t\tif _, ok := wantIndex.Weights[name]; !ok {\n\t\t\t\tif wantIndex.Weights == nil {\n\t\t\t\t\twantIndex.Weights = make(map[string]int)\n\t\t\t\t}\n\t\t\t\twantIndex.Weights[name] = 1\n\t\t\t}\n\t\t}\n\t\tc.Assert(gotIndex, DeepEquals, wantIndex)\n\n\t\t// Drop created index by key or by name if a custom name was used.\n\t\tif test.index.Name == \"\" {\n\t\t\terr = coll.DropIndex(test.index.Key...)\n\t\t\tc.Assert(err, IsNil)\n\t\t} else {\n\t\t\terr = coll.DropIndexName(test.index.Name)\n\t\t\tc.Assert(err, IsNil)\n\t\t}\n\t}\n}\n\nfunc (s *S) TestEnsureIndexWithBadInfo(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndex(mgo.Index{})\n\tc.Assert(err, ErrorMatches, \"invalid index key:.*\")\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"\"}})\n\tc.Assert(err, ErrorMatches, \"invalid index key:.*\")\n}\n\nfunc (s *S) TestEnsureIndexWithUnsafeSession(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetSafe(nil)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"a\": 1})\n\tc.Assert(err, IsNil)\n\n\t// Should fail since there are duplicated entries.\n\tindex := mgo.Index{\n\t\tKey:    []string{\"a\"},\n\t\tUnique: true,\n\t}\n\n\terr = coll.EnsureIndex(index)\n\tc.Assert(err, ErrorMatches, \".*duplicate key error.*\")\n}\n\nfunc (s *S) TestEnsureIndexKey(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndexKey(\"a\", \"-b\")\n\tc.Assert(err, IsNil)\n\n\tsysidx := session.DB(\"mydb\").C(\"system.indexes\")\n\n\tresult1 := M{}\n\terr = sysidx.Find(M{\"name\": \"a_1\"}).One(result1)\n\tc.Assert(err, IsNil)\n\n\tresult2 := M{}\n\terr = sysidx.Find(M{\"name\": \"a_1_b_-1\"}).One(result2)\n\tc.Assert(err, IsNil)\n\n\tdelete(result1, \"v\")\n\texpected1 := M{\n\t\t\"name\": \"a_1\",\n\t\t\"key\":  M{\"a\": 1},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t}\n\tc.Assert(result1, DeepEquals, expected1)\n\n\tdelete(result2, \"v\")\n\texpected2 := M{\n\t\t\"name\": \"a_1_b_-1\",\n\t\t\"key\":  M{\"a\": 1, \"b\": -1},\n\t\t\"ns\":   \"mydb.mycoll\",\n\t}\n\tc.Assert(result2, DeepEquals, expected2)\n}\n\nfunc (s *S) TestEnsureIndexDropIndex(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndexKey(\"-b\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.DropIndex(\"-b\")\n\tc.Assert(err, IsNil)\n\n\tsysidx := session.DB(\"mydb\").C(\"system.indexes\")\n\n\terr = sysidx.Find(M{\"name\": \"a_1\"}).One(nil)\n\tc.Assert(err, IsNil)\n\n\terr = sysidx.Find(M{\"name\": \"b_1\"}).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.DropIndex(\"a\")\n\tc.Assert(err, IsNil)\n\n\terr = sysidx.Find(M{\"name\": \"a_1\"}).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.DropIndex(\"a\")\n\tc.Assert(err, ErrorMatches, \"index not found.*\")\n}\n\nfunc (s *S) TestEnsureIndexDropIndexName(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"b\"}, Name: \"a\"})\n\tc.Assert(err, IsNil)\n\n\terr = coll.DropIndexName(\"a\")\n\tc.Assert(err, IsNil)\n\n\tsysidx := session.DB(\"mydb\").C(\"system.indexes\")\n\n\terr = sysidx.Find(M{\"name\": \"a_1\"}).One(nil)\n\tc.Assert(err, IsNil)\n\n\terr = sysidx.Find(M{\"name\": \"a\"}).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.DropIndexName(\"a_1\")\n\tc.Assert(err, IsNil)\n\n\terr = sysidx.Find(M{\"name\": \"a_1\"}).One(nil)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n\n\terr = coll.DropIndexName(\"a_1\")\n\tc.Assert(err, ErrorMatches, \"index not found.*\")\n}\n\nfunc (s *S) TestEnsureIndexCaching(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\tmgo.ResetStats()\n\n\t// Second EnsureIndex should be cached and do nothing.\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 0)\n\n\t// Resetting the cache should make it contact the server again.\n\tsession.ResetIndexCache()\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SentOps > 0, Equals, true)\n\n\t// Dropping the index should also drop the cached index key.\n\terr = coll.DropIndex(\"a\")\n\tc.Assert(err, IsNil)\n\n\tmgo.ResetStats()\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SentOps > 0, Equals, true)\n}\n\nfunc (s *S) TestEnsureIndexGetIndexes(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndexKey(\"-b\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\t// Obsolete.\n\terr = coll.EnsureIndexKey(\"@c\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndexKey(\"$2d:d\")\n\tc.Assert(err, IsNil)\n\n\t// Try to exercise cursor logic. 2.8.0-rc3 still ignores this.\n\tsession.SetBatch(2)\n\n\tindexes, err := coll.Indexes()\n\tc.Assert(err, IsNil)\n\n\tc.Assert(indexes[0].Name, Equals, \"_id_\")\n\tc.Assert(indexes[1].Name, Equals, \"a_1\")\n\tc.Assert(indexes[1].Key, DeepEquals, []string{\"a\"})\n\tc.Assert(indexes[2].Name, Equals, \"b_-1\")\n\tc.Assert(indexes[2].Key, DeepEquals, []string{\"-b\"})\n\tc.Assert(indexes[3].Name, Equals, \"c_2d\")\n\tc.Assert(indexes[3].Key, DeepEquals, []string{\"$2d:c\"})\n\tc.Assert(indexes[4].Name, Equals, \"d_2d\")\n\tc.Assert(indexes[4].Key, DeepEquals, []string{\"$2d:d\"})\n}\n\nfunc (s *S) TestEnsureIndexNameCaching(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"a\"}, Name: \"custom\"})\n\tc.Assert(err, IsNil)\n\n\tmgo.ResetStats()\n\n\t// Second EnsureIndex should be cached and do nothing.\n\terr = coll.EnsureIndexKey(\"a\")\n\tc.Assert(err, IsNil)\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"a\"}, Name: \"custom\"})\n\tc.Assert(err, IsNil)\n\n\tstats := mgo.GetStats()\n\tc.Assert(stats.SentOps, Equals, 0)\n\n\t// Resetting the cache should make it contact the server again.\n\tsession.ResetIndexCache()\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"a\"}, Name: \"custom\"})\n\tc.Assert(err, IsNil)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SentOps > 0, Equals, true)\n\n\t// Dropping the index should also drop the cached index key.\n\terr = coll.DropIndexName(\"custom\")\n\tc.Assert(err, IsNil)\n\n\tmgo.ResetStats()\n\n\terr = coll.EnsureIndex(mgo.Index{Key: []string{\"a\"}, Name: \"custom\"})\n\tc.Assert(err, IsNil)\n\n\tstats = mgo.GetStats()\n\tc.Assert(stats.SentOps > 0, Equals, true)\n}\n\nfunc (s *S) TestEnsureIndexEvalGetIndexes(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = session.Run(bson.D{{\"eval\", \"db.getSiblingDB('mydb').mycoll.ensureIndex({b: -1})\"}}, nil)\n\tc.Assert(err, IsNil)\n\terr = session.Run(bson.D{{\"eval\", \"db.getSiblingDB('mydb').mycoll.ensureIndex({a: 1})\"}}, nil)\n\tc.Assert(err, IsNil)\n\terr = session.Run(bson.D{{\"eval\", \"db.getSiblingDB('mydb').mycoll.ensureIndex({c: -1, e: 1})\"}}, nil)\n\tc.Assert(err, IsNil)\n\terr = session.Run(bson.D{{\"eval\", \"db.getSiblingDB('mydb').mycoll.ensureIndex({d: '2d'})\"}}, nil)\n\tc.Assert(err, IsNil)\n\n\tindexes, err := coll.Indexes()\n\tc.Assert(err, IsNil)\n\n\tc.Assert(indexes[0].Name, Equals, \"_id_\")\n\tc.Assert(indexes[1].Name, Equals, \"a_1\")\n\tc.Assert(indexes[1].Key, DeepEquals, []string{\"a\"})\n\tc.Assert(indexes[2].Name, Equals, \"b_-1\")\n\tc.Assert(indexes[2].Key, DeepEquals, []string{\"-b\"})\n\tc.Assert(indexes[3].Name, Equals, \"c_-1_e_1\")\n\tc.Assert(indexes[3].Key, DeepEquals, []string{\"-c\", \"e\"})\n\tif s.versionAtLeast(2, 2) {\n\t\tc.Assert(indexes[4].Name, Equals, \"d_2d\")\n\t\tc.Assert(indexes[4].Key, DeepEquals, []string{\"$2d:d\"})\n\t} else {\n\t\tc.Assert(indexes[4].Name, Equals, \"d_\")\n\t\tc.Assert(indexes[4].Key, DeepEquals, []string{\"$2d:d\"})\n\t}\n}\n\nvar testTTL = flag.Bool(\"test-ttl\", false, \"test TTL collections (may take 1 minute)\")\n\nfunc (s *S) TestEnsureIndexExpireAfter(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tsession.SetSafe(nil)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\terr = coll.Insert(M{\"n\": 1, \"t\": time.Now().Add(-120 * time.Second)})\n\tc.Assert(err, IsNil)\n\terr = coll.Insert(M{\"n\": 2, \"t\": time.Now()})\n\tc.Assert(err, IsNil)\n\n\t// Should fail since there are duplicated entries.\n\tindex := mgo.Index{\n\t\tKey:         []string{\"t\"},\n\t\tExpireAfter: 1 * time.Minute,\n\t}\n\n\terr = coll.EnsureIndex(index)\n\tc.Assert(err, IsNil)\n\n\tindexes, err := coll.Indexes()\n\tc.Assert(err, IsNil)\n\tc.Assert(indexes[1].Name, Equals, \"t_1\")\n\tc.Assert(indexes[1].ExpireAfter, Equals, 1*time.Minute)\n\n\tif *testTTL {\n\t\tworked := false\n\t\tstop := time.Now().Add(70 * time.Second)\n\t\tfor time.Now().Before(stop) {\n\t\t\tn, err := coll.Count()\n\t\t\tc.Assert(err, IsNil)\n\t\t\tif n == 1 {\n\t\t\t\tworked = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tc.Assert(n, Equals, 2)\n\t\t\tc.Logf(\"Still has 2 entries...\")\n\t\t\ttime.Sleep(1 * time.Second)\n\t\t}\n\t\tif !worked {\n\t\t\tc.Fatalf(\"TTL index didn't work\")\n\t\t}\n\t}\n}\n\nfunc (s *S) TestDistinct(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tvar result []int\n\terr = coll.Find(M{\"n\": M{\"$gt\": 2}}).Sort(\"n\").Distinct(\"n\", &result)\n\n\tsort.IntSlice(result).Sort()\n\tc.Assert(result, DeepEquals, []int{3, 4, 6})\n}\n\nfunc (s *S) TestMapReduce(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, 1); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t}\n\tvar result []struct {\n\t\tId    int \"_id\"\n\t\tValue int\n\t}\n\n\tinfo, err := coll.Find(M{\"n\": M{\"$gt\": 2}}).MapReduce(job, &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.InputCount, Equals, 4)\n\tc.Assert(info.EmitCount, Equals, 4)\n\tc.Assert(info.OutputCount, Equals, 3)\n\tc.Assert(info.VerboseTime, IsNil)\n\n\texpected := map[int]int{3: 1, 4: 2, 6: 1}\n\tfor _, item := range result {\n\t\tc.Logf(\"Item: %#v\", &item)\n\t\tc.Assert(item.Value, Equals, expected[item.Id])\n\t\texpected[item.Id] = -1\n\t}\n}\n\nfunc (s *S) TestMapReduceFinalize(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:      \"function() { emit(this.n, 1) }\",\n\t\tReduce:   \"function(key, values) { return Array.sum(values) }\",\n\t\tFinalize: \"function(key, count) { return {count: count} }\",\n\t}\n\tvar result []struct {\n\t\tId    int \"_id\"\n\t\tValue struct{ Count int }\n\t}\n\t_, err = coll.Find(nil).MapReduce(job, &result)\n\tc.Assert(err, IsNil)\n\n\texpected := map[int]int{1: 1, 2: 2, 3: 1, 4: 2, 6: 1}\n\tfor _, item := range result {\n\t\tc.Logf(\"Item: %#v\", &item)\n\t\tc.Assert(item.Value.Count, Equals, expected[item.Id])\n\t\texpected[item.Id] = -1\n\t}\n}\n\nfunc (s *S) TestMapReduceToCollection(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, 1); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t\tOut:    \"mr\",\n\t}\n\n\tinfo, err := coll.Find(nil).MapReduce(job, nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.InputCount, Equals, 7)\n\tc.Assert(info.EmitCount, Equals, 7)\n\tc.Assert(info.OutputCount, Equals, 5)\n\tc.Assert(info.Collection, Equals, \"mr\")\n\tc.Assert(info.Database, Equals, \"mydb\")\n\n\texpected := map[int]int{1: 1, 2: 2, 3: 1, 4: 2, 6: 1}\n\tvar item *struct {\n\t\tId    int \"_id\"\n\t\tValue int\n\t}\n\tmr := session.DB(\"mydb\").C(\"mr\")\n\titer := mr.Find(nil).Iter()\n\tfor iter.Next(&item) {\n\t\tc.Logf(\"Item: %#v\", &item)\n\t\tc.Assert(item.Value, Equals, expected[item.Id])\n\t\texpected[item.Id] = -1\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestMapReduceToOtherDb(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, 1); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t\tOut:    bson.D{{\"replace\", \"mr\"}, {\"db\", \"otherdb\"}},\n\t}\n\n\tinfo, err := coll.Find(nil).MapReduce(job, nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.InputCount, Equals, 7)\n\tc.Assert(info.EmitCount, Equals, 7)\n\tc.Assert(info.OutputCount, Equals, 5)\n\tc.Assert(info.Collection, Equals, \"mr\")\n\tc.Assert(info.Database, Equals, \"otherdb\")\n\n\texpected := map[int]int{1: 1, 2: 2, 3: 1, 4: 2, 6: 1}\n\tvar item *struct {\n\t\tId    int \"_id\"\n\t\tValue int\n\t}\n\tmr := session.DB(\"otherdb\").C(\"mr\")\n\titer := mr.Find(nil).Iter()\n\tfor iter.Next(&item) {\n\t\tc.Logf(\"Item: %#v\", &item)\n\t\tc.Assert(item.Value, Equals, expected[item.Id])\n\t\texpected[item.Id] = -1\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestMapReduceOutOfOrder(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, 1); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t\tOut:    bson.M{\"a\": \"a\", \"z\": \"z\", \"replace\": \"mr\", \"db\": \"otherdb\", \"b\": \"b\", \"y\": \"y\"},\n\t}\n\n\tinfo, err := coll.Find(nil).MapReduce(job, nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.Collection, Equals, \"mr\")\n\tc.Assert(info.Database, Equals, \"otherdb\")\n}\n\nfunc (s *S) TestMapReduceScope(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tcoll.Insert(M{\"n\": 1})\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, x); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t\tScope:  M{\"x\": 42},\n\t}\n\n\tvar result []bson.M\n\t_, err = coll.Find(nil).MapReduce(job, &result)\n\tc.Assert(len(result), Equals, 1)\n\tc.Assert(result[0][\"value\"], Equals, 42.0)\n}\n\nfunc (s *S) TestMapReduceVerbose(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor i := 0; i < 100; i++ {\n\t\terr = coll.Insert(M{\"n\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:     \"function() { emit(this.n, 1); }\",\n\t\tReduce:  \"function(key, values) { return Array.sum(values); }\",\n\t\tVerbose: true,\n\t}\n\n\tinfo, err := coll.Find(nil).MapReduce(job, nil)\n\tc.Assert(err, IsNil)\n\tc.Assert(info.VerboseTime, NotNil)\n}\n\nfunc (s *S) TestMapReduceLimit(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor _, i := range []int{1, 4, 6, 2, 2, 3, 4} {\n\t\tcoll.Insert(M{\"n\": i})\n\t}\n\n\tjob := &mgo.MapReduce{\n\t\tMap:    \"function() { emit(this.n, 1); }\",\n\t\tReduce: \"function(key, values) { return Array.sum(values); }\",\n\t}\n\n\tvar result []bson.M\n\t_, err = coll.Find(nil).Limit(3).MapReduce(job, &result)\n\tc.Assert(err, IsNil)\n\tc.Assert(len(result), Equals, 3)\n}\n\nfunc (s *S) TestBuildInfo(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tinfo, err := session.BuildInfo()\n\tc.Assert(err, IsNil)\n\n\tvar v []int\n\tfor i, a := range strings.Split(info.Version, \".\") {\n\t\tfor _, token := range []string{\"-rc\", \"-pre\"} {\n\t\t\tif i == 2 && strings.Contains(a, token) {\n\t\t\t\ta = a[:strings.Index(a, token)]\n\t\t\t\tinfo.VersionArray[len(info.VersionArray)-1] = 0\n\t\t\t}\n\t\t}\n\t\tn, err := strconv.Atoi(a)\n\t\tc.Assert(err, IsNil)\n\t\tv = append(v, n)\n\t}\n\tfor len(v) < 4 {\n\t\tv = append(v, 0)\n\t}\n\n\tc.Assert(info.VersionArray, DeepEquals, v)\n\tc.Assert(info.GitVersion, Matches, \"[a-z0-9]+\")\n\n\tif s.versionAtLeast(3, 2) {\n\t\t// It was deprecated in 3.2.\n\t\tc.Assert(info.SysInfo, Equals, \"\")\n\t} else {\n\t\tc.Assert(info.SysInfo, Matches, \".*[0-9:]+.*\")\n\t}\n\tif info.Bits != 32 && info.Bits != 64 {\n\t\tc.Fatalf(\"info.Bits is %d\", info.Bits)\n\t}\n\tif info.MaxObjectSize < 8192 {\n\t\tc.Fatalf(\"info.MaxObjectSize seems too small: %d\", info.MaxObjectSize)\n\t}\n}\n\nfunc (s *S) TestZeroTimeRoundtrip(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tvar d struct{ T time.Time }\n\tconn := session.DB(\"mydb\").C(\"mycoll\")\n\terr = conn.Insert(d)\n\tc.Assert(err, IsNil)\n\n\tvar result bson.M\n\terr = conn.Find(nil).One(&result)\n\tc.Assert(err, IsNil)\n\tt, isTime := result[\"t\"].(time.Time)\n\tc.Assert(isTime, Equals, true)\n\tc.Assert(t, Equals, time.Time{})\n}\n\nfunc (s *S) TestFsyncLock(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tclone := session.Clone()\n\tdefer clone.Close()\n\n\terr = session.FsyncLock()\n\tc.Assert(err, IsNil)\n\n\tdone := make(chan time.Time)\n\tgo func() {\n\t\ttime.Sleep(3 * time.Second)\n\t\tnow := time.Now()\n\t\terr := session.FsyncUnlock()\n\t\tc.Check(err, IsNil)\n\t\tdone <- now\n\t}()\n\n\terr = clone.DB(\"mydb\").C(\"mycoll\").Insert(bson.M{\"n\": 1})\n\tunlocked := time.Now()\n\tunlocking := <-done\n\tc.Assert(err, IsNil)\n\n\tc.Assert(unlocked.After(unlocking), Equals, true)\n}\n\nfunc (s *S) TestFsync(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\t// Not much to do here. Just a smoke check.\n\terr = session.Fsync(false)\n\tc.Assert(err, IsNil)\n\terr = session.Fsync(true)\n\tc.Assert(err, IsNil)\n}\n\nfunc (s *S) TestRepairCursor(c *C) {\n\tif !s.versionAtLeast(2, 7) {\n\t\tc.Skip(\"RepairCursor only works on 2.7+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\tsession.SetBatch(2)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll3\")\n\terr = coll.DropCollection()\n\n\tns := []int{0, 10, 20, 30, 40, 50}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\trepairIter := coll.Repair()\n\n\tc.Assert(repairIter.Err(), IsNil)\n\n\tresult := struct{ N int }{}\n\tresultCounts := map[int]int{}\n\tfor repairIter.Next(&result) {\n\t\tresultCounts[result.N]++\n\t}\n\n\tc.Assert(repairIter.Next(&result), Equals, false)\n\tc.Assert(repairIter.Err(), IsNil)\n\tc.Assert(repairIter.Close(), IsNil)\n\n\t// Verify that the results of the repair cursor are valid.\n\t// The repair cursor can return multiple copies\n\t// of the same document, so to check correctness we only\n\t// need to verify that at least 1 of each document was returned.\n\n\tfor _, key := range ns {\n\t\tc.Assert(resultCounts[key] > 0, Equals, true)\n\t}\n}\n\nfunc (s *S) TestPipeIter(c *C) {\n\tif !s.versionAtLeast(2, 1) {\n\t\tc.Skip(\"Pipe only works on 2.1+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\tpipe := coll.Pipe([]M{{\"$match\": M{\"n\": M{\"$gte\": 42}}}})\n\n\t// Ensure cursor logic is working by forcing a small batch.\n\tpipe.Batch(2)\n\n\t// Smoke test for AllowDiskUse.\n\tpipe.AllowDiskUse()\n\n\titer := pipe.Iter()\n\tresult := struct{ N int }{}\n\tfor i := 2; i < 7; i++ {\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true)\n\t\tc.Assert(result.N, Equals, ns[i])\n\t}\n\n\tc.Assert(iter.Next(&result), Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestPipeAll(c *C) {\n\tif !s.versionAtLeast(2, 1) {\n\t\tc.Skip(\"Pipe only works on 2.1+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr := coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tvar result []struct{ N int }\n\terr = coll.Pipe([]M{{\"$match\": M{\"n\": M{\"$gte\": 42}}}}).All(&result)\n\tc.Assert(err, IsNil)\n\tfor i := 2; i < 7; i++ {\n\t\tc.Assert(result[i-2].N, Equals, ns[i])\n\t}\n}\n\nfunc (s *S) TestPipeOne(c *C) {\n\tif !s.versionAtLeast(2, 1) {\n\t\tc.Skip(\"Pipe only works on 2.1+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.Insert(M{\"a\": 1, \"b\": 2})\n\n\tresult := struct{ A, B int }{}\n\n\tpipe := coll.Pipe([]M{{\"$project\": M{\"a\": 1, \"b\": M{\"$add\": []interface{}{\"$b\", 1}}}}})\n\terr = pipe.One(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.A, Equals, 1)\n\tc.Assert(result.B, Equals, 3)\n\n\tpipe = coll.Pipe([]M{{\"$match\": M{\"a\": 2}}})\n\terr = pipe.One(&result)\n\tc.Assert(err, Equals, mgo.ErrNotFound)\n}\n\nfunc (s *S) TestPipeExplain(c *C) {\n\tif !s.versionAtLeast(2, 1) {\n\t\tc.Skip(\"Pipe only works on 2.1+\")\n\t}\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tcoll.Insert(M{\"a\": 1, \"b\": 2})\n\n\tpipe := coll.Pipe([]M{{\"$project\": M{\"a\": 1, \"b\": M{\"$add\": []interface{}{\"$b\", 1}}}}})\n\n\t// The explain command result changes across versions.\n\tvar result struct{ Ok int }\n\terr = pipe.Explain(&result)\n\tc.Assert(err, IsNil)\n\tc.Assert(result.Ok, Equals, 1)\n}\n\nfunc (s *S) TestBatch1Bug(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor i := 0; i < 3; i++ {\n\t\terr := coll.Insert(M{\"n\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tvar ns []struct{ N int }\n\terr = coll.Find(nil).Batch(1).All(&ns)\n\tc.Assert(err, IsNil)\n\tc.Assert(len(ns), Equals, 3)\n\n\tsession.SetBatch(1)\n\terr = coll.Find(nil).All(&ns)\n\tc.Assert(err, IsNil)\n\tc.Assert(len(ns), Equals, 3)\n}\n\nfunc (s *S) TestInterfaceIterBug(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tfor i := 0; i < 3; i++ {\n\t\terr := coll.Insert(M{\"n\": i})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tvar result interface{}\n\n\ti := 0\n\titer := coll.Find(nil).Sort(\"n\").Iter()\n\tfor iter.Next(&result) {\n\t\tc.Assert(result.(bson.M)[\"n\"], Equals, i)\n\t\ti++\n\t}\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestFindIterCloseKillsCursor(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcursors := serverCursorsOpen(session)\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\terr = coll.Insert(M{\"n\": n})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\titer := coll.Find(nil).Batch(2).Iter()\n\tc.Assert(iter.Next(bson.M{}), Equals, true)\n\n\tc.Assert(iter.Close(), IsNil)\n\tc.Assert(serverCursorsOpen(session), Equals, cursors)\n}\n\nfunc (s *S) TestFindIterDoneWithBatches(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tns := []int{40, 41, 42, 43, 44, 45, 46}\n\tfor _, n := range ns {\n\t\tcoll.Insert(M{\"n\": n})\n\t}\n\n\titer := coll.Find(M{\"n\": M{\"$gte\": 42}}).Sort(\"$natural\").Prefetch(0).Batch(2).Iter()\n\tresult := struct{ N int }{}\n\tfor i := 2; i < 7; i++ {\n\t\t// first check will be with pending local record;\n\t\t// second will be with open cursor ID but no local\n\t\t// records\n\t\tc.Assert(iter.Done(), Equals, false)\n\t\tok := iter.Next(&result)\n\t\tc.Assert(ok, Equals, true, Commentf(\"err=%v\", err))\n\t}\n\n\tc.Assert(iter.Done(), Equals, true)\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Close(), IsNil)\n}\n\nfunc (s *S) TestFindIterDoneErr(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40002\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\titer := coll.Find(nil).Iter()\n\n\tresult := struct{}{}\n\tok := iter.Next(&result)\n\tc.Assert(iter.Done(), Equals, true)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Err(), ErrorMatches, \"unauthorized.*|not authorized.*\")\n}\n\nfunc (s *S) TestFindIterDoneNotFound(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\n\tresult := struct{ A, B int }{}\n\titer := coll.Find(M{\"a\": 1}).Iter()\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\tc.Assert(iter.Done(), Equals, true)\n}\n\nfunc (s *S) TestLogReplay(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tfor i := 0; i < 5; i++ {\n\t\terr = coll.Insert(M{\"ts\": time.Now()})\n\t\tc.Assert(err, IsNil)\n\t}\n\n\titer := coll.Find(nil).LogReplay().Iter()\n\tif s.versionAtLeast(2, 6) {\n\t\t// This used to fail in 2.4. Now it's just a smoke test.\n\t\tc.Assert(iter.Err(), IsNil)\n\t} else {\n\t\tc.Assert(iter.Next(bson.M{}), Equals, false)\n\t\tc.Assert(iter.Err(), ErrorMatches, \"no ts field in query\")\n\t}\n}\n\nfunc (s *S) TestSetCursorTimeout(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 42})\n\n\t// This is just a smoke test. Won't wait 10 minutes for an actual timeout.\n\n\tsession.SetCursorTimeout(0)\n\n\tvar result struct{ N int }\n\titer := coll.Find(nil).Iter()\n\tc.Assert(iter.Next(&result), Equals, true)\n\tc.Assert(result.N, Equals, 42)\n\tc.Assert(iter.Next(&result), Equals, false)\n}\n\nfunc (s *S) TestNewIterNoServer(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdata, err := bson.Marshal(bson.M{\"a\": 1})\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\titer := coll.NewIter(nil, []bson.Raw{{3, data}}, 42, nil)\n\n\tvar result struct{ A int }\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(result.A, Equals, 1)\n\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\n\tc.Assert(iter.Err(), ErrorMatches, \"server not available\")\n}\n\nfunc (s *S) TestNewIterNoServerPresetErr(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tdata, err := bson.Marshal(bson.M{\"a\": 1})\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\titer := coll.NewIter(nil, []bson.Raw{{3, data}}, 42, fmt.Errorf(\"my error\"))\n\n\tvar result struct{ A int }\n\tok := iter.Next(&result)\n\tc.Assert(ok, Equals, true)\n\tc.Assert(result.A, Equals, 1)\n\n\tok = iter.Next(&result)\n\tc.Assert(ok, Equals, false)\n\n\tc.Assert(iter.Err(), ErrorMatches, \"my error\")\n}\n\nfunc (s *S) TestBypassValidation(c *C) {\n\tif !s.versionAtLeast(3, 2) {\n\t\tc.Skip(\"validation supported on 3.2+\")\n\t}\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\terr = coll.Insert(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Database.Run(bson.D{\n\t\t{\"collMod\", \"mycoll\"},\n\t\t{\"validator\", M{\"s\": M{\"$type\": \"string\"}}},\n\t}, nil)\n\tc.Assert(err, IsNil)\n\n\terr = coll.Insert(M{\"n\": 2})\n\tc.Assert(err, ErrorMatches, \"Document failed validation\")\n\n\terr = coll.Update(M{\"n\": 1}, M{\"n\": 10})\n\tc.Assert(err, ErrorMatches, \"Document failed validation\")\n\n\tsession.SetBypassValidation(true)\n\n\terr = coll.Insert(M{\"n\": 3})\n\tc.Assert(err, IsNil)\n\n\terr = coll.Update(M{\"n\": 3}, M{\"n\": 4})\n\tc.Assert(err, IsNil)\n\n\t// Ensure this still works. Shouldn't be affected.\n\terr = coll.Remove(M{\"n\": 1})\n\tc.Assert(err, IsNil)\n\n\tvar result struct{ N int }\n\tvar ns []int\n\titer := coll.Find(nil).Iter()\n\tfor iter.Next(&result) {\n\t\tns = append(ns, result.N)\n\t}\n\tc.Assert(iter.Err(), IsNil)\n\tsort.Ints(ns)\n\tc.Assert(ns, DeepEquals, []int{4})\n}\n\nfunc (s *S) TestVersionAtLeast(c *C) {\n\ttests := [][][]int{\n\t\t{{3, 2, 1}, {3, 2, 0}},\n\t\t{{3, 2, 1}, {3, 2}},\n\t\t{{3, 2, 1}, {2, 5, 5, 5}},\n\t\t{{3, 2, 1}, {2, 5, 5}},\n\t\t{{3, 2, 1}, {2, 5}},\n\t}\n\tfor _, pair := range tests {\n\t\tbi := mgo.BuildInfo{VersionArray: pair[0]}\n\t\tc.Assert(bi.VersionAtLeast(pair[1]...), Equals, true)\n\n\t\tbi = mgo.BuildInfo{VersionArray: pair[0]}\n\t\tc.Assert(bi.VersionAtLeast(pair[0]...), Equals, true)\n\n\t\tbi = mgo.BuildInfo{VersionArray: pair[1]}\n\t\tc.Assert(bi.VersionAtLeast(pair[1]...), Equals, true)\n\n\t\tbi = mgo.BuildInfo{VersionArray: pair[1]}\n\t\tc.Assert(bi.VersionAtLeast(pair[0]...), Equals, false)\n\t}\n}\n\n// --------------------------------------------------------------------------\n// Some benchmarks that require a running database.\n\nfunc (s *S) BenchmarkFindIterRaw(c *C) {\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\tdefer session.Close()\n\n\tcoll := session.DB(\"mydb\").C(\"mycoll\")\n\tdoc := bson.D{\n\t\t{\"f2\", \"a short string\"},\n\t\t{\"f3\", bson.D{{\"1\", \"one\"}, {\"2\", 2.0}}},\n\t\t{\"f4\", []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}},\n\t}\n\n\tfor i := 0; i < c.N+1; i++ {\n\t\terr := coll.Insert(doc)\n\t\tc.Assert(err, IsNil)\n\t}\n\n\tsession.SetBatch(c.N)\n\n\tvar raw bson.Raw\n\titer := coll.Find(nil).Iter()\n\titer.Next(&raw)\n\tc.ResetTimer()\n\ti := 0\n\tfor iter.Next(&raw) {\n\t\ti++\n\t}\n\tc.StopTimer()\n\tc.Assert(iter.Err(), IsNil)\n\tc.Assert(i, Equals, c.N)\n}\n"
        },
        {
          "name": "socket.go",
          "type": "blob",
          "size": 18.03515625,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\ntype replyFunc func(err error, reply *replyOp, docNum int, docData []byte)\n\ntype mongoSocket struct {\n\tsync.Mutex\n\tserver        *mongoServer // nil when cached\n\tconn          net.Conn\n\ttimeout       time.Duration\n\taddr          string // For debugging only.\n\tnextRequestId uint32\n\treplyFuncs    map[uint32]replyFunc\n\treferences    int\n\tcreds         []Credential\n\tlogout        []Credential\n\tcachedNonce   string\n\tgotNonce      sync.Cond\n\tdead          error\n\tserverInfo    *mongoServerInfo\n}\n\ntype queryOpFlags uint32\n\nconst (\n\t_ queryOpFlags = 1 << iota\n\tflagTailable\n\tflagSlaveOk\n\tflagLogReplay\n\tflagNoCursorTimeout\n\tflagAwaitData\n)\n\ntype queryOp struct {\n\tcollection string\n\tquery      interface{}\n\tskip       int32\n\tlimit      int32\n\tselector   interface{}\n\tflags      queryOpFlags\n\treplyFunc  replyFunc\n\n\tmode       Mode\n\toptions    queryWrapper\n\thasOptions bool\n\tserverTags []bson.D\n}\n\ntype queryWrapper struct {\n\tQuery          interface{} \"$query\"\n\tOrderBy        interface{} \"$orderby,omitempty\"\n\tHint           interface{} \"$hint,omitempty\"\n\tExplain        bool        \"$explain,omitempty\"\n\tSnapshot       bool        \"$snapshot,omitempty\"\n\tReadPreference bson.D      \"$readPreference,omitempty\"\n\tMaxScan        int         \"$maxScan,omitempty\"\n\tMaxTimeMS      int         \"$maxTimeMS,omitempty\"\n\tComment        string      \"$comment,omitempty\"\n}\n\nfunc (op *queryOp) finalQuery(socket *mongoSocket) interface{} {\n\tif op.flags&flagSlaveOk != 0 && socket.ServerInfo().Mongos {\n\t\tvar modeName string\n\t\tswitch op.mode {\n\t\tcase Strong:\n\t\t\tmodeName = \"primary\"\n\t\tcase Monotonic, Eventual:\n\t\t\tmodeName = \"secondaryPreferred\"\n\t\tcase PrimaryPreferred:\n\t\t\tmodeName = \"primaryPreferred\"\n\t\tcase Secondary:\n\t\t\tmodeName = \"secondary\"\n\t\tcase SecondaryPreferred:\n\t\t\tmodeName = \"secondaryPreferred\"\n\t\tcase Nearest:\n\t\t\tmodeName = \"nearest\"\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unsupported read mode: %d\", op.mode))\n\t\t}\n\t\top.hasOptions = true\n\t\top.options.ReadPreference = make(bson.D, 0, 2)\n\t\top.options.ReadPreference = append(op.options.ReadPreference, bson.DocElem{\"mode\", modeName})\n\t\tif len(op.serverTags) > 0 {\n\t\t\top.options.ReadPreference = append(op.options.ReadPreference, bson.DocElem{\"tags\", op.serverTags})\n\t\t}\n\t}\n\tif op.hasOptions {\n\t\tif op.query == nil {\n\t\t\tvar empty bson.D\n\t\t\top.options.Query = empty\n\t\t} else {\n\t\t\top.options.Query = op.query\n\t\t}\n\t\tdebugf(\"final query is %#v\\n\", &op.options)\n\t\treturn &op.options\n\t}\n\treturn op.query\n}\n\ntype getMoreOp struct {\n\tcollection string\n\tlimit      int32\n\tcursorId   int64\n\treplyFunc  replyFunc\n}\n\ntype replyOp struct {\n\tflags     uint32\n\tcursorId  int64\n\tfirstDoc  int32\n\treplyDocs int32\n}\n\ntype insertOp struct {\n\tcollection string        // \"database.collection\"\n\tdocuments  []interface{} // One or more documents to insert\n\tflags      uint32\n}\n\ntype updateOp struct {\n\tCollection string      `bson:\"-\"` // \"database.collection\"\n\tSelector   interface{} `bson:\"q\"`\n\tUpdate     interface{} `bson:\"u\"`\n\tFlags      uint32      `bson:\"-\"`\n\tMulti      bool        `bson:\"multi,omitempty\"`\n\tUpsert     bool        `bson:\"upsert,omitempty\"`\n}\n\ntype deleteOp struct {\n\tCollection string      `bson:\"-\"` // \"database.collection\"\n\tSelector   interface{} `bson:\"q\"`\n\tFlags      uint32      `bson:\"-\"`\n\tLimit      int         `bson:\"limit\"`\n}\n\ntype killCursorsOp struct {\n\tcursorIds []int64\n}\n\ntype requestInfo struct {\n\tbufferPos int\n\treplyFunc replyFunc\n}\n\nfunc newSocket(server *mongoServer, conn net.Conn, timeout time.Duration) *mongoSocket {\n\tsocket := &mongoSocket{\n\t\tconn:       conn,\n\t\taddr:       server.Addr,\n\t\tserver:     server,\n\t\treplyFuncs: make(map[uint32]replyFunc),\n\t}\n\tsocket.gotNonce.L = &socket.Mutex\n\tif err := socket.InitialAcquire(server.Info(), timeout); err != nil {\n\t\tpanic(\"newSocket: InitialAcquire returned error: \" + err.Error())\n\t}\n\tstats.socketsAlive(+1)\n\tdebugf(\"Socket %p to %s: initialized\", socket, socket.addr)\n\tsocket.resetNonce()\n\tgo socket.readLoop()\n\treturn socket\n}\n\n// Server returns the server that the socket is associated with.\n// It returns nil while the socket is cached in its respective server.\nfunc (socket *mongoSocket) Server() *mongoServer {\n\tsocket.Lock()\n\tserver := socket.server\n\tsocket.Unlock()\n\treturn server\n}\n\n// ServerInfo returns details for the server at the time the socket\n// was initially acquired.\nfunc (socket *mongoSocket) ServerInfo() *mongoServerInfo {\n\tsocket.Lock()\n\tserverInfo := socket.serverInfo\n\tsocket.Unlock()\n\treturn serverInfo\n}\n\n// InitialAcquire obtains the first reference to the socket, either\n// right after the connection is made or once a recycled socket is\n// being put back in use.\nfunc (socket *mongoSocket) InitialAcquire(serverInfo *mongoServerInfo, timeout time.Duration) error {\n\tsocket.Lock()\n\tif socket.references > 0 {\n\t\tpanic(\"Socket acquired out of cache with references\")\n\t}\n\tif socket.dead != nil {\n\t\tdead := socket.dead\n\t\tsocket.Unlock()\n\t\treturn dead\n\t}\n\tsocket.references++\n\tsocket.serverInfo = serverInfo\n\tsocket.timeout = timeout\n\tstats.socketsInUse(+1)\n\tstats.socketRefs(+1)\n\tsocket.Unlock()\n\treturn nil\n}\n\n// Acquire obtains an additional reference to the socket.\n// The socket will only be recycled when it's released as many\n// times as it's been acquired.\nfunc (socket *mongoSocket) Acquire() (info *mongoServerInfo) {\n\tsocket.Lock()\n\tif socket.references == 0 {\n\t\tpanic(\"Socket got non-initial acquire with references == 0\")\n\t}\n\t// We'll track references to dead sockets as well.\n\t// Caller is still supposed to release the socket.\n\tsocket.references++\n\tstats.socketRefs(+1)\n\tserverInfo := socket.serverInfo\n\tsocket.Unlock()\n\treturn serverInfo\n}\n\n// Release decrements a socket reference. The socket will be\n// recycled once its released as many times as it's been acquired.\nfunc (socket *mongoSocket) Release() {\n\tsocket.Lock()\n\tif socket.references == 0 {\n\t\tpanic(\"socket.Release() with references == 0\")\n\t}\n\tsocket.references--\n\tstats.socketRefs(-1)\n\tif socket.references == 0 {\n\t\tstats.socketsInUse(-1)\n\t\tserver := socket.server\n\t\tsocket.Unlock()\n\t\tsocket.LogoutAll()\n\t\t// If the socket is dead server is nil.\n\t\tif server != nil {\n\t\t\tserver.RecycleSocket(socket)\n\t\t}\n\t} else {\n\t\tsocket.Unlock()\n\t}\n}\n\n// SetTimeout changes the timeout used on socket operations.\nfunc (socket *mongoSocket) SetTimeout(d time.Duration) {\n\tsocket.Lock()\n\tsocket.timeout = d\n\tsocket.Unlock()\n}\n\ntype deadlineType int\n\nconst (\n\treadDeadline  deadlineType = 1\n\twriteDeadline deadlineType = 2\n)\n\nfunc (socket *mongoSocket) updateDeadline(which deadlineType) {\n\tvar when time.Time\n\tif socket.timeout > 0 {\n\t\twhen = time.Now().Add(socket.timeout)\n\t}\n\twhichstr := \"\"\n\tswitch which {\n\tcase readDeadline | writeDeadline:\n\t\twhichstr = \"read/write\"\n\t\tsocket.conn.SetDeadline(when)\n\tcase readDeadline:\n\t\twhichstr = \"read\"\n\t\tsocket.conn.SetReadDeadline(when)\n\tcase writeDeadline:\n\t\twhichstr = \"write\"\n\t\tsocket.conn.SetWriteDeadline(when)\n\tdefault:\n\t\tpanic(\"invalid parameter to updateDeadline\")\n\t}\n\tdebugf(\"Socket %p to %s: updated %s deadline to %s ahead (%s)\", socket, socket.addr, whichstr, socket.timeout, when)\n}\n\n// Close terminates the socket use.\nfunc (socket *mongoSocket) Close() {\n\tsocket.kill(errors.New(\"Closed explicitly\"), false)\n}\n\nfunc (socket *mongoSocket) kill(err error, abend bool) {\n\tsocket.Lock()\n\tif socket.dead != nil {\n\t\tdebugf(\"Socket %p to %s: killed again: %s (previously: %s)\", socket, socket.addr, err.Error(), socket.dead.Error())\n\t\tsocket.Unlock()\n\t\treturn\n\t}\n\tlogf(\"Socket %p to %s: closing: %s (abend=%v)\", socket, socket.addr, err.Error(), abend)\n\tsocket.dead = err\n\tsocket.conn.Close()\n\tstats.socketsAlive(-1)\n\treplyFuncs := socket.replyFuncs\n\tsocket.replyFuncs = make(map[uint32]replyFunc)\n\tserver := socket.server\n\tsocket.server = nil\n\tsocket.gotNonce.Broadcast()\n\tsocket.Unlock()\n\tfor _, replyFunc := range replyFuncs {\n\t\tlogf(\"Socket %p to %s: notifying replyFunc of closed socket: %s\", socket, socket.addr, err.Error())\n\t\treplyFunc(err, nil, -1, nil)\n\t}\n\tif abend {\n\t\tserver.AbendSocket(socket)\n\t}\n}\n\nfunc (socket *mongoSocket) SimpleQuery(op *queryOp) (data []byte, err error) {\n\tvar wait, change sync.Mutex\n\tvar replyDone bool\n\tvar replyData []byte\n\tvar replyErr error\n\twait.Lock()\n\top.replyFunc = func(err error, reply *replyOp, docNum int, docData []byte) {\n\t\tchange.Lock()\n\t\tif !replyDone {\n\t\t\treplyDone = true\n\t\t\treplyErr = err\n\t\t\tif err == nil {\n\t\t\t\treplyData = docData\n\t\t\t}\n\t\t}\n\t\tchange.Unlock()\n\t\twait.Unlock()\n\t}\n\terr = socket.Query(op)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twait.Lock()\n\tchange.Lock()\n\tdata = replyData\n\terr = replyErr\n\tchange.Unlock()\n\treturn data, err\n}\n\nfunc (socket *mongoSocket) Query(ops ...interface{}) (err error) {\n\n\tif lops := socket.flushLogout(); len(lops) > 0 {\n\t\tops = append(lops, ops...)\n\t}\n\n\tbuf := make([]byte, 0, 256)\n\n\t// Serialize operations synchronously to avoid interrupting\n\t// other goroutines while we can't really be sending data.\n\t// Also, record id positions so that we can compute request\n\t// ids at once later with the lock already held.\n\trequests := make([]requestInfo, len(ops))\n\trequestCount := 0\n\n\tfor _, op := range ops {\n\t\tdebugf(\"Socket %p to %s: serializing op: %#v\", socket, socket.addr, op)\n\t\tif qop, ok := op.(*queryOp); ok {\n\t\t\tif cmd, ok := qop.query.(*findCmd); ok {\n\t\t\t\tdebugf(\"Socket %p to %s: find command: %#v\", socket, socket.addr, cmd)\n\t\t\t}\n\t\t}\n\t\tstart := len(buf)\n\t\tvar replyFunc replyFunc\n\t\tswitch op := op.(type) {\n\n\t\tcase *updateOp:\n\t\t\tbuf = addHeader(buf, 2001)\n\t\t\tbuf = addInt32(buf, 0) // Reserved\n\t\t\tbuf = addCString(buf, op.Collection)\n\t\t\tbuf = addInt32(buf, int32(op.Flags))\n\t\t\tdebugf(\"Socket %p to %s: serializing selector document: %#v\", socket, socket.addr, op.Selector)\n\t\t\tbuf, err = addBSON(buf, op.Selector)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdebugf(\"Socket %p to %s: serializing update document: %#v\", socket, socket.addr, op.Update)\n\t\t\tbuf, err = addBSON(buf, op.Update)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase *insertOp:\n\t\t\tbuf = addHeader(buf, 2002)\n\t\t\tbuf = addInt32(buf, int32(op.flags))\n\t\t\tbuf = addCString(buf, op.collection)\n\t\t\tfor _, doc := range op.documents {\n\t\t\t\tdebugf(\"Socket %p to %s: serializing document for insertion: %#v\", socket, socket.addr, doc)\n\t\t\t\tbuf, err = addBSON(buf, doc)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase *queryOp:\n\t\t\tbuf = addHeader(buf, 2004)\n\t\t\tbuf = addInt32(buf, int32(op.flags))\n\t\t\tbuf = addCString(buf, op.collection)\n\t\t\tbuf = addInt32(buf, op.skip)\n\t\t\tbuf = addInt32(buf, op.limit)\n\t\t\tbuf, err = addBSON(buf, op.finalQuery(socket))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif op.selector != nil {\n\t\t\t\tbuf, err = addBSON(buf, op.selector)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treplyFunc = op.replyFunc\n\n\t\tcase *getMoreOp:\n\t\t\tbuf = addHeader(buf, 2005)\n\t\t\tbuf = addInt32(buf, 0) // Reserved\n\t\t\tbuf = addCString(buf, op.collection)\n\t\t\tbuf = addInt32(buf, op.limit)\n\t\t\tbuf = addInt64(buf, op.cursorId)\n\t\t\treplyFunc = op.replyFunc\n\n\t\tcase *deleteOp:\n\t\t\tbuf = addHeader(buf, 2006)\n\t\t\tbuf = addInt32(buf, 0) // Reserved\n\t\t\tbuf = addCString(buf, op.Collection)\n\t\t\tbuf = addInt32(buf, int32(op.Flags))\n\t\t\tdebugf(\"Socket %p to %s: serializing selector document: %#v\", socket, socket.addr, op.Selector)\n\t\t\tbuf, err = addBSON(buf, op.Selector)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase *killCursorsOp:\n\t\t\tbuf = addHeader(buf, 2007)\n\t\t\tbuf = addInt32(buf, 0) // Reserved\n\t\t\tbuf = addInt32(buf, int32(len(op.cursorIds)))\n\t\t\tfor _, cursorId := range op.cursorIds {\n\t\t\t\tbuf = addInt64(buf, cursorId)\n\t\t\t}\n\n\t\tdefault:\n\t\t\tpanic(\"internal error: unknown operation type\")\n\t\t}\n\n\t\tsetInt32(buf, start, int32(len(buf)-start))\n\n\t\tif replyFunc != nil {\n\t\t\trequest := &requests[requestCount]\n\t\t\trequest.replyFunc = replyFunc\n\t\t\trequest.bufferPos = start\n\t\t\trequestCount++\n\t\t}\n\t}\n\n\t// Buffer is ready for the pipe.  Lock, allocate ids, and enqueue.\n\n\tsocket.Lock()\n\tif socket.dead != nil {\n\t\tdead := socket.dead\n\t\tsocket.Unlock()\n\t\tdebugf(\"Socket %p to %s: failing query, already closed: %s\", socket, socket.addr, socket.dead.Error())\n\t\t// XXX This seems necessary in case the session is closed concurrently\n\t\t// with a query being performed, but it's not yet tested:\n\t\tfor i := 0; i != requestCount; i++ {\n\t\t\trequest := &requests[i]\n\t\t\tif request.replyFunc != nil {\n\t\t\t\trequest.replyFunc(dead, nil, -1, nil)\n\t\t\t}\n\t\t}\n\t\treturn dead\n\t}\n\n\twasWaiting := len(socket.replyFuncs) > 0\n\n\t// Reserve id 0 for requests which should have no responses.\n\trequestId := socket.nextRequestId + 1\n\tif requestId == 0 {\n\t\trequestId++\n\t}\n\tsocket.nextRequestId = requestId + uint32(requestCount)\n\tfor i := 0; i != requestCount; i++ {\n\t\trequest := &requests[i]\n\t\tsetInt32(buf, request.bufferPos+4, int32(requestId))\n\t\tsocket.replyFuncs[requestId] = request.replyFunc\n\t\trequestId++\n\t}\n\n\tdebugf(\"Socket %p to %s: sending %d op(s) (%d bytes)\", socket, socket.addr, len(ops), len(buf))\n\tstats.sentOps(len(ops))\n\n\tsocket.updateDeadline(writeDeadline)\n\t_, err = socket.conn.Write(buf)\n\tif !wasWaiting && requestCount > 0 {\n\t\tsocket.updateDeadline(readDeadline)\n\t}\n\tsocket.Unlock()\n\treturn err\n}\n\nfunc fill(r net.Conn, b []byte) error {\n\tl := len(b)\n\tn, err := r.Read(b)\n\tfor n != l && err == nil {\n\t\tvar ni int\n\t\tni, err = r.Read(b[n:])\n\t\tn += ni\n\t}\n\treturn err\n}\n\n// Estimated minimum cost per socket: 1 goroutine + memory for the largest\n// document ever seen.\nfunc (socket *mongoSocket) readLoop() {\n\tp := make([]byte, 36) // 16 from header + 20 from OP_REPLY fixed fields\n\ts := make([]byte, 4)\n\tconn := socket.conn // No locking, conn never changes.\n\tfor {\n\t\terr := fill(conn, p)\n\t\tif err != nil {\n\t\t\tsocket.kill(err, true)\n\t\t\treturn\n\t\t}\n\n\t\ttotalLen := getInt32(p, 0)\n\t\tresponseTo := getInt32(p, 8)\n\t\topCode := getInt32(p, 12)\n\n\t\t// Don't use socket.server.Addr here.  socket is not\n\t\t// locked and socket.server may go away.\n\t\tdebugf(\"Socket %p to %s: got reply (%d bytes)\", socket, socket.addr, totalLen)\n\n\t\t_ = totalLen\n\n\t\tif opCode != 1 {\n\t\t\tsocket.kill(errors.New(\"opcode != 1, corrupted data?\"), true)\n\t\t\treturn\n\t\t}\n\n\t\treply := replyOp{\n\t\t\tflags:     uint32(getInt32(p, 16)),\n\t\t\tcursorId:  getInt64(p, 20),\n\t\t\tfirstDoc:  getInt32(p, 28),\n\t\t\treplyDocs: getInt32(p, 32),\n\t\t}\n\n\t\tstats.receivedOps(+1)\n\t\tstats.receivedDocs(int(reply.replyDocs))\n\n\t\tsocket.Lock()\n\t\treplyFunc, ok := socket.replyFuncs[uint32(responseTo)]\n\t\tif ok {\n\t\t\tdelete(socket.replyFuncs, uint32(responseTo))\n\t\t}\n\t\tsocket.Unlock()\n\n\t\tif replyFunc != nil && reply.replyDocs == 0 {\n\t\t\treplyFunc(nil, &reply, -1, nil)\n\t\t} else {\n\t\t\tfor i := 0; i != int(reply.replyDocs); i++ {\n\t\t\t\terr := fill(conn, s)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif replyFunc != nil {\n\t\t\t\t\t\treplyFunc(err, nil, -1, nil)\n\t\t\t\t\t}\n\t\t\t\t\tsocket.kill(err, true)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tb := make([]byte, int(getInt32(s, 0)))\n\n\t\t\t\t// copy(b, s) in an efficient way.\n\t\t\t\tb[0] = s[0]\n\t\t\t\tb[1] = s[1]\n\t\t\t\tb[2] = s[2]\n\t\t\t\tb[3] = s[3]\n\n\t\t\t\terr = fill(conn, b[4:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tif replyFunc != nil {\n\t\t\t\t\t\treplyFunc(err, nil, -1, nil)\n\t\t\t\t\t}\n\t\t\t\t\tsocket.kill(err, true)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif globalDebug && globalLogger != nil {\n\t\t\t\t\tm := bson.M{}\n\t\t\t\t\tif err := bson.Unmarshal(b, m); err == nil {\n\t\t\t\t\t\tdebugf(\"Socket %p to %s: received document: %#v\", socket, socket.addr, m)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif replyFunc != nil {\n\t\t\t\t\treplyFunc(nil, &reply, i, b)\n\t\t\t\t}\n\n\t\t\t\t// XXX Do bound checking against totalLen.\n\t\t\t}\n\t\t}\n\n\t\tsocket.Lock()\n\t\tif len(socket.replyFuncs) == 0 {\n\t\t\t// Nothing else to read for now. Disable deadline.\n\t\t\tsocket.conn.SetReadDeadline(time.Time{})\n\t\t} else {\n\t\t\tsocket.updateDeadline(readDeadline)\n\t\t}\n\t\tsocket.Unlock()\n\n\t\t// XXX Do bound checking against totalLen.\n\t}\n}\n\nvar emptyHeader = []byte{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\n\nfunc addHeader(b []byte, opcode int) []byte {\n\ti := len(b)\n\tb = append(b, emptyHeader...)\n\t// Enough for current opcodes.\n\tb[i+12] = byte(opcode)\n\tb[i+13] = byte(opcode >> 8)\n\treturn b\n}\n\nfunc addInt32(b []byte, i int32) []byte {\n\treturn append(b, byte(i), byte(i>>8), byte(i>>16), byte(i>>24))\n}\n\nfunc addInt64(b []byte, i int64) []byte {\n\treturn append(b, byte(i), byte(i>>8), byte(i>>16), byte(i>>24),\n\t\tbyte(i>>32), byte(i>>40), byte(i>>48), byte(i>>56))\n}\n\nfunc addCString(b []byte, s string) []byte {\n\tb = append(b, []byte(s)...)\n\tb = append(b, 0)\n\treturn b\n}\n\nfunc addBSON(b []byte, doc interface{}) ([]byte, error) {\n\tif doc == nil {\n\t\treturn append(b, 5, 0, 0, 0, 0), nil\n\t}\n\tdata, err := bson.Marshal(doc)\n\tif err != nil {\n\t\treturn b, err\n\t}\n\treturn append(b, data...), nil\n}\n\nfunc setInt32(b []byte, pos int, i int32) {\n\tb[pos] = byte(i)\n\tb[pos+1] = byte(i >> 8)\n\tb[pos+2] = byte(i >> 16)\n\tb[pos+3] = byte(i >> 24)\n}\n\nfunc getInt32(b []byte, pos int) int32 {\n\treturn (int32(b[pos+0])) |\n\t\t(int32(b[pos+1]) << 8) |\n\t\t(int32(b[pos+2]) << 16) |\n\t\t(int32(b[pos+3]) << 24)\n}\n\nfunc getInt64(b []byte, pos int) int64 {\n\treturn (int64(b[pos+0])) |\n\t\t(int64(b[pos+1]) << 8) |\n\t\t(int64(b[pos+2]) << 16) |\n\t\t(int64(b[pos+3]) << 24) |\n\t\t(int64(b[pos+4]) << 32) |\n\t\t(int64(b[pos+5]) << 40) |\n\t\t(int64(b[pos+6]) << 48) |\n\t\t(int64(b[pos+7]) << 56)\n}\n"
        },
        {
          "name": "stats.go",
          "type": "blob",
          "size": 3.361328125,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo\n\nimport (\n\t\"sync\"\n)\n\nvar stats *Stats\nvar statsMutex sync.Mutex\n\nfunc SetStats(enabled bool) {\n\tstatsMutex.Lock()\n\tif enabled {\n\t\tif stats == nil {\n\t\t\tstats = &Stats{}\n\t\t}\n\t} else {\n\t\tstats = nil\n\t}\n\tstatsMutex.Unlock()\n}\n\nfunc GetStats() (snapshot Stats) {\n\tstatsMutex.Lock()\n\tsnapshot = *stats\n\tstatsMutex.Unlock()\n\treturn\n}\n\nfunc ResetStats() {\n\tstatsMutex.Lock()\n\tdebug(\"Resetting stats\")\n\told := stats\n\tstats = &Stats{}\n\t// These are absolute values:\n\tstats.Clusters = old.Clusters\n\tstats.SocketsInUse = old.SocketsInUse\n\tstats.SocketsAlive = old.SocketsAlive\n\tstats.SocketRefs = old.SocketRefs\n\tstatsMutex.Unlock()\n\treturn\n}\n\ntype Stats struct {\n\tClusters     int\n\tMasterConns  int\n\tSlaveConns   int\n\tSentOps      int\n\tReceivedOps  int\n\tReceivedDocs int\n\tSocketsAlive int\n\tSocketsInUse int\n\tSocketRefs   int\n}\n\nfunc (stats *Stats) cluster(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.Clusters += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) conn(delta int, master bool) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tif master {\n\t\t\tstats.MasterConns += delta\n\t\t} else {\n\t\t\tstats.SlaveConns += delta\n\t\t}\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) sentOps(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.SentOps += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) receivedOps(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.ReceivedOps += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) receivedDocs(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.ReceivedDocs += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) socketsInUse(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.SocketsInUse += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) socketsAlive(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.SocketsAlive += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n\nfunc (stats *Stats) socketRefs(delta int) {\n\tif stats != nil {\n\t\tstatsMutex.Lock()\n\t\tstats.SocketRefs += delta\n\t\tstatsMutex.Unlock()\n\t}\n}\n"
        },
        {
          "name": "suite_test.go",
          "type": "blob",
          "size": 5.873046875,
          "content": "// mgo - MongoDB driver for Go\n//\n// Copyright (c) 2010-2012 - Gustavo Niemeyer <gustavo@niemeyer.net>\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this\n//    list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright notice,\n//    this list of conditions and the following disclaimer in the documentation\n//    and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage mgo_test\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"net\"\n\t\"os/exec\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t. \"gopkg.in/check.v1\"\n\t\"gopkg.in/mgo.v2\"\n\t\"gopkg.in/mgo.v2/bson\"\n)\n\nvar fast = flag.Bool(\"fast\", false, \"Skip slow tests\")\n\ntype M bson.M\n\ntype cLogger C\n\nfunc (c *cLogger) Output(calldepth int, s string) error {\n\tns := time.Now().UnixNano()\n\tt := float64(ns%100e9) / 1e9\n\t((*C)(c)).Logf(\"[LOG] %.05f %s\", t, s)\n\treturn nil\n}\n\nfunc TestAll(t *testing.T) {\n\tTestingT(t)\n}\n\ntype S struct {\n\tsession *mgo.Session\n\tstopped bool\n\tbuild   mgo.BuildInfo\n\tfrozen  []string\n}\n\nfunc (s *S) versionAtLeast(v ...int) (result bool) {\n\tfor i := range v {\n\t\tif i == len(s.build.VersionArray) {\n\t\t\treturn false\n\t\t}\n\t\tif s.build.VersionArray[i] != v[i] {\n\t\t\treturn s.build.VersionArray[i] >= v[i]\n\t\t}\n\t}\n\treturn true\n}\n\nvar _ = Suite(&S{})\n\nfunc (s *S) SetUpSuite(c *C) {\n\tmgo.SetDebug(true)\n\tmgo.SetStats(true)\n\ts.StartAll()\n\n\tsession, err := mgo.Dial(\"localhost:40001\")\n\tc.Assert(err, IsNil)\n\ts.build, err = session.BuildInfo()\n\tc.Check(err, IsNil)\n\tsession.Close()\n}\n\nfunc (s *S) SetUpTest(c *C) {\n\terr := run(\"mongo --nodb harness/mongojs/dropall.js\")\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\tmgo.SetLogger((*cLogger)(c))\n\tmgo.ResetStats()\n}\n\nfunc (s *S) TearDownTest(c *C) {\n\tif s.stopped {\n\t\ts.Stop(\":40201\")\n\t\ts.Stop(\":40202\")\n\t\ts.Stop(\":40203\")\n\t\ts.StartAll()\n\t}\n\tfor _, host := range s.frozen {\n\t\tif host != \"\" {\n\t\t\ts.Thaw(host)\n\t\t}\n\t}\n\tvar stats mgo.Stats\n\tfor i := 0; ; i++ {\n\t\tstats = mgo.GetStats()\n\t\tif stats.SocketsInUse == 0 && stats.SocketsAlive == 0 {\n\t\t\tbreak\n\t\t}\n\t\tif i == 20 {\n\t\t\tc.Fatal(\"Test left sockets in a dirty state\")\n\t\t}\n\t\tc.Logf(\"Waiting for sockets to die: %d in use, %d alive\", stats.SocketsInUse, stats.SocketsAlive)\n\t\ttime.Sleep(500 * time.Millisecond)\n\t}\n\tfor i := 0; ; i++ {\n\t\tstats = mgo.GetStats()\n\t\tif stats.Clusters == 0 {\n\t\t\tbreak\n\t\t}\n\t\tif i == 60 {\n\t\t\tc.Fatal(\"Test left clusters alive\")\n\t\t}\n\t\tc.Logf(\"Waiting for clusters to die: %d alive\", stats.Clusters)\n\t\ttime.Sleep(1 * time.Second)\n\t}\n}\n\nfunc (s *S) Stop(host string) {\n\t// Give a moment for slaves to sync and avoid getting rollback issues.\n\tpanicOnWindows()\n\ttime.Sleep(2 * time.Second)\n\terr := run(\"svc -d _harness/daemons/\" + supvName(host))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\ts.stopped = true\n}\n\nfunc (s *S) pid(host string) int {\n\t// Note recent releases of lsof force 'f' to be present in the output (WTF?).\n\tcmd := exec.Command(\"lsof\", \"-iTCP:\"+hostPort(host), \"-sTCP:LISTEN\", \"-Fpf\")\n\toutput, err := cmd.CombinedOutput()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tpidstr := string(bytes.Fields(output[1:])[0])\n\tpid, err := strconv.Atoi(pidstr)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"cannot convert pid to int: %q, command line: %q\", pidstr, cmd.Args))\n\t}\n\treturn pid\n}\n\nfunc (s *S) Freeze(host string) {\n\terr := stop(s.pid(host))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\ts.frozen = append(s.frozen, host)\n}\n\nfunc (s *S) Thaw(host string) {\n\terr := cont(s.pid(host))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfor i, frozen := range s.frozen {\n\t\tif frozen == host {\n\t\t\ts.frozen[i] = \"\"\n\t\t}\n\t}\n}\n\nfunc (s *S) StartAll() {\n\tif s.stopped {\n\t\t// Restart any stopped nodes.\n\t\trun(\"svc -u _harness/daemons/*\")\n\t\terr := run(\"mongo --nodb harness/mongojs/wait.js\")\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\ts.stopped = false\n\t}\n}\n\nfunc run(command string) error {\n\tvar output []byte\n\tvar err error\n\tif runtime.GOOS == \"windows\" {\n\t\toutput, err = exec.Command(\"cmd\", \"/C\", command).CombinedOutput()\n\t} else {\n\t\toutput, err = exec.Command(\"/bin/sh\", \"-c\", command).CombinedOutput()\n\t}\n\n\tif err != nil {\n\t\tmsg := fmt.Sprintf(\"Failed to execute: %s: %s\\n%s\", command, err.Error(), string(output))\n\t\treturn errors.New(msg)\n\t}\n\treturn nil\n}\n\nvar supvNames = map[string]string{\n\t\"40001\": \"db1\",\n\t\"40002\": \"db2\",\n\t\"40011\": \"rs1a\",\n\t\"40012\": \"rs1b\",\n\t\"40013\": \"rs1c\",\n\t\"40021\": \"rs2a\",\n\t\"40022\": \"rs2b\",\n\t\"40023\": \"rs2c\",\n\t\"40031\": \"rs3a\",\n\t\"40032\": \"rs3b\",\n\t\"40033\": \"rs3c\",\n\t\"40041\": \"rs4a\",\n\t\"40101\": \"cfg1\",\n\t\"40102\": \"cfg2\",\n\t\"40103\": \"cfg3\",\n\t\"40201\": \"s1\",\n\t\"40202\": \"s2\",\n\t\"40203\": \"s3\",\n}\n\n// supvName returns the daemon name for the given host address.\nfunc supvName(host string) string {\n\thost, port, err := net.SplitHostPort(host)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tname, ok := supvNames[port]\n\tif !ok {\n\t\tpanic(\"Unknown host: \" + host)\n\t}\n\treturn name\n}\n\nfunc hostPort(host string) string {\n\t_, port, err := net.SplitHostPort(host)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn port\n}\n\nfunc panicOnWindows() {\n\tif runtime.GOOS == \"windows\" {\n\t\tpanic(\"the test suite is not yet fully supported on Windows\")\n\t}\n}\n"
        },
        {
          "name": "syscall_test.go",
          "type": "blob",
          "size": 0.212890625,
          "content": "// +build !windows\n\npackage mgo_test\n\nimport (\n\t\"syscall\"\n)\n\nfunc stop(pid int) (err error) {\n\treturn syscall.Kill(pid, syscall.SIGSTOP)\n}\n\nfunc cont(pid int) (err error) {\n\treturn syscall.Kill(pid, syscall.SIGCONT)\n}\n"
        },
        {
          "name": "syscall_windows_test.go",
          "type": "blob",
          "size": 0.1767578125,
          "content": "package mgo_test\n\nfunc stop(pid int) (err error) {\n\tpanicOnWindows() // Always does.\n\treturn nil\n}\n\nfunc cont(pid int) (err error) {\n\tpanicOnWindows() // Always does.\n\treturn nil\n}\n"
        },
        {
          "name": "txn",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}