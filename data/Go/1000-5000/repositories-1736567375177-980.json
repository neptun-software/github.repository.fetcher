{
  "metadata": {
    "timestamp": 1736567375177,
    "page": 980,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "moby/libnetwork",
      "stars": 2162,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0380859375,
          "content": ".git\n.dockerignore\nDockerfile\nbin\ntags\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.474609375,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n*~\n.gtm\nbin/\ntags\n.DS_Store\n\n# Folders\nintegration-tmp/\n_obj\n_test\n.vagrant\n\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\ncmd/dnet/dnet\n\n# Coverage\n*.tmp\n*.coverprofile\n\n# IDE files and folders\n.project\n.settings/\n\nlibnetworkbuild.created\ntest/networkDb/testMain\ntest/networkDb/gossipdb\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 7.5048828125,
          "content": "# Changelog\n\n## 0.8.0-dev.2 (2016-05-07)\n- Fix an issue which may arise during sandbox cleanup (https://github.com/docker/libnetwork/pull/1157)\n- Fix cleanup logic in case of ipv6 allocation failure\n- Don't add /etc/hosts record if container's ip is empty (--net=none)\n- Fix default gw logic for internal networks\n- Error when updating IPv6 gateway (https://github.com/docker/libnetwork/issues/1142)\n- Fixes https://github.com/docker/libnetwork/issues/1113\n- Fixes https://github.com/docker/libnetwork/issues/1069\n- Fxies https://github.com/docker/libnetwork/issues/1117\n- Increase the concurrent query rate-limit count\n- Changes to build libnetwork in Solaris\n\n## 0.8.0-dev.1 (2016-04-16)\n- Fixes docker/docker#16964\n- Added maximum egress bandwidth qos for Windows\n\n## 0.7.0-rc.6 (2016-04-10)\n- Flush cached resolver socket on default gateway change\n\n## 0.7.0-rc.5 (2016-04-08)\n- Persist ipam driver options\n- Fixes https://github.com/docker/libnetwork/issues/1087\n- Use go vet from go tool\n- Godep update to pick up latest docker/docker packages\n- Validate remote driver response using docker plugins package method.\n\n## 0.7.0-rc.4 (2016-04-06)\n- Fix the handling for default gateway Endpoint join/leave.\n\n## 0.7.0-rc.3 (2016-04-05)\n- Revert fix for default gateway endpoint join/leave. Needs to be reworked.\n- Persist the network internal mode for bridge networks\n\n## 0.7.0-rc.2 (2016-04-05)\n- Fixes https://github.com/docker/libnetwork/issues/1070\n- Move IPAM resource initialization out of init()\n- Initialize overlay driver before network delete\n- Fix the handling for default gateway Endpoint join/lean\n\n## 0.7.0-rc.1 (2016-03-30)\n- Fixes https://github.com/docker/libnetwork/issues/985\n- Fixes https://github.com/docker/libnetwork/issues/945\n- Log time taken to set sandbox key\n- Limit number of concurrent DNS queries\n\n## 0.7.0-dev.10 (2016-03-21)\n- Add IPv6 service discovery (AAAA records) in embedded DNS server\n- Honor enableIPv6 flag in network create for the IP allocation\n- Avoid V6 queries in docker domain going to external nameservers\n\n## 0.7.0-dev.9 (2016-03-18)\n- Support labels on networks\n\n## 0.7.0-dev.8 (2016-03-16)\n- Windows driver to respect user set MAC address.\n- Fix possible nil pointer reference in ServeDNS() with concurrent go routines.\n- Fix netns path setting from hook (for containerd integration)\n- Clear cached udp connections on resolver Stop()\n- Avoid network/endpoint count inconsistences and remove stale networks after ungraceful shutdown\n- Fix possible endpoint count inconsistency after ungraceful shutdown\n- Reject a null v4 IPAM slice in exp vlan drivers\n- Removed experimental drivers modprobe check\n\n## 0.7.0-dev.7 (2016-03-11)\n- Bumped up the minimum kernel version for ipvlan to 4.2\n- Removed modprobe from macvlan/ipvlan drivers to resolve docker IT failures\n- Close dbus connection if firewalld is not started\n\n## 0.7.0-dev.6 (2016-03-10)\n- Experimental support for macvlan and ipvlan drivers\n\n## 0.7.0-dev.5 (2016-03-08)\n- Fixes https://github.com/docker/docker/issues/20847\n- Fixes https://github.com/docker/docker/issues/20997\n- Fixes issues unveiled by docker integ test over 0.7.0-dev.4\n\n## 0.7.0-dev.4 (2016-03-07)\n- Changed ownership of exposed ports and port-mapping options from Endpoint to Sandbox\n- Implement DNS RR in the Docker embedded DNS server\n- Fixes https://github.com/docker/libnetwork/issues/984 (multi container overlay veth leak)\n- Libnetwork to program container's interface MAC address\n- Fixed bug in iptables.Exists() logic\n- Fixes https://github.com/docker/docker/issues/20694\n- Source external DNS queries from container namespace\n- Added inbuilt nil IPAM driver\n- Windows drivers integration fixes\n- Extract hostname from (hostname.domainname). Related to https://github.com/docker/docker/issues/14282\n- Fixed race in sandbox statistics read\n- Fixes https://github.com/docker/libnetwork/issues/892 (docker start fails when ipv6.disable=1)\n- Fixed error message on bridge network creation conflict\n\n## 0.7.0-dev.3 (2016-02-17)\n- Fixes https://github.com/docker/docker/issues/20350\n- Fixes https://github.com/docker/docker/issues/20145\n- Initial Windows HNS integration\n- Allow passing global datastore config to libnetwork after boot\n- Set Recursion Available bit in DNS query responses\n- Make sure iptables chains are recreated on firewalld reload\n\n## 0.7.0-dev.2 (2016-02-11)\n- Fixes https://github.com/docker/docker/issues/20140\n\n## 0.7.0-dev.1 (2016-02-10)\n- Expose EnableIPV6 option\n- discoverapi refactoring\n- Fixed a few typos & docs update\n\n## 0.6.1-rc2 (2016-02-09)\n- Fixes https://github.com/docker/docker/issues/20132\n- Fixes https://github.com/docker/docker/issues/20140\n- Fixes https://github.com/docker/docker/issues/20019\n\n## 0.6.1-rc1 (2016-02-05)\n- Fixes https://github.com/docker/docker/issues/20026\n\n## 0.6.0-rc7 (2016-02-01)\n- Allow inter-network connections via exposed ports\n\n## 0.6.0-rc6 (2016-01-30)\n- Properly fixes https://github.com/docker/docker/issues/18814\n\n## 0.6.0-rc5 (2016-01-26)\n- Cleanup stale overlay sandboxes\n\n## 0.6.0-rc4 (2016-01-25)\n- Add Endpoints() API to Sandbox interface\n- Fixed a race-condition in default gateway network creation\n\n## 0.6.0-rc3 (2016-01-25)\n- Fixes docker/docker#19576\n- Fixed embedded DNS to listen in TCP as well\n- Fixed a race-condition in IPAM to choose non-overlapping subnet for concurrent requests\n\n## 0.6.0-rc2 (2016-01-21)\n- Fixes docker/docker#19376\n- Fixes docker/docker#15819\n- Fixes libnetwork/#885, Not filter v6 DNS servers from resolv.conf\n- Fixes docker/docker #19448, also handles the . in service and network names correctly.\n\n## 0.6.0-rc1 (2016-01-14)\n- Fixes docker/docker#19404\n- Fixes the ungraceful daemon restart issue in systemd with remote network plugin\n  (https://github.com/docker/libnetwork/issues/813)\n\n## 0.5.6 (2016-01-14)\n- Setup embedded DNS server correctly on container restart. Fixes docker/docker#19354\n\n## 0.5.5 (2016-01-14)\n- Allow network-scoped alias to be resolved for anonymous endpoint\n- Self repair corrupted IP database that could happen in 1.9.0 & 1.9.1\n- Skip IPTables cleanup if --iptables=false is set. Fixes docker/docker#19063\n\n## 0.5.4 (2016-01-12)\n- Removed the isNodeAlive protection when user forces an endpoint delete\n\n## 0.5.3 (2016-01-12)\n- Bridge driver supporting internal network option\n- Backend implementation to support \"force\" option to network disconnect\n- Fixing a regex in etchosts package to fix docker/docker#19080\n\n## 0.5.2 (2016-01-08)\n- Embedded DNS replacing /etc/hosts based Service Discovery\n- Container local alias and Network-scoped alias support\n- Backend support for internal network mode\n- Support for IPAM driver options\n- Fixes overlay veth cleanup issue : docker/docker#18814\n- fixes docker/docker#19139\n- disable IPv6 Duplicate Address Detection\n\n## 0.5.1 (2015-12-07)\n- Allowing user to assign IP Address for containers\n- Fixes docker/docker#18214\n- Fixes docker/docker#18380\n\n## 0.5.0 (2015-10-30)\n\n- Docker multi-host networking exiting experimental channel\n- Introduced IP Address Management and IPAM drivers\n- DEPRECATE service discovery from default bridge network\n- Introduced new network UX\n- Support for multiple networks in bridge driver\n- Local persistence with boltdb\n\n## 0.4.0 (2015-07-24)\n\n- Introduce experimental version of Overlay driver\n- Introduce experimental version of network plugins\n- Introduce experimental version of network & service UX\n- Introduced experimental /etc/hosts based service discovery\n- Integrated with libkv\n- Improving test coverage\n- Fixed a bunch of issues with osl namespace mgmt\n\n## 0.3.0 (2015-05-27)\n \n- Introduce CNM (Container Networking Model)\n- Replace docker networking with CNM & Bridge driver\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.724609375,
          "content": "ARG GO_VERSION=1.18.9\n\nFROM golang:${GO_VERSION}-buster as dev\nRUN apt-get update && apt-get -y install iptables \\\n\t\tprotobuf-compiler\n\nRUN git clone https://github.com/gogo/protobuf.git  /go/src/github.com/gogo/protobuf \\\n  && cd /go/src/github.com/gogo/protobuf/protoc-gen-gogo \\\n  && git reset --hard 30cf7ac33676b5786e78c746683f0d4cd64fa75b \\\n  && GO111MODULE=off go install\n\nRUN go install golang.org/x/lint/golint@latest \\\n && go install golang.org/x/tools/cmd/cover@latest \\\n && go install github.com/mattn/goveralls@latest \\\n && go install github.com/gordonklaus/ineffassign@latest \\\n && go install github.com/client9/misspell/cmd/misspell@latest\n\nWORKDIR /go/src/github.com/docker/libnetwork\nENV GO111MODULE=off\n\n\nFROM dev\n\nCOPY . .\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0595703125,
          "content": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n"
        },
        {
          "name": "MAINTAINERS",
          "type": "blob",
          "size": 2.447265625,
          "content": "# Libnetwork maintainers file\n#\n# This file describes who runs the docker/libnetwork project and how.\n# This is a living document - if you see something out of date or missing, speak up!\n#\n# It is structured to be consumable by both humans and programs.\n# To extract its contents programmatically, use any TOML-compliant parser.\n#\n# This file is compiled into the MAINTAINERS file in docker/opensource.\n#\n[Org]\n\t[Org.\"Core maintainers\"]\n\t\tpeople = [\n\t\t\t\"akerouanton\",\n\t\t\t\"akihirosuda\",\n\t\t\t\"arkodg\",\n\t\t\t\"corhere\",\n\t\t\t\"cpuguy83\",\n\t\t\t\"crazy-max\",\n\t\t\t\"euanh\",\n\t\t\t\"fcrisciani\",\n\t\t\t\"laurazard\",\n\t\t\t\"mavenugo\",\n\t\t\t\"neersighted\",\n\t\t\t\"rumpl\",\n\t\t\t\"selansen\",\n\t\t\t\"thajeztah\",\n\t\t\t\"tianon\",\n\t\t\t\"tonistiigi\",\n\t\t\t\"vvoland\",\n\t\t]\n\n[people]\n\n# A reference list of all people associated with the project.\n# All other sections should refer to people by their canonical key\n# in the people section.\n\n\t# ADD YOURSELF HERE IN ALPHABETICAL ORDER\n\n\t[people.akerouanton]\n\tName = \"Albin Kerouanton\"\n\tEmail = \"albinker@gmail.com\"\n\tGitHub = \"akerouanton\"\n\n\t[people.akihirosuda]\n\tName = \"Akihiro Suda\"\n\tEmail = \"akihiro.suda.cz@hco.ntt.co.jp\"\n\tGitHub = \"AkihiroSuda\"\n\n\t[people.arkodg]\n\tName = \"Arko Dasgupta\"\n\tEmail = \"arko.dasgupta@docker.com\"\n\tGitHub = \"arkodg\"\n\n\t[people.corhere]\n\tName = \"Cory Snider\"\n\tEmail = \"csnider@mirantis.com\"\n\tGitHub = \"corhere\"\n\n\t[people.cpuguy83]\n\tName = \"Brian Goff\"\n\tEmail = \"cpuguy83@gmail.com\"\n\tGitHub = \"cpuguy83\"\n\n\t[people.crazy-max]\n\tName = \"Kevin Alvarez\"\n\tEmail = \"contact@crazymax.dev\"\n\tGitHub = \"crazy-max\"\n\n\t[people.euanh]\n\tName = \"Euan Harris\"\n\tEmail = \"euan.harris@docker.com\"\n\tGitHub = \"euanh\"\n\n\t[people.fcrisciani]\n\tName = \"Flavio Crisciani\"\n\tEmail = \"flavio.crisciani@docker.com\"\n\tGitHub = \"fcrisciani\"\n\n\t[people.laurazard]\n\tName = \"Laura Brehm\"\n\tEmail = \"laura.brehm@docker.com\"\n\tGitHub = \"laurazard\"\n\n\t[people.mavenugo]\n\tName = \"Madhu Venugopal\"\n\tEmail = \"madhu@docker.com\"\n\tGitHub = \"mavenugo\"\n\n\t[people.rumpl]\n\tName = \"Djordje Lukic\"\n\tEmail = \"djordje.lukic@docker.com\"\n\tGitHub = \"rumpl\"\n\n\t[people.selansen]\n\tName = \"Elangovan Sivanandam\"\n\tEmail = \"elango.siva@docker.com\"\n\tGitHub = \"selansen\"\n\n\t[people.thajeztah]\n\tName = \"Sebastiaan van Stijn\"\n\tEmail = \"github@gone.nl\"\n\tGitHub = \"thaJeztah\"\n\n\t[people.tianon]\n\tName = \"Tianon Gravi\"\n\tEmail = \"admwiggin@gmail.com\"\n\tGitHub = \"tianon\"\n\n\t[people.tonistiigi]\n\tName = \"T√µnis Tiigi\"\n\tEmail = \"tonis@docker.com\"\n\tGitHub = \"tonistiigi\"\n\n\t[people.vvoland]\n\tName = \"Pawe≈Ç Gronowski\"\n\tEmail = \"pawel.gronowski@docker.com\"\n\tGitHub = \"vvoland\"\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 6.4794921875,
          "content": ".PHONY: all all-local build build-local clean cross cross-local vet lint misspell check check-local check-code check-format unit-tests protobuf protobuf-local check-protobuf\nSHELL=/bin/bash\n\ndockerbuildargs ?= --target dev - < Dockerfile\ndockerargs ?= --privileged -v $(shell pwd):/go/src/github.com/docker/libnetwork -w /go/src/github.com/docker/libnetwork\nbuild_image=libnetworkbuild\ncontainer_env = -e \"INSIDECONTAINER=-incontainer=true\"\ndocker = docker run --rm -it --init ${dockerargs} $$EXTRA_ARGS ${container_env} ${build_image}\n\nCROSS_PLATFORMS = linux/amd64 linux/386 linux/arm windows/amd64\nPACKAGES=$(shell go list ./... | grep -v /vendor/)\nPROTOC_FLAGS=-I=. -I=/go/src -I=/go/src/github.com/gogo/protobuf -I=/go/src/github.com/gogo/protobuf/protobuf\n\nexport PATH := $(CURDIR)/bin:$(PATH)\n\n\n# Several targets in this Makefile expect to run within the\n# libnetworkbuild container.   In general, a target named '<target>-local'\n# relies on utilities inside the build container.   Usually there is also\n# a wrapper called '<target>' which starts a container and runs\n# 'make <target>-local' inside it.\n\n###########################################################################\n# Top level targets\n###########################################################################\n\nall: build check clean\n\nall-local: build-local check-local clean\n\n\n###########################################################################\n# Build targets\n###########################################################################\n\n# builder builds the libnetworkbuild container.  All wrapper targets\n# must depend on this to ensure that the container exists.\nbuilder:\n\tDOCKER_BUILDKIT=1 docker build --progress=plain  -t ${build_image} --build-arg=GO_VERSION ${dockerbuildargs}\n\nbuild: builder\n\t@echo \"üê≥ $@\"\n\t@${docker} make build-local\n\nbuild-local:\n\t@echo \"üê≥ $@\"\n\t@mkdir -p \"bin\"\n\tGO111MODULE=off go build -tags experimental -o \"bin/dnet\" ./cmd/dnet\n\tGO111MODULE=off go build -o \"bin/docker-proxy\" ./cmd/proxy\n\tCGO_ENABLED=0 go build -o \"bin/diagnosticClient\" ./cmd/diagnostic\n\tCGO_ENABLED=0 go build -o \"bin/testMain\" ./cmd/networkdb-test/testMain.go\n\nbuild-images:\n\t@echo \"üê≥ $@\"\n\tcp cmd/diagnostic/daemon.json ./bin\n\tDOCKER_BUILDKIT=1 docker build --progress=plain  -f cmd/diagnostic/Dockerfile.client -t dockereng/network-diagnostic:onlyclient bin/\n\tDOCKER_BUILDKIT=1 docker build --progress=plain  -f cmd/diagnostic/Dockerfile.dind -t dockereng/network-diagnostic:17.12-dind bin/\n\tDOCKER_BUILDKIT=1 docker build --progress=plain  -f cmd/networkdb-test/Dockerfile -t dockereng/e2e-networkdb:master bin/\n\tDOCKER_BUILDKIT=1 docker build --progress=plain  -t dockereng/network-diagnostic:support.sh support/\n\npush-images: build-images\n\t@echo \"üê≥ $@\"\n\tdocker push dockereng/network-diagnostic:onlyclient\n\tdocker push dockereng/network-diagnostic:17.12-dind\n\tdocker push dockereng/e2e-networkdb:master\n\tdocker push dockereng/network-diagnostic:support.sh\n\nclean:\n\t@echo \"üê≥ $@\"\n\t@if [ -d bin ]; then \\\n\t\techo \"Removing binaries\"; \\\n\t\trm -rf bin; \\\n\tfi\n\ncross: builder\n\t@mkdir -p \"bin\"\n\t@for platform in ${CROSS_PLATFORMS}; do \\\n\t\tEXTRA_ARGS=\"-e GOOS=$${platform%/*} -e GOARCH=$${platform##*/}\" ; \\\n\t\techo \"$${platform}...\" ; \\\n\t\t${docker} make cross-local ; \\\n\tdone\n\ncross-local:\n\t@echo \"üê≥ $@\"\n\tGO111MODULE=off go build -o \"bin/dnet-$$GOOS-$$GOARCH\" ./cmd/dnet\n\tGO111MODULE=off go build -o \"bin/docker-proxy-$$GOOS-$$GOARCH\" ./cmd/proxy\n\n# Rebuild protocol buffers.\n# These may need to be rebuilt after vendoring updates, so .proto files are declared .PHONY so they are always rebuilt.\nPROTO_FILES=$(shell find . -path ./vendor -prune -o -name \\*.proto -print)\nPB_FILES=$(PROTO_FILES:.proto=.pb.go)\n\n# Pattern rule for protoc.   If PROTOC_CHECK is defined, it checks\n# whether the generated files are up to date and fails if they are not\n%.pb.go: %.proto\n\t@if [ ${PROTOC_CHECK} ]; then \\\n\tprotoc ${PROTOC_FLAGS} --gogo_out=/tmp $< ; \\\n\tdiff -q $@ /tmp/$@ >/dev/null || (echo \"üëπ $@ is out of date; please run 'make protobuf' and check in updates\" && exit 1) ; \\\n\telse \\\n\tprotoc ${PROTOC_FLAGS} --gogo_out=./ $< ; \\\n\tfi\n\n.PHONY: $(PROTO_FILES)\nprotobuf: builder\n\t@${docker} make protobuf-local\nprotobuf-local: $(PB_FILES)\n\n\n###########################################################################\n# Test targets\n###########################################################################\n\ncheck: builder\n\t@${docker} make check-local\n\ncheck-local: check-code check-format\n\ncheck-code: check-protobuf lint vet ineffassign\n\ncheck-format: fmt misspell\n\nunit-tests: builder\n\t${docker} make unit-tests-local\n\nunit-tests-local:\n\t@echo \"üê≥ Running tests... \"\n\t@echo \"mode: count\" > coverage.coverprofile\n\t@GO111MODULE=off go build -o \"bin/docker-proxy\" ./cmd/proxy\n\t@for dir in $$( find . -maxdepth 10 -not -path './.git*' -not -path '*/_*' -not -path './vendor/*' -type d); do \\\n\tif ls $$dir/*.go &> /dev/null; then \\\n\t\tpushd . &> /dev/null ; \\\n\t\tcd $$dir ; \\\n\t\tgo test ${INSIDECONTAINER} -test.parallel 5 -test.v -covermode=count -coverprofile=./profile.tmp ; \\\n\t\tret=$$? ;\\\n\t\tif [ $$ret -ne 0 ]; then exit $$ret; fi ;\\\n\t\tpopd &> /dev/null; \\\n\t\tif [ -f $$dir/profile.tmp ]; then \\\n\t\t\tcat $$dir/profile.tmp | tail -n +2 >> coverage.coverprofile ; \\\n\t\t\t\trm $$dir/profile.tmp ; \\\n\t    fi ; \\\n\tfi ; \\\n\tdone\n\t@echo \"Done running tests\"\n\n# Depends on binaries because vet will silently fail if it can not load compiled imports\nvet: ## run go vet\n\t@echo \"üê≥ $@\"\n\t@test -z \"$$(go vet ${PACKAGES} 2>&1 | grep -v 'constant [0-9]* not a string in call to Errorf' | egrep -v '(timestamp_test.go|duration_test.go|exit status 1)' | tee /dev/stderr)\"\n\nmisspell:\n\t@echo \"üê≥ $@\"\n\t@test -z \"$$(find . -type f | grep -v vendor/ | grep \"\\.go\\|\\.md\" | xargs misspell -error | tee /dev/stderr)\"\n\nfmt: ## run go fmt\n\t@echo \"üê≥ $@\"\n\t@test -z \"$$(gofmt -s -l . | grep -v vendor/ | grep -v \".pb.go$$\" | tee /dev/stderr)\" || \\\n\t\t(echo \"üëπ please format Go code with 'gofmt -s -w'\" && false)\n\nlint: ## run go lint\n\t@echo \"üê≥ $@\"\n\t@test -z \"$$(golint ./... | grep -v vendor/ | grep -v \".pb.go:\" | grep -v \".mock.go\" | tee /dev/stderr)\"\n\nineffassign: ## run ineffassign\n\t@echo \"üê≥ $@\"\n\t@test -z \"$$(ineffassign . | grep -v vendor/ | grep -v \".pb.go:\" | grep -v \".mock.go\" | tee /dev/stderr)\"\n\n# check-protobuf rebuilds .pb.go files and fails if they have changed\ncheck-protobuf: PROTOC_CHECK=1\ncheck-protobuf: $(PB_FILES)\n\t@echo \"üê≥ $@\"\n\n\n###########################################################################\n# Utility targets\n###########################################################################\n\nshell: builder\n\t@${docker} ${SHELL}\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.9189453125,
          "content": "> **Warning**\n> libnetwork was moved to https://github.com/moby/moby/tree/master/libnetwork\n>\n> libnetwork has been merged to the main repo of Moby since Docker 22.06.\n>\n> The old libnetwork repo (https://github.com/moby/libnetwork) now only accepts PR for Docker 20.10,\n> and will be archived after the EOL of Docker 20.10.\n\n- - -\n# libnetwork - networking for containers\n\n[![Circle CI](https://circleci.com/gh/docker/libnetwork/tree/master.svg?style=svg)](https://circleci.com/gh/docker/libnetwork/tree/master) [![Coverage Status](https://coveralls.io/repos/docker/libnetwork/badge.svg)](https://coveralls.io/r/docker/libnetwork) [![GoDoc](https://godoc.org/github.com/docker/libnetwork?status.svg)](https://godoc.org/github.com/docker/libnetwork) [![Go Report Card](https://goreportcard.com/badge/github.com/docker/libnetwork)](https://goreportcard.com/report/github.com/docker/libnetwork)\n\nLibnetwork provides a native Go implementation for connecting containers\n\nThe goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications.\n\n#### Design\nPlease refer to the [design](docs/design.md) for more information.\n\n#### Using libnetwork\n\nThere are many networking solutions available to suit a broad range of use-cases. libnetwork uses a driver / plugin model to support all of these solutions while abstracting the complexity of the driver implementations by exposing a simple and consistent Network Model to users.\n\n\n```go\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/docker/docker/pkg/reexec\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n)\n\nfunc main() {\n\tif reexec.Init() {\n\t\treturn\n\t}\n\n\t// Select and configure the network driver\n\tnetworkType := \"bridge\"\n\n\t// Create a new controller instance\n\tdriverOptions := options.Generic{}\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = driverOptions\n\tcontroller, err := libnetwork.New(config.OptionDriverConfig(networkType, genericOption))\n\tif err != nil {\n\t\tlog.Fatalf(\"libnetwork.New: %s\", err)\n\t}\n\n\t// Create a network for containers to join.\n\t// NewNetwork accepts Variadic optional arguments that libnetwork and Drivers can use.\n\tnetwork, err := controller.NewNetwork(networkType, \"network1\", \"\")\n\tif err != nil {\n\t\tlog.Fatalf(\"controller.NewNetwork: %s\", err)\n\t}\n\n\t// For each new container: allocate IP and interfaces. The returned network\n\t// settings will be used for container infos (inspect and such), as well as\n\t// iptables rules for port publishing. This info is contained or accessible\n\t// from the returned endpoint.\n\tep, err := network.CreateEndpoint(\"Endpoint1\")\n\tif err != nil {\n\t\tlog.Fatalf(\"network.CreateEndpoint: %s\", err)\n\t}\n\n\t// Create the sandbox for the container.\n\t// NewSandbox accepts Variadic optional arguments which libnetwork can use.\n\tsbx, err := controller.NewSandbox(\"container1\",\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"))\n\tif err != nil {\n\t\tlog.Fatalf(\"controller.NewSandbox: %s\", err)\n\t}\n\n\t// A sandbox can join the endpoint via the join api.\n\terr = ep.Join(sbx)\n\tif err != nil {\n\t\tlog.Fatalf(\"ep.Join: %s\", err)\n\t}\n\n\t// libnetwork client can check the endpoint's operational data via the Info() API\n\tepInfo, err := ep.DriverInfo()\n\tif err != nil {\n\t\tlog.Fatalf(\"ep.DriverInfo: %s\", err)\n\t}\n\n\tmacAddress, ok := epInfo[netlabel.MacAddress]\n\tif !ok {\n\t\tlog.Fatalf(\"failed to get mac address from endpoint info\")\n\t}\n\n\tfmt.Printf(\"Joined endpoint %s (%s) to sandbox %s (%s)\\n\", ep.Name(), macAddress, sbx.ContainerID(), sbx.Key())\n}\n```\n\n## Contributing\n\nWant to hack on libnetwork? [Docker's contributions guidelines](https://github.com/docker/docker/blob/master/CONTRIBUTING.md) apply.\n\n## Copyright and license\nCode and documentation copyright 2015 Docker, inc. Code released under the Apache 2.0 license. Docs released under Creative commons.\n"
        },
        {
          "name": "Vagrantfile",
          "type": "blob",
          "size": 1.8466796875,
          "content": "# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!\nVAGRANTFILE_API_VERSION = \"2\"\n\n$consul=<<SCRIPT\napt-get update\napt-get -y install wget\nwget -qO- https://experimental.docker.com/ | sh\ngpasswd -a vagrant docker\nservice docker restart\ndocker run -d -p 8500:8500 -p 8300-8302:8300-8302/tcp -p 8300-8302:8300-8302/udp -h consul progrium/consul -server -bootstrap\nSCRIPT\n\n$bootstrap=<<SCRIPT\napt-get update\napt-get -y install wget curl\napt-get -y install bridge-utils\nwget -qO- https://experimental.docker.com/ | sh\ngpasswd -a vagrant docker\necho DOCKER_OPTS=\\\\\"--cluster-store=consul://192.168.33.10:8500 --cluster-advertise=${1}:0\\\\\" >> /etc/default/docker\ncp /vagrant/docs/vagrant-systemd/docker.service /etc/systemd/system/\nsystemctl daemon-reload\nsystemctl restart docker.service\nSCRIPT\n\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n  config.ssh.shell = \"bash -c 'BASH_ENV=/etc/profile exec bash'\"\n  num_nodes = 2\n  base_ip = \"192.168.33.\"\n  net_ips = num_nodes.times.collect { |n| base_ip + \"#{n+11}\" }\n\n  config.vm.define \"consul-server\" do |consul|\n    consul.vm.box = \"ubuntu/trusty64\"\n    consul.vm.hostname = \"consul-server\"\n    consul.vm.network :private_network, ip: \"192.168.33.10\"\n    consul.vm.provider \"virtualbox\" do |vb|\n     vb.customize [\"modifyvm\", :id, \"--memory\", \"512\"]\n    end\n    consul.vm.provision :shell, inline: $consul\n  end\n\n  num_nodes.times do |n|\n    config.vm.define \"net-#{n+1}\" do |net|\n      net.vm.box = \"ubuntu/xenial64\"\n      net_ip = net_ips[n]\n      net_index = n+1\n      net.vm.hostname = \"net-#{net_index}\"\n      net.vm.provider \"virtualbox\" do |vb|\n        vb.customize [\"modifyvm\", :id, \"--memory\", \"1024\"]\n      end\n      net.vm.network :private_network, ip: \"#{net_ip}\"\n      net.vm.provision :shell, inline: $bootstrap, :args => \"#{net_ip}\"\n    end\n  end\n\nend\n"
        },
        {
          "name": "agent.go",
          "type": "blob",
          "size": 26.837890625,
          "content": "package libnetwork\n\n//go:generate protoc -I.:Godeps/_workspace/src/github.com/gogo/protobuf  --gogo_out=import_path=github.com/docker/libnetwork,Mgogoproto/gogo.proto=github.com/gogo/protobuf/gogoproto:. agent.proto\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"sort\"\n\t\"sync\"\n\n\t\"github.com/docker/go-events\"\n\t\"github.com/docker/libnetwork/cluster\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/discoverapi\"\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/networkdb\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst (\n\tsubsysGossip = \"networking:gossip\"\n\tsubsysIPSec  = \"networking:ipsec\"\n\tkeyringSize  = 3\n)\n\n// ByTime implements sort.Interface for []*types.EncryptionKey based on\n// the LamportTime field.\ntype ByTime []*types.EncryptionKey\n\nfunc (b ByTime) Len() int           { return len(b) }\nfunc (b ByTime) Swap(i, j int)      { b[i], b[j] = b[j], b[i] }\nfunc (b ByTime) Less(i, j int) bool { return b[i].LamportTime < b[j].LamportTime }\n\ntype agent struct {\n\tnetworkDB         *networkdb.NetworkDB\n\tbindAddr          string\n\tadvertiseAddr     string\n\tdataPathAddr      string\n\tcoreCancelFuncs   []func()\n\tdriverCancelFuncs map[string][]func()\n\tsync.Mutex\n}\n\nfunc (a *agent) dataPathAddress() string {\n\ta.Lock()\n\tdefer a.Unlock()\n\tif a.dataPathAddr != \"\" {\n\t\treturn a.dataPathAddr\n\t}\n\treturn a.advertiseAddr\n}\n\nconst libnetworkEPTable = \"endpoint_table\"\n\nfunc getBindAddr(ifaceName string) (string, error) {\n\tiface, err := net.InterfaceByName(ifaceName)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to find interface %s: %v\", ifaceName, err)\n\t}\n\n\taddrs, err := iface.Addrs()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to get interface addresses: %v\", err)\n\t}\n\n\tfor _, a := range addrs {\n\t\taddr, ok := a.(*net.IPNet)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\taddrIP := addr.IP\n\n\t\tif addrIP.IsLinkLocalUnicast() {\n\t\t\tcontinue\n\t\t}\n\n\t\treturn addrIP.String(), nil\n\t}\n\n\treturn \"\", fmt.Errorf(\"failed to get bind address\")\n}\n\nfunc resolveAddr(addrOrInterface string) (string, error) {\n\t// Try and see if this is a valid IP address\n\tif net.ParseIP(addrOrInterface) != nil {\n\t\treturn addrOrInterface, nil\n\t}\n\n\taddr, err := net.ResolveIPAddr(\"ip\", addrOrInterface)\n\tif err != nil {\n\t\t// If not a valid IP address, it should be a valid interface\n\t\treturn getBindAddr(addrOrInterface)\n\t}\n\treturn addr.String(), nil\n}\n\nfunc (c *controller) handleKeyChange(keys []*types.EncryptionKey) error {\n\tdrvEnc := discoverapi.DriverEncryptionUpdate{}\n\n\ta := c.getAgent()\n\tif a == nil {\n\t\tlogrus.Debug(\"Skipping key change as agent is nil\")\n\t\treturn nil\n\t}\n\n\t// Find the deleted key. If the deleted key was the primary key,\n\t// a new primary key should be set before removing if from keyring.\n\tc.Lock()\n\tadded := []byte{}\n\tdeleted := []byte{}\n\tj := len(c.keys)\n\tfor i := 0; i < j; {\n\t\tsame := false\n\t\tfor _, key := range keys {\n\t\t\tif same = key.LamportTime == c.keys[i].LamportTime; same {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !same {\n\t\t\tcKey := c.keys[i]\n\t\t\tif cKey.Subsystem == subsysGossip {\n\t\t\t\tdeleted = cKey.Key\n\t\t\t}\n\n\t\t\tif cKey.Subsystem == subsysIPSec {\n\t\t\t\tdrvEnc.Prune = cKey.Key\n\t\t\t\tdrvEnc.PruneTag = cKey.LamportTime\n\t\t\t}\n\t\t\tc.keys[i], c.keys[j-1] = c.keys[j-1], c.keys[i]\n\t\t\tc.keys[j-1] = nil\n\t\t\tj--\n\t\t}\n\t\ti++\n\t}\n\tc.keys = c.keys[:j]\n\n\t// Find the new key and add it to the key ring\n\tfor _, key := range keys {\n\t\tsame := false\n\t\tfor _, cKey := range c.keys {\n\t\t\tif same = cKey.LamportTime == key.LamportTime; same {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !same {\n\t\t\tc.keys = append(c.keys, key)\n\t\t\tif key.Subsystem == subsysGossip {\n\t\t\t\tadded = key.Key\n\t\t\t}\n\n\t\t\tif key.Subsystem == subsysIPSec {\n\t\t\t\tdrvEnc.Key = key.Key\n\t\t\t\tdrvEnc.Tag = key.LamportTime\n\t\t\t}\n\t\t}\n\t}\n\tc.Unlock()\n\n\tif len(added) > 0 {\n\t\ta.networkDB.SetKey(added)\n\t}\n\n\tkey, _, err := c.getPrimaryKeyTag(subsysGossip)\n\tif err != nil {\n\t\treturn err\n\t}\n\ta.networkDB.SetPrimaryKey(key)\n\n\tkey, tag, err := c.getPrimaryKeyTag(subsysIPSec)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdrvEnc.Primary = key\n\tdrvEnc.PrimaryTag = tag\n\n\tif len(deleted) > 0 {\n\t\ta.networkDB.RemoveKey(deleted)\n\t}\n\n\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\terr := driver.DiscoverNew(discoverapi.EncryptionKeysUpdate, drvEnc)\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Failed to update datapath keys in driver %s: %v\", name, err)\n\t\t\t// Attempt to reconfigure keys in case of a update failure\n\t\t\t// which can arise due to a mismatch of keys\n\t\t\t// if worker nodes get temporarily disconnected\n\t\t\tlogrus.Warnf(\"Reconfiguring datapath keys for  %s\", name)\n\t\t\tdrvCfgEnc := discoverapi.DriverEncryptionConfig{}\n\t\t\tdrvCfgEnc.Keys, drvCfgEnc.Tags = c.getKeys(subsysIPSec)\n\t\t\terr = driver.DiscoverNew(discoverapi.EncryptionKeysConfig, drvCfgEnc)\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to reset datapath keys in driver %s: %v\", name, err)\n\t\t\t}\n\t\t}\n\t\treturn false\n\t})\n\n\treturn nil\n}\n\nfunc (c *controller) agentSetup(clusterProvider cluster.Provider) error {\n\tagent := c.getAgent()\n\n\t// If the agent is already present there is no need to try to initialize it again\n\tif agent != nil {\n\t\treturn nil\n\t}\n\n\tbindAddr := clusterProvider.GetLocalAddress()\n\tadvAddr := clusterProvider.GetAdvertiseAddress()\n\tdataAddr := clusterProvider.GetDataPathAddress()\n\tremoteList := clusterProvider.GetRemoteAddressList()\n\tremoteAddrList := make([]string, 0, len(remoteList))\n\tfor _, remote := range remoteList {\n\t\taddr, _, _ := net.SplitHostPort(remote)\n\t\tremoteAddrList = append(remoteAddrList, addr)\n\t}\n\n\tlisten := clusterProvider.GetListenAddress()\n\tlistenAddr, _, _ := net.SplitHostPort(listen)\n\n\tlogrus.Infof(\"Initializing Libnetwork Agent Listen-Addr=%s Local-addr=%s Adv-addr=%s Data-addr=%s Remote-addr-list=%v MTU=%d\",\n\t\tlistenAddr, bindAddr, advAddr, dataAddr, remoteAddrList, c.Config().Daemon.NetworkControlPlaneMTU)\n\tif advAddr != \"\" && agent == nil {\n\t\tif err := c.agentInit(listenAddr, bindAddr, advAddr, dataAddr); err != nil {\n\t\t\tlogrus.Errorf(\"error in agentInit: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\t\tif capability.ConnectivityScope == datastore.GlobalScope {\n\t\t\t\tc.agentDriverNotify(driver)\n\t\t\t}\n\t\t\treturn false\n\t\t})\n\t}\n\n\tif len(remoteAddrList) > 0 {\n\t\tif err := c.agentJoin(remoteAddrList); err != nil {\n\t\t\tlogrus.Errorf(\"Error in joining gossip cluster : %v(join will be retried in background)\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// For a given subsystem getKeys sorts the keys by lamport time and returns\n// slice of keys and lamport time which can used as a unique tag for the keys\nfunc (c *controller) getKeys(subsys string) ([][]byte, []uint64) {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tsort.Sort(ByTime(c.keys))\n\n\tkeys := [][]byte{}\n\ttags := []uint64{}\n\tfor _, key := range c.keys {\n\t\tif key.Subsystem == subsys {\n\t\t\tkeys = append(keys, key.Key)\n\t\t\ttags = append(tags, key.LamportTime)\n\t\t}\n\t}\n\n\tkeys[0], keys[1] = keys[1], keys[0]\n\ttags[0], tags[1] = tags[1], tags[0]\n\treturn keys, tags\n}\n\n// getPrimaryKeyTag returns the primary key for a given subsystem from the\n// list of sorted key and the associated tag\nfunc (c *controller) getPrimaryKeyTag(subsys string) ([]byte, uint64, error) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tsort.Sort(ByTime(c.keys))\n\tkeys := []*types.EncryptionKey{}\n\tfor _, key := range c.keys {\n\t\tif key.Subsystem == subsys {\n\t\t\tkeys = append(keys, key)\n\t\t}\n\t}\n\treturn keys[1].Key, keys[1].LamportTime, nil\n}\n\nfunc (c *controller) agentInit(listenAddr, bindAddrOrInterface, advertiseAddr, dataPathAddr string) error {\n\tbindAddr, err := resolveAddr(bindAddrOrInterface)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tkeys, _ := c.getKeys(subsysGossip)\n\n\tnetDBConf := networkdb.DefaultConfig()\n\tnetDBConf.BindAddr = listenAddr\n\tnetDBConf.AdvertiseAddr = advertiseAddr\n\tnetDBConf.Keys = keys\n\tif c.Config().Daemon.NetworkControlPlaneMTU != 0 {\n\t\t// Consider the MTU remove the IP hdr (IPv4 or IPv6) and the TCP/UDP hdr.\n\t\t// To be on the safe side let's cut 100 bytes\n\t\tnetDBConf.PacketBufferSize = (c.Config().Daemon.NetworkControlPlaneMTU - 100)\n\t\tlogrus.Debugf(\"Control plane MTU: %d will initialize NetworkDB with: %d\",\n\t\t\tc.Config().Daemon.NetworkControlPlaneMTU, netDBConf.PacketBufferSize)\n\t}\n\tnDB, err := networkdb.New(netDBConf)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Register the diagnostic handlers\n\tc.DiagnosticServer.RegisterHandler(nDB, networkdb.NetDbPaths2Func)\n\n\tvar cancelList []func()\n\tch, cancel := nDB.Watch(libnetworkEPTable, \"\", \"\")\n\tcancelList = append(cancelList, cancel)\n\tnodeCh, cancel := nDB.Watch(networkdb.NodeTable, \"\", \"\")\n\tcancelList = append(cancelList, cancel)\n\n\tc.Lock()\n\tc.agent = &agent{\n\t\tnetworkDB:         nDB,\n\t\tbindAddr:          bindAddr,\n\t\tadvertiseAddr:     advertiseAddr,\n\t\tdataPathAddr:      dataPathAddr,\n\t\tcoreCancelFuncs:   cancelList,\n\t\tdriverCancelFuncs: make(map[string][]func()),\n\t}\n\tc.Unlock()\n\n\tgo c.handleTableEvents(ch, c.handleEpTableEvent)\n\tgo c.handleTableEvents(nodeCh, c.handleNodeTableEvent)\n\n\tdrvEnc := discoverapi.DriverEncryptionConfig{}\n\tkeys, tags := c.getKeys(subsysIPSec)\n\tdrvEnc.Keys = keys\n\tdrvEnc.Tags = tags\n\n\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\terr := driver.DiscoverNew(discoverapi.EncryptionKeysConfig, drvEnc)\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Failed to set datapath keys in driver %s: %v\", name, err)\n\t\t}\n\t\treturn false\n\t})\n\n\tc.WalkNetworks(joinCluster)\n\n\treturn nil\n}\n\nfunc (c *controller) agentJoin(remoteAddrList []string) error {\n\tagent := c.getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\treturn agent.networkDB.Join(remoteAddrList)\n}\n\nfunc (c *controller) agentDriverNotify(d driverapi.Driver) {\n\tagent := c.getAgent()\n\tif agent == nil {\n\t\treturn\n\t}\n\n\tif err := d.DiscoverNew(discoverapi.NodeDiscovery, discoverapi.NodeDiscoveryData{\n\t\tAddress:     agent.dataPathAddress(),\n\t\tBindAddress: agent.bindAddr,\n\t\tSelf:        true,\n\t}); err != nil {\n\t\tlogrus.Warnf(\"Failed the node discovery in driver: %v\", err)\n\t}\n\n\tdrvEnc := discoverapi.DriverEncryptionConfig{}\n\tkeys, tags := c.getKeys(subsysIPSec)\n\tdrvEnc.Keys = keys\n\tdrvEnc.Tags = tags\n\n\tif err := d.DiscoverNew(discoverapi.EncryptionKeysConfig, drvEnc); err != nil {\n\t\tlogrus.Warnf(\"Failed to set datapath keys in driver: %v\", err)\n\t}\n}\n\nfunc (c *controller) agentClose() {\n\t// Acquire current agent instance and reset its pointer\n\t// then run closing functions\n\tc.Lock()\n\tagent := c.agent\n\tc.agent = nil\n\tc.Unlock()\n\n\t// when the agent is closed the cluster provider should be cleaned up\n\tc.SetClusterProvider(nil)\n\n\tif agent == nil {\n\t\treturn\n\t}\n\n\tvar cancelList []func()\n\n\tagent.Lock()\n\tfor _, cancelFuncs := range agent.driverCancelFuncs {\n\t\tcancelList = append(cancelList, cancelFuncs...)\n\t}\n\n\t// Add also the cancel functions for the network db\n\tcancelList = append(cancelList, agent.coreCancelFuncs...)\n\tagent.Unlock()\n\n\tfor _, cancel := range cancelList {\n\t\tcancel()\n\t}\n\n\tagent.networkDB.Close()\n}\n\n// Task has the backend container details\ntype Task struct {\n\tName       string\n\tEndpointID string\n\tEndpointIP string\n\tInfo       map[string]string\n}\n\n// ServiceInfo has service specific details along with the list of backend tasks\ntype ServiceInfo struct {\n\tVIP          string\n\tLocalLBIndex int\n\tTasks        []Task\n\tPorts        []string\n}\n\ntype epRecord struct {\n\tep      EndpointRecord\n\tinfo    map[string]string\n\tlbIndex int\n}\n\nfunc (n *network) Services() map[string]ServiceInfo {\n\teps := make(map[string]epRecord)\n\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\n\t// Walk through libnetworkEPTable and fetch the driver agnostic endpoint info\n\tentries := agent.networkDB.GetTableByNetwork(libnetworkEPTable, n.id)\n\tfor eid, value := range entries {\n\t\tvar epRec EndpointRecord\n\t\tnid := n.ID()\n\t\tif err := proto.Unmarshal(value.Value, &epRec); err != nil {\n\t\t\tlogrus.Errorf(\"Unmarshal of libnetworkEPTable failed for endpoint %s in network %s, %v\", eid, nid, err)\n\t\t\tcontinue\n\t\t}\n\t\ti := n.getController().getLBIndex(epRec.ServiceID, nid, epRec.IngressPorts)\n\t\teps[eid] = epRecord{\n\t\t\tep:      epRec,\n\t\t\tlbIndex: i,\n\t\t}\n\t}\n\n\t// Walk through the driver's tables, have the driver decode the entries\n\t// and return the tuple {ep ID, value}. value is a string that coveys\n\t// relevant info about the endpoint.\n\td, err := n.driver(true)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Could not resolve driver for network %s/%s while fetching services: %v\", n.networkType, n.ID(), err)\n\t\treturn nil\n\t}\n\tfor _, table := range n.driverTables {\n\t\tif table.objType != driverapi.EndpointObject {\n\t\t\tcontinue\n\t\t}\n\t\tentries := agent.networkDB.GetTableByNetwork(table.name, n.id)\n\t\tfor key, value := range entries {\n\t\t\tepID, info := d.DecodeTableEntry(table.name, key, value.Value)\n\t\t\tif ep, ok := eps[epID]; !ok {\n\t\t\t\tlogrus.Errorf(\"Inconsistent driver and libnetwork state for endpoint %s\", epID)\n\t\t\t} else {\n\t\t\t\tep.info = info\n\t\t\t\teps[epID] = ep\n\t\t\t}\n\t\t}\n\t}\n\n\t// group the endpoints into a map keyed by the service name\n\tsinfo := make(map[string]ServiceInfo)\n\tfor ep, epr := range eps {\n\t\tvar (\n\t\t\ts  ServiceInfo\n\t\t\tok bool\n\t\t)\n\t\tif s, ok = sinfo[epr.ep.ServiceName]; !ok {\n\t\t\ts = ServiceInfo{\n\t\t\t\tVIP:          epr.ep.VirtualIP,\n\t\t\t\tLocalLBIndex: epr.lbIndex,\n\t\t\t}\n\t\t}\n\t\tports := []string{}\n\t\tif s.Ports == nil {\n\t\t\tfor _, port := range epr.ep.IngressPorts {\n\t\t\t\tp := fmt.Sprintf(\"Target: %d, Publish: %d\", port.TargetPort, port.PublishedPort)\n\t\t\t\tports = append(ports, p)\n\t\t\t}\n\t\t\ts.Ports = ports\n\t\t}\n\t\ts.Tasks = append(s.Tasks, Task{\n\t\t\tName:       epr.ep.Name,\n\t\t\tEndpointID: ep,\n\t\t\tEndpointIP: epr.ep.EndpointIP,\n\t\t\tInfo:       epr.info,\n\t\t})\n\t\tsinfo[epr.ep.ServiceName] = s\n\t}\n\treturn sinfo\n}\n\nfunc (n *network) isClusterEligible() bool {\n\tif n.scope != datastore.SwarmScope || !n.driverIsMultihost() {\n\t\treturn false\n\t}\n\treturn n.getController().getAgent() != nil\n}\n\nfunc (n *network) joinCluster() error {\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\n\treturn agent.networkDB.JoinNetwork(n.ID())\n}\n\nfunc (n *network) leaveCluster() error {\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\n\treturn agent.networkDB.LeaveNetwork(n.ID())\n}\n\nfunc (ep *endpoint) addDriverInfoToCluster() error {\n\tn := ep.getNetwork()\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\tif ep.joinInfo == nil {\n\t\treturn nil\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\n\tfor _, te := range ep.joinInfo.driverTableEntries {\n\t\tif err := agent.networkDB.CreateEntry(te.tableName, n.ID(), te.key, te.value); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (ep *endpoint) deleteDriverInfoFromCluster() error {\n\tn := ep.getNetwork()\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\tif ep.joinInfo == nil {\n\t\treturn nil\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn nil\n\t}\n\n\tfor _, te := range ep.joinInfo.driverTableEntries {\n\t\tif err := agent.networkDB.DeleteEntry(te.tableName, n.ID(), te.key); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (ep *endpoint) addServiceInfoToCluster(sb *sandbox) error {\n\tif ep.isAnonymous() && len(ep.myAliases) == 0 || ep.Iface() == nil || ep.Iface().Address() == nil {\n\t\treturn nil\n\t}\n\n\tn := ep.getNetwork()\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\n\tsb.Service.Lock()\n\tdefer sb.Service.Unlock()\n\tlogrus.Debugf(\"addServiceInfoToCluster START for %s %s\", ep.svcName, ep.ID())\n\n\t// Check that the endpoint is still present on the sandbox before adding it to the service discovery.\n\t// This is to handle a race between the EnableService and the sbLeave\n\t// It is possible that the EnableService starts, fetches the list of the endpoints and\n\t// by the time the addServiceInfoToCluster is called the endpoint got removed from the sandbox\n\t// The risk is that the deleteServiceInfoToCluster happens before the addServiceInfoToCluster.\n\t// This check under the Service lock of the sandbox ensure the correct behavior.\n\t// If the addServiceInfoToCluster arrives first may find or not the endpoint and will proceed or exit\n\t// but in any case the deleteServiceInfoToCluster will follow doing the cleanup if needed.\n\t// In case the deleteServiceInfoToCluster arrives first, this one is happening after the endpoint is\n\t// removed from the list, in this situation the delete will bail out not finding any data to cleanup\n\t// and the add will bail out not finding the endpoint on the sandbox.\n\tif e := sb.getEndpoint(ep.ID()); e == nil {\n\t\tlogrus.Warnf(\"addServiceInfoToCluster suppressing service resolution ep is not anymore in the sandbox %s\", ep.ID())\n\t\treturn nil\n\t}\n\n\tc := n.getController()\n\tagent := c.getAgent()\n\n\tname := ep.Name()\n\tif ep.isAnonymous() {\n\t\tname = ep.MyAliases()[0]\n\t}\n\n\tvar ingressPorts []*PortConfig\n\tif ep.svcID != \"\" {\n\t\t// This is a task part of a service\n\t\t// Gossip ingress ports only in ingress network.\n\t\tif n.ingress {\n\t\t\tingressPorts = ep.ingressPorts\n\t\t}\n\t\tif err := c.addServiceBinding(ep.svcName, ep.svcID, n.ID(), ep.ID(), name, ep.virtualIP, ingressPorts, ep.svcAliases, ep.myAliases, ep.Iface().Address().IP, \"addServiceInfoToCluster\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// This is a container simply attached to an attachable network\n\t\tif err := c.addContainerNameResolution(n.ID(), ep.ID(), name, ep.myAliases, ep.Iface().Address().IP, \"addServiceInfoToCluster\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tbuf, err := proto.Marshal(&EndpointRecord{\n\t\tName:            name,\n\t\tServiceName:     ep.svcName,\n\t\tServiceID:       ep.svcID,\n\t\tVirtualIP:       ep.virtualIP.String(),\n\t\tIngressPorts:    ingressPorts,\n\t\tAliases:         ep.svcAliases,\n\t\tTaskAliases:     ep.myAliases,\n\t\tEndpointIP:      ep.Iface().Address().IP.String(),\n\t\tServiceDisabled: false,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif agent != nil {\n\t\tif err := agent.networkDB.CreateEntry(libnetworkEPTable, n.ID(), ep.ID(), buf); err != nil {\n\t\t\tlogrus.Warnf(\"addServiceInfoToCluster NetworkDB CreateEntry failed for %s %s err:%s\", ep.id, n.id, err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlogrus.Debugf(\"addServiceInfoToCluster END for %s %s\", ep.svcName, ep.ID())\n\n\treturn nil\n}\n\nfunc (ep *endpoint) deleteServiceInfoFromCluster(sb *sandbox, fullRemove bool, method string) error {\n\tif ep.isAnonymous() && len(ep.myAliases) == 0 {\n\t\treturn nil\n\t}\n\n\tn := ep.getNetwork()\n\tif !n.isClusterEligible() {\n\t\treturn nil\n\t}\n\n\tsb.Service.Lock()\n\tdefer sb.Service.Unlock()\n\tlogrus.Debugf(\"deleteServiceInfoFromCluster from %s START for %s %s\", method, ep.svcName, ep.ID())\n\n\t// Avoid a race w/ with a container that aborts preemptively.  This would\n\t// get caught in disableServceInNetworkDB, but we check here to make the\n\t// nature of the condition more clear.\n\t// See comment in addServiceInfoToCluster()\n\tif e := sb.getEndpoint(ep.ID()); e == nil {\n\t\tlogrus.Warnf(\"deleteServiceInfoFromCluster suppressing service resolution ep is not anymore in the sandbox %s\", ep.ID())\n\t\treturn nil\n\t}\n\n\tc := n.getController()\n\tagent := c.getAgent()\n\n\tname := ep.Name()\n\tif ep.isAnonymous() {\n\t\tname = ep.MyAliases()[0]\n\t}\n\n\tif agent != nil {\n\t\t// First update the networkDB then locally\n\t\tif fullRemove {\n\t\t\tif err := agent.networkDB.DeleteEntry(libnetworkEPTable, n.ID(), ep.ID()); err != nil {\n\t\t\t\tlogrus.Warnf(\"deleteServiceInfoFromCluster NetworkDB DeleteEntry failed for %s %s err:%s\", ep.id, n.id, err)\n\t\t\t}\n\t\t} else {\n\t\t\tdisableServiceInNetworkDB(agent, n, ep)\n\t\t}\n\t}\n\n\tif ep.Iface() != nil && ep.Iface().Address() != nil {\n\t\tif ep.svcID != \"\" {\n\t\t\t// This is a task part of a service\n\t\t\tvar ingressPorts []*PortConfig\n\t\t\tif n.ingress {\n\t\t\t\tingressPorts = ep.ingressPorts\n\t\t\t}\n\t\t\tif err := c.rmServiceBinding(ep.svcName, ep.svcID, n.ID(), ep.ID(), name, ep.virtualIP, ingressPorts, ep.svcAliases, ep.myAliases, ep.Iface().Address().IP, \"deleteServiceInfoFromCluster\", true, fullRemove); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\t// This is a container simply attached to an attachable network\n\t\t\tif err := c.delContainerNameResolution(n.ID(), ep.ID(), name, ep.myAliases, ep.Iface().Address().IP, \"deleteServiceInfoFromCluster\"); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tlogrus.Debugf(\"deleteServiceInfoFromCluster from %s END for %s %s\", method, ep.svcName, ep.ID())\n\n\treturn nil\n}\n\nfunc disableServiceInNetworkDB(a *agent, n *network, ep *endpoint) {\n\tvar epRec EndpointRecord\n\n\tlogrus.Debugf(\"disableServiceInNetworkDB for %s %s\", ep.svcName, ep.ID())\n\n\t// Update existing record to indicate that the service is disabled\n\tinBuf, err := a.networkDB.GetEntry(libnetworkEPTable, n.ID(), ep.ID())\n\tif err != nil {\n\t\tlogrus.Warnf(\"disableServiceInNetworkDB GetEntry failed for %s %s err:%s\", ep.id, n.id, err)\n\t\treturn\n\t}\n\t// Should never fail\n\tif err := proto.Unmarshal(inBuf, &epRec); err != nil {\n\t\tlogrus.Errorf(\"disableServiceInNetworkDB unmarshal failed for %s %s err:%s\", ep.id, n.id, err)\n\t\treturn\n\t}\n\tepRec.ServiceDisabled = true\n\t// Should never fail\n\toutBuf, err := proto.Marshal(&epRec)\n\tif err != nil {\n\t\tlogrus.Errorf(\"disableServiceInNetworkDB marshalling failed for %s %s err:%s\", ep.id, n.id, err)\n\t\treturn\n\t}\n\t// Send update to the whole cluster\n\tif err := a.networkDB.UpdateEntry(libnetworkEPTable, n.ID(), ep.ID(), outBuf); err != nil {\n\t\tlogrus.Warnf(\"disableServiceInNetworkDB UpdateEntry failed for %s %s err:%s\", ep.id, n.id, err)\n\t}\n}\n\nfunc (n *network) addDriverWatches() {\n\tif !n.isClusterEligible() {\n\t\treturn\n\t}\n\n\tc := n.getController()\n\tagent := c.getAgent()\n\tif agent == nil {\n\t\treturn\n\t}\n\tfor _, table := range n.driverTables {\n\t\tch, cancel := agent.networkDB.Watch(table.name, n.ID(), \"\")\n\t\tagent.Lock()\n\t\tagent.driverCancelFuncs[n.ID()] = append(agent.driverCancelFuncs[n.ID()], cancel)\n\t\tagent.Unlock()\n\t\tgo c.handleTableEvents(ch, n.handleDriverTableEvent)\n\t\td, err := n.driver(false)\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Could not resolve driver %s while walking driver tabl: %v\", n.networkType, err)\n\t\t\treturn\n\t\t}\n\n\t\tagent.networkDB.WalkTable(table.name, func(nid, key string, value []byte, deleted bool) bool {\n\t\t\t// skip the entries that are mark for deletion, this is safe because this function is\n\t\t\t// called at initialization time so there is no state to delete\n\t\t\tif nid == n.ID() && !deleted {\n\t\t\t\td.EventNotify(driverapi.Create, nid, table.name, key, value)\n\t\t\t}\n\t\t\treturn false\n\t\t})\n\t}\n}\n\nfunc (n *network) cancelDriverWatches() {\n\tif !n.isClusterEligible() {\n\t\treturn\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn\n\t}\n\n\tagent.Lock()\n\tcancelFuncs := agent.driverCancelFuncs[n.ID()]\n\tdelete(agent.driverCancelFuncs, n.ID())\n\tagent.Unlock()\n\n\tfor _, cancel := range cancelFuncs {\n\t\tcancel()\n\t}\n}\n\nfunc (c *controller) handleTableEvents(ch *events.Channel, fn func(events.Event)) {\n\tfor {\n\t\tselect {\n\t\tcase ev := <-ch.C:\n\t\t\tfn(ev)\n\t\tcase <-ch.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (n *network) handleDriverTableEvent(ev events.Event) {\n\td, err := n.driver(false)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Could not resolve driver %s while handling driver table event: %v\", n.networkType, err)\n\t\treturn\n\t}\n\n\tvar (\n\t\tetype driverapi.EventType\n\t\ttname string\n\t\tkey   string\n\t\tvalue []byte\n\t)\n\n\tswitch event := ev.(type) {\n\tcase networkdb.CreateEvent:\n\t\ttname = event.Table\n\t\tkey = event.Key\n\t\tvalue = event.Value\n\t\tetype = driverapi.Create\n\tcase networkdb.DeleteEvent:\n\t\ttname = event.Table\n\t\tkey = event.Key\n\t\tvalue = event.Value\n\t\tetype = driverapi.Delete\n\tcase networkdb.UpdateEvent:\n\t\ttname = event.Table\n\t\tkey = event.Key\n\t\tvalue = event.Value\n\t\tetype = driverapi.Delete\n\t}\n\n\td.EventNotify(etype, n.ID(), tname, key, value)\n}\n\nfunc (c *controller) handleNodeTableEvent(ev events.Event) {\n\tvar (\n\t\tvalue    []byte\n\t\tisAdd    bool\n\t\tnodeAddr networkdb.NodeAddr\n\t)\n\tswitch event := ev.(type) {\n\tcase networkdb.CreateEvent:\n\t\tvalue = event.Value\n\t\tisAdd = true\n\tcase networkdb.DeleteEvent:\n\t\tvalue = event.Value\n\tcase networkdb.UpdateEvent:\n\t\tlogrus.Errorf(\"Unexpected update node table event = %#v\", event)\n\t}\n\n\terr := json.Unmarshal(value, &nodeAddr)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Error unmarshalling node table event %v\", err)\n\t\treturn\n\t}\n\tc.processNodeDiscovery([]net.IP{nodeAddr.Addr}, isAdd)\n\n}\n\nfunc (c *controller) handleEpTableEvent(ev events.Event) {\n\tvar (\n\t\tnid   string\n\t\teid   string\n\t\tvalue []byte\n\t\tepRec EndpointRecord\n\t)\n\n\tswitch event := ev.(type) {\n\tcase networkdb.CreateEvent:\n\t\tnid = event.NetworkID\n\t\teid = event.Key\n\t\tvalue = event.Value\n\tcase networkdb.DeleteEvent:\n\t\tnid = event.NetworkID\n\t\teid = event.Key\n\t\tvalue = event.Value\n\tcase networkdb.UpdateEvent:\n\t\tnid = event.NetworkID\n\t\teid = event.Key\n\t\tvalue = event.Value\n\tdefault:\n\t\tlogrus.Errorf(\"Unexpected update service table event = %#v\", event)\n\t\treturn\n\t}\n\n\terr := proto.Unmarshal(value, &epRec)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Failed to unmarshal service table value: %v\", err)\n\t\treturn\n\t}\n\n\tcontainerName := epRec.Name\n\tsvcName := epRec.ServiceName\n\tsvcID := epRec.ServiceID\n\tvip := net.ParseIP(epRec.VirtualIP)\n\tip := net.ParseIP(epRec.EndpointIP)\n\tingressPorts := epRec.IngressPorts\n\tserviceAliases := epRec.Aliases\n\ttaskAliases := epRec.TaskAliases\n\n\tif containerName == \"\" || ip == nil {\n\t\tlogrus.Errorf(\"Invalid endpoint name/ip received while handling service table event %s\", value)\n\t\treturn\n\t}\n\n\tswitch ev.(type) {\n\tcase networkdb.CreateEvent:\n\t\tlogrus.Debugf(\"handleEpTableEvent ADD %s R:%v\", eid, epRec)\n\t\tif svcID != \"\" {\n\t\t\t// This is a remote task part of a service\n\t\t\tif err := c.addServiceBinding(svcName, svcID, nid, eid, containerName, vip, ingressPorts, serviceAliases, taskAliases, ip, \"handleEpTableEvent\"); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed adding service binding for %s epRec:%v err:%v\", eid, epRec, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// This is a remote container simply attached to an attachable network\n\t\t\tif err := c.addContainerNameResolution(nid, eid, containerName, taskAliases, ip, \"handleEpTableEvent\"); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed adding container name resolution for %s epRec:%v err:%v\", eid, epRec, err)\n\t\t\t}\n\t\t}\n\n\tcase networkdb.DeleteEvent:\n\t\tlogrus.Debugf(\"handleEpTableEvent DEL %s R:%v\", eid, epRec)\n\t\tif svcID != \"\" {\n\t\t\t// This is a remote task part of a service\n\t\t\tif err := c.rmServiceBinding(svcName, svcID, nid, eid, containerName, vip, ingressPorts, serviceAliases, taskAliases, ip, \"handleEpTableEvent\", true, true); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed removing service binding for %s epRec:%v err:%v\", eid, epRec, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// This is a remote container simply attached to an attachable network\n\t\t\tif err := c.delContainerNameResolution(nid, eid, containerName, taskAliases, ip, \"handleEpTableEvent\"); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed removing container name resolution for %s epRec:%v err:%v\", eid, epRec, err)\n\t\t\t}\n\t\t}\n\tcase networkdb.UpdateEvent:\n\t\tlogrus.Debugf(\"handleEpTableEvent UPD %s R:%v\", eid, epRec)\n\t\t// We currently should only get these to inform us that an endpoint\n\t\t// is disabled.  Report if otherwise.\n\t\tif svcID == \"\" || !epRec.ServiceDisabled {\n\t\t\tlogrus.Errorf(\"Unexpected update table event for %s epRec:%v\", eid, epRec)\n\t\t\treturn\n\t\t}\n\t\t// This is a remote task that is part of a service that is now disabled\n\t\tif err := c.rmServiceBinding(svcName, svcID, nid, eid, containerName, vip, ingressPorts, serviceAliases, taskAliases, ip, \"handleEpTableEvent\", true, false); err != nil {\n\t\t\tlogrus.Errorf(\"failed disabling service binding for %s epRec:%v err:%v\", eid, epRec, err)\n\t\t\treturn\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "agent.pb.go",
          "type": "blob",
          "size": 27.892578125,
          "content": "// Code generated by protoc-gen-gogo. DO NOT EDIT.\n// source: agent.proto\n\n/*\n\tPackage libnetwork is a generated protocol buffer package.\n\n\tIt is generated from these files:\n\t\tagent.proto\n\n\tIt has these top-level messages:\n\t\tEndpointRecord\n\t\tPortConfig\n*/\npackage libnetwork\n\nimport proto \"github.com/gogo/protobuf/proto\"\nimport fmt \"fmt\"\nimport math \"math\"\nimport _ \"github.com/gogo/protobuf/gogoproto\"\n\nimport strings \"strings\"\nimport reflect \"reflect\"\n\nimport io \"io\"\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.GoGoProtoPackageIsVersion2 // please upgrade the proto package\n\ntype PortConfig_Protocol int32\n\nconst (\n\tProtocolTCP  PortConfig_Protocol = 0\n\tProtocolUDP  PortConfig_Protocol = 1\n\tProtocolSCTP PortConfig_Protocol = 2\n)\n\nvar PortConfig_Protocol_name = map[int32]string{\n\t0: \"TCP\",\n\t1: \"UDP\",\n\t2: \"SCTP\",\n}\nvar PortConfig_Protocol_value = map[string]int32{\n\t\"TCP\":  0,\n\t\"UDP\":  1,\n\t\"SCTP\": 2,\n}\n\nfunc (x PortConfig_Protocol) String() string {\n\treturn proto.EnumName(PortConfig_Protocol_name, int32(x))\n}\nfunc (PortConfig_Protocol) EnumDescriptor() ([]byte, []int) { return fileDescriptorAgent, []int{1, 0} }\n\n// EndpointRecord specifies all the endpoint specific information that\n// needs to gossiped to nodes participating in the network.\ntype EndpointRecord struct {\n\t// Name of the container\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Service name of the service to which this endpoint belongs.\n\tServiceName string `protobuf:\"bytes,2,opt,name=service_name,json=serviceName,proto3\" json:\"service_name,omitempty\"`\n\t// Service ID of the service to which this endpoint belongs.\n\tServiceID string `protobuf:\"bytes,3,opt,name=service_id,json=serviceId,proto3\" json:\"service_id,omitempty\"`\n\t// Virtual IP of the service to which this endpoint belongs.\n\tVirtualIP string `protobuf:\"bytes,4,opt,name=virtual_ip,json=virtualIp,proto3\" json:\"virtual_ip,omitempty\"`\n\t// IP assigned to this endpoint.\n\tEndpointIP string `protobuf:\"bytes,5,opt,name=endpoint_ip,json=endpointIp,proto3\" json:\"endpoint_ip,omitempty\"`\n\t// IngressPorts exposed by the service to which this endpoint belongs.\n\tIngressPorts []*PortConfig `protobuf:\"bytes,6,rep,name=ingress_ports,json=ingressPorts\" json:\"ingress_ports,omitempty\"`\n\t// A list of aliases which are alternate names for the service\n\tAliases []string `protobuf:\"bytes,7,rep,name=aliases\" json:\"aliases,omitempty\"`\n\t// List of aliases task specific aliases\n\tTaskAliases []string `protobuf:\"bytes,8,rep,name=task_aliases,json=taskAliases\" json:\"task_aliases,omitempty\"`\n\t// Whether this enpoint's service has been disabled\n\tServiceDisabled bool `protobuf:\"varint,9,opt,name=service_disabled,json=serviceDisabled,proto3\" json:\"service_disabled,omitempty\"`\n}\n\nfunc (m *EndpointRecord) Reset()                    { *m = EndpointRecord{} }\nfunc (*EndpointRecord) ProtoMessage()               {}\nfunc (*EndpointRecord) Descriptor() ([]byte, []int) { return fileDescriptorAgent, []int{0} }\n\nfunc (m *EndpointRecord) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *EndpointRecord) GetServiceName() string {\n\tif m != nil {\n\t\treturn m.ServiceName\n\t}\n\treturn \"\"\n}\n\nfunc (m *EndpointRecord) GetServiceID() string {\n\tif m != nil {\n\t\treturn m.ServiceID\n\t}\n\treturn \"\"\n}\n\nfunc (m *EndpointRecord) GetVirtualIP() string {\n\tif m != nil {\n\t\treturn m.VirtualIP\n\t}\n\treturn \"\"\n}\n\nfunc (m *EndpointRecord) GetEndpointIP() string {\n\tif m != nil {\n\t\treturn m.EndpointIP\n\t}\n\treturn \"\"\n}\n\nfunc (m *EndpointRecord) GetIngressPorts() []*PortConfig {\n\tif m != nil {\n\t\treturn m.IngressPorts\n\t}\n\treturn nil\n}\n\nfunc (m *EndpointRecord) GetAliases() []string {\n\tif m != nil {\n\t\treturn m.Aliases\n\t}\n\treturn nil\n}\n\nfunc (m *EndpointRecord) GetTaskAliases() []string {\n\tif m != nil {\n\t\treturn m.TaskAliases\n\t}\n\treturn nil\n}\n\nfunc (m *EndpointRecord) GetServiceDisabled() bool {\n\tif m != nil {\n\t\treturn m.ServiceDisabled\n\t}\n\treturn false\n}\n\n// PortConfig specifies an exposed port which can be\n// addressed using the given name. This can be later queried\n// using a service discovery api or a DNS SRV query. The node\n// port specifies a port that can be used to address this\n// service external to the cluster by sending a connection\n// request to this port to any node on the cluster.\ntype PortConfig struct {\n\t// Name for the port. If provided the port information can\n\t// be queried using the name as in a DNS SRV query.\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Protocol for the port which is exposed.\n\tProtocol PortConfig_Protocol `protobuf:\"varint,2,opt,name=protocol,proto3,enum=libnetwork.PortConfig_Protocol\" json:\"protocol,omitempty\"`\n\t// The port which the application is exposing and is bound to.\n\tTargetPort uint32 `protobuf:\"varint,3,opt,name=target_port,json=targetPort,proto3\" json:\"target_port,omitempty\"`\n\t// PublishedPort specifies the port on which the service is\n\t// exposed on all nodes on the cluster. If not specified an\n\t// arbitrary port in the node port range is allocated by the\n\t// system. If specified it should be within the node port\n\t// range and it should be available.\n\tPublishedPort uint32 `protobuf:\"varint,4,opt,name=published_port,json=publishedPort,proto3\" json:\"published_port,omitempty\"`\n}\n\nfunc (m *PortConfig) Reset()                    { *m = PortConfig{} }\nfunc (*PortConfig) ProtoMessage()               {}\nfunc (*PortConfig) Descriptor() ([]byte, []int) { return fileDescriptorAgent, []int{1} }\n\nfunc (m *PortConfig) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *PortConfig) GetProtocol() PortConfig_Protocol {\n\tif m != nil {\n\t\treturn m.Protocol\n\t}\n\treturn ProtocolTCP\n}\n\nfunc (m *PortConfig) GetTargetPort() uint32 {\n\tif m != nil {\n\t\treturn m.TargetPort\n\t}\n\treturn 0\n}\n\nfunc (m *PortConfig) GetPublishedPort() uint32 {\n\tif m != nil {\n\t\treturn m.PublishedPort\n\t}\n\treturn 0\n}\n\nfunc init() {\n\tproto.RegisterType((*EndpointRecord)(nil), \"libnetwork.EndpointRecord\")\n\tproto.RegisterType((*PortConfig)(nil), \"libnetwork.PortConfig\")\n\tproto.RegisterEnum(\"libnetwork.PortConfig_Protocol\", PortConfig_Protocol_name, PortConfig_Protocol_value)\n}\nfunc (this *EndpointRecord) GoString() string {\n\tif this == nil {\n\t\treturn \"nil\"\n\t}\n\ts := make([]string, 0, 13)\n\ts = append(s, \"&libnetwork.EndpointRecord{\")\n\ts = append(s, \"Name: \"+fmt.Sprintf(\"%#v\", this.Name)+\",\\n\")\n\ts = append(s, \"ServiceName: \"+fmt.Sprintf(\"%#v\", this.ServiceName)+\",\\n\")\n\ts = append(s, \"ServiceID: \"+fmt.Sprintf(\"%#v\", this.ServiceID)+\",\\n\")\n\ts = append(s, \"VirtualIP: \"+fmt.Sprintf(\"%#v\", this.VirtualIP)+\",\\n\")\n\ts = append(s, \"EndpointIP: \"+fmt.Sprintf(\"%#v\", this.EndpointIP)+\",\\n\")\n\tif this.IngressPorts != nil {\n\t\ts = append(s, \"IngressPorts: \"+fmt.Sprintf(\"%#v\", this.IngressPorts)+\",\\n\")\n\t}\n\ts = append(s, \"Aliases: \"+fmt.Sprintf(\"%#v\", this.Aliases)+\",\\n\")\n\ts = append(s, \"TaskAliases: \"+fmt.Sprintf(\"%#v\", this.TaskAliases)+\",\\n\")\n\ts = append(s, \"ServiceDisabled: \"+fmt.Sprintf(\"%#v\", this.ServiceDisabled)+\",\\n\")\n\ts = append(s, \"}\")\n\treturn strings.Join(s, \"\")\n}\nfunc (this *PortConfig) GoString() string {\n\tif this == nil {\n\t\treturn \"nil\"\n\t}\n\ts := make([]string, 0, 8)\n\ts = append(s, \"&libnetwork.PortConfig{\")\n\ts = append(s, \"Name: \"+fmt.Sprintf(\"%#v\", this.Name)+\",\\n\")\n\ts = append(s, \"Protocol: \"+fmt.Sprintf(\"%#v\", this.Protocol)+\",\\n\")\n\ts = append(s, \"TargetPort: \"+fmt.Sprintf(\"%#v\", this.TargetPort)+\",\\n\")\n\ts = append(s, \"PublishedPort: \"+fmt.Sprintf(\"%#v\", this.PublishedPort)+\",\\n\")\n\ts = append(s, \"}\")\n\treturn strings.Join(s, \"\")\n}\nfunc valueToGoStringAgent(v interface{}, typ string) string {\n\trv := reflect.ValueOf(v)\n\tif rv.IsNil() {\n\t\treturn \"nil\"\n\t}\n\tpv := reflect.Indirect(rv).Interface()\n\treturn fmt.Sprintf(\"func(v %v) *%v { return &v } ( %#v )\", typ, typ, pv)\n}\nfunc (m *EndpointRecord) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalTo(dAtA)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *EndpointRecord) MarshalTo(dAtA []byte) (int, error) {\n\tvar i int\n\t_ = i\n\tvar l int\n\t_ = l\n\tif len(m.Name) > 0 {\n\t\tdAtA[i] = 0xa\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.Name)))\n\t\ti += copy(dAtA[i:], m.Name)\n\t}\n\tif len(m.ServiceName) > 0 {\n\t\tdAtA[i] = 0x12\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.ServiceName)))\n\t\ti += copy(dAtA[i:], m.ServiceName)\n\t}\n\tif len(m.ServiceID) > 0 {\n\t\tdAtA[i] = 0x1a\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.ServiceID)))\n\t\ti += copy(dAtA[i:], m.ServiceID)\n\t}\n\tif len(m.VirtualIP) > 0 {\n\t\tdAtA[i] = 0x22\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.VirtualIP)))\n\t\ti += copy(dAtA[i:], m.VirtualIP)\n\t}\n\tif len(m.EndpointIP) > 0 {\n\t\tdAtA[i] = 0x2a\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.EndpointIP)))\n\t\ti += copy(dAtA[i:], m.EndpointIP)\n\t}\n\tif len(m.IngressPorts) > 0 {\n\t\tfor _, msg := range m.IngressPorts {\n\t\t\tdAtA[i] = 0x32\n\t\t\ti++\n\t\t\ti = encodeVarintAgent(dAtA, i, uint64(msg.Size()))\n\t\t\tn, err := msg.MarshalTo(dAtA[i:])\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\ti += n\n\t\t}\n\t}\n\tif len(m.Aliases) > 0 {\n\t\tfor _, s := range m.Aliases {\n\t\t\tdAtA[i] = 0x3a\n\t\t\ti++\n\t\t\tl = len(s)\n\t\t\tfor l >= 1<<7 {\n\t\t\t\tdAtA[i] = uint8(uint64(l)&0x7f | 0x80)\n\t\t\t\tl >>= 7\n\t\t\t\ti++\n\t\t\t}\n\t\t\tdAtA[i] = uint8(l)\n\t\t\ti++\n\t\t\ti += copy(dAtA[i:], s)\n\t\t}\n\t}\n\tif len(m.TaskAliases) > 0 {\n\t\tfor _, s := range m.TaskAliases {\n\t\t\tdAtA[i] = 0x42\n\t\t\ti++\n\t\t\tl = len(s)\n\t\t\tfor l >= 1<<7 {\n\t\t\t\tdAtA[i] = uint8(uint64(l)&0x7f | 0x80)\n\t\t\t\tl >>= 7\n\t\t\t\ti++\n\t\t\t}\n\t\t\tdAtA[i] = uint8(l)\n\t\t\ti++\n\t\t\ti += copy(dAtA[i:], s)\n\t\t}\n\t}\n\tif m.ServiceDisabled {\n\t\tdAtA[i] = 0x48\n\t\ti++\n\t\tif m.ServiceDisabled {\n\t\t\tdAtA[i] = 1\n\t\t} else {\n\t\t\tdAtA[i] = 0\n\t\t}\n\t\ti++\n\t}\n\treturn i, nil\n}\n\nfunc (m *PortConfig) Marshal() (dAtA []byte, err error) {\n\tsize := m.Size()\n\tdAtA = make([]byte, size)\n\tn, err := m.MarshalTo(dAtA)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dAtA[:n], nil\n}\n\nfunc (m *PortConfig) MarshalTo(dAtA []byte) (int, error) {\n\tvar i int\n\t_ = i\n\tvar l int\n\t_ = l\n\tif len(m.Name) > 0 {\n\t\tdAtA[i] = 0xa\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(len(m.Name)))\n\t\ti += copy(dAtA[i:], m.Name)\n\t}\n\tif m.Protocol != 0 {\n\t\tdAtA[i] = 0x10\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(m.Protocol))\n\t}\n\tif m.TargetPort != 0 {\n\t\tdAtA[i] = 0x18\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(m.TargetPort))\n\t}\n\tif m.PublishedPort != 0 {\n\t\tdAtA[i] = 0x20\n\t\ti++\n\t\ti = encodeVarintAgent(dAtA, i, uint64(m.PublishedPort))\n\t}\n\treturn i, nil\n}\n\nfunc encodeVarintAgent(dAtA []byte, offset int, v uint64) int {\n\tfor v >= 1<<7 {\n\t\tdAtA[offset] = uint8(v&0x7f | 0x80)\n\t\tv >>= 7\n\t\toffset++\n\t}\n\tdAtA[offset] = uint8(v)\n\treturn offset + 1\n}\nfunc (m *EndpointRecord) Size() (n int) {\n\tvar l int\n\t_ = l\n\tl = len(m.Name)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tl = len(m.ServiceName)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tl = len(m.ServiceID)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tl = len(m.VirtualIP)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tl = len(m.EndpointIP)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tif len(m.IngressPorts) > 0 {\n\t\tfor _, e := range m.IngressPorts {\n\t\t\tl = e.Size()\n\t\t\tn += 1 + l + sovAgent(uint64(l))\n\t\t}\n\t}\n\tif len(m.Aliases) > 0 {\n\t\tfor _, s := range m.Aliases {\n\t\t\tl = len(s)\n\t\t\tn += 1 + l + sovAgent(uint64(l))\n\t\t}\n\t}\n\tif len(m.TaskAliases) > 0 {\n\t\tfor _, s := range m.TaskAliases {\n\t\t\tl = len(s)\n\t\t\tn += 1 + l + sovAgent(uint64(l))\n\t\t}\n\t}\n\tif m.ServiceDisabled {\n\t\tn += 2\n\t}\n\treturn n\n}\n\nfunc (m *PortConfig) Size() (n int) {\n\tvar l int\n\t_ = l\n\tl = len(m.Name)\n\tif l > 0 {\n\t\tn += 1 + l + sovAgent(uint64(l))\n\t}\n\tif m.Protocol != 0 {\n\t\tn += 1 + sovAgent(uint64(m.Protocol))\n\t}\n\tif m.TargetPort != 0 {\n\t\tn += 1 + sovAgent(uint64(m.TargetPort))\n\t}\n\tif m.PublishedPort != 0 {\n\t\tn += 1 + sovAgent(uint64(m.PublishedPort))\n\t}\n\treturn n\n}\n\nfunc sovAgent(x uint64) (n int) {\n\tfor {\n\t\tn++\n\t\tx >>= 7\n\t\tif x == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn n\n}\nfunc sozAgent(x uint64) (n int) {\n\treturn sovAgent(uint64((x << 1) ^ uint64((int64(x) >> 63))))\n}\nfunc (this *EndpointRecord) String() string {\n\tif this == nil {\n\t\treturn \"nil\"\n\t}\n\ts := strings.Join([]string{`&EndpointRecord{`,\n\t\t`Name:` + fmt.Sprintf(\"%v\", this.Name) + `,`,\n\t\t`ServiceName:` + fmt.Sprintf(\"%v\", this.ServiceName) + `,`,\n\t\t`ServiceID:` + fmt.Sprintf(\"%v\", this.ServiceID) + `,`,\n\t\t`VirtualIP:` + fmt.Sprintf(\"%v\", this.VirtualIP) + `,`,\n\t\t`EndpointIP:` + fmt.Sprintf(\"%v\", this.EndpointIP) + `,`,\n\t\t`IngressPorts:` + strings.Replace(fmt.Sprintf(\"%v\", this.IngressPorts), \"PortConfig\", \"PortConfig\", 1) + `,`,\n\t\t`Aliases:` + fmt.Sprintf(\"%v\", this.Aliases) + `,`,\n\t\t`TaskAliases:` + fmt.Sprintf(\"%v\", this.TaskAliases) + `,`,\n\t\t`ServiceDisabled:` + fmt.Sprintf(\"%v\", this.ServiceDisabled) + `,`,\n\t\t`}`,\n\t}, \"\")\n\treturn s\n}\nfunc (this *PortConfig) String() string {\n\tif this == nil {\n\t\treturn \"nil\"\n\t}\n\ts := strings.Join([]string{`&PortConfig{`,\n\t\t`Name:` + fmt.Sprintf(\"%v\", this.Name) + `,`,\n\t\t`Protocol:` + fmt.Sprintf(\"%v\", this.Protocol) + `,`,\n\t\t`TargetPort:` + fmt.Sprintf(\"%v\", this.TargetPort) + `,`,\n\t\t`PublishedPort:` + fmt.Sprintf(\"%v\", this.PublishedPort) + `,`,\n\t\t`}`,\n\t}, \"\")\n\treturn s\n}\nfunc valueToStringAgent(v interface{}) string {\n\trv := reflect.ValueOf(v)\n\tif rv.IsNil() {\n\t\treturn \"nil\"\n\t}\n\tpv := reflect.Indirect(rv).Interface()\n\treturn fmt.Sprintf(\"*%v\", pv)\n}\nfunc (m *EndpointRecord) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= (uint64(b) & 0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: EndpointRecord: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: EndpointRecord: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Name\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Name = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 2:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field ServiceName\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.ServiceName = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 3:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field ServiceID\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.ServiceID = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 4:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field VirtualIP\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.VirtualIP = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 5:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field EndpointIP\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.EndpointIP = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 6:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field IngressPorts\", wireType)\n\t\t\t}\n\t\t\tvar msglen int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tmsglen |= (int(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif msglen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + msglen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.IngressPorts = append(m.IngressPorts, &PortConfig{})\n\t\t\tif err := m.IngressPorts[len(m.IngressPorts)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tiNdEx = postIndex\n\t\tcase 7:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Aliases\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Aliases = append(m.Aliases, string(dAtA[iNdEx:postIndex]))\n\t\t\tiNdEx = postIndex\n\t\tcase 8:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field TaskAliases\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.TaskAliases = append(m.TaskAliases, string(dAtA[iNdEx:postIndex]))\n\t\t\tiNdEx = postIndex\n\t\tcase 9:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field ServiceDisabled\", wireType)\n\t\t\t}\n\t\t\tvar v int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tv |= (int(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tm.ServiceDisabled = bool(v != 0)\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipAgent(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif skippy < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *PortConfig) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= (uint64(b) & 0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfieldNum := int32(wire >> 3)\n\t\twireType := int(wire & 0x7)\n\t\tif wireType == 4 {\n\t\t\treturn fmt.Errorf(\"proto: PortConfig: wiretype end group for non-group\")\n\t\t}\n\t\tif fieldNum <= 0 {\n\t\t\treturn fmt.Errorf(\"proto: PortConfig: illegal tag %d (wire type %d)\", fieldNum, wire)\n\t\t}\n\t\tswitch fieldNum {\n\t\tcase 1:\n\t\t\tif wireType != 2 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Name\", wireType)\n\t\t\t}\n\t\t\tvar stringLen uint64\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= (uint64(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Name = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tcase 2:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field Protocol\", wireType)\n\t\t\t}\n\t\t\tm.Protocol = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.Protocol |= (PortConfig_Protocol(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 3:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field TargetPort\", wireType)\n\t\t\t}\n\t\t\tm.TargetPort = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.TargetPort |= (uint32(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase 4:\n\t\t\tif wireType != 0 {\n\t\t\t\treturn fmt.Errorf(\"proto: wrong wireType = %d for field PublishedPort\", wireType)\n\t\t\t}\n\t\t\tm.PublishedPort = 0\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tm.PublishedPort |= (uint32(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipAgent(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif skippy < 0 {\n\t\t\t\treturn ErrInvalidLengthAgent\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc skipAgent(dAtA []byte) (n int, err error) {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn 0, ErrIntOverflowAgent\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= (uint64(b) & 0x7F) << shift\n\t\t\tif b < 0x80 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\twireType := int(wire & 0x7)\n\t\tswitch wireType {\n\t\tcase 0:\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn 0, ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tiNdEx++\n\t\t\t\tif dAtA[iNdEx-1] < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn iNdEx, nil\n\t\tcase 1:\n\t\t\tiNdEx += 8\n\t\t\treturn iNdEx, nil\n\t\tcase 2:\n\t\t\tvar length int\n\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\tif shift >= 64 {\n\t\t\t\t\treturn 0, ErrIntOverflowAgent\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tlength |= (int(b) & 0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tiNdEx += length\n\t\t\tif length < 0 {\n\t\t\t\treturn 0, ErrInvalidLengthAgent\n\t\t\t}\n\t\t\treturn iNdEx, nil\n\t\tcase 3:\n\t\t\tfor {\n\t\t\t\tvar innerWire uint64\n\t\t\t\tvar start int = iNdEx\n\t\t\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\t\t\tif shift >= 64 {\n\t\t\t\t\t\treturn 0, ErrIntOverflowAgent\n\t\t\t\t\t}\n\t\t\t\t\tif iNdEx >= l {\n\t\t\t\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t\t\t\t}\n\t\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\t\tiNdEx++\n\t\t\t\t\tinnerWire |= (uint64(b) & 0x7F) << shift\n\t\t\t\t\tif b < 0x80 {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tinnerWireType := int(innerWire & 0x7)\n\t\t\t\tif innerWireType == 4 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tnext, err := skipAgent(dAtA[start:])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\t\tiNdEx = start + next\n\t\t\t}\n\t\t\treturn iNdEx, nil\n\t\tcase 4:\n\t\t\treturn iNdEx, nil\n\t\tcase 5:\n\t\t\tiNdEx += 4\n\t\t\treturn iNdEx, nil\n\t\tdefault:\n\t\t\treturn 0, fmt.Errorf(\"proto: illegal wireType %d\", wireType)\n\t\t}\n\t}\n\tpanic(\"unreachable\")\n}\n\nvar (\n\tErrInvalidLengthAgent = fmt.Errorf(\"proto: negative length found during unmarshaling\")\n\tErrIntOverflowAgent   = fmt.Errorf(\"proto: integer overflow\")\n)\n\nfunc init() { proto.RegisterFile(\"agent.proto\", fileDescriptorAgent) }\n\nvar fileDescriptorAgent = []byte{\n\t// 459 bytes of a gzipped FileDescriptorProto\n\t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x91, 0x31, 0x6f, 0xd3, 0x4c,\n\t0x18, 0xc7, 0xe3, 0xc4, 0x6f, 0x1b, 0x3f, 0x4e, 0x52, 0xeb, 0xf4, 0x0a, 0x59, 0x1e, 0x1c, 0x13,\n\t0x09, 0x29, 0x48, 0x28, 0x95, 0xca, 0xd8, 0x89, 0x26, 0x0c, 0x5e, 0x90, 0x75, 0x4d, 0x59, 0x83,\n\t0x13, 0x1f, 0xe6, 0x54, 0xe3, 0xb3, 0xee, 0xae, 0x65, 0x65, 0x03, 0xf5, 0x3b, 0x74, 0xe2, 0xcb,\n\t0x30, 0x32, 0x32, 0x55, 0xd4, 0x9f, 0x80, 0x95, 0x0d, 0xdd, 0xf9, 0xae, 0x11, 0x52, 0xb7, 0xf3,\n\t0xef, 0xff, 0x3b, 0xeb, 0xb9, 0xff, 0x03, 0x7e, 0x5e, 0x92, 0x5a, 0x2e, 0x1a, 0xce, 0x24, 0x43,\n\t0x50, 0xd1, 0x6d, 0x4d, 0xe4, 0x27, 0xc6, 0x2f, 0xa3, 0xff, 0x4b, 0x56, 0x32, 0x8d, 0x8f, 0xd5,\n\t0xa9, 0x33, 0x66, 0x7f, 0xfa, 0x30, 0x79, 0x5d, 0x17, 0x0d, 0xa3, 0xb5, 0xc4, 0x64, 0xc7, 0x78,\n\t0x81, 0x10, 0xb8, 0x75, 0xfe, 0x91, 0x84, 0x4e, 0xe2, 0xcc, 0x3d, 0xac, 0xcf, 0xe8, 0x29, 0x8c,\n\t0x04, 0xe1, 0xd7, 0x74, 0x47, 0x36, 0x3a, 0xeb, 0xeb, 0xcc, 0x37, 0xec, 0x8d, 0x52, 0x5e, 0x00,\n\t0x58, 0x85, 0x16, 0xe1, 0x40, 0x09, 0x67, 0xe3, 0xf6, 0x6e, 0xea, 0x9d, 0x77, 0x34, 0x5d, 0x61,\n\t0xcf, 0x08, 0x69, 0xa1, 0xec, 0x6b, 0xca, 0xe5, 0x55, 0x5e, 0x6d, 0x68, 0x13, 0xba, 0x7b, 0xfb,\n\t0x6d, 0x47, 0xd3, 0x0c, 0x7b, 0x46, 0x48, 0x1b, 0x74, 0x0c, 0x3e, 0x31, 0x43, 0x2a, 0xfd, 0x3f,\n\t0xad, 0x4f, 0xda, 0xbb, 0x29, 0xd8, 0xd9, 0xd3, 0x0c, 0x83, 0x55, 0xd2, 0x06, 0x9d, 0xc2, 0x98,\n\t0xd6, 0x25, 0x27, 0x42, 0x6c, 0x1a, 0xc6, 0xa5, 0x08, 0x0f, 0x92, 0xc1, 0xdc, 0x3f, 0x79, 0xb2,\n\t0xd8, 0x17, 0xb2, 0xc8, 0x18, 0x97, 0x4b, 0x56, 0xbf, 0xa7, 0x25, 0x1e, 0x19, 0x59, 0x21, 0x81,\n\t0x42, 0x38, 0xcc, 0x2b, 0x9a, 0x0b, 0x22, 0xc2, 0xc3, 0x64, 0x30, 0xf7, 0xb0, 0xfd, 0x54, 0x35,\n\t0xc8, 0x5c, 0x5c, 0x6e, 0x6c, 0x3c, 0xd4, 0xb1, 0xaf, 0xd8, 0x2b, 0xa3, 0x3c, 0x87, 0xc0, 0xd6,\n\t0x50, 0x50, 0x91, 0x6f, 0x2b, 0x52, 0x84, 0x5e, 0xe2, 0xcc, 0x87, 0xf8, 0xc8, 0xf0, 0x95, 0xc1,\n\t0xb3, 0x2f, 0x7d, 0x80, 0xfd, 0x10, 0x8f, 0xf6, 0x7e, 0x0a, 0x43, 0xbd, 0xa7, 0x1d, 0xab, 0x74,\n\t0xe7, 0x93, 0x93, 0xe9, 0xe3, 0x4f, 0x58, 0x64, 0x46, 0xc3, 0x0f, 0x17, 0xd0, 0x14, 0x7c, 0x99,\n\t0xf3, 0x92, 0x48, 0xdd, 0x81, 0x5e, 0xc9, 0x18, 0x43, 0x87, 0xd4, 0x4d, 0xf4, 0x0c, 0x26, 0xcd,\n\t0xd5, 0xb6, 0xa2, 0xe2, 0x03, 0x29, 0x3a, 0xc7, 0xd5, 0xce, 0xf8, 0x81, 0x2a, 0x6d, 0xf6, 0x0e,\n\t0x86, 0xf6, 0xef, 0x28, 0x84, 0xc1, 0x7a, 0x99, 0x05, 0xbd, 0xe8, 0xe8, 0xe6, 0x36, 0xf1, 0x2d,\n\t0x5e, 0x2f, 0x33, 0x95, 0x5c, 0xac, 0xb2, 0xc0, 0xf9, 0x37, 0xb9, 0x58, 0x65, 0x28, 0x02, 0xf7,\n\t0x7c, 0xb9, 0xce, 0x82, 0x7e, 0x14, 0xdc, 0xdc, 0x26, 0x23, 0x1b, 0x29, 0x16, 0xb9, 0x5f, 0xbf,\n\t0xc5, 0xbd, 0xb3, 0xf0, 0xe7, 0x7d, 0xdc, 0xfb, 0x7d, 0x1f, 0x3b, 0x9f, 0xdb, 0xd8, 0xf9, 0xde,\n\t0xc6, 0xce, 0x8f, 0x36, 0x76, 0x7e, 0xb5, 0xb1, 0xb3, 0x3d, 0xd0, 0xaf, 0x79, 0xf9, 0x37, 0x00,\n\t0x00, 0xff, 0xff, 0x55, 0x29, 0x75, 0x5c, 0xd7, 0x02, 0x00, 0x00,\n}\n"
        },
        {
          "name": "agent.proto",
          "type": "blob",
          "size": 2.505859375,
          "content": "syntax = \"proto3\";\n\nimport \"gogoproto/gogo.proto\";\n\npackage libnetwork;\n\noption (gogoproto.marshaler_all) = true;\noption (gogoproto.unmarshaler_all) = true;\noption (gogoproto.stringer_all) = true;\noption (gogoproto.gostring_all) = true;\noption (gogoproto.sizer_all) = true;\noption (gogoproto.goproto_stringer_all) = false;\n\n// EndpointRecord specifies all the endpoint specific information that\n// needs to gossiped to nodes participating in the network.\nmessage EndpointRecord {\n\t// Name of the container\n\tstring name = 1;\n\n\t// Service name of the service to which this endpoint belongs.\n\tstring service_name = 2;\n\n\t// Service ID of the service to which this endpoint belongs.\n\tstring service_id = 3 [(gogoproto.customname) = \"ServiceID\"];\n\n\t// Virtual IP of the service to which this endpoint belongs.\n\tstring virtual_ip = 4 [(gogoproto.customname) = \"VirtualIP\"];\n\n\t// IP assigned to this endpoint.\n\tstring endpoint_ip = 5 [(gogoproto.customname) = \"EndpointIP\"];\n\n\t// IngressPorts exposed by the service to which this endpoint belongs.\n\trepeated PortConfig ingress_ports = 6;\n\n\t// A list of aliases which are alternate names for the service\n\trepeated string aliases = 7;\n\n\t// List of aliases task specific aliases\n\trepeated string task_aliases = 8;\n\n\t// Whether this enpoint's service has been disabled\n\tbool service_disabled = 9;\n}\n\n// PortConfig specifies an exposed port which can be\n// addressed using the given name. This can be later queried\n// using a service discovery api or a DNS SRV query. The node\n// port specifies a port that can be used to address this\n// service external to the cluster by sending a connection\n// request to this port to any node on the cluster.\nmessage PortConfig {\n\tenum Protocol {\n\t\toption (gogoproto.goproto_enum_prefix) = false;\n\n\t\tTCP = 0 [(gogoproto.enumvalue_customname) = \"ProtocolTCP\"];\n\t\tUDP = 1 [(gogoproto.enumvalue_customname) = \"ProtocolUDP\"];\n\t\tSCTP = 2 [(gogoproto.enumvalue_customname) = \"ProtocolSCTP\"];\n\t}\n\n\t// Name for the port. If provided the port information can\n\t// be queried using the name as in a DNS SRV query.\n\tstring name = 1;\n\n\t// Protocol for the port which is exposed.\n\tProtocol protocol = 2;\n\n\t// The port which the application is exposing and is bound to.\n\tuint32 target_port = 3;\n\n\t// PublishedPort specifies the port on which the service is\n\t// exposed on all nodes on the cluster. If not specified an\n\t// arbitrary port in the node port range is allocated by the\n\t// system. If specified it should be within the node port\n\t// range and it should be available.\n\tuint32 published_port = 4;\n}\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "bitseq",
          "type": "tree",
          "content": null
        },
        {
          "name": "client",
          "type": "tree",
          "content": null
        },
        {
          "name": "cluster",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "controller.go",
          "type": "blob",
          "size": 36.72265625,
          "content": "/*\nPackage libnetwork provides the basic functionality and extension points to\ncreate network namespaces and allocate interfaces for containers to use.\n\n\tnetworkType := \"bridge\"\n\n\t// Create a new controller instance\n\tdriverOptions := options.Generic{}\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = driverOptions\n\tcontroller, err := libnetwork.New(config.OptionDriverConfig(networkType, genericOption))\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// Create a network for containers to join.\n\t// NewNetwork accepts Variadic optional arguments that libnetwork and Drivers can make use of\n\tnetwork, err := controller.NewNetwork(networkType, \"network1\", \"\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// For each new container: allocate IP and interfaces. The returned network\n\t// settings will be used for container infos (inspect and such), as well as\n\t// iptables rules for port publishing. This info is contained or accessible\n\t// from the returned endpoint.\n\tep, err := network.CreateEndpoint(\"Endpoint1\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// Create the sandbox for the container.\n\t// NewSandbox accepts Variadic optional arguments which libnetwork can use.\n\tsbx, err := controller.NewSandbox(\"container1\",\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"))\n\n\t// A sandbox can join the endpoint via the join api.\n\terr = ep.Join(sbx)\n\tif err != nil {\n\t\treturn\n\t}\n*/\npackage libnetwork\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/pkg/discovery\"\n\t\"github.com/docker/docker/pkg/plugingetter\"\n\t\"github.com/docker/docker/pkg/plugins\"\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/libnetwork/cluster\"\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/diagnostic\"\n\t\"github.com/docker/libnetwork/discoverapi\"\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/drvregistry\"\n\t\"github.com/docker/libnetwork/hostdiscovery\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/moby/locker\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// NetworkController provides the interface for controller instance which manages\n// networks.\ntype NetworkController interface {\n\t// ID provides a unique identity for the controller\n\tID() string\n\n\t// BuiltinDrivers returns list of builtin drivers\n\tBuiltinDrivers() []string\n\n\t// BuiltinIPAMDrivers returns list of builtin ipam drivers\n\tBuiltinIPAMDrivers() []string\n\n\t// Config method returns the bootup configuration for the controller\n\tConfig() config.Config\n\n\t// Create a new network. The options parameter carries network specific options.\n\tNewNetwork(networkType, name string, id string, options ...NetworkOption) (Network, error)\n\n\t// Networks returns the list of Network(s) managed by this controller.\n\tNetworks() []Network\n\n\t// WalkNetworks uses the provided function to walk the Network(s) managed by this controller.\n\tWalkNetworks(walker NetworkWalker)\n\n\t// NetworkByName returns the Network which has the passed name. If not found, the error ErrNoSuchNetwork is returned.\n\tNetworkByName(name string) (Network, error)\n\n\t// NetworkByID returns the Network which has the passed id. If not found, the error ErrNoSuchNetwork is returned.\n\tNetworkByID(id string) (Network, error)\n\n\t// NewSandbox creates a new network sandbox for the passed container id\n\tNewSandbox(containerID string, options ...SandboxOption) (Sandbox, error)\n\n\t// Sandboxes returns the list of Sandbox(s) managed by this controller.\n\tSandboxes() []Sandbox\n\n\t// WalkSandboxes uses the provided function to walk the Sandbox(s) managed by this controller.\n\tWalkSandboxes(walker SandboxWalker)\n\n\t// SandboxByID returns the Sandbox which has the passed id. If not found, a types.NotFoundError is returned.\n\tSandboxByID(id string) (Sandbox, error)\n\n\t// SandboxDestroy destroys a sandbox given a container ID\n\tSandboxDestroy(id string) error\n\n\t// Stop network controller\n\tStop()\n\n\t// ReloadConfiguration updates the controller configuration\n\tReloadConfiguration(cfgOptions ...config.Option) error\n\n\t// SetClusterProvider sets cluster provider\n\tSetClusterProvider(provider cluster.Provider)\n\n\t// Wait for agent initialization complete in libnetwork controller\n\tAgentInitWait()\n\n\t// Wait for agent to stop if running\n\tAgentStopWait()\n\n\t// SetKeys configures the encryption key for gossip and overlay data path\n\tSetKeys(keys []*types.EncryptionKey) error\n\n\t// StartDiagnostic start the network diagnostic mode\n\tStartDiagnostic(port int)\n\t// StopDiagnostic start the network diagnostic mode\n\tStopDiagnostic()\n\t// IsDiagnosticEnabled returns true if the diagnostic is enabled\n\tIsDiagnosticEnabled() bool\n}\n\n// NetworkWalker is a client provided function which will be used to walk the Networks.\n// When the function returns true, the walk will stop.\ntype NetworkWalker func(nw Network) bool\n\n// SandboxWalker is a client provided function which will be used to walk the Sandboxes.\n// When the function returns true, the walk will stop.\ntype SandboxWalker func(sb Sandbox) bool\n\ntype sandboxTable map[string]*sandbox\n\ntype controller struct {\n\tid                     string\n\tdrvRegistry            *drvregistry.DrvRegistry\n\tsandboxes              sandboxTable\n\tcfg                    *config.Config\n\tstores                 []datastore.DataStore\n\tdiscovery              hostdiscovery.HostDiscovery\n\textKeyListener         net.Listener\n\twatchCh                chan *endpoint\n\tunWatchCh              chan *endpoint\n\tsvcRecords             map[string]svcInfo\n\tnmap                   map[string]*netWatch\n\tserviceBindings        map[serviceKey]*service\n\tdefOsSbox              osl.Sandbox\n\tingressSandbox         *sandbox\n\tsboxOnce               sync.Once\n\tagent                  *agent\n\tnetworkLocker          *locker.Locker\n\tagentInitDone          chan struct{}\n\tagentStopDone          chan struct{}\n\tkeys                   []*types.EncryptionKey\n\tclusterConfigAvailable bool\n\tDiagnosticServer       *diagnostic.Server\n\tsync.Mutex\n}\n\ntype initializer struct {\n\tfn    drvregistry.InitFunc\n\tntype string\n}\n\n// New creates a new instance of network controller.\nfunc New(cfgOptions ...config.Option) (NetworkController, error) {\n\tc := &controller{\n\t\tid:               stringid.GenerateRandomID(),\n\t\tcfg:              config.ParseConfigOptions(cfgOptions...),\n\t\tsandboxes:        sandboxTable{},\n\t\tsvcRecords:       make(map[string]svcInfo),\n\t\tserviceBindings:  make(map[serviceKey]*service),\n\t\tagentInitDone:    make(chan struct{}),\n\t\tnetworkLocker:    locker.New(),\n\t\tDiagnosticServer: diagnostic.New(),\n\t}\n\tc.DiagnosticServer.Init()\n\n\tif err := c.initStores(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdrvRegistry, err := drvregistry.New(c.getStore(datastore.LocalScope), c.getStore(datastore.GlobalScope), c.RegisterDriver, nil, c.cfg.PluginGetter)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, i := range getInitializers(c.cfg.Daemon.Experimental) {\n\t\tvar dcfg map[string]interface{}\n\n\t\t// External plugins don't need config passed through daemon. They can\n\t\t// bootstrap themselves\n\t\tif i.ntype != \"remote\" {\n\t\t\tdcfg = c.makeDriverConfig(i.ntype)\n\t\t}\n\n\t\tif err := drvRegistry.AddDriver(i.ntype, i.fn, dcfg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif err = initIPAMDrivers(drvRegistry, nil, c.getStore(datastore.GlobalScope), c.cfg.Daemon.DefaultAddressPool); err != nil {\n\t\treturn nil, err\n\t}\n\n\tc.drvRegistry = drvRegistry\n\n\tif c.cfg != nil && c.cfg.Cluster.Watcher != nil {\n\t\tif err := c.initDiscovery(c.cfg.Cluster.Watcher); err != nil {\n\t\t\t// Failing to initialize discovery is a bad situation to be in.\n\t\t\t// But it cannot fail creating the Controller\n\t\t\tlogrus.Errorf(\"Failed to Initialize Discovery : %v\", err)\n\t\t}\n\t}\n\n\tc.WalkNetworks(populateSpecial)\n\n\t// Reserve pools first before doing cleanup. Otherwise the\n\t// cleanups of endpoint/network and sandbox below will\n\t// generate many unnecessary warnings\n\tc.reservePools()\n\n\t// Cleanup resources\n\tc.sandboxCleanup(c.cfg.ActiveSandboxes)\n\tc.cleanupLocalEndpoints()\n\tc.networkCleanup()\n\n\tif err := c.startExternalKeyListener(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsetupArrangeUserFilterRule(c)\n\treturn c, nil\n}\n\nfunc (c *controller) SetClusterProvider(provider cluster.Provider) {\n\tvar sameProvider bool\n\tc.Lock()\n\t// Avoids to spawn multiple goroutine for the same cluster provider\n\tif c.cfg.Daemon.ClusterProvider == provider {\n\t\t// If the cluster provider is already set, there is already a go routine spawned\n\t\t// that is listening for events, so nothing to do here\n\t\tsameProvider = true\n\t} else {\n\t\tc.cfg.Daemon.ClusterProvider = provider\n\t}\n\tc.Unlock()\n\n\tif provider == nil || sameProvider {\n\t\treturn\n\t}\n\t// We don't want to spawn a new go routine if the previous one did not exit yet\n\tc.AgentStopWait()\n\tgo c.clusterAgentInit()\n}\n\nfunc isValidClusteringIP(addr string) bool {\n\treturn addr != \"\" && !net.ParseIP(addr).IsLoopback() && !net.ParseIP(addr).IsUnspecified()\n}\n\n// libnetwork side of agent depends on the keys. On the first receipt of\n// keys setup the agent. For subsequent key set handle the key change\nfunc (c *controller) SetKeys(keys []*types.EncryptionKey) error {\n\tsubsysKeys := make(map[string]int)\n\tfor _, key := range keys {\n\t\tif key.Subsystem != subsysGossip &&\n\t\t\tkey.Subsystem != subsysIPSec {\n\t\t\treturn fmt.Errorf(\"key received for unrecognized subsystem\")\n\t\t}\n\t\tsubsysKeys[key.Subsystem]++\n\t}\n\tfor s, count := range subsysKeys {\n\t\tif count != keyringSize {\n\t\t\treturn fmt.Errorf(\"incorrect number of keys for subsystem %v\", s)\n\t\t}\n\t}\n\n\tagent := c.getAgent()\n\n\tif agent == nil {\n\t\tc.Lock()\n\t\tc.keys = keys\n\t\tc.Unlock()\n\t\treturn nil\n\t}\n\treturn c.handleKeyChange(keys)\n}\n\nfunc (c *controller) getAgent() *agent {\n\tc.Lock()\n\tdefer c.Unlock()\n\treturn c.agent\n}\n\nfunc (c *controller) clusterAgentInit() {\n\tclusterProvider := c.cfg.Daemon.ClusterProvider\n\tvar keysAvailable bool\n\tfor {\n\t\teventType := <-clusterProvider.ListenClusterEvents()\n\t\t// The events: EventSocketChange, EventNodeReady and EventNetworkKeysAvailable are not ordered\n\t\t// when all the condition for the agent initialization are met then proceed with it\n\t\tswitch eventType {\n\t\tcase cluster.EventNetworkKeysAvailable:\n\t\t\t// Validates that the keys are actually available before starting the initialization\n\t\t\t// This will handle old spurious messages left on the channel\n\t\t\tc.Lock()\n\t\t\tkeysAvailable = c.keys != nil\n\t\t\tc.Unlock()\n\t\t\tfallthrough\n\t\tcase cluster.EventSocketChange, cluster.EventNodeReady:\n\t\t\tif keysAvailable && !c.isDistributedControl() {\n\t\t\t\tc.agentOperationStart()\n\t\t\t\tif err := c.agentSetup(clusterProvider); err != nil {\n\t\t\t\t\tc.agentStopComplete()\n\t\t\t\t} else {\n\t\t\t\t\tc.agentInitComplete()\n\t\t\t\t}\n\t\t\t}\n\t\tcase cluster.EventNodeLeave:\n\t\t\tc.agentOperationStart()\n\t\t\tc.Lock()\n\t\t\tc.keys = nil\n\t\t\tc.Unlock()\n\n\t\t\t// We are leaving the cluster. Make sure we\n\t\t\t// close the gossip so that we stop all\n\t\t\t// incoming gossip updates before cleaning up\n\t\t\t// any remaining service bindings. But before\n\t\t\t// deleting the networks since the networks\n\t\t\t// should still be present when cleaning up\n\t\t\t// service bindings\n\t\t\tc.agentClose()\n\t\t\tc.cleanupServiceDiscovery(\"\")\n\t\t\tc.cleanupServiceBindings(\"\")\n\n\t\t\tc.agentStopComplete()\n\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// AgentInitWait waits for agent initialization to be completed in the controller.\nfunc (c *controller) AgentInitWait() {\n\tc.Lock()\n\tagentInitDone := c.agentInitDone\n\tc.Unlock()\n\n\tif agentInitDone != nil {\n\t\t<-agentInitDone\n\t}\n}\n\n// AgentStopWait waits for the Agent stop to be completed in the controller\nfunc (c *controller) AgentStopWait() {\n\tc.Lock()\n\tagentStopDone := c.agentStopDone\n\tc.Unlock()\n\tif agentStopDone != nil {\n\t\t<-agentStopDone\n\t}\n}\n\n// agentOperationStart marks the start of an Agent Init or Agent Stop\nfunc (c *controller) agentOperationStart() {\n\tc.Lock()\n\tif c.agentInitDone == nil {\n\t\tc.agentInitDone = make(chan struct{})\n\t}\n\tif c.agentStopDone == nil {\n\t\tc.agentStopDone = make(chan struct{})\n\t}\n\tc.Unlock()\n}\n\n// agentInitComplete notifies the successful completion of the Agent initialization\nfunc (c *controller) agentInitComplete() {\n\tc.Lock()\n\tif c.agentInitDone != nil {\n\t\tclose(c.agentInitDone)\n\t\tc.agentInitDone = nil\n\t}\n\tc.Unlock()\n}\n\n// agentStopComplete notifies the successful completion of the Agent stop\nfunc (c *controller) agentStopComplete() {\n\tc.Lock()\n\tif c.agentStopDone != nil {\n\t\tclose(c.agentStopDone)\n\t\tc.agentStopDone = nil\n\t}\n\tc.Unlock()\n}\n\nfunc (c *controller) makeDriverConfig(ntype string) map[string]interface{} {\n\tif c.cfg == nil {\n\t\treturn nil\n\t}\n\n\tconfig := make(map[string]interface{})\n\n\tfor _, label := range c.cfg.Daemon.Labels {\n\t\tif !strings.HasPrefix(netlabel.Key(label), netlabel.DriverPrefix+\".\"+ntype) {\n\t\t\tcontinue\n\t\t}\n\n\t\tconfig[netlabel.Key(label)] = netlabel.Value(label)\n\t}\n\n\tdrvCfg, ok := c.cfg.Daemon.DriverCfg[ntype]\n\tif ok {\n\t\tfor k, v := range drvCfg.(map[string]interface{}) {\n\t\t\tconfig[k] = v\n\t\t}\n\t}\n\n\tfor k, v := range c.cfg.Scopes {\n\t\tif !v.IsValid() {\n\t\t\tcontinue\n\t\t}\n\t\tconfig[netlabel.MakeKVClient(k)] = discoverapi.DatastoreConfigData{\n\t\t\tScope:    k,\n\t\t\tProvider: v.Client.Provider,\n\t\t\tAddress:  v.Client.Address,\n\t\t\tConfig:   v.Client.Config,\n\t\t}\n\t}\n\n\treturn config\n}\n\nvar procReloadConfig = make(chan (bool), 1)\n\nfunc (c *controller) ReloadConfiguration(cfgOptions ...config.Option) error {\n\tprocReloadConfig <- true\n\tdefer func() { <-procReloadConfig }()\n\n\t// For now we accept the configuration reload only as a mean to provide a global store config after boot.\n\t// Refuse the configuration if it alters an existing datastore client configuration.\n\tupdate := false\n\tcfg := config.ParseConfigOptions(cfgOptions...)\n\n\tfor s := range c.cfg.Scopes {\n\t\tif _, ok := cfg.Scopes[s]; !ok {\n\t\t\treturn types.ForbiddenErrorf(\"cannot accept new configuration because it removes an existing datastore client\")\n\t\t}\n\t}\n\tfor s, nSCfg := range cfg.Scopes {\n\t\tif eSCfg, ok := c.cfg.Scopes[s]; ok {\n\t\t\tif eSCfg.Client.Provider != nSCfg.Client.Provider ||\n\t\t\t\teSCfg.Client.Address != nSCfg.Client.Address {\n\t\t\t\treturn types.ForbiddenErrorf(\"cannot accept new configuration because it modifies an existing datastore client\")\n\t\t\t}\n\t\t} else {\n\t\t\tif err := c.initScopedStore(s, nSCfg); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tupdate = true\n\t\t}\n\t}\n\tif !update {\n\t\treturn nil\n\t}\n\n\tc.Lock()\n\tc.cfg = cfg\n\tc.Unlock()\n\n\tvar dsConfig *discoverapi.DatastoreConfigData\n\tfor scope, sCfg := range cfg.Scopes {\n\t\tif scope == datastore.LocalScope || !sCfg.IsValid() {\n\t\t\tcontinue\n\t\t}\n\t\tdsConfig = &discoverapi.DatastoreConfigData{\n\t\t\tScope:    scope,\n\t\t\tProvider: sCfg.Client.Provider,\n\t\t\tAddress:  sCfg.Client.Address,\n\t\t\tConfig:   sCfg.Client.Config,\n\t\t}\n\t\tbreak\n\t}\n\tif dsConfig == nil {\n\t\treturn nil\n\t}\n\n\tc.drvRegistry.WalkIPAMs(func(name string, driver ipamapi.Ipam, cap *ipamapi.Capability) bool {\n\t\terr := driver.DiscoverNew(discoverapi.DatastoreConfig, *dsConfig)\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to set datastore in driver %s: %v\", name, err)\n\t\t}\n\t\treturn false\n\t})\n\n\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\terr := driver.DiscoverNew(discoverapi.DatastoreConfig, *dsConfig)\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to set datastore in driver %s: %v\", name, err)\n\t\t}\n\t\treturn false\n\t})\n\n\tif c.discovery == nil && c.cfg.Cluster.Watcher != nil {\n\t\tif err := c.initDiscovery(c.cfg.Cluster.Watcher); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to Initialize Discovery after configuration update: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) ID() string {\n\treturn c.id\n}\n\nfunc (c *controller) BuiltinDrivers() []string {\n\tdrivers := []string{}\n\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\tif driver.IsBuiltIn() {\n\t\t\tdrivers = append(drivers, name)\n\t\t}\n\t\treturn false\n\t})\n\treturn drivers\n}\n\nfunc (c *controller) BuiltinIPAMDrivers() []string {\n\tdrivers := []string{}\n\tc.drvRegistry.WalkIPAMs(func(name string, driver ipamapi.Ipam, cap *ipamapi.Capability) bool {\n\t\tif driver.IsBuiltIn() {\n\t\t\tdrivers = append(drivers, name)\n\t\t}\n\t\treturn false\n\t})\n\treturn drivers\n}\n\nfunc (c *controller) validateHostDiscoveryConfig() bool {\n\tif c.cfg == nil || c.cfg.Cluster.Discovery == \"\" || c.cfg.Cluster.Address == \"\" {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (c *controller) clusterHostID() string {\n\tc.Lock()\n\tdefer c.Unlock()\n\tif c.cfg == nil || c.cfg.Cluster.Address == \"\" {\n\t\treturn \"\"\n\t}\n\taddr := strings.Split(c.cfg.Cluster.Address, \":\")\n\treturn addr[0]\n}\n\nfunc (c *controller) isNodeAlive(node string) bool {\n\tif c.discovery == nil {\n\t\treturn false\n\t}\n\n\tnodes := c.discovery.Fetch()\n\tfor _, n := range nodes {\n\t\tif n.String() == node {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (c *controller) initDiscovery(watcher discovery.Watcher) error {\n\tif c.cfg == nil {\n\t\treturn fmt.Errorf(\"discovery initialization requires a valid configuration\")\n\t}\n\n\tc.discovery = hostdiscovery.NewHostDiscovery(watcher)\n\treturn c.discovery.Watch(c.activeCallback, c.hostJoinCallback, c.hostLeaveCallback)\n}\n\nfunc (c *controller) activeCallback() {\n\tds := c.getStore(datastore.GlobalScope)\n\tif ds != nil && !ds.Active() {\n\t\tds.RestartWatch()\n\t}\n}\n\nfunc (c *controller) hostJoinCallback(nodes []net.IP) {\n\tc.processNodeDiscovery(nodes, true)\n}\n\nfunc (c *controller) hostLeaveCallback(nodes []net.IP) {\n\tc.processNodeDiscovery(nodes, false)\n}\n\nfunc (c *controller) processNodeDiscovery(nodes []net.IP, add bool) {\n\tc.drvRegistry.WalkDrivers(func(name string, driver driverapi.Driver, capability driverapi.Capability) bool {\n\t\tc.pushNodeDiscovery(driver, capability, nodes, add)\n\t\treturn false\n\t})\n}\n\nfunc (c *controller) pushNodeDiscovery(d driverapi.Driver, cap driverapi.Capability, nodes []net.IP, add bool) {\n\tvar self net.IP\n\tif c.cfg != nil {\n\t\taddr := strings.Split(c.cfg.Cluster.Address, \":\")\n\t\tself = net.ParseIP(addr[0])\n\t\t// if external kvstore is not configured, try swarm-mode config\n\t\tif self == nil {\n\t\t\tif agent := c.getAgent(); agent != nil {\n\t\t\t\tself = net.ParseIP(agent.advertiseAddr)\n\t\t\t}\n\t\t}\n\t}\n\n\tif d == nil || cap.ConnectivityScope != datastore.GlobalScope || nodes == nil {\n\t\treturn\n\t}\n\n\tfor _, node := range nodes {\n\t\tnodeData := discoverapi.NodeDiscoveryData{Address: node.String(), Self: node.Equal(self)}\n\t\tvar err error\n\t\tif add {\n\t\t\terr = d.DiscoverNew(discoverapi.NodeDiscovery, nodeData)\n\t\t} else {\n\t\t\terr = d.DiscoverDelete(discoverapi.NodeDiscovery, nodeData)\n\t\t}\n\t\tif err != nil {\n\t\t\tlogrus.Debugf(\"discovery notification error: %v\", err)\n\t\t}\n\t}\n}\n\nfunc (c *controller) Config() config.Config {\n\tc.Lock()\n\tdefer c.Unlock()\n\tif c.cfg == nil {\n\t\treturn config.Config{}\n\t}\n\treturn *c.cfg\n}\n\nfunc (c *controller) isManager() bool {\n\tc.Lock()\n\tdefer c.Unlock()\n\tif c.cfg == nil || c.cfg.Daemon.ClusterProvider == nil {\n\t\treturn false\n\t}\n\treturn c.cfg.Daemon.ClusterProvider.IsManager()\n}\n\nfunc (c *controller) isAgent() bool {\n\tc.Lock()\n\tdefer c.Unlock()\n\tif c.cfg == nil || c.cfg.Daemon.ClusterProvider == nil {\n\t\treturn false\n\t}\n\treturn c.cfg.Daemon.ClusterProvider.IsAgent()\n}\n\nfunc (c *controller) isDistributedControl() bool {\n\treturn !c.isManager() && !c.isAgent()\n}\n\nfunc (c *controller) GetPluginGetter() plugingetter.PluginGetter {\n\treturn c.drvRegistry.GetPluginGetter()\n}\n\nfunc (c *controller) RegisterDriver(networkType string, driver driverapi.Driver, capability driverapi.Capability) error {\n\tc.Lock()\n\thd := c.discovery\n\tc.Unlock()\n\n\tif hd != nil {\n\t\tc.pushNodeDiscovery(driver, capability, hd.Fetch(), true)\n\t}\n\n\tc.agentDriverNotify(driver)\n\treturn nil\n}\n\n// XXX  This should be made driver agnostic.  See comment below.\nconst overlayDSROptionString = \"dsr\"\n\n// NewNetwork creates a new network of the specified network type. The options\n// are network specific and modeled in a generic way.\nfunc (c *controller) NewNetwork(networkType, name string, id string, options ...NetworkOption) (Network, error) {\n\tvar (\n\t\tcap            *driverapi.Capability\n\t\terr            error\n\t\tt              *network\n\t\tskipCfgEpCount bool\n\t)\n\n\tif id != \"\" {\n\t\tc.networkLocker.Lock(id)\n\t\tdefer c.networkLocker.Unlock(id)\n\n\t\tif _, err = c.NetworkByID(id); err == nil {\n\t\t\treturn nil, NetworkNameError(id)\n\t\t}\n\t}\n\n\tif !config.IsValidName(name) {\n\t\treturn nil, ErrInvalidName(name)\n\t}\n\n\tif id == \"\" {\n\t\tid = stringid.GenerateRandomID()\n\t}\n\n\tdefaultIpam := defaultIpamForNetworkType(networkType)\n\t// Construct the network object\n\tnetwork := &network{\n\t\tname:             name,\n\t\tnetworkType:      networkType,\n\t\tgeneric:          map[string]interface{}{netlabel.GenericData: make(map[string]string)},\n\t\tipamType:         defaultIpam,\n\t\tid:               id,\n\t\tcreated:          time.Now(),\n\t\tctrlr:            c,\n\t\tpersist:          true,\n\t\tdrvOnce:          &sync.Once{},\n\t\tloadBalancerMode: loadBalancerModeDefault,\n\t}\n\n\tnetwork.processOptions(options...)\n\tif err = network.validateConfiguration(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Reset network types, force local scope and skip allocation and\n\t// plumbing for configuration networks. Reset of the config-only\n\t// network drivers is needed so that this special network is not\n\t// usable by old engine versions.\n\tif network.configOnly {\n\t\tnetwork.scope = datastore.LocalScope\n\t\tnetwork.networkType = \"null\"\n\t\tgoto addToStore\n\t}\n\n\t_, cap, err = network.resolveDriver(network.networkType, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif network.scope == datastore.LocalScope && cap.DataScope == datastore.GlobalScope {\n\t\treturn nil, types.ForbiddenErrorf(\"cannot downgrade network scope for %s networks\", networkType)\n\n\t}\n\tif network.ingress && cap.DataScope != datastore.GlobalScope {\n\t\treturn nil, types.ForbiddenErrorf(\"Ingress network can only be global scope network\")\n\t}\n\n\t// At this point the network scope is still unknown if not set by user\n\tif (cap.DataScope == datastore.GlobalScope || network.scope == datastore.SwarmScope) &&\n\t\t!c.isDistributedControl() && !network.dynamic {\n\t\tif c.isManager() {\n\t\t\t// For non-distributed controlled environment, globalscoped non-dynamic networks are redirected to Manager\n\t\t\treturn nil, ManagerRedirectError(name)\n\t\t}\n\t\treturn nil, types.ForbiddenErrorf(\"Cannot create a multi-host network from a worker node. Please create the network from a manager node.\")\n\t}\n\n\tif network.scope == datastore.SwarmScope && c.isDistributedControl() {\n\t\treturn nil, types.ForbiddenErrorf(\"cannot create a swarm scoped network when swarm is not active\")\n\t}\n\n\t// Make sure we have a driver available for this network type\n\t// before we allocate anything.\n\tif _, err := network.driver(true); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// From this point on, we need the network specific configuration,\n\t// which may come from a configuration-only network\n\tif network.configFrom != \"\" {\n\t\tt, err = c.getConfigNetwork(network.configFrom)\n\t\tif err != nil {\n\t\t\treturn nil, types.NotFoundErrorf(\"configuration network %q does not exist\", network.configFrom)\n\t\t}\n\t\tif err = t.applyConfigurationTo(network); err != nil {\n\t\t\treturn nil, types.InternalErrorf(\"Failed to apply configuration: %v\", err)\n\t\t}\n\t\tnetwork.generic[netlabel.Internal] = network.internal\n\t\tdefer func() {\n\t\t\tif err == nil && !skipCfgEpCount {\n\t\t\t\tif err := t.getEpCnt().IncEndpointCnt(); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"Failed to update reference count for configuration network %q on creation of network %q: %v\",\n\t\t\t\t\t\tt.Name(), network.Name(), err)\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\n\terr = network.ipamAllocate()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tnetwork.ipamRelease()\n\t\t}\n\t}()\n\n\terr = c.addNetwork(network)\n\tif err != nil {\n\t\tif _, ok := err.(types.MaskableError); ok {\n\t\t\t// This error can be ignored and set this boolean\n\t\t\t// value to skip a refcount increment for configOnly networks\n\t\t\tskipCfgEpCount = true\n\t\t} else {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := network.deleteNetwork(); e != nil {\n\t\t\t\tlogrus.Warnf(\"couldn't roll back driver network on network %s creation failure: %v\", network.name, err)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// XXX If the driver type is \"overlay\" check the options for DSR\n\t// being set.  If so, set the network's load balancing mode to DSR.\n\t// This should really be done in a network option, but due to\n\t// time pressure to get this in without adding changes to moby,\n\t// swarm and CLI, it is being implemented as a driver-specific\n\t// option.  Unfortunately, drivers can't influence the core\n\t// \"libnetwork.network\" data type.  Hence we need this hack code\n\t// to implement in this manner.\n\tif gval, ok := network.generic[netlabel.GenericData]; ok && network.networkType == \"overlay\" {\n\t\toptMap := gval.(map[string]string)\n\t\tif _, ok := optMap[overlayDSROptionString]; ok {\n\t\t\tnetwork.loadBalancerMode = loadBalancerModeDSR\n\t\t}\n\t}\n\naddToStore:\n\t// First store the endpoint count, then the network. To avoid to\n\t// end up with a datastore containing a network and not an epCnt,\n\t// in case of an ungraceful shutdown during this function call.\n\tepCnt := &endpointCnt{n: network}\n\tif err = c.updateToStore(epCnt); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := c.deleteFromStore(epCnt); e != nil {\n\t\t\t\tlogrus.Warnf(\"could not rollback from store, epCnt %v on failure (%v): %v\", epCnt, err, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tnetwork.epCnt = epCnt\n\tif err = c.updateToStore(network); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := c.deleteFromStore(network); e != nil {\n\t\t\t\tlogrus.Warnf(\"could not rollback from store, network %v on failure (%v): %v\", network, err, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif network.configOnly {\n\t\treturn network, nil\n\t}\n\n\tjoinCluster(network)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tnetwork.cancelDriverWatches()\n\t\t\tif e := network.leaveCluster(); e != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to leave agent cluster on network %s on failure (%v): %v\", network.name, err, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif network.hasLoadBalancerEndpoint() {\n\t\tif err = network.createLoadBalancerSandbox(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif !c.isDistributedControl() {\n\t\tc.Lock()\n\t\tarrangeIngressFilterRule()\n\t\tc.Unlock()\n\t}\n\tarrangeUserFilterRule()\n\n\treturn network, nil\n}\n\nvar joinCluster NetworkWalker = func(nw Network) bool {\n\tn := nw.(*network)\n\tif n.configOnly {\n\t\treturn false\n\t}\n\tif err := n.joinCluster(); err != nil {\n\t\tlogrus.Errorf(\"Failed to join network %s (%s) into agent cluster: %v\", n.Name(), n.ID(), err)\n\t}\n\tn.addDriverWatches()\n\treturn false\n}\n\nfunc (c *controller) reservePools() {\n\tnetworks, err := c.getNetworksForScope(datastore.LocalScope)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Could not retrieve networks from local store during ipam allocation for existing networks: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, n := range networks {\n\t\tif n.configOnly {\n\t\t\tcontinue\n\t\t}\n\t\tif !doReplayPoolReserve(n) {\n\t\t\tcontinue\n\t\t}\n\t\t// Construct pseudo configs for the auto IP case\n\t\tautoIPv4 := (len(n.ipamV4Config) == 0 || (len(n.ipamV4Config) == 1 && n.ipamV4Config[0].PreferredPool == \"\")) && len(n.ipamV4Info) > 0\n\t\tautoIPv6 := (len(n.ipamV6Config) == 0 || (len(n.ipamV6Config) == 1 && n.ipamV6Config[0].PreferredPool == \"\")) && len(n.ipamV6Info) > 0\n\t\tif autoIPv4 {\n\t\t\tn.ipamV4Config = []*IpamConf{{PreferredPool: n.ipamV4Info[0].Pool.String()}}\n\t\t}\n\t\tif n.enableIPv6 && autoIPv6 {\n\t\t\tn.ipamV6Config = []*IpamConf{{PreferredPool: n.ipamV6Info[0].Pool.String()}}\n\t\t}\n\t\t// Account current network gateways\n\t\tfor i, c := range n.ipamV4Config {\n\t\t\tif c.Gateway == \"\" && n.ipamV4Info[i].Gateway != nil {\n\t\t\t\tc.Gateway = n.ipamV4Info[i].Gateway.IP.String()\n\t\t\t}\n\t\t}\n\t\tif n.enableIPv6 {\n\t\t\tfor i, c := range n.ipamV6Config {\n\t\t\t\tif c.Gateway == \"\" && n.ipamV6Info[i].Gateway != nil {\n\t\t\t\t\tc.Gateway = n.ipamV6Info[i].Gateway.IP.String()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Reserve pools\n\t\tif err := n.ipamAllocate(); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to allocate ipam pool(s) for network %q (%s): %v\", n.Name(), n.ID(), err)\n\t\t}\n\t\t// Reserve existing endpoints' addresses\n\t\tipam, _, err := n.getController().getIPAMDriver(n.ipamType)\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Failed to retrieve ipam driver for network %q (%s) during address reservation\", n.Name(), n.ID())\n\t\t\tcontinue\n\t\t}\n\t\tepl, err := n.getEndpointsFromStore()\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Failed to retrieve list of current endpoints on network %q (%s)\", n.Name(), n.ID())\n\t\t\tcontinue\n\t\t}\n\t\tfor _, ep := range epl {\n\t\t\tif ep.Iface() == nil {\n\t\t\t\tlogrus.Warnf(\"endpoint interface is empty for %q (%s)\", ep.Name(), ep.ID())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := ep.assignAddress(ipam, true, ep.Iface().AddressIPv6() != nil); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to reserve current address for endpoint %q (%s) on network %q (%s)\",\n\t\t\t\t\tep.Name(), ep.ID(), n.Name(), n.ID())\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc doReplayPoolReserve(n *network) bool {\n\t_, caps, err := n.getController().getIPAMDriver(n.ipamType)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to retrieve ipam driver for network %q (%s): %v\", n.Name(), n.ID(), err)\n\t\treturn false\n\t}\n\treturn caps.RequiresRequestReplay\n}\n\nfunc (c *controller) addNetwork(n *network) error {\n\td, err := n.driver(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Create the network\n\tif err := d.CreateNetwork(n.id, n.generic, n, n.getIPData(4), n.getIPData(6)); err != nil {\n\t\treturn err\n\t}\n\n\tn.startResolver()\n\n\treturn nil\n}\n\nfunc (c *controller) Networks() []Network {\n\tvar list []Network\n\n\tfor _, n := range c.getNetworksFromStore() {\n\t\tif n.inDelete {\n\t\t\tcontinue\n\t\t}\n\t\tlist = append(list, n)\n\t}\n\n\treturn list\n}\n\nfunc (c *controller) WalkNetworks(walker NetworkWalker) {\n\tfor _, n := range c.Networks() {\n\t\tif walker(n) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (c *controller) NetworkByName(name string) (Network, error) {\n\tif name == \"\" {\n\t\treturn nil, ErrInvalidName(name)\n\t}\n\tvar n Network\n\n\ts := func(current Network) bool {\n\t\tif current.Name() == name {\n\t\t\tn = current\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\tc.WalkNetworks(s)\n\n\tif n == nil {\n\t\treturn nil, ErrNoSuchNetwork(name)\n\t}\n\n\treturn n, nil\n}\n\nfunc (c *controller) NetworkByID(id string) (Network, error) {\n\tif id == \"\" {\n\t\treturn nil, ErrInvalidID(id)\n\t}\n\n\tn, err := c.getNetworkFromStore(id)\n\tif err != nil {\n\t\treturn nil, ErrNoSuchNetwork(id)\n\t}\n\n\treturn n, nil\n}\n\n// NewSandbox creates a new sandbox for the passed container id\nfunc (c *controller) NewSandbox(containerID string, options ...SandboxOption) (Sandbox, error) {\n\tif containerID == \"\" {\n\t\treturn nil, types.BadRequestErrorf(\"invalid container ID\")\n\t}\n\n\tvar sb *sandbox\n\tc.Lock()\n\tfor _, s := range c.sandboxes {\n\t\tif s.containerID == containerID {\n\t\t\t// If not a stub, then we already have a complete sandbox.\n\t\t\tif !s.isStub {\n\t\t\t\tsbID := s.ID()\n\t\t\t\tc.Unlock()\n\t\t\t\treturn nil, types.ForbiddenErrorf(\"container %s is already present in sandbox %s\", containerID, sbID)\n\t\t\t}\n\n\t\t\t// We already have a stub sandbox from the\n\t\t\t// store. Make use of it so that we don't lose\n\t\t\t// the endpoints from store but reset the\n\t\t\t// isStub flag.\n\t\t\tsb = s\n\t\t\tsb.isStub = false\n\t\t\tbreak\n\t\t}\n\t}\n\tc.Unlock()\n\n\tsandboxID := stringid.GenerateRandomID()\n\tif runtime.GOOS == \"windows\" {\n\t\tsandboxID = containerID\n\t}\n\n\t// Create sandbox and process options first. Key generation depends on an option\n\tif sb == nil {\n\t\tsb = &sandbox{\n\t\t\tid:                 sandboxID,\n\t\t\tcontainerID:        containerID,\n\t\t\tendpoints:          []*endpoint{},\n\t\t\tepPriority:         map[string]int{},\n\t\t\tpopulatedEndpoints: map[string]struct{}{},\n\t\t\tconfig:             containerConfig{},\n\t\t\tcontroller:         c,\n\t\t\textDNS:             []extDNSEntry{},\n\t\t}\n\t}\n\n\tsb.processOptions(options...)\n\n\tc.Lock()\n\tif sb.ingress && c.ingressSandbox != nil {\n\t\tc.Unlock()\n\t\treturn nil, types.ForbiddenErrorf(\"ingress sandbox already present\")\n\t}\n\n\tif sb.ingress {\n\t\tc.ingressSandbox = sb\n\t\tsb.config.hostsPath = filepath.Join(c.cfg.Daemon.DataDir, \"/network/files/hosts\")\n\t\tsb.config.resolvConfPath = filepath.Join(c.cfg.Daemon.DataDir, \"/network/files/resolv.conf\")\n\t\tsb.id = \"ingress_sbox\"\n\t} else if sb.loadBalancerNID != \"\" {\n\t\tsb.id = \"lb_\" + sb.loadBalancerNID\n\t}\n\tc.Unlock()\n\n\tvar err error\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.Lock()\n\t\t\tif sb.ingress {\n\t\t\t\tc.ingressSandbox = nil\n\t\t\t}\n\t\t\tc.Unlock()\n\t\t}\n\t}()\n\n\tif err = sb.setupResolutionFiles(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif sb.config.useDefaultSandBox {\n\t\tc.sboxOnce.Do(func() {\n\t\t\tc.defOsSbox, err = osl.NewSandbox(sb.Key(), false, false)\n\t\t})\n\n\t\tif err != nil {\n\t\t\tc.sboxOnce = sync.Once{}\n\t\t\treturn nil, fmt.Errorf(\"failed to create default sandbox: %v\", err)\n\t\t}\n\n\t\tsb.osSbox = c.defOsSbox\n\t}\n\n\tif sb.osSbox == nil && !sb.config.useExternalKey {\n\t\tif sb.osSbox, err = osl.NewSandbox(sb.Key(), !sb.config.useDefaultSandBox, false); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to create new osl sandbox: %v\", err)\n\t\t}\n\t}\n\n\tif sb.osSbox != nil {\n\t\t// Apply operating specific knobs on the load balancer sandbox\n\t\terr := sb.osSbox.InvokeFunc(func() {\n\t\t\tsb.osSbox.ApplyOSTweaks(sb.oslTypes)\n\t\t})\n\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to apply performance tuning sysctls to the sandbox: %v\", err)\n\t\t}\n\t\t// Keep this just so performance is not changed\n\t\tsb.osSbox.ApplyOSTweaks(sb.oslTypes)\n\t}\n\n\tc.Lock()\n\tc.sandboxes[sb.id] = sb\n\tc.Unlock()\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.Lock()\n\t\t\tdelete(c.sandboxes, sb.id)\n\t\t\tc.Unlock()\n\t\t}\n\t}()\n\n\terr = sb.storeUpdate()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to update the store state of sandbox: %v\", err)\n\t}\n\n\treturn sb, nil\n}\n\nfunc (c *controller) Sandboxes() []Sandbox {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tlist := make([]Sandbox, 0, len(c.sandboxes))\n\tfor _, s := range c.sandboxes {\n\t\t// Hide stub sandboxes from libnetwork users\n\t\tif s.isStub {\n\t\t\tcontinue\n\t\t}\n\n\t\tlist = append(list, s)\n\t}\n\n\treturn list\n}\n\nfunc (c *controller) WalkSandboxes(walker SandboxWalker) {\n\tfor _, sb := range c.Sandboxes() {\n\t\tif walker(sb) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (c *controller) SandboxByID(id string) (Sandbox, error) {\n\tif id == \"\" {\n\t\treturn nil, ErrInvalidID(id)\n\t}\n\tc.Lock()\n\ts, ok := c.sandboxes[id]\n\tc.Unlock()\n\tif !ok {\n\t\treturn nil, types.NotFoundErrorf(\"sandbox %s not found\", id)\n\t}\n\treturn s, nil\n}\n\n// SandboxDestroy destroys a sandbox given a container ID\nfunc (c *controller) SandboxDestroy(id string) error {\n\tvar sb *sandbox\n\tc.Lock()\n\tfor _, s := range c.sandboxes {\n\t\tif s.containerID == id {\n\t\t\tsb = s\n\t\t\tbreak\n\t\t}\n\t}\n\tc.Unlock()\n\n\t// It is not an error if sandbox is not available\n\tif sb == nil {\n\t\treturn nil\n\t}\n\n\treturn sb.Delete()\n}\n\n// SandboxContainerWalker returns a Sandbox Walker function which looks for an existing Sandbox with the passed containerID\nfunc SandboxContainerWalker(out *Sandbox, containerID string) SandboxWalker {\n\treturn func(sb Sandbox) bool {\n\t\tif sb.ContainerID() == containerID {\n\t\t\t*out = sb\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n}\n\n// SandboxKeyWalker returns a Sandbox Walker function which looks for an existing Sandbox with the passed key\nfunc SandboxKeyWalker(out *Sandbox, key string) SandboxWalker {\n\treturn func(sb Sandbox) bool {\n\t\tif sb.Key() == key {\n\t\t\t*out = sb\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n}\n\nfunc (c *controller) loadDriver(networkType string) error {\n\tvar err error\n\n\tif pg := c.GetPluginGetter(); pg != nil {\n\t\t_, err = pg.Get(networkType, driverapi.NetworkPluginEndpointType, plugingetter.Lookup)\n\t} else {\n\t\t_, err = plugins.Get(networkType, driverapi.NetworkPluginEndpointType)\n\t}\n\n\tif err != nil {\n\t\tif errors.Cause(err) == plugins.ErrNotFound {\n\t\t\treturn types.NotFoundErrorf(err.Error())\n\t\t}\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) loadIPAMDriver(name string) error {\n\tvar err error\n\n\tif pg := c.GetPluginGetter(); pg != nil {\n\t\t_, err = pg.Get(name, ipamapi.PluginEndpointType, plugingetter.Lookup)\n\t} else {\n\t\t_, err = plugins.Get(name, ipamapi.PluginEndpointType)\n\t}\n\n\tif err != nil {\n\t\tif errors.Cause(err) == plugins.ErrNotFound {\n\t\t\treturn types.NotFoundErrorf(err.Error())\n\t\t}\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) getIPAMDriver(name string) (ipamapi.Ipam, *ipamapi.Capability, error) {\n\tid, cap := c.drvRegistry.IPAM(name)\n\tif id == nil {\n\t\t// Might be a plugin name. Try loading it\n\t\tif err := c.loadIPAMDriver(name); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\t// Now that we resolved the plugin, try again looking up the registry\n\t\tid, cap = c.drvRegistry.IPAM(name)\n\t\tif id == nil {\n\t\t\treturn nil, nil, types.BadRequestErrorf(\"invalid ipam driver: %q\", name)\n\t\t}\n\t}\n\n\treturn id, cap, nil\n}\n\nfunc (c *controller) Stop() {\n\tc.closeStores()\n\tc.stopExternalKeyListener()\n\tosl.GC()\n}\n\n// StartDiagnostic start the network dias mode\nfunc (c *controller) StartDiagnostic(port int) {\n\tc.Lock()\n\tif !c.DiagnosticServer.IsDiagnosticEnabled() {\n\t\tc.DiagnosticServer.EnableDiagnostic(\"127.0.0.1\", port)\n\t}\n\tc.Unlock()\n}\n\n// StopDiagnostic start the network dias mode\nfunc (c *controller) StopDiagnostic() {\n\tc.Lock()\n\tif c.DiagnosticServer.IsDiagnosticEnabled() {\n\t\tc.DiagnosticServer.DisableDiagnostic()\n\t}\n\tc.Unlock()\n}\n\n// IsDiagnosticEnabled returns true if the dias is enabled\nfunc (c *controller) IsDiagnosticEnabled() bool {\n\tc.Lock()\n\tdefer c.Unlock()\n\treturn c.DiagnosticServer.IsDiagnosticEnabled()\n}\n\nfunc (c *controller) iptablesEnabled() bool {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tif c.cfg == nil {\n\t\treturn false\n\t}\n\t// parse map cfg[\"bridge\"][\"generic\"][\"EnableIPTable\"]\n\tcfgBridge, ok := c.cfg.Daemon.DriverCfg[\"bridge\"].(map[string]interface{})\n\tif !ok {\n\t\treturn false\n\t}\n\tcfgGeneric, ok := cfgBridge[netlabel.GenericData].(options.Generic)\n\tif !ok {\n\t\treturn false\n\t}\n\tenabled, ok := cfgGeneric[\"EnableIPTables\"].(bool)\n\tif !ok {\n\t\t// unless user explicitly stated, assume iptable is enabled\n\t\tenabled = true\n\t}\n\treturn enabled\n}\n"
        },
        {
          "name": "datastore",
          "type": "tree",
          "content": null
        },
        {
          "name": "default_gateway.go",
          "type": "blob",
          "size": 5.140625,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst (\n\tgwEPlen = 12\n)\n\nvar procGwNetwork = make(chan (bool), 1)\n\n/*\n   libnetwork creates a bridge network \"docker_gw_bridge\" for providing\n   default gateway for the containers if none of the container's endpoints\n   have GW set by the driver. ICC is set to false for the GW_bridge network.\n\n   If a driver can't provide external connectivity it can choose to not set\n   the GW IP for the endpoint.\n\n   endpoint on the GW_bridge network is managed dynamically by libnetwork.\n   ie:\n   - its created when an endpoint without GW joins the container\n   - its deleted when an endpoint with GW joins the container\n*/\n\nfunc (sb *sandbox) setupDefaultGW() error {\n\n\t// check if the container already has a GW endpoint\n\tif ep := sb.getEndpointInGWNetwork(); ep != nil {\n\t\treturn nil\n\t}\n\n\tc := sb.controller\n\n\t// Look for default gw network. In case of error (includes not found),\n\t// retry and create it if needed in a serialized execution.\n\tn, err := c.NetworkByName(libnGWNetwork)\n\tif err != nil {\n\t\tif n, err = c.defaultGwNetwork(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tcreateOptions := []EndpointOption{CreateOptionAnonymous()}\n\n\tvar gwName string\n\tif len(sb.containerID) <= gwEPlen {\n\t\tgwName = \"gateway_\" + sb.containerID\n\t} else {\n\t\tgwName = \"gateway_\" + sb.id[:gwEPlen]\n\t}\n\n\tsbLabels := sb.Labels()\n\n\tif sbLabels[netlabel.PortMap] != nil {\n\t\tcreateOptions = append(createOptions, CreateOptionPortMapping(sbLabels[netlabel.PortMap].([]types.PortBinding)))\n\t}\n\n\tif sbLabels[netlabel.ExposedPorts] != nil {\n\t\tcreateOptions = append(createOptions, CreateOptionExposedPorts(sbLabels[netlabel.ExposedPorts].([]types.TransportPort)))\n\t}\n\n\tepOption := getPlatformOption()\n\tif epOption != nil {\n\t\tcreateOptions = append(createOptions, epOption)\n\t}\n\n\tnewEp, err := n.CreateEndpoint(gwName, createOptions...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"container %s: endpoint create on GW Network failed: %v\", sb.containerID, err)\n\t}\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif err2 := newEp.Delete(true); err2 != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to remove gw endpoint for container %s after failing to join the gateway network: %v\",\n\t\t\t\t\tsb.containerID, err2)\n\t\t\t}\n\t\t}\n\t}()\n\n\tepLocal := newEp.(*endpoint)\n\n\tif err = epLocal.sbJoin(sb); err != nil {\n\t\treturn fmt.Errorf(\"container %s: endpoint join on GW Network failed: %v\", sb.containerID, err)\n\t}\n\n\treturn nil\n}\n\n// If present, detach and remove the endpoint connecting the sandbox to the default gw network.\nfunc (sb *sandbox) clearDefaultGW() error {\n\tvar ep *endpoint\n\n\tif ep = sb.getEndpointInGWNetwork(); ep == nil {\n\t\treturn nil\n\t}\n\tif err := ep.sbLeave(sb, false); err != nil {\n\t\treturn fmt.Errorf(\"container %s: endpoint leaving GW Network failed: %v\", sb.containerID, err)\n\t}\n\tif err := ep.Delete(false); err != nil {\n\t\treturn fmt.Errorf(\"container %s: deleting endpoint on GW Network failed: %v\", sb.containerID, err)\n\t}\n\treturn nil\n}\n\n// Evaluate whether the sandbox requires a default gateway based\n// on the endpoints to which it is connected. It does not account\n// for the default gateway network endpoint.\n\nfunc (sb *sandbox) needDefaultGW() bool {\n\tvar needGW bool\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.endpointInGWNetwork() {\n\t\t\tcontinue\n\t\t}\n\t\tif ep.getNetwork().Type() == \"null\" || ep.getNetwork().Type() == \"host\" {\n\t\t\tcontinue\n\t\t}\n\t\tif ep.getNetwork().Internal() {\n\t\t\tcontinue\n\t\t}\n\t\t// During stale sandbox cleanup, joinInfo may be nil\n\t\tif ep.joinInfo != nil && ep.joinInfo.disableGatewayService {\n\t\t\tcontinue\n\t\t}\n\t\t// TODO v6 needs to be handled.\n\t\tif len(ep.Gateway()) > 0 {\n\t\t\treturn false\n\t\t}\n\t\tfor _, r := range ep.StaticRoutes() {\n\t\t\tif r.Destination != nil && r.Destination.String() == \"0.0.0.0/0\" {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tneedGW = true\n\t}\n\n\treturn needGW\n}\n\nfunc (sb *sandbox) getEndpointInGWNetwork() *endpoint {\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.getNetwork().name == libnGWNetwork && strings.HasPrefix(ep.Name(), \"gateway_\") {\n\t\t\treturn ep\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (ep *endpoint) endpointInGWNetwork() bool {\n\tif ep.getNetwork().name == libnGWNetwork && strings.HasPrefix(ep.Name(), \"gateway_\") {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (sb *sandbox) getEPwithoutGateway() *endpoint {\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.getNetwork().Type() == \"null\" || ep.getNetwork().Type() == \"host\" {\n\t\t\tcontinue\n\t\t}\n\t\tif len(ep.Gateway()) == 0 {\n\t\t\treturn ep\n\t\t}\n\t}\n\treturn nil\n}\n\n// Looks for the default gw network and creates it if not there.\n// Parallel executions are serialized.\nfunc (c *controller) defaultGwNetwork() (Network, error) {\n\tprocGwNetwork <- true\n\tdefer func() { <-procGwNetwork }()\n\n\tn, err := c.NetworkByName(libnGWNetwork)\n\tif _, ok := err.(types.NotFoundError); ok {\n\t\tn, err = c.createGWNetwork()\n\t}\n\treturn n, err\n}\n\n// Returns the endpoint which is providing external connectivity to the sandbox\nfunc (sb *sandbox) getGatewayEndpoint() *endpoint {\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.getNetwork().Type() == \"null\" || ep.getNetwork().Type() == \"host\" {\n\t\t\tcontinue\n\t\t}\n\t\tif len(ep.Gateway()) != 0 {\n\t\t\treturn ep\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "default_gateway_freebsd.go",
          "type": "blob",
          "size": 0.318359375,
          "content": "package libnetwork\n\nimport \"github.com/docker/libnetwork/types\"\n\nconst libnGWNetwork = \"docker_gwbridge\"\n\nfunc getPlatformOption() EndpointOption {\n\treturn nil\n}\n\nfunc (c *controller) createGWNetwork() (Network, error) {\n\treturn nil, types.NotImplementedErrorf(\"default gateway functionality is not implemented in freebsd\")\n}\n"
        },
        {
          "name": "default_gateway_linux.go",
          "type": "blob",
          "size": 0.677734375,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\n\t\"github.com/docker/libnetwork/drivers/bridge\"\n)\n\nconst libnGWNetwork = \"docker_gwbridge\"\n\nfunc getPlatformOption() EndpointOption {\n\treturn nil\n}\n\nfunc (c *controller) createGWNetwork() (Network, error) {\n\tnetOption := map[string]string{\n\t\tbridge.BridgeName:         libnGWNetwork,\n\t\tbridge.EnableICC:          strconv.FormatBool(false),\n\t\tbridge.EnableIPMasquerade: strconv.FormatBool(true),\n\t}\n\n\tn, err := c.NewNetwork(\"bridge\", libnGWNetwork, \"\",\n\t\tNetworkOptionDriverOpts(netOption),\n\t\tNetworkOptionEnableIPv6(false),\n\t)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating external connectivity network: %v\", err)\n\t}\n\treturn n, err\n}\n"
        },
        {
          "name": "default_gateway_windows.go",
          "type": "blob",
          "size": 0.5263671875,
          "content": "package libnetwork\n\nimport (\n\twindriver \"github.com/docker/libnetwork/drivers/windows\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/types\"\n)\n\nconst libnGWNetwork = \"nat\"\n\nfunc getPlatformOption() EndpointOption {\n\n\tepOption := options.Generic{\n\t\twindriver.DisableICC: true,\n\t\twindriver.DisableDNS: true,\n\t}\n\treturn EndpointOptionGeneric(epOption)\n}\n\nfunc (c *controller) createGWNetwork() (Network, error) {\n\treturn nil, types.NotImplementedErrorf(\"default gateway functionality is not implemented in windows\")\n}\n"
        },
        {
          "name": "diagnostic",
          "type": "tree",
          "content": null
        },
        {
          "name": "discoverapi",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "driverapi",
          "type": "tree",
          "content": null
        },
        {
          "name": "drivers",
          "type": "tree",
          "content": null
        },
        {
          "name": "drivers_freebsd.go",
          "type": "blob",
          "size": 0.251953125,
          "content": "package libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/drivers/null\"\n\t\"github.com/docker/libnetwork/drivers/remote\"\n)\n\nfunc getInitializers(experimental bool) []initializer {\n\treturn []initializer{\n\t\t{null.Init, \"null\"},\n\t\t{remote.Init, \"remote\"},\n\t}\n}\n"
        },
        {
          "name": "drivers_ipam.go",
          "type": "blob",
          "size": 0.6962890625,
          "content": "package libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/drvregistry\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\tbuiltinIpam \"github.com/docker/libnetwork/ipams/builtin\"\n\tnullIpam \"github.com/docker/libnetwork/ipams/null\"\n\tremoteIpam \"github.com/docker/libnetwork/ipams/remote\"\n\t\"github.com/docker/libnetwork/ipamutils\"\n)\n\nfunc initIPAMDrivers(r *drvregistry.DrvRegistry, lDs, gDs interface{}, addressPool []*ipamutils.NetworkToSplit) error {\n\tbuiltinIpam.SetDefaultIPAddressPool(addressPool)\n\tfor _, fn := range [](func(ipamapi.Callback, interface{}, interface{}) error){\n\t\tbuiltinIpam.Init,\n\t\tremoteIpam.Init,\n\t\tnullIpam.Init,\n\t} {\n\t\tif err := fn(r, lDs, gDs); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "drivers_linux.go",
          "type": "blob",
          "size": 0.623046875,
          "content": "package libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/drivers/bridge\"\n\t\"github.com/docker/libnetwork/drivers/host\"\n\t\"github.com/docker/libnetwork/drivers/ipvlan\"\n\t\"github.com/docker/libnetwork/drivers/macvlan\"\n\t\"github.com/docker/libnetwork/drivers/null\"\n\t\"github.com/docker/libnetwork/drivers/overlay\"\n\t\"github.com/docker/libnetwork/drivers/remote\"\n)\n\nfunc getInitializers(experimental bool) []initializer {\n\tin := []initializer{\n\t\t{bridge.Init, \"bridge\"},\n\t\t{host.Init, \"host\"},\n\t\t{ipvlan.Init, \"ipvlan\"},\n\t\t{macvlan.Init, \"macvlan\"},\n\t\t{null.Init, \"null\"},\n\t\t{overlay.Init, \"overlay\"},\n\t\t{remote.Init, \"remote\"},\n\t}\n\treturn in\n}\n"
        },
        {
          "name": "drivers_windows.go",
          "type": "blob",
          "size": 0.673828125,
          "content": "package libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/drivers/null\"\n\t\"github.com/docker/libnetwork/drivers/remote\"\n\t\"github.com/docker/libnetwork/drivers/windows\"\n\t\"github.com/docker/libnetwork/drivers/windows/overlay\"\n)\n\nfunc getInitializers(experimental bool) []initializer {\n\treturn []initializer{\n\t\t{null.Init, \"null\"},\n\t\t{overlay.Init, \"overlay\"},\n\t\t{remote.Init, \"remote\"},\n\t\t{windows.GetInit(\"transparent\"), \"transparent\"},\n\t\t{windows.GetInit(\"l2bridge\"), \"l2bridge\"},\n\t\t{windows.GetInit(\"l2tunnel\"), \"l2tunnel\"},\n\t\t{windows.GetInit(\"nat\"), \"nat\"},\n\t\t{windows.GetInit(\"internal\"), \"internal\"},\n\t\t{windows.GetInit(\"private\"), \"private\"},\n\t\t{windows.GetInit(\"ics\"), \"ics\"},\n\t}\n}\n"
        },
        {
          "name": "drvregistry",
          "type": "tree",
          "content": null
        },
        {
          "name": "endpoint.go",
          "type": "blob",
          "size": 31.13671875,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Endpoint represents a logical connection between a network and a sandbox.\ntype Endpoint interface {\n\t// A system generated id for this endpoint.\n\tID() string\n\n\t// Name returns the name of this endpoint.\n\tName() string\n\n\t// Network returns the name of the network to which this endpoint is attached.\n\tNetwork() string\n\n\t// Join joins the sandbox to the endpoint and populates into the sandbox\n\t// the network resources allocated for the endpoint.\n\tJoin(sandbox Sandbox, options ...EndpointOption) error\n\n\t// Leave detaches the network resources populated in the sandbox.\n\tLeave(sandbox Sandbox, options ...EndpointOption) error\n\n\t// Return certain operational data belonging to this endpoint\n\tInfo() EndpointInfo\n\n\t// DriverInfo returns a collection of driver operational data related to this endpoint retrieved from the driver\n\tDriverInfo() (map[string]interface{}, error)\n\n\t// Delete and detaches this endpoint from the network.\n\tDelete(force bool) error\n}\n\n// EndpointOption is an option setter function type used to pass various options to Network\n// and Endpoint interfaces methods. The various setter functions of type EndpointOption are\n// provided by libnetwork, they look like <Create|Join|Leave>Option[...](...)\ntype EndpointOption func(ep *endpoint)\n\ntype endpoint struct {\n\tname              string\n\tid                string\n\tnetwork           *network\n\tiface             *endpointInterface\n\tjoinInfo          *endpointJoinInfo\n\tsandboxID         string\n\tlocator           string\n\texposedPorts      []types.TransportPort\n\tanonymous         bool\n\tdisableResolution bool\n\tgeneric           map[string]interface{}\n\tjoinLeaveDone     chan struct{}\n\tprefAddress       net.IP\n\tprefAddressV6     net.IP\n\tipamOptions       map[string]string\n\taliases           map[string]string\n\tmyAliases         []string\n\tsvcID             string\n\tsvcName           string\n\tvirtualIP         net.IP\n\tsvcAliases        []string\n\tingressPorts      []*PortConfig\n\tdbIndex           uint64\n\tdbExists          bool\n\tserviceEnabled    bool\n\tloadBalancer      bool\n\tsync.Mutex\n}\n\nfunc (ep *endpoint) MarshalJSON() ([]byte, error) {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tepMap := make(map[string]interface{})\n\tepMap[\"name\"] = ep.name\n\tepMap[\"id\"] = ep.id\n\tepMap[\"ep_iface\"] = ep.iface\n\tepMap[\"joinInfo\"] = ep.joinInfo\n\tepMap[\"exposed_ports\"] = ep.exposedPorts\n\tif ep.generic != nil {\n\t\tepMap[\"generic\"] = ep.generic\n\t}\n\tepMap[\"sandbox\"] = ep.sandboxID\n\tepMap[\"locator\"] = ep.locator\n\tepMap[\"anonymous\"] = ep.anonymous\n\tepMap[\"disableResolution\"] = ep.disableResolution\n\tepMap[\"myAliases\"] = ep.myAliases\n\tepMap[\"svcName\"] = ep.svcName\n\tepMap[\"svcID\"] = ep.svcID\n\tepMap[\"virtualIP\"] = ep.virtualIP.String()\n\tepMap[\"ingressPorts\"] = ep.ingressPorts\n\tepMap[\"svcAliases\"] = ep.svcAliases\n\tepMap[\"loadBalancer\"] = ep.loadBalancer\n\n\treturn json.Marshal(epMap)\n}\n\nfunc (ep *endpoint) UnmarshalJSON(b []byte) (err error) {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tvar epMap map[string]interface{}\n\tif err := json.Unmarshal(b, &epMap); err != nil {\n\t\treturn err\n\t}\n\tep.name = epMap[\"name\"].(string)\n\tep.id = epMap[\"id\"].(string)\n\n\tib, _ := json.Marshal(epMap[\"ep_iface\"])\n\tjson.Unmarshal(ib, &ep.iface)\n\n\tjb, _ := json.Marshal(epMap[\"joinInfo\"])\n\tjson.Unmarshal(jb, &ep.joinInfo)\n\n\ttb, _ := json.Marshal(epMap[\"exposed_ports\"])\n\tvar tPorts []types.TransportPort\n\tjson.Unmarshal(tb, &tPorts)\n\tep.exposedPorts = tPorts\n\n\tcb, _ := json.Marshal(epMap[\"sandbox\"])\n\tjson.Unmarshal(cb, &ep.sandboxID)\n\n\tif v, ok := epMap[\"generic\"]; ok {\n\t\tep.generic = v.(map[string]interface{})\n\n\t\tif opt, ok := ep.generic[netlabel.PortMap]; ok {\n\t\t\tpblist := []types.PortBinding{}\n\n\t\t\tfor i := 0; i < len(opt.([]interface{})); i++ {\n\t\t\t\tpb := types.PortBinding{}\n\t\t\t\ttmp := opt.([]interface{})[i].(map[string]interface{})\n\n\t\t\t\tbytes, err := json.Marshal(tmp)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Error(err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\terr = json.Unmarshal(bytes, &pb)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Error(err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tpblist = append(pblist, pb)\n\t\t\t}\n\t\t\tep.generic[netlabel.PortMap] = pblist\n\t\t}\n\n\t\tif opt, ok := ep.generic[netlabel.ExposedPorts]; ok {\n\t\t\ttplist := []types.TransportPort{}\n\n\t\t\tfor i := 0; i < len(opt.([]interface{})); i++ {\n\t\t\t\ttp := types.TransportPort{}\n\t\t\t\ttmp := opt.([]interface{})[i].(map[string]interface{})\n\n\t\t\t\tbytes, err := json.Marshal(tmp)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Error(err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\terr = json.Unmarshal(bytes, &tp)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Error(err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ttplist = append(tplist, tp)\n\t\t\t}\n\t\t\tep.generic[netlabel.ExposedPorts] = tplist\n\n\t\t}\n\t}\n\n\tif v, ok := epMap[\"anonymous\"]; ok {\n\t\tep.anonymous = v.(bool)\n\t}\n\tif v, ok := epMap[\"disableResolution\"]; ok {\n\t\tep.disableResolution = v.(bool)\n\t}\n\tif l, ok := epMap[\"locator\"]; ok {\n\t\tep.locator = l.(string)\n\t}\n\n\tif sn, ok := epMap[\"svcName\"]; ok {\n\t\tep.svcName = sn.(string)\n\t}\n\n\tif si, ok := epMap[\"svcID\"]; ok {\n\t\tep.svcID = si.(string)\n\t}\n\n\tif vip, ok := epMap[\"virtualIP\"]; ok {\n\t\tep.virtualIP = net.ParseIP(vip.(string))\n\t}\n\n\tif v, ok := epMap[\"loadBalancer\"]; ok {\n\t\tep.loadBalancer = v.(bool)\n\t}\n\n\tsal, _ := json.Marshal(epMap[\"svcAliases\"])\n\tvar svcAliases []string\n\tjson.Unmarshal(sal, &svcAliases)\n\tep.svcAliases = svcAliases\n\n\tpc, _ := json.Marshal(epMap[\"ingressPorts\"])\n\tvar ingressPorts []*PortConfig\n\tjson.Unmarshal(pc, &ingressPorts)\n\tep.ingressPorts = ingressPorts\n\n\tma, _ := json.Marshal(epMap[\"myAliases\"])\n\tvar myAliases []string\n\tjson.Unmarshal(ma, &myAliases)\n\tep.myAliases = myAliases\n\treturn nil\n}\n\nfunc (ep *endpoint) New() datastore.KVObject {\n\treturn &endpoint{network: ep.getNetwork()}\n}\n\nfunc (ep *endpoint) CopyTo(o datastore.KVObject) error {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tdstEp := o.(*endpoint)\n\tdstEp.name = ep.name\n\tdstEp.id = ep.id\n\tdstEp.sandboxID = ep.sandboxID\n\tdstEp.locator = ep.locator\n\tdstEp.dbIndex = ep.dbIndex\n\tdstEp.dbExists = ep.dbExists\n\tdstEp.anonymous = ep.anonymous\n\tdstEp.disableResolution = ep.disableResolution\n\tdstEp.svcName = ep.svcName\n\tdstEp.svcID = ep.svcID\n\tdstEp.virtualIP = ep.virtualIP\n\tdstEp.loadBalancer = ep.loadBalancer\n\n\tdstEp.svcAliases = make([]string, len(ep.svcAliases))\n\tcopy(dstEp.svcAliases, ep.svcAliases)\n\n\tdstEp.ingressPorts = make([]*PortConfig, len(ep.ingressPorts))\n\tcopy(dstEp.ingressPorts, ep.ingressPorts)\n\n\tif ep.iface != nil {\n\t\tdstEp.iface = &endpointInterface{}\n\t\tep.iface.CopyTo(dstEp.iface)\n\t}\n\n\tif ep.joinInfo != nil {\n\t\tdstEp.joinInfo = &endpointJoinInfo{}\n\t\tep.joinInfo.CopyTo(dstEp.joinInfo)\n\t}\n\n\tdstEp.exposedPorts = make([]types.TransportPort, len(ep.exposedPorts))\n\tcopy(dstEp.exposedPorts, ep.exposedPorts)\n\n\tdstEp.myAliases = make([]string, len(ep.myAliases))\n\tcopy(dstEp.myAliases, ep.myAliases)\n\n\tdstEp.generic = options.Generic{}\n\tfor k, v := range ep.generic {\n\t\tdstEp.generic[k] = v\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) ID() string {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\treturn ep.id\n}\n\nfunc (ep *endpoint) Name() string {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\treturn ep.name\n}\n\nfunc (ep *endpoint) MyAliases() []string {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\treturn ep.myAliases\n}\n\nfunc (ep *endpoint) Network() string {\n\tif ep.network == nil {\n\t\treturn \"\"\n\t}\n\n\treturn ep.network.name\n}\n\nfunc (ep *endpoint) isAnonymous() bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn ep.anonymous\n}\n\n// isServiceEnabled check if service is enabled on the endpoint\nfunc (ep *endpoint) isServiceEnabled() bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn ep.serviceEnabled\n}\n\n// enableService sets service enabled on the endpoint\nfunc (ep *endpoint) enableService() {\n\tep.Lock()\n\tdefer ep.Unlock()\n\tep.serviceEnabled = true\n}\n\n// disableService disables service on the endpoint\nfunc (ep *endpoint) disableService() {\n\tep.Lock()\n\tdefer ep.Unlock()\n\tep.serviceEnabled = false\n}\n\nfunc (ep *endpoint) needResolver() bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn !ep.disableResolution\n}\n\n// endpoint Key structure : endpoint/network-id/endpoint-id\nfunc (ep *endpoint) Key() []string {\n\tif ep.network == nil {\n\t\treturn nil\n\t}\n\n\treturn []string{datastore.EndpointKeyPrefix, ep.network.id, ep.id}\n}\n\nfunc (ep *endpoint) KeyPrefix() []string {\n\tif ep.network == nil {\n\t\treturn nil\n\t}\n\n\treturn []string{datastore.EndpointKeyPrefix, ep.network.id}\n}\n\nfunc (ep *endpoint) networkIDFromKey(key string) (string, error) {\n\t// endpoint Key structure : docker/libnetwork/endpoint/${network-id}/${endpoint-id}\n\t// it's an invalid key if the key doesn't have all the 5 key elements above\n\tkeyElements := strings.Split(key, \"/\")\n\tif !strings.HasPrefix(key, datastore.Key(datastore.EndpointKeyPrefix)) || len(keyElements) < 5 {\n\t\treturn \"\", fmt.Errorf(\"invalid endpoint key : %v\", key)\n\t}\n\t// network-id is placed at index=3. pls refer to endpoint.Key() method\n\treturn strings.Split(key, \"/\")[3], nil\n}\n\nfunc (ep *endpoint) Value() []byte {\n\tb, err := json.Marshal(ep)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn b\n}\n\nfunc (ep *endpoint) SetValue(value []byte) error {\n\treturn json.Unmarshal(value, ep)\n}\n\nfunc (ep *endpoint) Index() uint64 {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn ep.dbIndex\n}\n\nfunc (ep *endpoint) SetIndex(index uint64) {\n\tep.Lock()\n\tdefer ep.Unlock()\n\tep.dbIndex = index\n\tep.dbExists = true\n}\n\nfunc (ep *endpoint) Exists() bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn ep.dbExists\n}\n\nfunc (ep *endpoint) Skip() bool {\n\treturn ep.getNetwork().Skip()\n}\n\nfunc (ep *endpoint) processOptions(options ...EndpointOption) {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tfor _, opt := range options {\n\t\tif opt != nil {\n\t\t\topt(ep)\n\t\t}\n\t}\n}\n\nfunc (ep *endpoint) getNetwork() *network {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\treturn ep.network\n}\n\nfunc (ep *endpoint) getNetworkFromStore() (*network, error) {\n\tif ep.network == nil {\n\t\treturn nil, fmt.Errorf(\"invalid network object in endpoint %s\", ep.Name())\n\t}\n\n\treturn ep.network.getController().getNetworkFromStore(ep.network.id)\n}\n\nfunc (ep *endpoint) Join(sbox Sandbox, options ...EndpointOption) error {\n\tif sbox == nil {\n\t\treturn types.BadRequestErrorf(\"endpoint cannot be joined by nil container\")\n\t}\n\n\tsb, ok := sbox.(*sandbox)\n\tif !ok {\n\t\treturn types.BadRequestErrorf(\"not a valid Sandbox interface\")\n\t}\n\n\tsb.joinLeaveStart()\n\tdefer sb.joinLeaveEnd()\n\n\treturn ep.sbJoin(sb, options...)\n}\n\nfunc (ep *endpoint) sbJoin(sb *sandbox, options ...EndpointOption) (err error) {\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get network from store during join: %v\", err)\n\t}\n\n\tep, err = n.getEndpointFromStore(ep.ID())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get endpoint from store during join: %v\", err)\n\t}\n\n\tep.Lock()\n\tif ep.sandboxID != \"\" {\n\t\tep.Unlock()\n\t\treturn types.ForbiddenErrorf(\"another container is attached to the same network endpoint\")\n\t}\n\tep.network = n\n\tep.sandboxID = sb.ID()\n\tep.joinInfo = &endpointJoinInfo{}\n\tepid := ep.id\n\tep.Unlock()\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tep.Lock()\n\t\t\tep.sandboxID = \"\"\n\t\t\tep.Unlock()\n\t\t}\n\t}()\n\n\tnid := n.ID()\n\n\tep.processOptions(options...)\n\n\td, err := n.driver(true)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get driver during join: %v\", err)\n\t}\n\n\terr = d.Join(nid, epid, sb.Key(), ep, sb.Labels())\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := d.Leave(nid, epid); e != nil {\n\t\t\t\tlogrus.Warnf(\"driver leave failed while rolling back join: %v\", e)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Watch for service records\n\tif !n.getController().isAgent() {\n\t\tn.getController().watchSvcRecord(ep)\n\t}\n\n\tif doUpdateHostsFile(n, sb) {\n\t\tvar addresses []string\n\t\tif ip := ep.getFirstInterfaceIPv4Address(); ip != nil {\n\t\t\taddresses = append(addresses, ip.String())\n\t\t}\n\t\tif ip := ep.getFirstInterfaceIPv6Address(); ip != nil {\n\t\t\taddresses = append(addresses, ip.String())\n\t\t}\n\t\tif err = sb.updateHostsFile(addresses); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err = sb.updateDNS(n.enableIPv6); err != nil {\n\t\treturn err\n\t}\n\n\t// Current endpoint providing external connectivity for the sandbox\n\textEp := sb.getGatewayEndpoint()\n\n\tsb.addEndpoint(ep)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tsb.removeEndpoint(ep)\n\t\t}\n\t}()\n\n\tif err = sb.populateNetworkResources(ep); err != nil {\n\t\treturn err\n\t}\n\n\tif err = n.getController().updateToStore(ep); err != nil {\n\t\treturn err\n\t}\n\n\tif err = ep.addDriverInfoToCluster(); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := ep.deleteDriverInfoFromCluster(); e != nil {\n\t\t\t\tlogrus.Errorf(\"Could not delete endpoint state for endpoint %s from cluster on join failure: %v\", ep.Name(), e)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Load balancing endpoints should never have a default gateway nor\n\t// should they alter the status of a network's default gateway\n\tif ep.loadBalancer && !sb.ingress {\n\t\treturn nil\n\t}\n\n\tif sb.needDefaultGW() && sb.getEndpointInGWNetwork() == nil {\n\t\treturn sb.setupDefaultGW()\n\t}\n\n\tmoveExtConn := sb.getGatewayEndpoint() != extEp\n\n\tif moveExtConn {\n\t\tif extEp != nil {\n\t\t\tlogrus.Debugf(\"Revoking external connectivity on endpoint %s (%s)\", extEp.Name(), extEp.ID())\n\t\t\textN, err := extEp.getNetworkFromStore()\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to get network from store for revoking external connectivity during join: %v\", err)\n\t\t\t}\n\t\t\textD, err := extN.driver(true)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to get driver for revoking external connectivity during join: %v\", err)\n\t\t\t}\n\t\t\tif err = extD.RevokeExternalConnectivity(extEp.network.ID(), extEp.ID()); err != nil {\n\t\t\t\treturn types.InternalErrorf(\n\t\t\t\t\t\"driver failed revoking external connectivity on endpoint %s (%s): %v\",\n\t\t\t\t\textEp.Name(), extEp.ID(), err)\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err != nil {\n\t\t\t\t\tif e := extD.ProgramExternalConnectivity(extEp.network.ID(), extEp.ID(), sb.Labels()); e != nil {\n\t\t\t\t\t\tlogrus.Warnf(\"Failed to roll-back external connectivity on endpoint %s (%s): %v\",\n\t\t\t\t\t\t\textEp.Name(), extEp.ID(), e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t\tif !n.internal {\n\t\t\tlogrus.Debugf(\"Programming external connectivity on endpoint %s (%s)\", ep.Name(), ep.ID())\n\t\t\tif err = d.ProgramExternalConnectivity(n.ID(), ep.ID(), sb.Labels()); err != nil {\n\t\t\t\treturn types.InternalErrorf(\n\t\t\t\t\t\"driver failed programming external connectivity on endpoint %s (%s): %v\",\n\t\t\t\t\tep.Name(), ep.ID(), err)\n\t\t\t}\n\t\t}\n\n\t}\n\n\tif !sb.needDefaultGW() {\n\t\tif e := sb.clearDefaultGW(); e != nil {\n\t\t\tlogrus.Warnf(\"Failure while disconnecting sandbox %s (%s) from gateway network: %v\",\n\t\t\t\tsb.ID(), sb.ContainerID(), e)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc doUpdateHostsFile(n *network, sb *sandbox) bool {\n\treturn !n.ingress && n.Name() != libnGWNetwork\n}\n\nfunc (ep *endpoint) rename(name string) error {\n\tvar (\n\t\terr      error\n\t\tnetWatch *netWatch\n\t\tok       bool\n\t)\n\n\tn := ep.getNetwork()\n\tif n == nil {\n\t\treturn fmt.Errorf(\"network not connected for ep %q\", ep.name)\n\t}\n\n\tc := n.getController()\n\n\tsb, ok := ep.getSandbox()\n\tif !ok {\n\t\tlogrus.Warnf(\"rename for %s aborted, sandbox %s is not anymore present\", ep.ID(), ep.sandboxID)\n\t\treturn nil\n\t}\n\n\tif c.isAgent() {\n\t\tif err = ep.deleteServiceInfoFromCluster(sb, true, \"rename\"); err != nil {\n\t\t\treturn types.InternalErrorf(\"Could not delete service state for endpoint %s from cluster on rename: %v\", ep.Name(), err)\n\t\t}\n\t} else {\n\t\tc.Lock()\n\t\tnetWatch, ok = c.nmap[n.ID()]\n\t\tc.Unlock()\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"watch null for network %q\", n.Name())\n\t\t}\n\t\tn.updateSvcRecord(ep, c.getLocalEps(netWatch), false)\n\t}\n\n\toldName := ep.name\n\toldAnonymous := ep.anonymous\n\tep.name = name\n\tep.anonymous = false\n\n\tif c.isAgent() {\n\t\tif err = ep.addServiceInfoToCluster(sb); err != nil {\n\t\t\treturn types.InternalErrorf(\"Could not add service state for endpoint %s to cluster on rename: %v\", ep.Name(), err)\n\t\t}\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tep.deleteServiceInfoFromCluster(sb, true, \"rename\")\n\t\t\t\tep.name = oldName\n\t\t\t\tep.anonymous = oldAnonymous\n\t\t\t\tep.addServiceInfoToCluster(sb)\n\t\t\t}\n\t\t}()\n\t} else {\n\t\tn.updateSvcRecord(ep, c.getLocalEps(netWatch), true)\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tn.updateSvcRecord(ep, c.getLocalEps(netWatch), false)\n\t\t\t\tep.name = oldName\n\t\t\t\tep.anonymous = oldAnonymous\n\t\t\t\tn.updateSvcRecord(ep, c.getLocalEps(netWatch), true)\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Update the store with the updated name\n\tif err = c.updateToStore(ep); err != nil {\n\t\treturn err\n\t}\n\t// After the name change do a dummy endpoint count update to\n\t// trigger the service record update in the peer nodes\n\n\t// Ignore the error because updateStore fail for EpCnt is a\n\t// benign error. Besides there is no meaningful recovery that\n\t// we can do. When the cluster recovers subsequent EpCnt update\n\t// will force the peers to get the correct EP name.\n\tn.getEpCnt().updateStore()\n\n\treturn err\n}\n\nfunc (ep *endpoint) hasInterface(iName string) bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\treturn ep.iface != nil && ep.iface.srcName == iName\n}\n\nfunc (ep *endpoint) Leave(sbox Sandbox, options ...EndpointOption) error {\n\tif sbox == nil || sbox.ID() == \"\" || sbox.Key() == \"\" {\n\t\treturn types.BadRequestErrorf(\"invalid Sandbox passed to endpoint leave: %v\", sbox)\n\t}\n\n\tsb, ok := sbox.(*sandbox)\n\tif !ok {\n\t\treturn types.BadRequestErrorf(\"not a valid Sandbox interface\")\n\t}\n\n\tsb.joinLeaveStart()\n\tdefer sb.joinLeaveEnd()\n\n\treturn ep.sbLeave(sb, false, options...)\n}\n\nfunc (ep *endpoint) sbLeave(sb *sandbox, force bool, options ...EndpointOption) error {\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get network from store during leave: %v\", err)\n\t}\n\n\tep, err = n.getEndpointFromStore(ep.ID())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get endpoint from store during leave: %v\", err)\n\t}\n\n\tep.Lock()\n\tsid := ep.sandboxID\n\tep.Unlock()\n\n\tif sid == \"\" {\n\t\treturn types.ForbiddenErrorf(\"cannot leave endpoint with no attached sandbox\")\n\t}\n\tif sid != sb.ID() {\n\t\treturn types.ForbiddenErrorf(\"unexpected sandbox ID in leave request. Expected %s. Got %s\", ep.sandboxID, sb.ID())\n\t}\n\n\tep.processOptions(options...)\n\n\td, err := n.driver(!force)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get driver during endpoint leave: %v\", err)\n\t}\n\n\tep.Lock()\n\tep.sandboxID = \"\"\n\tep.network = n\n\tep.Unlock()\n\n\t// Current endpoint providing external connectivity to the sandbox\n\textEp := sb.getGatewayEndpoint()\n\tmoveExtConn := extEp != nil && (extEp.ID() == ep.ID())\n\n\tif d != nil {\n\t\tif moveExtConn {\n\t\t\tlogrus.Debugf(\"Revoking external connectivity on endpoint %s (%s)\", ep.Name(), ep.ID())\n\t\t\tif err := d.RevokeExternalConnectivity(n.id, ep.id); err != nil {\n\t\t\t\tlogrus.Warnf(\"driver failed revoking external connectivity on endpoint %s (%s): %v\",\n\t\t\t\t\tep.Name(), ep.ID(), err)\n\t\t\t}\n\t\t}\n\n\t\tif err := d.Leave(n.id, ep.id); err != nil {\n\t\t\tif _, ok := err.(types.MaskableError); !ok {\n\t\t\t\tlogrus.Warnf(\"driver error disconnecting container %s : %v\", ep.name, err)\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := ep.deleteServiceInfoFromCluster(sb, true, \"sbLeave\"); err != nil {\n\t\tlogrus.Warnf(\"Failed to clean up service info on container %s disconnect: %v\", ep.name, err)\n\t}\n\n\tif err := sb.clearNetworkResources(ep); err != nil {\n\t\tlogrus.Warnf(\"Failed to clean up network resources on container %s disconnect: %v\", ep.name, err)\n\t}\n\n\t// Update the store about the sandbox detach only after we\n\t// have completed sb.clearNetworkresources above to avoid\n\t// spurious logs when cleaning up the sandbox when the daemon\n\t// ungracefully exits and restarts before completing sandbox\n\t// detach but after store has been updated.\n\tif err := n.getController().updateToStore(ep); err != nil {\n\t\treturn err\n\t}\n\n\tif e := ep.deleteDriverInfoFromCluster(); e != nil {\n\t\tlogrus.Errorf(\"Failed to delete endpoint state for endpoint %s from cluster: %v\", ep.Name(), e)\n\t}\n\n\tsb.deleteHostsEntries(n.getSvcRecords(ep))\n\tif !sb.inDelete && sb.needDefaultGW() && sb.getEndpointInGWNetwork() == nil {\n\t\treturn sb.setupDefaultGW()\n\t}\n\n\t// New endpoint providing external connectivity for the sandbox\n\textEp = sb.getGatewayEndpoint()\n\tif moveExtConn && extEp != nil {\n\t\tlogrus.Debugf(\"Programming external connectivity on endpoint %s (%s)\", extEp.Name(), extEp.ID())\n\t\textN, err := extEp.getNetworkFromStore()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get network from store for programming external connectivity during leave: %v\", err)\n\t\t}\n\t\textD, err := extN.driver(true)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get driver for programming external connectivity during leave: %v\", err)\n\t\t}\n\t\tif err := extD.ProgramExternalConnectivity(extEp.network.ID(), extEp.ID(), sb.Labels()); err != nil {\n\t\t\tlogrus.Warnf(\"driver failed programming external connectivity on endpoint %s: (%s) %v\",\n\t\t\t\textEp.Name(), extEp.ID(), err)\n\t\t}\n\t}\n\n\tif !sb.needDefaultGW() {\n\t\tif err := sb.clearDefaultGW(); err != nil {\n\t\t\tlogrus.Warnf(\"Failure while disconnecting sandbox %s (%s) from gateway network: %v\",\n\t\t\t\tsb.ID(), sb.ContainerID(), err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) Delete(force bool) error {\n\tvar err error\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get network during Delete: %v\", err)\n\t}\n\n\tep, err = n.getEndpointFromStore(ep.ID())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get endpoint from store during Delete: %v\", err)\n\t}\n\n\tep.Lock()\n\tepid := ep.id\n\tname := ep.name\n\tsbid := ep.sandboxID\n\tep.Unlock()\n\n\tsb, _ := n.getController().SandboxByID(sbid)\n\tif sb != nil && !force {\n\t\treturn &ActiveContainerError{name: name, id: epid}\n\t}\n\n\tif sb != nil {\n\t\tif e := ep.sbLeave(sb.(*sandbox), force); e != nil {\n\t\t\tlogrus.Warnf(\"failed to leave sandbox for endpoint %s : %v\", name, e)\n\t\t}\n\t}\n\n\tif err = n.getController().deleteFromStore(ep); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tif err != nil && !force {\n\t\t\tep.dbExists = false\n\t\t\tif e := n.getController().updateToStore(ep); e != nil {\n\t\t\t\tlogrus.Warnf(\"failed to recreate endpoint in store %s : %v\", name, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// unwatch for service records\n\tn.getController().unWatchSvcRecord(ep)\n\n\tif err = ep.deleteEndpoint(force); err != nil && !force {\n\t\treturn err\n\t}\n\n\tep.releaseAddress()\n\n\tif err := n.getEpCnt().DecEndpointCnt(); err != nil {\n\t\tlogrus.Warnf(\"failed to decrement endpoint count for ep %s: %v\", ep.ID(), err)\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) deleteEndpoint(force bool) error {\n\tep.Lock()\n\tn := ep.network\n\tname := ep.name\n\tepid := ep.id\n\tep.Unlock()\n\n\tdriver, err := n.driver(!force)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to delete endpoint: %v\", err)\n\t}\n\n\tif driver == nil {\n\t\treturn nil\n\t}\n\n\tif err := driver.DeleteEndpoint(n.id, epid); err != nil {\n\t\tif _, ok := err.(types.ForbiddenError); ok {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, ok := err.(types.MaskableError); !ok {\n\t\t\tlogrus.Warnf(\"driver error deleting endpoint %s : %v\", name, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) getSandbox() (*sandbox, bool) {\n\tc := ep.network.getController()\n\tep.Lock()\n\tsid := ep.sandboxID\n\tep.Unlock()\n\n\tc.Lock()\n\tps, ok := c.sandboxes[sid]\n\tc.Unlock()\n\n\treturn ps, ok\n}\n\nfunc (ep *endpoint) getFirstInterfaceIPv4Address() net.IP {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.iface.addr != nil {\n\t\treturn ep.iface.addr.IP\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) getFirstInterfaceIPv6Address() net.IP {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.iface.addrv6 != nil {\n\t\treturn ep.iface.addrv6.IP\n\t}\n\n\treturn nil\n}\n\n// EndpointOptionGeneric function returns an option setter for a Generic option defined\n// in a Dictionary of Key-Value pair\nfunc EndpointOptionGeneric(generic map[string]interface{}) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tfor k, v := range generic {\n\t\t\tep.generic[k] = v\n\t\t}\n\t}\n}\n\nvar (\n\tlinkLocalMask     = net.CIDRMask(16, 32)\n\tlinkLocalMaskIPv6 = net.CIDRMask(64, 128)\n)\n\n// CreateOptionIpam function returns an option setter for the ipam configuration for this endpoint\nfunc CreateOptionIpam(ipV4, ipV6 net.IP, llIPs []net.IP, ipamOptions map[string]string) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.prefAddress = ipV4\n\t\tep.prefAddressV6 = ipV6\n\t\tif len(llIPs) != 0 {\n\t\t\tfor _, ip := range llIPs {\n\t\t\t\tnw := &net.IPNet{IP: ip, Mask: linkLocalMask}\n\t\t\t\tif ip.To4() == nil {\n\t\t\t\t\tnw.Mask = linkLocalMaskIPv6\n\t\t\t\t}\n\t\t\t\tep.iface.llAddrs = append(ep.iface.llAddrs, nw)\n\t\t\t}\n\t\t}\n\t\tep.ipamOptions = ipamOptions\n\t}\n}\n\n// CreateOptionExposedPorts function returns an option setter for the container exposed\n// ports option to be passed to network.CreateEndpoint() method.\nfunc CreateOptionExposedPorts(exposedPorts []types.TransportPort) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\t// Defensive copy\n\t\teps := make([]types.TransportPort, len(exposedPorts))\n\t\tcopy(eps, exposedPorts)\n\t\t// Store endpoint label and in generic because driver needs it\n\t\tep.exposedPorts = eps\n\t\tep.generic[netlabel.ExposedPorts] = eps\n\t}\n}\n\n// CreateOptionPortMapping function returns an option setter for the mapping\n// ports option to be passed to network.CreateEndpoint() method.\nfunc CreateOptionPortMapping(portBindings []types.PortBinding) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\t// Store a copy of the bindings as generic data to pass to the driver\n\t\tpbs := make([]types.PortBinding, len(portBindings))\n\t\tcopy(pbs, portBindings)\n\t\tep.generic[netlabel.PortMap] = pbs\n\t}\n}\n\n// CreateOptionDNS function returns an option setter for dns entry option to\n// be passed to container Create method.\nfunc CreateOptionDNS(dns []string) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.generic[netlabel.DNSServers] = dns\n\t}\n}\n\n// CreateOptionAnonymous function returns an option setter for setting\n// this endpoint as anonymous\nfunc CreateOptionAnonymous() EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.anonymous = true\n\t}\n}\n\n// CreateOptionDisableResolution function returns an option setter to indicate\n// this endpoint doesn't want embedded DNS server functionality\nfunc CreateOptionDisableResolution() EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.disableResolution = true\n\t}\n}\n\n// CreateOptionAlias function returns an option setter for setting endpoint alias\nfunc CreateOptionAlias(name string, alias string) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tif ep.aliases == nil {\n\t\t\tep.aliases = make(map[string]string)\n\t\t}\n\t\tep.aliases[alias] = name\n\t}\n}\n\n// CreateOptionService function returns an option setter for setting service binding configuration\nfunc CreateOptionService(name, id string, vip net.IP, ingressPorts []*PortConfig, aliases []string) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.svcName = name\n\t\tep.svcID = id\n\t\tep.virtualIP = vip\n\t\tep.ingressPorts = ingressPorts\n\t\tep.svcAliases = aliases\n\t}\n}\n\n// CreateOptionMyAlias function returns an option setter for setting endpoint's self alias\nfunc CreateOptionMyAlias(alias string) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.myAliases = append(ep.myAliases, alias)\n\t}\n}\n\n// CreateOptionLoadBalancer function returns an option setter for denoting the endpoint is a load balancer for a network\nfunc CreateOptionLoadBalancer() EndpointOption {\n\treturn func(ep *endpoint) {\n\t\tep.loadBalancer = true\n\t}\n}\n\n// JoinOptionPriority function returns an option setter for priority option to\n// be passed to the endpoint.Join() method.\nfunc JoinOptionPriority(prio int) EndpointOption {\n\treturn func(ep *endpoint) {\n\t\t// ep lock already acquired\n\t\tc := ep.network.getController()\n\t\tc.Lock()\n\t\tsb, ok := c.sandboxes[ep.sandboxID]\n\t\tc.Unlock()\n\t\tif !ok {\n\t\t\tlogrus.Errorf(\"Could not set endpoint priority value during Join to endpoint %s: No sandbox id present in endpoint\", ep.id)\n\t\t\treturn\n\t\t}\n\t\tsb.epPriority[ep.id] = prio\n\t}\n}\n\nfunc (ep *endpoint) DataScope() string {\n\treturn ep.getNetwork().DataScope()\n}\n\nfunc (ep *endpoint) assignAddress(ipam ipamapi.Ipam, assignIPv4, assignIPv6 bool) error {\n\tvar err error\n\n\tn := ep.getNetwork()\n\tif n.hasSpecialDriver() {\n\t\treturn nil\n\t}\n\n\tlogrus.Debugf(\"Assigning addresses for endpoint %s's interface on network %s\", ep.Name(), n.Name())\n\n\tif assignIPv4 {\n\t\tif err = ep.assignAddressVersion(4, ipam); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif assignIPv6 {\n\t\terr = ep.assignAddressVersion(6, ipam)\n\t}\n\n\treturn err\n}\n\nfunc (ep *endpoint) assignAddressVersion(ipVer int, ipam ipamapi.Ipam) error {\n\tvar (\n\t\tpoolID  *string\n\t\taddress **net.IPNet\n\t\tprefAdd net.IP\n\t\tprogAdd net.IP\n\t)\n\n\tn := ep.getNetwork()\n\tswitch ipVer {\n\tcase 4:\n\t\tpoolID = &ep.iface.v4PoolID\n\t\taddress = &ep.iface.addr\n\t\tprefAdd = ep.prefAddress\n\tcase 6:\n\t\tpoolID = &ep.iface.v6PoolID\n\t\taddress = &ep.iface.addrv6\n\t\tprefAdd = ep.prefAddressV6\n\tdefault:\n\t\treturn types.InternalErrorf(\"incorrect ip version number passed: %d\", ipVer)\n\t}\n\n\tipInfo := n.getIPInfo(ipVer)\n\n\t// ipv6 address is not mandatory\n\tif len(ipInfo) == 0 && ipVer == 6 {\n\t\treturn nil\n\t}\n\n\t// The address to program may be chosen by the user or by the network driver in one specific\n\t// case to support backward compatibility with `docker daemon --fixed-cidrv6` use case\n\tif prefAdd != nil {\n\t\tprogAdd = prefAdd\n\t} else if *address != nil {\n\t\tprogAdd = (*address).IP\n\t}\n\n\tfor _, d := range ipInfo {\n\t\tif progAdd != nil && !d.Pool.Contains(progAdd) {\n\t\t\tcontinue\n\t\t}\n\t\taddr, _, err := ipam.RequestAddress(d.PoolID, progAdd, ep.ipamOptions)\n\t\tif err == nil {\n\t\t\tep.Lock()\n\t\t\t*address = addr\n\t\t\t*poolID = d.PoolID\n\t\t\tep.Unlock()\n\t\t\treturn nil\n\t\t}\n\t\tif err != ipamapi.ErrNoAvailableIPs || progAdd != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif progAdd != nil {\n\t\treturn types.BadRequestErrorf(\"Invalid address %s: It does not belong to any of this network's subnets\", prefAdd)\n\t}\n\treturn fmt.Errorf(\"no available IPv%d addresses on this network's address pools: %s (%s)\", ipVer, n.Name(), n.ID())\n}\n\nfunc (ep *endpoint) releaseAddress() {\n\tn := ep.getNetwork()\n\tif n.hasSpecialDriver() {\n\t\treturn\n\t}\n\n\tlogrus.Debugf(\"Releasing addresses for endpoint %s's interface on network %s\", ep.Name(), n.Name())\n\n\tipam, _, err := n.getController().getIPAMDriver(n.ipamType)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to retrieve ipam driver to release interface address on delete of endpoint %s (%s): %v\", ep.Name(), ep.ID(), err)\n\t\treturn\n\t}\n\n\tif ep.iface.addr != nil {\n\t\tif err := ipam.ReleaseAddress(ep.iface.v4PoolID, ep.iface.addr.IP); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to release ip address %s on delete of endpoint %s (%s): %v\", ep.iface.addr.IP, ep.Name(), ep.ID(), err)\n\t\t}\n\t}\n\n\tif ep.iface.addrv6 != nil && ep.iface.addrv6.IP.IsGlobalUnicast() {\n\t\tif err := ipam.ReleaseAddress(ep.iface.v6PoolID, ep.iface.addrv6.IP); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to release ip address %s on delete of endpoint %s (%s): %v\", ep.iface.addrv6.IP, ep.Name(), ep.ID(), err)\n\t\t}\n\t}\n}\n\nfunc (c *controller) cleanupLocalEndpoints() {\n\t// Get used endpoints\n\teps := make(map[string]interface{})\n\tfor _, sb := range c.sandboxes {\n\t\tfor _, ep := range sb.endpoints {\n\t\t\teps[ep.id] = true\n\t\t}\n\t}\n\tnl, err := c.getNetworksForScope(datastore.LocalScope)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Could not get list of networks during endpoint cleanup: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, n := range nl {\n\t\tif n.ConfigOnly() {\n\t\t\tcontinue\n\t\t}\n\t\tepl, err := n.getEndpointsFromStore()\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Could not get list of endpoints in network %s during endpoint cleanup: %v\", n.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, ep := range epl {\n\t\t\tif _, ok := eps[ep.id]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tlogrus.Infof(\"Removing stale endpoint %s (%s)\", ep.name, ep.id)\n\t\t\tif err := ep.Delete(true); err != nil {\n\t\t\t\tlogrus.Warnf(\"Could not delete local endpoint %s during endpoint cleanup: %v\", ep.name, err)\n\t\t\t}\n\t\t}\n\n\t\tepl, err = n.getEndpointsFromStore()\n\t\tif err != nil {\n\t\t\tlogrus.Warnf(\"Could not get list of endpoints in network %s for count update: %v\", n.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tepCnt := n.getEpCnt().EndpointCnt()\n\t\tif epCnt != uint64(len(epl)) {\n\t\t\tlogrus.Infof(\"Fixing inconsistent endpoint_cnt for network %s. Expected=%d, Actual=%d\", n.name, len(epl), epCnt)\n\t\t\tn.getEpCnt().setCnt(uint64(len(epl)))\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "endpoint_cnt.go",
          "type": "blob",
          "size": 3.3603515625,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/datastore\"\n)\n\ntype endpointCnt struct {\n\tn        *network\n\tCount    uint64\n\tdbIndex  uint64\n\tdbExists bool\n\tsync.Mutex\n}\n\nconst epCntKeyPrefix = \"endpoint_count\"\n\nfunc (ec *endpointCnt) Key() []string {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\treturn []string{epCntKeyPrefix, ec.n.id}\n}\n\nfunc (ec *endpointCnt) KeyPrefix() []string {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\treturn []string{epCntKeyPrefix, ec.n.id}\n}\n\nfunc (ec *endpointCnt) Value() []byte {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\tb, err := json.Marshal(ec)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn b\n}\n\nfunc (ec *endpointCnt) SetValue(value []byte) error {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\treturn json.Unmarshal(value, &ec)\n}\n\nfunc (ec *endpointCnt) Index() uint64 {\n\tec.Lock()\n\tdefer ec.Unlock()\n\treturn ec.dbIndex\n}\n\nfunc (ec *endpointCnt) SetIndex(index uint64) {\n\tec.Lock()\n\tec.dbIndex = index\n\tec.dbExists = true\n\tec.Unlock()\n}\n\nfunc (ec *endpointCnt) Exists() bool {\n\tec.Lock()\n\tdefer ec.Unlock()\n\treturn ec.dbExists\n}\n\nfunc (ec *endpointCnt) Skip() bool {\n\tec.Lock()\n\tdefer ec.Unlock()\n\treturn !ec.n.persist\n}\n\nfunc (ec *endpointCnt) New() datastore.KVObject {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\treturn &endpointCnt{\n\t\tn: ec.n,\n\t}\n}\n\nfunc (ec *endpointCnt) CopyTo(o datastore.KVObject) error {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\tdstEc := o.(*endpointCnt)\n\tdstEc.n = ec.n\n\tdstEc.Count = ec.Count\n\tdstEc.dbExists = ec.dbExists\n\tdstEc.dbIndex = ec.dbIndex\n\n\treturn nil\n}\n\nfunc (ec *endpointCnt) DataScope() string {\n\treturn ec.n.DataScope()\n}\n\nfunc (ec *endpointCnt) EndpointCnt() uint64 {\n\tec.Lock()\n\tdefer ec.Unlock()\n\n\treturn ec.Count\n}\n\nfunc (ec *endpointCnt) updateStore() error {\n\tstore := ec.n.getController().getStore(ec.DataScope())\n\tif store == nil {\n\t\treturn fmt.Errorf(\"store not found for scope %s on endpoint count update\", ec.DataScope())\n\t}\n\t// make a copy of count and n to avoid being overwritten by store.GetObject\n\tcount := ec.EndpointCnt()\n\tn := ec.n\n\tfor {\n\t\tif err := ec.n.getController().updateToStore(ec); err == nil || err != datastore.ErrKeyModified {\n\t\t\treturn err\n\t\t}\n\t\tif err := store.GetObject(datastore.Key(ec.Key()...), ec); err != nil {\n\t\t\treturn fmt.Errorf(\"could not update the kvobject to latest on endpoint count update: %v\", err)\n\t\t}\n\t\tec.Lock()\n\t\tec.Count = count\n\t\tec.n = n\n\t\tec.Unlock()\n\t}\n}\n\nfunc (ec *endpointCnt) setCnt(cnt uint64) error {\n\tec.Lock()\n\tec.Count = cnt\n\tec.Unlock()\n\treturn ec.updateStore()\n}\n\nfunc (ec *endpointCnt) atomicIncDecEpCnt(inc bool) error {\n\tstore := ec.n.getController().getStore(ec.DataScope())\n\tif store == nil {\n\t\treturn fmt.Errorf(\"store not found for scope %s\", ec.DataScope())\n\t}\n\n\ttmp := &endpointCnt{n: ec.n}\n\tif err := store.GetObject(datastore.Key(ec.Key()...), tmp); err != nil {\n\t\treturn err\n\t}\nretry:\n\tec.Lock()\n\tif inc {\n\t\tec.Count++\n\t} else {\n\t\tif ec.Count > 0 {\n\t\t\tec.Count--\n\t\t}\n\t}\n\tec.Unlock()\n\n\tif err := ec.n.getController().updateToStore(ec); err != nil {\n\t\tif err == datastore.ErrKeyModified {\n\t\t\tif err := store.GetObject(datastore.Key(ec.Key()...), ec); err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not update the kvobject to latest when trying to atomic add endpoint count: %v\", err)\n\t\t\t}\n\n\t\t\tgoto retry\n\t\t}\n\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (ec *endpointCnt) IncEndpointCnt() error {\n\treturn ec.atomicIncDecEpCnt(true)\n}\n\nfunc (ec *endpointCnt) DecEndpointCnt() error {\n\treturn ec.atomicIncDecEpCnt(false)\n}\n"
        },
        {
          "name": "endpoint_info.go",
          "type": "blob",
          "size": 11.302734375,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/types\"\n)\n\n// EndpointInfo provides an interface to retrieve network resources bound to the endpoint.\ntype EndpointInfo interface {\n\t// Iface returns InterfaceInfo, go interface that can be used\n\t// to get more information on the interface which was assigned to\n\t// the endpoint by the driver. This can be used after the\n\t// endpoint has been created.\n\tIface() InterfaceInfo\n\n\t// Gateway returns the IPv4 gateway assigned by the driver.\n\t// This will only return a valid value if a container has joined the endpoint.\n\tGateway() net.IP\n\n\t// GatewayIPv6 returns the IPv6 gateway assigned by the driver.\n\t// This will only return a valid value if a container has joined the endpoint.\n\tGatewayIPv6() net.IP\n\n\t// StaticRoutes returns the list of static routes configured by the network\n\t// driver when the container joins a network\n\tStaticRoutes() []*types.StaticRoute\n\n\t// Sandbox returns the attached sandbox if there, nil otherwise.\n\tSandbox() Sandbox\n\n\t// LoadBalancer returns whether the endpoint is the load balancer endpoint for the network.\n\tLoadBalancer() bool\n}\n\n// InterfaceInfo provides an interface to retrieve interface addresses bound to the endpoint.\ntype InterfaceInfo interface {\n\t// MacAddress returns the MAC address assigned to the endpoint.\n\tMacAddress() net.HardwareAddr\n\n\t// Address returns the IPv4 address assigned to the endpoint.\n\tAddress() *net.IPNet\n\n\t// AddressIPv6 returns the IPv6 address assigned to the endpoint.\n\tAddressIPv6() *net.IPNet\n\n\t// LinkLocalAddresses returns the list of link-local (IPv4/IPv6) addresses assigned to the endpoint.\n\tLinkLocalAddresses() []*net.IPNet\n\n\t// SrcName returns the name of the interface w/in the container\n\tSrcName() string\n}\n\ntype endpointInterface struct {\n\tmac       net.HardwareAddr\n\taddr      *net.IPNet\n\taddrv6    *net.IPNet\n\tllAddrs   []*net.IPNet\n\tsrcName   string\n\tdstPrefix string\n\troutes    []*net.IPNet\n\tv4PoolID  string\n\tv6PoolID  string\n}\n\nfunc (epi *endpointInterface) MarshalJSON() ([]byte, error) {\n\tepMap := make(map[string]interface{})\n\tif epi.mac != nil {\n\t\tepMap[\"mac\"] = epi.mac.String()\n\t}\n\tif epi.addr != nil {\n\t\tepMap[\"addr\"] = epi.addr.String()\n\t}\n\tif epi.addrv6 != nil {\n\t\tepMap[\"addrv6\"] = epi.addrv6.String()\n\t}\n\tif len(epi.llAddrs) != 0 {\n\t\tlist := make([]string, 0, len(epi.llAddrs))\n\t\tfor _, ll := range epi.llAddrs {\n\t\t\tlist = append(list, ll.String())\n\t\t}\n\t\tepMap[\"llAddrs\"] = list\n\t}\n\tepMap[\"srcName\"] = epi.srcName\n\tepMap[\"dstPrefix\"] = epi.dstPrefix\n\tvar routes []string\n\tfor _, route := range epi.routes {\n\t\troutes = append(routes, route.String())\n\t}\n\tepMap[\"routes\"] = routes\n\tepMap[\"v4PoolID\"] = epi.v4PoolID\n\tepMap[\"v6PoolID\"] = epi.v6PoolID\n\treturn json.Marshal(epMap)\n}\n\nfunc (epi *endpointInterface) UnmarshalJSON(b []byte) error {\n\tvar (\n\t\terr   error\n\t\tepMap map[string]interface{}\n\t)\n\tif err = json.Unmarshal(b, &epMap); err != nil {\n\t\treturn err\n\t}\n\tif v, ok := epMap[\"mac\"]; ok {\n\t\tif epi.mac, err = net.ParseMAC(v.(string)); err != nil {\n\t\t\treturn types.InternalErrorf(\"failed to decode endpoint interface mac address after json unmarshal: %s\", v.(string))\n\t\t}\n\t}\n\tif v, ok := epMap[\"addr\"]; ok {\n\t\tif epi.addr, err = types.ParseCIDR(v.(string)); err != nil {\n\t\t\treturn types.InternalErrorf(\"failed to decode endpoint interface ipv4 address after json unmarshal: %v\", err)\n\t\t}\n\t}\n\tif v, ok := epMap[\"addrv6\"]; ok {\n\t\tif epi.addrv6, err = types.ParseCIDR(v.(string)); err != nil {\n\t\t\treturn types.InternalErrorf(\"failed to decode endpoint interface ipv6 address after json unmarshal: %v\", err)\n\t\t}\n\t}\n\tif v, ok := epMap[\"llAddrs\"]; ok {\n\t\tlist := v.([]interface{})\n\t\tepi.llAddrs = make([]*net.IPNet, 0, len(list))\n\t\tfor _, llS := range list {\n\t\t\tll, err := types.ParseCIDR(llS.(string))\n\t\t\tif err != nil {\n\t\t\t\treturn types.InternalErrorf(\"failed to decode endpoint interface link-local address (%v) after json unmarshal: %v\", llS, err)\n\t\t\t}\n\t\t\tepi.llAddrs = append(epi.llAddrs, ll)\n\t\t}\n\t}\n\tepi.srcName = epMap[\"srcName\"].(string)\n\tepi.dstPrefix = epMap[\"dstPrefix\"].(string)\n\n\trb, _ := json.Marshal(epMap[\"routes\"])\n\tvar routes []string\n\tjson.Unmarshal(rb, &routes)\n\tepi.routes = make([]*net.IPNet, 0)\n\tfor _, route := range routes {\n\t\tip, ipr, err := net.ParseCIDR(route)\n\t\tif err == nil {\n\t\t\tipr.IP = ip\n\t\t\tepi.routes = append(epi.routes, ipr)\n\t\t}\n\t}\n\tepi.v4PoolID = epMap[\"v4PoolID\"].(string)\n\tepi.v6PoolID = epMap[\"v6PoolID\"].(string)\n\n\treturn nil\n}\n\nfunc (epi *endpointInterface) CopyTo(dstEpi *endpointInterface) error {\n\tdstEpi.mac = types.GetMacCopy(epi.mac)\n\tdstEpi.addr = types.GetIPNetCopy(epi.addr)\n\tdstEpi.addrv6 = types.GetIPNetCopy(epi.addrv6)\n\tdstEpi.srcName = epi.srcName\n\tdstEpi.dstPrefix = epi.dstPrefix\n\tdstEpi.v4PoolID = epi.v4PoolID\n\tdstEpi.v6PoolID = epi.v6PoolID\n\tif len(epi.llAddrs) != 0 {\n\t\tdstEpi.llAddrs = make([]*net.IPNet, 0, len(epi.llAddrs))\n\t\tdstEpi.llAddrs = append(dstEpi.llAddrs, epi.llAddrs...)\n\t}\n\n\tfor _, route := range epi.routes {\n\t\tdstEpi.routes = append(dstEpi.routes, types.GetIPNetCopy(route))\n\t}\n\n\treturn nil\n}\n\ntype endpointJoinInfo struct {\n\tgw                    net.IP\n\tgw6                   net.IP\n\tStaticRoutes          []*types.StaticRoute\n\tdriverTableEntries    []*tableEntry\n\tdisableGatewayService bool\n}\n\ntype tableEntry struct {\n\ttableName string\n\tkey       string\n\tvalue     []byte\n}\n\nfunc (ep *endpoint) Info() EndpointInfo {\n\tif ep.sandboxID != \"\" {\n\t\treturn ep\n\t}\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tep, err = n.getEndpointFromStore(ep.ID())\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tsb, ok := ep.getSandbox()\n\tif !ok {\n\t\t// endpoint hasn't joined any sandbox.\n\t\t// Just return the endpoint\n\t\treturn ep\n\t}\n\n\treturn sb.getEndpoint(ep.ID())\n}\n\nfunc (ep *endpoint) Iface() InterfaceInfo {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.iface != nil {\n\t\treturn ep.iface\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) Interface() driverapi.InterfaceInfo {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.iface != nil {\n\t\treturn ep.iface\n\t}\n\n\treturn nil\n}\n\nfunc (epi *endpointInterface) SetMacAddress(mac net.HardwareAddr) error {\n\tif epi.mac != nil {\n\t\treturn types.ForbiddenErrorf(\"endpoint interface MAC address present (%s). Cannot be modified with %s.\", epi.mac, mac)\n\t}\n\tif mac == nil {\n\t\treturn types.BadRequestErrorf(\"tried to set nil MAC address to endpoint interface\")\n\t}\n\tepi.mac = types.GetMacCopy(mac)\n\treturn nil\n}\n\nfunc (epi *endpointInterface) SetIPAddress(address *net.IPNet) error {\n\tif address.IP == nil {\n\t\treturn types.BadRequestErrorf(\"tried to set nil IP address to endpoint interface\")\n\t}\n\tif address.IP.To4() == nil {\n\t\treturn setAddress(&epi.addrv6, address)\n\t}\n\treturn setAddress(&epi.addr, address)\n}\n\nfunc setAddress(ifaceAddr **net.IPNet, address *net.IPNet) error {\n\tif *ifaceAddr != nil {\n\t\treturn types.ForbiddenErrorf(\"endpoint interface IP present (%s). Cannot be modified with (%s).\", *ifaceAddr, address)\n\t}\n\t*ifaceAddr = types.GetIPNetCopy(address)\n\treturn nil\n}\n\nfunc (epi *endpointInterface) MacAddress() net.HardwareAddr {\n\treturn types.GetMacCopy(epi.mac)\n}\n\nfunc (epi *endpointInterface) Address() *net.IPNet {\n\treturn types.GetIPNetCopy(epi.addr)\n}\n\nfunc (epi *endpointInterface) AddressIPv6() *net.IPNet {\n\treturn types.GetIPNetCopy(epi.addrv6)\n}\n\nfunc (epi *endpointInterface) LinkLocalAddresses() []*net.IPNet {\n\treturn epi.llAddrs\n}\n\nfunc (epi *endpointInterface) SrcName() string {\n\treturn epi.srcName\n}\n\nfunc (epi *endpointInterface) SetNames(srcName string, dstPrefix string) error {\n\tepi.srcName = srcName\n\tepi.dstPrefix = dstPrefix\n\treturn nil\n}\n\nfunc (ep *endpoint) InterfaceName() driverapi.InterfaceNameInfo {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.iface != nil {\n\t\treturn ep.iface\n\t}\n\n\treturn nil\n}\n\nfunc (ep *endpoint) AddStaticRoute(destination *net.IPNet, routeType int, nextHop net.IP) error {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tr := types.StaticRoute{Destination: destination, RouteType: routeType, NextHop: nextHop}\n\n\tif routeType == types.NEXTHOP {\n\t\t// If the route specifies a next-hop, then it's loosely routed (i.e. not bound to a particular interface).\n\t\tep.joinInfo.StaticRoutes = append(ep.joinInfo.StaticRoutes, &r)\n\t} else {\n\t\t// If the route doesn't specify a next-hop, it must be a connected route, bound to an interface.\n\t\tep.iface.routes = append(ep.iface.routes, r.Destination)\n\t}\n\treturn nil\n}\n\nfunc (ep *endpoint) AddTableEntry(tableName, key string, value []byte) error {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tep.joinInfo.driverTableEntries = append(ep.joinInfo.driverTableEntries, &tableEntry{\n\t\ttableName: tableName,\n\t\tkey:       key,\n\t\tvalue:     value,\n\t})\n\n\treturn nil\n}\n\nfunc (ep *endpoint) Sandbox() Sandbox {\n\tcnt, ok := ep.getSandbox()\n\tif !ok {\n\t\treturn nil\n\t}\n\treturn cnt\n}\n\nfunc (ep *endpoint) LoadBalancer() bool {\n\tep.Lock()\n\tdefer ep.Unlock()\n\treturn ep.loadBalancer\n}\n\nfunc (ep *endpoint) StaticRoutes() []*types.StaticRoute {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.joinInfo == nil {\n\t\treturn nil\n\t}\n\n\treturn ep.joinInfo.StaticRoutes\n}\n\nfunc (ep *endpoint) Gateway() net.IP {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.joinInfo == nil {\n\t\treturn net.IP{}\n\t}\n\n\treturn types.GetIPCopy(ep.joinInfo.gw)\n}\n\nfunc (ep *endpoint) GatewayIPv6() net.IP {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tif ep.joinInfo == nil {\n\t\treturn net.IP{}\n\t}\n\n\treturn types.GetIPCopy(ep.joinInfo.gw6)\n}\n\nfunc (ep *endpoint) SetGateway(gw net.IP) error {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tep.joinInfo.gw = types.GetIPCopy(gw)\n\treturn nil\n}\n\nfunc (ep *endpoint) SetGatewayIPv6(gw6 net.IP) error {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tep.joinInfo.gw6 = types.GetIPCopy(gw6)\n\treturn nil\n}\n\nfunc (ep *endpoint) retrieveFromStore() (*endpoint, error) {\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not find network in store to get latest endpoint %s: %v\", ep.Name(), err)\n\t}\n\treturn n.getEndpointFromStore(ep.ID())\n}\n\nfunc (ep *endpoint) DisableGatewayService() {\n\tep.Lock()\n\tdefer ep.Unlock()\n\n\tep.joinInfo.disableGatewayService = true\n}\n\nfunc (epj *endpointJoinInfo) MarshalJSON() ([]byte, error) {\n\tepMap := make(map[string]interface{})\n\tif epj.gw != nil {\n\t\tepMap[\"gw\"] = epj.gw.String()\n\t}\n\tif epj.gw6 != nil {\n\t\tepMap[\"gw6\"] = epj.gw6.String()\n\t}\n\tepMap[\"disableGatewayService\"] = epj.disableGatewayService\n\tepMap[\"StaticRoutes\"] = epj.StaticRoutes\n\treturn json.Marshal(epMap)\n}\n\nfunc (epj *endpointJoinInfo) UnmarshalJSON(b []byte) error {\n\tvar (\n\t\terr   error\n\t\tepMap map[string]interface{}\n\t)\n\tif err = json.Unmarshal(b, &epMap); err != nil {\n\t\treturn err\n\t}\n\tif v, ok := epMap[\"gw\"]; ok {\n\t\tepj.gw = net.ParseIP(v.(string))\n\t}\n\tif v, ok := epMap[\"gw6\"]; ok {\n\t\tepj.gw6 = net.ParseIP(v.(string))\n\t}\n\tepj.disableGatewayService = epMap[\"disableGatewayService\"].(bool)\n\n\tvar tStaticRoute []types.StaticRoute\n\tif v, ok := epMap[\"StaticRoutes\"]; ok {\n\t\ttb, _ := json.Marshal(v)\n\t\tvar tStaticRoute []types.StaticRoute\n\t\tjson.Unmarshal(tb, &tStaticRoute)\n\t}\n\tvar StaticRoutes []*types.StaticRoute\n\tfor _, r := range tStaticRoute {\n\t\tStaticRoutes = append(StaticRoutes, &r)\n\t}\n\tepj.StaticRoutes = StaticRoutes\n\n\treturn nil\n}\n\nfunc (epj *endpointJoinInfo) CopyTo(dstEpj *endpointJoinInfo) error {\n\tdstEpj.disableGatewayService = epj.disableGatewayService\n\tdstEpj.StaticRoutes = make([]*types.StaticRoute, len(epj.StaticRoutes))\n\tcopy(dstEpj.StaticRoutes, epj.StaticRoutes)\n\tdstEpj.driverTableEntries = make([]*tableEntry, len(epj.driverTableEntries))\n\tcopy(dstEpj.driverTableEntries, epj.driverTableEntries)\n\tdstEpj.gw = types.GetIPCopy(epj.gw)\n\tdstEpj.gw6 = types.GetIPCopy(epj.gw6)\n\treturn nil\n}\n"
        },
        {
          "name": "endpoint_info_unix.go",
          "type": "blob",
          "size": 0.6591796875,
          "content": "//go:build !windows\n// +build !windows\n\npackage libnetwork\n\nimport \"fmt\"\n\nfunc (ep *endpoint) DriverInfo() (map[string]interface{}, error) {\n\tep, err := ep.retrieveFromStore()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif sb, ok := ep.getSandbox(); ok {\n\t\tif gwep := sb.getEndpointInGWNetwork(); gwep != nil && gwep.ID() != ep.ID() {\n\t\t\treturn gwep.DriverInfo()\n\t\t}\n\t}\n\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not find network in store for driver info: %v\", err)\n\t}\n\n\tdriver, err := n.driver(true)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get driver info: %v\", err)\n\t}\n\n\treturn driver.EndpointOperInfo(n.ID(), ep.ID())\n}\n"
        },
        {
          "name": "endpoint_info_windows.go",
          "type": "blob",
          "size": 0.9033203125,
          "content": "//go:build windows\n// +build windows\n\npackage libnetwork\n\nimport \"fmt\"\n\nfunc (ep *endpoint) DriverInfo() (map[string]interface{}, error) {\n\tep, err := ep.retrieveFromStore()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar gwDriverInfo map[string]interface{}\n\tif sb, ok := ep.getSandbox(); ok {\n\t\tif gwep := sb.getEndpointInGWNetwork(); gwep != nil && gwep.ID() != ep.ID() {\n\n\t\t\tgwDriverInfo, err = gwep.DriverInfo()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tn, err := ep.getNetworkFromStore()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not find network in store for driver info: %v\", err)\n\t}\n\n\tdriver, err := n.driver(true)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get driver info: %v\", err)\n\t}\n\n\tepInfo, err := driver.EndpointOperInfo(n.ID(), ep.ID())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif epInfo != nil {\n\t\tepInfo[\"GW_INFO\"] = gwDriverInfo\n\t\treturn epInfo, nil\n\t}\n\n\treturn gwDriverInfo, nil\n}\n"
        },
        {
          "name": "endpoint_test.go",
          "type": "blob",
          "size": 1.7314453125,
          "content": "//go:build !windows\n// +build !windows\n\npackage libnetwork\n\nimport (\n\t\"io/ioutil\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/docker/libnetwork/testutils\"\n)\n\nfunc TestHostsEntries(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\texpectedHostsFile := `127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nff00::0\tip6-mcastprefix\nff02::1\tip6-allnodes\nff02::2\tip6-allrouters\n192.168.222.2\tsomehost.example.com somehost\nfe90::2\tsomehost.example.com somehost\n`\n\n\topts := []NetworkOption{NetworkOptionEnableIPv6(true), NetworkOptionIpam(ipamapi.DefaultIPAM, \"\",\n\t\t[]*IpamConf{{PreferredPool: \"192.168.222.0/24\", Gateway: \"192.168.222.1\"}},\n\t\t[]*IpamConf{{PreferredPool: \"fe90::/64\", Gateway: \"fe90::1\"}},\n\t\tnil)}\n\n\tc, nws := getTestEnv(t, opts)\n\tctrlr := c.(*controller)\n\n\thostsFile, err := ioutil.TempFile(\"\", \"\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer os.Remove(hostsFile.Name())\n\n\tsbx, err := ctrlr.NewSandbox(\"sandbox1\", OptionHostsPath(hostsFile.Name()), OptionHostname(\"somehost.example.com\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep1, err := nws[0].CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep1.Join(sbx, JoinOptionPriority(1)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdata, err := ioutil.ReadFile(hostsFile.Name())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif string(data) != expectedHostsFile {\n\t\tt.Fatalf(\"expected the hosts file to read:\\n%q\\nbut instead got the following:\\n%q\\n\", expectedHostsFile, string(data))\n\t}\n\n\tif err := sbx.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(ctrlr.sandboxes) != 0 {\n\t\tt.Fatalf(\"controller sandboxes is not empty. len = %d\", len(ctrlr.sandboxes))\n\t}\n\n\tosl.GC()\n}\n"
        },
        {
          "name": "error.go",
          "type": "blob",
          "size": 5.4755859375,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n)\n\n// ErrNoSuchNetwork is returned when a network query finds no result\ntype ErrNoSuchNetwork string\n\nfunc (nsn ErrNoSuchNetwork) Error() string {\n\treturn fmt.Sprintf(\"network %s not found\", string(nsn))\n}\n\n// NotFound denotes the type of this error\nfunc (nsn ErrNoSuchNetwork) NotFound() {}\n\n// ErrNoSuchEndpoint is returned when an endpoint query finds no result\ntype ErrNoSuchEndpoint string\n\nfunc (nse ErrNoSuchEndpoint) Error() string {\n\treturn fmt.Sprintf(\"endpoint %s not found\", string(nse))\n}\n\n// NotFound denotes the type of this error\nfunc (nse ErrNoSuchEndpoint) NotFound() {}\n\n// ErrInvalidNetworkDriver is returned if an invalid driver\n// name is passed.\ntype ErrInvalidNetworkDriver string\n\nfunc (ind ErrInvalidNetworkDriver) Error() string {\n\treturn fmt.Sprintf(\"invalid driver bound to network: %s\", string(ind))\n}\n\n// BadRequest denotes the type of this error\nfunc (ind ErrInvalidNetworkDriver) BadRequest() {}\n\n// ErrInvalidJoin is returned if a join is attempted on an endpoint\n// which already has a container joined.\ntype ErrInvalidJoin struct{}\n\nfunc (ij ErrInvalidJoin) Error() string {\n\treturn \"a container has already joined the endpoint\"\n}\n\n// BadRequest denotes the type of this error\nfunc (ij ErrInvalidJoin) BadRequest() {}\n\n// ErrNoContainer is returned when the endpoint has no container\n// attached to it.\ntype ErrNoContainer struct{}\n\nfunc (nc ErrNoContainer) Error() string {\n\treturn \"no container is attached to the endpoint\"\n}\n\n// Maskable denotes the type of this error\nfunc (nc ErrNoContainer) Maskable() {}\n\n// ErrInvalidID is returned when a query-by-id method is being invoked\n// with an empty id parameter\ntype ErrInvalidID string\n\nfunc (ii ErrInvalidID) Error() string {\n\treturn fmt.Sprintf(\"invalid id: %s\", string(ii))\n}\n\n// BadRequest denotes the type of this error\nfunc (ii ErrInvalidID) BadRequest() {}\n\n// ErrInvalidName is returned when a query-by-name or resource create method is\n// invoked with an empty name parameter\ntype ErrInvalidName string\n\nfunc (in ErrInvalidName) Error() string {\n\treturn fmt.Sprintf(\"invalid name: %s\", string(in))\n}\n\n// BadRequest denotes the type of this error\nfunc (in ErrInvalidName) BadRequest() {}\n\n// ErrInvalidConfigFile type is returned when an invalid LibNetwork config file is detected\ntype ErrInvalidConfigFile string\n\nfunc (cf ErrInvalidConfigFile) Error() string {\n\treturn fmt.Sprintf(\"Invalid Config file %q\", string(cf))\n}\n\n// NetworkTypeError type is returned when the network type string is not\n// known to libnetwork.\ntype NetworkTypeError string\n\nfunc (nt NetworkTypeError) Error() string {\n\treturn fmt.Sprintf(\"unknown driver %q\", string(nt))\n}\n\n// NotFound denotes the type of this error\nfunc (nt NetworkTypeError) NotFound() {}\n\n// NetworkNameError is returned when a network with the same name already exists.\ntype NetworkNameError string\n\nfunc (nnr NetworkNameError) Error() string {\n\treturn fmt.Sprintf(\"network with name %s already exists\", string(nnr))\n}\n\n// Forbidden denotes the type of this error\nfunc (nnr NetworkNameError) Forbidden() {}\n\n// UnknownNetworkError is returned when libnetwork could not find in its database\n// a network with the same name and id.\ntype UnknownNetworkError struct {\n\tname string\n\tid   string\n}\n\nfunc (une *UnknownNetworkError) Error() string {\n\treturn fmt.Sprintf(\"unknown network %s id %s\", une.name, une.id)\n}\n\n// NotFound denotes the type of this error\nfunc (une *UnknownNetworkError) NotFound() {}\n\n// ActiveEndpointsError is returned when a network is deleted which has active\n// endpoints in it.\ntype ActiveEndpointsError struct {\n\tname string\n\tid   string\n}\n\nfunc (aee *ActiveEndpointsError) Error() string {\n\treturn fmt.Sprintf(\"network %s id %s has active endpoints\", aee.name, aee.id)\n}\n\n// Forbidden denotes the type of this error\nfunc (aee *ActiveEndpointsError) Forbidden() {}\n\n// UnknownEndpointError is returned when libnetwork could not find in its database\n// an endpoint with the same name and id.\ntype UnknownEndpointError struct {\n\tname string\n\tid   string\n}\n\nfunc (uee *UnknownEndpointError) Error() string {\n\treturn fmt.Sprintf(\"unknown endpoint %s id %s\", uee.name, uee.id)\n}\n\n// NotFound denotes the type of this error\nfunc (uee *UnknownEndpointError) NotFound() {}\n\n// ActiveContainerError is returned when an endpoint is deleted which has active\n// containers attached to it.\ntype ActiveContainerError struct {\n\tname string\n\tid   string\n}\n\nfunc (ace *ActiveContainerError) Error() string {\n\treturn fmt.Sprintf(\"endpoint with name %s id %s has active containers\", ace.name, ace.id)\n}\n\n// Forbidden denotes the type of this error\nfunc (ace *ActiveContainerError) Forbidden() {}\n\n// InvalidContainerIDError is returned when an invalid container id is passed\n// in Join/Leave\ntype InvalidContainerIDError string\n\nfunc (id InvalidContainerIDError) Error() string {\n\treturn fmt.Sprintf(\"invalid container id %s\", string(id))\n}\n\n// BadRequest denotes the type of this error\nfunc (id InvalidContainerIDError) BadRequest() {}\n\n// ManagerRedirectError is returned when the request should be redirected to Manager\ntype ManagerRedirectError string\n\nfunc (mr ManagerRedirectError) Error() string {\n\treturn \"Redirect the request to the manager\"\n}\n\n// Maskable denotes the type of this error\nfunc (mr ManagerRedirectError) Maskable() {}\n\n// ErrDataStoreNotInitialized is returned if an invalid data scope is passed\n// for getting data store\ntype ErrDataStoreNotInitialized string\n\nfunc (dsni ErrDataStoreNotInitialized) Error() string {\n\treturn fmt.Sprintf(\"datastore for scope %q is not initialized\", string(dsni))\n}\n"
        },
        {
          "name": "errors_test.go",
          "type": "blob",
          "size": 1.3876953125,
          "content": "package libnetwork\n\nimport (\n\t\"testing\"\n\n\t\"github.com/docker/libnetwork/types\"\n)\n\nfunc TestErrorInterfaces(t *testing.T) {\n\n\tbadRequestErrorList := []error{ErrInvalidID(\"\"), ErrInvalidName(\"\"), ErrInvalidJoin{}, ErrInvalidNetworkDriver(\"\"), InvalidContainerIDError(\"\"), ErrNoSuchNetwork(\"\"), ErrNoSuchEndpoint(\"\")}\n\tfor _, err := range badRequestErrorList {\n\t\tswitch u := err.(type) {\n\t\tcase types.BadRequestError:\n\t\t\treturn\n\t\tdefault:\n\t\t\tt.Fatalf(\"Failed to detect err %v is of type BadRequestError. Got type: %T\", err, u)\n\t\t}\n\t}\n\n\tmaskableErrorList := []error{ErrNoContainer{}}\n\tfor _, err := range maskableErrorList {\n\t\tswitch u := err.(type) {\n\t\tcase types.MaskableError:\n\t\t\treturn\n\t\tdefault:\n\t\t\tt.Fatalf(\"Failed to detect err %v is of type MaskableError. Got type: %T\", err, u)\n\t\t}\n\t}\n\n\tnotFoundErrorList := []error{NetworkTypeError(\"\"), &UnknownNetworkError{}, &UnknownEndpointError{}}\n\tfor _, err := range notFoundErrorList {\n\t\tswitch u := err.(type) {\n\t\tcase types.NotFoundError:\n\t\t\treturn\n\t\tdefault:\n\t\t\tt.Fatalf(\"Failed to detect err %v is of type NotFoundError. Got type: %T\", err, u)\n\t\t}\n\t}\n\n\tforbiddenErrorList := []error{NetworkTypeError(\"\"), &UnknownNetworkError{}, &UnknownEndpointError{}}\n\tfor _, err := range forbiddenErrorList {\n\t\tswitch u := err.(type) {\n\t\tcase types.ForbiddenError:\n\t\t\treturn\n\t\tdefault:\n\t\t\tt.Fatalf(\"Failed to detect err %v is of type ForbiddenError. Got type: %T\", err, u)\n\t\t}\n\t}\n\n}\n"
        },
        {
          "name": "etchosts",
          "type": "tree",
          "content": null
        },
        {
          "name": "firewall_linux.go",
          "type": "blob",
          "size": 1.2626953125,
          "content": "package libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/iptables\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst userChain = \"DOCKER-USER\"\n\nvar (\n\tctrl *controller = nil\n)\n\nfunc setupArrangeUserFilterRule(c *controller) {\n\tctrl = c\n\tiptables.OnReloaded(arrangeUserFilterRule)\n}\n\n// This chain allow users to configure firewall policies in a way that persists\n// docker operations/restarts. Docker will not delete or modify any pre-existing\n// rules from the DOCKER-USER filter chain.\n// Note once DOCKER-USER chain is created, docker engine does not remove it when\n// IPTableForwarding is disabled, because it contains rules configured by user that\n// are beyond docker engine's control.\nfunc arrangeUserFilterRule() {\n\tif ctrl == nil || !ctrl.iptablesEnabled() {\n\t\treturn\n\t}\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\t_, err := iptable.NewChain(userChain, iptables.Filter, false)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to create %s chain: %v\", userChain, err)\n\t\treturn\n\t}\n\n\tif err = iptable.AddReturnRule(userChain); err != nil {\n\t\tlogrus.Warnf(\"Failed to add the RETURN rule for %s: %v\", userChain, err)\n\t\treturn\n\t}\n\n\terr = iptable.EnsureJumpRule(\"FORWARD\", userChain)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to ensure the jump rule for %s: %v\", userChain, err)\n\t}\n}\n"
        },
        {
          "name": "firewall_others.go",
          "type": "blob",
          "size": 0.15234375,
          "content": "//go:build !linux\n// +build !linux\n\npackage libnetwork\n\nfunc setupArrangeUserFilterRule(c *controller) {}\nfunc arrangeUserFilterRule()                   {}\n"
        },
        {
          "name": "firewall_test.go",
          "type": "blob",
          "size": 2.4765625,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/docker/libnetwork/iptables\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"gotest.tools/v3/assert\"\n)\n\nconst (\n\tfwdChainName = \"FORWARD\"\n\tusrChainName = userChain\n)\n\nfunc TestUserChain(t *testing.T) {\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\n\tnc, err := New()\n\tassert.NilError(t, err)\n\n\ttests := []struct {\n\t\tiptables  bool\n\t\tinsert    bool // insert other rules to FORWARD\n\t\tfwdChain  []string\n\t\tuserChain []string\n\t}{\n\t\t{\n\t\t\tiptables: false,\n\t\t\tinsert:   false,\n\t\t\tfwdChain: []string{\"-P FORWARD ACCEPT\"},\n\t\t},\n\t\t{\n\t\t\tiptables:  true,\n\t\t\tinsert:    false,\n\t\t\tfwdChain:  []string{\"-P FORWARD ACCEPT\", \"-A FORWARD -j DOCKER-USER\"},\n\t\t\tuserChain: []string{\"-N DOCKER-USER\", \"-A DOCKER-USER -j RETURN\"},\n\t\t},\n\t\t{\n\t\t\tiptables:  true,\n\t\t\tinsert:    true,\n\t\t\tfwdChain:  []string{\"-P FORWARD ACCEPT\", \"-A FORWARD -j DOCKER-USER\", \"-A FORWARD -j DROP\"},\n\t\t\tuserChain: []string{\"-N DOCKER-USER\", \"-A DOCKER-USER -j RETURN\"},\n\t\t},\n\t}\n\n\tresetIptables(t)\n\tfor _, tc := range tests {\n\t\ttc := tc\n\t\tt.Run(fmt.Sprintf(\"iptables=%v,insert=%v\", tc.iptables, tc.insert), func(t *testing.T) {\n\t\t\tc := nc.(*controller)\n\t\t\tc.cfg.Daemon.DriverCfg[\"bridge\"] = map[string]interface{}{\n\t\t\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\t\t\"EnableIPTables\": tc.iptables,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// init. condition, FORWARD chain empty DOCKER-USER not exist\n\t\t\tassert.DeepEqual(t, getRules(t, fwdChainName), []string{\"-P FORWARD ACCEPT\"})\n\n\t\t\tif tc.insert {\n\t\t\t\t_, err = iptable.Raw(\"-A\", fwdChainName, \"-j\", \"DROP\")\n\t\t\t\tassert.NilError(t, err)\n\t\t\t}\n\t\t\tarrangeUserFilterRule()\n\n\t\t\tassert.DeepEqual(t, getRules(t, fwdChainName), tc.fwdChain)\n\t\t\tif tc.userChain != nil {\n\t\t\t\tassert.DeepEqual(t, getRules(t, usrChainName), tc.userChain)\n\t\t\t} else {\n\t\t\t\t_, err := iptable.Raw(\"-S\", usrChainName)\n\t\t\t\tassert.Assert(t, err != nil, \"chain %v: created unexpectedly\", usrChainName)\n\t\t\t}\n\t\t})\n\t\tresetIptables(t)\n\t}\n}\n\nfunc getRules(t *testing.T, chain string) []string {\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\n\tt.Helper()\n\toutput, err := iptable.Raw(\"-S\", chain)\n\tassert.NilError(t, err, \"chain %s: failed to get rules\", chain)\n\n\trules := strings.Split(string(output), \"\\n\")\n\tif len(rules) > 0 {\n\t\trules = rules[:len(rules)-1]\n\t}\n\treturn rules\n}\n\nfunc resetIptables(t *testing.T) {\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\n\tt.Helper()\n\t_, err := iptable.Raw(\"-F\", fwdChainName)\n\tassert.NilError(t, err)\n\t_ = iptable.RemoveExistingChain(usrChainName, \"\")\n}\n"
        },
        {
          "name": "hostdiscovery",
          "type": "tree",
          "content": null
        },
        {
          "name": "idm",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "ipam",
          "type": "tree",
          "content": null
        },
        {
          "name": "ipamapi",
          "type": "tree",
          "content": null
        },
        {
          "name": "ipams",
          "type": "tree",
          "content": null
        },
        {
          "name": "ipamutils",
          "type": "tree",
          "content": null
        },
        {
          "name": "iptables",
          "type": "tree",
          "content": null
        },
        {
          "name": "libnetwork_internal_test.go",
          "type": "blob",
          "size": 18.11328125,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/discoverapi\"\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/internal/setmatrix\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/netutils\"\n\t\"github.com/docker/libnetwork/testutils\"\n\t\"github.com/docker/libnetwork/types\"\n)\n\nfunc TestNetworkMarshalling(t *testing.T) {\n\tn := &network{\n\t\tname:        \"Miao\",\n\t\tid:          \"abccba\",\n\t\tipamType:    \"default\",\n\t\taddrSpace:   \"viola\",\n\t\tnetworkType: \"bridge\",\n\t\tenableIPv6:  true,\n\t\tpersist:     true,\n\t\tconfigOnly:  true,\n\t\tconfigFrom:  \"configOnlyX\",\n\t\tipamOptions: map[string]string{\n\t\t\tnetlabel.MacAddress: \"a:b:c:d:e:f\",\n\t\t\t\"primary\":           \"\",\n\t\t},\n\t\tipamV4Config: []*IpamConf{\n\t\t\t{\n\t\t\t\tPreferredPool: \"10.2.0.0/16\",\n\t\t\t\tSubPool:       \"10.2.0.0/24\",\n\t\t\t\tGateway:       \"\",\n\t\t\t\tAuxAddresses:  nil,\n\t\t\t},\n\t\t\t{\n\t\t\t\tPreferredPool: \"10.2.0.0/16\",\n\t\t\t\tSubPool:       \"10.2.1.0/24\",\n\t\t\t\tGateway:       \"10.2.1.254\",\n\t\t\t},\n\t\t},\n\t\tipamV6Config: []*IpamConf{\n\t\t\t{\n\t\t\t\tPreferredPool: \"abcd::/64\",\n\t\t\t\tSubPool:       \"abcd:abcd:abcd:abcd:abcd::/80\",\n\t\t\t\tGateway:       \"abcd::29/64\",\n\t\t\t\tAuxAddresses:  nil,\n\t\t\t},\n\t\t},\n\t\tipamV4Info: []*IpamInfo{\n\t\t\t{\n\t\t\t\tPoolID: \"ipoolverde123\",\n\t\t\t\tMeta: map[string]string{\n\t\t\t\t\tnetlabel.Gateway: \"10.2.1.255/16\",\n\t\t\t\t},\n\t\t\t\tIPAMData: driverapi.IPAMData{\n\t\t\t\t\tAddressSpace: \"viola\",\n\t\t\t\t\tPool: &net.IPNet{\n\t\t\t\t\t\tIP:   net.IP{10, 2, 0, 0},\n\t\t\t\t\t\tMask: net.IPMask{255, 255, 255, 0},\n\t\t\t\t\t},\n\t\t\t\t\tGateway:      nil,\n\t\t\t\t\tAuxAddresses: nil,\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tPoolID: \"ipoolblue345\",\n\t\t\t\tMeta: map[string]string{\n\t\t\t\t\tnetlabel.Gateway: \"10.2.1.255/16\",\n\t\t\t\t},\n\t\t\t\tIPAMData: driverapi.IPAMData{\n\t\t\t\t\tAddressSpace: \"viola\",\n\t\t\t\t\tPool: &net.IPNet{\n\t\t\t\t\t\tIP:   net.IP{10, 2, 1, 0},\n\t\t\t\t\t\tMask: net.IPMask{255, 255, 255, 0},\n\t\t\t\t\t},\n\t\t\t\t\tGateway: &net.IPNet{IP: net.IP{10, 2, 1, 254}, Mask: net.IPMask{255, 255, 255, 0}},\n\t\t\t\t\tAuxAddresses: map[string]*net.IPNet{\n\t\t\t\t\t\t\"ip3\": {IP: net.IP{10, 2, 1, 3}, Mask: net.IPMask{255, 255, 255, 0}},\n\t\t\t\t\t\t\"ip5\": {IP: net.IP{10, 2, 1, 55}, Mask: net.IPMask{255, 255, 255, 0}},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tPoolID: \"weirdinfo\",\n\t\t\t\tIPAMData: driverapi.IPAMData{\n\t\t\t\t\tGateway: &net.IPNet{\n\t\t\t\t\t\tIP:   net.IP{11, 2, 1, 255},\n\t\t\t\t\t\tMask: net.IPMask{255, 0, 0, 0},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tipamV6Info: []*IpamInfo{\n\t\t\t{\n\t\t\t\tPoolID: \"ipoolv6\",\n\t\t\t\tIPAMData: driverapi.IPAMData{\n\t\t\t\t\tAddressSpace: \"viola\",\n\t\t\t\t\tPool: &net.IPNet{\n\t\t\t\t\t\tIP:   net.IP{0xab, 0xcd, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t\tMask: net.IPMask{255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t},\n\t\t\t\t\tGateway: &net.IPNet{\n\t\t\t\t\t\tIP:   net.IP{0xab, 0xcd, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29},\n\t\t\t\t\t\tMask: net.IPMask{255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t},\n\t\t\t\t\tAuxAddresses: nil,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tlabels: map[string]string{\n\t\t\t\"color\":        \"blue\",\n\t\t\t\"superimposed\": \"\",\n\t\t},\n\t\tcreated: time.Now(),\n\t}\n\n\tb, err := json.Marshal(n)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnn := &network{}\n\terr = json.Unmarshal(b, nn)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif n.name != nn.name || n.id != nn.id || n.networkType != nn.networkType || n.ipamType != nn.ipamType ||\n\t\tn.addrSpace != nn.addrSpace || n.enableIPv6 != nn.enableIPv6 ||\n\t\tn.persist != nn.persist || !compareIpamConfList(n.ipamV4Config, nn.ipamV4Config) ||\n\t\t!compareIpamInfoList(n.ipamV4Info, nn.ipamV4Info) || !compareIpamConfList(n.ipamV6Config, nn.ipamV6Config) ||\n\t\t!compareIpamInfoList(n.ipamV6Info, nn.ipamV6Info) ||\n\t\t!compareStringMaps(n.ipamOptions, nn.ipamOptions) ||\n\t\t!compareStringMaps(n.labels, nn.labels) ||\n\t\t!n.created.Equal(nn.created) ||\n\t\tn.configOnly != nn.configOnly || n.configFrom != nn.configFrom {\n\t\tt.Fatalf(\"JSON marsh/unmarsh failed.\"+\n\t\t\t\"\\nOriginal:\\n%#v\\nDecoded:\\n%#v\"+\n\t\t\t\"\\nOriginal ipamV4Conf: %#v\\n\\nDecoded ipamV4Conf: %#v\"+\n\t\t\t\"\\nOriginal ipamV4Info: %s\\n\\nDecoded ipamV4Info: %s\"+\n\t\t\t\"\\nOriginal ipamV6Conf: %#v\\n\\nDecoded ipamV6Conf: %#v\"+\n\t\t\t\"\\nOriginal ipamV6Info: %s\\n\\nDecoded ipamV6Info: %s\",\n\t\t\tn, nn, printIpamConf(n.ipamV4Config), printIpamConf(nn.ipamV4Config),\n\t\t\tprintIpamInfo(n.ipamV4Info), printIpamInfo(nn.ipamV4Info),\n\t\t\tprintIpamConf(n.ipamV6Config), printIpamConf(nn.ipamV6Config),\n\t\t\tprintIpamInfo(n.ipamV6Info), printIpamInfo(nn.ipamV6Info))\n\t}\n}\n\nfunc printIpamConf(list []*IpamConf) string {\n\ts := fmt.Sprintf(\"\\n[]*IpamConfig{\")\n\tfor _, i := range list {\n\t\ts = fmt.Sprintf(\"%s %v,\", s, i)\n\t}\n\ts = fmt.Sprintf(\"%s}\", s)\n\treturn s\n}\n\nfunc printIpamInfo(list []*IpamInfo) string {\n\ts := fmt.Sprintf(\"\\n[]*IpamInfo{\")\n\tfor _, i := range list {\n\t\ts = fmt.Sprintf(\"%s\\n{\\n%s\\n}\", s, i)\n\t}\n\ts = fmt.Sprintf(\"%s\\n}\", s)\n\treturn s\n}\n\nfunc TestEndpointMarshalling(t *testing.T) {\n\tip, nw6, err := net.ParseCIDR(\"2001:db8:4003::122/64\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tnw6.IP = ip\n\n\tvar lla []*net.IPNet\n\tfor _, nw := range []string{\"169.254.0.1/16\", \"169.254.1.1/16\", \"169.254.2.2/16\"} {\n\t\tll, _ := types.ParseCIDR(nw)\n\t\tlla = append(lla, ll)\n\t}\n\n\te := &endpoint{\n\t\tname:      \"Bau\",\n\t\tid:        \"efghijklmno\",\n\t\tsandboxID: \"ambarabaciccicocco\",\n\t\tanonymous: true,\n\t\tiface: &endpointInterface{\n\t\t\tmac: []byte{11, 12, 13, 14, 15, 16},\n\t\t\taddr: &net.IPNet{\n\t\t\t\tIP:   net.IP{10, 0, 1, 23},\n\t\t\t\tMask: net.IPMask{255, 255, 255, 0},\n\t\t\t},\n\t\t\taddrv6:    nw6,\n\t\t\tsrcName:   \"veth12ab1314\",\n\t\t\tdstPrefix: \"eth\",\n\t\t\tv4PoolID:  \"poolpool\",\n\t\t\tv6PoolID:  \"poolv6\",\n\t\t\tllAddrs:   lla,\n\t\t},\n\t}\n\n\tb, err := json.Marshal(e)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tee := &endpoint{}\n\terr = json.Unmarshal(b, ee)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif e.name != ee.name || e.id != ee.id || e.sandboxID != ee.sandboxID || !compareEndpointInterface(e.iface, ee.iface) || e.anonymous != ee.anonymous {\n\t\tt.Fatalf(\"JSON marsh/unmarsh failed.\\nOriginal:\\n%#v\\nDecoded:\\n%#v\\nOriginal iface: %#v\\nDecodediface:\\n%#v\", e, ee, e.iface, ee.iface)\n\t}\n}\n\nfunc compareEndpointInterface(a, b *endpointInterface) bool {\n\tif a == b {\n\t\treturn true\n\t}\n\tif a == nil || b == nil {\n\t\treturn false\n\t}\n\treturn a.srcName == b.srcName && a.dstPrefix == b.dstPrefix && a.v4PoolID == b.v4PoolID && a.v6PoolID == b.v6PoolID &&\n\t\ttypes.CompareIPNet(a.addr, b.addr) && types.CompareIPNet(a.addrv6, b.addrv6) && compareNwLists(a.llAddrs, b.llAddrs)\n}\n\nfunc compareIpamConfList(listA, listB []*IpamConf) bool {\n\tvar a, b *IpamConf\n\tif len(listA) != len(listB) {\n\t\treturn false\n\t}\n\tfor i := 0; i < len(listA); i++ {\n\t\ta = listA[i]\n\t\tb = listB[i]\n\t\tif a.PreferredPool != b.PreferredPool ||\n\t\t\ta.SubPool != b.SubPool ||\n\t\t\ta.Gateway != b.Gateway || !compareStringMaps(a.AuxAddresses, b.AuxAddresses) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc compareIpamInfoList(listA, listB []*IpamInfo) bool {\n\tvar a, b *IpamInfo\n\tif len(listA) != len(listB) {\n\t\treturn false\n\t}\n\tfor i := 0; i < len(listA); i++ {\n\t\ta = listA[i]\n\t\tb = listB[i]\n\t\tif a.PoolID != b.PoolID || !compareStringMaps(a.Meta, b.Meta) ||\n\t\t\t!types.CompareIPNet(a.Gateway, b.Gateway) ||\n\t\t\ta.AddressSpace != b.AddressSpace ||\n\t\t\t!types.CompareIPNet(a.Pool, b.Pool) ||\n\t\t\t!compareAddresses(a.AuxAddresses, b.AuxAddresses) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc compareStringMaps(a, b map[string]string) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tif len(a) > 0 {\n\t\tfor k := range a {\n\t\t\tif a[k] != b[k] {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc compareAddresses(a, b map[string]*net.IPNet) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tif len(a) > 0 {\n\t\tfor k := range a {\n\t\t\tif !types.CompareIPNet(a[k], b[k]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc compareNwLists(a, b []*net.IPNet) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tfor k := range a {\n\t\tif !types.CompareIPNet(a[k], b[k]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc TestAuxAddresses(t *testing.T) {\n\tc, err := New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tn := &network{ipamType: ipamapi.DefaultIPAM, networkType: \"bridge\", ctrlr: c.(*controller)}\n\n\tinput := []struct {\n\t\tmasterPool   string\n\t\tsubPool      string\n\t\tauxAddresses map[string]string\n\t\tgood         bool\n\t}{\n\t\t{\"192.168.0.0/16\", \"\", map[string]string{\"goodOne\": \"192.168.2.2\"}, true},\n\t\t{\"192.168.0.0/16\", \"\", map[string]string{\"badOne\": \"192.169.2.3\"}, false},\n\t\t{\"192.168.0.0/16\", \"192.168.1.0/24\", map[string]string{\"goodOne\": \"192.168.1.2\"}, true},\n\t\t{\"192.168.0.0/16\", \"192.168.1.0/24\", map[string]string{\"stillGood\": \"192.168.2.4\"}, true},\n\t\t{\"192.168.0.0/16\", \"192.168.1.0/24\", map[string]string{\"badOne\": \"192.169.2.4\"}, false},\n\t}\n\n\tfor _, i := range input {\n\n\t\tn.ipamV4Config = []*IpamConf{{PreferredPool: i.masterPool, SubPool: i.subPool, AuxAddresses: i.auxAddresses}}\n\n\t\terr = n.ipamAllocate()\n\n\t\tif i.good != (err == nil) {\n\t\t\tt.Fatalf(\"Unexpected result for %v: %v\", i, err)\n\t\t}\n\n\t\tn.ipamRelease()\n\t}\n}\n\nfunc TestSRVServiceQuery(t *testing.T) {\n\tc, err := New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tn, err := c.NewNetwork(\"bridge\", \"net1\", \"\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsb, err := c.NewSandbox(\"c1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsr := svcInfo{\n\t\tsvcMap:     setmatrix.NewSetMatrix(),\n\t\tsvcIPv6Map: setmatrix.NewSetMatrix(),\n\t\tipMap:      setmatrix.NewSetMatrix(),\n\t\tservice:    make(map[string][]servicePorts),\n\t}\n\t// backing container for the service\n\tcTarget := serviceTarget{\n\t\tname: \"task1.web.swarm\",\n\t\tip:   net.ParseIP(\"192.168.10.2\"),\n\t\tport: 80,\n\t}\n\t// backing host for the service\n\thTarget := serviceTarget{\n\t\tname: \"node1.docker-cluster\",\n\t\tip:   net.ParseIP(\"10.10.10.2\"),\n\t\tport: 45321,\n\t}\n\thttpPort := servicePorts{\n\t\tportName: \"_http\",\n\t\tproto:    \"_tcp\",\n\t\ttarget:   []serviceTarget{cTarget},\n\t}\n\n\textHTTPPort := servicePorts{\n\t\tportName: \"_host_http\",\n\t\tproto:    \"_tcp\",\n\t\ttarget:   []serviceTarget{hTarget},\n\t}\n\tsr.service[\"web.swarm\"] = append(sr.service[\"web.swarm\"], httpPort)\n\tsr.service[\"web.swarm\"] = append(sr.service[\"web.swarm\"], extHTTPPort)\n\n\tc.(*controller).svcRecords[n.ID()] = sr\n\n\t_, ip := ep.Info().Sandbox().ResolveService(\"_http._tcp.web.swarm\")\n\n\tif len(ip) == 0 {\n\t\tt.Fatal(err)\n\t}\n\tif ip[0].String() != \"192.168.10.2\" {\n\t\tt.Fatal(err)\n\t}\n\n\t_, ip = ep.Info().Sandbox().ResolveService(\"_host_http._tcp.web.swarm\")\n\n\tif len(ip) == 0 {\n\t\tt.Fatal(err)\n\t}\n\tif ip[0].String() != \"10.10.10.2\" {\n\t\tt.Fatal(err)\n\t}\n\n\t// Service name with invalid protocol name. Should fail without error\n\t_, ip = ep.Info().Sandbox().ResolveService(\"_http._icmp.web.swarm\")\n\tif len(ip) != 0 {\n\t\tt.Fatal(\"Valid response for invalid service name\")\n\t}\n}\n\nfunc TestServiceVIPReuse(t *testing.T) {\n\tc, err := New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tn, err := c.NewNetwork(\"bridge\", \"net1\", \"\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsb, err := c.NewSandbox(\"c1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Add 2 services with same name but different service ID to share the same VIP\n\tn.(*network).addSvcRecords(\"ep1\", \"service_test\", \"serviceID1\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\tn.(*network).addSvcRecords(\"ep2\", \"service_test\", \"serviceID2\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\n\tipToResolve := netutils.ReverseIP(\"192.168.0.1\")\n\n\tipList, _ := n.(*network).ResolveName(\"service_test\", types.IPv4)\n\tif len(ipList) == 0 {\n\t\tt.Fatal(\"There must be the VIP\")\n\t}\n\tif len(ipList) != 1 {\n\t\tt.Fatal(\"It must return only 1 VIP\")\n\t}\n\tif ipList[0].String() != \"192.168.0.1\" {\n\t\tt.Fatal(\"The service VIP is 192.168.0.1\")\n\t}\n\tname := n.(*network).ResolveIP(ipToResolve)\n\tif name == \"\" {\n\t\tt.Fatal(\"It must return a name\")\n\t}\n\tif name != \"service_test.net1\" {\n\t\tt.Fatalf(\"It must return the service_test.net1 != %s\", name)\n\t}\n\n\t// Delete service record for one of the services, the IP should remain because one service is still associated with it\n\tn.(*network).deleteSvcRecords(\"ep1\", \"service_test\", \"serviceID1\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\tipList, _ = n.(*network).ResolveName(\"service_test\", types.IPv4)\n\tif len(ipList) == 0 {\n\t\tt.Fatal(\"There must be the VIP\")\n\t}\n\tif len(ipList) != 1 {\n\t\tt.Fatal(\"It must return only 1 VIP\")\n\t}\n\tif ipList[0].String() != \"192.168.0.1\" {\n\t\tt.Fatal(\"The service VIP is 192.168.0.1\")\n\t}\n\tname = n.(*network).ResolveIP(ipToResolve)\n\tif name == \"\" {\n\t\tt.Fatal(\"It must return a name\")\n\t}\n\tif name != \"service_test.net1\" {\n\t\tt.Fatalf(\"It must return the service_test.net1 != %s\", name)\n\t}\n\n\t// Delete again the service using the previous service ID, nothing should happen\n\tn.(*network).deleteSvcRecords(\"ep2\", \"service_test\", \"serviceID1\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\tipList, _ = n.(*network).ResolveName(\"service_test\", types.IPv4)\n\tif len(ipList) == 0 {\n\t\tt.Fatal(\"There must be the VIP\")\n\t}\n\tif len(ipList) != 1 {\n\t\tt.Fatal(\"It must return only 1 VIP\")\n\t}\n\tif ipList[0].String() != \"192.168.0.1\" {\n\t\tt.Fatal(\"The service VIP is 192.168.0.1\")\n\t}\n\tname = n.(*network).ResolveIP(ipToResolve)\n\tif name == \"\" {\n\t\tt.Fatal(\"It must return a name\")\n\t}\n\tif name != \"service_test.net1\" {\n\t\tt.Fatalf(\"It must return the service_test.net1 != %s\", name)\n\t}\n\n\t// Delete now using the second service ID, now all the entries should be gone\n\tn.(*network).deleteSvcRecords(\"ep2\", \"service_test\", \"serviceID2\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\tipList, _ = n.(*network).ResolveName(\"service_test\", types.IPv4)\n\tif len(ipList) != 0 {\n\t\tt.Fatal(\"All the VIPs should be gone now\")\n\t}\n\tname = n.(*network).ResolveIP(ipToResolve)\n\tif name != \"\" {\n\t\tt.Fatalf(\"It must return empty no more services associated, instead:%s\", name)\n\t}\n}\n\nfunc TestIpamReleaseOnNetDriverFailures(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tcfgOptions, err := OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tc, err := New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tcc := c.(*controller)\n\n\tif err := cc.drvRegistry.AddDriver(badDriverName, badDriverInit, nil); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Test whether ipam state release is invoked  on network create failure from net driver\n\t// by checking whether subsequent network creation requesting same gateway IP succeeds\n\tipamOpt := NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", []*IpamConf{{PreferredPool: \"10.34.0.0/16\", Gateway: \"10.34.255.254\"}}, nil, nil)\n\tif _, err := c.NewNetwork(badDriverName, \"badnet1\", \"\", ipamOpt); err == nil {\n\t\tt.Fatalf(\"bad network driver should have failed network creation\")\n\t}\n\n\tgnw, err := c.NewNetwork(\"bridge\", \"goodnet1\", \"\", ipamOpt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tgnw.Delete()\n\n\t// Now check whether ipam release works on endpoint creation failure\n\tbd.failNetworkCreation = false\n\tbnw, err := c.NewNetwork(badDriverName, \"badnet2\", \"\", ipamOpt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer bnw.Delete()\n\n\tif _, err := bnw.CreateEndpoint(\"ep0\"); err == nil {\n\t\tt.Fatalf(\"bad network driver should have failed endpoint creation\")\n\t}\n\n\t// Now create good bridge network with different gateway\n\tipamOpt2 := NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", []*IpamConf{{PreferredPool: \"10.35.0.0/16\", Gateway: \"10.35.255.253\"}}, nil, nil)\n\tgnw, err = c.NewNetwork(\"bridge\", \"goodnet2\", \"\", ipamOpt2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer gnw.Delete()\n\n\tep, err := gnw.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer ep.Delete(false)\n\n\texpectedIP, _ := types.ParseCIDR(\"10.35.0.1/16\")\n\tif !types.CompareIPNet(ep.Info().Iface().Address(), expectedIP) {\n\t\tt.Fatalf(\"Ipam release must have failed, endpoint has unexpected address: %v\", ep.Info().Iface().Address())\n\t}\n}\n\nvar badDriverName = \"bad network driver\"\n\ntype badDriver struct {\n\tfailNetworkCreation bool\n}\n\nvar bd = badDriver{failNetworkCreation: true}\n\nfunc badDriverInit(reg driverapi.DriverCallback, opt map[string]interface{}) error {\n\treturn reg.RegisterDriver(badDriverName, &bd, driverapi.Capability{DataScope: datastore.LocalScope})\n}\n\nfunc (b *badDriver) CreateNetwork(nid string, options map[string]interface{}, nInfo driverapi.NetworkInfo, ipV4Data, ipV6Data []driverapi.IPAMData) error {\n\tif b.failNetworkCreation {\n\t\treturn fmt.Errorf(\"I will not create any network\")\n\t}\n\treturn nil\n}\nfunc (b *badDriver) DeleteNetwork(nid string) error {\n\treturn nil\n}\nfunc (b *badDriver) CreateEndpoint(nid, eid string, ifInfo driverapi.InterfaceInfo, options map[string]interface{}) error {\n\treturn fmt.Errorf(\"I will not create any endpoint\")\n}\nfunc (b *badDriver) DeleteEndpoint(nid, eid string) error {\n\treturn nil\n}\nfunc (b *badDriver) EndpointOperInfo(nid, eid string) (map[string]interface{}, error) {\n\treturn nil, nil\n}\nfunc (b *badDriver) Join(nid, eid string, sboxKey string, jinfo driverapi.JoinInfo, options map[string]interface{}) error {\n\treturn fmt.Errorf(\"I will not allow any join\")\n}\nfunc (b *badDriver) Leave(nid, eid string) error {\n\treturn nil\n}\nfunc (b *badDriver) DiscoverNew(dType discoverapi.DiscoveryType, data interface{}) error {\n\treturn nil\n}\nfunc (b *badDriver) DiscoverDelete(dType discoverapi.DiscoveryType, data interface{}) error {\n\treturn nil\n}\nfunc (b *badDriver) Type() string {\n\treturn badDriverName\n}\nfunc (b *badDriver) IsBuiltIn() bool {\n\treturn false\n}\nfunc (b *badDriver) ProgramExternalConnectivity(nid, eid string, options map[string]interface{}) error {\n\treturn nil\n}\nfunc (b *badDriver) RevokeExternalConnectivity(nid, eid string) error {\n\treturn nil\n}\n\nfunc (b *badDriver) NetworkAllocate(id string, option map[string]string, ipV4Data, ipV6Data []driverapi.IPAMData) (map[string]string, error) {\n\treturn nil, types.NotImplementedErrorf(\"not implemented\")\n}\n\nfunc (b *badDriver) NetworkFree(id string) error {\n\treturn types.NotImplementedErrorf(\"not implemented\")\n}\n\nfunc (b *badDriver) EventNotify(etype driverapi.EventType, nid, tableName, key string, value []byte) {\n}\n\nfunc (b *badDriver) DecodeTableEntry(tablename string, key string, value []byte) (string, map[string]string) {\n\treturn \"\", nil\n}\n"
        },
        {
          "name": "libnetwork_linux_test.go",
          "type": "blob",
          "size": 22.48046875,
          "content": "package libnetwork_test\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/docker/docker/pkg/reexec\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/docker/libnetwork/testutils\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink\"\n\t\"github.com/vishvananda/netns\"\n)\n\nfunc TestHost(t *testing.T) {\n\tsbx1, err := controller.NewSandbox(\"host_c1\",\n\t\tlibnetwork.OptionHostname(\"test1\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"),\n\t\tlibnetwork.OptionUseDefaultSandbox())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsbx2, err := controller.NewSandbox(\"host_c2\",\n\t\tlibnetwork.OptionHostname(\"test2\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"),\n\t\tlibnetwork.OptionUseDefaultSandbox())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tnetwork, err := createTestNetwork(\"host\", \"testhost\", options.Generic{}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep1, err := network.CreateEndpoint(\"testep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep1.Join(sbx1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep2, err := network.CreateEndpoint(\"testep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep2.Join(sbx2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep1.Leave(sbx1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep2.Leave(sbx2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep1.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep2.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Try to create another host endpoint and join/leave that.\n\tcnt3, err := controller.NewSandbox(\"host_c3\",\n\t\tlibnetwork.OptionHostname(\"test3\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"),\n\t\tlibnetwork.OptionUseDefaultSandbox())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := cnt3.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep3, err := network.CreateEndpoint(\"testep3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep3.Join(sbx2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep3.Leave(sbx2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep3.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// Testing IPV6 from MAC address\nfunc TestBridgeIpv6FromMac(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\":         \"testipv6mac\",\n\t\t\t\"EnableICC\":          true,\n\t\t\t\"EnableIPMasquerade\": true,\n\t\t},\n\t}\n\tipamV4ConfList := []*libnetwork.IpamConf{{PreferredPool: \"192.168.100.0/24\", Gateway: \"192.168.100.1\"}}\n\tipamV6ConfList := []*libnetwork.IpamConf{{PreferredPool: \"fe90::/64\", Gateway: \"fe90::22\"}}\n\n\tnetwork, err := controller.NewNetwork(bridgeNetType, \"testipv6mac\", \"\",\n\t\tlibnetwork.NetworkOptionGeneric(netOption),\n\t\tlibnetwork.NetworkOptionEnableIPv6(true),\n\t\tlibnetwork.NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", ipamV4ConfList, ipamV6ConfList, nil),\n\t\tlibnetwork.NetworkOptionDeferIPv6Alloc(true))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmac := net.HardwareAddr{0xaa, 0xbb, 0xcc, 0xdd, 0xee, 0xff}\n\tepOption := options.Generic{netlabel.MacAddress: mac}\n\n\tep, err := network.CreateEndpoint(\"testep\", libnetwork.EndpointOptionGeneric(epOption))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tiface := ep.Info().Iface()\n\tif !bytes.Equal(iface.MacAddress(), mac) {\n\t\tt.Fatalf(\"Unexpected mac address: %v\", iface.MacAddress())\n\t}\n\n\tip, expIP, _ := net.ParseCIDR(\"fe90::aabb:ccdd:eeff/64\")\n\texpIP.IP = ip\n\tif !types.CompareIPNet(expIP, iface.AddressIPv6()) {\n\t\tt.Fatalf(\"Expected %v. Got: %v\", expIP, iface.AddressIPv6())\n\t}\n\n\tif err := ep.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := network.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc checkSandbox(t *testing.T, info libnetwork.EndpointInfo) {\n\tkey := info.Sandbox().Key()\n\tsbNs, err := netns.GetFromPath(key)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to get network namespace path %q: %v\", key, err)\n\t}\n\tdefer sbNs.Close()\n\n\tnh, err := netlink.NewHandleAt(sbNs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = nh.LinkByName(\"eth0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Could not find the interface eth0 inside the sandbox: %v\", err)\n\t}\n\n\t_, err = nh.LinkByName(\"eth1\")\n\tif err != nil {\n\t\tt.Fatalf(\"Could not find the interface eth1 inside the sandbox: %v\", err)\n\t}\n}\n\nfunc TestEndpointJoin(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t// Create network 1 and add 2 endpoint: ep11, ep12\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\":         \"testnetwork1\",\n\t\t\t\"EnableICC\":          true,\n\t\t\t\"EnableIPMasquerade\": true,\n\t\t},\n\t}\n\tipamV6ConfList := []*libnetwork.IpamConf{{PreferredPool: \"fe90::/64\", Gateway: \"fe90::22\"}}\n\tn1, err := controller.NewNetwork(bridgeNetType, \"testnetwork1\", \"\",\n\t\tlibnetwork.NetworkOptionGeneric(netOption),\n\t\tlibnetwork.NetworkOptionEnableIPv6(true),\n\t\tlibnetwork.NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", nil, ipamV6ConfList, nil),\n\t\tlibnetwork.NetworkOptionDeferIPv6Alloc(true))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep1, err := n1.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep1.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Validate if ep.Info() only gives me IP address info and not names and gateway during CreateEndpoint()\n\tinfo := ep1.Info()\n\tiface := info.Iface()\n\tif iface.Address() != nil && iface.Address().IP.To4() == nil {\n\t\tt.Fatalf(\"Invalid IP address returned: %v\", iface.Address())\n\t}\n\tif iface.AddressIPv6() != nil && iface.AddressIPv6().IP == nil {\n\t\tt.Fatalf(\"Invalid IPv6 address returned: %v\", iface.Address())\n\t}\n\n\tif len(info.Gateway()) != 0 {\n\t\tt.Fatalf(\"Expected empty gateway for an empty endpoint. Instead found a gateway: %v\", info.Gateway())\n\t}\n\tif len(info.GatewayIPv6()) != 0 {\n\t\tt.Fatalf(\"Expected empty gateway for an empty ipv6 endpoint. Instead found a gateway: %v\", info.GatewayIPv6())\n\t}\n\n\tif info.Sandbox() != nil {\n\t\tt.Fatalf(\"Expected an empty sandbox key for an empty endpoint. Instead found a non-empty sandbox key: %s\", info.Sandbox().Key())\n\t}\n\n\t// test invalid joins\n\terr = ep1.Join(nil)\n\tif err == nil {\n\t\tt.Fatalf(\"Expected to fail join with nil Sandbox\")\n\t}\n\tif _, ok := err.(types.BadRequestError); !ok {\n\t\tt.Fatalf(\"Unexpected error type returned: %T\", err)\n\t}\n\n\tfsbx := &fakeSandbox{}\n\tif err = ep1.Join(fsbx); err == nil {\n\t\tt.Fatalf(\"Expected to fail join with invalid Sandbox\")\n\t}\n\tif _, ok := err.(types.BadRequestError); !ok {\n\t\tt.Fatalf(\"Unexpected error type returned: %T\", err)\n\t}\n\n\tsb, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep1.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep1.Leave(sb)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Validate if ep.Info() only gives valid gateway and sandbox key after has container has joined.\n\tinfo = ep1.Info()\n\tif len(info.Gateway()) == 0 {\n\t\tt.Fatalf(\"Expected a valid gateway for a joined endpoint. Instead found an invalid gateway: %v\", info.Gateway())\n\t}\n\tif len(info.GatewayIPv6()) == 0 {\n\t\tt.Fatalf(\"Expected a valid ipv6 gateway for a joined endpoint. Instead found an invalid gateway: %v\", info.GatewayIPv6())\n\t}\n\n\tif info.Sandbox() == nil {\n\t\tt.Fatalf(\"Expected an non-empty sandbox key for a joined endpoint. Instead found an empty sandbox key\")\n\t}\n\n\t// Check endpoint provided container information\n\tif ep1.Info().Sandbox().Key() != sb.Key() {\n\t\tt.Fatalf(\"Endpoint Info returned unexpected sandbox key: %s\", sb.Key())\n\t}\n\n\t// Attempt retrieval of endpoint interfaces statistics\n\tstats, err := sb.Statistics()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif _, ok := stats[\"eth0\"]; !ok {\n\t\tt.Fatalf(\"Did not find eth0 statistics\")\n\t}\n\n\t// Now test the container joining another network\n\tn2, err := createTestNetwork(bridgeNetType, \"testnetwork2\",\n\t\toptions.Generic{\n\t\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\t\"BridgeName\": \"testnetwork2\",\n\t\t\t},\n\t\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep2, err := n2.CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep2.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep2.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep2.Leave(sb)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif ep1.Info().Sandbox().Key() != ep2.Info().Sandbox().Key() {\n\t\tt.Fatalf(\"ep1 and ep2 returned different container sandbox key\")\n\t}\n\n\tcheckSandbox(t, info)\n}\n\nfunc TestExternalKey(t *testing.T) {\n\texternalKeyTest(t, false)\n}\n\nfunc externalKeyTest(t *testing.T, reexec bool) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tn2, err := createTestNetwork(bridgeNetType, \"testnetwork2\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork2\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep.Delete(false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep2, err := n2.CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep2.Delete(false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcnt, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionUseExternalKey(),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tdefer func() {\n\t\tif err := cnt.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tosl.GC()\n\t}()\n\n\t// Join endpoint to sandbox before SetKey\n\terr = ep.Join(cnt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep.Leave(cnt)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsbox := ep.Info().Sandbox()\n\tif sbox == nil {\n\t\tt.Fatalf(\"Expected to have a valid Sandbox\")\n\t}\n\n\tif reexec {\n\t\terr := reexecSetKey(\"this-must-fail\", containerID, controller.ID())\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"SetExternalKey must fail if the corresponding namespace is not created\")\n\t\t}\n\t} else {\n\t\t// Setting an non-existing key (namespace) must fail\n\t\tif err := sbox.SetKey(\"this-must-fail\"); err == nil {\n\t\t\tt.Fatalf(\"Setkey must fail if the corresponding namespace is not created\")\n\t\t}\n\t}\n\n\t// Create a new OS sandbox using the osl API before using it in SetKey\n\tif extOsBox, err := osl.NewSandbox(\"ValidKey\", true, false); err != nil {\n\t\tt.Fatalf(\"Failed to create new osl sandbox\")\n\t} else {\n\t\tdefer func() {\n\t\t\tif err := extOsBox.Destroy(); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to remove os sandbox: %v\", err)\n\t\t\t}\n\t\t}()\n\t}\n\n\tif reexec {\n\t\terr := reexecSetKey(\"ValidKey\", containerID, controller.ID())\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"SetExternalKey failed with %v\", err)\n\t\t}\n\t} else {\n\t\tif err := sbox.SetKey(\"ValidKey\"); err != nil {\n\t\t\tt.Fatalf(\"Setkey failed with %v\", err)\n\t\t}\n\t}\n\n\t// Join endpoint to sandbox after SetKey\n\terr = ep2.Join(sbox)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep2.Leave(sbox)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif ep.Info().Sandbox().Key() != ep2.Info().Sandbox().Key() {\n\t\tt.Fatalf(\"ep1 and ep2 returned different container sandbox key\")\n\t}\n\n\tcheckSandbox(t, ep.Info())\n}\n\nfunc reexecSetKey(key string, containerID string, controllerID string) error {\n\ttype libcontainerState struct {\n\t\tNamespacePaths map[string]string\n\t}\n\tvar (\n\t\tstate libcontainerState\n\t\tb     []byte\n\t\terr   error\n\t)\n\n\tstate.NamespacePaths = make(map[string]string)\n\tstate.NamespacePaths[\"NEWNET\"] = key\n\tif b, err = json.Marshal(state); err != nil {\n\t\treturn err\n\t}\n\tcmd := &exec.Cmd{\n\t\tPath:   reexec.Self(),\n\t\tArgs:   append([]string{\"libnetwork-setkey\"}, containerID, controllerID),\n\t\tStdin:  strings.NewReader(string(b)),\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n\treturn cmd.Run()\n}\n\nfunc TestEnableIPv6(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\ttmpResolvConf := []byte(\"search pommesfrites.fr\\nnameserver 12.34.56.78\\nnameserver 2001:4860:4860::8888\\n\")\n\texpectedResolvConf := []byte(\"search pommesfrites.fr\\nnameserver 127.0.0.11\\nnameserver 2001:4860:4860::8888\\noptions ndots:0\\n\")\n\t//take a copy of resolv.conf for restoring after test completes\n\tresolvConfSystem, err := ioutil.ReadFile(\"/etc/resolv.conf\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t//cleanup\n\tdefer func() {\n\t\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", resolvConfSystem, 0644); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tnetOption := options.Generic{\n\t\tnetlabel.EnableIPv6: true,\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\tipamV6ConfList := []*libnetwork.IpamConf{{PreferredPool: \"fe99::/64\", Gateway: \"fe99::9\"}}\n\n\tn, err := createTestNetwork(\"bridge\", \"testnetwork\", netOption, nil, ipamV6ConfList)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep1, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", tmpResolvConf, 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tresolvConfPath := \"/tmp/libnetwork_test/resolv.conf\"\n\tdefer os.Remove(resolvConfPath)\n\n\tsb, err := controller.NewSandbox(containerID, libnetwork.OptionResolvConfPath(resolvConfPath))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep1.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcontent, err := ioutil.ReadFile(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(content, expectedResolvConf) {\n\t\tt.Fatalf(\"Expected:\\n%s\\nGot:\\n%s\", string(expectedResolvConf), string(content))\n\t}\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestResolvConfHost(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\ttmpResolvConf := []byte(\"search localhost.net\\nnameserver 127.0.0.1\\nnameserver 2001:4860:4860::8888\\n\")\n\n\t//take a copy of resolv.conf for restoring after test completes\n\tresolvConfSystem, err := ioutil.ReadFile(\"/etc/resolv.conf\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t//cleanup\n\tdefer func() {\n\t\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", resolvConfSystem, 0644); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tn, err := controller.NetworkByName(\"testhost\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep1, err := n.CreateEndpoint(\"ep1\", libnetwork.CreateOptionDisableResolution())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", tmpResolvConf, 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tresolvConfPath := \"/tmp/libnetwork_test/resolv.conf\"\n\tdefer os.Remove(resolvConfPath)\n\n\tsb, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionUseDefaultSandbox(),\n\t\tlibnetwork.OptionResolvConfPath(resolvConfPath),\n\t\tlibnetwork.OptionOriginResolvConfPath(\"/etc/resolv.conf\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep1.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep1.Leave(sb)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tfinfo, err := os.Stat(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfmode := (os.FileMode)(0644)\n\tif finfo.Mode() != fmode {\n\t\tt.Fatalf(\"Expected file mode %s, got %s\", fmode.String(), finfo.Mode().String())\n\t}\n\n\tcontent, err := ioutil.ReadFile(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(content, tmpResolvConf) {\n\t\tt.Fatalf(\"Expected:\\n%s\\nGot:\\n%s\", string(tmpResolvConf), string(content))\n\t}\n}\n\nfunc TestResolvConf(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\ttmpResolvConf1 := []byte(\"search pommesfrites.fr\\nnameserver 12.34.56.78\\nnameserver 2001:4860:4860::8888\\n\")\n\ttmpResolvConf2 := []byte(\"search pommesfrites.fr\\nnameserver 112.34.56.78\\nnameserver 2001:4860:4860::8888\\n\")\n\texpectedResolvConf1 := []byte(\"search pommesfrites.fr\\nnameserver 127.0.0.11\\noptions ndots:0\\n\")\n\ttmpResolvConf3 := []byte(\"search pommesfrites.fr\\nnameserver 113.34.56.78\\n\")\n\n\t//take a copy of resolv.conf for restoring after test completes\n\tresolvConfSystem, err := ioutil.ReadFile(\"/etc/resolv.conf\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t//cleanup\n\tdefer func() {\n\t\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", resolvConfSystem, 0644); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\tn, err := createTestNetwork(\"bridge\", \"testnetwork\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", tmpResolvConf1, 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tresolvConfPath := \"/tmp/libnetwork_test/resolv.conf\"\n\tdefer os.Remove(resolvConfPath)\n\n\tsb1, err := controller.NewSandbox(containerID, libnetwork.OptionResolvConfPath(resolvConfPath))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sb1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfinfo, err := os.Stat(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfmode := (os.FileMode)(0644)\n\tif finfo.Mode() != fmode {\n\t\tt.Fatalf(\"Expected file mode %s, got %s\", fmode.String(), finfo.Mode().String())\n\t}\n\n\tcontent, err := ioutil.ReadFile(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(content, expectedResolvConf1) {\n\t\tfmt.Printf(\"\\n%v\\n%v\\n\", expectedResolvConf1, content)\n\t\tt.Fatalf(\"Expected:\\n%s\\nGot:\\n%s\", string(expectedResolvConf1), string(content))\n\t}\n\n\terr = ep.Leave(sb1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ioutil.WriteFile(\"/etc/resolv.conf\", tmpResolvConf2, 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsb2, err := controller.NewSandbox(containerID+\"_2\", libnetwork.OptionResolvConfPath(resolvConfPath))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sb2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcontent, err = ioutil.ReadFile(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(content, expectedResolvConf1) {\n\t\tt.Fatalf(\"Expected:\\n%s\\nGot:\\n%s\", string(expectedResolvConf1), string(content))\n\t}\n\n\tif err := ioutil.WriteFile(resolvConfPath, tmpResolvConf3, 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep.Leave(sb2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep.Join(sb2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcontent, err = ioutil.ReadFile(resolvConfPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(content, tmpResolvConf3) {\n\t\tt.Fatalf(\"Expected:\\n%s\\nGot:\\n%s\", string(tmpResolvConf3), string(content))\n\t}\n}\n\nfunc parallelJoin(t *testing.T, rc libnetwork.Sandbox, ep libnetwork.Endpoint, thrNumber int) {\n\tdebugf(\"J%d.\", thrNumber)\n\tvar err error\n\n\tsb := sboxes[thrNumber-1]\n\terr = ep.Join(sb)\n\n\truntime.LockOSThread()\n\tif err != nil {\n\t\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\t\tt.Fatalf(\"thread %d: %v\", thrNumber, err)\n\t\t}\n\t\tdebugf(\"JE%d(%v).\", thrNumber, err)\n\t}\n\tdebugf(\"JD%d.\", thrNumber)\n}\n\nfunc parallelLeave(t *testing.T, rc libnetwork.Sandbox, ep libnetwork.Endpoint, thrNumber int) {\n\tdebugf(\"L%d.\", thrNumber)\n\tvar err error\n\n\tsb := sboxes[thrNumber-1]\n\n\terr = ep.Leave(sb)\n\truntime.LockOSThread()\n\tif err != nil {\n\t\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\t\tt.Fatalf(\"thread %d: %v\", thrNumber, err)\n\t\t}\n\t\tdebugf(\"LE%d(%v).\", thrNumber, err)\n\t}\n\tdebugf(\"LD%d.\", thrNumber)\n}\n\nfunc runParallelTests(t *testing.T, thrNumber int) {\n\tvar (\n\t\tep  libnetwork.Endpoint\n\t\tsb  libnetwork.Sandbox\n\t\terr error\n\t)\n\n\tt.Parallel()\n\n\tpTest := flag.Lookup(\"test.parallel\")\n\tif pTest == nil {\n\t\tt.Skip(\"Skipped because test.parallel flag not set;\")\n\t}\n\tnumParallel, err := strconv.Atoi(pTest.Value.String())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif numParallel < numThreads {\n\t\tt.Skip(\"Skipped because t.parallel was less than \", numThreads)\n\t}\n\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\tif thrNumber == first {\n\t\tcreateGlobalInstance(t)\n\t}\n\n\tif thrNumber != first {\n\t\t<-start\n\n\t\tthrdone := make(chan struct{})\n\t\tdone <- thrdone\n\t\tdefer close(thrdone)\n\n\t\tif thrNumber == last {\n\t\t\tdefer close(done)\n\t\t}\n\n\t\terr = netns.Set(testns)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tdefer netns.Set(origins)\n\n\tnet1, err := controller.NetworkByName(\"testhost\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif net1 == nil {\n\t\tt.Fatal(\"Could not find testhost\")\n\t}\n\n\tnet2, err := controller.NetworkByName(\"network2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif net2 == nil {\n\t\tt.Fatal(\"Could not find network2\")\n\t}\n\n\tepName := fmt.Sprintf(\"pep%d\", thrNumber)\n\n\tif thrNumber == first {\n\t\tep, err = net1.EndpointByName(epName)\n\t} else {\n\t\tep, err = net2.EndpointByName(epName)\n\t}\n\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif ep == nil {\n\t\tt.Fatal(\"Got nil ep with no error\")\n\t}\n\n\tcid := fmt.Sprintf(\"%drace\", thrNumber)\n\tcontroller.WalkSandboxes(libnetwork.SandboxContainerWalker(&sb, cid))\n\tif sb == nil {\n\t\tt.Fatalf(\"Got nil sandbox for container: %s\", cid)\n\t}\n\n\tfor i := 0; i < iterCnt; i++ {\n\t\tparallelJoin(t, sb, ep, thrNumber)\n\t\tparallelLeave(t, sb, ep, thrNumber)\n\t}\n\n\tdebugf(\"\\n\")\n\n\terr = sb.Delete()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif thrNumber == first {\n\t\tfor thrdone := range done {\n\t\t\t<-thrdone\n\t\t}\n\n\t\ttestns.Close()\n\t\tif err := net2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t} else {\n\t\terr = ep.Delete(false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc TestParallel1(t *testing.T) {\n\trunParallelTests(t, 1)\n}\n\nfunc TestParallel2(t *testing.T) {\n\trunParallelTests(t, 2)\n}\n\nfunc TestParallel3(t *testing.T) {\n\trunParallelTests(t, 3)\n}\n\nfunc TestNullIpam(t *testing.T) {\n\t_, err := controller.NewNetwork(bridgeNetType, \"testnetworkinternal\", \"\", libnetwork.NetworkOptionIpam(ipamapi.NullIPAM, \"\", nil, nil, nil))\n\tif err == nil || err.Error() != \"ipv4 pool is empty\" {\n\t\tt.Fatal(\"bridge network should complain empty pool\")\n\t}\n}\n"
        },
        {
          "name": "libnetwork_test.go",
          "type": "blob",
          "size": 34.033203125,
          "content": "package libnetwork_test\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"os\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/docker/docker/pkg/plugins\"\n\t\"github.com/docker/docker/pkg/reexec\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/drivers/bridge\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/testutils\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netns\"\n)\n\nconst (\n\tbridgeNetType = \"bridge\"\n)\n\nvar controller libnetwork.NetworkController\n\nfunc TestMain(m *testing.M) {\n\tif reexec.Init() {\n\t\treturn\n\t}\n\n\tif err := createController(); err != nil {\n\t\tlogrus.Errorf(\"Error creating controller: %v\", err)\n\t\tos.Exit(1)\n\t}\n\n\tx := m.Run()\n\tcontroller.Stop()\n\tos.Exit(x)\n}\n\nfunc createController() error {\n\tvar err error\n\n\t// Cleanup local datastore file\n\tos.Remove(datastore.DefaultScopes(\"\")[datastore.LocalScope].Client.Address)\n\n\toption := options.Generic{\n\t\t\"EnableIPForwarding\": true,\n\t}\n\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = option\n\n\tcfgOptions, err := libnetwork.OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\treturn err\n\t}\n\tcontroller, err = libnetwork.New(append(cfgOptions, config.OptionDriverConfig(bridgeNetType, genericOption))...)\n\treturn err\n}\n\nfunc createTestNetwork(networkType, networkName string, netOption options.Generic, ipamV4Configs, ipamV6Configs []*libnetwork.IpamConf) (libnetwork.Network, error) {\n\treturn controller.NewNetwork(networkType, networkName, \"\",\n\t\tlibnetwork.NetworkOptionGeneric(netOption),\n\t\tlibnetwork.NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", ipamV4Configs, ipamV6Configs, nil))\n}\n\nfunc getEmptyGenericOption() map[string]interface{} {\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = options.Generic{}\n\treturn genericOption\n}\n\nfunc getPortMapping() []types.PortBinding {\n\treturn []types.PortBinding{\n\t\t{Proto: types.TCP, Port: uint16(230), HostPort: uint16(23000)},\n\t\t{Proto: types.UDP, Port: uint16(200), HostPort: uint16(22000)},\n\t\t{Proto: types.TCP, Port: uint16(120), HostPort: uint16(12000)},\n\t\t{Proto: types.TCP, Port: uint16(320), HostPort: uint16(32000), HostPortEnd: uint16(32999)},\n\t\t{Proto: types.UDP, Port: uint16(420), HostPort: uint16(42000), HostPortEnd: uint16(42001)},\n\t}\n}\n\nfunc isNotFound(err error) bool {\n\t_, ok := (err).(types.NotFoundError)\n\treturn ok\n}\n\nfunc TestNull(t *testing.T) {\n\tcnt, err := controller.NewSandbox(\"null_container\",\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnetwork, err := createTestNetwork(\"null\", \"testnull\", options.Generic{}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep, err := network.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep.Join(cnt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep.Leave(cnt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := cnt.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// host type is special network. Cannot be removed.\n\terr = network.Delete()\n\tif err == nil {\n\t\tt.Fatal(err)\n\t}\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Unexpected error type\")\n\t}\n}\n\nfunc TestBridge(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.EnableIPv6: true,\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\":         \"testnetwork\",\n\t\t\t\"EnableICC\":          true,\n\t\t\t\"EnableIPMasquerade\": true,\n\t\t},\n\t}\n\tipamV4ConfList := []*libnetwork.IpamConf{{PreferredPool: \"192.168.100.0/24\", Gateway: \"192.168.100.1\"}}\n\tipamV6ConfList := []*libnetwork.IpamConf{{PreferredPool: \"fe90::/64\", Gateway: \"fe90::22\"}}\n\n\tnetwork, err := createTestNetwork(bridgeNetType, \"testnetwork\", netOption, ipamV4ConfList, ipamV6ConfList)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := network.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := network.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsb, err := controller.NewSandbox(containerID, libnetwork.OptionPortMapping(getPortMapping()))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tepInfo, err := ep.DriverInfo()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tpmd, ok := epInfo[netlabel.PortMap]\n\tif !ok {\n\t\tt.Fatalf(\"Could not find expected info in endpoint data\")\n\t}\n\tpm, ok := pmd.([]types.PortBinding)\n\tif !ok {\n\t\tt.Fatalf(\"Unexpected format for port mapping in endpoint operational data\")\n\t}\n\texpectedLen := 10\n\tif !bridge.IsV6Listenable() {\n\t\texpectedLen = 5\n\t}\n\tif len(pm) != expectedLen {\n\t\tt.Fatalf(\"Incomplete data for port mapping in endpoint operational data: %d\", len(pm))\n\t}\n}\n\nfunc TestUnknownDriver(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t_, err := createTestNetwork(\"unknowndriver\", \"testnetwork\", options.Generic{}, nil, nil)\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif !isNotFound(err) {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestNilRemoteDriver(t *testing.T) {\n\t_, err := controller.NewNetwork(\"framerelay\", \"dummy\", \"\",\n\t\tlibnetwork.NetworkOptionGeneric(getEmptyGenericOption()))\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif !isNotFound(err) {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestNetworkName(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\n\t_, err := createTestNetwork(bridgeNetType, \"\", netOption, nil, nil)\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(libnetwork.ErrInvalidName); !ok {\n\t\tt.Fatalf(\"Expected to fail with ErrInvalidName error. Got %v\", err)\n\t}\n\n\tnetworkName := \"testnetwork\"\n\tn, err := createTestNetwork(bridgeNetType, networkName, netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif n.Name() != networkName {\n\t\tt.Fatalf(\"Expected network name %s, got %s\", networkName, n.Name())\n\t}\n}\n\nfunc TestNetworkType(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif n.Type() != bridgeNetType {\n\t\tt.Fatalf(\"Expected network type %s, got %s\", bridgeNetType, n.Type())\n\t}\n}\n\nfunc TestNetworkID(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif n.ID() == \"\" {\n\t\tt.Fatal(\"Expected non-empty network id\")\n\t}\n}\n\nfunc TestDeleteNetworkWithActiveEndpoints(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\t\"BridgeName\": \"testnetwork\",\n\t}\n\toption := options.Generic{\n\t\tnetlabel.GenericData: netOption,\n\t}\n\n\tnetwork, err := createTestNetwork(bridgeNetType, \"testnetwork\", option, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep, err := network.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = network.Delete()\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(*libnetwork.ActiveEndpointsError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n\n\t// Done testing. Now cleanup.\n\tif err := ep.Delete(false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := network.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestNetworkConfig(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t// Verify config network cannot inherit another config network\n\tconfigNetwork, err := controller.NewNetwork(\"bridge\", \"config_network0\", \"\",\n\t\tlibnetwork.NetworkOptionConfigOnly(),\n\t\tlibnetwork.NetworkOptionConfigFrom(\"anotherConfigNw\"))\n\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n\n\t// Create supported config network\n\tnetOption := options.Generic{\n\t\t\"EnableICC\": false,\n\t}\n\toption := options.Generic{\n\t\tnetlabel.GenericData: netOption,\n\t}\n\tipamV4ConfList := []*libnetwork.IpamConf{{PreferredPool: \"192.168.100.0/24\", SubPool: \"192.168.100.128/25\", Gateway: \"192.168.100.1\"}}\n\tipamV6ConfList := []*libnetwork.IpamConf{{PreferredPool: \"2001:db8:abcd::/64\", SubPool: \"2001:db8:abcd::ef99/80\", Gateway: \"2001:db8:abcd::22\"}}\n\n\tnetOptions := []libnetwork.NetworkOption{\n\t\tlibnetwork.NetworkOptionConfigOnly(),\n\t\tlibnetwork.NetworkOptionEnableIPv6(true),\n\t\tlibnetwork.NetworkOptionGeneric(option),\n\t\tlibnetwork.NetworkOptionIpam(\"default\", \"\", ipamV4ConfList, ipamV6ConfList, nil),\n\t}\n\n\tconfigNetwork, err = controller.NewNetwork(bridgeNetType, \"config_network0\", \"\", netOptions...)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify a config-only network cannot be created with network operator configurations\n\tfor i, opt := range []libnetwork.NetworkOption{\n\t\tlibnetwork.NetworkOptionInternalNetwork(),\n\t\tlibnetwork.NetworkOptionAttachable(true),\n\t\tlibnetwork.NetworkOptionIngress(true),\n\t} {\n\t\t_, err = controller.NewNetwork(bridgeNetType, \"testBR\", \"\",\n\t\t\tlibnetwork.NetworkOptionConfigOnly(), opt)\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Expected to fail. But instead succeeded for option: %d\", i)\n\t\t}\n\t\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t\t}\n\t}\n\n\t// Verify a network cannot be created with both config-from and network specific configurations\n\tfor i, opt := range []libnetwork.NetworkOption{\n\t\tlibnetwork.NetworkOptionEnableIPv6(true),\n\t\tlibnetwork.NetworkOptionIpam(\"my-ipam\", \"\", nil, nil, nil),\n\t\tlibnetwork.NetworkOptionIpam(\"\", \"\", ipamV4ConfList, nil, nil),\n\t\tlibnetwork.NetworkOptionIpam(\"\", \"\", nil, ipamV6ConfList, nil),\n\t\tlibnetwork.NetworkOptionLabels(map[string]string{\"number\": \"two\"}),\n\t\tlibnetwork.NetworkOptionDriverOpts(map[string]string{\"com.docker.network.driver.mtu\": \"1600\"}),\n\t} {\n\t\t_, err = controller.NewNetwork(bridgeNetType, \"testBR\", \"\",\n\t\t\tlibnetwork.NetworkOptionConfigFrom(\"config_network0\"), opt)\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Expected to fail. But instead succeeded for option: %d\", i)\n\t\t}\n\t\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t\t}\n\t}\n\n\t// Create a valid network\n\tnetwork, err := controller.NewNetwork(bridgeNetType, \"testBR\", \"\",\n\t\tlibnetwork.NetworkOptionConfigFrom(\"config_network0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify the config network cannot be removed\n\terr = configNetwork.Delete()\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n\n\t// Delete network\n\tif err := network.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify the config network can now be removed\n\tif err := configNetwork.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n}\n\nfunc TestUnknownNetwork(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\t\"BridgeName\": \"testnetwork\",\n\t}\n\toption := options.Generic{\n\t\tnetlabel.GenericData: netOption,\n\t}\n\n\tnetwork, err := createTestNetwork(bridgeNetType, \"testnetwork\", option, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = network.Delete()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = network.Delete()\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(*libnetwork.UnknownNetworkError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestUnknownEndpoint(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\t\"BridgeName\": \"testnetwork\",\n\t}\n\toption := options.Generic{\n\t\tnetlabel.GenericData: netOption,\n\t}\n\tipamV4ConfList := []*libnetwork.IpamConf{{PreferredPool: \"192.168.100.0/24\"}}\n\n\tnetwork, err := createTestNetwork(bridgeNetType, \"testnetwork\", option, ipamV4ConfList, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = network.CreateEndpoint(\"\")\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\tif _, ok := err.(libnetwork.ErrInvalidName); !ok {\n\t\tt.Fatalf(\"Expected to fail with ErrInvalidName error. Actual error: %v\", err)\n\t}\n\n\tep, err := network.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep.Delete(false)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Done testing. Now cleanup\n\tif err := network.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestNetworkEndpointsWalkers(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t// Create network 1 and add 2 endpoint: ep11, ep12\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network1\",\n\t\t},\n\t}\n\n\tnet1, err := createTestNetwork(bridgeNetType, \"network1\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := net1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep11, err := net1.CreateEndpoint(\"ep11\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep11.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep12, err := net1.CreateEndpoint(\"ep12\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep12.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Test list methods on net1\n\tepList1 := net1.Endpoints()\n\tif len(epList1) != 2 {\n\t\tt.Fatalf(\"Endpoints() returned wrong number of elements: %d instead of 2\", len(epList1))\n\t}\n\t// endpoint order is not guaranteed\n\tfor _, e := range epList1 {\n\t\tif e != ep11 && e != ep12 {\n\t\t\tt.Fatal(\"Endpoints() did not return all the expected elements\")\n\t\t}\n\t}\n\n\t// Test Endpoint Walk method\n\tvar epName string\n\tvar epWanted libnetwork.Endpoint\n\twlk := func(ep libnetwork.Endpoint) bool {\n\t\tif ep.Name() == epName {\n\t\t\tepWanted = ep\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\t// Look for ep1 on network1\n\tepName = \"ep11\"\n\tnet1.WalkEndpoints(wlk)\n\tif epWanted == nil {\n\t\tt.Fatal(err)\n\t}\n\tif ep11 != epWanted {\n\t\tt.Fatal(err)\n\t}\n\n\tcurrent := len(controller.Networks())\n\n\t// Create network 2\n\tnetOption = options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network2\",\n\t\t},\n\t}\n\n\tnet2, err := createTestNetwork(bridgeNetType, \"network2\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := net2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Test Networks method\n\tif len(controller.Networks()) != current+1 {\n\t\tt.Fatalf(\"Did not find the expected number of networks\")\n\t}\n\n\t// Test Network Walk method\n\tvar netName string\n\tvar netWanted libnetwork.Network\n\tnwWlk := func(nw libnetwork.Network) bool {\n\t\tif nw.Name() == netName {\n\t\t\tnetWanted = nw\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\t// Look for network named \"network1\" and \"network2\"\n\tnetName = \"network1\"\n\tcontroller.WalkNetworks(nwWlk)\n\tif netWanted == nil {\n\t\tt.Fatal(err)\n\t}\n\tif net1.ID() != netWanted.ID() {\n\t\tt.Fatal(err)\n\t}\n\n\tnetName = \"network2\"\n\tcontroller.WalkNetworks(nwWlk)\n\tif netWanted == nil {\n\t\tt.Fatal(err)\n\t}\n\tif net2.ID() != netWanted.ID() {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestDuplicateEndpoint(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep2, err := n.CreateEndpoint(\"ep1\")\n\tdefer func() {\n\t\t// Cleanup ep2 as well, else network cleanup might fail for failure cases\n\t\tif ep2 != nil {\n\t\t\tif err := ep2.Delete(false); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestControllerQuery(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t// Create network 1\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network1\",\n\t\t},\n\t}\n\tnet1, err := createTestNetwork(bridgeNetType, \"network1\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := net1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Create network 2\n\tnetOption = options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network2\",\n\t\t},\n\t}\n\tnet2, err := createTestNetwork(bridgeNetType, \"network2\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := net2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t_, err = controller.NetworkByName(\"\")\n\tif err == nil {\n\t\tt.Fatalf(\"NetworkByName() succeeded with invalid target name\")\n\t}\n\tif _, ok := err.(libnetwork.ErrInvalidName); !ok {\n\t\tt.Fatalf(\"Expected NetworkByName() to fail with ErrInvalidName error. Got: %v\", err)\n\t}\n\n\t_, err = controller.NetworkByID(\"\")\n\tif err == nil {\n\t\tt.Fatalf(\"NetworkByID() succeeded with invalid target id\")\n\t}\n\tif _, ok := err.(libnetwork.ErrInvalidID); !ok {\n\t\tt.Fatalf(\"NetworkByID() failed with unexpected error: %v\", err)\n\t}\n\n\tg, err := controller.NetworkByID(\"network1\")\n\tif err == nil {\n\t\tt.Fatalf(\"Unexpected success for NetworkByID(): %v\", g)\n\t}\n\tif _, ok := err.(libnetwork.ErrNoSuchNetwork); !ok {\n\t\tt.Fatalf(\"NetworkByID() failed with unexpected error: %v\", err)\n\t}\n\n\tg, err = controller.NetworkByName(\"network1\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected failure for NetworkByName(): %v\", err)\n\t}\n\tif g == nil {\n\t\tt.Fatalf(\"NetworkByName() did not find the network\")\n\t}\n\n\tif g != net1 {\n\t\tt.Fatalf(\"NetworkByName() returned the wrong network\")\n\t}\n\n\tg, err = controller.NetworkByID(net1.ID())\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected failure for NetworkByID(): %v\", err)\n\t}\n\tif net1.ID() != g.ID() {\n\t\tt.Fatalf(\"NetworkByID() returned unexpected element: %v\", g)\n\t}\n\n\tg, err = controller.NetworkByName(\"network2\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected failure for NetworkByName(): %v\", err)\n\t}\n\tif g == nil {\n\t\tt.Fatalf(\"NetworkByName() did not find the network\")\n\t}\n\n\tif g != net2 {\n\t\tt.Fatalf(\"NetworkByName() returned the wrong network\")\n\t}\n\n\tg, err = controller.NetworkByID(net2.ID())\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected failure for NetworkByID(): %v\", err)\n\t}\n\tif net2.ID() != g.ID() {\n\t\tt.Fatalf(\"NetworkByID() returned unexpected element: %v\", g)\n\t}\n}\n\nfunc TestNetworkQuery(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\t// Create network 1 and add 2 endpoint: ep11, ep12\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network1\",\n\t\t},\n\t}\n\tnet1, err := createTestNetwork(bridgeNetType, \"network1\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := net1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep11, err := net1.CreateEndpoint(\"ep11\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep11.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep12, err := net1.CreateEndpoint(\"ep12\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep12.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\te, err := net1.EndpointByName(\"ep11\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif ep11 != e {\n\t\tt.Fatalf(\"EndpointByName() returned %v instead of %v\", e, ep11)\n\t}\n\n\te, err = net1.EndpointByName(\"\")\n\tif err == nil {\n\t\tt.Fatalf(\"EndpointByName() succeeded with invalid target name\")\n\t}\n\tif _, ok := err.(libnetwork.ErrInvalidName); !ok {\n\t\tt.Fatalf(\"Expected EndpointByName() to fail with ErrInvalidName error. Got: %v\", err)\n\t}\n\n\te, err = net1.EndpointByName(\"IamNotAnEndpoint\")\n\tif err == nil {\n\t\tt.Fatalf(\"EndpointByName() succeeded with unknown target name\")\n\t}\n\tif _, ok := err.(libnetwork.ErrNoSuchEndpoint); !ok {\n\t\tt.Fatal(err)\n\t}\n\tif e != nil {\n\t\tt.Fatalf(\"EndpointByName(): expected nil, got %v\", e)\n\t}\n\n\te, err = net1.EndpointByID(ep12.ID())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif ep12.ID() != e.ID() {\n\t\tt.Fatalf(\"EndpointByID() returned %v instead of %v\", e, ep12)\n\t}\n\n\te, err = net1.EndpointByID(\"\")\n\tif err == nil {\n\t\tt.Fatalf(\"EndpointByID() succeeded with invalid target id\")\n\t}\n\tif _, ok := err.(libnetwork.ErrInvalidID); !ok {\n\t\tt.Fatalf(\"EndpointByID() failed with unexpected error: %v\", err)\n\t}\n}\n\nconst containerID = \"valid_c\"\n\ntype fakeSandbox struct{}\n\nfunc (f *fakeSandbox) ID() string {\n\treturn \"fake sandbox\"\n}\n\nfunc (f *fakeSandbox) ContainerID() string {\n\treturn \"\"\n}\n\nfunc (f *fakeSandbox) Key() string {\n\treturn \"fake key\"\n}\n\nfunc (f *fakeSandbox) Labels() map[string]interface{} {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) Statistics() (map[string]*types.InterfaceStatistics, error) {\n\treturn nil, nil\n}\n\nfunc (f *fakeSandbox) Refresh(opts ...libnetwork.SandboxOption) error {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) Delete() error {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) Rename(name string) error {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) SetKey(key string) error {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) ResolveName(name string, ipType int) ([]net.IP, bool) {\n\treturn nil, false\n}\n\nfunc (f *fakeSandbox) ResolveIP(ip string) string {\n\treturn \"\"\n}\n\nfunc (f *fakeSandbox) ResolveService(name string) ([]*net.SRV, []net.IP) {\n\treturn nil, nil\n}\n\nfunc (f *fakeSandbox) Endpoints() []libnetwork.Endpoint {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) EnableService() error {\n\treturn nil\n}\n\nfunc (f *fakeSandbox) DisableService() error {\n\treturn nil\n}\n\nfunc TestEndpointDeleteWithActiveContainer(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tn2, err := createTestNetwork(bridgeNetType, \"testnetwork2\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork2\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep.Delete(false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcnt, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tdefer func() {\n\t\tif err := cnt.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(cnt)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep.Leave(cnt)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Delete(false)\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif _, ok := err.(*libnetwork.ActiveContainerError); !ok {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestEndpointMultipleJoins(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testmultiple\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testmultiple\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsbx1, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsbx2, err := controller.NewSandbox(\"c2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sbx1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\terr = ep.Leave(sbx1)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Join(sbx2)\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail multiple joins for the same endpoint\")\n\t}\n\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Failed with unexpected error type: %T. Desc: %s\", err, err.Error())\n\t}\n\n}\n\nfunc TestLeaveAll(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\t// If this goes through, it means cnt.Delete() effectively detached from all the endpoints\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tn2, err := createTestNetwork(bridgeNetType, \"testnetwork2\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork2\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep1, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep2, err := n2.CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcnt, err := controller.NewSandbox(\"leaveall\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep1.Join(cnt)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to join ep1: %v\", err)\n\t}\n\n\terr = ep2.Join(cnt)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to join ep2: %v\", err)\n\t}\n\n\terr = cnt.Delete()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestContainerInvalidLeave(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(bridgeNetType, \"testnetwork\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := ep.Delete(false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tcnt, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := cnt.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep.Leave(cnt)\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail leave from an endpoint which has no active join\")\n\t}\n\tif _, ok := err.(types.ForbiddenError); !ok {\n\t\tt.Fatalf(\"Failed with unexpected error type: %T. Desc: %s\", err, err.Error())\n\t}\n\n\tif err = ep.Leave(nil); err == nil {\n\t\tt.Fatalf(\"Expected to fail leave nil Sandbox\")\n\t}\n\tif _, ok := err.(types.BadRequestError); !ok {\n\t\tt.Fatalf(\"Unexpected error type returned: %T. Desc: %s\", err, err.Error())\n\t}\n\n\tfsbx := &fakeSandbox{}\n\tif err = ep.Leave(fsbx); err == nil {\n\t\tt.Fatalf(\"Expected to fail leave with invalid Sandbox\")\n\t}\n\tif _, ok := err.(types.BadRequestError); !ok {\n\t\tt.Fatalf(\"Unexpected error type returned: %T. Desc: %s\", err, err.Error())\n\t}\n}\n\nfunc TestEndpointUpdateParent(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\tn, err := createTestNetwork(\"bridge\", \"testnetwork\", options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"testnetwork\",\n\t\t},\n\t}, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep1, err := n.CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tep2, err := n.CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsbx1, err := controller.NewSandbox(containerID,\n\t\tlibnetwork.OptionHostname(\"test\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.1\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx1.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsbx2, err := controller.NewSandbox(\"c2\",\n\t\tlibnetwork.OptionHostname(\"test2\"),\n\t\tlibnetwork.OptionDomainname(\"docker.io\"),\n\t\tlibnetwork.OptionHostsPath(\"/var/lib/docker/test_network/container2/hosts\"),\n\t\tlibnetwork.OptionExtraHost(\"web\", \"192.168.0.2\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := sbx2.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\terr = ep1.Join(sbx1)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ep2.Join(sbx2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestInvalidRemoteDriver(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tt.Skip(\"Skipping test when not running inside a Container\")\n\t}\n\n\tmux := http.NewServeMux()\n\tserver := httptest.NewServer(mux)\n\tif server == nil {\n\t\tt.Fatal(\"Failed to start an HTTP Server\")\n\t}\n\tdefer server.Close()\n\n\ttype pluginRequest struct {\n\t\tname string\n\t}\n\n\tmux.HandleFunc(\"/Plugin.Activate\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/vnd.docker.plugins.v1+json\")\n\t\tfmt.Fprintln(w, `{\"Implements\": [\"InvalidDriver\"]}`)\n\t})\n\n\tif err := os.MkdirAll(\"/etc/docker/plugins\", 0755); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := os.RemoveAll(\"/etc/docker/plugins\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif err := ioutil.WriteFile(\"/etc/docker/plugins/invalid-network-driver.spec\", []byte(server.URL), 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tctrlr, err := libnetwork.New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer ctrlr.Stop()\n\n\t_, err = ctrlr.NewNetwork(\"invalid-network-driver\", \"dummy\", \"\",\n\t\tlibnetwork.NetworkOptionGeneric(getEmptyGenericOption()))\n\tif err == nil {\n\t\tt.Fatal(\"Expected to fail. But instead succeeded\")\n\t}\n\n\tif err != plugins.ErrNotImplements {\n\t\tt.Fatalf(\"Did not fail with expected error. Actual error: %v\", err)\n\t}\n}\n\nfunc TestValidRemoteDriver(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tt.Skip(\"Skipping test when not running inside a Container\")\n\t}\n\n\tmux := http.NewServeMux()\n\tserver := httptest.NewServer(mux)\n\tif server == nil {\n\t\tt.Fatal(\"Failed to start an HTTP Server\")\n\t}\n\tdefer server.Close()\n\n\ttype pluginRequest struct {\n\t\tname string\n\t}\n\n\tmux.HandleFunc(\"/Plugin.Activate\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/vnd.docker.plugins.v1+json\")\n\t\tfmt.Fprintf(w, `{\"Implements\": [\"%s\"]}`, driverapi.NetworkPluginEndpointType)\n\t})\n\tmux.HandleFunc(fmt.Sprintf(\"/%s.GetCapabilities\", driverapi.NetworkPluginEndpointType), func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/vnd.docker.plugins.v1+json\")\n\t\tfmt.Fprintf(w, `{\"Scope\":\"local\"}`)\n\t})\n\tmux.HandleFunc(fmt.Sprintf(\"/%s.CreateNetwork\", driverapi.NetworkPluginEndpointType), func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/vnd.docker.plugins.v1+json\")\n\t\tfmt.Fprintf(w, \"null\")\n\t})\n\tmux.HandleFunc(fmt.Sprintf(\"/%s.DeleteNetwork\", driverapi.NetworkPluginEndpointType), func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Header().Set(\"Content-Type\", \"application/vnd.docker.plugins.v1+json\")\n\t\tfmt.Fprintf(w, \"null\")\n\t})\n\n\tif err := os.MkdirAll(\"/etc/docker/plugins\", 0755); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := os.RemoveAll(\"/etc/docker/plugins\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tif err := ioutil.WriteFile(\"/etc/docker/plugins/valid-network-driver.spec\", []byte(server.URL), 0644); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tn, err := controller.NewNetwork(\"valid-network-driver\", \"dummy\", \"\",\n\t\tlibnetwork.NetworkOptionGeneric(getEmptyGenericOption()))\n\tif err != nil {\n\t\t// Only fail if we could not find the plugin driver\n\t\tif isNotFound(err) {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n}\n\nvar (\n\tonce    sync.Once\n\tstart   = make(chan struct{})\n\tdone    = make(chan chan struct{}, numThreads-1)\n\torigins = netns.None()\n\ttestns  = netns.None()\n\tsboxes  = make([]libnetwork.Sandbox, numThreads)\n)\n\nconst (\n\titerCnt    = 25\n\tnumThreads = 3\n\tfirst      = 1\n\tlast       = numThreads\n\tdebug      = false\n)\n\nfunc createGlobalInstance(t *testing.T) {\n\tvar err error\n\tdefer close(start)\n\n\torigins, err = netns.Get()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif testutils.IsRunningInContainer() {\n\t\ttestns = origins\n\t} else {\n\t\ttestns, err = netns.New()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tnetOption := options.Generic{\n\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\"BridgeName\": \"network\",\n\t\t},\n\t}\n\n\tnet1, err := controller.NetworkByName(\"testhost\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tnet2, err := createTestNetwork(\"bridge\", \"network2\", netOption, nil, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = net1.CreateEndpoint(\"pep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = net2.CreateEndpoint(\"pep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t_, err = net2.CreateEndpoint(\"pep3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif sboxes[first-1], err = controller.NewSandbox(fmt.Sprintf(\"%drace\", first), libnetwork.OptionUseDefaultSandbox()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfor thd := first + 1; thd <= last; thd++ {\n\t\tif sboxes[thd-1], err = controller.NewSandbox(fmt.Sprintf(\"%drace\", thd)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc debugf(format string, a ...interface{}) (int, error) {\n\tif debug {\n\t\treturn fmt.Printf(format, a...)\n\t}\n\n\treturn 0, nil\n}\n"
        },
        {
          "name": "machines",
          "type": "blob",
          "size": 2.37109375,
          "content": "#/bin/sh\n\nset -e\n\nusage()\n{\ncat << EOF\nNAME:\n    machines - Create Test Environments for Docker Networking\n\nVERSION:\n    0.1\n\nUSAGE:\n    $0 <command> [command_options] [arguments...]\n\nCOMMANDS:\n    help    \n            Help and usage\n\n    up <kv-store> <scale>    \n            Create environment with given KV store\n            zookeeper | etcd | consul (default)\n\t    Create N nodes, default = 2\n\n    destroy \n            Destroy Environment\n\nEOF\n}\n\nstep() {\n    printf \"\\033[0;36m-----> $@\\033[0m\\n\"\n}\n\nup()\n{\n\tstep \"Creating KV Store Machine\"\n\tdocker-machine create \\\n\t    -d virtualbox \\\n\t    mh-kv\n\n\tstep \"KV Store is $1\"\n        step \"Starting KV Container\"\n        case \"$1\" in\n            etcd)\n            cluster_store=\"cluster-store=etcd://$(docker-machine ip mh-kv):2379\"\n            docker $(docker-machine config mh-kv) run -d \\\n                -p \"2379:2379\" \\\n                -h \"etcd\" \\\n                --name \"etcd\" \\\n                quay.io/coreos/etcd:v2.2.1 \\\n\t\t--listen-client-urls=\"http://0.0.0.0:2379\" \\\n                --advertise-client-urls=\"http://$(docker-machine ip mh-kv):2379\"\n           ;;\n            zookeeper)\n            cluster_store=\"cluster-store=zk://$(docker-machine ip mh-kv):2181\" \n            docker $(docker-machine config mh-kv) run -d \\\n                -p \"2181:2181\" \\\n                -h \"zookeeper\" \\\n                --name \"zookeeper\" \\\n                tianon/zookeeper\n            ;;\n            *)\n            cluster_store=\"cluster-store=consul://$(docker-machine ip mh-kv):8500\"\n            docker $(docker-machine config mh-kv) run -d \\\n\t        -p \"8500:8500\" \\\n\t        -h \"consul\" \\\n                --name \"consul\" \\\n\t        progrium/consul -server -bootstrap-expect 1\n            ;;\n        esac\n\n\tmachines=$2\n        if [ -z machines ]; then\n            machines=2\n        fi\n\tstep \"Creating $machines Machines\"       \n \n        for i in $(seq $machines); do\n\t    step \"Creating machine $i\"\n            docker-machine create \\\n\t        -d virtualbox \\\n\t        --engine-opt=\"cluster-advertise=eth1:2376\" \\\n\t        --engine-opt=\"$cluster_store\" \\\n\t        mh-$i\n        done\t\t\t\t\n}\n\ndestroy()\n{\n    for x in $(docker-machine ls | grep mh- | awk '{ print $1 }'); do\n\tdocker-machine rm $x\n    done\n}\n\ncase \"$1\" in\n    up)\n        shift\n        up $@\n        ;;\n    destroy)\n        destroy $@\n        ;;\n    help)\n        usage\n        ;;\n    *)\n\tusage\n        ;;\nesac\n"
        },
        {
          "name": "netlabel",
          "type": "tree",
          "content": null
        },
        {
          "name": "netutils",
          "type": "tree",
          "content": null
        },
        {
          "name": "network.go",
          "type": "blob",
          "size": 57.1005859375,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/driverapi\"\n\t\"github.com/docker/libnetwork/etchosts\"\n\t\"github.com/docker/libnetwork/internal/setmatrix\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/netutils\"\n\t\"github.com/docker/libnetwork/networkdb\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// A Network represents a logical connectivity zone that containers may\n// join using the Link method. A Network is managed by a specific driver.\ntype Network interface {\n\t// A user chosen name for this network.\n\tName() string\n\n\t// A system generated id for this network.\n\tID() string\n\n\t// The type of network, which corresponds to its managing driver.\n\tType() string\n\n\t// Create a new endpoint to this network symbolically identified by the\n\t// specified unique name. The options parameter carries driver specific options.\n\tCreateEndpoint(name string, options ...EndpointOption) (Endpoint, error)\n\n\t// Delete the network.\n\tDelete(options ...NetworkDeleteOption) error\n\n\t// Endpoints returns the list of Endpoint(s) in this network.\n\tEndpoints() []Endpoint\n\n\t// WalkEndpoints uses the provided function to walk the Endpoints\n\tWalkEndpoints(walker EndpointWalker)\n\n\t// EndpointByName returns the Endpoint which has the passed name. If not found, the error ErrNoSuchEndpoint is returned.\n\tEndpointByName(name string) (Endpoint, error)\n\n\t// EndpointByID returns the Endpoint which has the passed id. If not found, the error ErrNoSuchEndpoint is returned.\n\tEndpointByID(id string) (Endpoint, error)\n\n\t// Return certain operational data belonging to this network\n\tInfo() NetworkInfo\n}\n\n// NetworkInfo returns some configuration and operational information about the network\ntype NetworkInfo interface {\n\tIpamConfig() (string, map[string]string, []*IpamConf, []*IpamConf)\n\tIpamInfo() ([]*IpamInfo, []*IpamInfo)\n\tDriverOptions() map[string]string\n\tScope() string\n\tIPv6Enabled() bool\n\tInternal() bool\n\tAttachable() bool\n\tIngress() bool\n\tConfigFrom() string\n\tConfigOnly() bool\n\tLabels() map[string]string\n\tDynamic() bool\n\tCreated() time.Time\n\t// Peers returns a slice of PeerInfo structures which has the information about the peer\n\t// nodes participating in the same overlay network. This is currently the per-network\n\t// gossip cluster. For non-dynamic overlay networks and bridge networks it returns an\n\t// empty slice\n\tPeers() []networkdb.PeerInfo\n\t//Services returns a map of services keyed by the service name with the details\n\t//of all the tasks that belong to the service. Applicable only in swarm mode.\n\tServices() map[string]ServiceInfo\n}\n\n// EndpointWalker is a client provided function which will be used to walk the Endpoints.\n// When the function returns true, the walk will stop.\ntype EndpointWalker func(ep Endpoint) bool\n\n// ipInfo is the reverse mapping from IP to service name to serve the PTR query.\n// extResolver is set if an external server resolves a service name to this IP.\n// Its an indication to defer PTR queries also to that external server.\ntype ipInfo struct {\n\tname        string\n\tserviceID   string\n\textResolver bool\n}\n\n// svcMapEntry is the body of the element into the svcMap\n// The ip is a string because the SetMatrix does not accept non hashable values\ntype svcMapEntry struct {\n\tip        string\n\tserviceID string\n}\n\ntype svcInfo struct {\n\tsvcMap     setmatrix.SetMatrix\n\tsvcIPv6Map setmatrix.SetMatrix\n\tipMap      setmatrix.SetMatrix\n\tservice    map[string][]servicePorts\n}\n\n// backing container or host's info\ntype serviceTarget struct {\n\tname string\n\tip   net.IP\n\tport uint16\n}\n\ntype servicePorts struct {\n\tportName string\n\tproto    string\n\ttarget   []serviceTarget\n}\n\ntype networkDBTable struct {\n\tname    string\n\tobjType driverapi.ObjectType\n}\n\n// IpamConf contains all the ipam related configurations for a network\ntype IpamConf struct {\n\t// The master address pool for containers and network interfaces\n\tPreferredPool string\n\t// A subset of the master pool. If specified,\n\t// this becomes the container pool\n\tSubPool string\n\t// Preferred Network Gateway address (optional)\n\tGateway string\n\t// Auxiliary addresses for network driver. Must be within the master pool.\n\t// libnetwork will reserve them if they fall into the container pool\n\tAuxAddresses map[string]string\n}\n\n// Validate checks whether the configuration is valid\nfunc (c *IpamConf) Validate() error {\n\tif c.Gateway != \"\" && nil == net.ParseIP(c.Gateway) {\n\t\treturn types.BadRequestErrorf(\"invalid gateway address %s in Ipam configuration\", c.Gateway)\n\t}\n\treturn nil\n}\n\n// IpamInfo contains all the ipam related operational info for a network\ntype IpamInfo struct {\n\tPoolID string\n\tMeta   map[string]string\n\tdriverapi.IPAMData\n}\n\n// MarshalJSON encodes IpamInfo into json message\nfunc (i *IpamInfo) MarshalJSON() ([]byte, error) {\n\tm := map[string]interface{}{\n\t\t\"PoolID\": i.PoolID,\n\t}\n\tv, err := json.Marshal(&i.IPAMData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tm[\"IPAMData\"] = string(v)\n\n\tif i.Meta != nil {\n\t\tm[\"Meta\"] = i.Meta\n\t}\n\treturn json.Marshal(m)\n}\n\n// UnmarshalJSON decodes json message into PoolData\nfunc (i *IpamInfo) UnmarshalJSON(data []byte) error {\n\tvar (\n\t\tm   map[string]interface{}\n\t\terr error\n\t)\n\tif err = json.Unmarshal(data, &m); err != nil {\n\t\treturn err\n\t}\n\ti.PoolID = m[\"PoolID\"].(string)\n\tif v, ok := m[\"Meta\"]; ok {\n\t\tb, _ := json.Marshal(v)\n\t\tif err = json.Unmarshal(b, &i.Meta); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif v, ok := m[\"IPAMData\"]; ok {\n\t\tif err = json.Unmarshal([]byte(v.(string)), &i.IPAMData); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\ntype network struct {\n\tctrlr            *controller\n\tname             string\n\tnetworkType      string\n\tid               string\n\tcreated          time.Time\n\tscope            string // network data scope\n\tlabels           map[string]string\n\tipamType         string\n\tipamOptions      map[string]string\n\taddrSpace        string\n\tipamV4Config     []*IpamConf\n\tipamV6Config     []*IpamConf\n\tipamV4Info       []*IpamInfo\n\tipamV6Info       []*IpamInfo\n\tenableIPv6       bool\n\tpostIPv6         bool\n\tepCnt            *endpointCnt\n\tgeneric          options.Generic\n\tdbIndex          uint64\n\tdbExists         bool\n\tpersist          bool\n\tstopWatchCh      chan struct{}\n\tdrvOnce          *sync.Once\n\tresolverOnce     sync.Once\n\tresolver         []Resolver\n\tinternal         bool\n\tattachable       bool\n\tinDelete         bool\n\tingress          bool\n\tdriverTables     []networkDBTable\n\tdynamic          bool\n\tconfigOnly       bool\n\tconfigFrom       string\n\tloadBalancerIP   net.IP\n\tloadBalancerMode string\n\tsync.Mutex\n}\n\nconst (\n\tloadBalancerModeNAT     = \"NAT\"\n\tloadBalancerModeDSR     = \"DSR\"\n\tloadBalancerModeDefault = loadBalancerModeNAT\n)\n\nfunc (n *network) Name() string {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.name\n}\n\nfunc (n *network) ID() string {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.id\n}\n\nfunc (n *network) Created() time.Time {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.created\n}\n\nfunc (n *network) Type() string {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.networkType\n}\n\nfunc (n *network) Key() []string {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn []string{datastore.NetworkKeyPrefix, n.id}\n}\n\nfunc (n *network) KeyPrefix() []string {\n\treturn []string{datastore.NetworkKeyPrefix}\n}\n\nfunc (n *network) Value() []byte {\n\tn.Lock()\n\tdefer n.Unlock()\n\tb, err := json.Marshal(n)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn b\n}\n\nfunc (n *network) SetValue(value []byte) error {\n\treturn json.Unmarshal(value, n)\n}\n\nfunc (n *network) Index() uint64 {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.dbIndex\n}\n\nfunc (n *network) SetIndex(index uint64) {\n\tn.Lock()\n\tn.dbIndex = index\n\tn.dbExists = true\n\tn.Unlock()\n}\n\nfunc (n *network) Exists() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.dbExists\n}\n\nfunc (n *network) Skip() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn !n.persist\n}\n\nfunc (n *network) New() datastore.KVObject {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn &network{\n\t\tctrlr:   n.ctrlr,\n\t\tdrvOnce: &sync.Once{},\n\t\tscope:   n.scope,\n\t}\n}\n\n// CopyTo deep copies to the destination IpamConfig\nfunc (c *IpamConf) CopyTo(dstC *IpamConf) error {\n\tdstC.PreferredPool = c.PreferredPool\n\tdstC.SubPool = c.SubPool\n\tdstC.Gateway = c.Gateway\n\tif c.AuxAddresses != nil {\n\t\tdstC.AuxAddresses = make(map[string]string, len(c.AuxAddresses))\n\t\tfor k, v := range c.AuxAddresses {\n\t\t\tdstC.AuxAddresses[k] = v\n\t\t}\n\t}\n\treturn nil\n}\n\n// CopyTo deep copies to the destination IpamInfo\nfunc (i *IpamInfo) CopyTo(dstI *IpamInfo) error {\n\tdstI.PoolID = i.PoolID\n\tif i.Meta != nil {\n\t\tdstI.Meta = make(map[string]string)\n\t\tfor k, v := range i.Meta {\n\t\t\tdstI.Meta[k] = v\n\t\t}\n\t}\n\n\tdstI.AddressSpace = i.AddressSpace\n\tdstI.Pool = types.GetIPNetCopy(i.Pool)\n\tdstI.Gateway = types.GetIPNetCopy(i.Gateway)\n\n\tif i.AuxAddresses != nil {\n\t\tdstI.AuxAddresses = make(map[string]*net.IPNet)\n\t\tfor k, v := range i.AuxAddresses {\n\t\t\tdstI.AuxAddresses[k] = types.GetIPNetCopy(v)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (n *network) validateConfiguration() error {\n\tif n.configOnly {\n\t\t// Only supports network specific configurations.\n\t\t// Network operator configurations are not supported.\n\t\tif n.ingress || n.internal || n.attachable || n.scope != \"\" {\n\t\t\treturn types.ForbiddenErrorf(\"configuration network can only contain network \" +\n\t\t\t\t\"specific fields. Network operator fields like \" +\n\t\t\t\t\"[ ingress | internal | attachable | scope ] are not supported.\")\n\t\t}\n\t}\n\tif n.configFrom != \"\" {\n\t\tif n.configOnly {\n\t\t\treturn types.ForbiddenErrorf(\"a configuration network cannot depend on another configuration network\")\n\t\t}\n\t\tif n.ipamType != \"\" &&\n\t\t\tn.ipamType != defaultIpamForNetworkType(n.networkType) ||\n\t\t\tn.enableIPv6 ||\n\t\t\tlen(n.labels) > 0 || len(n.ipamOptions) > 0 ||\n\t\t\tlen(n.ipamV4Config) > 0 || len(n.ipamV6Config) > 0 {\n\t\t\treturn types.ForbiddenErrorf(\"user specified configurations are not supported if the network depends on a configuration network\")\n\t\t}\n\t\tif len(n.generic) > 0 {\n\t\t\tif data, ok := n.generic[netlabel.GenericData]; ok {\n\t\t\t\tvar (\n\t\t\t\t\tdriverOptions map[string]string\n\t\t\t\t\topts          interface{}\n\t\t\t\t)\n\t\t\t\tswitch t := data.(type) {\n\t\t\t\tcase map[string]interface{}, map[string]string:\n\t\t\t\t\topts = t\n\t\t\t\t}\n\t\t\t\tba, err := json.Marshal(opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to validate network configuration: %v\", err)\n\t\t\t\t}\n\t\t\t\tif err := json.Unmarshal(ba, &driverOptions); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to validate network configuration: %v\", err)\n\t\t\t\t}\n\t\t\t\tif len(driverOptions) > 0 {\n\t\t\t\t\treturn types.ForbiddenErrorf(\"network driver options are not supported if the network depends on a configuration network\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Applies network specific configurations\nfunc (n *network) applyConfigurationTo(to *network) error {\n\tto.enableIPv6 = n.enableIPv6\n\tif len(n.labels) > 0 {\n\t\tto.labels = make(map[string]string, len(n.labels))\n\t\tfor k, v := range n.labels {\n\t\t\tif _, ok := to.labels[k]; !ok {\n\t\t\t\tto.labels[k] = v\n\t\t\t}\n\t\t}\n\t}\n\tif len(n.ipamType) != 0 {\n\t\tto.ipamType = n.ipamType\n\t}\n\tif len(n.ipamOptions) > 0 {\n\t\tto.ipamOptions = make(map[string]string, len(n.ipamOptions))\n\t\tfor k, v := range n.ipamOptions {\n\t\t\tif _, ok := to.ipamOptions[k]; !ok {\n\t\t\t\tto.ipamOptions[k] = v\n\t\t\t}\n\t\t}\n\t}\n\tif len(n.ipamV4Config) > 0 {\n\t\tto.ipamV4Config = make([]*IpamConf, 0, len(n.ipamV4Config))\n\t\tto.ipamV4Config = append(to.ipamV4Config, n.ipamV4Config...)\n\t}\n\tif len(n.ipamV6Config) > 0 {\n\t\tto.ipamV6Config = make([]*IpamConf, 0, len(n.ipamV6Config))\n\t\tto.ipamV6Config = append(to.ipamV6Config, n.ipamV6Config...)\n\t}\n\tif len(n.generic) > 0 {\n\t\tto.generic = options.Generic{}\n\t\tfor k, v := range n.generic {\n\t\t\tto.generic[k] = v\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *network) CopyTo(o datastore.KVObject) error {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tdstN := o.(*network)\n\tdstN.name = n.name\n\tdstN.id = n.id\n\tdstN.created = n.created\n\tdstN.networkType = n.networkType\n\tdstN.scope = n.scope\n\tdstN.dynamic = n.dynamic\n\tdstN.ipamType = n.ipamType\n\tdstN.enableIPv6 = n.enableIPv6\n\tdstN.persist = n.persist\n\tdstN.postIPv6 = n.postIPv6\n\tdstN.dbIndex = n.dbIndex\n\tdstN.dbExists = n.dbExists\n\tdstN.drvOnce = n.drvOnce\n\tdstN.internal = n.internal\n\tdstN.attachable = n.attachable\n\tdstN.inDelete = n.inDelete\n\tdstN.ingress = n.ingress\n\tdstN.configOnly = n.configOnly\n\tdstN.configFrom = n.configFrom\n\tdstN.loadBalancerIP = n.loadBalancerIP\n\tdstN.loadBalancerMode = n.loadBalancerMode\n\n\t// copy labels\n\tif dstN.labels == nil {\n\t\tdstN.labels = make(map[string]string, len(n.labels))\n\t}\n\tfor k, v := range n.labels {\n\t\tdstN.labels[k] = v\n\t}\n\n\tif n.ipamOptions != nil {\n\t\tdstN.ipamOptions = make(map[string]string, len(n.ipamOptions))\n\t\tfor k, v := range n.ipamOptions {\n\t\t\tdstN.ipamOptions[k] = v\n\t\t}\n\t}\n\n\tfor _, v4conf := range n.ipamV4Config {\n\t\tdstV4Conf := &IpamConf{}\n\t\tv4conf.CopyTo(dstV4Conf)\n\t\tdstN.ipamV4Config = append(dstN.ipamV4Config, dstV4Conf)\n\t}\n\n\tfor _, v4info := range n.ipamV4Info {\n\t\tdstV4Info := &IpamInfo{}\n\t\tv4info.CopyTo(dstV4Info)\n\t\tdstN.ipamV4Info = append(dstN.ipamV4Info, dstV4Info)\n\t}\n\n\tfor _, v6conf := range n.ipamV6Config {\n\t\tdstV6Conf := &IpamConf{}\n\t\tv6conf.CopyTo(dstV6Conf)\n\t\tdstN.ipamV6Config = append(dstN.ipamV6Config, dstV6Conf)\n\t}\n\n\tfor _, v6info := range n.ipamV6Info {\n\t\tdstV6Info := &IpamInfo{}\n\t\tv6info.CopyTo(dstV6Info)\n\t\tdstN.ipamV6Info = append(dstN.ipamV6Info, dstV6Info)\n\t}\n\n\tdstN.generic = options.Generic{}\n\tfor k, v := range n.generic {\n\t\tdstN.generic[k] = v\n\t}\n\n\treturn nil\n}\n\nfunc (n *network) DataScope() string {\n\ts := n.Scope()\n\t// All swarm scope networks have local datascope\n\tif s == datastore.SwarmScope {\n\t\ts = datastore.LocalScope\n\t}\n\treturn s\n}\n\nfunc (n *network) getEpCnt() *endpointCnt {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.epCnt\n}\n\n// TODO : Can be made much more generic with the help of reflection (but has some golang limitations)\nfunc (n *network) MarshalJSON() ([]byte, error) {\n\tnetMap := make(map[string]interface{})\n\tnetMap[\"name\"] = n.name\n\tnetMap[\"id\"] = n.id\n\tnetMap[\"created\"] = n.created\n\tnetMap[\"networkType\"] = n.networkType\n\tnetMap[\"scope\"] = n.scope\n\tnetMap[\"labels\"] = n.labels\n\tnetMap[\"ipamType\"] = n.ipamType\n\tnetMap[\"ipamOptions\"] = n.ipamOptions\n\tnetMap[\"addrSpace\"] = n.addrSpace\n\tnetMap[\"enableIPv6\"] = n.enableIPv6\n\tif n.generic != nil {\n\t\tnetMap[\"generic\"] = n.generic\n\t}\n\tnetMap[\"persist\"] = n.persist\n\tnetMap[\"postIPv6\"] = n.postIPv6\n\tif len(n.ipamV4Config) > 0 {\n\t\tics, err := json.Marshal(n.ipamV4Config)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnetMap[\"ipamV4Config\"] = string(ics)\n\t}\n\tif len(n.ipamV4Info) > 0 {\n\t\tiis, err := json.Marshal(n.ipamV4Info)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnetMap[\"ipamV4Info\"] = string(iis)\n\t}\n\tif len(n.ipamV6Config) > 0 {\n\t\tics, err := json.Marshal(n.ipamV6Config)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnetMap[\"ipamV6Config\"] = string(ics)\n\t}\n\tif len(n.ipamV6Info) > 0 {\n\t\tiis, err := json.Marshal(n.ipamV6Info)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnetMap[\"ipamV6Info\"] = string(iis)\n\t}\n\tnetMap[\"internal\"] = n.internal\n\tnetMap[\"attachable\"] = n.attachable\n\tnetMap[\"inDelete\"] = n.inDelete\n\tnetMap[\"ingress\"] = n.ingress\n\tnetMap[\"configOnly\"] = n.configOnly\n\tnetMap[\"configFrom\"] = n.configFrom\n\tnetMap[\"loadBalancerIP\"] = n.loadBalancerIP\n\tnetMap[\"loadBalancerMode\"] = n.loadBalancerMode\n\treturn json.Marshal(netMap)\n}\n\n// TODO : Can be made much more generic with the help of reflection (but has some golang limitations)\nfunc (n *network) UnmarshalJSON(b []byte) (err error) {\n\tvar netMap map[string]interface{}\n\tif err := json.Unmarshal(b, &netMap); err != nil {\n\t\treturn err\n\t}\n\tn.name = netMap[\"name\"].(string)\n\tn.id = netMap[\"id\"].(string)\n\t// \"created\" is not available in older versions\n\tif v, ok := netMap[\"created\"]; ok {\n\t\t// n.created is time.Time but marshalled as string\n\t\tif err = n.created.UnmarshalText([]byte(v.(string))); err != nil {\n\t\t\tlogrus.Warnf(\"failed to unmarshal creation time %v: %v\", v, err)\n\t\t\tn.created = time.Time{}\n\t\t}\n\t}\n\tn.networkType = netMap[\"networkType\"].(string)\n\tn.enableIPv6 = netMap[\"enableIPv6\"].(bool)\n\n\t// if we weren't unmarshaling to netMap we could simply set n.labels\n\t// unfortunately, we can't because map[string]interface{} != map[string]string\n\tif labels, ok := netMap[\"labels\"].(map[string]interface{}); ok {\n\t\tn.labels = make(map[string]string, len(labels))\n\t\tfor label, value := range labels {\n\t\t\tn.labels[label] = value.(string)\n\t\t}\n\t}\n\n\tif v, ok := netMap[\"ipamOptions\"]; ok {\n\t\tif iOpts, ok := v.(map[string]interface{}); ok {\n\t\t\tn.ipamOptions = make(map[string]string, len(iOpts))\n\t\t\tfor k, v := range iOpts {\n\t\t\t\tn.ipamOptions[k] = v.(string)\n\t\t\t}\n\t\t}\n\t}\n\n\tif v, ok := netMap[\"generic\"]; ok {\n\t\tn.generic = v.(map[string]interface{})\n\t\t// Restore opts in their map[string]string form\n\t\tif v, ok := n.generic[netlabel.GenericData]; ok {\n\t\t\tvar lmap map[string]string\n\t\t\tba, err := json.Marshal(v)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := json.Unmarshal(ba, &lmap); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tn.generic[netlabel.GenericData] = lmap\n\t\t}\n\t}\n\tif v, ok := netMap[\"persist\"]; ok {\n\t\tn.persist = v.(bool)\n\t}\n\tif v, ok := netMap[\"postIPv6\"]; ok {\n\t\tn.postIPv6 = v.(bool)\n\t}\n\tif v, ok := netMap[\"ipamType\"]; ok {\n\t\tn.ipamType = v.(string)\n\t} else {\n\t\tn.ipamType = ipamapi.DefaultIPAM\n\t}\n\tif v, ok := netMap[\"addrSpace\"]; ok {\n\t\tn.addrSpace = v.(string)\n\t}\n\tif v, ok := netMap[\"ipamV4Config\"]; ok {\n\t\tif err := json.Unmarshal([]byte(v.(string)), &n.ipamV4Config); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif v, ok := netMap[\"ipamV4Info\"]; ok {\n\t\tif err := json.Unmarshal([]byte(v.(string)), &n.ipamV4Info); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif v, ok := netMap[\"ipamV6Config\"]; ok {\n\t\tif err := json.Unmarshal([]byte(v.(string)), &n.ipamV6Config); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif v, ok := netMap[\"ipamV6Info\"]; ok {\n\t\tif err := json.Unmarshal([]byte(v.(string)), &n.ipamV6Info); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif v, ok := netMap[\"internal\"]; ok {\n\t\tn.internal = v.(bool)\n\t}\n\tif v, ok := netMap[\"attachable\"]; ok {\n\t\tn.attachable = v.(bool)\n\t}\n\tif s, ok := netMap[\"scope\"]; ok {\n\t\tn.scope = s.(string)\n\t}\n\tif v, ok := netMap[\"inDelete\"]; ok {\n\t\tn.inDelete = v.(bool)\n\t}\n\tif v, ok := netMap[\"ingress\"]; ok {\n\t\tn.ingress = v.(bool)\n\t}\n\tif v, ok := netMap[\"configOnly\"]; ok {\n\t\tn.configOnly = v.(bool)\n\t}\n\tif v, ok := netMap[\"configFrom\"]; ok {\n\t\tn.configFrom = v.(string)\n\t}\n\tif v, ok := netMap[\"loadBalancerIP\"]; ok {\n\t\tn.loadBalancerIP = net.ParseIP(v.(string))\n\t}\n\tn.loadBalancerMode = loadBalancerModeDefault\n\tif v, ok := netMap[\"loadBalancerMode\"]; ok {\n\t\tn.loadBalancerMode = v.(string)\n\t}\n\t// Reconcile old networks with the recently added `--ipv6` flag\n\tif !n.enableIPv6 {\n\t\tn.enableIPv6 = len(n.ipamV6Info) > 0\n\t}\n\treturn nil\n}\n\n// NetworkOption is an option setter function type used to pass various options to\n// NewNetwork method. The various setter functions of type NetworkOption are\n// provided by libnetwork, they look like NetworkOptionXXXX(...)\ntype NetworkOption func(n *network)\n\n// NetworkOptionGeneric function returns an option setter for a Generic option defined\n// in a Dictionary of Key-Value pair\nfunc NetworkOptionGeneric(generic map[string]interface{}) NetworkOption {\n\treturn func(n *network) {\n\t\tif n.generic == nil {\n\t\t\tn.generic = make(map[string]interface{})\n\t\t}\n\t\tif val, ok := generic[netlabel.EnableIPv6]; ok {\n\t\t\tn.enableIPv6 = val.(bool)\n\t\t}\n\t\tif val, ok := generic[netlabel.Internal]; ok {\n\t\t\tn.internal = val.(bool)\n\t\t}\n\t\tfor k, v := range generic {\n\t\t\tn.generic[k] = v\n\t\t}\n\t}\n}\n\n// NetworkOptionIngress returns an option setter to indicate if a network is\n// an ingress network.\nfunc NetworkOptionIngress(ingress bool) NetworkOption {\n\treturn func(n *network) {\n\t\tn.ingress = ingress\n\t}\n}\n\n// NetworkOptionPersist returns an option setter to set persistence policy for a network\nfunc NetworkOptionPersist(persist bool) NetworkOption {\n\treturn func(n *network) {\n\t\tn.persist = persist\n\t}\n}\n\n// NetworkOptionEnableIPv6 returns an option setter to explicitly configure IPv6\nfunc NetworkOptionEnableIPv6(enableIPv6 bool) NetworkOption {\n\treturn func(n *network) {\n\t\tif n.generic == nil {\n\t\t\tn.generic = make(map[string]interface{})\n\t\t}\n\t\tn.enableIPv6 = enableIPv6\n\t\tn.generic[netlabel.EnableIPv6] = enableIPv6\n\t}\n}\n\n// NetworkOptionInternalNetwork returns an option setter to config the network\n// to be internal which disables default gateway service\nfunc NetworkOptionInternalNetwork() NetworkOption {\n\treturn func(n *network) {\n\t\tif n.generic == nil {\n\t\t\tn.generic = make(map[string]interface{})\n\t\t}\n\t\tn.internal = true\n\t\tn.generic[netlabel.Internal] = true\n\t}\n}\n\n// NetworkOptionAttachable returns an option setter to set attachable for a network\nfunc NetworkOptionAttachable(attachable bool) NetworkOption {\n\treturn func(n *network) {\n\t\tn.attachable = attachable\n\t}\n}\n\n// NetworkOptionScope returns an option setter to overwrite the network's scope.\n// By default the network's scope is set to the network driver's datascope.\nfunc NetworkOptionScope(scope string) NetworkOption {\n\treturn func(n *network) {\n\t\tn.scope = scope\n\t}\n}\n\n// NetworkOptionIpam function returns an option setter for the ipam configuration for this network\nfunc NetworkOptionIpam(ipamDriver string, addrSpace string, ipV4 []*IpamConf, ipV6 []*IpamConf, opts map[string]string) NetworkOption {\n\treturn func(n *network) {\n\t\tif ipamDriver != \"\" {\n\t\t\tn.ipamType = ipamDriver\n\t\t\tif ipamDriver == ipamapi.DefaultIPAM {\n\t\t\t\tn.ipamType = defaultIpamForNetworkType(n.Type())\n\t\t\t}\n\t\t}\n\t\tn.ipamOptions = opts\n\t\tn.addrSpace = addrSpace\n\t\tn.ipamV4Config = ipV4\n\t\tn.ipamV6Config = ipV6\n\t}\n}\n\n// NetworkOptionLBEndpoint function returns an option setter for the configuration of the load balancer endpoint for this network\nfunc NetworkOptionLBEndpoint(ip net.IP) NetworkOption {\n\treturn func(n *network) {\n\t\tn.loadBalancerIP = ip\n\t}\n}\n\n// NetworkOptionDriverOpts function returns an option setter for any driver parameter described by a map\nfunc NetworkOptionDriverOpts(opts map[string]string) NetworkOption {\n\treturn func(n *network) {\n\t\tif n.generic == nil {\n\t\t\tn.generic = make(map[string]interface{})\n\t\t}\n\t\tif opts == nil {\n\t\t\topts = make(map[string]string)\n\t\t}\n\t\t// Store the options\n\t\tn.generic[netlabel.GenericData] = opts\n\t}\n}\n\n// NetworkOptionLabels function returns an option setter for labels specific to a network\nfunc NetworkOptionLabels(labels map[string]string) NetworkOption {\n\treturn func(n *network) {\n\t\tn.labels = labels\n\t}\n}\n\n// NetworkOptionDynamic function returns an option setter for dynamic option for a network\nfunc NetworkOptionDynamic() NetworkOption {\n\treturn func(n *network) {\n\t\tn.dynamic = true\n\t}\n}\n\n// NetworkOptionDeferIPv6Alloc instructs the network to defer the IPV6 address allocation until after the endpoint has been created\n// It is being provided to support the specific docker daemon flags where user can deterministically assign an IPv6 address\n// to a container as combination of fixed-cidr-v6 + mac-address\n// TODO: Remove this option setter once we support endpoint ipam options\nfunc NetworkOptionDeferIPv6Alloc(enable bool) NetworkOption {\n\treturn func(n *network) {\n\t\tn.postIPv6 = enable\n\t}\n}\n\n// NetworkOptionConfigOnly tells controller this network is\n// a configuration only network. It serves as a configuration\n// for other networks.\nfunc NetworkOptionConfigOnly() NetworkOption {\n\treturn func(n *network) {\n\t\tn.configOnly = true\n\t}\n}\n\n// NetworkOptionConfigFrom tells controller to pick the\n// network configuration from a configuration only network\nfunc NetworkOptionConfigFrom(name string) NetworkOption {\n\treturn func(n *network) {\n\t\tn.configFrom = name\n\t}\n}\n\nfunc (n *network) processOptions(options ...NetworkOption) {\n\tfor _, opt := range options {\n\t\tif opt != nil {\n\t\t\topt(n)\n\t\t}\n\t}\n}\n\ntype networkDeleteParams struct {\n\trmLBEndpoint bool\n}\n\n// NetworkDeleteOption is a type for optional parameters to pass to the\n// network.Delete() function.\ntype NetworkDeleteOption func(p *networkDeleteParams)\n\n// NetworkDeleteOptionRemoveLB informs a network.Delete() operation that should\n// remove the load balancer endpoint for this network.  Note that the Delete()\n// method will automatically remove a load balancing endpoint for most networks\n// when the network is otherwise empty.  However, this does not occur for some\n// networks.  In particular, networks marked as ingress (which are supposed to\n// be more permanent than other overlay networks) won't automatically remove\n// the LB endpoint on Delete().  This method allows for explicit removal of\n// such networks provided there are no other endpoints present in the network.\n// If the network still has non-LB endpoints present, Delete() will not\n// remove the LB endpoint and will return an error.\nfunc NetworkDeleteOptionRemoveLB(p *networkDeleteParams) {\n\tp.rmLBEndpoint = true\n}\n\nfunc (n *network) resolveDriver(name string, load bool) (driverapi.Driver, *driverapi.Capability, error) {\n\tc := n.getController()\n\n\t// Check if a driver for the specified network type is available\n\td, cap := c.drvRegistry.Driver(name)\n\tif d == nil {\n\t\tif load {\n\t\t\terr := c.loadDriver(name)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\n\t\t\td, cap = c.drvRegistry.Driver(name)\n\t\t\tif d == nil {\n\t\t\t\treturn nil, nil, fmt.Errorf(\"could not resolve driver %s in registry\", name)\n\t\t\t}\n\t\t} else {\n\t\t\t// don't fail if driver loading is not required\n\t\t\treturn nil, nil, nil\n\t\t}\n\t}\n\n\treturn d, cap, nil\n}\n\nfunc (n *network) driverScope() string {\n\t_, cap, err := n.resolveDriver(n.networkType, true)\n\tif err != nil {\n\t\t// If driver could not be resolved simply return an empty string\n\t\treturn \"\"\n\t}\n\n\treturn cap.DataScope\n}\n\nfunc (n *network) driverIsMultihost() bool {\n\t_, cap, err := n.resolveDriver(n.networkType, true)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn cap.ConnectivityScope == datastore.GlobalScope\n}\n\nfunc (n *network) driver(load bool) (driverapi.Driver, error) {\n\td, cap, err := n.resolveDriver(n.networkType, load)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tn.Lock()\n\t// If load is not required, driver, cap and err may all be nil\n\tif n.scope == \"\" && cap != nil {\n\t\tn.scope = cap.DataScope\n\t}\n\tif n.dynamic {\n\t\t// If the network is dynamic, then it is swarm\n\t\t// scoped regardless of the backing driver.\n\t\tn.scope = datastore.SwarmScope\n\t}\n\tn.Unlock()\n\treturn d, nil\n}\n\nfunc (n *network) Delete(options ...NetworkDeleteOption) error {\n\tvar params networkDeleteParams\n\tfor _, opt := range options {\n\t\topt(&params)\n\t}\n\treturn n.delete(false, params.rmLBEndpoint)\n}\n\n// This function gets called in 3 ways:\n//   - Delete() -- (false, false)\n//     remove if endpoint count == 0 or endpoint count == 1 and\n//     there is a load balancer IP\n//   - Delete(libnetwork.NetworkDeleteOptionRemoveLB) -- (false, true)\n//     remove load balancer and network if endpoint count == 1\n//   - controller.networkCleanup() -- (true, true)\n//     remove the network no matter what\nfunc (n *network) delete(force bool, rmLBEndpoint bool) error {\n\tn.Lock()\n\tc := n.ctrlr\n\tname := n.name\n\tid := n.id\n\tn.Unlock()\n\n\tc.networkLocker.Lock(id)\n\tdefer c.networkLocker.Unlock(id)\n\n\tn, err := c.getNetworkFromStore(id)\n\tif err != nil {\n\t\treturn &UnknownNetworkError{name: name, id: id}\n\t}\n\n\t// Only remove ingress on force removal or explicit LB endpoint removal\n\tif n.ingress && !force && !rmLBEndpoint {\n\t\treturn &ActiveEndpointsError{name: n.name, id: n.id}\n\t}\n\n\t// Check that the network is empty\n\tvar emptyCount uint64\n\tif n.hasLoadBalancerEndpoint() {\n\t\temptyCount = 1\n\t}\n\tif !force && n.getEpCnt().EndpointCnt() > emptyCount {\n\t\tif n.configOnly {\n\t\t\treturn types.ForbiddenErrorf(\"configuration network %q is in use\", n.Name())\n\t\t}\n\t\treturn &ActiveEndpointsError{name: n.name, id: n.id}\n\t}\n\n\tif n.hasLoadBalancerEndpoint() {\n\t\t// If we got to this point, then the following must hold:\n\t\t//  * force is true OR endpoint count == 1\n\t\tif err := n.deleteLoadBalancerSandbox(); err != nil {\n\t\t\tif !force {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// continue deletion when force is true even on error\n\t\t\tlogrus.Warnf(\"Error deleting load balancer sandbox: %v\", err)\n\t\t}\n\t\t//Reload the network from the store to update the epcnt.\n\t\tn, err = c.getNetworkFromStore(id)\n\t\tif err != nil {\n\t\t\treturn &UnknownNetworkError{name: name, id: id}\n\t\t}\n\t}\n\n\t// Up to this point, errors that we returned were recoverable.\n\t// From here on, any errors leave us in an inconsistent state.\n\t// This is unfortunate, but there isn't a safe way to\n\t// reconstitute a load-balancer endpoint after removing it.\n\n\t// Mark the network for deletion\n\tn.inDelete = true\n\tif err = c.updateToStore(n); err != nil {\n\t\treturn fmt.Errorf(\"error marking network %s (%s) for deletion: %v\", n.Name(), n.ID(), err)\n\t}\n\n\tif n.ConfigFrom() != \"\" {\n\t\tif t, err := c.getConfigNetwork(n.ConfigFrom()); err == nil {\n\t\t\tif err := t.getEpCnt().DecEndpointCnt(); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to update reference count for configuration network %q on removal of network %q: %v\",\n\t\t\t\t\tt.Name(), n.Name(), err)\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.Warnf(\"Could not find configuration network %q during removal of network %q\", n.configFrom, n.Name())\n\t\t}\n\t}\n\n\tif n.configOnly {\n\t\tgoto removeFromStore\n\t}\n\n\tn.ipamRelease()\n\n\t// We are about to delete the network. Leave the gossip\n\t// cluster for the network to stop all incoming network\n\t// specific gossip updates before cleaning up all the service\n\t// bindings for the network. But cleanup service binding\n\t// before deleting the network from the store since service\n\t// bindings cleanup requires the network in the store.\n\tn.cancelDriverWatches()\n\tif err = n.leaveCluster(); err != nil {\n\t\tlogrus.Errorf(\"Failed leaving network %s from the agent cluster: %v\", n.Name(), err)\n\t}\n\n\t// Cleanup the service discovery for this network\n\tc.cleanupServiceDiscovery(n.ID())\n\n\t// Cleanup the load balancer. On Windows this call is required\n\t// to remove remote loadbalancers in VFP, and must be performed before\n\t// dataplane network deletion.\n\tif runtime.GOOS == \"windows\" {\n\t\tc.cleanupServiceBindings(n.ID())\n\t}\n\n\t// Delete the network from the dataplane\n\tif err = n.deleteNetwork(); err != nil {\n\t\tif !force {\n\t\t\treturn err\n\t\t}\n\t\tlogrus.Debugf(\"driver failed to delete stale network %s (%s): %v\", n.Name(), n.ID(), err)\n\t}\n\nremoveFromStore:\n\t// deleteFromStore performs an atomic delete operation and the\n\t// network.epCnt will help prevent any possible\n\t// race between endpoint join and network delete\n\tif err = c.deleteFromStore(n.getEpCnt()); err != nil {\n\t\tif !force {\n\t\t\treturn fmt.Errorf(\"error deleting network endpoint count from store: %v\", err)\n\t\t}\n\t\tlogrus.Debugf(\"Error deleting endpoint count from store for stale network %s (%s) for deletion: %v\", n.Name(), n.ID(), err)\n\t}\n\n\tif err = c.deleteFromStore(n); err != nil {\n\t\treturn fmt.Errorf(\"error deleting network from store: %v\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (n *network) deleteNetwork() error {\n\td, err := n.driver(true)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed deleting network: %v\", err)\n\t}\n\n\tif err := d.DeleteNetwork(n.ID()); err != nil {\n\t\t// Forbidden Errors should be honored\n\t\tif _, ok := err.(types.ForbiddenError); ok {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, ok := err.(types.MaskableError); !ok {\n\t\t\tlogrus.Warnf(\"driver error deleting network %s : %v\", n.name, err)\n\t\t}\n\t}\n\n\tfor _, resolver := range n.resolver {\n\t\tresolver.Stop()\n\t}\n\treturn nil\n}\n\nfunc (n *network) addEndpoint(ep *endpoint) error {\n\td, err := n.driver(true)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to add endpoint: %v\", err)\n\t}\n\n\terr = d.CreateEndpoint(n.id, ep.id, ep.Interface(), ep.generic)\n\tif err != nil {\n\t\treturn types.InternalErrorf(\"failed to create endpoint %s on network %s: %v\",\n\t\t\tep.Name(), n.Name(), err)\n\t}\n\n\treturn nil\n}\n\nfunc (n *network) CreateEndpoint(name string, options ...EndpointOption) (Endpoint, error) {\n\tvar err error\n\tif !config.IsValidName(name) {\n\t\treturn nil, ErrInvalidName(name)\n\t}\n\n\tif n.ConfigOnly() {\n\t\treturn nil, types.ForbiddenErrorf(\"cannot create endpoint on configuration-only network\")\n\t}\n\n\tif _, err = n.EndpointByName(name); err == nil {\n\t\treturn nil, types.ForbiddenErrorf(\"endpoint with name %s already exists in network %s\", name, n.Name())\n\t}\n\n\tn.ctrlr.networkLocker.Lock(n.id)\n\tdefer n.ctrlr.networkLocker.Unlock(n.id)\n\n\treturn n.createEndpoint(name, options...)\n\n}\n\nfunc (n *network) createEndpoint(name string, options ...EndpointOption) (Endpoint, error) {\n\tvar err error\n\n\tep := &endpoint{name: name, generic: make(map[string]interface{}), iface: &endpointInterface{}}\n\tep.id = stringid.GenerateRandomID()\n\n\t// Initialize ep.network with a possibly stale copy of n. We need this to get network from\n\t// store. But once we get it from store we will have the most uptodate copy possibly.\n\tep.network = n\n\tep.locator = n.getController().clusterHostID()\n\tep.network, err = ep.getNetworkFromStore()\n\tif err != nil {\n\t\tlogrus.Errorf(\"failed to get network during CreateEndpoint: %v\", err)\n\t\treturn nil, err\n\t}\n\tn = ep.network\n\n\tep.processOptions(options...)\n\n\tfor _, llIPNet := range ep.Iface().LinkLocalAddresses() {\n\t\tif !llIPNet.IP.IsLinkLocalUnicast() {\n\t\t\treturn nil, types.BadRequestErrorf(\"invalid link local IP address: %v\", llIPNet.IP)\n\t\t}\n\t}\n\n\tif opt, ok := ep.generic[netlabel.MacAddress]; ok {\n\t\tif mac, ok := opt.(net.HardwareAddr); ok {\n\t\t\tep.iface.mac = mac\n\t\t}\n\t}\n\n\tipam, cap, err := n.getController().getIPAMDriver(n.ipamType)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif cap.RequiresMACAddress {\n\t\tif ep.iface.mac == nil {\n\t\t\tep.iface.mac = netutils.GenerateRandomMAC()\n\t\t}\n\t\tif ep.ipamOptions == nil {\n\t\t\tep.ipamOptions = make(map[string]string)\n\t\t}\n\t\tep.ipamOptions[netlabel.MacAddress] = ep.iface.mac.String()\n\t}\n\n\tif err = ep.assignAddress(ipam, true, n.enableIPv6 && !n.postIPv6); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tep.releaseAddress()\n\t\t}\n\t}()\n\n\tif err = n.addEndpoint(ep); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := ep.deleteEndpoint(false); e != nil {\n\t\t\t\tlogrus.Warnf(\"cleaning up endpoint failed %s : %v\", name, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// We should perform updateToStore call right after addEndpoint\n\t// in order to have iface properly configured\n\tif err = n.getController().updateToStore(ep); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif e := n.getController().deleteFromStore(ep); e != nil {\n\t\t\t\tlogrus.Warnf(\"error rolling back endpoint %s from store: %v\", name, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err = ep.assignAddress(ipam, false, n.enableIPv6 && n.postIPv6); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Watch for service records\n\tn.getController().watchSvcRecord(ep)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tn.getController().unWatchSvcRecord(ep)\n\t\t}\n\t}()\n\n\t// Increment endpoint count to indicate completion of endpoint addition\n\tif err = n.getEpCnt().IncEndpointCnt(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn ep, nil\n}\n\nfunc (n *network) Endpoints() []Endpoint {\n\tvar list []Endpoint\n\n\tendpoints, err := n.getEndpointsFromStore()\n\tif err != nil {\n\t\tlogrus.Error(err)\n\t}\n\n\tfor _, ep := range endpoints {\n\t\tlist = append(list, ep)\n\t}\n\n\treturn list\n}\n\nfunc (n *network) WalkEndpoints(walker EndpointWalker) {\n\tfor _, e := range n.Endpoints() {\n\t\tif walker(e) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (n *network) EndpointByName(name string) (Endpoint, error) {\n\tif name == \"\" {\n\t\treturn nil, ErrInvalidName(name)\n\t}\n\tvar e Endpoint\n\n\ts := func(current Endpoint) bool {\n\t\tif current.Name() == name {\n\t\t\te = current\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\tn.WalkEndpoints(s)\n\n\tif e == nil {\n\t\treturn nil, ErrNoSuchEndpoint(name)\n\t}\n\n\treturn e, nil\n}\n\nfunc (n *network) EndpointByID(id string) (Endpoint, error) {\n\tif id == \"\" {\n\t\treturn nil, ErrInvalidID(id)\n\t}\n\n\tep, err := n.getEndpointFromStore(id)\n\tif err != nil {\n\t\treturn nil, ErrNoSuchEndpoint(id)\n\t}\n\n\treturn ep, nil\n}\n\nfunc (n *network) updateSvcRecord(ep *endpoint, localEps []*endpoint, isAdd bool) {\n\tvar ipv6 net.IP\n\tepName := ep.Name()\n\tif iface := ep.Iface(); iface != nil && iface.Address() != nil {\n\t\tmyAliases := ep.MyAliases()\n\t\tif iface.AddressIPv6() != nil {\n\t\t\tipv6 = iface.AddressIPv6().IP\n\t\t}\n\n\t\tserviceID := ep.svcID\n\t\tif serviceID == \"\" {\n\t\t\tserviceID = ep.ID()\n\t\t}\n\t\tif isAdd {\n\t\t\t// If anonymous endpoint has an alias use the first alias\n\t\t\t// for ip->name mapping. Not having the reverse mapping\n\t\t\t// breaks some apps\n\t\t\tif ep.isAnonymous() {\n\t\t\t\tif len(myAliases) > 0 {\n\t\t\t\t\tn.addSvcRecords(ep.ID(), myAliases[0], serviceID, iface.Address().IP, ipv6, true, \"updateSvcRecord\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tn.addSvcRecords(ep.ID(), epName, serviceID, iface.Address().IP, ipv6, true, \"updateSvcRecord\")\n\t\t\t}\n\t\t\tfor _, alias := range myAliases {\n\t\t\t\tn.addSvcRecords(ep.ID(), alias, serviceID, iface.Address().IP, ipv6, false, \"updateSvcRecord\")\n\t\t\t}\n\t\t} else {\n\t\t\tif ep.isAnonymous() {\n\t\t\t\tif len(myAliases) > 0 {\n\t\t\t\t\tn.deleteSvcRecords(ep.ID(), myAliases[0], serviceID, iface.Address().IP, ipv6, true, \"updateSvcRecord\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tn.deleteSvcRecords(ep.ID(), epName, serviceID, iface.Address().IP, ipv6, true, \"updateSvcRecord\")\n\t\t\t}\n\t\t\tfor _, alias := range myAliases {\n\t\t\t\tn.deleteSvcRecords(ep.ID(), alias, serviceID, iface.Address().IP, ipv6, false, \"updateSvcRecord\")\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc addIPToName(ipMap setmatrix.SetMatrix, name, serviceID string, ip net.IP) {\n\treverseIP := netutils.ReverseIP(ip.String())\n\tipMap.Insert(reverseIP, ipInfo{\n\t\tname:      name,\n\t\tserviceID: serviceID,\n\t})\n}\n\nfunc delIPToName(ipMap setmatrix.SetMatrix, name, serviceID string, ip net.IP) {\n\treverseIP := netutils.ReverseIP(ip.String())\n\tipMap.Remove(reverseIP, ipInfo{\n\t\tname:      name,\n\t\tserviceID: serviceID,\n\t})\n}\n\nfunc addNameToIP(svcMap setmatrix.SetMatrix, name, serviceID string, epIP net.IP) {\n\t// Since DNS name resolution is case-insensitive, Use the lower-case form\n\t// of the name as the key into svcMap\n\tlowerCaseName := strings.ToLower(name)\n\tsvcMap.Insert(lowerCaseName, svcMapEntry{\n\t\tip:        epIP.String(),\n\t\tserviceID: serviceID,\n\t})\n}\n\nfunc delNameToIP(svcMap setmatrix.SetMatrix, name, serviceID string, epIP net.IP) {\n\tlowerCaseName := strings.ToLower(name)\n\tsvcMap.Remove(lowerCaseName, svcMapEntry{\n\t\tip:        epIP.String(),\n\t\tserviceID: serviceID,\n\t})\n}\n\nfunc (n *network) addSvcRecords(eID, name, serviceID string, epIP, epIPv6 net.IP, ipMapUpdate bool, method string) {\n\t// Do not add service names for ingress network as this is a\n\t// routing only network\n\tif n.ingress {\n\t\treturn\n\t}\n\tnetworkID := n.ID()\n\tlogrus.Debugf(\"%s (%.7s).addSvcRecords(%s, %s, %s, %t) %s sid:%s\", eID, networkID, name, epIP, epIPv6, ipMapUpdate, method, serviceID)\n\n\tc := n.getController()\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tsr, ok := c.svcRecords[networkID]\n\tif !ok {\n\t\tsr = svcInfo{\n\t\t\tsvcMap:     setmatrix.NewSetMatrix(),\n\t\t\tsvcIPv6Map: setmatrix.NewSetMatrix(),\n\t\t\tipMap:      setmatrix.NewSetMatrix(),\n\t\t}\n\t\tc.svcRecords[networkID] = sr\n\t}\n\n\tif ipMapUpdate {\n\t\taddIPToName(sr.ipMap, name, serviceID, epIP)\n\t\tif epIPv6 != nil {\n\t\t\taddIPToName(sr.ipMap, name, serviceID, epIPv6)\n\t\t}\n\t}\n\n\taddNameToIP(sr.svcMap, name, serviceID, epIP)\n\tif epIPv6 != nil {\n\t\taddNameToIP(sr.svcIPv6Map, name, serviceID, epIPv6)\n\t}\n}\n\nfunc (n *network) deleteSvcRecords(eID, name, serviceID string, epIP net.IP, epIPv6 net.IP, ipMapUpdate bool, method string) {\n\t// Do not delete service names from ingress network as this is a\n\t// routing only network\n\tif n.ingress {\n\t\treturn\n\t}\n\tnetworkID := n.ID()\n\tlogrus.Debugf(\"%s (%.7s).deleteSvcRecords(%s, %s, %s, %t) %s sid:%s \", eID, networkID, name, epIP, epIPv6, ipMapUpdate, method, serviceID)\n\n\tc := n.getController()\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tsr, ok := c.svcRecords[networkID]\n\tif !ok {\n\t\treturn\n\t}\n\n\tif ipMapUpdate {\n\t\tdelIPToName(sr.ipMap, name, serviceID, epIP)\n\n\t\tif epIPv6 != nil {\n\t\t\tdelIPToName(sr.ipMap, name, serviceID, epIPv6)\n\t\t}\n\t}\n\n\tdelNameToIP(sr.svcMap, name, serviceID, epIP)\n\n\tif epIPv6 != nil {\n\t\tdelNameToIP(sr.svcIPv6Map, name, serviceID, epIPv6)\n\t}\n}\n\nfunc (n *network) getSvcRecords(ep *endpoint) []etchosts.Record {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif ep == nil {\n\t\treturn nil\n\t}\n\n\tvar recs []etchosts.Record\n\n\tepName := ep.Name()\n\n\tn.ctrlr.Lock()\n\tdefer n.ctrlr.Unlock()\n\tsr, ok := n.ctrlr.svcRecords[n.id]\n\tif !ok || sr.svcMap == nil {\n\t\treturn nil\n\t}\n\n\tsvcMapKeys := sr.svcMap.Keys()\n\t// Loop on service names on this network\n\tfor _, k := range svcMapKeys {\n\t\tif strings.Split(k, \".\")[0] == epName {\n\t\t\tcontinue\n\t\t}\n\t\t// Get all the IPs associated to this service\n\t\tmapEntryList, ok := sr.svcMap.Get(k)\n\t\tif !ok {\n\t\t\t// The key got deleted\n\t\t\tcontinue\n\t\t}\n\t\tif len(mapEntryList) == 0 {\n\t\t\tlogrus.Warnf(\"Found empty list of IP addresses for service %s on network %s (%s)\", k, n.name, n.id)\n\t\t\tcontinue\n\t\t}\n\n\t\trecs = append(recs, etchosts.Record{\n\t\t\tHosts: k,\n\t\t\tIP:    mapEntryList[0].(svcMapEntry).ip,\n\t\t})\n\t}\n\n\treturn recs\n}\n\nfunc (n *network) getController() *controller {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.ctrlr\n}\n\nfunc (n *network) ipamAllocate() error {\n\tif n.hasSpecialDriver() {\n\t\treturn nil\n\t}\n\n\tipam, _, err := n.getController().getIPAMDriver(n.ipamType)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif n.addrSpace == \"\" {\n\t\tif n.addrSpace, err = n.deriveAddressSpace(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\terr = n.ipamAllocateVersion(4, ipam)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tn.ipamReleaseVersion(4, ipam)\n\t\t}\n\t}()\n\n\tif !n.enableIPv6 {\n\t\treturn nil\n\t}\n\n\terr = n.ipamAllocateVersion(6, ipam)\n\treturn err\n}\n\nfunc (n *network) requestPoolHelper(ipam ipamapi.Ipam, addressSpace, preferredPool, subPool string, options map[string]string, v6 bool) (string, *net.IPNet, map[string]string, error) {\n\tfor {\n\t\tpoolID, pool, meta, err := ipam.RequestPool(addressSpace, preferredPool, subPool, options, v6)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, nil, err\n\t\t}\n\n\t\t// If the network belongs to global scope or the pool was\n\t\t// explicitly chosen or it is invalid, do not perform the overlap check.\n\t\tif n.Scope() == datastore.GlobalScope || preferredPool != \"\" || !types.IsIPNetValid(pool) {\n\t\t\treturn poolID, pool, meta, nil\n\t\t}\n\n\t\t// Check for overlap and if none found, we have found the right pool.\n\t\tif _, err := netutils.FindAvailableNetwork([]*net.IPNet{pool}); err == nil {\n\t\t\treturn poolID, pool, meta, nil\n\t\t}\n\n\t\t// Pool obtained in this iteration is\n\t\t// overlapping. Hold onto the pool and don't release\n\t\t// it yet, because we don't want ipam to give us back\n\t\t// the same pool over again. But make sure we still do\n\t\t// a deferred release when we have either obtained a\n\t\t// non-overlapping pool or ran out of pre-defined\n\t\t// pools.\n\t\tdefer func() {\n\t\t\tif err := ipam.ReleasePool(poolID); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to release overlapping pool %s while returning from pool request helper for network %s\", pool, n.Name())\n\t\t\t}\n\t\t}()\n\n\t\t// If this is a preferred pool request and the network\n\t\t// is local scope and there is an overlap, we fail the\n\t\t// network creation right here. The pool will be\n\t\t// released in the defer.\n\t\tif preferredPool != \"\" {\n\t\t\treturn \"\", nil, nil, fmt.Errorf(\"requested subnet %s overlaps in the host\", preferredPool)\n\t\t}\n\t}\n}\n\nfunc (n *network) ipamAllocateVersion(ipVer int, ipam ipamapi.Ipam) error {\n\tvar (\n\t\tcfgList  *[]*IpamConf\n\t\tinfoList *[]*IpamInfo\n\t\terr      error\n\t)\n\n\tswitch ipVer {\n\tcase 4:\n\t\tcfgList = &n.ipamV4Config\n\t\tinfoList = &n.ipamV4Info\n\tcase 6:\n\t\tcfgList = &n.ipamV6Config\n\t\tinfoList = &n.ipamV6Info\n\tdefault:\n\t\treturn types.InternalErrorf(\"incorrect ip version passed to ipam allocate: %d\", ipVer)\n\t}\n\n\tif len(*cfgList) == 0 {\n\t\t*cfgList = []*IpamConf{{}}\n\t}\n\n\t*infoList = make([]*IpamInfo, len(*cfgList))\n\n\tlogrus.Debugf(\"Allocating IPv%d pools for network %s (%s)\", ipVer, n.Name(), n.ID())\n\n\tfor i, cfg := range *cfgList {\n\t\tif err = cfg.Validate(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\td := &IpamInfo{}\n\t\t(*infoList)[i] = d\n\n\t\td.AddressSpace = n.addrSpace\n\t\td.PoolID, d.Pool, d.Meta, err = n.requestPoolHelper(ipam, n.addrSpace, cfg.PreferredPool, cfg.SubPool, n.ipamOptions, ipVer == 6)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tif err := ipam.ReleasePool(d.PoolID); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"Failed to release address pool %s after failure to create network %s (%s)\", d.PoolID, n.Name(), n.ID())\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\tif gws, ok := d.Meta[netlabel.Gateway]; ok {\n\t\t\tif d.Gateway, err = types.ParseCIDR(gws); err != nil {\n\t\t\t\treturn types.BadRequestErrorf(\"failed to parse gateway address (%v) returned by ipam driver: %v\", gws, err)\n\t\t\t}\n\t\t}\n\n\t\t// If user requested a specific gateway, libnetwork will allocate it\n\t\t// irrespective of whether ipam driver returned a gateway already.\n\t\t// If none of the above is true, libnetwork will allocate one.\n\t\tif cfg.Gateway != \"\" || d.Gateway == nil {\n\t\t\tvar gatewayOpts = map[string]string{\n\t\t\t\tipamapi.RequestAddressType: netlabel.Gateway,\n\t\t\t}\n\t\t\tif d.Gateway, _, err = ipam.RequestAddress(d.PoolID, net.ParseIP(cfg.Gateway), gatewayOpts); err != nil {\n\t\t\t\treturn types.InternalErrorf(\"failed to allocate gateway (%v): %v\", cfg.Gateway, err)\n\t\t\t}\n\t\t}\n\n\t\t// Auxiliary addresses must be part of the master address pool\n\t\t// If they fall into the container addressable pool, libnetwork will reserve them\n\t\tif cfg.AuxAddresses != nil {\n\t\t\tvar ip net.IP\n\t\t\td.IPAMData.AuxAddresses = make(map[string]*net.IPNet, len(cfg.AuxAddresses))\n\t\t\tfor k, v := range cfg.AuxAddresses {\n\t\t\t\tif ip = net.ParseIP(v); ip == nil {\n\t\t\t\t\treturn types.BadRequestErrorf(\"non parsable secondary ip address (%s:%s) passed for network %s\", k, v, n.Name())\n\t\t\t\t}\n\t\t\t\tif !d.Pool.Contains(ip) {\n\t\t\t\t\treturn types.ForbiddenErrorf(\"auxiliary address: (%s:%s) must belong to the master pool: %s\", k, v, d.Pool)\n\t\t\t\t}\n\t\t\t\t// Attempt reservation in the container addressable pool, silent the error if address does not belong to that pool\n\t\t\t\tif d.IPAMData.AuxAddresses[k], _, err = ipam.RequestAddress(d.PoolID, ip, nil); err != nil && err != ipamapi.ErrIPOutOfRange {\n\t\t\t\t\treturn types.InternalErrorf(\"failed to allocate secondary ip address (%s:%s): %v\", k, v, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (n *network) ipamRelease() {\n\tif n.hasSpecialDriver() {\n\t\treturn\n\t}\n\tipam, _, err := n.getController().getIPAMDriver(n.ipamType)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to retrieve ipam driver to release address pool(s) on delete of network %s (%s): %v\", n.Name(), n.ID(), err)\n\t\treturn\n\t}\n\tn.ipamReleaseVersion(4, ipam)\n\tn.ipamReleaseVersion(6, ipam)\n}\n\nfunc (n *network) ipamReleaseVersion(ipVer int, ipam ipamapi.Ipam) {\n\tvar infoList *[]*IpamInfo\n\n\tswitch ipVer {\n\tcase 4:\n\t\tinfoList = &n.ipamV4Info\n\tcase 6:\n\t\tinfoList = &n.ipamV6Info\n\tdefault:\n\t\tlogrus.Warnf(\"incorrect ip version passed to ipam release: %d\", ipVer)\n\t\treturn\n\t}\n\n\tif len(*infoList) == 0 {\n\t\treturn\n\t}\n\n\tlogrus.Debugf(\"releasing IPv%d pools from network %s (%s)\", ipVer, n.Name(), n.ID())\n\n\tfor _, d := range *infoList {\n\t\tif d.Gateway != nil {\n\t\t\tif err := ipam.ReleaseAddress(d.PoolID, d.Gateway.IP); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed to release gateway ip address %s on delete of network %s (%s): %v\", d.Gateway.IP, n.Name(), n.ID(), err)\n\t\t\t}\n\t\t}\n\t\tif d.IPAMData.AuxAddresses != nil {\n\t\t\tfor k, nw := range d.IPAMData.AuxAddresses {\n\t\t\t\tif d.Pool.Contains(nw.IP) {\n\t\t\t\t\tif err := ipam.ReleaseAddress(d.PoolID, nw.IP); err != nil && err != ipamapi.ErrIPOutOfRange {\n\t\t\t\t\t\tlogrus.Warnf(\"Failed to release secondary ip address %s (%v) on delete of network %s (%s): %v\", k, nw.IP, n.Name(), n.ID(), err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif err := ipam.ReleasePool(d.PoolID); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to release address pool %s on delete of network %s (%s): %v\", d.PoolID, n.Name(), n.ID(), err)\n\t\t}\n\t}\n\n\t*infoList = nil\n}\n\nfunc (n *network) getIPInfo(ipVer int) []*IpamInfo {\n\tvar info []*IpamInfo\n\tswitch ipVer {\n\tcase 4:\n\t\tinfo = n.ipamV4Info\n\tcase 6:\n\t\tinfo = n.ipamV6Info\n\tdefault:\n\t\treturn nil\n\t}\n\tl := make([]*IpamInfo, 0, len(info))\n\tn.Lock()\n\tl = append(l, info...)\n\tn.Unlock()\n\treturn l\n}\n\nfunc (n *network) getIPData(ipVer int) []driverapi.IPAMData {\n\tvar info []*IpamInfo\n\tswitch ipVer {\n\tcase 4:\n\t\tinfo = n.ipamV4Info\n\tcase 6:\n\t\tinfo = n.ipamV6Info\n\tdefault:\n\t\treturn nil\n\t}\n\tl := make([]driverapi.IPAMData, 0, len(info))\n\tn.Lock()\n\tfor _, d := range info {\n\t\tl = append(l, d.IPAMData)\n\t}\n\tn.Unlock()\n\treturn l\n}\n\nfunc (n *network) deriveAddressSpace() (string, error) {\n\tlocal, global, err := n.getController().drvRegistry.IPAMDefaultAddressSpaces(n.ipamType)\n\tif err != nil {\n\t\treturn \"\", types.NotFoundErrorf(\"failed to get default address space: %v\", err)\n\t}\n\tif n.DataScope() == datastore.GlobalScope {\n\t\treturn global, nil\n\t}\n\treturn local, nil\n}\n\nfunc (n *network) Info() NetworkInfo {\n\treturn n\n}\n\nfunc (n *network) Peers() []networkdb.PeerInfo {\n\tif !n.Dynamic() {\n\t\treturn []networkdb.PeerInfo{}\n\t}\n\n\tagent := n.getController().getAgent()\n\tif agent == nil {\n\t\treturn []networkdb.PeerInfo{}\n\t}\n\n\treturn agent.networkDB.Peers(n.ID())\n}\n\nfunc (n *network) DriverOptions() map[string]string {\n\tn.Lock()\n\tdefer n.Unlock()\n\tif n.generic != nil {\n\t\tif m, ok := n.generic[netlabel.GenericData]; ok {\n\t\t\treturn m.(map[string]string)\n\t\t}\n\t}\n\treturn map[string]string{}\n}\n\nfunc (n *network) Scope() string {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.scope\n}\n\nfunc (n *network) IpamConfig() (string, map[string]string, []*IpamConf, []*IpamConf) {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tv4L := make([]*IpamConf, len(n.ipamV4Config))\n\tv6L := make([]*IpamConf, len(n.ipamV6Config))\n\n\tfor i, c := range n.ipamV4Config {\n\t\tcc := &IpamConf{}\n\t\tc.CopyTo(cc)\n\t\tv4L[i] = cc\n\t}\n\n\tfor i, c := range n.ipamV6Config {\n\t\tcc := &IpamConf{}\n\t\tc.CopyTo(cc)\n\t\tv6L[i] = cc\n\t}\n\n\treturn n.ipamType, n.ipamOptions, v4L, v6L\n}\n\nfunc (n *network) IpamInfo() ([]*IpamInfo, []*IpamInfo) {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tv4Info := make([]*IpamInfo, len(n.ipamV4Info))\n\tv6Info := make([]*IpamInfo, len(n.ipamV6Info))\n\n\tfor i, info := range n.ipamV4Info {\n\t\tic := &IpamInfo{}\n\t\tinfo.CopyTo(ic)\n\t\tv4Info[i] = ic\n\t}\n\n\tfor i, info := range n.ipamV6Info {\n\t\tic := &IpamInfo{}\n\t\tinfo.CopyTo(ic)\n\t\tv6Info[i] = ic\n\t}\n\n\treturn v4Info, v6Info\n}\n\nfunc (n *network) Internal() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.internal\n}\n\nfunc (n *network) Attachable() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.attachable\n}\n\nfunc (n *network) Ingress() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.ingress\n}\n\nfunc (n *network) Dynamic() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.dynamic\n}\n\nfunc (n *network) IPv6Enabled() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.enableIPv6\n}\n\nfunc (n *network) ConfigFrom() string {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.configFrom\n}\n\nfunc (n *network) ConfigOnly() bool {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\treturn n.configOnly\n}\n\nfunc (n *network) Labels() map[string]string {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tvar lbls = make(map[string]string, len(n.labels))\n\tfor k, v := range n.labels {\n\t\tlbls[k] = v\n\t}\n\n\treturn lbls\n}\n\nfunc (n *network) TableEventRegister(tableName string, objType driverapi.ObjectType) error {\n\tif !driverapi.IsValidType(objType) {\n\t\treturn fmt.Errorf(\"invalid object type %v in registering table, %s\", objType, tableName)\n\t}\n\n\tt := networkDBTable{\n\t\tname:    tableName,\n\t\tobjType: objType,\n\t}\n\tn.Lock()\n\tdefer n.Unlock()\n\tn.driverTables = append(n.driverTables, t)\n\treturn nil\n}\n\nfunc (n *network) UpdateIpamConfig(ipV4Data []driverapi.IPAMData) {\n\n\tipamV4Config := make([]*IpamConf, len(ipV4Data))\n\n\tfor i, data := range ipV4Data {\n\t\tic := &IpamConf{}\n\t\tic.PreferredPool = data.Pool.String()\n\t\tic.Gateway = data.Gateway.IP.String()\n\t\tipamV4Config[i] = ic\n\t}\n\n\tn.Lock()\n\tdefer n.Unlock()\n\tn.ipamV4Config = ipamV4Config\n}\n\n// Special drivers are ones which do not need to perform any network plumbing\nfunc (n *network) hasSpecialDriver() bool {\n\treturn n.Type() == \"host\" || n.Type() == \"null\"\n}\n\nfunc (n *network) hasLoadBalancerEndpoint() bool {\n\treturn len(n.loadBalancerIP) != 0\n}\n\nfunc (n *network) ResolveName(req string, ipType int) ([]net.IP, bool) {\n\tvar ipv6Miss bool\n\n\tc := n.getController()\n\tnetworkID := n.ID()\n\tc.Lock()\n\tdefer c.Unlock()\n\tsr, ok := c.svcRecords[networkID]\n\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\treq = strings.TrimSuffix(req, \".\")\n\treq = strings.ToLower(req)\n\tipSet, ok := sr.svcMap.Get(req)\n\n\tif ipType == types.IPv6 {\n\t\t// If the name resolved to v4 address then its a valid name in\n\t\t// the docker network domain. If the network is not v6 enabled\n\t\t// set ipv6Miss to filter the DNS query from going to external\n\t\t// resolvers.\n\t\tif ok && !n.enableIPv6 {\n\t\t\tipv6Miss = true\n\t\t}\n\t\tipSet, ok = sr.svcIPv6Map.Get(req)\n\t}\n\n\tif ok && len(ipSet) > 0 {\n\t\t// this map is to avoid IP duplicates, this can happen during a transition period where 2 services are using the same IP\n\t\tnoDup := make(map[string]bool)\n\t\tvar ipLocal []net.IP\n\t\tfor _, ip := range ipSet {\n\t\t\tif _, dup := noDup[ip.(svcMapEntry).ip]; !dup {\n\t\t\t\tnoDup[ip.(svcMapEntry).ip] = true\n\t\t\t\tipLocal = append(ipLocal, net.ParseIP(ip.(svcMapEntry).ip))\n\t\t\t}\n\t\t}\n\t\treturn ipLocal, ok\n\t}\n\n\treturn nil, ipv6Miss\n}\n\nfunc (n *network) HandleQueryResp(name string, ip net.IP) {\n\tnetworkID := n.ID()\n\tc := n.getController()\n\tc.Lock()\n\tdefer c.Unlock()\n\tsr, ok := c.svcRecords[networkID]\n\n\tif !ok {\n\t\treturn\n\t}\n\n\tipStr := netutils.ReverseIP(ip.String())\n\t// If an object with extResolver == true is already in the set this call will fail\n\t// but anyway it means that has already been inserted before\n\tif ok, _ := sr.ipMap.Contains(ipStr, ipInfo{name: name}); ok {\n\t\tsr.ipMap.Remove(ipStr, ipInfo{name: name})\n\t\tsr.ipMap.Insert(ipStr, ipInfo{name: name, extResolver: true})\n\t}\n}\n\nfunc (n *network) ResolveIP(ip string) string {\n\tnetworkID := n.ID()\n\tc := n.getController()\n\tc.Lock()\n\tdefer c.Unlock()\n\tsr, ok := c.svcRecords[networkID]\n\n\tif !ok {\n\t\treturn \"\"\n\t}\n\n\tnwName := n.Name()\n\n\telemSet, ok := sr.ipMap.Get(ip)\n\tif !ok || len(elemSet) == 0 {\n\t\treturn \"\"\n\t}\n\t// NOTE it is possible to have more than one element in the Set, this will happen\n\t// because of interleave of different events from different sources (local container create vs\n\t// network db notifications)\n\t// In such cases the resolution will be based on the first element of the set, and can vary\n\t// during the system stabilitation\n\telem, ok := elemSet[0].(ipInfo)\n\tif !ok {\n\t\tsetStr, b := sr.ipMap.String(ip)\n\t\tlogrus.Errorf(\"expected set of ipInfo type for key %s set:%t %s\", ip, b, setStr)\n\t\treturn \"\"\n\t}\n\n\tif elem.extResolver {\n\t\treturn \"\"\n\t}\n\n\treturn elem.name + \".\" + nwName\n}\n\nfunc (n *network) ResolveService(name string) ([]*net.SRV, []net.IP) {\n\tc := n.getController()\n\n\tsrv := []*net.SRV{}\n\tip := []net.IP{}\n\n\tlogrus.Debugf(\"Service name To resolve: %v\", name)\n\n\t// There are DNS implementations that allow SRV queries for names not in\n\t// the format defined by RFC 2782. Hence specific validations checks are\n\t// not done\n\tparts := strings.Split(name, \".\")\n\tif len(parts) < 3 {\n\t\treturn nil, nil\n\t}\n\n\tportName := parts[0]\n\tproto := parts[1]\n\tsvcName := strings.Join(parts[2:], \".\")\n\n\tnetworkID := n.ID()\n\tc.Lock()\n\tdefer c.Unlock()\n\tsr, ok := c.svcRecords[networkID]\n\n\tif !ok {\n\t\treturn nil, nil\n\t}\n\n\tsvcs, ok := sr.service[svcName]\n\tif !ok {\n\t\treturn nil, nil\n\t}\n\n\tfor _, svc := range svcs {\n\t\tif svc.portName != portName {\n\t\t\tcontinue\n\t\t}\n\t\tif svc.proto != proto {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range svc.target {\n\t\t\tsrv = append(srv,\n\t\t\t\t&net.SRV{\n\t\t\t\t\tTarget: t.name,\n\t\t\t\t\tPort:   t.port,\n\t\t\t\t})\n\n\t\t\tip = append(ip, t.ip)\n\t\t}\n\t}\n\n\treturn srv, ip\n}\n\nfunc (n *network) ExecFunc(f func()) error {\n\treturn types.NotImplementedErrorf(\"ExecFunc not supported by network\")\n}\n\nfunc (n *network) NdotsSet() bool {\n\treturn false\n}\n\n// config-only network is looked up by name\nfunc (c *controller) getConfigNetwork(name string) (*network, error) {\n\tvar n Network\n\n\ts := func(current Network) bool {\n\t\tif current.Info().ConfigOnly() && current.Name() == name {\n\t\t\tn = current\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\tc.WalkNetworks(s)\n\n\tif n == nil {\n\t\treturn nil, types.NotFoundErrorf(\"configuration network %q not found\", name)\n\t}\n\n\treturn n.(*network), nil\n}\n\nfunc (n *network) lbSandboxName() string {\n\tname := \"lb-\" + n.name\n\tif n.ingress {\n\t\tname = n.name + \"-sbox\"\n\t}\n\treturn name\n}\n\nfunc (n *network) lbEndpointName() string {\n\treturn n.name + \"-endpoint\"\n}\n\nfunc (n *network) createLoadBalancerSandbox() (retErr error) {\n\tsandboxName := n.lbSandboxName()\n\t// Mark the sandbox to be a load balancer\n\tsbOptions := []SandboxOption{OptionLoadBalancer(n.id)}\n\tif n.ingress {\n\t\tsbOptions = append(sbOptions, OptionIngress())\n\t}\n\tsb, err := n.ctrlr.NewSandbox(sandboxName, sbOptions...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif e := n.ctrlr.SandboxDestroy(sandboxName); e != nil {\n\t\t\t\tlogrus.Warnf(\"could not delete sandbox %s on failure on failure (%v): %v\", sandboxName, retErr, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tendpointName := n.lbEndpointName()\n\tepOptions := []EndpointOption{\n\t\tCreateOptionIpam(n.loadBalancerIP, nil, nil, nil),\n\t\tCreateOptionLoadBalancer(),\n\t}\n\tif n.hasLoadBalancerEndpoint() && !n.ingress {\n\t\t// Mark LB endpoints as anonymous so they don't show up in DNS\n\t\tepOptions = append(epOptions, CreateOptionAnonymous())\n\t}\n\tep, err := n.createEndpoint(endpointName, epOptions...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif e := ep.Delete(true); e != nil {\n\t\t\t\tlogrus.Warnf(\"could not delete endpoint %s on failure on failure (%v): %v\", endpointName, retErr, e)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err := ep.Join(sb, nil); err != nil {\n\t\treturn err\n\t}\n\n\treturn sb.EnableService()\n}\n\nfunc (n *network) deleteLoadBalancerSandbox() error {\n\tn.Lock()\n\tc := n.ctrlr\n\tname := n.name\n\tn.Unlock()\n\n\tsandboxName := n.lbSandboxName()\n\tendpointName := n.lbEndpointName()\n\n\tendpoint, err := n.EndpointByName(endpointName)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Failed to find load balancer endpoint %s on network %s: %v\", endpointName, name, err)\n\t} else {\n\n\t\tinfo := endpoint.Info()\n\t\tif info != nil {\n\t\t\tsb := info.Sandbox()\n\t\t\tif sb != nil {\n\t\t\t\tif err := sb.DisableService(); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"Failed to disable service on sandbox %s: %v\", sandboxName, err)\n\t\t\t\t\t//Ignore error and attempt to delete the load balancer endpoint\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif err := endpoint.Delete(true); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to delete endpoint %s (%s) in %s: %v\", endpoint.Name(), endpoint.ID(), sandboxName, err)\n\t\t\t//Ignore error and attempt to delete the sandbox.\n\t\t}\n\t}\n\n\tif err := c.SandboxDestroy(sandboxName); err != nil {\n\t\treturn fmt.Errorf(\"Failed to delete %s sandbox: %v\", sandboxName, err)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "network_unix.go",
          "type": "blob",
          "size": 0.2802734375,
          "content": "//go:build !windows\n// +build !windows\n\npackage libnetwork\n\nimport \"github.com/docker/libnetwork/ipamapi\"\n\n// Stub implementations for DNS related functions\n\nfunc (n *network) startResolver() {\n}\n\nfunc defaultIpamForNetworkType(networkType string) string {\n\treturn ipamapi.DefaultIPAM\n}\n"
        },
        {
          "name": "network_windows.go",
          "type": "blob",
          "size": 1.8720703125,
          "content": "//go:build windows\n// +build windows\n\npackage libnetwork\n\nimport (\n\t\"runtime\"\n\t\"time\"\n\n\t\"github.com/Microsoft/hcsshim\"\n\t\"github.com/docker/libnetwork/drivers/windows\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/ipams/windowsipam\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nfunc executeInCompartment(compartmentID uint32, x func()) {\n\truntime.LockOSThread()\n\n\tif err := hcsshim.SetCurrentThreadCompartmentId(compartmentID); err != nil {\n\t\tlogrus.Error(err)\n\t}\n\tdefer func() {\n\t\thcsshim.SetCurrentThreadCompartmentId(0)\n\t\truntime.UnlockOSThread()\n\t}()\n\n\tx()\n}\n\nfunc (n *network) startResolver() {\n\tif n.networkType == \"ics\" {\n\t\treturn\n\t}\n\tn.resolverOnce.Do(func() {\n\t\tlogrus.Debugf(\"Launching DNS server for network %q\", n.Name())\n\t\toptions := n.Info().DriverOptions()\n\t\thnsid := options[windows.HNSID]\n\n\t\tif hnsid == \"\" {\n\t\t\treturn\n\t\t}\n\n\t\thnsresponse, err := hcsshim.HNSNetworkRequest(\"GET\", hnsid, \"\")\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Resolver Setup/Start failed for container %s, %q\", n.Name(), err)\n\t\t\treturn\n\t\t}\n\n\t\tfor _, subnet := range hnsresponse.Subnets {\n\t\t\tif subnet.GatewayAddress != \"\" {\n\t\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\t\tresolver := NewResolver(subnet.GatewayAddress, false, \"\", n)\n\t\t\t\t\tlogrus.Debugf(\"Binding a resolver on network %s gateway %s\", n.Name(), subnet.GatewayAddress)\n\t\t\t\t\texecuteInCompartment(hnsresponse.DNSServerCompartment, resolver.SetupFunc(53))\n\n\t\t\t\t\tif err = resolver.Start(); err != nil {\n\t\t\t\t\t\tlogrus.Errorf(\"Resolver Setup/Start failed for container %s, %q\", n.Name(), err)\n\t\t\t\t\t\ttime.Sleep(1 * time.Second)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogrus.Debugf(\"Resolver bound successfully for network %s\", n.Name())\n\t\t\t\t\t\tn.resolver = append(n.resolver, resolver)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc defaultIpamForNetworkType(networkType string) string {\n\tif windows.IsBuiltinLocalDriver(networkType) {\n\t\treturn windowsipam.DefaultIPAM\n\t}\n\treturn ipamapi.DefaultIPAM\n}\n"
        },
        {
          "name": "networkdb",
          "type": "tree",
          "content": null
        },
        {
          "name": "ns",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "osl",
          "type": "tree",
          "content": null
        },
        {
          "name": "portallocator",
          "type": "tree",
          "content": null
        },
        {
          "name": "portmapper",
          "type": "tree",
          "content": null
        },
        {
          "name": "resolvconf",
          "type": "tree",
          "content": null
        },
        {
          "name": "resolver.go",
          "type": "blob",
          "size": 14.8779296875,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/miekg/dns\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Resolver represents the embedded DNS server in Docker. It operates\n// by listening on container's loopback interface for DNS queries.\ntype Resolver interface {\n\t// Start starts the name server for the container\n\tStart() error\n\t// Stop stops the name server for the container. Stopped resolver\n\t// can be reused after running the SetupFunc again.\n\tStop()\n\t// SetupFunc() provides the setup function that should be run\n\t// in the container's network namespace.\n\tSetupFunc(int) func()\n\t// NameServer() returns the IP of the DNS resolver for the\n\t// containers.\n\tNameServer() string\n\t// SetExtServers configures the external nameservers the resolver\n\t// should use to forward queries\n\tSetExtServers([]extDNSEntry)\n\t// ResolverOptions returns resolv.conf options that should be set\n\tResolverOptions() []string\n}\n\n// DNSBackend represents a backend DNS resolver used for DNS name\n// resolution. All the queries to the resolver are forwarded to the\n// backend resolver.\ntype DNSBackend interface {\n\t// ResolveName resolves a service name to an IPv4 or IPv6 address by searching\n\t// the networks the sandbox is connected to. For IPv6 queries, second return\n\t// value will be true if the name exists in docker domain but doesn't have an\n\t// IPv6 address. Such queries shouldn't be forwarded to external nameservers.\n\tResolveName(name string, iplen int) ([]net.IP, bool)\n\t// ResolveIP returns the service name for the passed in IP. IP is in reverse dotted\n\t// notation; the format used for DNS PTR records\n\tResolveIP(name string) string\n\t// ResolveService returns all the backend details about the containers or hosts\n\t// backing a service. Its purpose is to satisfy an SRV query\n\tResolveService(name string) ([]*net.SRV, []net.IP)\n\t// ExecFunc allows a function to be executed in the context of the backend\n\t// on behalf of the resolver.\n\tExecFunc(f func()) error\n\t//NdotsSet queries the backends ndots dns option settings\n\tNdotsSet() bool\n\t// HandleQueryResp passes the name & IP from a response to the backend. backend\n\t// can use it to maintain any required state about the resolution\n\tHandleQueryResp(name string, ip net.IP)\n}\n\nconst (\n\tdnsPort         = \"53\"\n\tptrIPv4domain   = \".in-addr.arpa.\"\n\tptrIPv6domain   = \".ip6.arpa.\"\n\trespTTL         = 600\n\tmaxExtDNS       = 3 //max number of external servers to try\n\textIOTimeout    = 4 * time.Second\n\tdefaultRespSize = 512\n\tmaxConcurrent   = 1024\n\tlogInterval     = 2 * time.Second\n)\n\ntype extDNSEntry struct {\n\tIPStr        string\n\tHostLoopback bool\n}\n\n// resolver implements the Resolver interface\ntype resolver struct {\n\tbackend       DNSBackend\n\textDNSList    [maxExtDNS]extDNSEntry\n\tserver        *dns.Server\n\tconn          *net.UDPConn\n\ttcpServer     *dns.Server\n\ttcpListen     *net.TCPListener\n\terr           error\n\tcount         int32\n\ttStamp        time.Time\n\tqueryLock     sync.Mutex\n\tlistenAddress string\n\tproxyDNS      bool\n\tresolverKey   string\n\tstartCh       chan struct{}\n}\n\nfunc init() {\n\trand.Seed(time.Now().Unix())\n}\n\n// NewResolver creates a new instance of the Resolver\nfunc NewResolver(address string, proxyDNS bool, resolverKey string, backend DNSBackend) Resolver {\n\treturn &resolver{\n\t\tbackend:       backend,\n\t\tproxyDNS:      proxyDNS,\n\t\tlistenAddress: address,\n\t\tresolverKey:   resolverKey,\n\t\terr:           fmt.Errorf(\"setup not done yet\"),\n\t\tstartCh:       make(chan struct{}, 1),\n\t}\n}\n\nfunc (r *resolver) SetupFunc(port int) func() {\n\treturn func() {\n\t\tvar err error\n\n\t\t// DNS operates primarily on UDP\n\t\taddr := &net.UDPAddr{\n\t\t\tIP:   net.ParseIP(r.listenAddress),\n\t\t\tPort: port,\n\t\t}\n\n\t\tr.conn, err = net.ListenUDP(\"udp\", addr)\n\t\tif err != nil {\n\t\t\tr.err = fmt.Errorf(\"error in opening name server socket %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Listen on a TCP as well\n\t\ttcpaddr := &net.TCPAddr{\n\t\t\tIP:   net.ParseIP(r.listenAddress),\n\t\t\tPort: port,\n\t\t}\n\n\t\tr.tcpListen, err = net.ListenTCP(\"tcp\", tcpaddr)\n\t\tif err != nil {\n\t\t\tr.err = fmt.Errorf(\"error in opening name TCP server socket %v\", err)\n\t\t\treturn\n\t\t}\n\t\tr.err = nil\n\t}\n}\n\nfunc (r *resolver) Start() error {\n\tr.startCh <- struct{}{}\n\tdefer func() { <-r.startCh }()\n\n\t// make sure the resolver has been setup before starting\n\tif r.err != nil {\n\t\treturn r.err\n\t}\n\n\tif err := r.setupIPTable(); err != nil {\n\t\treturn fmt.Errorf(\"setting up IP table rules failed: %v\", err)\n\t}\n\n\ts := &dns.Server{Handler: r, PacketConn: r.conn}\n\tr.server = s\n\tgo func() {\n\t\ts.ActivateAndServe()\n\t}()\n\n\ttcpServer := &dns.Server{Handler: r, Listener: r.tcpListen}\n\tr.tcpServer = tcpServer\n\tgo func() {\n\t\ttcpServer.ActivateAndServe()\n\t}()\n\treturn nil\n}\n\nfunc (r *resolver) Stop() {\n\tr.startCh <- struct{}{}\n\tdefer func() { <-r.startCh }()\n\n\tif r.server != nil {\n\t\tr.server.Shutdown()\n\t}\n\tif r.tcpServer != nil {\n\t\tr.tcpServer.Shutdown()\n\t}\n\tr.conn = nil\n\tr.tcpServer = nil\n\tr.err = fmt.Errorf(\"setup not done yet\")\n\tr.tStamp = time.Time{}\n\tr.count = 0\n\tr.queryLock = sync.Mutex{}\n}\n\nfunc (r *resolver) SetExtServers(extDNS []extDNSEntry) {\n\tl := len(extDNS)\n\tif l > maxExtDNS {\n\t\tl = maxExtDNS\n\t}\n\tfor i := 0; i < l; i++ {\n\t\tr.extDNSList[i] = extDNS[i]\n\t}\n}\n\nfunc (r *resolver) NameServer() string {\n\treturn r.listenAddress\n}\n\nfunc (r *resolver) ResolverOptions() []string {\n\treturn []string{\"ndots:0\"}\n}\n\nfunc setCommonFlags(msg *dns.Msg) {\n\tmsg.RecursionAvailable = true\n}\n\nfunc shuffleAddr(addr []net.IP) []net.IP {\n\tfor i := len(addr) - 1; i > 0; i-- {\n\t\tr := rand.Intn(i + 1)\n\t\taddr[i], addr[r] = addr[r], addr[i]\n\t}\n\treturn addr\n}\n\nfunc createRespMsg(query *dns.Msg) *dns.Msg {\n\tresp := new(dns.Msg)\n\tresp.SetReply(query)\n\tsetCommonFlags(resp)\n\n\treturn resp\n}\n\nfunc (r *resolver) handleMXQuery(name string, query *dns.Msg) (*dns.Msg, error) {\n\taddrv4, _ := r.backend.ResolveName(name, types.IPv4)\n\taddrv6, _ := r.backend.ResolveName(name, types.IPv6)\n\n\tif addrv4 == nil && addrv6 == nil {\n\t\treturn nil, nil\n\t}\n\n\t// We were able to resolve the name. Respond with an empty list with\n\t// RcodeSuccess/NOERROR so that email clients can treat it as \"implicit MX\"\n\t// [RFC 5321 Section-5.1] and issue a Type A/AAAA query for the name.\n\n\tresp := createRespMsg(query)\n\treturn resp, nil\n}\n\nfunc (r *resolver) handleIPQuery(name string, query *dns.Msg, ipType int) (*dns.Msg, error) {\n\tvar addr []net.IP\n\tvar ipv6Miss bool\n\taddr, ipv6Miss = r.backend.ResolveName(name, ipType)\n\n\tif addr == nil && ipv6Miss {\n\t\t// Send a reply without any Answer sections\n\t\tlogrus.Debugf(\"[resolver] lookup name %s present without IPv6 address\", name)\n\t\tresp := createRespMsg(query)\n\t\treturn resp, nil\n\t}\n\tif addr == nil {\n\t\treturn nil, nil\n\t}\n\n\tlogrus.Debugf(\"[resolver] lookup for %s: IP %v\", name, addr)\n\n\tresp := createRespMsg(query)\n\tif len(addr) > 1 {\n\t\taddr = shuffleAddr(addr)\n\t}\n\tif ipType == types.IPv4 {\n\t\tfor _, ip := range addr {\n\t\t\trr := new(dns.A)\n\t\t\trr.Hdr = dns.RR_Header{Name: name, Rrtype: dns.TypeA, Class: dns.ClassINET, Ttl: respTTL}\n\t\t\trr.A = ip\n\t\t\tresp.Answer = append(resp.Answer, rr)\n\t\t}\n\t} else {\n\t\tfor _, ip := range addr {\n\t\t\trr := new(dns.AAAA)\n\t\t\trr.Hdr = dns.RR_Header{Name: name, Rrtype: dns.TypeAAAA, Class: dns.ClassINET, Ttl: respTTL}\n\t\t\trr.AAAA = ip\n\t\t\tresp.Answer = append(resp.Answer, rr)\n\t\t}\n\t}\n\treturn resp, nil\n}\n\nfunc (r *resolver) handlePTRQuery(ptr string, query *dns.Msg) (*dns.Msg, error) {\n\tvar parts []string\n\n\tif strings.HasSuffix(ptr, ptrIPv4domain) {\n\t\tparts = strings.Split(ptr, ptrIPv4domain)\n\t} else if strings.HasSuffix(ptr, ptrIPv6domain) {\n\t\tparts = strings.Split(ptr, ptrIPv6domain)\n\t} else {\n\t\treturn nil, fmt.Errorf(\"invalid PTR query, %v\", ptr)\n\t}\n\n\thost := r.backend.ResolveIP(parts[0])\n\n\tif len(host) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tlogrus.Debugf(\"[resolver] lookup for IP %s: name %s\", parts[0], host)\n\tfqdn := dns.Fqdn(host)\n\n\tresp := new(dns.Msg)\n\tresp.SetReply(query)\n\tsetCommonFlags(resp)\n\n\trr := new(dns.PTR)\n\trr.Hdr = dns.RR_Header{Name: ptr, Rrtype: dns.TypePTR, Class: dns.ClassINET, Ttl: respTTL}\n\trr.Ptr = fqdn\n\tresp.Answer = append(resp.Answer, rr)\n\treturn resp, nil\n}\n\nfunc (r *resolver) handleSRVQuery(svc string, query *dns.Msg) (*dns.Msg, error) {\n\n\tsrv, ip := r.backend.ResolveService(svc)\n\n\tif len(srv) == 0 {\n\t\treturn nil, nil\n\t}\n\tif len(srv) != len(ip) {\n\t\treturn nil, fmt.Errorf(\"invalid reply for SRV query %s\", svc)\n\t}\n\n\tresp := createRespMsg(query)\n\n\tfor i, r := range srv {\n\t\trr := new(dns.SRV)\n\t\trr.Hdr = dns.RR_Header{Name: svc, Rrtype: dns.TypePTR, Class: dns.ClassINET, Ttl: respTTL}\n\t\trr.Port = r.Port\n\t\trr.Target = r.Target\n\t\tresp.Answer = append(resp.Answer, rr)\n\n\t\trr1 := new(dns.A)\n\t\trr1.Hdr = dns.RR_Header{Name: r.Target, Rrtype: dns.TypeA, Class: dns.ClassINET, Ttl: respTTL}\n\t\trr1.A = ip[i]\n\t\tresp.Extra = append(resp.Extra, rr1)\n\t}\n\treturn resp, nil\n\n}\n\nfunc truncateResp(resp *dns.Msg, maxSize int, isTCP bool) {\n\tif !isTCP {\n\t\tresp.Truncated = true\n\t}\n\n\tsrv := resp.Question[0].Qtype == dns.TypeSRV\n\t// trim the Answer RRs one by one till the whole message fits\n\t// within the reply size\n\tfor resp.Len() > maxSize {\n\t\tresp.Answer = resp.Answer[:len(resp.Answer)-1]\n\n\t\tif srv && len(resp.Extra) > 0 {\n\t\t\tresp.Extra = resp.Extra[:len(resp.Extra)-1]\n\t\t}\n\t}\n}\n\nfunc (r *resolver) ServeDNS(w dns.ResponseWriter, query *dns.Msg) {\n\tvar (\n\t\textConn net.Conn\n\t\tresp    *dns.Msg\n\t\terr     error\n\t)\n\n\tif query == nil || len(query.Question) == 0 {\n\t\treturn\n\t}\n\n\tname := query.Question[0].Name\n\tswitch query.Question[0].Qtype {\n\tcase dns.TypeA:\n\t\tresp, err = r.handleIPQuery(name, query, types.IPv4)\n\tcase dns.TypeAAAA:\n\t\tresp, err = r.handleIPQuery(name, query, types.IPv6)\n\tcase dns.TypeMX:\n\t\tresp, err = r.handleMXQuery(name, query)\n\tcase dns.TypePTR:\n\t\tresp, err = r.handlePTRQuery(name, query)\n\tcase dns.TypeSRV:\n\t\tresp, err = r.handleSRVQuery(name, query)\n\t}\n\n\tif err != nil {\n\t\tlogrus.Error(err)\n\t\treturn\n\t}\n\n\tif resp == nil {\n\t\t// If the backend doesn't support proxying dns request\n\t\t// fail the response\n\t\tif !r.proxyDNS {\n\t\t\tresp = new(dns.Msg)\n\t\t\tresp.SetRcode(query, dns.RcodeServerFailure)\n\t\t\tw.WriteMsg(resp)\n\t\t\treturn\n\t\t}\n\n\t\t// If the user sets ndots > 0 explicitly and the query is\n\t\t// in the root domain don't forward it out. We will return\n\t\t// failure and let the client retry with the search domain\n\t\t// attached\n\t\tswitch query.Question[0].Qtype {\n\t\tcase dns.TypeA:\n\t\t\tfallthrough\n\t\tcase dns.TypeAAAA:\n\t\t\tif r.backend.NdotsSet() && !strings.Contains(strings.TrimSuffix(name, \".\"), \".\") {\n\t\t\t\tresp = createRespMsg(query)\n\t\t\t}\n\t\t}\n\t}\n\n\tproto := w.LocalAddr().Network()\n\tmaxSize := 0\n\tif proto == \"tcp\" {\n\t\tmaxSize = dns.MaxMsgSize - 1\n\t} else if proto == \"udp\" {\n\t\toptRR := query.IsEdns0()\n\t\tif optRR != nil {\n\t\t\tmaxSize = int(optRR.UDPSize())\n\t\t}\n\t\tif maxSize < defaultRespSize {\n\t\t\tmaxSize = defaultRespSize\n\t\t}\n\t}\n\n\tif resp != nil {\n\t\tif resp.Len() > maxSize {\n\t\t\ttruncateResp(resp, maxSize, proto == \"tcp\")\n\t\t}\n\t} else {\n\t\tfor i := 0; i < maxExtDNS; i++ {\n\t\t\textDNS := &r.extDNSList[i]\n\t\t\tif extDNS.IPStr == \"\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\textConnect := func() {\n\t\t\t\taddr := fmt.Sprintf(\"%s:%d\", extDNS.IPStr, 53)\n\t\t\t\textConn, err = net.DialTimeout(proto, addr, extIOTimeout)\n\t\t\t}\n\n\t\t\tif extDNS.HostLoopback {\n\t\t\t\textConnect()\n\t\t\t} else {\n\t\t\t\texecErr := r.backend.ExecFunc(extConnect)\n\t\t\t\tif execErr != nil {\n\t\t\t\t\tlogrus.Warn(execErr)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Warnf(\"[resolver] connect failed: %s\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tqueryType := dns.TypeToString[query.Question[0].Qtype]\n\t\t\tlogrus.Debugf(\"[resolver] query %s (%s) from %s, forwarding to %s:%s\", name, queryType,\n\t\t\t\textConn.LocalAddr().String(), proto, extDNS.IPStr)\n\n\t\t\t// Timeout has to be set for every IO operation.\n\t\t\textConn.SetDeadline(time.Now().Add(extIOTimeout))\n\t\t\tco := &dns.Conn{\n\t\t\t\tConn:    extConn,\n\t\t\t\tUDPSize: uint16(maxSize),\n\t\t\t}\n\t\t\tdefer co.Close()\n\n\t\t\t// limits the number of outstanding concurrent queries.\n\t\t\tif !r.forwardQueryStart() {\n\t\t\t\told := r.tStamp\n\t\t\t\tr.tStamp = time.Now()\n\t\t\t\tif r.tStamp.Sub(old) > logInterval {\n\t\t\t\t\tlogrus.Errorf(\"[resolver] more than %v concurrent queries from %s\", maxConcurrent, extConn.LocalAddr().String())\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\terr = co.WriteMsg(query)\n\t\t\tif err != nil {\n\t\t\t\tr.forwardQueryEnd()\n\t\t\t\tlogrus.Debugf(\"[resolver] send to DNS server failed, %s\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresp, err = co.ReadMsg()\n\t\t\t// Truncated DNS replies should be sent to the client so that the\n\t\t\t// client can retry over TCP\n\t\t\tif err != nil && (resp == nil || !resp.Truncated) {\n\t\t\t\tr.forwardQueryEnd()\n\t\t\t\tlogrus.Debugf(\"[resolver] read from DNS server failed, %s\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tr.forwardQueryEnd()\n\n\t\t\tif resp == nil {\n\t\t\t\tlogrus.Debugf(\"[resolver] external DNS %s:%s returned empty response for %q\", proto, extDNS.IPStr, name)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tswitch resp.Rcode {\n\t\t\tcase dns.RcodeServerFailure, dns.RcodeRefused:\n\t\t\t\t// Server returned FAILURE: continue with the next external DNS server\n\t\t\t\t// Server returned REFUSED: this can be a transitional status, so continue with the next external DNS server\n\t\t\t\tlogrus.Debugf(\"[resolver] external DNS %s:%s responded with %s for %q\", proto, extDNS.IPStr, statusString(resp.Rcode), name)\n\t\t\t\tcontinue\n\t\t\tcase dns.RcodeNameError:\n\t\t\t\t// Server returned NXDOMAIN. Stop resolution if it's an authoritative answer (see RFC 8020: https://tools.ietf.org/html/rfc8020#section-2)\n\t\t\t\tlogrus.Debugf(\"[resolver] external DNS %s:%s responded with %s for %q\", proto, extDNS.IPStr, statusString(resp.Rcode), name)\n\t\t\t\tif resp.Authoritative {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\tcase dns.RcodeSuccess:\n\t\t\t\t// All is well\n\t\t\tdefault:\n\t\t\t\t// Server gave some error. Log the error, and continue with the next external DNS server\n\t\t\t\tlogrus.Debugf(\"[resolver] external DNS %s:%s responded with %s (code %d) for %q\", proto, extDNS.IPStr, statusString(resp.Rcode), resp.Rcode, name)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tanswers := 0\n\t\t\tfor _, rr := range resp.Answer {\n\t\t\t\th := rr.Header()\n\t\t\t\tswitch h.Rrtype {\n\t\t\t\tcase dns.TypeA:\n\t\t\t\t\tanswers++\n\t\t\t\t\tip := rr.(*dns.A).A\n\t\t\t\t\tlogrus.Debugf(\"[resolver] received A record %q for %q from %s:%s\", ip, h.Name, proto, extDNS.IPStr)\n\t\t\t\t\tr.backend.HandleQueryResp(h.Name, ip)\n\t\t\t\tcase dns.TypeAAAA:\n\t\t\t\t\tanswers++\n\t\t\t\t\tip := rr.(*dns.AAAA).AAAA\n\t\t\t\t\tlogrus.Debugf(\"[resolver] received AAAA record %q for %q from %s:%s\", ip, h.Name, proto, extDNS.IPStr)\n\t\t\t\t\tr.backend.HandleQueryResp(h.Name, ip)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif resp.Answer == nil || answers == 0 {\n\t\t\t\tlogrus.Debugf(\"[resolver] external DNS %s:%s did not return any %s records for %q\", proto, extDNS.IPStr, queryType, name)\n\t\t\t}\n\t\t\tresp.Compress = true\n\t\t\tbreak\n\t\t}\n\t\tif resp == nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif err = w.WriteMsg(resp); err != nil {\n\t\tlogrus.Errorf(\"[resolver] error writing resolver resp, %s\", err)\n\t}\n}\n\nfunc statusString(responseCode int) string {\n\tif s, ok := dns.RcodeToString[responseCode]; ok {\n\t\treturn s\n\t}\n\treturn \"UNKNOWN\"\n}\n\nfunc (r *resolver) forwardQueryStart() bool {\n\tr.queryLock.Lock()\n\tdefer r.queryLock.Unlock()\n\n\tif r.count == maxConcurrent {\n\t\treturn false\n\t}\n\tr.count++\n\n\treturn true\n}\n\nfunc (r *resolver) forwardQueryEnd() {\n\tr.queryLock.Lock()\n\tdefer r.queryLock.Unlock()\n\n\tif r.count == 0 {\n\t\tlogrus.Error(\"[resolver] invalid concurrent query count\")\n\t} else {\n\t\tr.count--\n\t}\n}\n"
        },
        {
          "name": "resolver_test.go",
          "type": "blob",
          "size": 7.0556640625,
          "content": "package libnetwork\n\nimport (\n\t\"bytes\"\n\t\"net\"\n\t\"syscall\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/miekg/dns\"\n)\n\n// a simple/null address type that will be used to fake a local address for unit testing\ntype tstaddr struct {\n\tnet string\n}\n\nfunc (a *tstaddr) Network() string { return \"tcp\" }\n\nfunc (a *tstaddr) String() string { return \"127.0.0.1\" }\n\n// a simple writer that implements dns.ResponseWriter for unit testing purposes\ntype tstwriter struct {\n\tmsg *dns.Msg\n}\n\nfunc (w *tstwriter) WriteMsg(m *dns.Msg) (err error) {\n\tw.msg = m\n\treturn nil\n}\n\nfunc (w *tstwriter) Write(m []byte) (int, error) { return 0, nil }\n\nfunc (w *tstwriter) LocalAddr() net.Addr { return new(tstaddr) }\n\nfunc (w *tstwriter) RemoteAddr() net.Addr { return new(tstaddr) }\n\nfunc (w *tstwriter) TsigStatus() error { return nil }\n\nfunc (w *tstwriter) TsigTimersOnly(b bool) {}\n\nfunc (w *tstwriter) Hijack() {}\n\nfunc (w *tstwriter) Close() error { return nil }\n\nfunc (w *tstwriter) GetResponse() *dns.Msg { return w.msg }\n\nfunc (w *tstwriter) ClearResponse() { w.msg = nil }\n\nfunc checkNonNullResponse(t *testing.T, m *dns.Msg) {\n\tif m == nil {\n\t\tt.Fatal(\"Null DNS response found. Non Null response msg expected.\")\n\t}\n}\n\nfunc checkNullResponse(t *testing.T, m *dns.Msg) {\n\tif m != nil {\n\t\tt.Fatal(\"Non Null DNS response found. Null response msg expected.\")\n\t}\n}\n\nfunc checkDNSAnswersCount(t *testing.T, m *dns.Msg, expected int) {\n\tanswers := len(m.Answer)\n\tif answers != expected {\n\t\tt.Fatalf(\"Expected number of answers in response: %d. Found: %d\", expected, answers)\n\t}\n}\n\nfunc checkDNSResponseCode(t *testing.T, m *dns.Msg, expected int) {\n\tif m.MsgHdr.Rcode != expected {\n\t\tt.Fatalf(\"Expected DNS response code: %d. Found: %d\", expected, m.MsgHdr.Rcode)\n\t}\n}\n\nfunc checkDNSRRType(t *testing.T, actual, expected uint16) {\n\tif actual != expected {\n\t\tt.Fatalf(\"Expected DNS Rrtype: %d. Found: %d\", expected, actual)\n\t}\n}\n\nfunc TestDNSIPQuery(t *testing.T) {\n\tc, err := New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tn, err := c.NewNetwork(\"bridge\", \"dtnet1\", \"\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tep, err := n.CreateEndpoint(\"testep\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsb, err := c.NewSandbox(\"c1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// we need the endpoint only to populate ep_list for the sandbox as part of resolve_name\n\t// it is not set as a target for name resolution and does not serve any other purpose\n\terr = ep.Join(sb)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// add service records which are used to resolve names. These are the real targets for the DNS querries\n\tn.(*network).addSvcRecords(\"ep1\", \"name1\", \"svc1\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\n\tw := new(tstwriter)\n\t// the unit tests right now will focus on non-proxyed DNS requests\n\tr := NewResolver(resolverIPSandbox, false, sb.Key(), sb.(*sandbox))\n\n\t// test name1's IP is resolved correctly with the default A type query\n\t// Also make sure DNS lookups are case insensitive\n\tnames := []string{\"name1\", \"NaMe1\"}\n\tfor _, name := range names {\n\t\tq := new(dns.Msg)\n\t\tq.SetQuestion(name, dns.TypeA)\n\t\tr.(*resolver).ServeDNS(w, q)\n\t\tresp := w.GetResponse()\n\t\tcheckNonNullResponse(t, resp)\n\t\tt.Log(\"Response: \", resp.String())\n\t\tcheckDNSResponseCode(t, resp, dns.RcodeSuccess)\n\t\tcheckDNSAnswersCount(t, resp, 1)\n\t\tcheckDNSRRType(t, resp.Answer[0].Header().Rrtype, dns.TypeA)\n\t\tif answer, ok := resp.Answer[0].(*dns.A); ok {\n\t\t\tif !bytes.Equal(answer.A, net.ParseIP(\"192.168.0.1\")) {\n\t\t\t\tt.Fatalf(\"IP response in Answer %v does not match 192.168.0.1\", answer.A)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Fatal(\"Answer of type A not found\")\n\t\t}\n\t\tw.ClearResponse()\n\t}\n\n\t// test MX query with name1 results in Success response with 0 answer records\n\tq := new(dns.Msg)\n\tq.SetQuestion(\"name1\", dns.TypeMX)\n\tr.(*resolver).ServeDNS(w, q)\n\tresp := w.GetResponse()\n\tcheckNonNullResponse(t, resp)\n\tt.Log(\"Response: \", resp.String())\n\tcheckDNSResponseCode(t, resp, dns.RcodeSuccess)\n\tcheckDNSAnswersCount(t, resp, 0)\n\tw.ClearResponse()\n\n\t// test MX query with non existent name results in ServFail response with 0 answer records\n\t// since this is a unit test env, we disable proxying DNS above which results in ServFail rather than NXDOMAIN\n\tq = new(dns.Msg)\n\tq.SetQuestion(\"nonexistent\", dns.TypeMX)\n\tr.(*resolver).ServeDNS(w, q)\n\tresp = w.GetResponse()\n\tcheckNonNullResponse(t, resp)\n\tt.Log(\"Response: \", resp.String())\n\tcheckDNSResponseCode(t, resp, dns.RcodeServerFailure)\n\tw.ClearResponse()\n\n}\n\nfunc newDNSHandlerServFailOnce(requests *int) func(w dns.ResponseWriter, r *dns.Msg) {\n\treturn func(w dns.ResponseWriter, r *dns.Msg) {\n\t\tm := new(dns.Msg)\n\t\tm.SetReply(r)\n\t\tm.Compress = false\n\t\tif *requests == 0 {\n\t\t\tm.SetRcode(r, dns.RcodeServerFailure)\n\t\t}\n\t\t*requests = *requests + 1\n\t\tw.WriteMsg(m)\n\t}\n}\n\nfunc waitForLocalDNSServer(t *testing.T) {\n\tretries := 0\n\tmaxRetries := 10\n\n\tfor retries < maxRetries {\n\t\tt.Log(\"Try connecting to DNS server ...\")\n\t\t// this test and retry mechanism only works for TCP. With UDP there is no\n\t\t// connection and the test becomes inaccurate leading to unpredictable results\n\t\ttconn, err := net.DialTimeout(\"tcp\", \"127.0.0.1:53\", 10*time.Second)\n\t\tretries = retries + 1\n\t\tif err != nil {\n\t\t\tif oerr, ok := err.(*net.OpError); ok {\n\t\t\t\t// server is probably initializing\n\t\t\t\tif oerr.Err == syscall.ECONNREFUSED {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// something is wrong: we should stop for analysis\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t\tif tconn != nil {\n\t\t\ttconn.Close()\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc TestDNSProxyServFail(t *testing.T) {\n\tc, err := New()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer c.Stop()\n\n\tn, err := c.NewNetwork(\"bridge\", \"dtnet2\", \"\", nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer func() {\n\t\tif err := n.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tsb, err := c.NewSandbox(\"c1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdefer func() {\n\t\tif err := sb.Delete(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tvar nRequests int\n\t// initialize a local DNS server and configure it to fail the first query\n\tdns.HandleFunc(\".\", newDNSHandlerServFailOnce(&nRequests))\n\t// use TCP for predictable results. Connection tests (to figure out DNS server initialization) don't work with UDP\n\tserver := &dns.Server{Addr: \":53\", Net: \"tcp\"}\n\tgo server.ListenAndServe()\n\tdefer server.Shutdown()\n\n\twaitForLocalDNSServer(t)\n\tt.Log(\"DNS Server can be reached\")\n\n\tw := new(tstwriter)\n\tr := NewResolver(resolverIPSandbox, true, sb.Key(), sb.(*sandbox))\n\tq := new(dns.Msg)\n\tq.SetQuestion(\"name1.\", dns.TypeA)\n\n\tvar localDNSEntries []extDNSEntry\n\textTestDNSEntry := extDNSEntry{IPStr: \"127.0.0.1\", HostLoopback: true}\n\n\t// configure two external DNS entries and point both to local DNS server thread\n\tlocalDNSEntries = append(localDNSEntries, extTestDNSEntry)\n\tlocalDNSEntries = append(localDNSEntries, extTestDNSEntry)\n\n\t// this should generate two requests: the first will fail leading to a retry\n\tr.(*resolver).SetExtServers(localDNSEntries)\n\tr.(*resolver).ServeDNS(w, q)\n\tif nRequests != 2 {\n\t\tt.Fatalf(\"Expected 2 DNS querries. Found: %d\", nRequests)\n\t}\n\tt.Logf(\"Expected number of DNS requests generated\")\n}\n"
        },
        {
          "name": "resolver_unix.go",
          "type": "blob",
          "size": 3.0595703125,
          "content": "//go:build !windows\n// +build !windows\n\npackage libnetwork\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"runtime\"\n\n\t\"github.com/docker/docker/pkg/reexec\"\n\t\"github.com/docker/libnetwork/iptables\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netns\"\n)\n\nfunc init() {\n\treexec.Register(\"setup-resolver\", reexecSetupResolver)\n}\n\nconst (\n\t// outputChain used for docker embed dns\n\toutputChain = \"DOCKER_OUTPUT\"\n\t//postroutingchain used for docker embed dns\n\tpostroutingchain = \"DOCKER_POSTROUTING\"\n)\n\nfunc reexecSetupResolver() {\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\tif len(os.Args) < 4 {\n\t\tlogrus.Error(\"invalid number of arguments..\")\n\t\tos.Exit(1)\n\t}\n\n\tresolverIP, ipPort, _ := net.SplitHostPort(os.Args[2])\n\t_, tcpPort, _ := net.SplitHostPort(os.Args[3])\n\trules := [][]string{\n\t\t{\"-t\", \"nat\", \"-I\", outputChain, \"-d\", resolverIP, \"-p\", \"udp\", \"--dport\", dnsPort, \"-j\", \"DNAT\", \"--to-destination\", os.Args[2]},\n\t\t{\"-t\", \"nat\", \"-I\", postroutingchain, \"-s\", resolverIP, \"-p\", \"udp\", \"--sport\", ipPort, \"-j\", \"SNAT\", \"--to-source\", \":\" + dnsPort},\n\t\t{\"-t\", \"nat\", \"-I\", outputChain, \"-d\", resolverIP, \"-p\", \"tcp\", \"--dport\", dnsPort, \"-j\", \"DNAT\", \"--to-destination\", os.Args[3]},\n\t\t{\"-t\", \"nat\", \"-I\", postroutingchain, \"-s\", resolverIP, \"-p\", \"tcp\", \"--sport\", tcpPort, \"-j\", \"SNAT\", \"--to-source\", \":\" + dnsPort},\n\t}\n\n\tf, err := os.OpenFile(os.Args[1], os.O_RDONLY, 0)\n\tif err != nil {\n\t\tlogrus.Errorf(\"failed get network namespace %q: %v\", os.Args[1], err)\n\t\tos.Exit(2)\n\t}\n\tdefer f.Close()\n\n\tnsFD := f.Fd()\n\tif err = netns.Set(netns.NsHandle(nsFD)); err != nil {\n\t\tlogrus.Errorf(\"setting into container net ns %v failed, %v\", os.Args[1], err)\n\t\tos.Exit(3)\n\t}\n\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\n\t// insert outputChain and postroutingchain\n\terr = iptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-C\", \"OUTPUT\", \"-d\", resolverIP, \"-j\", outputChain)\n\tif err == nil {\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-F\", outputChain)\n\t} else {\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-N\", outputChain)\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-I\", \"OUTPUT\", \"-d\", resolverIP, \"-j\", outputChain)\n\t}\n\n\terr = iptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-C\", \"POSTROUTING\", \"-d\", resolverIP, \"-j\", postroutingchain)\n\tif err == nil {\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-F\", postroutingchain)\n\t} else {\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-N\", postroutingchain)\n\t\tiptable.RawCombinedOutputNative(\"-t\", \"nat\", \"-I\", \"POSTROUTING\", \"-d\", resolverIP, \"-j\", postroutingchain)\n\t}\n\n\tfor _, rule := range rules {\n\t\tif iptable.RawCombinedOutputNative(rule...) != nil {\n\t\t\tlogrus.Errorf(\"set up rule failed, %v\", rule)\n\t\t}\n\t}\n}\n\nfunc (r *resolver) setupIPTable() error {\n\tif r.err != nil {\n\t\treturn r.err\n\t}\n\tladdr := r.conn.LocalAddr().String()\n\tltcpaddr := r.tcpListen.Addr().String()\n\n\tcmd := &exec.Cmd{\n\t\tPath:   reexec.Self(),\n\t\tArgs:   append([]string{\"setup-resolver\"}, r.resolverKey, laddr, ltcpaddr),\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n\tif err := cmd.Run(); err != nil {\n\t\treturn fmt.Errorf(\"reexec failed: %v\", err)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "resolver_windows.go",
          "type": "blob",
          "size": 0.111328125,
          "content": "//go:build windows\n// +build windows\n\npackage libnetwork\n\nfunc (r *resolver) setupIPTable() error {\n\treturn nil\n}\n"
        },
        {
          "name": "sandbox.go",
          "type": "blob",
          "size": 31.560546875,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/libnetwork/etchosts\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Sandbox provides the control over the network container entity. It is a one to one mapping with the container.\ntype Sandbox interface {\n\t// ID returns the ID of the sandbox\n\tID() string\n\t// Key returns the sandbox's key\n\tKey() string\n\t// ContainerID returns the container id associated to this sandbox\n\tContainerID() string\n\t// Labels returns the sandbox's labels\n\tLabels() map[string]interface{}\n\t// Statistics retrieves the interfaces' statistics for the sandbox\n\tStatistics() (map[string]*types.InterfaceStatistics, error)\n\t// Refresh leaves all the endpoints, resets and re-applies the options,\n\t// re-joins all the endpoints without destroying the osl sandbox\n\tRefresh(options ...SandboxOption) error\n\t// SetKey updates the Sandbox Key\n\tSetKey(key string) error\n\t// Rename changes the name of all attached Endpoints\n\tRename(name string) error\n\t// Delete destroys this container after detaching it from all connected endpoints.\n\tDelete() error\n\t// Endpoints returns all the endpoints connected to the sandbox\n\tEndpoints() []Endpoint\n\t// ResolveService returns all the backend details about the containers or hosts\n\t// backing a service. Its purpose is to satisfy an SRV query\n\tResolveService(name string) ([]*net.SRV, []net.IP)\n\t// EnableService  makes a managed container's service available by adding the\n\t// endpoint to the service load balancer and service discovery\n\tEnableService() error\n\t// DisableService removes a managed container's endpoints from the load balancer\n\t// and service discovery\n\tDisableService() error\n}\n\n// SandboxOption is an option setter function type used to pass various options to\n// NewNetContainer method. The various setter functions of type SandboxOption are\n// provided by libnetwork, they look like ContainerOptionXXXX(...)\ntype SandboxOption func(sb *sandbox)\n\nfunc (sb *sandbox) processOptions(options ...SandboxOption) {\n\tfor _, opt := range options {\n\t\tif opt != nil {\n\t\t\topt(sb)\n\t\t}\n\t}\n}\n\ntype sandbox struct {\n\tid                 string\n\tcontainerID        string\n\tconfig             containerConfig\n\textDNS             []extDNSEntry\n\tosSbox             osl.Sandbox\n\tcontroller         *controller\n\tresolver           Resolver\n\tresolverOnce       sync.Once\n\trefCnt             int\n\tendpoints          []*endpoint\n\tepPriority         map[string]int\n\tpopulatedEndpoints map[string]struct{}\n\tjoinLeaveDone      chan struct{}\n\tdbIndex            uint64\n\tdbExists           bool\n\tisStub             bool\n\tinDelete           bool\n\tingress            bool\n\tndotsSet           bool\n\toslTypes           []osl.SandboxType // slice of properties of this sandbox\n\tloadBalancerNID    string            // NID that this SB is a load balancer for\n\tsync.Mutex\n\t// This mutex is used to serialize service related operation for an endpoint\n\t// The lock is here because the endpoint is saved into the store so is not unique\n\tService sync.Mutex\n}\n\n// These are the container configs used to customize container /etc/hosts file.\ntype hostsPathConfig struct {\n\thostName        string\n\tdomainName      string\n\thostsPath       string\n\toriginHostsPath string\n\textraHosts      []extraHost\n\tparentUpdates   []parentUpdate\n}\n\ntype parentUpdate struct {\n\tcid  string\n\tname string\n\tip   string\n}\n\ntype extraHost struct {\n\tname string\n\tIP   string\n}\n\n// These are the container configs used to customize container /etc/resolv.conf file.\ntype resolvConfPathConfig struct {\n\tresolvConfPath       string\n\toriginResolvConfPath string\n\tresolvConfHashFile   string\n\tdnsList              []string\n\tdnsSearchList        []string\n\tdnsOptionsList       []string\n}\n\ntype containerConfig struct {\n\thostsPathConfig\n\tresolvConfPathConfig\n\tgeneric           map[string]interface{}\n\tuseDefaultSandBox bool\n\tuseExternalKey    bool\n\tprio              int // higher the value, more the priority\n\texposedPorts      []types.TransportPort\n}\n\nconst (\n\tresolverIPSandbox = \"127.0.0.11\"\n)\n\nfunc (sb *sandbox) ID() string {\n\treturn sb.id\n}\n\nfunc (sb *sandbox) ContainerID() string {\n\treturn sb.containerID\n}\n\nfunc (sb *sandbox) Key() string {\n\tif sb.config.useDefaultSandBox {\n\t\treturn osl.GenerateKey(\"default\")\n\t}\n\treturn osl.GenerateKey(sb.id)\n}\n\nfunc (sb *sandbox) Labels() map[string]interface{} {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\topts := make(map[string]interface{}, len(sb.config.generic))\n\tfor k, v := range sb.config.generic {\n\t\topts[k] = v\n\t}\n\treturn opts\n}\n\nfunc (sb *sandbox) Statistics() (map[string]*types.InterfaceStatistics, error) {\n\tm := make(map[string]*types.InterfaceStatistics)\n\n\tsb.Lock()\n\tosb := sb.osSbox\n\tsb.Unlock()\n\tif osb == nil {\n\t\treturn m, nil\n\t}\n\n\tvar err error\n\tfor _, i := range osb.Info().Interfaces() {\n\t\tif m[i.DstName()], err = i.Statistics(); err != nil {\n\t\t\treturn m, err\n\t\t}\n\t}\n\n\treturn m, nil\n}\n\nfunc (sb *sandbox) Delete() error {\n\treturn sb.delete(false)\n}\n\nfunc (sb *sandbox) delete(force bool) error {\n\tsb.Lock()\n\tif sb.inDelete {\n\t\tsb.Unlock()\n\t\treturn types.ForbiddenErrorf(\"another sandbox delete in progress\")\n\t}\n\t// Set the inDelete flag. This will ensure that we don't\n\t// update the store until we have completed all the endpoint\n\t// leaves and deletes. And when endpoint leaves and deletes\n\t// are completed then we can finally delete the sandbox object\n\t// altogether from the data store. If the daemon exits\n\t// ungracefully in the middle of a sandbox delete this way we\n\t// will have all the references to the endpoints in the\n\t// sandbox so that we can clean them up when we restart\n\tsb.inDelete = true\n\tsb.Unlock()\n\n\tc := sb.controller\n\n\t// Detach from all endpoints\n\tretain := false\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\t// gw network endpoint detach and removal are automatic\n\t\tif ep.endpointInGWNetwork() && !force {\n\t\t\tcontinue\n\t\t}\n\t\t// Retain the sanbdox if we can't obtain the network from store.\n\t\tif _, err := c.getNetworkFromStore(ep.getNetwork().ID()); err != nil {\n\t\t\tif c.isDistributedControl() {\n\t\t\t\tretain = true\n\t\t\t}\n\t\t\tlogrus.Warnf(\"Failed getting network for ep %s during sandbox %s delete: %v\", ep.ID(), sb.ID(), err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !force {\n\t\t\tif err := ep.Leave(sb); err != nil {\n\t\t\t\tlogrus.Warnf(\"Failed detaching sandbox %s from endpoint %s: %v\\n\", sb.ID(), ep.ID(), err)\n\t\t\t}\n\t\t}\n\n\t\tif err := ep.Delete(force); err != nil {\n\t\t\tlogrus.Warnf(\"Failed deleting endpoint %s: %v\\n\", ep.ID(), err)\n\t\t}\n\t}\n\n\tif retain {\n\t\tsb.Lock()\n\t\tsb.inDelete = false\n\t\tsb.Unlock()\n\t\treturn fmt.Errorf(\"could not cleanup all the endpoints in container %s / sandbox %s\", sb.containerID, sb.id)\n\t}\n\t// Container is going away. Path cache in etchosts is most\n\t// likely not required any more. Drop it.\n\tetchosts.Drop(sb.config.hostsPath)\n\n\tif sb.resolver != nil {\n\t\tsb.resolver.Stop()\n\t}\n\n\tif sb.osSbox != nil && !sb.config.useDefaultSandBox {\n\t\tsb.osSbox.Destroy()\n\t}\n\n\tif err := sb.storeDelete(); err != nil {\n\t\tlogrus.Warnf(\"Failed to delete sandbox %s from store: %v\", sb.ID(), err)\n\t}\n\n\tc.Lock()\n\tif sb.ingress {\n\t\tc.ingressSandbox = nil\n\t}\n\tdelete(c.sandboxes, sb.ID())\n\tc.Unlock()\n\n\treturn nil\n}\n\nfunc (sb *sandbox) Rename(name string) error {\n\tvar err error\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.endpointInGWNetwork() {\n\t\t\tcontinue\n\t\t}\n\n\t\toldName := ep.Name()\n\t\tlEp := ep\n\t\tif err = ep.rename(name); err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tlEp.rename(oldName)\n\t\t\t}\n\t\t}()\n\t}\n\n\treturn err\n}\n\nfunc (sb *sandbox) Refresh(options ...SandboxOption) error {\n\t// Store connected endpoints\n\tepList := sb.getConnectedEndpoints()\n\n\t// Detach from all endpoints\n\tfor _, ep := range epList {\n\t\tif err := ep.Leave(sb); err != nil {\n\t\t\tlogrus.Warnf(\"Failed detaching sandbox %s from endpoint %s: %v\\n\", sb.ID(), ep.ID(), err)\n\t\t}\n\t}\n\n\t// Re-apply options\n\tsb.config = containerConfig{}\n\tsb.processOptions(options...)\n\n\t// Setup discovery files\n\tif err := sb.setupResolutionFiles(); err != nil {\n\t\treturn err\n\t}\n\n\t// Re-connect to all endpoints\n\tfor _, ep := range epList {\n\t\tif err := ep.Join(sb); err != nil {\n\t\t\tlogrus.Warnf(\"Failed attach sandbox %s to endpoint %s: %v\\n\", sb.ID(), ep.ID(), err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) MarshalJSON() ([]byte, error) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\t// We are just interested in the container ID. This can be expanded to include all of containerInfo if there is a need\n\treturn json.Marshal(sb.id)\n}\n\nfunc (sb *sandbox) UnmarshalJSON(b []byte) (err error) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tvar id string\n\tif err := json.Unmarshal(b, &id); err != nil {\n\t\treturn err\n\t}\n\tsb.id = id\n\treturn nil\n}\n\nfunc (sb *sandbox) Endpoints() []Endpoint {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tendpoints := make([]Endpoint, len(sb.endpoints))\n\tfor i, ep := range sb.endpoints {\n\t\tendpoints[i] = ep\n\t}\n\treturn endpoints\n}\n\nfunc (sb *sandbox) getConnectedEndpoints() []*endpoint {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\teps := make([]*endpoint, len(sb.endpoints))\n\tcopy(eps, sb.endpoints)\n\n\treturn eps\n}\n\nfunc (sb *sandbox) addEndpoint(ep *endpoint) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tl := len(sb.endpoints)\n\ti := sort.Search(l, func(j int) bool {\n\t\treturn ep.Less(sb.endpoints[j])\n\t})\n\n\tsb.endpoints = append(sb.endpoints, nil)\n\tcopy(sb.endpoints[i+1:], sb.endpoints[i:])\n\tsb.endpoints[i] = ep\n}\n\nfunc (sb *sandbox) removeEndpoint(ep *endpoint) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tsb.removeEndpointRaw(ep)\n}\n\nfunc (sb *sandbox) removeEndpointRaw(ep *endpoint) {\n\tfor i, e := range sb.endpoints {\n\t\tif e == ep {\n\t\t\tsb.endpoints = append(sb.endpoints[:i], sb.endpoints[i+1:]...)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (sb *sandbox) getEndpoint(id string) *endpoint {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tfor _, ep := range sb.endpoints {\n\t\tif ep.id == id {\n\t\t\treturn ep\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) updateGateway(ep *endpoint) error {\n\tsb.Lock()\n\tosSbox := sb.osSbox\n\tsb.Unlock()\n\tif osSbox == nil {\n\t\treturn nil\n\t}\n\tosSbox.UnsetGateway()\n\tosSbox.UnsetGatewayIPv6()\n\n\tif ep == nil {\n\t\treturn nil\n\t}\n\n\tep.Lock()\n\tjoinInfo := ep.joinInfo\n\tep.Unlock()\n\n\tif err := osSbox.SetGateway(joinInfo.gw); err != nil {\n\t\treturn fmt.Errorf(\"failed to set gateway while updating gateway: %v\", err)\n\t}\n\n\tif err := osSbox.SetGatewayIPv6(joinInfo.gw6); err != nil {\n\t\treturn fmt.Errorf(\"failed to set IPv6 gateway while updating gateway: %v\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) HandleQueryResp(name string, ip net.IP) {\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tn := ep.getNetwork()\n\t\tn.HandleQueryResp(name, ip)\n\t}\n}\n\nfunc (sb *sandbox) ResolveIP(ip string) string {\n\tvar svc string\n\tlogrus.Debugf(\"IP To resolve %v\", ip)\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tn := ep.getNetwork()\n\t\tsvc = n.ResolveIP(ip)\n\t\tif len(svc) != 0 {\n\t\t\treturn svc\n\t\t}\n\t}\n\n\treturn svc\n}\n\nfunc (sb *sandbox) ExecFunc(f func()) error {\n\tsb.Lock()\n\tosSbox := sb.osSbox\n\tsb.Unlock()\n\tif osSbox != nil {\n\t\treturn osSbox.InvokeFunc(f)\n\t}\n\treturn fmt.Errorf(\"osl sandbox unavailable in ExecFunc for %v\", sb.ContainerID())\n}\n\nfunc (sb *sandbox) ResolveService(name string) ([]*net.SRV, []net.IP) {\n\tsrv := []*net.SRV{}\n\tip := []net.IP{}\n\n\tlogrus.Debugf(\"Service name To resolve: %v\", name)\n\n\t// There are DNS implementations that allow SRV queries for names not in\n\t// the format defined by RFC 2782. Hence specific validations checks are\n\t// not done\n\tparts := strings.Split(name, \".\")\n\tif len(parts) < 3 {\n\t\treturn nil, nil\n\t}\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tn := ep.getNetwork()\n\n\t\tsrv, ip = n.ResolveService(name)\n\t\tif len(srv) > 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn srv, ip\n}\n\nfunc getDynamicNwEndpoints(epList []*endpoint) []*endpoint {\n\teps := []*endpoint{}\n\tfor _, ep := range epList {\n\t\tn := ep.getNetwork()\n\t\tif n.dynamic && !n.ingress {\n\t\t\teps = append(eps, ep)\n\t\t}\n\t}\n\treturn eps\n}\n\nfunc getIngressNwEndpoint(epList []*endpoint) *endpoint {\n\tfor _, ep := range epList {\n\t\tn := ep.getNetwork()\n\t\tif n.ingress {\n\t\t\treturn ep\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc getLocalNwEndpoints(epList []*endpoint) []*endpoint {\n\teps := []*endpoint{}\n\tfor _, ep := range epList {\n\t\tn := ep.getNetwork()\n\t\tif !n.dynamic && !n.ingress {\n\t\t\teps = append(eps, ep)\n\t\t}\n\t}\n\treturn eps\n}\n\nfunc (sb *sandbox) ResolveName(name string, ipType int) ([]net.IP, bool) {\n\t// Embedded server owns the docker network domain. Resolution should work\n\t// for both container_name and container_name.network_name\n\t// We allow '.' in service name and network name. For a name a.b.c.d the\n\t// following have to tried;\n\t// {a.b.c.d in the networks container is connected to}\n\t// {a.b.c in network d},\n\t// {a.b in network c.d},\n\t// {a in network b.c.d},\n\n\tlogrus.Debugf(\"Name To resolve: %v\", name)\n\tname = strings.TrimSuffix(name, \".\")\n\treqName := []string{name}\n\tnetworkName := []string{\"\"}\n\n\tif strings.Contains(name, \".\") {\n\t\tvar i int\n\t\tdup := name\n\t\tfor {\n\t\t\tif i = strings.LastIndex(dup, \".\"); i == -1 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tnetworkName = append(networkName, name[i+1:])\n\t\t\treqName = append(reqName, name[:i])\n\n\t\t\tdup = dup[:i]\n\t\t}\n\t}\n\n\tepList := sb.getConnectedEndpoints()\n\n\t// In swarm mode services with exposed ports are connected to user overlay\n\t// network, ingress network and docker_gwbridge network. Name resolution\n\t// should prioritize returning the VIP/IPs on user overlay network.\n\tnewList := []*endpoint{}\n\tif !sb.controller.isDistributedControl() {\n\t\tnewList = append(newList, getDynamicNwEndpoints(epList)...)\n\t\tingressEP := getIngressNwEndpoint(epList)\n\t\tif ingressEP != nil {\n\t\t\tnewList = append(newList, ingressEP)\n\t\t}\n\t\tnewList = append(newList, getLocalNwEndpoints(epList)...)\n\t\tepList = newList\n\t}\n\n\tfor i := 0; i < len(reqName); i++ {\n\n\t\t// First check for local container alias\n\t\tip, ipv6Miss := sb.resolveName(reqName[i], networkName[i], epList, true, ipType)\n\t\tif ip != nil {\n\t\t\treturn ip, false\n\t\t}\n\t\tif ipv6Miss {\n\t\t\treturn ip, ipv6Miss\n\t\t}\n\n\t\t// Resolve the actual container name\n\t\tip, ipv6Miss = sb.resolveName(reqName[i], networkName[i], epList, false, ipType)\n\t\tif ip != nil {\n\t\t\treturn ip, false\n\t\t}\n\t\tif ipv6Miss {\n\t\t\treturn ip, ipv6Miss\n\t\t}\n\t}\n\treturn nil, false\n}\n\nfunc (sb *sandbox) resolveName(req string, networkName string, epList []*endpoint, alias bool, ipType int) ([]net.IP, bool) {\n\tvar ipv6Miss bool\n\n\tfor _, ep := range epList {\n\t\tname := req\n\t\tn := ep.getNetwork()\n\n\t\tif networkName != \"\" && networkName != n.Name() {\n\t\t\tcontinue\n\t\t}\n\n\t\tif alias {\n\t\t\tif ep.aliases == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar ok bool\n\t\t\tep.Lock()\n\t\t\tname, ok = ep.aliases[req]\n\t\t\tep.Unlock()\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\t// If it is a regular lookup and if the requested name is an alias\n\t\t\t// don't perform a svc lookup for this endpoint.\n\t\t\tep.Lock()\n\t\t\tif _, ok := ep.aliases[req]; ok {\n\t\t\t\tep.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tep.Unlock()\n\t\t}\n\n\t\tip, miss := n.ResolveName(name, ipType)\n\n\t\tif ip != nil {\n\t\t\treturn ip, false\n\t\t}\n\n\t\tif miss {\n\t\t\tipv6Miss = miss\n\t\t}\n\t}\n\treturn nil, ipv6Miss\n}\n\nfunc (sb *sandbox) SetKey(basePath string) error {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlogrus.Debugf(\"sandbox set key processing took %s for container %s\", time.Since(start), sb.ContainerID())\n\t}()\n\n\tif basePath == \"\" {\n\t\treturn types.BadRequestErrorf(\"invalid sandbox key\")\n\t}\n\n\tsb.Lock()\n\tif sb.inDelete {\n\t\tsb.Unlock()\n\t\treturn types.ForbiddenErrorf(\"failed to SetKey: sandbox %q delete in progress\", sb.id)\n\t}\n\toldosSbox := sb.osSbox\n\tsb.Unlock()\n\n\tif oldosSbox != nil {\n\t\t// If we already have an OS sandbox, release the network resources from that\n\t\t// and destroy the OS snab. We are moving into a new home further down. Note that none\n\t\t// of the network resources gets destroyed during the move.\n\t\tsb.releaseOSSbox()\n\t}\n\n\tosSbox, err := osl.GetSandboxForExternalKey(basePath, sb.Key())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsb.Lock()\n\tsb.osSbox = osSbox\n\tsb.Unlock()\n\n\t// If the resolver was setup before stop it and set it up in the\n\t// new osl sandbox.\n\tif oldosSbox != nil && sb.resolver != nil {\n\t\tsb.resolver.Stop()\n\n\t\tif err := sb.osSbox.InvokeFunc(sb.resolver.SetupFunc(0)); err == nil {\n\t\t\tif err := sb.resolver.Start(); err != nil {\n\t\t\t\tlogrus.Errorf(\"Resolver Start failed for container %s, %q\", sb.ContainerID(), err)\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.Errorf(\"Resolver Setup Function failed for container %s, %q\", sb.ContainerID(), err)\n\t\t}\n\t}\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif err = sb.populateNetworkResources(ep); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (sb *sandbox) EnableService() (err error) {\n\tlogrus.Debugf(\"EnableService %s START\", sb.containerID)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tsb.DisableService()\n\t\t}\n\t}()\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif !ep.isServiceEnabled() {\n\t\t\tif err := ep.addServiceInfoToCluster(sb); err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not update state for endpoint %s into cluster: %v\", ep.Name(), err)\n\t\t\t}\n\t\t\tep.enableService()\n\t\t}\n\t}\n\tlogrus.Debugf(\"EnableService %s DONE\", sb.containerID)\n\treturn nil\n}\n\nfunc (sb *sandbox) DisableService() (err error) {\n\tlogrus.Debugf(\"DisableService %s START\", sb.containerID)\n\tfailedEps := []string{}\n\tdefer func() {\n\t\tif len(failedEps) > 0 {\n\t\t\terr = fmt.Errorf(\"failed to disable service on sandbox:%s, for endpoints %s\", sb.ID(), strings.Join(failedEps, \",\"))\n\t\t}\n\t}()\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\tif ep.isServiceEnabled() {\n\t\t\tif err := ep.deleteServiceInfoFromCluster(sb, false, \"DisableService\"); err != nil {\n\t\t\t\tfailedEps = append(failedEps, ep.Name())\n\t\t\t\tlogrus.Warnf(\"failed update state for endpoint %s into cluster: %v\", ep.Name(), err)\n\t\t\t}\n\t\t\tep.disableService()\n\t\t}\n\t}\n\tlogrus.Debugf(\"DisableService %s DONE\", sb.containerID)\n\treturn nil\n}\n\nfunc releaseOSSboxResources(osSbox osl.Sandbox, ep *endpoint) {\n\tfor _, i := range osSbox.Info().Interfaces() {\n\t\t// Only remove the interfaces owned by this endpoint from the sandbox.\n\t\tif ep.hasInterface(i.SrcName()) {\n\t\t\tif err := i.Remove(); err != nil {\n\t\t\t\tlogrus.Debugf(\"Remove interface %s failed: %v\", i.SrcName(), err)\n\t\t\t}\n\t\t}\n\t}\n\n\tep.Lock()\n\tjoinInfo := ep.joinInfo\n\tvip := ep.virtualIP\n\tlbModeIsDSR := ep.network.loadBalancerMode == loadBalancerModeDSR\n\tep.Unlock()\n\n\tif len(vip) > 0 && lbModeIsDSR {\n\t\tipNet := &net.IPNet{IP: vip, Mask: net.CIDRMask(32, 32)}\n\t\tif err := osSbox.RemoveAliasIP(osSbox.GetLoopbackIfaceName(), ipNet); err != nil {\n\t\t\tlogrus.WithError(err).Debugf(\"failed to remove virtual ip %v to loopback\", ipNet)\n\t\t}\n\t}\n\n\tif joinInfo == nil {\n\t\treturn\n\t}\n\n\t// Remove non-interface routes.\n\tfor _, r := range joinInfo.StaticRoutes {\n\t\tif err := osSbox.RemoveStaticRoute(r); err != nil {\n\t\t\tlogrus.Debugf(\"Remove route failed: %v\", err)\n\t\t}\n\t}\n}\n\nfunc (sb *sandbox) releaseOSSbox() {\n\tsb.Lock()\n\tosSbox := sb.osSbox\n\tsb.osSbox = nil\n\tsb.Unlock()\n\n\tif osSbox == nil {\n\t\treturn\n\t}\n\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\treleaseOSSboxResources(osSbox, ep)\n\t}\n\n\tosSbox.Destroy()\n}\n\nfunc (sb *sandbox) restoreOslSandbox() error {\n\tvar routes []*types.StaticRoute\n\n\t// restore osl sandbox\n\tIfaces := make(map[string][]osl.IfaceOption)\n\tfor _, ep := range sb.endpoints {\n\t\tvar ifaceOptions []osl.IfaceOption\n\t\tep.Lock()\n\t\tjoinInfo := ep.joinInfo\n\t\ti := ep.iface\n\t\tep.Unlock()\n\n\t\tif i == nil {\n\t\t\tlogrus.Errorf(\"error restoring endpoint %s for container %s\", ep.Name(), sb.ContainerID())\n\t\t\tcontinue\n\t\t}\n\n\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().Address(i.addr), sb.osSbox.InterfaceOptions().Routes(i.routes))\n\t\tif i.addrv6 != nil && i.addrv6.IP.To16() != nil {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().AddressIPv6(i.addrv6))\n\t\t}\n\t\tif i.mac != nil {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().MacAddress(i.mac))\n\t\t}\n\t\tif len(i.llAddrs) != 0 {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().LinkLocalAddresses(i.llAddrs))\n\t\t}\n\t\tIfaces[fmt.Sprintf(\"%s+%s\", i.srcName, i.dstPrefix)] = ifaceOptions\n\t\tif joinInfo != nil {\n\t\t\troutes = append(routes, joinInfo.StaticRoutes...)\n\t\t}\n\t\tif ep.needResolver() {\n\t\t\tsb.startResolver(true)\n\t\t}\n\t}\n\n\tgwep := sb.getGatewayEndpoint()\n\tif gwep == nil {\n\t\treturn nil\n\t}\n\n\t// restore osl sandbox\n\terr := sb.osSbox.Restore(Ifaces, routes, gwep.joinInfo.gw, gwep.joinInfo.gw6)\n\treturn err\n}\n\nfunc (sb *sandbox) populateNetworkResources(ep *endpoint) error {\n\tsb.Lock()\n\tif sb.osSbox == nil {\n\t\tsb.Unlock()\n\t\treturn nil\n\t}\n\tinDelete := sb.inDelete\n\tsb.Unlock()\n\n\tep.Lock()\n\tjoinInfo := ep.joinInfo\n\ti := ep.iface\n\tlbModeIsDSR := ep.network.loadBalancerMode == loadBalancerModeDSR\n\tep.Unlock()\n\n\tif ep.needResolver() {\n\t\tsb.startResolver(false)\n\t}\n\n\tif i != nil && i.srcName != \"\" {\n\t\tvar ifaceOptions []osl.IfaceOption\n\n\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().Address(i.addr), sb.osSbox.InterfaceOptions().Routes(i.routes))\n\t\tif i.addrv6 != nil && i.addrv6.IP.To16() != nil {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().AddressIPv6(i.addrv6))\n\t\t}\n\t\tif len(i.llAddrs) != 0 {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().LinkLocalAddresses(i.llAddrs))\n\t\t}\n\t\tif i.mac != nil {\n\t\t\tifaceOptions = append(ifaceOptions, sb.osSbox.InterfaceOptions().MacAddress(i.mac))\n\t\t}\n\n\t\tif err := sb.osSbox.AddInterface(i.srcName, i.dstPrefix, ifaceOptions...); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to add interface %s to sandbox: %v\", i.srcName, err)\n\t\t}\n\n\t\tif len(ep.virtualIP) > 0 && lbModeIsDSR {\n\t\t\tif sb.loadBalancerNID == \"\" {\n\t\t\t\tif err := sb.osSbox.DisableARPForVIP(i.srcName); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed disable ARP for VIP: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tipNet := &net.IPNet{IP: ep.virtualIP, Mask: net.CIDRMask(32, 32)}\n\t\t\tif err := sb.osSbox.AddAliasIP(sb.osSbox.GetLoopbackIfaceName(), ipNet); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add virtual ip %v to loopback: %v\", ipNet, err)\n\t\t\t}\n\t\t}\n\t}\n\n\tif joinInfo != nil {\n\t\t// Set up non-interface routes.\n\t\tfor _, r := range joinInfo.StaticRoutes {\n\t\t\tif err := sb.osSbox.AddStaticRoute(r); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add static route %s: %v\", r.Destination.String(), err)\n\t\t\t}\n\t\t}\n\t}\n\n\tif ep == sb.getGatewayEndpoint() {\n\t\tif err := sb.updateGateway(ep); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Make sure to add the endpoint to the populated endpoint set\n\t// before populating loadbalancers.\n\tsb.Lock()\n\tsb.populatedEndpoints[ep.ID()] = struct{}{}\n\tsb.Unlock()\n\n\t// Populate load balancer only after updating all the other\n\t// information including gateway and other routes so that\n\t// loadbalancers are populated all the network state is in\n\t// place in the sandbox.\n\tsb.populateLoadBalancers(ep)\n\n\t// Only update the store if we did not come here as part of\n\t// sandbox delete. If we came here as part of delete then do\n\t// not bother updating the store. The sandbox object will be\n\t// deleted anyway\n\tif !inDelete {\n\t\treturn sb.storeUpdate()\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) clearNetworkResources(origEp *endpoint) error {\n\tep := sb.getEndpoint(origEp.id)\n\tif ep == nil {\n\t\treturn fmt.Errorf(\"could not find the sandbox endpoint data for endpoint %s\",\n\t\t\torigEp.id)\n\t}\n\n\tsb.Lock()\n\tosSbox := sb.osSbox\n\tinDelete := sb.inDelete\n\tsb.Unlock()\n\tif osSbox != nil {\n\t\treleaseOSSboxResources(osSbox, ep)\n\t}\n\n\tsb.Lock()\n\tdelete(sb.populatedEndpoints, ep.ID())\n\n\tif len(sb.endpoints) == 0 {\n\t\t// sb.endpoints should never be empty and this is unexpected error condition\n\t\t// We log an error message to note this down for debugging purposes.\n\t\tlogrus.Errorf(\"No endpoints in sandbox while trying to remove endpoint %s\", ep.Name())\n\t\tsb.Unlock()\n\t\treturn nil\n\t}\n\n\tvar (\n\t\tgwepBefore, gwepAfter *endpoint\n\t\tindex                 = -1\n\t)\n\tfor i, e := range sb.endpoints {\n\t\tif e == ep {\n\t\t\tindex = i\n\t\t}\n\t\tif len(e.Gateway()) > 0 && gwepBefore == nil {\n\t\t\tgwepBefore = e\n\t\t}\n\t\tif index != -1 && gwepBefore != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif index == -1 {\n\t\tlogrus.Warnf(\"Endpoint %s has already been deleted\", ep.Name())\n\t\tsb.Unlock()\n\t\treturn nil\n\t}\n\n\tsb.removeEndpointRaw(ep)\n\tfor _, e := range sb.endpoints {\n\t\tif len(e.Gateway()) > 0 {\n\t\t\tgwepAfter = e\n\t\t\tbreak\n\t\t}\n\t}\n\tdelete(sb.epPriority, ep.ID())\n\tsb.Unlock()\n\n\tif gwepAfter != nil && gwepBefore != gwepAfter {\n\t\tsb.updateGateway(gwepAfter)\n\t}\n\n\t// Only update the store if we did not come here as part of\n\t// sandbox delete. If we came here as part of delete then do\n\t// not bother updating the store. The sandbox object will be\n\t// deleted anyway\n\tif !inDelete {\n\t\treturn sb.storeUpdate()\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) isEndpointPopulated(ep *endpoint) bool {\n\tsb.Lock()\n\t_, ok := sb.populatedEndpoints[ep.ID()]\n\tsb.Unlock()\n\treturn ok\n}\n\n// joinLeaveStart waits to ensure there are no joins or leaves in progress and\n// marks this join/leave in progress without race\nfunc (sb *sandbox) joinLeaveStart() {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tfor sb.joinLeaveDone != nil {\n\t\tjoinLeaveDone := sb.joinLeaveDone\n\t\tsb.Unlock()\n\n\t\t<-joinLeaveDone\n\n\t\tsb.Lock()\n\t}\n\n\tsb.joinLeaveDone = make(chan struct{})\n}\n\n// joinLeaveEnd marks the end of this join/leave operation and\n// signals the same without race to other join and leave waiters\nfunc (sb *sandbox) joinLeaveEnd() {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\n\tif sb.joinLeaveDone != nil {\n\t\tclose(sb.joinLeaveDone)\n\t\tsb.joinLeaveDone = nil\n\t}\n}\n\nfunc (sb *sandbox) hasPortConfigs() bool {\n\topts := sb.Labels()\n\t_, hasExpPorts := opts[netlabel.ExposedPorts]\n\t_, hasPortMaps := opts[netlabel.PortMap]\n\treturn hasExpPorts || hasPortMaps\n}\n\n// OptionHostname function returns an option setter for hostname option to\n// be passed to NewSandbox method.\nfunc OptionHostname(name string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.hostName = name\n\t}\n}\n\n// OptionDomainname function returns an option setter for domainname option to\n// be passed to NewSandbox method.\nfunc OptionDomainname(name string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.domainName = name\n\t}\n}\n\n// OptionHostsPath function returns an option setter for hostspath option to\n// be passed to NewSandbox method.\nfunc OptionHostsPath(path string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.hostsPath = path\n\t}\n}\n\n// OptionOriginHostsPath function returns an option setter for origin hosts file path\n// to be passed to NewSandbox method.\nfunc OptionOriginHostsPath(path string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.originHostsPath = path\n\t}\n}\n\n// OptionExtraHost function returns an option setter for extra /etc/hosts options\n// which is a name and IP as strings.\nfunc OptionExtraHost(name string, IP string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.extraHosts = append(sb.config.extraHosts, extraHost{name: name, IP: IP})\n\t}\n}\n\n// OptionParentUpdate function returns an option setter for parent container\n// which needs to update the IP address for the linked container.\nfunc OptionParentUpdate(cid string, name, ip string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.parentUpdates = append(sb.config.parentUpdates, parentUpdate{cid: cid, name: name, ip: ip})\n\t}\n}\n\n// OptionResolvConfPath function returns an option setter for resolvconfpath option to\n// be passed to net container methods.\nfunc OptionResolvConfPath(path string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.resolvConfPath = path\n\t}\n}\n\n// OptionOriginResolvConfPath function returns an option setter to set the path to the\n// origin resolv.conf file to be passed to net container methods.\nfunc OptionOriginResolvConfPath(path string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.originResolvConfPath = path\n\t}\n}\n\n// OptionDNS function returns an option setter for dns entry option to\n// be passed to container Create method.\nfunc OptionDNS(dns string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.dnsList = append(sb.config.dnsList, dns)\n\t}\n}\n\n// OptionDNSSearch function returns an option setter for dns search entry option to\n// be passed to container Create method.\nfunc OptionDNSSearch(search string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.dnsSearchList = append(sb.config.dnsSearchList, search)\n\t}\n}\n\n// OptionDNSOptions function returns an option setter for dns options entry option to\n// be passed to container Create method.\nfunc OptionDNSOptions(options string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.dnsOptionsList = append(sb.config.dnsOptionsList, options)\n\t}\n}\n\n// OptionUseDefaultSandbox function returns an option setter for using default sandbox\n// (host namespace) to be passed to container Create method.\nfunc OptionUseDefaultSandbox() SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.useDefaultSandBox = true\n\t}\n}\n\n// OptionUseExternalKey function returns an option setter for using provided namespace\n// instead of creating one.\nfunc OptionUseExternalKey() SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.config.useExternalKey = true\n\t}\n}\n\n// OptionGeneric function returns an option setter for Generic configuration\n// that is not managed by libNetwork but can be used by the Drivers during the call to\n// net container creation method. Container Labels are a good example.\nfunc OptionGeneric(generic map[string]interface{}) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tif sb.config.generic == nil {\n\t\t\tsb.config.generic = make(map[string]interface{}, len(generic))\n\t\t}\n\t\tfor k, v := range generic {\n\t\t\tsb.config.generic[k] = v\n\t\t}\n\t}\n}\n\n// OptionExposedPorts function returns an option setter for the container exposed\n// ports option to be passed to container Create method.\nfunc OptionExposedPorts(exposedPorts []types.TransportPort) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tif sb.config.generic == nil {\n\t\t\tsb.config.generic = make(map[string]interface{})\n\t\t}\n\t\t// Defensive copy\n\t\teps := make([]types.TransportPort, len(exposedPorts))\n\t\tcopy(eps, exposedPorts)\n\t\t// Store endpoint label and in generic because driver needs it\n\t\tsb.config.exposedPorts = eps\n\t\tsb.config.generic[netlabel.ExposedPorts] = eps\n\t}\n}\n\n// OptionPortMapping function returns an option setter for the mapping\n// ports option to be passed to container Create method.\nfunc OptionPortMapping(portBindings []types.PortBinding) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tif sb.config.generic == nil {\n\t\t\tsb.config.generic = make(map[string]interface{})\n\t\t}\n\t\t// Store a copy of the bindings as generic data to pass to the driver\n\t\tpbs := make([]types.PortBinding, len(portBindings))\n\t\tcopy(pbs, portBindings)\n\t\tsb.config.generic[netlabel.PortMap] = pbs\n\t}\n}\n\n// OptionIngress function returns an option setter for marking a\n// sandbox as the controller's ingress sandbox.\nfunc OptionIngress() SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.ingress = true\n\t\tsb.oslTypes = append(sb.oslTypes, osl.SandboxTypeIngress)\n\t}\n}\n\n// OptionLoadBalancer function returns an option setter for marking a\n// sandbox as a load balancer sandbox.\nfunc OptionLoadBalancer(nid string) SandboxOption {\n\treturn func(sb *sandbox) {\n\t\tsb.loadBalancerNID = nid\n\t\tsb.oslTypes = append(sb.oslTypes, osl.SandboxTypeLoadBalancer)\n\t}\n}\n\n// <=> Returns true if a < b, false if a > b and advances to next level if a == b\n// epi.prio <=> epj.prio           # 2 < 1\n// epi.gw <=> epj.gw               # non-gw < gw\n// epi.internal <=> epj.internal   # non-internal < internal\n// epi.joininfo <=> epj.joininfo   # ipv6 < ipv4\n// epi.name <=> epj.name           # bar < foo\nfunc (epi *endpoint) Less(epj *endpoint) bool {\n\tvar (\n\t\tprioi, prioj int\n\t)\n\n\tsbi, _ := epi.getSandbox()\n\tsbj, _ := epj.getSandbox()\n\n\t// Prio defaults to 0\n\tif sbi != nil {\n\t\tprioi = sbi.epPriority[epi.ID()]\n\t}\n\tif sbj != nil {\n\t\tprioj = sbj.epPriority[epj.ID()]\n\t}\n\n\tif prioi != prioj {\n\t\treturn prioi > prioj\n\t}\n\n\tgwi := epi.endpointInGWNetwork()\n\tgwj := epj.endpointInGWNetwork()\n\tif gwi != gwj {\n\t\treturn gwj\n\t}\n\n\tinti := epi.getNetwork().Internal()\n\tintj := epj.getNetwork().Internal()\n\tif inti != intj {\n\t\treturn intj\n\t}\n\n\tjii := 0\n\tif epi.joinInfo != nil {\n\t\tif epi.joinInfo.gw != nil {\n\t\t\tjii = jii + 1\n\t\t}\n\t\tif epi.joinInfo.gw6 != nil {\n\t\t\tjii = jii + 2\n\t\t}\n\t}\n\n\tjij := 0\n\tif epj.joinInfo != nil {\n\t\tif epj.joinInfo.gw != nil {\n\t\t\tjij = jij + 1\n\t\t}\n\t\tif epj.joinInfo.gw6 != nil {\n\t\t\tjij = jij + 2\n\t\t}\n\t}\n\n\tif jii != jij {\n\t\treturn jii > jij\n\t}\n\n\treturn epi.network.Name() < epj.network.Name()\n}\n\nfunc (sb *sandbox) NdotsSet() bool {\n\treturn sb.ndotsSet\n}\n"
        },
        {
          "name": "sandbox_dns_unix.go",
          "type": "blob",
          "size": 12.7705078125,
          "content": "//go:build !windows\n// +build !windows\n\npackage libnetwork\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/docker/libnetwork/etchosts\"\n\t\"github.com/docker/libnetwork/resolvconf\"\n\t\"github.com/docker/libnetwork/resolvconf/dns\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst (\n\tdefaultPrefix = \"/var/lib/docker/network/files\"\n\tdirPerm       = 0755\n\tfilePerm      = 0644\n)\n\nfunc (sb *sandbox) startResolver(restore bool) {\n\tsb.resolverOnce.Do(func() {\n\t\tvar err error\n\t\tsb.resolver = NewResolver(resolverIPSandbox, true, sb.Key(), sb)\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tsb.resolver = nil\n\t\t\t}\n\t\t}()\n\n\t\t// In the case of live restore container is already running with\n\t\t// right resolv.conf contents created before. Just update the\n\t\t// external DNS servers from the restored sandbox for embedded\n\t\t// server to use.\n\t\tif !restore {\n\t\t\terr = sb.rebuildDNS()\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"Updating resolv.conf failed for container %s, %q\", sb.ContainerID(), err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tsb.resolver.SetExtServers(sb.extDNS)\n\n\t\tif err = sb.osSbox.InvokeFunc(sb.resolver.SetupFunc(0)); err != nil {\n\t\t\tlogrus.Errorf(\"Resolver Setup function failed for container %s, %q\", sb.ContainerID(), err)\n\t\t\treturn\n\t\t}\n\n\t\tif err = sb.resolver.Start(); err != nil {\n\t\t\tlogrus.Errorf(\"Resolver Start failed for container %s, %q\", sb.ContainerID(), err)\n\t\t}\n\t})\n}\n\nfunc (sb *sandbox) setupResolutionFiles() error {\n\tif err := sb.buildHostsFile(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := sb.updateParentHosts(); err != nil {\n\t\treturn err\n\t}\n\n\treturn sb.setupDNS()\n}\n\nfunc (sb *sandbox) buildHostsFile() error {\n\tif sb.config.hostsPath == \"\" {\n\t\tsb.config.hostsPath = defaultPrefix + \"/\" + sb.id + \"/hosts\"\n\t}\n\n\tdir, _ := filepath.Split(sb.config.hostsPath)\n\tif err := createBasePath(dir); err != nil {\n\t\treturn err\n\t}\n\n\t// This is for the host mode networking\n\tif sb.config.useDefaultSandBox && len(sb.config.extraHosts) == 0 {\n\t\t// We are working under the assumption that the origin file option had been properly expressed by the upper layer\n\t\t// if not here we are going to error out\n\t\tif err := copyFile(sb.config.originHostsPath, sb.config.hostsPath); err != nil && !os.IsNotExist(err) {\n\t\t\treturn types.InternalErrorf(\"could not copy source hosts file %s to %s: %v\", sb.config.originHostsPath, sb.config.hostsPath, err)\n\t\t}\n\t\treturn nil\n\t}\n\n\textraContent := make([]etchosts.Record, 0, len(sb.config.extraHosts))\n\tfor _, extraHost := range sb.config.extraHosts {\n\t\textraContent = append(extraContent, etchosts.Record{Hosts: extraHost.name, IP: extraHost.IP})\n\t}\n\n\treturn etchosts.Build(sb.config.hostsPath, \"\", sb.config.hostName, sb.config.domainName, extraContent)\n}\n\nfunc (sb *sandbox) updateHostsFile(ifaceIPs []string) error {\n\tif ifaceIPs == nil || len(ifaceIPs) == 0 {\n\t\treturn nil\n\t}\n\n\tif sb.config.originHostsPath != \"\" {\n\t\treturn nil\n\t}\n\n\t// User might have provided a FQDN in hostname or split it across hostname\n\t// and domainname.  We want the FQDN and the bare hostname.\n\tfqdn := sb.config.hostName\n\tmhost := sb.config.hostName\n\tif sb.config.domainName != \"\" {\n\t\tfqdn = fmt.Sprintf(\"%s.%s\", fqdn, sb.config.domainName)\n\t}\n\n\tparts := strings.SplitN(fqdn, \".\", 2)\n\tif len(parts) == 2 {\n\t\tmhost = fmt.Sprintf(\"%s %s\", fqdn, parts[0])\n\t}\n\n\tvar extraContent []etchosts.Record\n\tfor _, ip := range ifaceIPs {\n\t\textraContent = append(extraContent, etchosts.Record{Hosts: mhost, IP: ip})\n\t}\n\n\tsb.addHostsEntries(extraContent)\n\treturn nil\n}\n\nfunc (sb *sandbox) addHostsEntries(recs []etchosts.Record) {\n\tif err := etchosts.Add(sb.config.hostsPath, recs); err != nil {\n\t\tlogrus.Warnf(\"Failed adding service host entries to the running container: %v\", err)\n\t}\n}\n\nfunc (sb *sandbox) deleteHostsEntries(recs []etchosts.Record) {\n\tif err := etchosts.Delete(sb.config.hostsPath, recs); err != nil {\n\t\tlogrus.Warnf(\"Failed deleting service host entries to the running container: %v\", err)\n\t}\n}\n\nfunc (sb *sandbox) updateParentHosts() error {\n\tvar pSb Sandbox\n\n\tfor _, update := range sb.config.parentUpdates {\n\t\tsb.controller.WalkSandboxes(SandboxContainerWalker(&pSb, update.cid))\n\t\tif pSb == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif err := etchosts.Update(pSb.(*sandbox).config.hostsPath, update.ip, update.name); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) restorePath() {\n\tif sb.config.resolvConfPath == \"\" {\n\t\tsb.config.resolvConfPath = defaultPrefix + \"/\" + sb.id + \"/resolv.conf\"\n\t}\n\tsb.config.resolvConfHashFile = sb.config.resolvConfPath + \".hash\"\n\tif sb.config.hostsPath == \"\" {\n\t\tsb.config.hostsPath = defaultPrefix + \"/\" + sb.id + \"/hosts\"\n\t}\n}\n\nfunc (sb *sandbox) setExternalResolvers(content []byte, addrType int, checkLoopback bool) {\n\tservers := resolvconf.GetNameservers(content, addrType)\n\tfor _, ip := range servers {\n\t\thostLoopback := false\n\t\tif checkLoopback {\n\t\t\thostLoopback = dns.IsIPv4Localhost(ip)\n\t\t}\n\t\tsb.extDNS = append(sb.extDNS, extDNSEntry{\n\t\t\tIPStr:        ip,\n\t\t\tHostLoopback: hostLoopback,\n\t\t})\n\t}\n}\n\nfunc (sb *sandbox) setupDNS() error {\n\tvar newRC *resolvconf.File\n\n\tif sb.config.resolvConfPath == \"\" {\n\t\tsb.config.resolvConfPath = defaultPrefix + \"/\" + sb.id + \"/resolv.conf\"\n\t}\n\n\tsb.config.resolvConfHashFile = sb.config.resolvConfPath + \".hash\"\n\n\tdir, _ := filepath.Split(sb.config.resolvConfPath)\n\tif err := createBasePath(dir); err != nil {\n\t\treturn err\n\t}\n\n\t// When the user specify a conainter in the host namespace and do no have any dns option specified\n\t// we just copy the host resolv.conf from the host itself\n\tif sb.config.useDefaultSandBox &&\n\t\tlen(sb.config.dnsList) == 0 && len(sb.config.dnsSearchList) == 0 && len(sb.config.dnsOptionsList) == 0 {\n\n\t\t// We are working under the assumption that the origin file option had been properly expressed by the upper layer\n\t\t// if not here we are going to error out\n\t\tif err := copyFile(sb.config.originResolvConfPath, sb.config.resolvConfPath); err != nil {\n\t\t\tif !os.IsNotExist(err) {\n\t\t\t\treturn fmt.Errorf(\"could not copy source resolv.conf file %s to %s: %v\", sb.config.originResolvConfPath, sb.config.resolvConfPath, err)\n\t\t\t}\n\t\t\tlogrus.Infof(\"%s does not exist, we create an empty resolv.conf for container\", sb.config.originResolvConfPath)\n\t\t\tif err := createFile(sb.config.resolvConfPath); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\toriginResolvConfPath := sb.config.originResolvConfPath\n\tif originResolvConfPath == \"\" {\n\t\t// fallback if not specified\n\t\toriginResolvConfPath = resolvconf.Path()\n\t}\n\tcurrRC, err := resolvconf.GetSpecific(originResolvConfPath)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\t// it's ok to continue if /etc/resolv.conf doesn't exist, default resolvers (Google's Public DNS)\n\t\t// will be used\n\t\tcurrRC = &resolvconf.File{}\n\t\tlogrus.Infof(\"/etc/resolv.conf does not exist\")\n\t}\n\n\tif len(sb.config.dnsList) > 0 || len(sb.config.dnsSearchList) > 0 || len(sb.config.dnsOptionsList) > 0 {\n\t\tvar (\n\t\t\terr            error\n\t\t\tdnsList        = resolvconf.GetNameservers(currRC.Content, types.IP)\n\t\t\tdnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)\n\t\t\tdnsOptionsList = resolvconf.GetOptions(currRC.Content)\n\t\t)\n\t\tif len(sb.config.dnsList) > 0 {\n\t\t\tdnsList = sb.config.dnsList\n\t\t}\n\t\tif len(sb.config.dnsSearchList) > 0 {\n\t\t\tdnsSearchList = sb.config.dnsSearchList\n\t\t}\n\t\tif len(sb.config.dnsOptionsList) > 0 {\n\t\t\tdnsOptionsList = sb.config.dnsOptionsList\n\t\t}\n\t\tnewRC, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// After building the resolv.conf from the user config save the\n\t\t// external resolvers in the sandbox. Note that --dns 127.0.0.x\n\t\t// config refers to the loopback in the container namespace\n\t\tsb.setExternalResolvers(newRC.Content, types.IPv4, false)\n\t} else {\n\t\t// If the host resolv.conf file has 127.0.0.x container should\n\t\t// use the host resolver for queries. This is supported by the\n\t\t// docker embedded DNS server. Hence save the external resolvers\n\t\t// before filtering it out.\n\t\tsb.setExternalResolvers(currRC.Content, types.IPv4, true)\n\n\t\t// Replace any localhost/127.* (at this point we have no info about ipv6, pass it as true)\n\t\tif newRC, err = resolvconf.FilterResolvDNS(currRC.Content, true); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// No contention on container resolv.conf file at sandbox creation\n\t\tif err := ioutil.WriteFile(sb.config.resolvConfPath, newRC.Content, filePerm); err != nil {\n\t\t\treturn types.InternalErrorf(\"failed to write unhaltered resolv.conf file content when setting up dns for sandbox %s: %v\", sb.ID(), err)\n\t\t}\n\t}\n\n\t// Write hash\n\tif err := ioutil.WriteFile(sb.config.resolvConfHashFile, []byte(newRC.Hash), filePerm); err != nil {\n\t\treturn types.InternalErrorf(\"failed to write resolv.conf hash file when setting up dns for sandbox %s: %v\", sb.ID(), err)\n\t}\n\n\treturn nil\n}\n\nfunc (sb *sandbox) updateDNS(ipv6Enabled bool) error {\n\tvar (\n\t\tcurrHash string\n\t\thashFile = sb.config.resolvConfHashFile\n\t)\n\n\t// This is for the host mode networking\n\tif sb.config.useDefaultSandBox {\n\t\treturn nil\n\t}\n\n\tif len(sb.config.dnsList) > 0 || len(sb.config.dnsSearchList) > 0 || len(sb.config.dnsOptionsList) > 0 {\n\t\treturn nil\n\t}\n\n\tcurrRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\th, err := ioutil.ReadFile(hashFile)\n\t\tif err != nil {\n\t\t\tif !os.IsNotExist(err) {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tcurrHash = string(h)\n\t\t}\n\t}\n\n\tif currHash != \"\" && currHash != currRC.Hash {\n\t\t// Seems the user has changed the container resolv.conf since the last time\n\t\t// we checked so return without doing anything.\n\t\t//logrus.Infof(\"Skipping update of resolv.conf file with ipv6Enabled: %t because file was touched by user\", ipv6Enabled)\n\t\treturn nil\n\t}\n\n\t// replace any localhost/127.* and remove IPv6 nameservers if IPv6 disabled.\n\tnewRC, err := resolvconf.FilterResolvDNS(currRC.Content, ipv6Enabled)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = ioutil.WriteFile(sb.config.resolvConfPath, newRC.Content, 0644)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// write the new hash in a temp file and rename it to make the update atomic\n\tdir := path.Dir(sb.config.resolvConfPath)\n\ttmpHashFile, err := ioutil.TempFile(dir, \"hash\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = tmpHashFile.Chmod(filePerm); err != nil {\n\t\ttmpHashFile.Close()\n\t\treturn err\n\t}\n\t_, err = tmpHashFile.Write([]byte(newRC.Hash))\n\tif err1 := tmpHashFile.Close(); err == nil {\n\t\terr = err1\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tmpHashFile.Name(), hashFile)\n}\n\n// Embedded DNS server has to be enabled for this sandbox. Rebuild the container's\n// resolv.conf by doing the following\n// - Add only the embedded server's IP to container's resolv.conf\n// - If the embedded server needs any resolv.conf options add it to the current list\nfunc (sb *sandbox) rebuildDNS() error {\n\tcurrRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(sb.extDNS) == 0 {\n\t\tsb.setExternalResolvers(currRC.Content, types.IPv4, false)\n\t}\n\tvar (\n\t\tdnsList        = []string{sb.resolver.NameServer()}\n\t\tdnsOptionsList = resolvconf.GetOptions(currRC.Content)\n\t\tdnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)\n\t)\n\n\t// external v6 DNS servers has to be listed in resolv.conf\n\tdnsList = append(dnsList, resolvconf.GetNameservers(currRC.Content, types.IPv6)...)\n\n\t// If the user config and embedded DNS server both have ndots option set,\n\t// remember the user's config so that unqualified names not in the docker\n\t// domain can be dropped.\n\tresOptions := sb.resolver.ResolverOptions()\n\ndnsOpt:\n\tfor _, resOpt := range resOptions {\n\t\tif strings.Contains(resOpt, \"ndots\") {\n\t\t\tfor _, option := range dnsOptionsList {\n\t\t\t\tif strings.Contains(option, \"ndots\") {\n\t\t\t\t\tparts := strings.Split(option, \":\")\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid ndots option %v\", option)\n\t\t\t\t\t}\n\t\t\t\t\tif num, err := strconv.Atoi(parts[1]); err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid number for ndots option: %v\", parts[1])\n\t\t\t\t\t} else if num >= 0 {\n\t\t\t\t\t\t// if the user sets ndots, use the user setting\n\t\t\t\t\t\tsb.ndotsSet = true\n\t\t\t\t\t\tbreak dnsOpt\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid number for ndots option: %v\", num)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif !sb.ndotsSet {\n\t\t// if the user did not set the ndots, set it to 0 to prioritize the service name resolution\n\t\t// Ref: https://linux.die.net/man/5/resolv.conf\n\t\tdnsOptionsList = append(dnsOptionsList, resOptions...)\n\t}\n\n\t_, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)\n\treturn err\n}\n\nfunc createBasePath(dir string) error {\n\treturn os.MkdirAll(dir, dirPerm)\n}\n\nfunc createFile(path string) error {\n\tvar f *os.File\n\n\tdir, _ := filepath.Split(path)\n\terr := createBasePath(dir)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tf, err = os.Create(path)\n\tif err == nil {\n\t\tf.Close()\n\t}\n\n\treturn err\n}\n\nfunc copyFile(src, dst string) error {\n\tsBytes, err := ioutil.ReadFile(src)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn ioutil.WriteFile(dst, sBytes, filePerm)\n}\n"
        },
        {
          "name": "sandbox_dns_windows.go",
          "type": "blob",
          "size": 0.5732421875,
          "content": "//go:build windows\n// +build windows\n\npackage libnetwork\n\nimport (\n\t\"github.com/docker/libnetwork/etchosts\"\n)\n\n// Stub implementations for DNS related functions\n\nfunc (sb *sandbox) startResolver(bool) {\n}\n\nfunc (sb *sandbox) setupResolutionFiles() error {\n\treturn nil\n}\n\nfunc (sb *sandbox) restorePath() {\n}\n\nfunc (sb *sandbox) updateHostsFile(ifaceIP []string) error {\n\treturn nil\n}\n\nfunc (sb *sandbox) addHostsEntries(recs []etchosts.Record) {\n\n}\n\nfunc (sb *sandbox) deleteHostsEntries(recs []etchosts.Record) {\n\n}\n\nfunc (sb *sandbox) updateDNS(ipv6Enabled bool) error {\n\treturn nil\n}\n"
        },
        {
          "name": "sandbox_externalkey.go",
          "type": "blob",
          "size": 0.2041015625,
          "content": "package libnetwork\n\nimport \"github.com/docker/docker/pkg/reexec\"\n\ntype setKeyData struct {\n\tContainerID string\n\tKey         string\n}\n\nfunc init() {\n\treexec.Register(\"libnetwork-setkey\", processSetKeyReexec)\n}\n"
        },
        {
          "name": "sandbox_externalkey_unix.go",
          "type": "blob",
          "size": 4.484375,
          "content": "//go:build linux || freebsd\n// +build linux freebsd\n\npackage libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"github.com/opencontainers/runtime-spec/specs-go\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst (\n\texecSubdir      = \"libnetwork\"\n\tdefaultExecRoot = \"/run/docker\"\n\tsuccess         = \"success\"\n)\n\n// processSetKeyReexec is a private function that must be called only on an reexec path\n// It expects 3 args { [0] = \"libnetwork-setkey\", [1] = <container-id>, [2] = <short-controller-id> }\n// It also expects specs.State as a json string in <stdin>\n// Refer to https://github.com/opencontainers/runc/pull/160/ for more information\n// The docker exec-root can be specified as \"-exec-root\" flag. The default value is \"/run/docker\".\nfunc processSetKeyReexec() {\n\tvar err error\n\n\t// Return a failure to the calling process via ExitCode\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tlogrus.Fatalf(\"%v\", err)\n\t\t}\n\t}()\n\n\texecRoot := flag.String(\"exec-root\", defaultExecRoot, \"docker exec root\")\n\tflag.Parse()\n\n\t// expecting 3 os.Args {[0]=\"libnetwork-setkey\", [1]=<container-id>, [2]=<short-controller-id> }\n\t// (i.e. expecting 2 flag.Args())\n\targs := flag.Args()\n\tif len(args) < 2 {\n\t\terr = fmt.Errorf(\"Re-exec expects 2 args (after parsing flags), received : %d\", len(args))\n\t\treturn\n\t}\n\tcontainerID, shortCtlrID := args[0], args[1]\n\n\t// We expect specs.State as a json string in <stdin>\n\tstateBuf, err := ioutil.ReadAll(os.Stdin)\n\tif err != nil {\n\t\treturn\n\t}\n\tvar state specs.State\n\tif err = json.Unmarshal(stateBuf, &state); err != nil {\n\t\treturn\n\t}\n\n\terr = SetExternalKey(shortCtlrID, containerID, fmt.Sprintf(\"/proc/%d/ns/net\", state.Pid), *execRoot)\n}\n\n// SetExternalKey provides a convenient way to set an External key to a sandbox\nfunc SetExternalKey(shortCtlrID string, containerID string, key string, execRoot string) error {\n\tkeyData := setKeyData{\n\t\tContainerID: containerID,\n\t\tKey:         key}\n\n\tuds := filepath.Join(execRoot, execSubdir, shortCtlrID+\".sock\")\n\tc, err := net.Dial(\"unix\", uds)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer c.Close()\n\n\tif err = sendKey(c, keyData); err != nil {\n\t\treturn fmt.Errorf(\"sendKey failed with : %v\", err)\n\t}\n\treturn processReturn(c)\n}\n\nfunc sendKey(c net.Conn, data setKeyData) error {\n\tvar err error\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.Close()\n\t\t}\n\t}()\n\n\tvar b []byte\n\tif b, err = json.Marshal(data); err != nil {\n\t\treturn err\n\t}\n\n\t_, err = c.Write(b)\n\treturn err\n}\n\nfunc processReturn(r io.Reader) error {\n\tbuf := make([]byte, 1024)\n\tn, err := r.Read(buf[:])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to read buf in processReturn : %v\", err)\n\t}\n\tif string(buf[0:n]) != success {\n\t\treturn fmt.Errorf(string(buf[0:n]))\n\t}\n\treturn nil\n}\n\nfunc (c *controller) startExternalKeyListener() error {\n\texecRoot := defaultExecRoot\n\tif v := c.Config().Daemon.ExecRoot; v != \"\" {\n\t\texecRoot = v\n\t}\n\tudsBase := filepath.Join(execRoot, execSubdir)\n\tif err := os.MkdirAll(udsBase, 0600); err != nil {\n\t\treturn err\n\t}\n\tshortCtlrID := stringid.TruncateID(c.id)\n\tuds := filepath.Join(udsBase, shortCtlrID+\".sock\")\n\tl, err := net.Listen(\"unix\", uds)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := os.Chmod(uds, 0600); err != nil {\n\t\tl.Close()\n\t\treturn err\n\t}\n\tc.Lock()\n\tc.extKeyListener = l\n\tc.Unlock()\n\n\tgo c.acceptClientConnections(uds, l)\n\treturn nil\n}\n\nfunc (c *controller) acceptClientConnections(sock string, l net.Listener) {\n\tfor {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\tif _, err1 := os.Stat(sock); os.IsNotExist(err1) {\n\t\t\t\tlogrus.Debugf(\"Unix socket %s doesn't exist. cannot accept client connections\", sock)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlogrus.Errorf(\"Error accepting connection %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tgo func() {\n\t\t\tdefer conn.Close()\n\n\t\t\terr := c.processExternalKey(conn)\n\t\t\tret := success\n\t\t\tif err != nil {\n\t\t\t\tret = err.Error()\n\t\t\t}\n\n\t\t\t_, err = conn.Write([]byte(ret))\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"Error returning to the client %v\", err)\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc (c *controller) processExternalKey(conn net.Conn) error {\n\tbuf := make([]byte, 1280)\n\tnr, err := conn.Read(buf)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar s setKeyData\n\tif err = json.Unmarshal(buf[0:nr], &s); err != nil {\n\t\treturn err\n\t}\n\n\tvar sandbox Sandbox\n\tsearch := SandboxContainerWalker(&sandbox, s.ContainerID)\n\tc.WalkSandboxes(search)\n\tif sandbox == nil {\n\t\treturn types.BadRequestErrorf(\"no sandbox present for %s\", s.ContainerID)\n\t}\n\n\treturn sandbox.SetKey(s.Key)\n}\n\nfunc (c *controller) stopExternalKeyListener() {\n\tc.extKeyListener.Close()\n}\n"
        },
        {
          "name": "sandbox_externalkey_windows.go",
          "type": "blob",
          "size": 1.365234375,
          "content": "//go:build windows\n// +build windows\n\npackage libnetwork\n\nimport (\n\t\"io\"\n\t\"net\"\n\n\t\"github.com/docker/libnetwork/types\"\n)\n\n// processSetKeyReexec is a private function that must be called only on an reexec path\n// It expects 3 args { [0] = \"libnetwork-setkey\", [1] = <container-id>, [2] = <controller-id> }\n// It also expects configs.HookState as a json string in <stdin>\n// Refer to https://github.com/opencontainers/runc/pull/160/ for more information\nfunc processSetKeyReexec() {\n}\n\n// SetExternalKey provides a convenient way to set an External key to a sandbox\nfunc SetExternalKey(controllerID string, containerID string, key string) error {\n\treturn types.NotImplementedErrorf(\"SetExternalKey isn't supported on non linux systems\")\n}\n\nfunc sendKey(c net.Conn, data setKeyData) error {\n\treturn types.NotImplementedErrorf(\"sendKey isn't supported on non linux systems\")\n}\n\nfunc processReturn(r io.Reader) error {\n\treturn types.NotImplementedErrorf(\"processReturn isn't supported on non linux systems\")\n}\n\n// no-op on non linux systems\nfunc (c *controller) startExternalKeyListener() error {\n\treturn nil\n}\n\nfunc (c *controller) acceptClientConnections(sock string, l net.Listener) {\n}\n\nfunc (c *controller) processExternalKey(conn net.Conn) error {\n\treturn types.NotImplementedErrorf(\"processExternalKey isn't supported on non linux systems\")\n}\n\nfunc (c *controller) stopExternalKeyListener() {\n}\n"
        },
        {
          "name": "sandbox_store.go",
          "type": "blob",
          "size": 6.8623046875,
          "content": "package libnetwork\n\nimport (\n\t\"encoding/json\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst (\n\tsandboxPrefix = \"sandbox\"\n)\n\ntype epState struct {\n\tEid string\n\tNid string\n}\n\ntype sbState struct {\n\tID         string\n\tCid        string\n\tc          *controller\n\tdbIndex    uint64\n\tdbExists   bool\n\tEps        []epState\n\tEpPriority map[string]int\n\t// external servers have to be persisted so that on restart of a live-restore\n\t// enabled daemon we get the external servers for the running containers.\n\t// We have two versions of ExtDNS to support upgrade & downgrade of the daemon\n\t// between >=1.14 and <1.14 versions.\n\tExtDNS  []string\n\tExtDNS2 []extDNSEntry\n}\n\nfunc (sbs *sbState) Key() []string {\n\treturn []string{sandboxPrefix, sbs.ID}\n}\n\nfunc (sbs *sbState) KeyPrefix() []string {\n\treturn []string{sandboxPrefix}\n}\n\nfunc (sbs *sbState) Value() []byte {\n\tb, err := json.Marshal(sbs)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn b\n}\n\nfunc (sbs *sbState) SetValue(value []byte) error {\n\treturn json.Unmarshal(value, sbs)\n}\n\nfunc (sbs *sbState) Index() uint64 {\n\tsbi, err := sbs.c.SandboxByID(sbs.ID)\n\tif err != nil {\n\t\treturn sbs.dbIndex\n\t}\n\n\tsb := sbi.(*sandbox)\n\tmaxIndex := sb.dbIndex\n\tif sbs.dbIndex > maxIndex {\n\t\tmaxIndex = sbs.dbIndex\n\t}\n\n\treturn maxIndex\n}\n\nfunc (sbs *sbState) SetIndex(index uint64) {\n\tsbs.dbIndex = index\n\tsbs.dbExists = true\n\n\tsbi, err := sbs.c.SandboxByID(sbs.ID)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tsb := sbi.(*sandbox)\n\tsb.dbIndex = index\n\tsb.dbExists = true\n}\n\nfunc (sbs *sbState) Exists() bool {\n\tif sbs.dbExists {\n\t\treturn sbs.dbExists\n\t}\n\n\tsbi, err := sbs.c.SandboxByID(sbs.ID)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\tsb := sbi.(*sandbox)\n\treturn sb.dbExists\n}\n\nfunc (sbs *sbState) Skip() bool {\n\treturn false\n}\n\nfunc (sbs *sbState) New() datastore.KVObject {\n\treturn &sbState{c: sbs.c}\n}\n\nfunc (sbs *sbState) CopyTo(o datastore.KVObject) error {\n\tdstSbs := o.(*sbState)\n\tdstSbs.c = sbs.c\n\tdstSbs.ID = sbs.ID\n\tdstSbs.Cid = sbs.Cid\n\tdstSbs.dbIndex = sbs.dbIndex\n\tdstSbs.dbExists = sbs.dbExists\n\tdstSbs.EpPriority = sbs.EpPriority\n\n\tdstSbs.Eps = append(dstSbs.Eps, sbs.Eps...)\n\n\tif len(sbs.ExtDNS2) > 0 {\n\t\tfor _, dns := range sbs.ExtDNS2 {\n\t\t\tdstSbs.ExtDNS2 = append(dstSbs.ExtDNS2, dns)\n\t\t\tdstSbs.ExtDNS = append(dstSbs.ExtDNS, dns.IPStr)\n\t\t}\n\t\treturn nil\n\t}\n\tfor _, dns := range sbs.ExtDNS {\n\t\tdstSbs.ExtDNS = append(dstSbs.ExtDNS, dns)\n\t\tdstSbs.ExtDNS2 = append(dstSbs.ExtDNS2, extDNSEntry{IPStr: dns})\n\t}\n\n\treturn nil\n}\n\nfunc (sbs *sbState) DataScope() string {\n\treturn datastore.LocalScope\n}\n\nfunc (sb *sandbox) storeUpdate() error {\n\tsbs := &sbState{\n\t\tc:          sb.controller,\n\t\tID:         sb.id,\n\t\tCid:        sb.containerID,\n\t\tEpPriority: sb.epPriority,\n\t\tExtDNS2:    sb.extDNS,\n\t}\n\n\tfor _, ext := range sb.extDNS {\n\t\tsbs.ExtDNS = append(sbs.ExtDNS, ext.IPStr)\n\t}\n\nretry:\n\tsbs.Eps = nil\n\tfor _, ep := range sb.getConnectedEndpoints() {\n\t\t// If the endpoint is not persisted then do not add it to\n\t\t// the sandbox checkpoint\n\t\tif ep.Skip() {\n\t\t\tcontinue\n\t\t}\n\n\t\teps := epState{\n\t\t\tNid: ep.getNetwork().ID(),\n\t\t\tEid: ep.ID(),\n\t\t}\n\n\t\tsbs.Eps = append(sbs.Eps, eps)\n\t}\n\n\terr := sb.controller.updateToStore(sbs)\n\tif err == datastore.ErrKeyModified {\n\t\t// When we get ErrKeyModified it is sufficient to just\n\t\t// go back and retry.  No need to get the object from\n\t\t// the store because we always regenerate the store\n\t\t// state from in memory sandbox state\n\t\tgoto retry\n\t}\n\n\treturn err\n}\n\nfunc (sb *sandbox) storeDelete() error {\n\tsbs := &sbState{\n\t\tc:        sb.controller,\n\t\tID:       sb.id,\n\t\tCid:      sb.containerID,\n\t\tdbIndex:  sb.dbIndex,\n\t\tdbExists: sb.dbExists,\n\t}\n\n\treturn sb.controller.deleteFromStore(sbs)\n}\n\nfunc (c *controller) sandboxCleanup(activeSandboxes map[string]interface{}) {\n\tstore := c.getStore(datastore.LocalScope)\n\tif store == nil {\n\t\tlogrus.Error(\"Could not find local scope store while trying to cleanup sandboxes\")\n\t\treturn\n\t}\n\n\tkvol, err := store.List(datastore.Key(sandboxPrefix), &sbState{c: c})\n\tif err != nil && err != datastore.ErrKeyNotFound {\n\t\tlogrus.Errorf(\"failed to get sandboxes for scope %s: %v\", store.Scope(), err)\n\t\treturn\n\t}\n\n\t// It's normal for no sandboxes to be found. Just bail out.\n\tif err == datastore.ErrKeyNotFound {\n\t\treturn\n\t}\n\n\tfor _, kvo := range kvol {\n\t\tsbs := kvo.(*sbState)\n\n\t\tsb := &sandbox{\n\t\t\tid:                 sbs.ID,\n\t\t\tcontroller:         sbs.c,\n\t\t\tcontainerID:        sbs.Cid,\n\t\t\tendpoints:          []*endpoint{},\n\t\t\tpopulatedEndpoints: map[string]struct{}{},\n\t\t\tdbIndex:            sbs.dbIndex,\n\t\t\tisStub:             true,\n\t\t\tdbExists:           true,\n\t\t}\n\t\t// If we are restoring from a older version extDNSEntry won't have the\n\t\t// HostLoopback field\n\t\tif len(sbs.ExtDNS2) > 0 {\n\t\t\tsb.extDNS = sbs.ExtDNS2\n\t\t} else {\n\t\t\tfor _, dns := range sbs.ExtDNS {\n\t\t\t\tsb.extDNS = append(sb.extDNS, extDNSEntry{IPStr: dns})\n\t\t\t}\n\t\t}\n\n\t\tmsg := \" for cleanup\"\n\t\tcreate := true\n\t\tisRestore := false\n\t\tif val, ok := activeSandboxes[sb.ID()]; ok {\n\t\t\tmsg = \"\"\n\t\t\tsb.isStub = false\n\t\t\tisRestore = true\n\t\t\topts := val.([]SandboxOption)\n\t\t\tsb.processOptions(opts...)\n\t\t\tsb.restorePath()\n\t\t\tcreate = !sb.config.useDefaultSandBox\n\t\t}\n\t\tsb.osSbox, err = osl.NewSandbox(sb.Key(), create, isRestore)\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"failed to create osl sandbox while trying to restore sandbox %.7s%s: %v\", sb.ID(), msg, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tc.Lock()\n\t\tc.sandboxes[sb.id] = sb\n\t\tc.Unlock()\n\n\t\tfor _, eps := range sbs.Eps {\n\t\t\tn, err := c.getNetworkFromStore(eps.Nid)\n\t\t\tvar ep *endpoint\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"getNetworkFromStore for nid %s failed while trying to build sandbox for cleanup: %v\", eps.Nid, err)\n\t\t\t\tn = &network{id: eps.Nid, ctrlr: c, drvOnce: &sync.Once{}, persist: true}\n\t\t\t\tep = &endpoint{id: eps.Eid, network: n, sandboxID: sbs.ID}\n\t\t\t} else {\n\t\t\t\tep, err = n.getEndpointFromStore(eps.Eid)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Errorf(\"getEndpointFromStore for eid %s failed while trying to build sandbox for cleanup: %v\", eps.Eid, err)\n\t\t\t\t\tep = &endpoint{id: eps.Eid, network: n, sandboxID: sbs.ID}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif _, ok := activeSandboxes[sb.ID()]; ok && err != nil {\n\t\t\t\tlogrus.Errorf(\"failed to restore endpoint %s in %s for container %s due to %v\", eps.Eid, eps.Nid, sb.ContainerID(), err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsb.addEndpoint(ep)\n\t\t}\n\n\t\tif _, ok := activeSandboxes[sb.ID()]; !ok {\n\t\t\tlogrus.Infof(\"Removing stale sandbox %s (%s)\", sb.id, sb.containerID)\n\t\t\tif err := sb.delete(true); err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to delete sandbox %s while trying to cleanup: %v\", sb.id, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// reconstruct osl sandbox field\n\t\tif !sb.config.useDefaultSandBox {\n\t\t\tif err := sb.restoreOslSandbox(); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed to populate fields for osl sandbox %s\", sb.ID())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tc.sboxOnce.Do(func() {\n\t\t\t\tc.defOsSbox = sb.osSbox\n\t\t\t})\n\t\t}\n\n\t\tfor _, ep := range sb.endpoints {\n\t\t\t// Watch for service records\n\t\t\tif !c.isAgent() {\n\t\t\t\tc.watchSvcRecord(ep)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "sandbox_test.go",
          "type": "blob",
          "size": 6.0302734375,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/ipamapi\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n\t\"github.com/docker/libnetwork/osl\"\n\t\"github.com/docker/libnetwork/testutils\"\n)\n\nfunc getTestEnv(t *testing.T, opts ...[]NetworkOption) (NetworkController, []Network) {\n\tnetType := \"bridge\"\n\n\toption := options.Generic{\n\t\t\"EnableIPForwarding\": true,\n\t}\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = option\n\n\tcfgOptions, err := OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tc, err := New(append(cfgOptions, config.OptionDriverConfig(netType, genericOption))...)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(opts) == 0 {\n\t\treturn c, nil\n\t}\n\n\tnwList := make([]Network, 0, len(opts))\n\tfor i, opt := range opts {\n\t\tname := fmt.Sprintf(\"test_nw_%d\", i)\n\t\tnetOption := options.Generic{\n\t\t\tnetlabel.GenericData: options.Generic{\n\t\t\t\t\"BridgeName\": name,\n\t\t\t},\n\t\t}\n\t\tnewOptions := make([]NetworkOption, 1, len(opt)+1)\n\t\tnewOptions[0] = NetworkOptionGeneric(netOption)\n\t\tnewOptions = append(newOptions, opt...)\n\t\tn, err := c.NewNetwork(netType, name, \"\", newOptions...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tnwList = append(nwList, n)\n\t}\n\n\treturn c, nwList\n}\n\nfunc TestSandboxAddEmpty(t *testing.T) {\n\tc, _ := getTestEnv(t)\n\tctrlr := c.(*controller)\n\n\tsbx, err := ctrlr.NewSandbox(\"sandbox0\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := sbx.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(ctrlr.sandboxes) != 0 {\n\t\tt.Fatalf(\"controller sandboxes is not empty. len = %d\", len(ctrlr.sandboxes))\n\t}\n\n\tosl.GC()\n}\n\n// // If different priorities are specified, internal option and ipv6 addresses mustn't influence endpoint order\nfunc TestSandboxAddMultiPrio(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\topts := [][]NetworkOption{\n\t\t{NetworkOptionEnableIPv6(true), NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", nil, []*IpamConf{{PreferredPool: \"fe90::/64\"}}, nil)},\n\t\t{NetworkOptionInternalNetwork()},\n\t\t{},\n\t}\n\n\tc, nws := getTestEnv(t, opts...)\n\tctrlr := c.(*controller)\n\n\tsbx, err := ctrlr.NewSandbox(\"sandbox1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tsid := sbx.ID()\n\n\tep1, err := nws[0].CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tep2, err := nws[1].CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tep3, err := nws[2].CreateEndpoint(\"ep3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep1.Join(sbx, JoinOptionPriority(1)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep2.Join(sbx, JoinOptionPriority(2)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := ep3.Join(sbx, JoinOptionPriority(3)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != ep3.ID() {\n\t\tt.Fatal(\"Expected ep3 to be at the top of the heap. But did not find ep3 at the top of the heap\")\n\t}\n\n\tif len(sbx.Endpoints()) != 3 {\n\t\tt.Fatal(\"Expected 3 endpoints to be connected to the sandbox.\")\n\t}\n\n\tif err := ep3.Leave(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != ep2.ID() {\n\t\tt.Fatal(\"Expected ep2 to be at the top of the heap after removing ep3. But did not find ep2 at the top of the heap\")\n\t}\n\n\tif err := ep2.Leave(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != ep1.ID() {\n\t\tt.Fatal(\"Expected ep1 to be at the top of the heap after removing ep2. But did not find ep1 at the top of the heap\")\n\t}\n\n\t// Re-add ep3 back\n\tif err := ep3.Join(sbx, JoinOptionPriority(3)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != ep3.ID() {\n\t\tt.Fatal(\"Expected ep3 to be at the top of the heap after adding ep3 back. But did not find ep3 at the top of the heap\")\n\t}\n\n\tif err := sbx.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(ctrlr.sandboxes) != 0 {\n\t\tt.Fatalf(\"controller sandboxes is not empty. len = %d\", len(ctrlr.sandboxes))\n\t}\n\n\tosl.GC()\n}\n\nfunc TestSandboxAddSamePrio(t *testing.T) {\n\tif !testutils.IsRunningInContainer() {\n\t\tdefer testutils.SetupTestOSContext(t)()\n\t}\n\n\topts := [][]NetworkOption{\n\t\t{},\n\t\t{},\n\t\t{NetworkOptionEnableIPv6(true), NetworkOptionIpam(ipamapi.DefaultIPAM, \"\", nil, []*IpamConf{{PreferredPool: \"fe90::/64\"}}, nil)},\n\t\t{NetworkOptionInternalNetwork()},\n\t}\n\n\tc, nws := getTestEnv(t, opts...)\n\n\tctrlr := c.(*controller)\n\n\tsbx, err := ctrlr.NewSandbox(\"sandbox1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tsid := sbx.ID()\n\n\tepNw1, err := nws[1].CreateEndpoint(\"ep1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tepIPv6, err := nws[2].CreateEndpoint(\"ep2\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tepInternal, err := nws[3].CreateEndpoint(\"ep3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tepNw0, err := nws[0].CreateEndpoint(\"ep4\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := epNw1.Join(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := epIPv6.Join(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := epInternal.Join(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := epNw0.Join(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// order should now be: epIPv6, epNw0, epNw1, epInternal\n\tif len(sbx.Endpoints()) != 4 {\n\t\tt.Fatal(\"Expected 4 endpoints to be connected to the sandbox.\")\n\t}\n\n\t// IPv6 has precedence over IPv4\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != epIPv6.ID() {\n\t\tt.Fatal(\"Expected epIPv6 to be at the top of the heap. But did not find epIPv6 at the top of the heap\")\n\t}\n\n\t// internal network has lowest precedence\n\tif ctrlr.sandboxes[sid].endpoints[3].ID() != epInternal.ID() {\n\t\tt.Fatal(\"Expected epInternal to be at the bottom of the heap. But did not find epInternal at the bottom of the heap\")\n\t}\n\n\tif err := epIPv6.Leave(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// 'test_nw_0' has precedence over 'test_nw_1'\n\tif ctrlr.sandboxes[sid].endpoints[0].ID() != epNw0.ID() {\n\t\tt.Fatal(\"Expected epNw0 to be at the top of the heap after removing epIPv6. But did not find epNw0 at the top of the heap\")\n\t}\n\n\tif err := epNw1.Leave(sbx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := sbx.Delete(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(ctrlr.sandboxes) != 0 {\n\t\tt.Fatalf(\"controller containers is not empty. len = %d\", len(ctrlr.sandboxes))\n\t}\n\n\tosl.GC()\n}\n"
        },
        {
          "name": "service.go",
          "type": "blob",
          "size": 2.3408203125,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/internal/setmatrix\"\n)\n\nvar (\n\t// A global monotonic counter to assign firewall marks to\n\t// services.\n\tfwMarkCtr   uint32 = 256\n\tfwMarkCtrMu sync.Mutex\n)\n\ntype portConfigs []*PortConfig\n\nfunc (p portConfigs) String() string {\n\tif len(p) == 0 {\n\t\treturn \"\"\n\t}\n\n\tpc := p[0]\n\tstr := fmt.Sprintf(\"%d:%d/%s\", pc.PublishedPort, pc.TargetPort, PortConfig_Protocol_name[int32(pc.Protocol)])\n\tfor _, pc := range p[1:] {\n\t\tstr = str + fmt.Sprintf(\",%d:%d/%s\", pc.PublishedPort, pc.TargetPort, PortConfig_Protocol_name[int32(pc.Protocol)])\n\t}\n\n\treturn str\n}\n\ntype serviceKey struct {\n\tid    string\n\tports string\n}\n\ntype service struct {\n\tname string // Service Name\n\tid   string // Service ID\n\n\t// Map of loadbalancers for the service one-per attached\n\t// network. It is keyed with network ID.\n\tloadBalancers map[string]*loadBalancer\n\n\t// List of ingress ports exposed by the service\n\tingressPorts portConfigs\n\n\t// Service aliases\n\taliases []string\n\n\t// This maps tracks for each IP address the list of endpoints ID\n\t// associated with it. At stable state the endpoint ID expected is 1\n\t// but during transition and service change it is possible to have\n\t// temporary more than 1\n\tipToEndpoint setmatrix.SetMatrix\n\n\tdeleted bool\n\n\tsync.Mutex\n}\n\n// assignIPToEndpoint inserts the mapping between the IP and the endpoint identifier\n// returns true if the mapping was not present, false otherwise\n// returns also the number of endpoints associated to the IP\nfunc (s *service) assignIPToEndpoint(ip, eID string) (bool, int) {\n\treturn s.ipToEndpoint.Insert(ip, eID)\n}\n\n// removeIPToEndpoint removes the mapping between the IP and the endpoint identifier\n// returns true if the mapping was deleted, false otherwise\n// returns also the number of endpoints associated to the IP\nfunc (s *service) removeIPToEndpoint(ip, eID string) (bool, int) {\n\treturn s.ipToEndpoint.Remove(ip, eID)\n}\n\nfunc (s *service) printIPToEndpoint(ip string) (string, bool) {\n\treturn s.ipToEndpoint.String(ip)\n}\n\ntype lbBackend struct {\n\tip       net.IP\n\tdisabled bool\n}\n\ntype loadBalancer struct {\n\tvip    net.IP\n\tfwMark uint32\n\n\t// Map of backend IPs backing this loadbalancer on this\n\t// network. It is keyed with endpoint ID.\n\tbackEnds map[string]*lbBackend\n\n\t// Back pointer to service to which the loadbalancer belongs.\n\tservice *service\n\tsync.Mutex\n}\n"
        },
        {
          "name": "service_common.go",
          "type": "blob",
          "size": 12.6123046875,
          "content": "//go:build linux || windows\n// +build linux windows\n\npackage libnetwork\n\nimport (\n\t\"net\"\n\n\t\"github.com/docker/libnetwork/internal/setmatrix\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nconst maxSetStringLen = 350\n\nfunc (c *controller) addEndpointNameResolution(svcName, svcID, nID, eID, containerName string, vip net.IP, serviceAliases, taskAliases []string, ip net.IP, addService bool, method string) error {\n\tn, err := c.NetworkByID(nID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlogrus.Debugf(\"addEndpointNameResolution %s %s add_service:%t sAliases:%v tAliases:%v\", eID, svcName, addService, serviceAliases, taskAliases)\n\n\t// Add container resolution mappings\n\tc.addContainerNameResolution(nID, eID, containerName, taskAliases, ip, method)\n\n\tserviceID := svcID\n\tif serviceID == \"\" {\n\t\t// This is the case of a normal container not part of a service\n\t\tserviceID = eID\n\t}\n\n\t// Add endpoint IP to special \"tasks.svc_name\" so that the applications have access to DNS RR.\n\tn.(*network).addSvcRecords(eID, \"tasks.\"+svcName, serviceID, ip, nil, false, method)\n\tfor _, alias := range serviceAliases {\n\t\tn.(*network).addSvcRecords(eID, \"tasks.\"+alias, serviceID, ip, nil, false, method)\n\t}\n\n\t// Add service name to vip in DNS, if vip is valid. Otherwise resort to DNS RR\n\tif len(vip) == 0 {\n\t\tn.(*network).addSvcRecords(eID, svcName, serviceID, ip, nil, false, method)\n\t\tfor _, alias := range serviceAliases {\n\t\t\tn.(*network).addSvcRecords(eID, alias, serviceID, ip, nil, false, method)\n\t\t}\n\t}\n\n\tif addService && len(vip) != 0 {\n\t\tn.(*network).addSvcRecords(eID, svcName, serviceID, vip, nil, false, method)\n\t\tfor _, alias := range serviceAliases {\n\t\t\tn.(*network).addSvcRecords(eID, alias, serviceID, vip, nil, false, method)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) addContainerNameResolution(nID, eID, containerName string, taskAliases []string, ip net.IP, method string) error {\n\tn, err := c.NetworkByID(nID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.Debugf(\"addContainerNameResolution %s %s\", eID, containerName)\n\n\t// Add resolution for container name\n\tn.(*network).addSvcRecords(eID, containerName, eID, ip, nil, true, method)\n\n\t// Add resolution for taskaliases\n\tfor _, alias := range taskAliases {\n\t\tn.(*network).addSvcRecords(eID, alias, eID, ip, nil, false, method)\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) deleteEndpointNameResolution(svcName, svcID, nID, eID, containerName string, vip net.IP, serviceAliases, taskAliases []string, ip net.IP, rmService, multipleEntries bool, method string) error {\n\tn, err := c.NetworkByID(nID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlogrus.Debugf(\"deleteEndpointNameResolution %s %s rm_service:%t suppress:%t sAliases:%v tAliases:%v\", eID, svcName, rmService, multipleEntries, serviceAliases, taskAliases)\n\n\t// Delete container resolution mappings\n\tc.delContainerNameResolution(nID, eID, containerName, taskAliases, ip, method)\n\n\tserviceID := svcID\n\tif serviceID == \"\" {\n\t\t// This is the case of a normal container not part of a service\n\t\tserviceID = eID\n\t}\n\n\t// Delete the special \"tasks.svc_name\" backend record.\n\tif !multipleEntries {\n\t\tn.(*network).deleteSvcRecords(eID, \"tasks.\"+svcName, serviceID, ip, nil, false, method)\n\t\tfor _, alias := range serviceAliases {\n\t\t\tn.(*network).deleteSvcRecords(eID, \"tasks.\"+alias, serviceID, ip, nil, false, method)\n\t\t}\n\t}\n\n\t// If we are doing DNS RR delete the endpoint IP from DNS record right away.\n\tif !multipleEntries && len(vip) == 0 {\n\t\tn.(*network).deleteSvcRecords(eID, svcName, serviceID, ip, nil, false, method)\n\t\tfor _, alias := range serviceAliases {\n\t\t\tn.(*network).deleteSvcRecords(eID, alias, serviceID, ip, nil, false, method)\n\t\t}\n\t}\n\n\t// Remove the DNS record for VIP only if we are removing the service\n\tif rmService && len(vip) != 0 && !multipleEntries {\n\t\tn.(*network).deleteSvcRecords(eID, svcName, serviceID, vip, nil, false, method)\n\t\tfor _, alias := range serviceAliases {\n\t\t\tn.(*network).deleteSvcRecords(eID, alias, serviceID, vip, nil, false, method)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) delContainerNameResolution(nID, eID, containerName string, taskAliases []string, ip net.IP, method string) error {\n\tn, err := c.NetworkByID(nID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.Debugf(\"delContainerNameResolution %s %s\", eID, containerName)\n\n\t// Delete resolution for container name\n\tn.(*network).deleteSvcRecords(eID, containerName, eID, ip, nil, true, method)\n\n\t// Delete resolution for taskaliases\n\tfor _, alias := range taskAliases {\n\t\tn.(*network).deleteSvcRecords(eID, alias, eID, ip, nil, true, method)\n\t}\n\n\treturn nil\n}\n\nfunc newService(name string, id string, ingressPorts []*PortConfig, serviceAliases []string) *service {\n\treturn &service{\n\t\tname:          name,\n\t\tid:            id,\n\t\tingressPorts:  ingressPorts,\n\t\tloadBalancers: make(map[string]*loadBalancer),\n\t\taliases:       serviceAliases,\n\t\tipToEndpoint:  setmatrix.NewSetMatrix(),\n\t}\n}\n\nfunc (c *controller) getLBIndex(sid, nid string, ingressPorts []*PortConfig) int {\n\tskey := serviceKey{\n\t\tid:    sid,\n\t\tports: portConfigs(ingressPorts).String(),\n\t}\n\tc.Lock()\n\ts, ok := c.serviceBindings[skey]\n\tc.Unlock()\n\n\tif !ok {\n\t\treturn 0\n\t}\n\n\ts.Lock()\n\tlb := s.loadBalancers[nid]\n\ts.Unlock()\n\n\treturn int(lb.fwMark)\n}\n\n// cleanupServiceDiscovery when the network is being deleted, erase all the associated service discovery records\nfunc (c *controller) cleanupServiceDiscovery(cleanupNID string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tif cleanupNID == \"\" {\n\t\tlogrus.Debugf(\"cleanupServiceDiscovery for all networks\")\n\t\tc.svcRecords = make(map[string]svcInfo)\n\t\treturn\n\t}\n\tlogrus.Debugf(\"cleanupServiceDiscovery for network:%s\", cleanupNID)\n\tdelete(c.svcRecords, cleanupNID)\n}\n\nfunc (c *controller) cleanupServiceBindings(cleanupNID string) {\n\tvar cleanupFuncs []func()\n\n\tlogrus.Debugf(\"cleanupServiceBindings for %s\", cleanupNID)\n\tc.Lock()\n\tservices := make([]*service, 0, len(c.serviceBindings))\n\tfor _, s := range c.serviceBindings {\n\t\tservices = append(services, s)\n\t}\n\tc.Unlock()\n\n\tfor _, s := range services {\n\t\ts.Lock()\n\t\t// Skip the serviceBindings that got deleted\n\t\tif s.deleted {\n\t\t\ts.Unlock()\n\t\t\tcontinue\n\t\t}\n\t\tfor nid, lb := range s.loadBalancers {\n\t\t\tif cleanupNID != \"\" && nid != cleanupNID {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor eid, be := range lb.backEnds {\n\t\t\t\tcleanupFuncs = append(cleanupFuncs, makeServiceCleanupFunc(c, s, nid, eid, lb.vip, be.ip))\n\t\t\t}\n\t\t}\n\t\ts.Unlock()\n\t}\n\n\tfor _, f := range cleanupFuncs {\n\t\tf()\n\t}\n\n}\n\nfunc makeServiceCleanupFunc(c *controller, s *service, nID, eID string, vip net.IP, ip net.IP) func() {\n\t// ContainerName and taskAliases are not available here, this is still fine because the Service discovery\n\t// cleanup already happened before. The only thing that rmServiceBinding is still doing here a part from the Load\n\t// Balancer bookeeping, is to keep consistent the mapping of endpoint to IP.\n\treturn func() {\n\t\tif err := c.rmServiceBinding(s.name, s.id, nID, eID, \"\", vip, s.ingressPorts, s.aliases, []string{}, ip, \"cleanupServiceBindings\", false, true); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove service bindings for service %s network %s endpoint %s while cleanup: %v\", s.id, nID, eID, err)\n\t\t}\n\t}\n}\n\nfunc (c *controller) addServiceBinding(svcName, svcID, nID, eID, containerName string, vip net.IP, ingressPorts []*PortConfig, serviceAliases, taskAliases []string, ip net.IP, method string) error {\n\tvar addService bool\n\n\t// Failure to lock the network ID on add can result in racing\n\t// racing against network deletion resulting in inconsistent\n\t// state in the c.serviceBindings map and it's sub-maps. Also,\n\t// always lock network ID before services to avoid deadlock.\n\tc.networkLocker.Lock(nID)\n\tdefer c.networkLocker.Unlock(nID)\n\n\tn, err := c.NetworkByID(nID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tskey := serviceKey{\n\t\tid:    svcID,\n\t\tports: portConfigs(ingressPorts).String(),\n\t}\n\n\tvar s *service\n\tfor {\n\t\tc.Lock()\n\t\tvar ok bool\n\t\ts, ok = c.serviceBindings[skey]\n\t\tif !ok {\n\t\t\t// Create a new service if we are seeing this service\n\t\t\t// for the first time.\n\t\t\ts = newService(svcName, svcID, ingressPorts, serviceAliases)\n\t\t\tc.serviceBindings[skey] = s\n\t\t}\n\t\tc.Unlock()\n\t\ts.Lock()\n\t\tif !s.deleted {\n\t\t\t// ok the object is good to be used\n\t\t\tbreak\n\t\t}\n\t\ts.Unlock()\n\t}\n\tlogrus.Debugf(\"addServiceBinding from %s START for %s %s p:%p nid:%s skey:%v\", method, svcName, eID, s, nID, skey)\n\tdefer s.Unlock()\n\n\tlb, ok := s.loadBalancers[nID]\n\tif !ok {\n\t\t// Create a new load balancer if we are seeing this\n\t\t// network attachment on the service for the first\n\t\t// time.\n\t\tfwMarkCtrMu.Lock()\n\n\t\tlb = &loadBalancer{\n\t\t\tvip:      vip,\n\t\t\tfwMark:   fwMarkCtr,\n\t\t\tbackEnds: make(map[string]*lbBackend),\n\t\t\tservice:  s,\n\t\t}\n\n\t\tfwMarkCtr++\n\t\tfwMarkCtrMu.Unlock()\n\n\t\ts.loadBalancers[nID] = lb\n\t\taddService = true\n\t}\n\n\tlb.backEnds[eID] = &lbBackend{ip, false}\n\n\tok, entries := s.assignIPToEndpoint(ip.String(), eID)\n\tif !ok || entries > 1 {\n\t\tsetStr, b := s.printIPToEndpoint(ip.String())\n\t\tif len(setStr) > maxSetStringLen {\n\t\t\tsetStr = setStr[:maxSetStringLen]\n\t\t}\n\t\tlogrus.Warnf(\"addServiceBinding %s possible transient state ok:%t entries:%d set:%t %s\", eID, ok, entries, b, setStr)\n\t}\n\n\t// Add loadbalancer service and backend to the network\n\tn.(*network).addLBBackend(ip, lb)\n\n\t// Add the appropriate name resolutions\n\tc.addEndpointNameResolution(svcName, svcID, nID, eID, containerName, vip, serviceAliases, taskAliases, ip, addService, \"addServiceBinding\")\n\n\tlogrus.Debugf(\"addServiceBinding from %s END for %s %s\", method, svcName, eID)\n\n\treturn nil\n}\n\nfunc (c *controller) rmServiceBinding(svcName, svcID, nID, eID, containerName string, vip net.IP, ingressPorts []*PortConfig, serviceAliases []string, taskAliases []string, ip net.IP, method string, deleteSvcRecords bool, fullRemove bool) error {\n\n\tvar rmService bool\n\n\tskey := serviceKey{\n\t\tid:    svcID,\n\t\tports: portConfigs(ingressPorts).String(),\n\t}\n\n\tc.Lock()\n\ts, ok := c.serviceBindings[skey]\n\tc.Unlock()\n\tif !ok {\n\t\tlogrus.Warnf(\"rmServiceBinding %s %s %s aborted c.serviceBindings[skey] !ok\", method, svcName, eID)\n\t\treturn nil\n\t}\n\n\ts.Lock()\n\tdefer s.Unlock()\n\tlogrus.Debugf(\"rmServiceBinding from %s START for %s %s p:%p nid:%s sKey:%v deleteSvc:%t\", method, svcName, eID, s, nID, skey, deleteSvcRecords)\n\tlb, ok := s.loadBalancers[nID]\n\tif !ok {\n\t\tlogrus.Warnf(\"rmServiceBinding %s %s %s aborted s.loadBalancers[nid] !ok\", method, svcName, eID)\n\t\treturn nil\n\t}\n\n\tbe, ok := lb.backEnds[eID]\n\tif !ok {\n\t\tlogrus.Warnf(\"rmServiceBinding %s %s %s aborted lb.backEnds[eid] && lb.disabled[eid] !ok\", method, svcName, eID)\n\t\treturn nil\n\t}\n\n\tif fullRemove {\n\t\t// delete regardless\n\t\tdelete(lb.backEnds, eID)\n\t} else {\n\t\tbe.disabled = true\n\t}\n\n\tif len(lb.backEnds) == 0 {\n\t\t// All the backends for this service have been\n\t\t// removed. Time to remove the load balancer and also\n\t\t// remove the service entry in IPVS.\n\t\trmService = true\n\n\t\tdelete(s.loadBalancers, nID)\n\t\tlogrus.Debugf(\"rmServiceBinding %s delete %s, p:%p in loadbalancers len:%d\", eID, nID, lb, len(s.loadBalancers))\n\t}\n\n\tok, entries := s.removeIPToEndpoint(ip.String(), eID)\n\tif !ok || entries > 0 {\n\t\tsetStr, b := s.printIPToEndpoint(ip.String())\n\t\tif len(setStr) > maxSetStringLen {\n\t\t\tsetStr = setStr[:maxSetStringLen]\n\t\t}\n\t\tlogrus.Warnf(\"rmServiceBinding %s possible transient state ok:%t entries:%d set:%t %s\", eID, ok, entries, b, setStr)\n\t}\n\n\t// Remove loadbalancer service(if needed) and backend in all\n\t// sandboxes in the network only if the vip is valid.\n\tif entries == 0 {\n\t\t// The network may well have been deleted from the store (and\n\t\t// dataplane) before the last of the service bindings.  On Linux that's\n\t\t// ok because removing the network sandbox from the dataplane\n\t\t// implicitly cleans up all related dataplane state.\n\t\t// On the Windows dataplane, VFP policylists must be removed\n\t\t// independently of the network, and they must be removed before the HNS\n\t\t// network. Otherwise, policylist removal fails with \"network not\n\t\t// found.\" On Windows cleanupServiceBindings must be called prior to\n\t\t// removing the network from the store or dataplane.\n\t\tn, err := c.NetworkByID(nID)\n\t\tif err == nil {\n\t\t\tn.(*network).rmLBBackend(ip, lb, rmService, fullRemove)\n\t\t}\n\t}\n\n\t// Delete the name resolutions\n\tif deleteSvcRecords {\n\t\tc.deleteEndpointNameResolution(svcName, svcID, nID, eID, containerName, vip, serviceAliases, taskAliases, ip, rmService, entries > 0, \"rmServiceBinding\")\n\t}\n\n\tif len(s.loadBalancers) == 0 {\n\t\t// All loadbalancers for the service removed. Time to\n\t\t// remove the service itself.\n\t\tc.Lock()\n\n\t\t// Mark the object as deleted so that the add won't use it wrongly\n\t\ts.deleted = true\n\t\t// NOTE The delete from the serviceBindings map has to be the last operation else we are allowing a race between this service\n\t\t// that is getting deleted and a new service that will be created if the entry is not anymore there\n\t\tdelete(c.serviceBindings, skey)\n\t\tc.Unlock()\n\t}\n\n\tlogrus.Debugf(\"rmServiceBinding from %s END for %s %s\", method, svcName, eID)\n\treturn nil\n}\n"
        },
        {
          "name": "service_common_test.go",
          "type": "blob",
          "size": 3.6435546875,
          "content": "package libnetwork\n\nimport (\n\t\"net\"\n\t\"testing\"\n\n\t\"github.com/docker/libnetwork/resolvconf\"\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestCleanupServiceDiscovery(t *testing.T) {\n\tc, err := New()\n\tassert.NilError(t, err)\n\tdefer c.Stop()\n\n\tn1, err := c.NewNetwork(\"bridge\", \"net1\", \"\", nil)\n\tassert.NilError(t, err)\n\tdefer n1.Delete()\n\n\tn2, err := c.NewNetwork(\"bridge\", \"net2\", \"\", nil)\n\tassert.NilError(t, err)\n\tdefer n2.Delete()\n\n\tn1.(*network).addSvcRecords(\"N1ep1\", \"service_test\", \"serviceID1\", net.ParseIP(\"192.168.0.1\"), net.IP{}, true, \"test\")\n\tn1.(*network).addSvcRecords(\"N2ep2\", \"service_test\", \"serviceID2\", net.ParseIP(\"192.168.0.2\"), net.IP{}, true, \"test\")\n\n\tn2.(*network).addSvcRecords(\"N2ep1\", \"service_test\", \"serviceID1\", net.ParseIP(\"192.168.1.1\"), net.IP{}, true, \"test\")\n\tn2.(*network).addSvcRecords(\"N2ep2\", \"service_test\", \"serviceID2\", net.ParseIP(\"192.168.1.2\"), net.IP{}, true, \"test\")\n\n\tif len(c.(*controller).svcRecords) != 2 {\n\t\tt.Fatalf(\"Service record not added correctly:%v\", c.(*controller).svcRecords)\n\t}\n\n\t// cleanup net1\n\tc.(*controller).cleanupServiceDiscovery(n1.ID())\n\n\tif len(c.(*controller).svcRecords) != 1 {\n\t\tt.Fatalf(\"Service record not cleaned correctly:%v\", c.(*controller).svcRecords)\n\t}\n\n\tc.(*controller).cleanupServiceDiscovery(\"\")\n\n\tif len(c.(*controller).svcRecords) != 0 {\n\t\tt.Fatalf(\"Service record not cleaned correctly:%v\", c.(*controller).svcRecords)\n\t}\n}\n\nfunc TestDNSOptions(t *testing.T) {\n\tc, err := New()\n\tassert.NilError(t, err)\n\n\tsb, err := c.(*controller).NewSandbox(\"cnt1\", nil)\n\tassert.NilError(t, err)\n\tdefer sb.Delete()\n\tsb.(*sandbox).startResolver(false)\n\n\terr = sb.(*sandbox).setupDNS()\n\tassert.NilError(t, err)\n\terr = sb.(*sandbox).rebuildDNS()\n\tassert.NilError(t, err)\n\tcurrRC, err := resolvconf.GetSpecific(sb.(*sandbox).config.resolvConfPath)\n\tassert.NilError(t, err)\n\tdnsOptionsList := resolvconf.GetOptions(currRC.Content)\n\tassert.Check(t, is.Len(dnsOptionsList, 1))\n\tassert.Check(t, is.Equal(\"ndots:0\", dnsOptionsList[0]))\n\n\tsb.(*sandbox).config.dnsOptionsList = []string{\"ndots:5\"}\n\terr = sb.(*sandbox).setupDNS()\n\tassert.NilError(t, err)\n\tcurrRC, err = resolvconf.GetSpecific(sb.(*sandbox).config.resolvConfPath)\n\tassert.NilError(t, err)\n\tdnsOptionsList = resolvconf.GetOptions(currRC.Content)\n\tassert.Check(t, is.Len(dnsOptionsList, 1))\n\tassert.Check(t, is.Equal(\"ndots:5\", dnsOptionsList[0]))\n\n\terr = sb.(*sandbox).rebuildDNS()\n\tassert.NilError(t, err)\n\tcurrRC, err = resolvconf.GetSpecific(sb.(*sandbox).config.resolvConfPath)\n\tassert.NilError(t, err)\n\tdnsOptionsList = resolvconf.GetOptions(currRC.Content)\n\tassert.Check(t, is.Len(dnsOptionsList, 1))\n\tassert.Check(t, is.Equal(\"ndots:5\", dnsOptionsList[0]))\n\n\tsb2, err := c.(*controller).NewSandbox(\"cnt2\", nil)\n\tassert.NilError(t, err)\n\tdefer sb2.Delete()\n\tsb2.(*sandbox).startResolver(false)\n\n\tsb2.(*sandbox).config.dnsOptionsList = []string{\"ndots:0\"}\n\terr = sb2.(*sandbox).setupDNS()\n\tassert.NilError(t, err)\n\terr = sb2.(*sandbox).rebuildDNS()\n\tassert.NilError(t, err)\n\tcurrRC, err = resolvconf.GetSpecific(sb2.(*sandbox).config.resolvConfPath)\n\tassert.NilError(t, err)\n\tdnsOptionsList = resolvconf.GetOptions(currRC.Content)\n\tassert.Check(t, is.Len(dnsOptionsList, 1))\n\tassert.Check(t, is.Equal(\"ndots:0\", dnsOptionsList[0]))\n\n\tsb2.(*sandbox).config.dnsOptionsList = []string{\"ndots:foobar\"}\n\terr = sb2.(*sandbox).setupDNS()\n\tassert.NilError(t, err)\n\terr = sb2.(*sandbox).rebuildDNS()\n\tassert.Error(t, err, \"invalid number for ndots option: foobar\")\n\n\tsb2.(*sandbox).config.dnsOptionsList = []string{\"ndots:-1\"}\n\terr = sb2.(*sandbox).setupDNS()\n\tassert.NilError(t, err)\n\terr = sb2.(*sandbox).rebuildDNS()\n\tassert.Error(t, err, \"invalid number for ndots option: -1\")\n}\n"
        },
        {
          "name": "service_linux.go",
          "type": "blob",
          "size": 24.546875,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\n\t\"github.com/docker/docker/pkg/reexec\"\n\t\"github.com/docker/libnetwork/iptables\"\n\t\"github.com/docker/libnetwork/ns\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/ishidawataru/sctp\"\n\t\"github.com/moby/ipvs\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink/nl\"\n\t\"github.com/vishvananda/netns\"\n)\n\nfunc init() {\n\treexec.Register(\"fwmarker\", fwMarker)\n\treexec.Register(\"redirector\", redirector)\n}\n\n// Populate all loadbalancers on the network that the passed endpoint\n// belongs to, into this sandbox.\nfunc (sb *sandbox) populateLoadBalancers(ep *endpoint) {\n\t// This is an interface less endpoint. Nothing to do.\n\tif ep.Iface() == nil {\n\t\treturn\n\t}\n\n\tn := ep.getNetwork()\n\teIP := ep.Iface().Address()\n\n\tif n.ingress {\n\t\tif err := addRedirectRules(sb.Key(), eIP, ep.ingressPorts); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to add redirect rules for ep %s (%.7s): %v\", ep.Name(), ep.ID(), err)\n\t\t}\n\t}\n}\n\nfunc (n *network) findLBEndpointSandbox() (*endpoint, *sandbox, error) {\n\t// TODO: get endpoint from store?  See EndpointInfo()\n\tvar ep *endpoint\n\t// Find this node's LB sandbox endpoint:  there should be exactly one\n\tfor _, e := range n.Endpoints() {\n\t\tepi := e.Info()\n\t\tif epi != nil && epi.LoadBalancer() {\n\t\t\tep = e.(*endpoint)\n\t\t\tbreak\n\t\t}\n\t}\n\tif ep == nil {\n\t\treturn nil, nil, fmt.Errorf(\"Unable to find load balancing endpoint for network %s\", n.ID())\n\t}\n\t// Get the load balancer sandbox itself as well\n\tsb, ok := ep.getSandbox()\n\tif !ok {\n\t\treturn nil, nil, fmt.Errorf(\"Unable to get sandbox for %s(%s) in for %s\", ep.Name(), ep.ID(), n.ID())\n\t}\n\tvar sep *endpoint\n\tsep = sb.getEndpoint(ep.ID())\n\tif sep == nil {\n\t\treturn nil, nil, fmt.Errorf(\"Load balancing endpoint %s(%s) removed from %s\", ep.Name(), ep.ID(), n.ID())\n\t}\n\treturn sep, sb, nil\n}\n\n// Searches the OS sandbox for the name of the endpoint interface\n// within the sandbox.   This is required for adding/removing IP\n// aliases to the interface.\nfunc findIfaceDstName(sb *sandbox, ep *endpoint) string {\n\tsrcName := ep.Iface().SrcName()\n\tfor _, i := range sb.osSbox.Info().Interfaces() {\n\t\tif i.SrcName() == srcName {\n\t\t\treturn i.DstName()\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// Add loadbalancer backend to the loadbalncer sandbox for the network.\n// If needed add the service as well.\nfunc (n *network) addLBBackend(ip net.IP, lb *loadBalancer) {\n\tif len(lb.vip) == 0 {\n\t\treturn\n\t}\n\tep, sb, err := n.findLBEndpointSandbox()\n\tif err != nil {\n\t\tlogrus.Errorf(\"addLBBackend %s/%s: %v\", n.ID(), n.Name(), err)\n\t\treturn\n\t}\n\tif sb.osSbox == nil {\n\t\treturn\n\t}\n\n\teIP := ep.Iface().Address()\n\n\ti, err := ipvs.New(sb.Key())\n\tif err != nil {\n\t\tlogrus.Errorf(\"Failed to create an ipvs handle for sbox %.7s (%.7s,%s) for lb addition: %v\", sb.ID(), sb.ContainerID(), sb.Key(), err)\n\t\treturn\n\t}\n\tdefer i.Close()\n\n\ts := &ipvs.Service{\n\t\tAddressFamily: nl.FAMILY_V4,\n\t\tFWMark:        lb.fwMark,\n\t\tSchedName:     ipvs.RoundRobin,\n\t}\n\n\tif !i.IsServicePresent(s) {\n\t\t// Add IP alias for the VIP to the endpoint\n\t\tifName := findIfaceDstName(sb, ep)\n\t\tif ifName == \"\" {\n\t\t\tlogrus.Errorf(\"Failed find interface name for endpoint %s(%s) to create LB alias\", ep.ID(), ep.Name())\n\t\t\treturn\n\t\t}\n\t\terr := sb.osSbox.AddAliasIP(ifName, &net.IPNet{IP: lb.vip, Mask: net.CIDRMask(32, 32)})\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed add IP alias %s to network %s LB endpoint interface %s: %v\", lb.vip, n.ID(), ifName, err)\n\t\t\treturn\n\t\t}\n\n\t\tif sb.ingress {\n\t\t\tvar gwIP net.IP\n\t\t\tif ep := sb.getGatewayEndpoint(); ep != nil {\n\t\t\t\tgwIP = ep.Iface().Address().IP\n\t\t\t}\n\t\t\tif err := programIngress(gwIP, lb.service.ingressPorts, false); err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to add ingress: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tlogrus.Debugf(\"Creating service for vip %s fwMark %d ingressPorts %#v in sbox %.7s (%.7s)\", lb.vip, lb.fwMark, lb.service.ingressPorts, sb.ID(), sb.ContainerID())\n\t\tif err := invokeFWMarker(sb.Key(), lb.vip, lb.fwMark, lb.service.ingressPorts, eIP, false, n.loadBalancerMode); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to add firewall mark rule in sbox %.7s (%.7s): %v\", sb.ID(), sb.ContainerID(), err)\n\t\t\treturn\n\t\t}\n\n\t\tif err := i.NewService(s); err != nil && err != syscall.EEXIST {\n\t\t\tlogrus.Errorf(\"Failed to create a new service for vip %s fwmark %d in sbox %.7s (%.7s): %v\", lb.vip, lb.fwMark, sb.ID(), sb.ContainerID(), err)\n\t\t\treturn\n\t\t}\n\t}\n\n\td := &ipvs.Destination{\n\t\tAddressFamily: nl.FAMILY_V4,\n\t\tAddress:       ip,\n\t\tWeight:        1,\n\t}\n\tif n.loadBalancerMode == loadBalancerModeDSR {\n\t\td.ConnectionFlags = ipvs.ConnFwdDirectRoute\n\t}\n\n\t// Remove the sched name before using the service to add\n\t// destination.\n\ts.SchedName = \"\"\n\tif err := i.NewDestination(s, d); err != nil && err != syscall.EEXIST {\n\t\tlogrus.Errorf(\"Failed to create real server %s for vip %s fwmark %d in sbox %.7s (%.7s): %v\", ip, lb.vip, lb.fwMark, sb.ID(), sb.ContainerID(), err)\n\t}\n\n\t// Ensure that kernel tweaks are applied in case this is the first time\n\t// we've initialized ip_vs\n\tsb.osSbox.ApplyOSTweaks(sb.oslTypes)\n}\n\n// Remove loadbalancer backend the load balancing endpoint for this\n// network. If 'rmService' is true, then remove the service entry as well.\n// If 'fullRemove' is true then completely remove the entry, otherwise\n// just deweight it for now.\nfunc (n *network) rmLBBackend(ip net.IP, lb *loadBalancer, rmService bool, fullRemove bool) {\n\tif len(lb.vip) == 0 {\n\t\treturn\n\t}\n\tep, sb, err := n.findLBEndpointSandbox()\n\tif err != nil {\n\t\tlogrus.Debugf(\"rmLBBackend for %s/%s: %v -- probably transient state\", n.ID(), n.Name(), err)\n\t\treturn\n\t}\n\tif sb.osSbox == nil {\n\t\treturn\n\t}\n\n\teIP := ep.Iface().Address()\n\n\ti, err := ipvs.New(sb.Key())\n\tif err != nil {\n\t\tlogrus.Errorf(\"Failed to create an ipvs handle for sbox %.7s (%.7s,%s) for lb removal: %v\", sb.ID(), sb.ContainerID(), sb.Key(), err)\n\t\treturn\n\t}\n\tdefer i.Close()\n\n\ts := &ipvs.Service{\n\t\tAddressFamily: nl.FAMILY_V4,\n\t\tFWMark:        lb.fwMark,\n\t}\n\n\td := &ipvs.Destination{\n\t\tAddressFamily: nl.FAMILY_V4,\n\t\tAddress:       ip,\n\t\tWeight:        1,\n\t}\n\tif n.loadBalancerMode == loadBalancerModeDSR {\n\t\td.ConnectionFlags = ipvs.ConnFwdDirectRoute\n\t}\n\n\tif fullRemove {\n\t\tif err := i.DelDestination(s, d); err != nil && err != syscall.ENOENT {\n\t\t\tlogrus.Errorf(\"Failed to delete real server %s for vip %s fwmark %d in sbox %.7s (%.7s): %v\", ip, lb.vip, lb.fwMark, sb.ID(), sb.ContainerID(), err)\n\t\t}\n\t} else {\n\t\td.Weight = 0\n\t\tif err := i.UpdateDestination(s, d); err != nil && err != syscall.ENOENT {\n\t\t\tlogrus.Errorf(\"Failed to set LB weight of real server %s to 0 for vip %s fwmark %d in sbox %.7s (%.7s): %v\", ip, lb.vip, lb.fwMark, sb.ID(), sb.ContainerID(), err)\n\t\t}\n\t}\n\n\tif rmService {\n\t\ts.SchedName = ipvs.RoundRobin\n\t\tif err := i.DelService(s); err != nil && err != syscall.ENOENT {\n\t\t\tlogrus.Errorf(\"Failed to delete service for vip %s fwmark %d in sbox %.7s (%.7s): %v\", lb.vip, lb.fwMark, sb.ID(), sb.ContainerID(), err)\n\t\t}\n\n\t\tif sb.ingress {\n\t\t\tvar gwIP net.IP\n\t\t\tif ep := sb.getGatewayEndpoint(); ep != nil {\n\t\t\t\tgwIP = ep.Iface().Address().IP\n\t\t\t}\n\t\t\tif err := programIngress(gwIP, lb.service.ingressPorts, true); err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to delete ingress: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif err := invokeFWMarker(sb.Key(), lb.vip, lb.fwMark, lb.service.ingressPorts, eIP, true, n.loadBalancerMode); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to delete firewall mark rule in sbox %.7s (%.7s): %v\", sb.ID(), sb.ContainerID(), err)\n\t\t}\n\n\t\t// Remove IP alias from the VIP to the endpoint\n\t\tifName := findIfaceDstName(sb, ep)\n\t\tif ifName == \"\" {\n\t\t\tlogrus.Errorf(\"Failed find interface name for endpoint %s(%s) to create LB alias\", ep.ID(), ep.Name())\n\t\t\treturn\n\t\t}\n\t\terr := sb.osSbox.RemoveAliasIP(ifName, &net.IPNet{IP: lb.vip, Mask: net.CIDRMask(32, 32)})\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed add IP alias %s to network %s LB endpoint interface %s: %v\", lb.vip, n.ID(), ifName, err)\n\t\t}\n\t}\n}\n\nconst ingressChain = \"DOCKER-INGRESS\"\n\nvar (\n\tingressOnce     sync.Once\n\tingressMu       sync.Mutex // lock for operations on ingress\n\tingressProxyTbl = make(map[string]io.Closer)\n\tportConfigMu    sync.Mutex\n\tportConfigTbl   = make(map[PortConfig]int)\n)\n\nfunc filterPortConfigs(ingressPorts []*PortConfig, isDelete bool) []*PortConfig {\n\tportConfigMu.Lock()\n\tiPorts := make([]*PortConfig, 0, len(ingressPorts))\n\tfor _, pc := range ingressPorts {\n\t\tif isDelete {\n\t\t\tif cnt, ok := portConfigTbl[*pc]; ok {\n\t\t\t\t// This is the last reference to this\n\t\t\t\t// port config. Delete the port config\n\t\t\t\t// and add it to filtered list to be\n\t\t\t\t// plumbed.\n\t\t\t\tif cnt == 1 {\n\t\t\t\t\tdelete(portConfigTbl, *pc)\n\t\t\t\t\tiPorts = append(iPorts, pc)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tportConfigTbl[*pc] = cnt - 1\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\tif cnt, ok := portConfigTbl[*pc]; ok {\n\t\t\tportConfigTbl[*pc] = cnt + 1\n\t\t\tcontinue\n\t\t}\n\n\t\t// We are adding it for the first time. Add it to the\n\t\t// filter list to be plumbed.\n\t\tportConfigTbl[*pc] = 1\n\t\tiPorts = append(iPorts, pc)\n\t}\n\tportConfigMu.Unlock()\n\n\treturn iPorts\n}\n\nfunc programIngress(gwIP net.IP, ingressPorts []*PortConfig, isDelete bool) error {\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\n\taddDelOpt := \"-I\"\n\trollbackAddDelOpt := \"-D\"\n\tif isDelete {\n\t\taddDelOpt = \"-D\"\n\t\trollbackAddDelOpt = \"-I\"\n\t}\n\n\tingressMu.Lock()\n\tdefer ingressMu.Unlock()\n\n\tchainExists := iptable.ExistChain(ingressChain, iptables.Nat)\n\tfilterChainExists := iptable.ExistChain(ingressChain, iptables.Filter)\n\n\tingressOnce.Do(func() {\n\t\t// Flush nat table and filter table ingress chain rules during init if it\n\t\t// exists. It might contain stale rules from previous life.\n\t\tif chainExists {\n\t\t\tif err := iptable.RawCombinedOutput(\"-t\", \"nat\", \"-F\", ingressChain); err != nil {\n\t\t\t\tlogrus.Errorf(\"Could not flush nat table ingress chain rules during init: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif filterChainExists {\n\t\t\tif err := iptable.RawCombinedOutput(\"-F\", ingressChain); err != nil {\n\t\t\t\tlogrus.Errorf(\"Could not flush filter table ingress chain rules during init: %v\", err)\n\t\t\t}\n\t\t}\n\t})\n\n\tif !isDelete {\n\t\tif !chainExists {\n\t\t\tif err := iptable.RawCombinedOutput(\"-t\", \"nat\", \"-N\", ingressChain); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to create ingress chain: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif !filterChainExists {\n\t\t\tif err := iptable.RawCombinedOutput(\"-N\", ingressChain); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to create filter table ingress chain: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif !iptable.Exists(iptables.Nat, ingressChain, \"-j\", \"RETURN\") {\n\t\t\tif err := iptable.RawCombinedOutput(\"-t\", \"nat\", \"-A\", ingressChain, \"-j\", \"RETURN\"); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add return rule in nat table ingress chain: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif !iptable.Exists(iptables.Filter, ingressChain, \"-j\", \"RETURN\") {\n\t\t\tif err := iptable.RawCombinedOutput(\"-A\", ingressChain, \"-j\", \"RETURN\"); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add return rule to filter table ingress chain: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tfor _, chain := range []string{\"OUTPUT\", \"PREROUTING\"} {\n\t\t\tif !iptable.Exists(iptables.Nat, chain, \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\", \"-j\", ingressChain) {\n\t\t\t\tif err := iptable.RawCombinedOutput(\"-t\", \"nat\", \"-I\", chain, \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\", \"-j\", ingressChain); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to add jump rule in %s to ingress chain: %v\", chain, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif !iptable.Exists(iptables.Filter, \"FORWARD\", \"-j\", ingressChain) {\n\t\t\tif err := iptable.RawCombinedOutput(\"-I\", \"FORWARD\", \"-j\", ingressChain); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add jump rule to %s in filter table forward chain: %v\", ingressChain, err)\n\t\t\t}\n\t\t\tarrangeUserFilterRule()\n\t\t}\n\n\t\toifName, err := findOIFName(gwIP)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to find gateway bridge interface name for %s: %v\", gwIP, err)\n\t\t}\n\n\t\tpath := filepath.Join(\"/proc/sys/net/ipv4/conf\", oifName, \"route_localnet\")\n\t\tif err := ioutil.WriteFile(path, []byte{'1', '\\n'}, 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"could not write to %s: %v\", path, err)\n\t\t}\n\n\t\truleArgs := strings.Fields(fmt.Sprintf(\"-m addrtype --src-type LOCAL -o %s -j MASQUERADE\", oifName))\n\t\tif !iptable.Exists(iptables.Nat, \"POSTROUTING\", ruleArgs...) {\n\t\t\tif err := iptable.RawCombinedOutput(append([]string{\"-t\", \"nat\", \"-I\", \"POSTROUTING\"}, ruleArgs...)...); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to add ingress localhost POSTROUTING rule for %s: %v\", oifName, err)\n\t\t\t}\n\t\t}\n\t}\n\n\t//Filter the ingress ports until port rules start to be added/deleted\n\tfilteredPorts := filterPortConfigs(ingressPorts, isDelete)\n\trollbackRules := make([][]string, 0, len(filteredPorts)*3)\n\tvar portErr error\n\tdefer func() {\n\t\tif portErr != nil && !isDelete {\n\t\t\tfilterPortConfigs(filteredPorts, !isDelete)\n\t\t\tfor _, rule := range rollbackRules {\n\t\t\t\tif err := iptable.RawCombinedOutput(rule...); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"roll back rule failed, %v: %v\", rule, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor _, iPort := range filteredPorts {\n\t\tif iptable.ExistChain(ingressChain, iptables.Nat) {\n\t\t\trule := strings.Fields(fmt.Sprintf(\"-t nat %s %s -p %s --dport %d -j DNAT --to-destination %s:%d\",\n\t\t\t\taddDelOpt, ingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort, gwIP, iPort.PublishedPort))\n\t\t\tif portErr = iptable.RawCombinedOutput(rule...); portErr != nil {\n\t\t\t\terrStr := fmt.Sprintf(\"set up rule failed, %v: %v\", rule, portErr)\n\t\t\t\tif !isDelete {\n\t\t\t\t\treturn fmt.Errorf(\"%s\", errStr)\n\t\t\t\t}\n\t\t\t\tlogrus.Infof(\"%s\", errStr)\n\t\t\t}\n\t\t\trollbackRule := strings.Fields(fmt.Sprintf(\"-t nat %s %s -p %s --dport %d -j DNAT --to-destination %s:%d\", rollbackAddDelOpt,\n\t\t\t\tingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort, gwIP, iPort.PublishedPort))\n\t\t\trollbackRules = append(rollbackRules, rollbackRule)\n\t\t}\n\n\t\t// Filter table rules to allow a published service to be accessible in the local node from..\n\t\t// 1) service tasks attached to other networks\n\t\t// 2) unmanaged containers on bridge networks\n\t\trule := strings.Fields(fmt.Sprintf(\"%s %s -m state -p %s --sport %d --state ESTABLISHED,RELATED -j ACCEPT\",\n\t\t\taddDelOpt, ingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort))\n\t\tif portErr = iptable.RawCombinedOutput(rule...); portErr != nil {\n\t\t\terrStr := fmt.Sprintf(\"set up rule failed, %v: %v\", rule, portErr)\n\t\t\tif !isDelete {\n\t\t\t\treturn fmt.Errorf(\"%s\", errStr)\n\t\t\t}\n\t\t\tlogrus.Warnf(\"%s\", errStr)\n\t\t}\n\t\trollbackRule := strings.Fields(fmt.Sprintf(\"%s %s -m state -p %s --sport %d --state ESTABLISHED,RELATED -j ACCEPT\", rollbackAddDelOpt,\n\t\t\tingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort))\n\t\trollbackRules = append(rollbackRules, rollbackRule)\n\n\t\trule = strings.Fields(fmt.Sprintf(\"%s %s -p %s --dport %d -j ACCEPT\",\n\t\t\taddDelOpt, ingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort))\n\t\tif portErr = iptable.RawCombinedOutput(rule...); portErr != nil {\n\t\t\terrStr := fmt.Sprintf(\"set up rule failed, %v: %v\", rule, portErr)\n\t\t\tif !isDelete {\n\t\t\t\treturn fmt.Errorf(\"%s\", errStr)\n\t\t\t}\n\t\t\tlogrus.Warnf(\"%s\", errStr)\n\t\t}\n\t\trollbackRule = strings.Fields(fmt.Sprintf(\"%s %s -p %s --dport %d -j ACCEPT\", rollbackAddDelOpt,\n\t\t\tingressChain, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort))\n\t\trollbackRules = append(rollbackRules, rollbackRule)\n\n\t\tif err := plumbProxy(iPort, isDelete); err != nil {\n\t\t\tlogrus.Warnf(\"failed to create proxy for port %d: %v\", iPort.PublishedPort, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// In the filter table FORWARD chain the first rule should be to jump to\n// DOCKER-USER so the user is able to filter packet first.\n// The second rule should be jump to INGRESS-CHAIN.\n// This chain has the rules to allow access to the published ports for swarm tasks\n// from local bridge networks and docker_gwbridge (ie:taks on other swarm networks)\nfunc arrangeIngressFilterRule() {\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\tif iptable.ExistChain(ingressChain, iptables.Filter) {\n\t\tif iptable.Exists(iptables.Filter, \"FORWARD\", \"-j\", ingressChain) {\n\t\t\tif err := iptable.RawCombinedOutput(\"-D\", \"FORWARD\", \"-j\", ingressChain); err != nil {\n\t\t\t\tlogrus.Warnf(\"failed to delete jump rule to ingressChain in filter table: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif err := iptable.RawCombinedOutput(\"-I\", \"FORWARD\", \"-j\", ingressChain); err != nil {\n\t\t\tlogrus.Warnf(\"failed to add jump rule to ingressChain in filter table: %v\", err)\n\t\t}\n\t}\n}\n\nfunc findOIFName(ip net.IP) (string, error) {\n\tnlh := ns.NlHandle()\n\n\troutes, err := nlh.RouteGet(ip)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif len(routes) == 0 {\n\t\treturn \"\", fmt.Errorf(\"no route to %s\", ip)\n\t}\n\n\t// Pick the first route(typically there is only one route). We\n\t// don't support multipath.\n\tlink, err := nlh.LinkByIndex(routes[0].LinkIndex)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn link.Attrs().Name, nil\n}\n\nfunc plumbProxy(iPort *PortConfig, isDelete bool) error {\n\tvar (\n\t\terr error\n\t\tl   io.Closer\n\t)\n\n\tportSpec := fmt.Sprintf(\"%d/%s\", iPort.PublishedPort, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]))\n\tif isDelete {\n\t\tif listener, ok := ingressProxyTbl[portSpec]; ok {\n\t\t\tif listener != nil {\n\t\t\t\tlistener.Close()\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tswitch iPort.Protocol {\n\tcase ProtocolTCP:\n\t\tl, err = net.ListenTCP(\"tcp\", &net.TCPAddr{Port: int(iPort.PublishedPort)})\n\tcase ProtocolUDP:\n\t\tl, err = net.ListenUDP(\"udp\", &net.UDPAddr{Port: int(iPort.PublishedPort)})\n\tcase ProtocolSCTP:\n\t\tl, err = sctp.ListenSCTP(\"sctp\", &sctp.SCTPAddr{Port: int(iPort.PublishedPort)})\n\tdefault:\n\t\terr = fmt.Errorf(\"unknown protocol %v\", iPort.Protocol)\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tingressProxyTbl[portSpec] = l\n\n\treturn nil\n}\n\nfunc writePortsToFile(ports []*PortConfig) (string, error) {\n\tf, err := ioutil.TempFile(\"\", \"port_configs\")\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer f.Close()\n\n\tbuf, _ := proto.Marshal(&EndpointRecord{\n\t\tIngressPorts: ports,\n\t})\n\n\tn, err := f.Write(buf)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif n < len(buf) {\n\t\treturn \"\", io.ErrShortWrite\n\t}\n\n\treturn f.Name(), nil\n}\n\nfunc readPortsFromFile(fileName string) ([]*PortConfig, error) {\n\tbuf, err := ioutil.ReadFile(fileName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar epRec EndpointRecord\n\terr = proto.Unmarshal(buf, &epRec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn epRec.IngressPorts, nil\n}\n\n// Invoke fwmarker reexec routine to mark vip destined packets with\n// the passed firewall mark.\nfunc invokeFWMarker(path string, vip net.IP, fwMark uint32, ingressPorts []*PortConfig, eIP *net.IPNet, isDelete bool, lbMode string) error {\n\tvar ingressPortsFile string\n\n\tif len(ingressPorts) != 0 {\n\t\tvar err error\n\t\tingressPortsFile, err = writePortsToFile(ingressPorts)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdefer os.Remove(ingressPortsFile)\n\t}\n\n\taddDelOpt := \"-A\"\n\tif isDelete {\n\t\taddDelOpt = \"-D\"\n\t}\n\n\tcmd := &exec.Cmd{\n\t\tPath:   reexec.Self(),\n\t\tArgs:   append([]string{\"fwmarker\"}, path, vip.String(), fmt.Sprintf(\"%d\", fwMark), addDelOpt, ingressPortsFile, eIP.String(), lbMode),\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n\n\tif err := cmd.Run(); err != nil {\n\t\treturn fmt.Errorf(\"reexec failed: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// Firewall marker reexec function.\nfunc fwMarker() {\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\tif len(os.Args) < 8 {\n\t\tlogrus.Error(\"invalid number of arguments..\")\n\t\tos.Exit(1)\n\t}\n\n\tvar ingressPorts []*PortConfig\n\tif os.Args[5] != \"\" {\n\t\tvar err error\n\t\tingressPorts, err = readPortsFromFile(os.Args[5])\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed reading ingress ports file: %v\", err)\n\t\t\tos.Exit(2)\n\t\t}\n\t}\n\n\tvip := os.Args[2]\n\tfwMark, err := strconv.ParseUint(os.Args[3], 10, 32)\n\tif err != nil {\n\t\tlogrus.Errorf(\"bad fwmark value(%s) passed: %v\", os.Args[3], err)\n\t\tos.Exit(3)\n\t}\n\taddDelOpt := os.Args[4]\n\n\trules := [][]string{}\n\tfor _, iPort := range ingressPorts {\n\t\trule := strings.Fields(fmt.Sprintf(\"-t mangle %s PREROUTING -p %s --dport %d -j MARK --set-mark %d\",\n\t\t\taddDelOpt, strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort, fwMark))\n\t\trules = append(rules, rule)\n\t}\n\n\tns, err := netns.GetFromPath(os.Args[1])\n\tif err != nil {\n\t\tlogrus.Errorf(\"failed get network namespace %q: %v\", os.Args[1], err)\n\t\tos.Exit(4)\n\t}\n\tdefer ns.Close()\n\n\tif err := netns.Set(ns); err != nil {\n\t\tlogrus.Errorf(\"setting into container net ns %v failed, %v\", os.Args[1], err)\n\t\tos.Exit(5)\n\t}\n\n\tlbMode := os.Args[7]\n\tif addDelOpt == \"-A\" && lbMode == loadBalancerModeNAT {\n\t\teIP, subnet, err := net.ParseCIDR(os.Args[6])\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to parse endpoint IP %s: %v\", os.Args[6], err)\n\t\t\tos.Exit(6)\n\t\t}\n\n\t\truleParams := strings.Fields(fmt.Sprintf(\"-m ipvs --ipvs -d %s -j SNAT --to-source %s\", subnet, eIP))\n\t\tif !iptable.Exists(\"nat\", \"POSTROUTING\", ruleParams...) {\n\t\t\trule := append(strings.Fields(\"-t nat -A POSTROUTING\"), ruleParams...)\n\t\t\trules = append(rules, rule)\n\n\t\t\terr := ioutil.WriteFile(\"/proc/sys/net/ipv4/vs/conntrack\", []byte{'1', '\\n'}, 0644)\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to write to /proc/sys/net/ipv4/vs/conntrack: %v\", err)\n\t\t\t\tos.Exit(7)\n\t\t\t}\n\t\t}\n\t}\n\n\trule := strings.Fields(fmt.Sprintf(\"-t mangle %s INPUT -d %s/32 -j MARK --set-mark %d\", addDelOpt, vip, fwMark))\n\trules = append(rules, rule)\n\n\tfor _, rule := range rules {\n\t\tif err := iptable.RawCombinedOutputNative(rule...); err != nil {\n\t\t\tlogrus.Errorf(\"set up rule failed, %v: %v\", rule, err)\n\t\t\tos.Exit(8)\n\t\t}\n\t}\n}\n\nfunc addRedirectRules(path string, eIP *net.IPNet, ingressPorts []*PortConfig) error {\n\tvar ingressPortsFile string\n\n\tif len(ingressPorts) != 0 {\n\t\tvar err error\n\t\tingressPortsFile, err = writePortsToFile(ingressPorts)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer os.Remove(ingressPortsFile)\n\t}\n\n\tcmd := &exec.Cmd{\n\t\tPath:   reexec.Self(),\n\t\tArgs:   append([]string{\"redirector\"}, path, eIP.String(), ingressPortsFile),\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n\n\tif err := cmd.Run(); err != nil {\n\t\treturn fmt.Errorf(\"reexec failed: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// Redirector reexec function.\nfunc redirector() {\n\t// TODO IPv6 support\n\tiptable := iptables.GetIptable(iptables.IPv4)\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\tif len(os.Args) < 4 {\n\t\tlogrus.Error(\"invalid number of arguments..\")\n\t\tos.Exit(1)\n\t}\n\n\tvar ingressPorts []*PortConfig\n\tif os.Args[3] != \"\" {\n\t\tvar err error\n\t\tingressPorts, err = readPortsFromFile(os.Args[3])\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed reading ingress ports file: %v\", err)\n\t\t\tos.Exit(2)\n\t\t}\n\t}\n\n\teIP, _, err := net.ParseCIDR(os.Args[2])\n\tif err != nil {\n\t\tlogrus.Errorf(\"Failed to parse endpoint IP %s: %v\", os.Args[2], err)\n\t\tos.Exit(3)\n\t}\n\n\trules := [][]string{}\n\tfor _, iPort := range ingressPorts {\n\t\trule := strings.Fields(fmt.Sprintf(\"-t nat -A PREROUTING -d %s -p %s --dport %d -j REDIRECT --to-port %d\",\n\t\t\teIP.String(), strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.PublishedPort, iPort.TargetPort))\n\t\trules = append(rules, rule)\n\t\t// Allow only incoming connections to exposed ports\n\t\tiRule := strings.Fields(fmt.Sprintf(\"-I INPUT -d %s -p %s --dport %d -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\",\n\t\t\teIP.String(), strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.TargetPort))\n\t\trules = append(rules, iRule)\n\t\t// Allow only outgoing connections from exposed ports\n\t\toRule := strings.Fields(fmt.Sprintf(\"-I OUTPUT -s %s -p %s --sport %d -m conntrack --ctstate ESTABLISHED -j ACCEPT\",\n\t\t\teIP.String(), strings.ToLower(PortConfig_Protocol_name[int32(iPort.Protocol)]), iPort.TargetPort))\n\t\trules = append(rules, oRule)\n\t}\n\n\tns, err := netns.GetFromPath(os.Args[1])\n\tif err != nil {\n\t\tlogrus.Errorf(\"failed get network namespace %q: %v\", os.Args[1], err)\n\t\tos.Exit(4)\n\t}\n\tdefer ns.Close()\n\n\tif err := netns.Set(ns); err != nil {\n\t\tlogrus.Errorf(\"setting into container net ns %v failed, %v\", os.Args[1], err)\n\t\tos.Exit(5)\n\t}\n\n\tfor _, rule := range rules {\n\t\tif err := iptable.RawCombinedOutputNative(rule...); err != nil {\n\t\t\tlogrus.Errorf(\"set up rule failed, %v: %v\", rule, err)\n\t\t\tos.Exit(6)\n\t\t}\n\t}\n\n\tif len(ingressPorts) == 0 {\n\t\treturn\n\t}\n\n\t// Ensure blocking rules for anything else in/to ingress network\n\tfor _, rule := range [][]string{\n\t\t{\"-d\", eIP.String(), \"-p\", \"sctp\", \"-j\", \"DROP\"},\n\t\t{\"-d\", eIP.String(), \"-p\", \"udp\", \"-j\", \"DROP\"},\n\t\t{\"-d\", eIP.String(), \"-p\", \"tcp\", \"-j\", \"DROP\"},\n\t} {\n\t\tif !iptable.ExistsNative(iptables.Filter, \"INPUT\", rule...) {\n\t\t\tif err := iptable.RawCombinedOutputNative(append([]string{\"-A\", \"INPUT\"}, rule...)...); err != nil {\n\t\t\t\tlogrus.Errorf(\"set up rule failed, %v: %v\", rule, err)\n\t\t\t\tos.Exit(7)\n\t\t\t}\n\t\t}\n\t\trule[0] = \"-s\"\n\t\tif !iptable.ExistsNative(iptables.Filter, \"OUTPUT\", rule...) {\n\t\t\tif err := iptable.RawCombinedOutputNative(append([]string{\"-A\", \"OUTPUT\"}, rule...)...); err != nil {\n\t\t\t\tlogrus.Errorf(\"set up rule failed, %v: %v\", rule, err)\n\t\t\t\tos.Exit(8)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "service_unsupported.go",
          "type": "blob",
          "size": 0.6103515625,
          "content": "//go:build !linux && !windows\n// +build !linux,!windows\n\npackage libnetwork\n\nimport (\n\t\"fmt\"\n\t\"net\"\n)\n\nfunc (c *controller) cleanupServiceBindings(nid string) {\n}\n\nfunc (c *controller) addServiceBinding(name, sid, nid, eid string, vip net.IP, ingressPorts []*PortConfig, aliases []string, ip net.IP) error {\n\treturn fmt.Errorf(\"not supported\")\n}\n\nfunc (c *controller) rmServiceBinding(name, sid, nid, eid string, vip net.IP, ingressPorts []*PortConfig, aliases []string, ip net.IP) error {\n\treturn fmt.Errorf(\"not supported\")\n}\n\nfunc (sb *sandbox) populateLoadBalancers(ep *endpoint) {\n}\n\nfunc arrangeIngressFilterRule() {\n}\n"
        },
        {
          "name": "service_windows.go",
          "type": "blob",
          "size": 4.3310546875,
          "content": "package libnetwork\n\nimport (\n\t\"net\"\n\n\t\"github.com/Microsoft/hcsshim\"\n\t\"github.com/Microsoft/hcsshim/osversion\"\n\t\"github.com/sirupsen/logrus\"\n)\n\ntype policyLists struct {\n\tilb *hcsshim.PolicyList\n\telb *hcsshim.PolicyList\n}\n\nvar lbPolicylistMap map[*loadBalancer]*policyLists\n\nfunc init() {\n\tlbPolicylistMap = make(map[*loadBalancer]*policyLists)\n}\n\nfunc (n *network) addLBBackend(ip net.IP, lb *loadBalancer) {\n\tif len(lb.vip) == 0 {\n\t\treturn\n\t}\n\n\tvip := lb.vip\n\tingressPorts := lb.service.ingressPorts\n\n\tif osversion.Build() > 16236 {\n\t\tlb.Lock()\n\t\tdefer lb.Unlock()\n\t\t//find the load balancer IP for the network.\n\t\tvar sourceVIP string\n\t\tfor _, e := range n.Endpoints() {\n\t\t\tepInfo := e.Info()\n\t\t\tif epInfo == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif epInfo.LoadBalancer() {\n\t\t\t\tsourceVIP = epInfo.Iface().Address().IP.String()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif sourceVIP == \"\" {\n\t\t\tlogrus.Errorf(\"Failed to find load balancer IP for network %s\", n.Name())\n\t\t\treturn\n\t\t}\n\n\t\tvar endpoints []hcsshim.HNSEndpoint\n\n\t\tfor eid, be := range lb.backEnds {\n\t\t\tif be.disabled {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t//Call HNS to get back ID (GUID) corresponding to the endpoint.\n\t\t\thnsEndpoint, err := hcsshim.GetHNSEndpointByName(eid)\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to find HNS ID for endpoint %v: %v\", eid, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tendpoints = append(endpoints, *hnsEndpoint)\n\t\t}\n\n\t\tif policies, ok := lbPolicylistMap[lb]; ok {\n\n\t\t\tif policies.ilb != nil {\n\t\t\t\tpolicies.ilb.Delete()\n\t\t\t\tpolicies.ilb = nil\n\t\t\t}\n\n\t\t\tif policies.elb != nil {\n\t\t\t\tpolicies.elb.Delete()\n\t\t\t\tpolicies.elb = nil\n\t\t\t}\n\t\t\tdelete(lbPolicylistMap, lb)\n\t\t}\n\n\t\tilbPolicy, err := hcsshim.AddLoadBalancer(endpoints, true, sourceVIP, vip.String(), 0, 0, 0)\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to add ILB policy for service %s (%s) with endpoints %v using load balancer IP %s on network %s: %v\",\n\t\t\t\tlb.service.name, vip.String(), endpoints, sourceVIP, n.Name(), err)\n\t\t\treturn\n\t\t}\n\n\t\tlbPolicylistMap[lb] = &policyLists{\n\t\t\tilb: ilbPolicy,\n\t\t}\n\n\t\tpublishedPorts := make(map[uint32]uint32)\n\n\t\tfor i, port := range ingressPorts {\n\t\t\tprotocol := uint16(6)\n\n\t\t\t// Skip already published port\n\t\t\tif publishedPorts[port.PublishedPort] == port.TargetPort {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif port.Protocol == ProtocolUDP {\n\t\t\t\tprotocol = 17\n\t\t\t}\n\n\t\t\t// check if already has udp matching to add wild card publishing\n\t\t\tfor j := i + 1; j < len(ingressPorts); j++ {\n\t\t\t\tif ingressPorts[j].TargetPort == port.TargetPort &&\n\t\t\t\t\tingressPorts[j].PublishedPort == port.PublishedPort {\n\t\t\t\t\tprotocol = 0\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tpublishedPorts[port.PublishedPort] = port.TargetPort\n\n\t\t\tlbPolicylistMap[lb].elb, err = hcsshim.AddLoadBalancer(endpoints, false, sourceVIP, \"\", protocol, uint16(port.TargetPort), uint16(port.PublishedPort))\n\t\t\tif err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to add ELB policy for service %s (ip:%s target port:%v published port:%v) with endpoints %v using load balancer IP %s on network %s: %v\",\n\t\t\t\t\tlb.service.name, vip.String(), uint16(port.TargetPort), uint16(port.PublishedPort), endpoints, sourceVIP, n.Name(), err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (n *network) rmLBBackend(ip net.IP, lb *loadBalancer, rmService bool, fullRemove bool) {\n\tif len(lb.vip) == 0 {\n\t\treturn\n\t}\n\n\tif osversion.Build() > 16236 {\n\t\tif numEnabledBackends(lb) > 0 {\n\t\t\t//Reprogram HNS (actually VFP) with the existing backends.\n\t\t\tn.addLBBackend(ip, lb)\n\t\t} else {\n\t\t\tlb.Lock()\n\t\t\tdefer lb.Unlock()\n\t\t\tlogrus.Debugf(\"No more backends for service %s (ip:%s).  Removing all policies\", lb.service.name, lb.vip.String())\n\n\t\t\tif policyLists, ok := lbPolicylistMap[lb]; ok {\n\t\t\t\tif policyLists.ilb != nil {\n\t\t\t\t\tif _, err := policyLists.ilb.Delete(); err != nil {\n\t\t\t\t\t\tlogrus.Errorf(\"Failed to remove HNS ILB policylist %s: %s\", policyLists.ilb.ID, err)\n\t\t\t\t\t}\n\t\t\t\t\tpolicyLists.ilb = nil\n\t\t\t\t}\n\n\t\t\t\tif policyLists.elb != nil {\n\t\t\t\t\tif _, err := policyLists.elb.Delete(); err != nil {\n\t\t\t\t\t\tlogrus.Errorf(\"Failed to remove HNS ELB policylist %s: %s\", policyLists.elb.ID, err)\n\t\t\t\t\t}\n\t\t\t\t\tpolicyLists.elb = nil\n\t\t\t\t}\n\t\t\t\tdelete(lbPolicylistMap, lb)\n\n\t\t\t} else {\n\t\t\t\tlogrus.Errorf(\"Failed to find policies for service %s (%s)\", lb.service.name, lb.vip.String())\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc numEnabledBackends(lb *loadBalancer) int {\n\tnEnabled := 0\n\tfor _, be := range lb.backEnds {\n\t\tif !be.disabled {\n\t\t\tnEnabled++\n\t\t}\n\t}\n\treturn nEnabled\n}\n\nfunc (sb *sandbox) populateLoadBalancers(ep *endpoint) {\n}\n\nfunc arrangeIngressFilterRule() {\n}\n"
        },
        {
          "name": "store.go",
          "type": "blob",
          "size": 10.7626953125,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/docker/libkv/store/boltdb\"\n\t\"github.com/docker/libkv/store/consul\"\n\t\"github.com/docker/libkv/store/etcd\"\n\t\"github.com/docker/libkv/store/zookeeper\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nfunc registerKVStores() {\n\tconsul.Register()\n\tzookeeper.Register()\n\tetcd.Register()\n\tboltdb.Register()\n}\n\nfunc (c *controller) initScopedStore(scope string, scfg *datastore.ScopeCfg) error {\n\tstore, err := datastore.NewDataStore(scope, scfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\tc.Lock()\n\tc.stores = append(c.stores, store)\n\tc.Unlock()\n\n\treturn nil\n}\n\nfunc (c *controller) initStores() error {\n\tregisterKVStores()\n\n\tc.Lock()\n\tif c.cfg == nil {\n\t\tc.Unlock()\n\t\treturn nil\n\t}\n\tscopeConfigs := c.cfg.Scopes\n\tc.stores = nil\n\tc.Unlock()\n\n\tfor scope, scfg := range scopeConfigs {\n\t\tif err := c.initScopedStore(scope, scfg); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tc.startWatch()\n\treturn nil\n}\n\nfunc (c *controller) closeStores() {\n\tfor _, store := range c.getStores() {\n\t\tstore.Close()\n\t}\n}\n\nfunc (c *controller) getStore(scope string) datastore.DataStore {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tfor _, store := range c.stores {\n\t\tif store.Scope() == scope {\n\t\t\treturn store\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) getStores() []datastore.DataStore {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\treturn c.stores\n}\n\nfunc (c *controller) getNetworkFromStore(nid string) (*network, error) {\n\tfor _, n := range c.getNetworksFromStore() {\n\t\tif n.id == nid {\n\t\t\treturn n, nil\n\t\t}\n\t}\n\treturn nil, ErrNoSuchNetwork(nid)\n}\n\nfunc (c *controller) getNetworksForScope(scope string) ([]*network, error) {\n\tvar nl []*network\n\n\tstore := c.getStore(scope)\n\tif store == nil {\n\t\treturn nil, nil\n\t}\n\n\tkvol, err := store.List(datastore.Key(datastore.NetworkKeyPrefix),\n\t\t&network{ctrlr: c})\n\tif err != nil && err != datastore.ErrKeyNotFound {\n\t\treturn nil, fmt.Errorf(\"failed to get networks for scope %s: %v\",\n\t\t\tscope, err)\n\t}\n\n\tfor _, kvo := range kvol {\n\t\tn := kvo.(*network)\n\t\tn.ctrlr = c\n\n\t\tec := &endpointCnt{n: n}\n\t\terr = store.GetObject(datastore.Key(ec.Key()...), ec)\n\t\tif err != nil && !n.inDelete {\n\t\t\tlogrus.Warnf(\"Could not find endpoint count key %s for network %s while listing: %v\", datastore.Key(ec.Key()...), n.Name(), err)\n\t\t\tcontinue\n\t\t}\n\n\t\tn.epCnt = ec\n\t\tif n.scope == \"\" {\n\t\t\tn.scope = scope\n\t\t}\n\t\tnl = append(nl, n)\n\t}\n\n\treturn nl, nil\n}\n\nfunc (c *controller) getNetworksFromStore() []*network {\n\tvar nl []*network\n\n\tfor _, store := range c.getStores() {\n\t\tkvol, err := store.List(datastore.Key(datastore.NetworkKeyPrefix), &network{ctrlr: c})\n\t\t// Continue searching in the next store if no keys found in this store\n\t\tif err != nil {\n\t\t\tif err != datastore.ErrKeyNotFound {\n\t\t\t\tlogrus.Debugf(\"failed to get networks for scope %s: %v\", store.Scope(), err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tkvep, err := store.Map(datastore.Key(epCntKeyPrefix), &endpointCnt{})\n\t\tif err != nil && err != datastore.ErrKeyNotFound {\n\t\t\tlogrus.Warnf(\"failed to get endpoint_count map for scope %s: %v\", store.Scope(), err)\n\t\t}\n\n\t\tfor _, kvo := range kvol {\n\t\t\tn := kvo.(*network)\n\t\t\tn.Lock()\n\t\t\tn.ctrlr = c\n\t\t\tec := &endpointCnt{n: n}\n\t\t\t// Trim the leading & trailing \"/\" to make it consistent across all stores\n\t\t\tif val, ok := kvep[strings.Trim(datastore.Key(ec.Key()...), \"/\")]; ok {\n\t\t\t\tec = val.(*endpointCnt)\n\t\t\t\tec.n = n\n\t\t\t\tn.epCnt = ec\n\t\t\t}\n\t\t\tif n.scope == \"\" {\n\t\t\t\tn.scope = store.Scope()\n\t\t\t}\n\t\t\tn.Unlock()\n\t\t\tnl = append(nl, n)\n\t\t}\n\t}\n\n\treturn nl\n}\n\nfunc (n *network) getEndpointFromStore(eid string) (*endpoint, error) {\n\tvar errors []string\n\tfor _, store := range n.ctrlr.getStores() {\n\t\tep := &endpoint{id: eid, network: n}\n\t\terr := store.GetObject(datastore.Key(ep.Key()...), ep)\n\t\t// Continue searching in the next store if the key is not found in this store\n\t\tif err != nil {\n\t\t\tif err != datastore.ErrKeyNotFound {\n\t\t\t\terrors = append(errors, fmt.Sprintf(\"{%s:%v}, \", store.Scope(), err))\n\t\t\t\tlogrus.Debugf(\"could not find endpoint %s in %s: %v\", eid, store.Scope(), err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\treturn ep, nil\n\t}\n\treturn nil, fmt.Errorf(\"could not find endpoint %s: %v\", eid, errors)\n}\n\nfunc (n *network) getEndpointsFromStore() ([]*endpoint, error) {\n\tvar epl []*endpoint\n\n\ttmp := endpoint{network: n}\n\tfor _, store := range n.getController().getStores() {\n\t\tkvol, err := store.List(datastore.Key(tmp.KeyPrefix()...), &endpoint{network: n})\n\t\t// Continue searching in the next store if no keys found in this store\n\t\tif err != nil {\n\t\t\tif err != datastore.ErrKeyNotFound {\n\t\t\t\tlogrus.Debugf(\"failed to get endpoints for network %s scope %s: %v\",\n\t\t\t\t\tn.Name(), store.Scope(), err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, kvo := range kvol {\n\t\t\tep := kvo.(*endpoint)\n\t\t\tepl = append(epl, ep)\n\t\t}\n\t}\n\n\treturn epl, nil\n}\n\nfunc (c *controller) updateToStore(kvObject datastore.KVObject) error {\n\tcs := c.getStore(kvObject.DataScope())\n\tif cs == nil {\n\t\treturn ErrDataStoreNotInitialized(kvObject.DataScope())\n\t}\n\n\tif err := cs.PutObjectAtomic(kvObject); err != nil {\n\t\tif err == datastore.ErrKeyModified {\n\t\t\treturn err\n\t\t}\n\t\treturn fmt.Errorf(\"failed to update store for object type %T: %v\", kvObject, err)\n\t}\n\n\treturn nil\n}\n\nfunc (c *controller) deleteFromStore(kvObject datastore.KVObject) error {\n\tcs := c.getStore(kvObject.DataScope())\n\tif cs == nil {\n\t\treturn ErrDataStoreNotInitialized(kvObject.DataScope())\n\t}\n\nretry:\n\tif err := cs.DeleteObjectAtomic(kvObject); err != nil {\n\t\tif err == datastore.ErrKeyModified {\n\t\t\tif err := cs.GetObject(datastore.Key(kvObject.Key()...), kvObject); err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not update the kvobject to latest when trying to delete: %v\", err)\n\t\t\t}\n\t\t\tlogrus.Warnf(\"Error (%v) deleting object %v, retrying....\", err, kvObject.Key())\n\t\t\tgoto retry\n\t\t}\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\ntype netWatch struct {\n\tlocalEps  map[string]*endpoint\n\tremoteEps map[string]*endpoint\n\tstopCh    chan struct{}\n}\n\nfunc (c *controller) getLocalEps(nw *netWatch) []*endpoint {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tvar epl []*endpoint\n\tfor _, ep := range nw.localEps {\n\t\tepl = append(epl, ep)\n\t}\n\n\treturn epl\n}\n\nfunc (c *controller) watchSvcRecord(ep *endpoint) {\n\tc.watchCh <- ep\n}\n\nfunc (c *controller) unWatchSvcRecord(ep *endpoint) {\n\tc.unWatchCh <- ep\n}\n\nfunc (c *controller) networkWatchLoop(nw *netWatch, ep *endpoint, ecCh <-chan datastore.KVObject) {\n\tfor {\n\t\tselect {\n\t\tcase <-nw.stopCh:\n\t\t\treturn\n\t\tcase o := <-ecCh:\n\t\t\tec := o.(*endpointCnt)\n\n\t\t\tepl, err := ec.n.getEndpointsFromStore()\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tc.Lock()\n\t\t\tvar addEp []*endpoint\n\n\t\t\tdelEpMap := make(map[string]*endpoint)\n\t\t\trenameEpMap := make(map[string]bool)\n\t\t\tfor k, v := range nw.remoteEps {\n\t\t\t\tdelEpMap[k] = v\n\t\t\t}\n\n\t\t\tfor _, lEp := range epl {\n\t\t\t\tif _, ok := nw.localEps[lEp.ID()]; ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif ep, ok := nw.remoteEps[lEp.ID()]; ok {\n\t\t\t\t\t// On a container rename EP ID will remain\n\t\t\t\t\t// the same but the name will change. service\n\t\t\t\t\t// records should reflect the change.\n\t\t\t\t\t// Keep old EP entry in the delEpMap and add\n\t\t\t\t\t// EP from the store (which has the new name)\n\t\t\t\t\t// into the new list\n\t\t\t\t\tif lEp.name == ep.name {\n\t\t\t\t\t\tdelete(delEpMap, lEp.ID())\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\trenameEpMap[lEp.ID()] = true\n\t\t\t\t}\n\t\t\t\tnw.remoteEps[lEp.ID()] = lEp\n\t\t\t\taddEp = append(addEp, lEp)\n\t\t\t}\n\n\t\t\t// EPs whose name are to be deleted from the svc records\n\t\t\t// should also be removed from nw's remote EP list, except\n\t\t\t// the ones that are getting renamed.\n\t\t\tfor _, lEp := range delEpMap {\n\t\t\t\tif !renameEpMap[lEp.ID()] {\n\t\t\t\t\tdelete(nw.remoteEps, lEp.ID())\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.Unlock()\n\n\t\t\tfor _, lEp := range delEpMap {\n\t\t\t\tep.getNetwork().updateSvcRecord(lEp, c.getLocalEps(nw), false)\n\n\t\t\t}\n\t\t\tfor _, lEp := range addEp {\n\t\t\t\tep.getNetwork().updateSvcRecord(lEp, c.getLocalEps(nw), true)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (c *controller) processEndpointCreate(nmap map[string]*netWatch, ep *endpoint) {\n\tn := ep.getNetwork()\n\tif !c.isDistributedControl() && n.Scope() == datastore.SwarmScope && n.driverIsMultihost() {\n\t\treturn\n\t}\n\n\tnetworkID := n.ID()\n\tendpointID := ep.ID()\n\n\tc.Lock()\n\tnw, ok := nmap[networkID]\n\tc.Unlock()\n\n\tif ok {\n\t\t// Update the svc db for the local endpoint join right away\n\t\tn.updateSvcRecord(ep, c.getLocalEps(nw), true)\n\n\t\tc.Lock()\n\t\tnw.localEps[endpointID] = ep\n\n\t\t// If we had learned that from the kv store remove it\n\t\t// from remote ep list now that we know that this is\n\t\t// indeed a local endpoint\n\t\tdelete(nw.remoteEps, endpointID)\n\t\tc.Unlock()\n\t\treturn\n\t}\n\n\tnw = &netWatch{\n\t\tlocalEps:  make(map[string]*endpoint),\n\t\tremoteEps: make(map[string]*endpoint),\n\t}\n\n\t// Update the svc db for the local endpoint join right away\n\t// Do this before adding this ep to localEps so that we don't\n\t// try to update this ep's container's svc records\n\tn.updateSvcRecord(ep, c.getLocalEps(nw), true)\n\n\tc.Lock()\n\tnw.localEps[endpointID] = ep\n\tnmap[networkID] = nw\n\tnw.stopCh = make(chan struct{})\n\tc.Unlock()\n\n\tstore := c.getStore(n.DataScope())\n\tif store == nil {\n\t\treturn\n\t}\n\n\tif !store.Watchable() {\n\t\treturn\n\t}\n\n\tch, err := store.Watch(n.getEpCnt(), nw.stopCh)\n\tif err != nil {\n\t\tlogrus.Warnf(\"Error creating watch for network: %v\", err)\n\t\treturn\n\t}\n\n\tgo c.networkWatchLoop(nw, ep, ch)\n}\n\nfunc (c *controller) processEndpointDelete(nmap map[string]*netWatch, ep *endpoint) {\n\tn := ep.getNetwork()\n\tif !c.isDistributedControl() && n.Scope() == datastore.SwarmScope && n.driverIsMultihost() {\n\t\treturn\n\t}\n\n\tnetworkID := n.ID()\n\tendpointID := ep.ID()\n\n\tc.Lock()\n\tnw, ok := nmap[networkID]\n\n\tif ok {\n\t\tdelete(nw.localEps, endpointID)\n\t\tc.Unlock()\n\n\t\t// Update the svc db about local endpoint leave right away\n\t\t// Do this after we remove this ep from localEps so that we\n\t\t// don't try to remove this svc record from this ep's container.\n\t\tn.updateSvcRecord(ep, c.getLocalEps(nw), false)\n\n\t\tc.Lock()\n\t\tif len(nw.localEps) == 0 {\n\t\t\tclose(nw.stopCh)\n\n\t\t\t// This is the last container going away for the network. Destroy\n\t\t\t// this network's svc db entry\n\t\t\tdelete(c.svcRecords, networkID)\n\n\t\t\tdelete(nmap, networkID)\n\t\t}\n\t}\n\tc.Unlock()\n}\n\nfunc (c *controller) watchLoop() {\n\tfor {\n\t\tselect {\n\t\tcase ep := <-c.watchCh:\n\t\t\tc.processEndpointCreate(c.nmap, ep)\n\t\tcase ep := <-c.unWatchCh:\n\t\t\tc.processEndpointDelete(c.nmap, ep)\n\t\t}\n\t}\n}\n\nfunc (c *controller) startWatch() {\n\tif c.watchCh != nil {\n\t\treturn\n\t}\n\tc.watchCh = make(chan *endpoint)\n\tc.unWatchCh = make(chan *endpoint)\n\tc.nmap = make(map[string]*netWatch)\n\n\tgo c.watchLoop()\n}\n\nfunc (c *controller) networkCleanup() {\n\tfor _, n := range c.getNetworksFromStore() {\n\t\tif n.inDelete {\n\t\t\tlogrus.Infof(\"Removing stale network %s (%s)\", n.Name(), n.ID())\n\t\t\tif err := n.delete(true, true); err != nil {\n\t\t\t\tlogrus.Debugf(\"Error while removing stale network: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nvar populateSpecial NetworkWalker = func(nw Network) bool {\n\tif n := nw.(*network); n.hasSpecialDriver() && !n.ConfigOnly() {\n\t\tif err := n.getController().addNetwork(n); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to populate network %q with driver %q\", nw.Name(), nw.Type())\n\t\t}\n\t}\n\treturn false\n}\n"
        },
        {
          "name": "store_linux_test.go",
          "type": "blob",
          "size": 1.4306640625,
          "content": "package libnetwork\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/docker/libkv/store\"\n\t\"github.com/docker/libnetwork/datastore\"\n)\n\nfunc TestBoltdbBackend(t *testing.T) {\n\tdefer os.Remove(datastore.DefaultScopes(\"\")[datastore.LocalScope].Client.Address)\n\ttestLocalBackend(t, \"\", \"\", nil)\n\tdefer os.Remove(\"/tmp/boltdb.db\")\n\tconfig := &store.Config{Bucket: \"testBackend\"}\n\ttestLocalBackend(t, \"boltdb\", \"/tmp/boltdb.db\", config)\n\n}\n\nfunc TestNoPersist(t *testing.T) {\n\tcfgOptions, err := OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating random boltdb file : %v\", err)\n\t}\n\tctrl, err := New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error new controller: %v\", err)\n\t}\n\tnw, err := ctrl.NewNetwork(\"host\", \"host\", \"\", NetworkOptionPersist(false))\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating default \\\"host\\\" network: %v\", err)\n\t}\n\tep, err := nw.CreateEndpoint(\"newendpoint\", []EndpointOption{}...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating endpoint: %v\", err)\n\t}\n\tstore := ctrl.(*controller).getStore(datastore.LocalScope).KVStore()\n\tif exists, _ := store.Exists(datastore.Key(datastore.NetworkKeyPrefix, string(nw.ID()))); exists {\n\t\tt.Fatalf(\"Network with persist=false should not be stored in KV Store\")\n\t}\n\tif exists, _ := store.Exists(datastore.Key([]string{datastore.EndpointKeyPrefix, string(nw.ID()), string(ep.ID())}...)); exists {\n\t\tt.Fatalf(\"Endpoint in Network with persist=false should not be stored in KV Store\")\n\t}\n\tstore.Close()\n}\n"
        },
        {
          "name": "store_test.go",
          "type": "blob",
          "size": 3.5029296875,
          "content": "package libnetwork\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"testing\"\n\n\t\"github.com/docker/libkv/store\"\n\t\"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/datastore\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/options\"\n)\n\nfunc testZooKeeperBackend(t *testing.T) {\n\tc, err := testNewController(t, \"zk\", \"127.0.0.1:2181/custom_prefix\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tc.Stop()\n}\n\nfunc testNewController(t *testing.T, provider, url string) (NetworkController, error) {\n\tcfgOptions, err := OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcfgOptions = append(cfgOptions, config.OptionKVProvider(provider))\n\tcfgOptions = append(cfgOptions, config.OptionKVProviderURL(url))\n\treturn New(cfgOptions...)\n}\n\nfunc testLocalBackend(t *testing.T, provider, url string, storeConfig *store.Config) {\n\tcfgOptions := []config.Option{}\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProvider(provider))\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProviderURL(url))\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProviderConfig(storeConfig))\n\n\tdriverOptions := options.Generic{}\n\tgenericOption := make(map[string]interface{})\n\tgenericOption[netlabel.GenericData] = driverOptions\n\tcfgOptions = append(cfgOptions, config.OptionDriverConfig(\"host\", genericOption))\n\n\tctrl, err := New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error new controller: %v\", err)\n\t}\n\tnw, err := ctrl.NewNetwork(\"host\", \"host\", \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating default \\\"host\\\" network: %v\", err)\n\t}\n\tep, err := nw.CreateEndpoint(\"newendpoint\", []EndpointOption{}...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating endpoint: %v\", err)\n\t}\n\tstore := ctrl.(*controller).getStore(datastore.LocalScope).KVStore()\n\tif exists, err := store.Exists(datastore.Key(datastore.NetworkKeyPrefix, string(nw.ID()))); !exists || err != nil {\n\t\tt.Fatalf(\"Network key should have been created.\")\n\t}\n\tif exists, err := store.Exists(datastore.Key([]string{datastore.EndpointKeyPrefix, string(nw.ID()), string(ep.ID())}...)); !exists || err != nil {\n\t\tt.Fatalf(\"Endpoint key should have been created.\")\n\t}\n\tstore.Close()\n\n\t// test restore of local store\n\tctrl, err = New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating controller: %v\", err)\n\t}\n\tif _, err = ctrl.NetworkByID(nw.ID()); err != nil {\n\t\tt.Fatalf(\"Error getting network %v\", err)\n\t}\n}\n\n// OptionBoltdbWithRandomDBFile function returns a random dir for local store backend\nfunc OptionBoltdbWithRandomDBFile() ([]config.Option, error) {\n\ttmp, err := ioutil.TempFile(\"\", \"libnetwork-\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error creating temp file: %v\", err)\n\t}\n\tif err := tmp.Close(); err != nil {\n\t\treturn nil, fmt.Errorf(\"Error closing temp file: %v\", err)\n\t}\n\tcfgOptions := []config.Option{}\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProvider(\"boltdb\"))\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProviderURL(tmp.Name()))\n\tsCfg := &store.Config{Bucket: \"testBackend\"}\n\tcfgOptions = append(cfgOptions, config.OptionLocalKVProviderConfig(sCfg))\n\treturn cfgOptions, nil\n}\n\nfunc TestMultipleControllersWithSameStore(t *testing.T) {\n\tcfgOptions, err := OptionBoltdbWithRandomDBFile()\n\tif err != nil {\n\t\tt.Fatalf(\"Error getting random boltdb configs %v\", err)\n\t}\n\tctrl1, err := New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatalf(\"Error new controller: %v\", err)\n\t}\n\tdefer ctrl1.Stop()\n\t// Use the same boltdb file without closing the previous controller\n\t_, err = New(cfgOptions...)\n\tif err != nil {\n\t\tt.Fatalf(\"Local store must support concurrent controllers\")\n\t}\n}\n"
        },
        {
          "name": "support",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "testutils",
          "type": "tree",
          "content": null
        },
        {
          "name": "types",
          "type": "tree",
          "content": null
        },
        {
          "name": "vendor.conf",
          "type": "blob",
          "size": 4.6396484375,
          "content": "github.com/Azure/go-ansiterm                        d6e3b3328b783f23731bc4d058875b0371ff8109\ngithub.com/BurntSushi/toml                          3012a1dbe2e4bd1391d42b32f0577cb7bbc7f005 # v0.3.1\ngithub.com/containerd/cgroups                       318312a373405e5e91134d8063d04d59768a1bff\ngithub.com/Microsoft/go-winio                       6c72808b55902eae4c5943626030429ff20f3b63 # v0.4.14\ngithub.com/Microsoft/hcsshim                        a11a2c44e8a4aa9d66314b1d759ef582df5ab5e8 # moby branch\ngithub.com/armon/go-metrics                         eb0af217e5e9747e41dd5303755356b62d28e3ec\ngithub.com/armon/go-radix                           e39d623f12e8e41c7b5529e9a9dd67a1e2261f80\ngithub.com/coreos/etcd                              d57e8b8d97adfc4a6c224fe116714bf1a1f3beb9 # v3.3.12\ngithub.com/coreos/go-semver                         8ab6407b697782a06568d4b7f1db25550ec2e4c6 # v0.2.0\ngithub.com/deckarep/golang-set                      ef32fa3046d9f249d399f98ebaf9be944430fd1d\ngo.etcd.io/bbolt                                    232d8fc87f50244f9c808f4745759e08a304c029 # v1.3.5\n\ngithub.com/docker/docker                            7ca355652fe0e2f7401d424d65a81dc248360127\ngithub.com/moby/term                                73f35e472e8f0a3f91347164138ce6bd73b756a9\ngithub.com/docker/go-connections                    7395e3f8aa162843a74ed6d48e79627d9792ac55 # v0.4.0\ngithub.com/docker/go-events                         e31b211e4f1cd09aa76fe4ac244571fab96ae47f\ngithub.com/docker/libkv                             458977154600b9f23984d9f4b82e79570b5ae12b\n\ngithub.com/gogo/protobuf                            5628607bb4c51c3157aacc3a50f0ab707582b805 # v1.3.1\n\ngithub.com/godbus/dbus/v5                           37bf87eef99d69c4f1d3528bd66e3a87dc201472 # v5.0.3\ngithub.com/gorilla/mux                              98cb6bf42e086f6af920b965c38cacc07402d51b # v1.8.0\ngithub.com/hashicorp/consul                         9a9cc9341bb487651a0399e3fc5e1e8a42e62dd9 # v0.5.2\ngithub.com/hashicorp/errwrap                        8a6fb523712970c966eefc6b39ed2c5e74880354 # v1.0.0\ngithub.com/hashicorp/go-msgpack                     71c2886f5a673a35f909803f38ece5810165097b\ngithub.com/hashicorp/go-multierror                  886a7fbe3eb1c874d46f623bfa70af45f425b3d1 # v1.0.0\ngithub.com/hashicorp/memberlist                     3d8438da9589e7b608a83ffac1ef8211486bcb7c\ngithub.com/hashicorp/golang-lru                     7f827b33c0f158ec5dfbba01bb0b14a4541fd81d # v0.5.3\ngithub.com/sean-/seed                               e2103e2c35297fb7e17febb81e49b312087a2372\ngithub.com/hashicorp/go-sockaddr                    c7188e74f6acae5a989bdc959aa779f8b9f42faf # v1.0.2\ngithub.com/hashicorp/serf                           598c54895cc5a7b1a24a398d635e8c0ea0959870\ngithub.com/miekg/dns                                6c0c4e6581f8e173cc562c8b3363ab984e4ae071 # v1.1.27\ngithub.com/opencontainers/runtime-spec              4d89ac9fbff6c455f46a5bb59c6b1bb7184a5e43 # v1.0.3-0.20200728170252-4d89ac9fbff6\ngithub.com/samuel/go-zookeeper                      d0e0d8e11f318e000a8cc434616d69e329edc374\ngithub.com/sirupsen/logrus                          60c74ad9be0d874af0ab0daef6ab07c5c5911f0d # v1.6.0\ngithub.com/konsorten/go-windows-terminal-sequences  edb144dfd453055e1e49a3d8b410a660b5a87613 # v1.0.3\ngithub.com/ugorji/go                                b4c50a2b199d93b13dc15e78929cfb23bfdf21ab # v1.1.1\ngithub.com/urfave/cli                               a65b733b303f0055f8d324d805f393cd3e7a7904\ngithub.com/vishvananda/netlink                      f049be6f391489d3f374498fe0c8df8449258372 # v1.1.0\ngithub.com/vishvananda/netns                        db3c7e526aae966c4ccfa6c8189b693d6ac5d202\ngolang.org/x/crypto                                 75b288015ac94e66e3d6715fb68a9b41bf046ec2\ngolang.org/x/net                                    ab34263943818b32f575efc978a3d24e80b04bd7\ngolang.org/x/sys                                    ed371f2e16b4b305ee99df548828de367527b76b\ngolang.org/x/sync                                   cd5d95a43a6e21273425c7ae415d3df9ea832eeb\ngithub.com/pkg/errors                               614d223910a179a466c1767a985424175c39b465 # v0.9.1\ngithub.com/ishidawataru/sctp                        f2269e66cdee387bd321445d5d300893449805be\ngo.opencensus.io                                    9c377598961b706d1542bd2d84d538b5094d596e # v0.22.0\n\ngotest.tools/v3                                     bb0d8a963040ea5048dcef1a14d8f8b58a33d4b3 # v3.0.2\ngithub.com/google/go-cmp                            3af367b6b30c263d47e8895973edcca9a49cf029 # v0.2.0\n\ngithub.com/moby/ipvs                                4566ccea0e08d68e9614c3e7a64a23b850c4bb35 # v1.0.1\ngithub.com/moby/locker                              281af2d563954745bea9d1487c965f24d30742fe # v1.0.1\n"
        },
        {
          "name": "vendor",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}