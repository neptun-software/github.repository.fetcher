{
  "metadata": {
    "timestamp": 1736567206140,
    "page": 806,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "gocraft/work",
      "stars": 2447,
      "defaultBranch": "master",
      "files": [
        {
          "name": "DEVELOPING.md",
          "type": "blob",
          "size": 0.6669921875,
          "content": "## Web UI\n\n```\ncd cmd/workwebui\ngo run main.go\nopen \"http://localhost:5040/\"\n```\n\n## Assets\n\nWeb UI frontend is written in [react](https://facebook.github.io/react/). [Webpack](https://webpack.github.io/) is used to transpile and bundle es7 and jsx to run on modern browsers.\nFinally bundled js is embedded in a go file.\n\nAll NPM commands can be found in `package.json`.\n\n- fetch dependency: `npm install`\n- test: `npm test`\n- generate test coverage: `npm run cover`\n- lint: `npm run lint`\n- bundle for production: `npm run build`\n- bundle for testing: `npm run dev`\n\nTo embed bundled js, do\n\n```\ngo get -u github.com/jteeuwen/go-bindata/...\ncd webui/internal/assets\ngo generate\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 Jonathan Novak\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.955078125,
          "content": "# gocraft/work [![GoDoc](https://godoc.org/github.com/gocraft/work?status.png)](https://godoc.org/github.com/gocraft/work)\n\ngocraft/work lets you enqueue and processes background jobs in Go. Jobs are durable and backed by Redis. Very similar to Sidekiq for Go.\n\n* Fast and efficient. Faster than [this](https://www.github.com/jrallison/go-workers), [this](https://www.github.com/benmanns/goworker), and [this](https://www.github.com/albrow/jobs). See below for benchmarks.\n* Reliable - don't lose jobs even if your process crashes.\n* Middleware on jobs -- good for metrics instrumentation, logging, etc.\n* If a job fails, it will be retried a specified number of times.\n* Schedule jobs to happen in the future.\n* Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once.\n* Web UI to manage failed jobs and observe the system.\n* Periodically enqueue jobs on a cron-like schedule.\n* Pause / unpause jobs and control concurrency within and across processes\n\n## Enqueue new jobs\n\nTo enqueue jobs, you need to make an Enqueuer with a redis namespace and a redigo pool. Each enqueued job has a name and can take optional arguments. Arguments are k/v pairs (serialized as JSON internally).\n\n```go\npackage main\n\nimport (\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/gocraft/work\"\n)\n\n// Make a redis pool\nvar redisPool = &redis.Pool{\n\tMaxActive: 5,\n\tMaxIdle: 5,\n\tWait: true,\n\tDial: func() (redis.Conn, error) {\n\t\treturn redis.Dial(\"tcp\", \":6379\")\n\t},\n}\n\n// Make an enqueuer with a particular namespace\nvar enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool)\n\nfunc main() {\n\t// Enqueue a job named \"send_email\" with the specified parameters.\n\t_, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\n\n```\n\n## Process jobs\n\nIn order to process jobs, you'll need to make a WorkerPool. Add middleware and jobs to the pool, and start the pool.\n\n```go\npackage main\n\nimport (\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/gocraft/work\"\n\t\"os\"\n\t\"os/signal\"\n)\n\n// Make a redis pool\nvar redisPool = &redis.Pool{\n\tMaxActive: 5,\n\tMaxIdle: 5,\n\tWait: true,\n\tDial: func() (redis.Conn, error) {\n\t\treturn redis.Dial(\"tcp\", \":6379\")\n\t},\n}\n\ntype Context struct{\n    customerID int64\n}\n\nfunc main() {\n\t// Make a new pool. Arguments:\n\t// Context{} is a struct that will be the context for the request.\n\t// 10 is the max concurrency\n\t// \"my_app_namespace\" is the Redis namespace\n\t// redisPool is a Redis pool\n\tpool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool)\n\n\t// Add middleware that will be executed for each job\n\tpool.Middleware((*Context).Log)\n\tpool.Middleware((*Context).FindCustomer)\n\n\t// Map the name of jobs to handler functions\n\tpool.Job(\"send_email\", (*Context).SendEmail)\n\n\t// Customize options:\n\tpool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export)\n\n\t// Start processing jobs\n\tpool.Start()\n\n\t// Wait for a signal to quit:\n\tsignalChan := make(chan os.Signal, 1)\n\tsignal.Notify(signalChan, os.Interrupt, os.Kill)\n\t<-signalChan\n\n\t// Stop the pool\n\tpool.Stop()\n}\n\nfunc (c *Context) Log(job *work.Job, next work.NextMiddlewareFunc) error {\n\tfmt.Println(\"Starting job: \", job.Name)\n\treturn next()\n}\n\nfunc (c *Context) FindCustomer(job *work.Job, next work.NextMiddlewareFunc) error {\n\t// If there's a customer_id param, set it in the context for future middleware and handlers to use.\n\tif _, ok := job.Args[\"customer_id\"]; ok {\n\t\tc.customerID = job.ArgInt64(\"customer_id\")\n\t\tif err := job.ArgError(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn next()\n}\n\nfunc (c *Context) SendEmail(job *work.Job) error {\n\t// Extract arguments:\n\taddr := job.ArgString(\"address\")\n\tsubject := job.ArgString(\"subject\")\n\tif err := job.ArgError(); err != nil {\n\t\treturn err\n\t}\n\n\t// Go ahead and send the email...\n\t// sendEmailTo(addr, subject)\n\n\treturn nil\n}\n\nfunc (c *Context) Export(job *work.Job) error {\n\treturn nil\n}\n```\n\n## Redis Cluster\nIf you're attempting to use gocraft/work on a `Redis Cluster` deployment, then you may encounter a `CROSSSLOT Keys in request don't hash to the same slot` error during the execution of the various lua scripts used to manage job data (see [Issue 93](https://github.com/gocraft/work/issues/93#issuecomment-401134340)). The current workaround is to force the keys for an entire `namespace` for a given worker pool on a single node in the cluster using [Redis Hash Tags](https://redis.io/topics/cluster-spec#keys-hash-tags). Using the example above:\n\n```go\nfunc main() {\n\t// Make a new pool. Arguments:\n\t// Context{} is a struct that will be the context for the request.\n\t// 10 is the max concurrency\n\t// \"my_app_namespace\" is the Redis namespace and the {} chars forces all of the keys onto a single node\n\t// redisPool is a Redis pool\n\tpool := work.NewWorkerPool(Context{}, 10, \"{my_app_namespace}\", redisPool)\n```\n\n*Note* this is not an issue for Redis Sentinel deployments.\n\n## Special Features\n\n### Contexts\n\nJust like in [gocraft/web](https://www.github.com/gocraft/web), gocraft/work lets you use your own contexts. Your context can be empty or it can have various fields in it. The fields can be whatever you want - it's your type! When a new job is processed by a worker, we'll allocate an instance of this struct and pass it to your middleware and handlers. This allows you to pass information from one middleware function to the next, and onto your handlers.\n\nCustom contexts aren't really needed for trivial example applications, but are very important for production apps. For instance, one field in your context can be your tagged logger. Your tagged logger augments your log statements with a job-id. This lets you filter your logs by that job-id.\n\n### Check-ins\n\nSince this is a background job processing library, it's fairly common to have jobs that that take a long time to execute. Imagine you have a job that takes an hour to run. It can often be frustrating to know if it's hung, or about to finish, or if it has 30 more minutes to go.\n\nTo solve this, you can instrument your jobs to \"checkin\" every so often with a string message. This checkin status will show up in the web UI. For instance, your job could look like this:\n\n```go\nfunc (c *Context) Export(job *work.Job) error {\n\trowsToExport := getRows()\n\tfor i, row := range rowsToExport {\n\t\texportRow(row)\n\t\tif i % 1000 == 0 {\n\t\t\tjob.Checkin(\"i=\" + fmt.Sprint(i))   // Here's the magic! This tells gocraft/work our status\n\t\t}\n\t}\n}\n\n```\n\nThen in the web UI, you'll see the status of the worker:\n\n| Name | Arguments | Started At | Check-in At | Check-in |\n| --- | --- | --- | --- | --- |\n| export | {\"account_id\": 123} | 2016/07/09 04:16:51 | 2016/07/09 05:03:13 | i=335000 |\n\n### Scheduled Jobs\n\nYou can schedule jobs to be executed in the future. To do so, make a new ```Enqueuer``` and call its ```EnqueueIn``` method:\n\n```go\nenqueuer := work.NewEnqueuer(\"my_app_namespace\", redisPool)\nsecondsInTheFuture := 300\n_, err := enqueuer.EnqueueIn(\"send_welcome_email\", secondsInTheFuture, work.Q{\"address\": \"test@example.com\"})\n```\n\n### Unique Jobs\n\nYou can enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. For instance, you might have a worker that expires the cache of an object. It doesn't make sense for multiple such jobs to exist at once. Also note that unique jobs are supported for normal enqueues as well as scheduled enqueues.\n\n```go\nenqueuer := work.NewEnqueuer(\"my_app_namespace\", redisPool)\njob, err := enqueuer.EnqueueUnique(\"clear_cache\", work.Q{\"object_id_\": \"123\"}) // job returned\njob, err = enqueuer.EnqueueUnique(\"clear_cache\", work.Q{\"object_id_\": \"123\"}) // job == nil -- this duplicate job isn't enqueued.\njob, err = enqueuer.EnqueueUniqueIn(\"clear_cache\", 300, work.Q{\"object_id_\": \"789\"}) // job != nil (diff id)\n```\n\nAlternatively, you can provide your own key for making a job unique. When another job is enqueued with the same key as a job already in the queue, it will simply update the arguments.\n```go\nenqueuer := work.NewEnqueuer(\"my_app_namespace\", redisPool)\njob, err := enqueuer.EnqueueUniqueByKey(\"clear_cache\", work.Q{\"object_id_\": \"123\"}, map[string]interface{}{\"my_key\": \"586\"})\njob, err = enqueuer.EnqueueUniqueInByKey(\"clear_cache\", 300, work.Q{\"object_id_\": \"789\"}, map[string]interface{}{\"my_key\": \"586\"})\n```\nFor information on how this map will be serialized to form a unique key, see (https://golang.org/pkg/encoding/json/#Marshal).\n\n### Periodic Enqueueing (Cron)\n\nYou can periodically enqueue jobs on your gocraft/work cluster using your worker pool. The [scheduling specification](https://godoc.org/github.com/robfig/cron#hdr-CRON_Expression_Format) uses a Cron syntax where the fields represent seconds, minutes, hours, day of the month, month, and week of the day, respectively. Even if you have multiple worker pools on different machines, they'll all coordinate and only enqueue your job once.\n\n```go\npool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool)\npool.PeriodicallyEnqueue(\"0 0 * * * *\", \"calculate_caches\") // This will enqueue a \"calculate_caches\" job every hour\npool.Job(\"calculate_caches\", (*Context).CalculateCaches) // Still need to register a handler for this job separately\n```\n\n## Job concurrency\n\nYou can control job concurrency using `JobOptions{MaxConcurrency: <num>}`. Unlike the WorkerPool concurrency, this controls the limit on the number jobs of that type that can be active at one time by within a single redis instance. This works by putting a precondition on enqueuing function, meaning a new job will not be scheduled if we are at or over a job's `MaxConcurrency` limit. A redis key (see `redis.go::redisKeyJobsLock`) is used as a counting semaphore in order to track job concurrency per job type. The default value is `0`, which means \"no limit on job concurrency\".\n\n**Note:** if you want to run jobs \"single threaded\" then you can set the `MaxConcurrency` accordingly:\n```go\n      worker_pool.JobWithOptions(jobName, JobOptions{MaxConcurrency: 1}, (*Context).WorkFxn)\n```\n\n\n## Run the Web UI\n\nThe web UI provides a view to view the state of your gocraft/work cluster, inspect queued jobs, and retry or delete dead jobs.\n\nBuilding an installing the binary:\n```bash\ngo get github.com/gocraft/work/cmd/workwebui\ngo install github.com/gocraft/work/cmd/workwebui\n```\n\nThen, you can run it:\n```bash\nworkwebui -redis=\"redis:6379\" -ns=\"work\" -listen=\":5040\"\n```\n\nNavigate to ```http://localhost:5040/```.\n\nYou'll see a view that looks like this:\n\n![Web UI Screenshot](https://gocraft.github.io/work/images/webui.png)\n\n## Design and concepts\n\n### Enqueueing jobs\n\n* When jobs are enqueued, they're serialized with JSON and added to a simple Redis list with LPUSH.\n* Jobs are added to a list with the same name as the job. Each job name gets its own queue. Whereas with other job systems you have to design which jobs go on which queues, there's no need for that here.\n\n### Scheduling algorithm\n\n* Each job lives in a list-based queue with the same name as the job.\n* Each of these queues can have an associated priority. The priority is a number from 1 to 100000.\n* Each time a worker pulls a job, it needs to choose a queue. It chooses a queue probabilistically based on its relative priority.\n* If the sum of priorities among all queues is 1000, and one queue has priority 100, jobs will be pulled from that queue 10% of the time.\n* Obviously if a queue is empty, it won't be considered.\n* The semantics of \"always process X jobs before Y jobs\" can be accurately approximated by giving X a large number (like 10000) and Y a small number (like 1).\n\n### Processing a job\n\n* To process a job, a worker will execute a Lua script to atomically move a job its queue to an in-progress queue.\n  * A job is dequeued and moved to in-progress if the job queue is not paused and the number of active jobs does not exceed concurrency limit for the job type\n* The worker will then run the job and increment the job lock. The job will either finish successfully or result in an error or panic.\n  * If the process completely crashes, the reaper will eventually find it in its in-progress queue and requeue it.\n* If the job is successful, we'll simply remove the job from the in-progress queue.\n* If the job returns an error or panic, we'll see how many retries a job has left. If it doesn't have any, we'll move it to the dead queue. If it has retries left, we'll consume a retry and add the job to the retry queue.\n\n### Workers and WorkerPools\n\n* WorkerPools provide the public API of gocraft/work.\n  * You can attach jobs and middleware to them.\n  * You can start and stop them.\n  * Based on their concurrency setting, they'll spin up N worker goroutines.\n* Each worker is run in a goroutine. It will get a job from redis, run it, get the next job, etc.\n  * Each worker is independent. They are not dispatched work -- they get their own work.\n\n### Retry job, scheduled jobs, and the requeuer\n\n* In addition to the normal list-based queues that normal jobs live in, there are two other types of queues: the retry queue and the scheduled job queue.\n* Both of these are implemented as Redis z-sets. The score is the unix timestamp when the job should be run. The value is the bytes of the job.\n* The requeuer will occasionally look for jobs in these queues that should be run now. If they should be, they'll be atomically moved to the normal list-based queue and eventually processed.\n\n### Dead jobs\n\n* After a job has failed a specified number of times, it will be added to the dead job queue.\n* The dead job queue is just a Redis z-set. The score is the timestamp it failed and the value is the job.\n* To retry failed jobs, use the UI or the Client API.\n\n### The reaper\n\n* If a process crashes hard (eg, the power on the server turns off or the kernal freezes), some jobs may be in progress and we won't want to lose them. They're safe in their in-progress queue.\n* The reaper will look for worker pools without a heartbeat. It will scan their in-progress queues and requeue anything it finds.\n\n### Unique jobs\n\n* You can enqueue unique jobs such that a given name/arguments are on the queue at once.\n* Both normal queues and the scheduled queue are considered.\n* When a unique job is enqueued, we'll atomically set a redis key that includes the job name and arguments and enqueue the job.\n* When the job is processed, we'll delete that key to permit another job to be enqueued.\n\n### Periodic jobs\n\n* You can tell a worker pool to enqueue jobs periodically using a cron schedule.\n* Each worker pool will wake up every 2 minutes, and if jobs haven't been scheduled yet, it will schedule all the jobs that would be executed in the next five minutes.\n* Each periodic job that runs at a given time has a predictable byte pattern. Since jobs are scheduled on the scheduled job queue (a Redis z-set), if the same job is scheduled twice for a given time, it can only exist in the z-set once.\n\n### Paused jobs\n\n* You can pause jobs from being processed from a specific queue by setting a \"paused\" redis key (see `redisKeyJobsPaused`)\n* Conversely, jobs in the queue will resume being processed once the paused redis key is removed\n\n### Terminology reference\n* \"worker pool\" - a pool of workers\n* \"worker\" - an individual worker in a single goroutine. Gets a job from redis, does job, gets next job...\n* \"heartbeater\" or \"worker pool heartbeater\" - goroutine owned by worker pool that runs concurrently with workers. Writes the worker pool's config/status (aka \"heartbeat\") every 5 seconds.\n* \"heartbeat\" - the status written by the heartbeater.\n* \"observer\" or \"worker observer\" - observes a worker. Writes stats. makes \"observations\".\n* \"worker observation\" - A snapshot made by an observer of what a worker is working on.\n* \"periodic enqueuer\" - A process that runs with a worker pool that periodically enqueues new jobs based on cron schedules.\n* \"job\" - the actual bundle of data that constitutes one job\n* \"job name\" - each job has a name, like \"create_watch\"\n* \"job type\" - backend/private nomenclature for the handler+options for processing a job\n* \"queue\" - each job creates a queue with the same name as the job. only jobs named X go into the X queue.\n* \"retry jobs\" - if a job fails and needs to be retried, it will be put on this queue.\n* \"scheduled jobs\" - jobs enqueued to be run in th future will be put on a scheduled job queue.\n* \"dead jobs\" - if a job exceeds its MaxFails count, it will be put on the dead job queue.\n* \"paused jobs\" - if paused key is present for a queue, then no jobs from that queue will be processed by any workers until that queue's paused key is removed\n* \"job concurrency\" - the number of jobs being actively processed  of a particular type across worker pool processes but within a single redis instance\n\n## Benchmarks\n\nThe benches folder contains various benchmark code. In each case, we enqueue 100k jobs across 5 queues. The jobs are almost no-op jobs: they simply increment an atomic counter. We then measure the rate of change of the counter to obtain our measurement.\n\n| Library | Speed |\n| --- | --- |\n| [gocraft/work](https://www.github.com/gocraft/work) | **20944 jobs/s** |\n| [jrallison/go-workers](https://www.github.com/jrallison/go-workers) | 19945 jobs/s |\n| [benmanns/goworker](https://www.github.com/benmanns/goworker) | 10328.5 jobs/s |\n| [albrow/jobs](https://www.github.com/albrow/jobs) | 40 jobs/s |\n\n\n## gocraft\n\ngocraft offers a toolkit for building web apps. Currently these packages are available:\n\n* [gocraft/web](https://github.com/gocraft/web) - Go Router + Middleware. Your Contexts.\n* [gocraft/dbr](https://github.com/gocraft/dbr) - Additions to Go's database/sql for super fast performance and convenience.\n* [gocraft/health](https://github.com/gocraft/health) - Instrument your web apps with logging and metrics.\n* [gocraft/work](https://github.com/gocraft/work) - Process background jobs in Go.\n\nThese packages were developed by the [engineering team](https://eng.uservoice.com) at [UserVoice](https://www.uservoice.com) and currently power much of its infrastructure and tech stack.\n\n## Authors\n\n* Jonathan Novak -- [https://github.com/cypriss](https://github.com/cypriss)\n* Tai-Lin Chu -- [https://github.com/taylorchu](https://github.com/taylorchu)\n* Sponsored by [UserVoice](https://eng.uservoice.com)\n"
        },
        {
          "name": "benches",
          "type": "tree",
          "content": null
        },
        {
          "name": "client.go",
          "type": "blob",
          "size": 15.5322265625,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\n// ErrNotDeleted is returned by functions that delete jobs to indicate that although the redis commands were successful,\n// no object was actually deleted by those commmands.\nvar ErrNotDeleted = fmt.Errorf(\"nothing deleted\")\n\n// ErrNotRetried is returned by functions that retry jobs to indicate that although the redis commands were successful,\n// no object was actually retried by those commmands.\nvar ErrNotRetried = fmt.Errorf(\"nothing retried\")\n\n// Client implements all of the functionality of the web UI. It can be used to inspect the status of a running cluster and retry dead jobs.\ntype Client struct {\n\tnamespace string\n\tpool      *redis.Pool\n}\n\n// NewClient creates a new Client with the specified redis namespace and connection pool.\nfunc NewClient(namespace string, pool *redis.Pool) *Client {\n\treturn &Client{\n\t\tnamespace: namespace,\n\t\tpool:      pool,\n\t}\n}\n\n// WorkerPoolHeartbeat represents the heartbeat from a worker pool. WorkerPool's write a heartbeat every 5 seconds so we know they're alive and includes config information.\ntype WorkerPoolHeartbeat struct {\n\tWorkerPoolID string   `json:\"worker_pool_id\"`\n\tStartedAt    int64    `json:\"started_at\"`\n\tHeartbeatAt  int64    `json:\"heartbeat_at\"`\n\tJobNames     []string `json:\"job_names\"`\n\tConcurrency  uint     `json:\"concurrency\"`\n\tHost         string   `json:\"host\"`\n\tPid          int      `json:\"pid\"`\n\tWorkerIDs    []string `json:\"worker_ids\"`\n}\n\n// WorkerPoolHeartbeats queries Redis and returns all WorkerPoolHeartbeat's it finds (even for those worker pools which don't have a current heartbeat).\nfunc (c *Client) WorkerPoolHeartbeats() ([]*WorkerPoolHeartbeat, error) {\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(c.namespace)\n\n\tworkerPoolIDs, err := redis.Strings(conn.Do(\"SMEMBERS\", workerPoolsKey))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsort.Strings(workerPoolIDs)\n\n\tfor _, wpid := range workerPoolIDs {\n\t\tkey := redisKeyHeartbeat(c.namespace, wpid)\n\t\tconn.Send(\"HGETALL\", key)\n\t}\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"worker_pool_statuses.flush\", err)\n\t\treturn nil, err\n\t}\n\n\theartbeats := make([]*WorkerPoolHeartbeat, 0, len(workerPoolIDs))\n\n\tfor _, wpid := range workerPoolIDs {\n\t\tvals, err := redis.Strings(conn.Receive())\n\t\tif err != nil {\n\t\t\tlogError(\"worker_pool_statuses.receive\", err)\n\t\t\treturn nil, err\n\t\t}\n\n\t\theartbeat := &WorkerPoolHeartbeat{\n\t\t\tWorkerPoolID: wpid,\n\t\t}\n\n\t\tfor i := 0; i < len(vals)-1; i += 2 {\n\t\t\tkey := vals[i]\n\t\t\tvalue := vals[i+1]\n\n\t\t\tvar err error\n\t\t\tif key == \"heartbeat_at\" {\n\t\t\t\theartbeat.HeartbeatAt, err = strconv.ParseInt(value, 10, 64)\n\t\t\t} else if key == \"started_at\" {\n\t\t\t\theartbeat.StartedAt, err = strconv.ParseInt(value, 10, 64)\n\t\t\t} else if key == \"job_names\" {\n\t\t\t\theartbeat.JobNames = strings.Split(value, \",\")\n\t\t\t\tsort.Strings(heartbeat.JobNames)\n\t\t\t} else if key == \"concurrency\" {\n\t\t\t\tvar vv uint64\n\t\t\t\tvv, err = strconv.ParseUint(value, 10, 0)\n\t\t\t\theartbeat.Concurrency = uint(vv)\n\t\t\t} else if key == \"host\" {\n\t\t\t\theartbeat.Host = value\n\t\t\t} else if key == \"pid\" {\n\t\t\t\tvar vv int64\n\t\t\t\tvv, err = strconv.ParseInt(value, 10, 0)\n\t\t\t\theartbeat.Pid = int(vv)\n\t\t\t} else if key == \"worker_ids\" {\n\t\t\t\theartbeat.WorkerIDs = strings.Split(value, \",\")\n\t\t\t\tsort.Strings(heartbeat.WorkerIDs)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"worker_pool_statuses.parse\", err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\theartbeats = append(heartbeats, heartbeat)\n\t}\n\n\treturn heartbeats, nil\n}\n\n// WorkerObservation represents the latest observation taken from a worker. The observation indicates whether the worker is busy processing a job, and if so, information about that job.\ntype WorkerObservation struct {\n\tWorkerID string `json:\"worker_id\"`\n\tIsBusy   bool   `json:\"is_busy\"`\n\n\t// If IsBusy:\n\tJobName   string `json:\"job_name\"`\n\tJobID     string `json:\"job_id\"`\n\tStartedAt int64  `json:\"started_at\"`\n\tArgsJSON  string `json:\"args_json\"`\n\tCheckin   string `json:\"checkin\"`\n\tCheckinAt int64  `json:\"checkin_at\"`\n}\n\n// WorkerObservations returns all of the WorkerObservation's it finds for all worker pools' workers.\nfunc (c *Client) WorkerObservations() ([]*WorkerObservation, error) {\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\thbs, err := c.WorkerPoolHeartbeats()\n\tif err != nil {\n\t\tlogError(\"worker_observations.worker_pool_heartbeats\", err)\n\t\treturn nil, err\n\t}\n\n\tvar workerIDs []string\n\tfor _, hb := range hbs {\n\t\tworkerIDs = append(workerIDs, hb.WorkerIDs...)\n\t}\n\n\tfor _, wid := range workerIDs {\n\t\tkey := redisKeyWorkerObservation(c.namespace, wid)\n\t\tconn.Send(\"HGETALL\", key)\n\t}\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"worker_observations.flush\", err)\n\t\treturn nil, err\n\t}\n\n\tobservations := make([]*WorkerObservation, 0, len(workerIDs))\n\n\tfor _, wid := range workerIDs {\n\t\tvals, err := redis.Strings(conn.Receive())\n\t\tif err != nil {\n\t\t\tlogError(\"worker_observations.receive\", err)\n\t\t\treturn nil, err\n\t\t}\n\n\t\tob := &WorkerObservation{\n\t\t\tWorkerID: wid,\n\t\t}\n\n\t\tfor i := 0; i < len(vals)-1; i += 2 {\n\t\t\tkey := vals[i]\n\t\t\tvalue := vals[i+1]\n\n\t\t\tob.IsBusy = true\n\n\t\t\tvar err error\n\t\t\tif key == \"job_name\" {\n\t\t\t\tob.JobName = value\n\t\t\t} else if key == \"job_id\" {\n\t\t\t\tob.JobID = value\n\t\t\t} else if key == \"started_at\" {\n\t\t\t\tob.StartedAt, err = strconv.ParseInt(value, 10, 64)\n\t\t\t} else if key == \"args\" {\n\t\t\t\tob.ArgsJSON = value\n\t\t\t} else if key == \"checkin\" {\n\t\t\t\tob.Checkin = value\n\t\t\t} else if key == \"checkin_at\" {\n\t\t\t\tob.CheckinAt, err = strconv.ParseInt(value, 10, 64)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"worker_observations.parse\", err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tobservations = append(observations, ob)\n\t}\n\n\treturn observations, nil\n}\n\n// Queue represents a queue that holds jobs with the same name. It indicates their name, count, and latency (in seconds). Latency is a measurement of how long ago the next job to be processed was enqueued.\ntype Queue struct {\n\tJobName string `json:\"job_name\"`\n\tCount   int64  `json:\"count\"`\n\tLatency int64  `json:\"latency\"`\n}\n\n// Queues returns the Queue's it finds.\nfunc (c *Client) Queues() ([]*Queue, error) {\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\tkey := redisKeyKnownJobs(c.namespace)\n\tjobNames, err := redis.Strings(conn.Do(\"SMEMBERS\", key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsort.Strings(jobNames)\n\n\tfor _, jobName := range jobNames {\n\t\tconn.Send(\"LLEN\", redisKeyJobs(c.namespace, jobName))\n\t}\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"client.queues.flush\", err)\n\t\treturn nil, err\n\t}\n\n\tqueues := make([]*Queue, 0, len(jobNames))\n\n\tfor _, jobName := range jobNames {\n\t\tcount, err := redis.Int64(conn.Receive())\n\t\tif err != nil {\n\t\t\tlogError(\"client.queues.receive\", err)\n\t\t\treturn nil, err\n\t\t}\n\n\t\tqueue := &Queue{\n\t\t\tJobName: jobName,\n\t\t\tCount:   count,\n\t\t}\n\n\t\tqueues = append(queues, queue)\n\t}\n\n\tfor _, s := range queues {\n\t\tif s.Count > 0 {\n\t\t\tconn.Send(\"LINDEX\", redisKeyJobs(c.namespace, s.JobName), -1)\n\t\t}\n\t}\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"client.queues.flush2\", err)\n\t\treturn nil, err\n\t}\n\n\tnow := nowEpochSeconds()\n\n\tfor _, s := range queues {\n\t\tif s.Count > 0 {\n\t\t\tb, err := redis.Bytes(conn.Receive())\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"client.queues.receive2\", err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tjob, err := newJob(b, nil, nil)\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"client.queues.new_job\", err)\n\t\t\t}\n\t\t\ts.Latency = now - job.EnqueuedAt\n\t\t}\n\t}\n\n\treturn queues, nil\n}\n\n// RetryJob represents a job in the retry queue.\ntype RetryJob struct {\n\tRetryAt int64 `json:\"retry_at\"`\n\t*Job\n}\n\n// ScheduledJob represents a job in the scheduled queue.\ntype ScheduledJob struct {\n\tRunAt int64 `json:\"run_at\"`\n\t*Job\n}\n\n// DeadJob represents a job in the dead queue.\ntype DeadJob struct {\n\tDiedAt int64 `json:\"died_at\"`\n\t*Job\n}\n\n// ScheduledJobs returns a list of ScheduledJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of scheduled jobs is also returned.\nfunc (c *Client) ScheduledJobs(page uint) ([]*ScheduledJob, int64, error) {\n\tkey := redisKeyScheduled(c.namespace)\n\tjobsWithScores, count, err := c.getZsetPage(key, page)\n\tif err != nil {\n\t\tlogError(\"client.scheduled_jobs.get_zset_page\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tjobs := make([]*ScheduledJob, 0, len(jobsWithScores))\n\n\tfor _, jws := range jobsWithScores {\n\t\tjobs = append(jobs, &ScheduledJob{RunAt: jws.Score, Job: jws.job})\n\t}\n\n\treturn jobs, count, nil\n}\n\n// RetryJobs returns a list of RetryJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of retry jobs is also returned.\nfunc (c *Client) RetryJobs(page uint) ([]*RetryJob, int64, error) {\n\tkey := redisKeyRetry(c.namespace)\n\tjobsWithScores, count, err := c.getZsetPage(key, page)\n\tif err != nil {\n\t\tlogError(\"client.retry_jobs.get_zset_page\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tjobs := make([]*RetryJob, 0, len(jobsWithScores))\n\n\tfor _, jws := range jobsWithScores {\n\t\tjobs = append(jobs, &RetryJob{RetryAt: jws.Score, Job: jws.job})\n\t}\n\n\treturn jobs, count, nil\n}\n\n// DeadJobs returns a list of DeadJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of dead jobs is also returned.\nfunc (c *Client) DeadJobs(page uint) ([]*DeadJob, int64, error) {\n\tkey := redisKeyDead(c.namespace)\n\tjobsWithScores, count, err := c.getZsetPage(key, page)\n\tif err != nil {\n\t\tlogError(\"client.dead_jobs.get_zset_page\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tjobs := make([]*DeadJob, 0, len(jobsWithScores))\n\n\tfor _, jws := range jobsWithScores {\n\t\tjobs = append(jobs, &DeadJob{DiedAt: jws.Score, Job: jws.job})\n\t}\n\n\treturn jobs, count, nil\n}\n\n// DeleteDeadJob deletes a dead job from Redis.\nfunc (c *Client) DeleteDeadJob(diedAt int64, jobID string) error {\n\tok, _, err := c.deleteZsetJob(redisKeyDead(c.namespace), diedAt, jobID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !ok {\n\t\treturn ErrNotDeleted\n\t}\n\treturn nil\n}\n\n// RetryDeadJob retries a dead job. The job will be re-queued on the normal work queue for eventual processing by a worker.\nfunc (c *Client) RetryDeadJob(diedAt int64, jobID string) error {\n\t// Get queues for job names\n\tqueues, err := c.Queues()\n\tif err != nil {\n\t\tlogError(\"client.retry_all_dead_jobs.queues\", err)\n\t\treturn err\n\t}\n\n\t// Extract job names\n\tvar jobNames []string\n\tfor _, q := range queues {\n\t\tjobNames = append(jobNames, q.JobName)\n\t}\n\n\tscript := redis.NewScript(len(jobNames)+1, redisLuaRequeueSingleDeadCmd)\n\n\targs := make([]interface{}, 0, len(jobNames)+1+3)\n\targs = append(args, redisKeyDead(c.namespace)) // KEY[1]\n\tfor _, jobName := range jobNames {\n\t\targs = append(args, redisKeyJobs(c.namespace, jobName)) // KEY[2, 3, ...]\n\t}\n\targs = append(args, redisKeyJobsPrefix(c.namespace)) // ARGV[1]\n\targs = append(args, nowEpochSeconds())\n\targs = append(args, diedAt)\n\targs = append(args, jobID)\n\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\tcnt, err := redis.Int64(script.Do(conn, args...))\n\tif err != nil {\n\t\tlogError(\"client.retry_dead_job.do\", err)\n\t\treturn err\n\t}\n\n\tif cnt == 0 {\n\t\treturn ErrNotRetried\n\t}\n\n\treturn nil\n}\n\n// RetryAllDeadJobs requeues all dead jobs. In other words, it puts them all back on the normal work queue for workers to pull from and process.\nfunc (c *Client) RetryAllDeadJobs() error {\n\t// Get queues for job names\n\tqueues, err := c.Queues()\n\tif err != nil {\n\t\tlogError(\"client.retry_all_dead_jobs.queues\", err)\n\t\treturn err\n\t}\n\n\t// Extract job names\n\tvar jobNames []string\n\tfor _, q := range queues {\n\t\tjobNames = append(jobNames, q.JobName)\n\t}\n\n\tscript := redis.NewScript(len(jobNames)+1, redisLuaRequeueAllDeadCmd)\n\n\targs := make([]interface{}, 0, len(jobNames)+1+3)\n\targs = append(args, redisKeyDead(c.namespace)) // KEY[1]\n\tfor _, jobName := range jobNames {\n\t\targs = append(args, redisKeyJobs(c.namespace, jobName)) // KEY[2, 3, ...]\n\t}\n\targs = append(args, redisKeyJobsPrefix(c.namespace)) // ARGV[1]\n\targs = append(args, nowEpochSeconds())\n\targs = append(args, 1000)\n\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\t// Cap iterations for safety (which could reprocess 1k*1k jobs).\n\t// This is conceptually an infinite loop but let's be careful.\n\tfor i := 0; i < 1000; i++ {\n\t\tres, err := redis.Int64(script.Do(conn, args...))\n\t\tif err != nil {\n\t\t\tlogError(\"client.retry_all_dead_jobs.do\", err)\n\t\t\treturn err\n\t\t}\n\n\t\tif res == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// DeleteAllDeadJobs deletes all dead jobs.\nfunc (c *Client) DeleteAllDeadJobs() error {\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\t_, err := conn.Do(\"DEL\", redisKeyDead(c.namespace))\n\tif err != nil {\n\t\tlogError(\"client.delete_all_dead_jobs\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// DeleteScheduledJob deletes a job in the scheduled queue.\nfunc (c *Client) DeleteScheduledJob(scheduledFor int64, jobID string) error {\n\tok, jobBytes, err := c.deleteZsetJob(redisKeyScheduled(c.namespace), scheduledFor, jobID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If we get a job back, parse it and see if it's a unique job. If it is, we need to delete the unique key.\n\tif len(jobBytes) > 0 {\n\t\tjob, err := newJob(jobBytes, nil, nil)\n\t\tif err != nil {\n\t\t\tlogError(\"client.delete_scheduled_job.new_job\", err)\n\t\t\treturn err\n\t\t}\n\n\t\tif job.Unique {\n\t\t\tuniqueKey, err := redisKeyUniqueJob(c.namespace, job.Name, job.Args)\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"client.delete_scheduled_job.redis_key_unique_job\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tconn := c.pool.Get()\n\t\t\tdefer conn.Close()\n\n\t\t\t_, err = conn.Do(\"DEL\", uniqueKey)\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"worker.delete_unique_job.del\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif !ok {\n\t\treturn ErrNotDeleted\n\t}\n\treturn nil\n}\n\n// DeleteRetryJob deletes a job in the retry queue.\nfunc (c *Client) DeleteRetryJob(retryAt int64, jobID string) error {\n\tok, _, err := c.deleteZsetJob(redisKeyRetry(c.namespace), retryAt, jobID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !ok {\n\t\treturn ErrNotDeleted\n\t}\n\treturn nil\n}\n\n// deleteZsetJob deletes the job in the specified zset (dead, retry, or scheduled queue). zsetKey is like \"work:dead\" or \"work:scheduled\". The function deletes all jobs with the given jobID with the specified zscore (there should only be one, but in theory there could be bad data). It will return if at least one job is deleted and if\nfunc (c *Client) deleteZsetJob(zsetKey string, zscore int64, jobID string) (bool, []byte, error) {\n\tscript := redis.NewScript(1, redisLuaDeleteSingleCmd)\n\n\targs := make([]interface{}, 0, 1+2)\n\targs = append(args, zsetKey) // KEY[1]\n\targs = append(args, zscore)  // ARGV[1]\n\targs = append(args, jobID)   // ARGV[2]\n\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\tvalues, err := redis.Values(script.Do(conn, args...))\n\tif len(values) != 2 {\n\t\treturn false, nil, fmt.Errorf(\"need 2 elements back from redis command\")\n\t}\n\n\tcnt, err := redis.Int64(values[0], err)\n\tjobBytes, err := redis.Bytes(values[1], err)\n\tif err != nil {\n\t\tlogError(\"client.delete_zset_job.do\", err)\n\t\treturn false, nil, err\n\t}\n\n\treturn cnt > 0, jobBytes, nil\n}\n\ntype jobScore struct {\n\tJobBytes []byte\n\tScore    int64\n\tjob      *Job\n}\n\nfunc (c *Client) getZsetPage(key string, page uint) ([]jobScore, int64, error) {\n\tconn := c.pool.Get()\n\tdefer conn.Close()\n\n\tif page == 0 {\n\t\tpage = 1\n\t}\n\n\tvalues, err := redis.Values(conn.Do(\"ZRANGEBYSCORE\", key, \"-inf\", \"+inf\", \"WITHSCORES\", \"LIMIT\", (page-1)*20, 20))\n\tif err != nil {\n\t\tlogError(\"client.get_zset_page.values\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tvar jobsWithScores []jobScore\n\n\tif err := redis.ScanSlice(values, &jobsWithScores); err != nil {\n\t\tlogError(\"client.get_zset_page.scan_slice\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tfor i, jws := range jobsWithScores {\n\t\tjob, err := newJob(jws.JobBytes, nil, nil)\n\t\tif err != nil {\n\t\t\tlogError(\"client.get_zset_page.new_job\", err)\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\tjobsWithScores[i].job = job\n\t}\n\n\tcount, err := redis.Int64(conn.Do(\"ZCARD\", key))\n\tif err != nil {\n\t\tlogError(\"client.get_zset_page.int64\", err)\n\t\treturn nil, 0, err\n\t}\n\n\treturn jobsWithScores, count, nil\n}\n"
        },
        {
          "name": "client_test.go",
          "type": "blob",
          "size": 18.6142578125,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype TestContext struct{}\n\nfunc TestClientWorkerPoolHeartbeats(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(job *Job) error { return nil })\n\twp.Job(\"bob\", func(job *Job) error { return nil })\n\twp.Start()\n\n\twp2 := NewWorkerPool(TestContext{}, 11, ns, pool)\n\twp2.Job(\"foo\", func(job *Job) error { return nil })\n\twp2.Job(\"bar\", func(job *Job) error { return nil })\n\twp2.Start()\n\n\ttime.Sleep(20 * time.Millisecond)\n\n\tclient := NewClient(ns, pool)\n\n\thbs, err := client.WorkerPoolHeartbeats()\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 2, len(hbs))\n\tif len(hbs) == 2 {\n\t\tvar hbwp, hbwp2 *WorkerPoolHeartbeat\n\n\t\tif wp.workerPoolID == hbs[0].WorkerPoolID {\n\t\t\thbwp = hbs[0]\n\t\t\thbwp2 = hbs[1]\n\t\t} else {\n\t\t\thbwp = hbs[1]\n\t\t\thbwp2 = hbs[0]\n\t\t}\n\n\t\tassert.Equal(t, wp.workerPoolID, hbwp.WorkerPoolID)\n\t\tassert.EqualValues(t, uint(10), hbwp.Concurrency)\n\t\tassert.Equal(t, []string{\"bob\", \"wat\"}, hbwp.JobNames)\n\t\tassert.Equal(t, wp.workerIDs(), hbwp.WorkerIDs)\n\n\t\tassert.Equal(t, wp2.workerPoolID, hbwp2.WorkerPoolID)\n\t\tassert.EqualValues(t, uint(11), hbwp2.Concurrency)\n\t\tassert.Equal(t, []string{\"bar\", \"foo\"}, hbwp2.JobNames)\n\t\tassert.Equal(t, wp2.workerIDs(), hbwp2.WorkerIDs)\n\t}\n\n\twp.Stop()\n\twp2.Stop()\n\n\thbs, err = client.WorkerPoolHeartbeats()\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, len(hbs))\n}\n\nfunc TestClientWorkerObservations(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": 2})\n\tassert.Nil(t, err)\n\t_, err = enqueuer.Enqueue(\"foo\", Q{\"a\": 3, \"b\": 4})\n\tassert.Nil(t, err)\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(job *Job) error {\n\t\ttime.Sleep(50 * time.Millisecond)\n\t\treturn nil\n\t})\n\twp.Job(\"foo\", func(job *Job) error {\n\t\ttime.Sleep(50 * time.Millisecond)\n\t\treturn nil\n\t})\n\twp.Start()\n\n\ttime.Sleep(10 * time.Millisecond)\n\n\tclient := NewClient(ns, pool)\n\tobservations, err := client.WorkerObservations()\n\tassert.NoError(t, err)\n\tassert.Equal(t, 10, len(observations))\n\n\twatCount := 0\n\tfooCount := 0\n\tfor _, ob := range observations {\n\t\tif ob.JobName == \"foo\" {\n\t\t\tfooCount++\n\t\t\tassert.True(t, ob.IsBusy)\n\t\t\tassert.Equal(t, `{\"a\":3,\"b\":4}`, ob.ArgsJSON)\n\t\t\tassert.True(t, (nowEpochSeconds()-ob.StartedAt) <= 3)\n\t\t\tassert.True(t, ob.JobID != \"\")\n\t\t} else if ob.JobName == \"wat\" {\n\t\t\twatCount++\n\t\t\tassert.True(t, ob.IsBusy)\n\t\t\tassert.Equal(t, `{\"a\":1,\"b\":2}`, ob.ArgsJSON)\n\t\t\tassert.True(t, (nowEpochSeconds()-ob.StartedAt) <= 3)\n\t\t\tassert.True(t, ob.JobID != \"\")\n\t\t} else {\n\t\t\tassert.False(t, ob.IsBusy)\n\t\t}\n\t\tassert.True(t, ob.WorkerID != \"\")\n\t}\n\tassert.Equal(t, 1, watCount)\n\tassert.Equal(t, 1, fooCount)\n\n\t// time.Sleep(2000 * time.Millisecond)\n\t//\n\t// observations, err = client.WorkerObservations()\n\t// assert.NoError(t, err)\n\t// assert.Equal(t, 10, len(observations))\n\t// for _, ob := range observations {\n\t// \tassert.False(t, ob.IsBusy)\n\t// \tassert.True(t, ob.WorkerID != \"\")\n\t// }\n\n\twp.Stop()\n\n\tobservations, err = client.WorkerObservations()\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, len(observations))\n}\n\nfunc TestClientQueues(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(\"wat\", nil)\n\t_, err = enqueuer.Enqueue(\"foo\", nil)\n\t_, err = enqueuer.Enqueue(\"zaz\", nil)\n\n\t// Start a pool to work on it. It's going to work on the queues\n\t// side effect of that is knowing which jobs are avail\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(job *Job) error {\n\t\treturn nil\n\t})\n\twp.Job(\"foo\", func(job *Job) error {\n\t\treturn nil\n\t})\n\twp.Job(\"zaz\", func(job *Job) error {\n\t\treturn nil\n\t})\n\twp.Start()\n\ttime.Sleep(20 * time.Millisecond)\n\twp.Stop()\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\tenqueuer.Enqueue(\"foo\", nil)\n\tsetNowEpochSecondsMock(1425263509)\n\tenqueuer.Enqueue(\"foo\", nil)\n\tsetNowEpochSecondsMock(1425263609)\n\tenqueuer.Enqueue(\"wat\", nil)\n\n\tsetNowEpochSecondsMock(1425263709)\n\tclient := NewClient(ns, pool)\n\tqueues, err := client.Queues()\n\tassert.NoError(t, err)\n\n\tassert.Equal(t, 3, len(queues))\n\tassert.Equal(t, \"foo\", queues[0].JobName)\n\tassert.EqualValues(t, 2, queues[0].Count)\n\tassert.EqualValues(t, 300, queues[0].Latency)\n\tassert.Equal(t, \"wat\", queues[1].JobName)\n\tassert.EqualValues(t, 1, queues[1].Count)\n\tassert.EqualValues(t, 100, queues[1].Latency)\n\tassert.Equal(t, \"zaz\", queues[2].JobName)\n\tassert.EqualValues(t, 0, queues[2].Count)\n\tassert.EqualValues(t, 0, queues[2].Latency)\n}\n\nfunc TestClientScheduledJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\t_, err := enqueuer.EnqueueIn(\"wat\", 0, Q{\"a\": 1, \"b\": 2})\n\t_, err = enqueuer.EnqueueIn(\"zaz\", 4, Q{\"a\": 3, \"b\": 4})\n\t_, err = enqueuer.EnqueueIn(\"foo\", 2, Q{\"a\": 3, \"b\": 4})\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.ScheduledJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 3, len(jobs))\n\tassert.EqualValues(t, 3, count)\n\tif len(jobs) == 3 {\n\t\tassert.EqualValues(t, 1425263409, jobs[0].RunAt)\n\t\tassert.EqualValues(t, 1425263411, jobs[1].RunAt)\n\t\tassert.EqualValues(t, 1425263413, jobs[2].RunAt)\n\n\t\tassert.Equal(t, \"wat\", jobs[0].Name)\n\t\tassert.Equal(t, \"foo\", jobs[1].Name)\n\t\tassert.Equal(t, \"zaz\", jobs[2].Name)\n\n\t\tassert.EqualValues(t, 1425263409, jobs[0].EnqueuedAt)\n\t\tassert.EqualValues(t, 1425263409, jobs[1].EnqueuedAt)\n\t\tassert.EqualValues(t, 1425263409, jobs[2].EnqueuedAt)\n\n\t\tassert.EqualValues(t, interface{}(1), jobs[0].Args[\"a\"])\n\t\tassert.EqualValues(t, interface{}(2), jobs[0].Args[\"b\"])\n\n\t\tassert.EqualValues(t, 0, jobs[0].Fails)\n\t\tassert.EqualValues(t, 0, jobs[1].Fails)\n\t\tassert.EqualValues(t, 0, jobs[2].Fails)\n\n\t\tassert.EqualValues(t, 0, jobs[0].FailedAt)\n\t\tassert.EqualValues(t, 0, jobs[1].FailedAt)\n\t\tassert.EqualValues(t, 0, jobs[2].FailedAt)\n\n\t\tassert.Equal(t, \"\", jobs[0].LastErr)\n\t\tassert.Equal(t, \"\", jobs[1].LastErr)\n\t\tassert.Equal(t, \"\", jobs[2].LastErr)\n\t}\n}\n\nfunc TestClientRetryJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": 2})\n\tassert.Nil(t, err)\n\n\tsetNowEpochSecondsMock(1425263429)\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(job *Job) error {\n\t\treturn fmt.Errorf(\"ohno\")\n\t})\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.RetryJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, len(jobs))\n\tassert.EqualValues(t, 1, count)\n\n\tif len(jobs) == 1 {\n\t\tassert.EqualValues(t, 1425263429, jobs[0].FailedAt)\n\t\tassert.Equal(t, \"wat\", jobs[0].Name)\n\t\tassert.EqualValues(t, 1425263409, jobs[0].EnqueuedAt)\n\t\tassert.EqualValues(t, interface{}(1), jobs[0].Args[\"a\"])\n\t\tassert.EqualValues(t, 1, jobs[0].Fails)\n\t\tassert.EqualValues(t, 1425263429, jobs[0].Job.FailedAt)\n\t\tassert.Equal(t, \"ohno\", jobs[0].LastErr)\n\t}\n}\n\nfunc TestClientDeadJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": 2})\n\tassert.Nil(t, err)\n\n\tsetNowEpochSecondsMock(1425263429)\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.JobWithOptions(\"wat\", JobOptions{Priority: 1, MaxFails: 1}, func(job *Job) error {\n\t\treturn fmt.Errorf(\"ohno\")\n\t})\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, len(jobs))\n\tassert.EqualValues(t, 1, count)\n\n\tdeadJob := jobs[0]\n\tif len(jobs) == 1 {\n\t\tassert.EqualValues(t, 1425263429, jobs[0].FailedAt)\n\t\tassert.Equal(t, \"wat\", jobs[0].Name)\n\t\tassert.EqualValues(t, 1425263409, jobs[0].EnqueuedAt)\n\t\tassert.EqualValues(t, interface{}(1), jobs[0].Args[\"a\"])\n\t\tassert.EqualValues(t, 1, jobs[0].Fails)\n\t\tassert.EqualValues(t, 1425263429, jobs[0].Job.FailedAt)\n\t\tassert.Equal(t, \"ohno\", jobs[0].LastErr)\n\t}\n\n\t// Test pagination a bit\n\tjobs, count, err = client.DeadJobs(2)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, len(jobs))\n\tassert.EqualValues(t, 1, count)\n\n\t// Delete it!\n\terr = client.DeleteDeadJob(deadJob.DiedAt, deadJob.ID)\n\tassert.NoError(t, err)\n\n\tjobs, count, err = client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, len(jobs))\n\tassert.EqualValues(t, 0, count)\n}\n\nfunc TestClientDeleteDeadJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Insert a dead job:\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12349)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12350)\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 4, len(jobs))\n\tassert.EqualValues(t, 4, count)\n\n\ttot := count\n\tfor _, j := range jobs {\n\t\terr = client.DeleteDeadJob(j.DiedAt, j.ID)\n\t\tassert.NoError(t, err)\n\t\t_, count, err = client.DeadJobs(1)\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, tot-1, count)\n\t\ttot--\n\t}\n\n}\n\nfunc TestClientRetryDeadJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Insert a dead job:\n\tinsertDeadJob(ns, pool, \"wat1\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat2\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat3\", 12345, 12349)\n\tinsertDeadJob(ns, pool, \"wat4\", 12345, 12350)\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 4, len(jobs))\n\tassert.EqualValues(t, 4, count)\n\n\ttot := count\n\tfor _, j := range jobs {\n\t\terr = client.RetryDeadJob(j.DiedAt, j.ID)\n\t\tassert.NoError(t, err)\n\t\t_, count, err = client.DeadJobs(1)\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, tot-1, count)\n\t\ttot--\n\t}\n\n\tjob1 := getQueuedJob(ns, pool, \"wat1\")\n\tassert.NotNil(t, job1)\n\tassert.Equal(t, \"wat1\", job1.Name)\n\tassert.EqualValues(t, 0, job1.Fails)\n\tassert.Equal(t, \"\", job1.LastErr)\n\tassert.EqualValues(t, 0, job1.FailedAt)\n\n\tjob1 = getQueuedJob(ns, pool, \"wat2\")\n\tassert.NotNil(t, job1)\n\tassert.Equal(t, \"wat2\", job1.Name)\n\tassert.EqualValues(t, 0, job1.Fails)\n\tassert.Equal(t, \"\", job1.LastErr)\n\tassert.EqualValues(t, 0, job1.FailedAt)\n\n\tjob1 = getQueuedJob(ns, pool, \"wat3\")\n\tassert.NotNil(t, job1)\n\tassert.Equal(t, \"wat3\", job1.Name)\n\tassert.EqualValues(t, 0, job1.Fails)\n\tassert.Equal(t, \"\", job1.LastErr)\n\tassert.EqualValues(t, 0, job1.FailedAt)\n\n\tjob1 = getQueuedJob(ns, pool, \"wat4\")\n\tassert.NotNil(t, job1)\n\tassert.Equal(t, \"wat4\", job1.Name)\n\tassert.EqualValues(t, 0, job1.Fails)\n\tassert.Equal(t, \"\", job1.LastErr)\n\tassert.EqualValues(t, 0, job1.FailedAt)\n}\n\nfunc TestClientRetryDeadJobWithArgs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Enqueue a job with arguments\n\tname := \"foobar\"\n\tencAt := int64(12345)\n\tfailAt := int64(12347)\n\tjob := &Job{\n\t\tName:       name,\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: encAt,\n\t\tArgs:       map[string]interface{}{\"a\": \"wat\"},\n\t\tFails:      3,\n\t\tLastErr:    \"sorry\",\n\t\tFailedAt:   failAt,\n\t}\n\n\trawJSON, _ := job.serialize()\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\t_, err := conn.Do(\"ZADD\", redisKeyDead(ns), failAt, rawJSON)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tif _, err := conn.Do(\"SADD\", redisKeyKnownJobs(ns), name); err != nil {\n\t\tpanic(err)\n\t}\n\n\tclient := NewClient(ns, pool)\n\terr = client.RetryDeadJob(failAt, job.ID)\n\tassert.NoError(t, err)\n\n\tjob1 := getQueuedJob(ns, pool, name)\n\tif assert.NotNil(t, job1) {\n\t\tassert.Equal(t, name, job1.Name)\n\t\tassert.Equal(t, \"wat\", job1.ArgString(\"a\"))\n\t\tassert.NoError(t, job1.ArgError())\n\t}\n}\n\nfunc TestClientDeleteAllDeadJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Insert a dead job:\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12349)\n\tinsertDeadJob(ns, pool, \"wat\", 12345, 12350)\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 4, len(jobs))\n\tassert.EqualValues(t, 4, count)\n\n\terr = client.DeleteAllDeadJobs()\n\tassert.NoError(t, err)\n\n\tjobs, count, err = client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, len(jobs))\n\tassert.EqualValues(t, 0, count)\n}\n\nfunc TestClientRetryAllDeadJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\n\tinsertDeadJob(ns, pool, \"wat1\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat2\", 12345, 12347)\n\tinsertDeadJob(ns, pool, \"wat3\", 12345, 12349)\n\tinsertDeadJob(ns, pool, \"wat4\", 12345, 12350)\n\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 4, len(jobs))\n\tassert.EqualValues(t, 4, count)\n\n\terr = client.RetryAllDeadJobs()\n\tassert.NoError(t, err)\n\t_, count, err = client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 0, count)\n\n\tjob := getQueuedJob(ns, pool, \"wat1\")\n\tassert.NotNil(t, job)\n\tassert.Equal(t, \"wat1\", job.Name)\n\tassert.EqualValues(t, 1425263409, job.EnqueuedAt)\n\tassert.EqualValues(t, 0, job.Fails)\n\tassert.Equal(t, \"\", job.LastErr)\n\tassert.EqualValues(t, 0, job.FailedAt)\n\n\tjob = getQueuedJob(ns, pool, \"wat2\")\n\tassert.NotNil(t, job)\n\tassert.Equal(t, \"wat2\", job.Name)\n\tassert.EqualValues(t, 1425263409, job.EnqueuedAt)\n\tassert.EqualValues(t, 0, job.Fails)\n\tassert.Equal(t, \"\", job.LastErr)\n\tassert.EqualValues(t, 0, job.FailedAt)\n\n\tjob = getQueuedJob(ns, pool, \"wat3\")\n\tassert.NotNil(t, job)\n\tassert.Equal(t, \"wat3\", job.Name)\n\tassert.EqualValues(t, 1425263409, job.EnqueuedAt)\n\tassert.EqualValues(t, 0, job.Fails)\n\tassert.Equal(t, \"\", job.LastErr)\n\tassert.EqualValues(t, 0, job.FailedAt)\n\n\tjob = getQueuedJob(ns, pool, \"wat4\")\n\tassert.NotNil(t, job)\n\tassert.Equal(t, \"wat4\", job.Name)\n\tassert.EqualValues(t, 1425263409, job.EnqueuedAt)\n\tassert.EqualValues(t, 0, job.Fails)\n\tassert.Equal(t, \"\", job.LastErr)\n\tassert.EqualValues(t, 0, job.FailedAt)\n}\n\nfunc TestClientRetryAllDeadJobsBig(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\t// Ok, we need to efficiently add 10k jobs to the dead queue.\n\t// I tried using insertDeadJob but it was too slow (increased test time by 1 second)\n\tdead := redisKeyDead(ns)\n\tfor i := 0; i < 10000; i++ {\n\t\tjob := &Job{\n\t\t\tName:       \"wat1\",\n\t\t\tID:         makeIdentifier(),\n\t\t\tEnqueuedAt: 12345,\n\t\t\tArgs:       nil,\n\t\t\tFails:      3,\n\t\t\tLastErr:    \"sorry\",\n\t\t\tFailedAt:   12347,\n\t\t}\n\n\t\trawJSON, _ := job.serialize()\n\t\tconn.Send(\"ZADD\", dead, 12347, rawJSON)\n\t}\n\terr := conn.Flush()\n\tassert.NoError(t, err)\n\n\tif _, err := conn.Do(\"SADD\", redisKeyKnownJobs(ns), \"wat1\"); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Add a dead job with a non-existent queue:\n\tjob := &Job{\n\t\tName:       \"dontexist\",\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: 12345,\n\t\tArgs:       nil,\n\t\tFails:      3,\n\t\tLastErr:    \"sorry\",\n\t\tFailedAt:   12347,\n\t}\n\n\trawJSON, _ := job.serialize()\n\n\t_, err = conn.Do(\"ZADD\", dead, 12347, rawJSON)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tclient := NewClient(ns, pool)\n\t_, count, err := client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 10001, count)\n\n\terr = client.RetryAllDeadJobs()\n\tassert.NoError(t, err)\n\t_, count, err = client.DeadJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 1, count) // the funny job that we didn't know how to queue up\n\n\tjobCount := listSize(pool, redisKeyJobs(ns, \"wat1\"))\n\tassert.EqualValues(t, 10000, jobCount)\n\n\t_, job = jobOnZset(pool, dead)\n\tassert.Equal(t, \"dontexist\", job.Name)\n\tassert.Equal(t, \"unknown job when requeueing\", job.LastErr)\n}\n\nfunc TestClientDeleteScheduledJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Delete an invalid job. Make sure we get error\n\tclient := NewClient(ns, pool)\n\terr := client.DeleteScheduledJob(3, \"bob\")\n\tassert.Equal(t, ErrNotDeleted, err)\n\n\t// Schedule a job. Delete it.\n\tenq := NewEnqueuer(ns, pool)\n\tj, err := enq.EnqueueIn(\"foo\", 10, nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, j)\n\n\terr = client.DeleteScheduledJob(j.RunAt, j.ID)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyScheduled(ns)))\n}\n\nfunc TestClientDeleteScheduledUniqueJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\t// Schedule a unique job. Delete it. Ensure we can schedule it again.\n\tenq := NewEnqueuer(ns, pool)\n\tj, err := enq.EnqueueUniqueIn(\"foo\", 10, nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, j)\n\n\tclient := NewClient(ns, pool)\n\terr = client.DeleteScheduledJob(j.RunAt, j.ID)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyScheduled(ns)))\n\n\tj, err = enq.EnqueueUniqueIn(\"foo\", 10, nil) // Can do it again\n\tassert.NoError(t, err)\n\tassert.NotNil(t, j) // Nil? We didn't clear the unique job signature.\n}\n\nfunc TestClientDeleteRetryJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"testwork\"\n\tcleanKeyspace(ns, pool)\n\n\tsetNowEpochSecondsMock(1425263409)\n\tdefer resetNowEpochSecondsMock()\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\tjob, err := enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": 2})\n\tassert.Nil(t, err)\n\n\tsetNowEpochSecondsMock(1425263429)\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(job *Job) error {\n\t\treturn fmt.Errorf(\"ohno\")\n\t})\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n\n\t// Ok so now we have a retry job\n\tclient := NewClient(ns, pool)\n\tjobs, count, err := client.RetryJobs(1)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, len(jobs))\n\tif assert.EqualValues(t, 1, count) {\n\t\terr = client.DeleteRetryJob(jobs[0].RetryAt, job.ID)\n\t\tassert.NoError(t, err)\n\t\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyRetry(ns)))\n\t}\n}\n\nfunc insertDeadJob(ns string, pool *redis.Pool, name string, encAt, failAt int64) *Job {\n\tjob := &Job{\n\t\tName:       name,\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: encAt,\n\t\tArgs:       nil,\n\t\tFails:      3,\n\t\tLastErr:    \"sorry\",\n\t\tFailedAt:   failAt,\n\t}\n\n\trawJSON, _ := job.serialize()\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\t_, err := conn.Do(\"ZADD\", redisKeyDead(ns), failAt, rawJSON)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tif _, err := conn.Do(\"SADD\", redisKeyKnownJobs(ns), name); err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn job\n}\n\nfunc getQueuedJob(ns string, pool *redis.Pool, name string) *Job {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\tjobBytes, err := redis.Bytes(conn.Do(\"RPOP\", redisKeyJobsPrefix(ns)+name))\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tjob, err := newJob(jobBytes, nil, nil)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn job\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "dead_pool_reaper.go",
          "type": "blob",
          "size": 5.0849609375,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\nconst (\n\tdeadTime          = 10 * time.Second // 2 x heartbeat\n\treapPeriod        = 10 * time.Minute\n\treapJitterSecs    = 30\n\trequeueKeysPerJob = 4\n)\n\ntype deadPoolReaper struct {\n\tnamespace   string\n\tpool        *redis.Pool\n\tdeadTime    time.Duration\n\treapPeriod  time.Duration\n\tcurJobTypes []string\n\n\tstopChan         chan struct{}\n\tdoneStoppingChan chan struct{}\n}\n\nfunc newDeadPoolReaper(namespace string, pool *redis.Pool, curJobTypes []string) *deadPoolReaper {\n\treturn &deadPoolReaper{\n\t\tnamespace:        namespace,\n\t\tpool:             pool,\n\t\tdeadTime:         deadTime,\n\t\treapPeriod:       reapPeriod,\n\t\tcurJobTypes:      curJobTypes,\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\t}\n}\n\nfunc (r *deadPoolReaper) start() {\n\tgo r.loop()\n}\n\nfunc (r *deadPoolReaper) stop() {\n\tr.stopChan <- struct{}{}\n\t<-r.doneStoppingChan\n}\n\nfunc (r *deadPoolReaper) loop() {\n\t// Reap immediately after we provide some time for initialization\n\ttimer := time.NewTimer(r.deadTime)\n\tdefer timer.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-r.stopChan:\n\t\t\tr.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t\t// Schedule next occurrence periodically with jitter\n\t\t\ttimer.Reset(r.reapPeriod + time.Duration(rand.Intn(reapJitterSecs))*time.Second)\n\n\t\t\t// Reap\n\t\t\tif err := r.reap(); err != nil {\n\t\t\t\tlogError(\"dead_pool_reaper.reap\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (r *deadPoolReaper) reap() error {\n\t// Get dead pools\n\tdeadPoolIDs, err := r.findDeadPools()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tconn := r.pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(r.namespace)\n\n\t// Cleanup all dead pools\n\tfor deadPoolID, jobTypes := range deadPoolIDs {\n\t\tlockJobTypes := jobTypes\n\t\t// if we found jobs from the heartbeat, requeue them and remove the heartbeat\n\t\tif len(jobTypes) > 0 {\n\t\t\tr.requeueInProgressJobs(deadPoolID, jobTypes)\n\t\t\tif _, err = conn.Do(\"DEL\", redisKeyHeartbeat(r.namespace, deadPoolID)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\t// try to clean up locks for the current set of jobs if heartbeat was not found\n\t\t\tlockJobTypes = r.curJobTypes\n\t\t}\n\t\t// Remove dead pool from worker pools set\n\t\tif _, err = conn.Do(\"SREM\", workerPoolsKey, deadPoolID); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Cleanup any stale lock info\n\t\tif err = r.cleanStaleLockInfo(deadPoolID, lockJobTypes); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (r *deadPoolReaper) cleanStaleLockInfo(poolID string, jobTypes []string) error {\n\tnumKeys := len(jobTypes) * 2\n\tredisReapLocksScript := redis.NewScript(numKeys, redisLuaReapStaleLocks)\n\tvar scriptArgs = make([]interface{}, 0, numKeys+1) // +1 for argv[1]\n\n\tfor _, jobType := range jobTypes {\n\t\tscriptArgs = append(scriptArgs, redisKeyJobsLock(r.namespace, jobType), redisKeyJobsLockInfo(r.namespace, jobType))\n\t}\n\tscriptArgs = append(scriptArgs, poolID) // ARGV[1]\n\n\tconn := r.pool.Get()\n\tdefer conn.Close()\n\tif _, err := redisReapLocksScript.Do(conn, scriptArgs...); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (r *deadPoolReaper) requeueInProgressJobs(poolID string, jobTypes []string) error {\n\tnumKeys := len(jobTypes) * requeueKeysPerJob\n\tredisRequeueScript := redis.NewScript(numKeys, redisLuaReenqueueJob)\n\tvar scriptArgs = make([]interface{}, 0, numKeys+1)\n\n\tfor _, jobType := range jobTypes {\n\t\t// pops from in progress, push into job queue and decrement the queue lock\n\t\tscriptArgs = append(scriptArgs, redisKeyJobsInProgress(r.namespace, poolID, jobType), redisKeyJobs(r.namespace, jobType), redisKeyJobsLock(r.namespace, jobType), redisKeyJobsLockInfo(r.namespace, jobType)) // KEYS[1-4 * N]\n\t}\n\tscriptArgs = append(scriptArgs, poolID) // ARGV[1]\n\n\tconn := r.pool.Get()\n\tdefer conn.Close()\n\n\t// Keep moving jobs until all queues are empty\n\tfor {\n\t\tvalues, err := redis.Values(redisRequeueScript.Do(conn, scriptArgs...))\n\t\tif err == redis.ErrNil {\n\t\t\treturn nil\n\t\t} else if err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif len(values) != 3 {\n\t\t\treturn fmt.Errorf(\"need 3 elements back\")\n\t\t}\n\t}\n}\n\nfunc (r *deadPoolReaper) findDeadPools() (map[string][]string, error) {\n\tconn := r.pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(r.namespace)\n\n\tworkerPoolIDs, err := redis.Strings(conn.Do(\"SMEMBERS\", workerPoolsKey))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdeadPools := map[string][]string{}\n\tfor _, workerPoolID := range workerPoolIDs {\n\t\theartbeatKey := redisKeyHeartbeat(r.namespace, workerPoolID)\n\t\theartbeatAt, err := redis.Int64(conn.Do(\"HGET\", heartbeatKey, \"heartbeat_at\"))\n\t\tif err == redis.ErrNil {\n\t\t\t// heartbeat expired, save dead pool and use cur set of jobs from reaper\n\t\t\tdeadPools[workerPoolID] = []string{}\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Check that last heartbeat was long enough ago to consider the pool dead\n\t\tif time.Unix(heartbeatAt, 0).Add(r.deadTime).After(time.Now()) {\n\t\t\tcontinue\n\t\t}\n\n\t\tjobTypesList, err := redis.String(conn.Do(\"HGET\", heartbeatKey, \"job_names\"))\n\t\tif err == redis.ErrNil {\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdeadPools[workerPoolID] = strings.Split(jobTypesList, \",\")\n\t}\n\n\treturn deadPools, nil\n}\n"
        },
        {
          "name": "dead_pool_reaper_test.go",
          "type": "blob",
          "size": 10.8232421875,
          "content": "package work\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestDeadPoolReaper(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(ns)\n\n\t// Create redis data\n\tvar err error\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"1\")\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"2\")\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"3\")\n\tassert.NoError(t, err)\n\n\terr = conn.Send(\"HMSET\", redisKeyHeartbeat(ns, \"1\"),\n\t\t\"heartbeat_at\", time.Now().Unix(),\n\t\t\"job_names\", \"type1,type2\",\n\t)\n\tassert.NoError(t, err)\n\n\terr = conn.Send(\"HMSET\", redisKeyHeartbeat(ns, \"2\"),\n\t\t\"heartbeat_at\", time.Now().Add(-1*time.Hour).Unix(),\n\t\t\"job_names\", \"type1,type2\",\n\t)\n\tassert.NoError(t, err)\n\n\terr = conn.Send(\"HMSET\", redisKeyHeartbeat(ns, \"3\"),\n\t\t\"heartbeat_at\", time.Now().Add(-1*time.Hour).Unix(),\n\t\t\"job_names\", \"type1,type2\",\n\t)\n\tassert.NoError(t, err)\n\terr = conn.Flush()\n\tassert.NoError(t, err)\n\n\t// Test getting dead pool\n\treaper := newDeadPoolReaper(ns, pool, []string{})\n\tdeadPools, err := reaper.findDeadPools()\n\tassert.NoError(t, err)\n\tassert.Equal(t, map[string][]string{\"2\": {\"type1\", \"type2\"}, \"3\": {\"type1\", \"type2\"}}, deadPools)\n\n\t// Test requeueing jobs\n\t_, err = conn.Do(\"lpush\", redisKeyJobsInProgress(ns, \"2\", \"type1\"), \"foo\")\n\tassert.NoError(t, err)\n\t_, err = conn.Do(\"incr\", redisKeyJobsLock(ns, \"type1\"))\n\tassert.NoError(t, err)\n\t_, err = conn.Do(\"hincrby\", redisKeyJobsLockInfo(ns, \"type1\"), \"2\", 1) // worker pool 2 has lock\n\tassert.NoError(t, err)\n\n\t// Ensure 0 jobs in jobs queue\n\tjobsCount, err := redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Ensure 1 job in inprogress queue\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Reap\n\terr = reaper.reap()\n\tassert.NoError(t, err)\n\n\t// Ensure 1 jobs in jobs queue\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Ensure 0 job in inprogress queue\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Locks should get cleaned up\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, \"type1\")))\n\tv, _ := conn.Do(\"HGET\", redisKeyJobsLockInfo(ns, \"type1\"), \"2\")\n\tassert.Nil(t, v)\n}\n\nfunc TestDeadPoolReaperNoHeartbeat(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(ns)\n\n\t// Create redis data\n\tvar err error\n\tcleanKeyspace(ns, pool)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"1\")\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"2\")\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"3\")\n\tassert.NoError(t, err)\n\t// stale lock info\n\terr = conn.Send(\"SET\", redisKeyJobsLock(ns, \"type1\"), 3)\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", redisKeyJobsLockInfo(ns, \"type1\"), \"1\", 1)\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", redisKeyJobsLockInfo(ns, \"type1\"), \"2\", 1)\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", redisKeyJobsLockInfo(ns, \"type1\"), \"3\", 1)\n\tassert.NoError(t, err)\n\terr = conn.Flush()\n\tassert.NoError(t, err)\n\n\t// make sure test data was created\n\tnumPools, err := redis.Int(conn.Do(\"scard\", workerPoolsKey))\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 3, numPools)\n\n\t// Test getting dead pool ids\n\treaper := newDeadPoolReaper(ns, pool, []string{\"type1\"})\n\tdeadPools, err := reaper.findDeadPools()\n\tassert.NoError(t, err)\n\tassert.Equal(t, map[string][]string{\"1\": {}, \"2\": {}, \"3\": {}}, deadPools)\n\n\t// Test requeueing jobs\n\t_, err = conn.Do(\"lpush\", redisKeyJobsInProgress(ns, \"2\", \"type1\"), \"foo\")\n\tassert.NoError(t, err)\n\n\t// Ensure 0 jobs in jobs queue\n\tjobsCount, err := redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Ensure 1 job in inprogress queue\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Ensure dead worker pools still in the set\n\tjobsCount, err = redis.Int(conn.Do(\"scard\", redisKeyWorkerPools(ns)))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 3, jobsCount)\n\n\t// Reap\n\terr = reaper.reap()\n\tassert.NoError(t, err)\n\n\t// Ensure jobs queue was not altered\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Ensure inprogress queue was not altered\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Ensure dead worker pools were removed from the set\n\tjobsCount, err = redis.Int(conn.Do(\"scard\", redisKeyWorkerPools(ns)))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Stale lock info was cleaned up using reap.curJobTypes\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, \"type1\")))\n\tfor _, poolID := range []string{\"1\", \"2\", \"3\"} {\n\t\tv, _ := conn.Do(\"HGET\", redisKeyJobsLockInfo(ns, \"type1\"), poolID)\n\t\tassert.Nil(t, v)\n\t}\n}\n\nfunc TestDeadPoolReaperNoJobTypes(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(ns)\n\n\t// Create redis data\n\tvar err error\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"1\")\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SADD\", workerPoolsKey, \"2\")\n\tassert.NoError(t, err)\n\n\terr = conn.Send(\"HMSET\", redisKeyHeartbeat(ns, \"1\"),\n\t\t\"heartbeat_at\", time.Now().Add(-1*time.Hour).Unix(),\n\t)\n\tassert.NoError(t, err)\n\n\terr = conn.Send(\"HMSET\", redisKeyHeartbeat(ns, \"2\"),\n\t\t\"heartbeat_at\", time.Now().Add(-1*time.Hour).Unix(),\n\t\t\"job_names\", \"type1,type2\",\n\t)\n\tassert.NoError(t, err)\n\n\terr = conn.Flush()\n\tassert.NoError(t, err)\n\n\t// Test getting dead pool\n\treaper := newDeadPoolReaper(ns, pool, []string{})\n\tdeadPools, err := reaper.findDeadPools()\n\tassert.NoError(t, err)\n\tassert.Equal(t, map[string][]string{\"2\": {\"type1\", \"type2\"}}, deadPools)\n\n\t// Test requeueing jobs\n\t_, err = conn.Do(\"lpush\", redisKeyJobsInProgress(ns, \"1\", \"type1\"), \"foo\")\n\tassert.NoError(t, err)\n\t_, err = conn.Do(\"lpush\", redisKeyJobsInProgress(ns, \"2\", \"type1\"), \"foo\")\n\tassert.NoError(t, err)\n\n\t// Ensure 0 jobs in jobs queue\n\tjobsCount, err := redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n\n\t// Ensure 1 job in inprogress queue for each job\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"1\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Reap. Ensure job 2 is requeued but not job 1\n\terr = reaper.reap()\n\tassert.NoError(t, err)\n\n\t// Ensure 1 jobs in jobs queue\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobs(ns, \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Ensure 1 job in inprogress queue for 1\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"1\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 1, jobsCount)\n\n\t// Ensure 0 jobs in inprogress queue for 2\n\tjobsCount, err = redis.Int(conn.Do(\"llen\", redisKeyJobsInProgress(ns, \"2\", \"type1\")))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, jobsCount)\n}\n\nfunc TestDeadPoolReaperWithWorkerPools(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tstalePoolID := \"aaa\"\n\tcleanKeyspace(ns, pool)\n\t// test vars\n\texpectedDeadTime := 5 * time.Millisecond\n\n\t// create a stale job with a heartbeat\n\tconn := pool.Get()\n\tdefer conn.Close()\n\t_, err := conn.Do(\"SADD\", redisKeyWorkerPools(ns), stalePoolID)\n\tassert.NoError(t, err)\n\t_, err = conn.Do(\"LPUSH\", redisKeyJobsInProgress(ns, stalePoolID, job1), `{\"sleep\": 10}`)\n\tassert.NoError(t, err)\n\tjobTypes := map[string]*jobType{\"job1\": nil}\n\tstaleHeart := newWorkerPoolHeartbeater(ns, pool, stalePoolID, jobTypes, 1, []string{\"id1\"})\n\tstaleHeart.start()\n\n\t// should have 1 stale job and empty job queue\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobsInProgress(ns, stalePoolID, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\n\t// setup a worker pool and start the reaper, which should restart the stale job above\n\twp := setupTestWorkerPool(pool, ns, job1, 1, JobOptions{Priority: 1})\n\twp.deadPoolReaper = newDeadPoolReaper(wp.namespace, wp.pool, []string{\"job1\"})\n\twp.deadPoolReaper.deadTime = expectedDeadTime\n\twp.deadPoolReaper.start()\n\n\t// sleep long enough for staleJob to be considered dead\n\ttime.Sleep(expectedDeadTime * 2)\n\n\t// now we should have 1 job in queue and no more stale jobs\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, wp.workerPoolID, job1)))\n\tstaleHeart.stop()\n\twp.deadPoolReaper.stop()\n}\n\nfunc TestDeadPoolReaperCleanStaleLocks(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\tjob1, job2 := \"type1\", \"type2\"\n\tjobNames := []string{job1, job2}\n\tworkerPoolID1, workerPoolID2 := \"1\", \"2\"\n\tlock1 := redisKeyJobsLock(ns, job1)\n\tlock2 := redisKeyJobsLock(ns, job2)\n\tlockInfo1 := redisKeyJobsLockInfo(ns, job1)\n\tlockInfo2 := redisKeyJobsLockInfo(ns, job2)\n\n\t// Create redis data\n\tvar err error\n\terr = conn.Send(\"SET\", lock1, 3)\n\tassert.NoError(t, err)\n\terr = conn.Send(\"SET\", lock2, 1)\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", lockInfo1, workerPoolID1, 1) // workerPoolID1 holds 1 lock on job1\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", lockInfo1, workerPoolID2, 2) // workerPoolID2 holds 2 locks on job1\n\tassert.NoError(t, err)\n\terr = conn.Send(\"HSET\", lockInfo2, workerPoolID2, 2) // test that we don't go below 0 on job2 lock\n\tassert.NoError(t, err)\n\terr = conn.Flush()\n\tassert.NoError(t, err)\n\n\treaper := newDeadPoolReaper(ns, pool, jobNames)\n\t// clean lock info for workerPoolID1\n\treaper.cleanStaleLockInfo(workerPoolID1, jobNames)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 2, getInt64(pool, lock1))   // job1 lock should be decr by 1\n\tassert.EqualValues(t, 1, getInt64(pool, lock2))   // job2 lock is unchanged\n\tv, _ := conn.Do(\"HGET\", lockInfo1, workerPoolID1) // workerPoolID1 removed from job1's lock info\n\tassert.Nil(t, v)\n\n\t// now clean lock info for workerPoolID2\n\treaper.cleanStaleLockInfo(workerPoolID2, jobNames)\n\tassert.NoError(t, err)\n\t// both locks should be at 0\n\tassert.EqualValues(t, 0, getInt64(pool, lock1))\n\tassert.EqualValues(t, 0, getInt64(pool, lock2))\n\t// worker pool ID 2 removed from both lock info hashes\n\tv, err = conn.Do(\"HGET\", lockInfo1, workerPoolID2)\n\tassert.Nil(t, v)\n\tv, err = conn.Do(\"HGET\", lockInfo2, workerPoolID2)\n\tassert.Nil(t, v)\n}\n"
        },
        {
          "name": "enqueue.go",
          "type": "blob",
          "size": 7.5419921875,
          "content": "package work\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\n// Enqueuer can enqueue jobs.\ntype Enqueuer struct {\n\tNamespace string // eg, \"myapp-work\"\n\tPool      *redis.Pool\n\n\tqueuePrefix           string // eg, \"myapp-work:jobs:\"\n\tknownJobs             map[string]int64\n\tenqueueUniqueScript   *redis.Script\n\tenqueueUniqueInScript *redis.Script\n\tmtx                   sync.RWMutex\n}\n\n// NewEnqueuer creates a new enqueuer with the specified Redis namespace and Redis pool.\nfunc NewEnqueuer(namespace string, pool *redis.Pool) *Enqueuer {\n\tif pool == nil {\n\t\tpanic(\"NewEnqueuer needs a non-nil *redis.Pool\")\n\t}\n\n\treturn &Enqueuer{\n\t\tNamespace:             namespace,\n\t\tPool:                  pool,\n\t\tqueuePrefix:           redisKeyJobsPrefix(namespace),\n\t\tknownJobs:             make(map[string]int64),\n\t\tenqueueUniqueScript:   redis.NewScript(2, redisLuaEnqueueUnique),\n\t\tenqueueUniqueInScript: redis.NewScript(2, redisLuaEnqueueUniqueIn),\n\t}\n}\n\n// Enqueue will enqueue the specified job name and arguments. The args param can be nil if no args ar needed.\n// Example: e.Enqueue(\"send_email\", work.Q{\"addr\": \"test@example.com\"})\nfunc (e *Enqueuer) Enqueue(jobName string, args map[string]interface{}) (*Job, error) {\n\tjob := &Job{\n\t\tName:       jobName,\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: nowEpochSeconds(),\n\t\tArgs:       args,\n\t}\n\n\trawJSON, err := job.serialize()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconn := e.Pool.Get()\n\tdefer conn.Close()\n\n\tif _, err := conn.Do(\"LPUSH\", e.queuePrefix+jobName, rawJSON); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := e.addToKnownJobs(conn, jobName); err != nil {\n\t\treturn job, err\n\t}\n\n\treturn job, nil\n}\n\n// EnqueueIn enqueues a job in the scheduled job queue for execution in secondsFromNow seconds.\nfunc (e *Enqueuer) EnqueueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (*ScheduledJob, error) {\n\tjob := &Job{\n\t\tName:       jobName,\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: nowEpochSeconds(),\n\t\tArgs:       args,\n\t}\n\n\trawJSON, err := job.serialize()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconn := e.Pool.Get()\n\tdefer conn.Close()\n\n\tscheduledJob := &ScheduledJob{\n\t\tRunAt: nowEpochSeconds() + secondsFromNow,\n\t\tJob:   job,\n\t}\n\n\t_, err = conn.Do(\"ZADD\", redisKeyScheduled(e.Namespace), scheduledJob.RunAt, rawJSON)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := e.addToKnownJobs(conn, jobName); err != nil {\n\t\treturn scheduledJob, err\n\t}\n\n\treturn scheduledJob, nil\n}\n\n// EnqueueUnique enqueues a job unless a job is already enqueued with the same name and arguments.\n// The already-enqueued job can be in the normal work queue or in the scheduled job queue.\n// Once a worker begins processing a job, another job with the same name and arguments can be enqueued again.\n// Any failed jobs in the retry queue or dead queue don't count against the uniqueness -- so if a job fails and is retried, two unique jobs with the same name and arguments can be enqueued at once.\n// In order to add robustness to the system, jobs are only unique for 24 hours after they're enqueued. This is mostly relevant for scheduled jobs.\n// EnqueueUnique returns the job if it was enqueued and nil if it wasn't\nfunc (e *Enqueuer) EnqueueUnique(jobName string, args map[string]interface{}) (*Job, error) {\n\treturn e.EnqueueUniqueByKey(jobName, args, nil)\n}\n\n// EnqueueUniqueIn enqueues a unique job in the scheduled job queue for execution in secondsFromNow seconds. See EnqueueUnique for the semantics of unique jobs.\nfunc (e *Enqueuer) EnqueueUniqueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (*ScheduledJob, error) {\n\treturn e.EnqueueUniqueInByKey(jobName, secondsFromNow, args, nil)\n}\n\n// EnqueueUniqueByKey enqueues a job unless a job is already enqueued with the same name and key, updating arguments.\n// The already-enqueued job can be in the normal work queue or in the scheduled job queue.\n// Once a worker begins processing a job, another job with the same name and key can be enqueued again.\n// Any failed jobs in the retry queue or dead queue don't count against the uniqueness -- so if a job fails and is retried, two unique jobs with the same name and arguments can be enqueued at once.\n// In order to add robustness to the system, jobs are only unique for 24 hours after they're enqueued. This is mostly relevant for scheduled jobs.\n// EnqueueUniqueByKey returns the job if it was enqueued and nil if it wasn't\nfunc (e *Enqueuer) EnqueueUniqueByKey(jobName string, args map[string]interface{}, keyMap map[string]interface{}) (*Job, error) {\n\tenqueue, job, err := e.uniqueJobHelper(jobName, args, keyMap)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tres, err := enqueue(nil)\n\n\tif res == \"ok\" && err == nil {\n\t\treturn job, nil\n\t}\n\treturn nil, err\n}\n\n// EnqueueUniqueInByKey enqueues a job in the scheduled job queue that is unique on specified key for execution in secondsFromNow seconds. See EnqueueUnique for the semantics of unique jobs.\n// Subsequent calls with same key will update arguments\nfunc (e *Enqueuer) EnqueueUniqueInByKey(jobName string, secondsFromNow int64, args map[string]interface{}, keyMap map[string]interface{}) (*ScheduledJob, error) {\n\tenqueue, job, err := e.uniqueJobHelper(jobName, args, keyMap)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tscheduledJob := &ScheduledJob{\n\t\tRunAt: nowEpochSeconds() + secondsFromNow,\n\t\tJob:   job,\n\t}\n\n\tres, err := enqueue(&scheduledJob.RunAt)\n\tif res == \"ok\" && err == nil {\n\t\treturn scheduledJob, nil\n\t}\n\treturn nil, err\n}\n\nfunc (e *Enqueuer) addToKnownJobs(conn redis.Conn, jobName string) error {\n\tneedSadd := true\n\tnow := time.Now().Unix()\n\n\te.mtx.RLock()\n\tt, ok := e.knownJobs[jobName]\n\te.mtx.RUnlock()\n\n\tif ok {\n\t\tif now < t {\n\t\t\tneedSadd = false\n\t\t}\n\t}\n\tif needSadd {\n\t\tif _, err := conn.Do(\"SADD\", redisKeyKnownJobs(e.Namespace), jobName); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\te.mtx.Lock()\n\t\te.knownJobs[jobName] = now + 300\n\t\te.mtx.Unlock()\n\t}\n\n\treturn nil\n}\n\ntype enqueueFnType func(*int64) (string, error)\n\nfunc (e *Enqueuer) uniqueJobHelper(jobName string, args map[string]interface{}, keyMap map[string]interface{}) (enqueueFnType, *Job, error) {\n\tuseDefaultKeys := false\n\tif keyMap == nil {\n\t\tuseDefaultKeys = true\n\t\tkeyMap = args\n\t}\n\n\tuniqueKey, err := redisKeyUniqueJob(e.Namespace, jobName, keyMap)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tjob := &Job{\n\t\tName:       jobName,\n\t\tID:         makeIdentifier(),\n\t\tEnqueuedAt: nowEpochSeconds(),\n\t\tArgs:       args,\n\t\tUnique:     true,\n\t\tUniqueKey:  uniqueKey,\n\t}\n\n\trawJSON, err := job.serialize()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tenqueueFn := func(runAt *int64) (string, error) {\n\t\tconn := e.Pool.Get()\n\t\tdefer conn.Close()\n\n\t\tif err := e.addToKnownJobs(conn, jobName); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tscriptArgs := []interface{}{}\n\t\tscript := e.enqueueUniqueScript\n\n\t\tscriptArgs = append(scriptArgs, e.queuePrefix+jobName) // KEY[1]\n\t\tscriptArgs = append(scriptArgs, uniqueKey)             // KEY[2]\n\t\tscriptArgs = append(scriptArgs, rawJSON)               // ARGV[1]\n\t\tif useDefaultKeys {\n\t\t\t// keying on arguments so arguments can't be updated\n\t\t\t// we'll just get them off the original job so to save space, make this \"1\"\n\t\t\tscriptArgs = append(scriptArgs, \"1\") // ARGV[2]\n\t\t} else {\n\t\t\t// we'll use this for updated arguments since the job on the queue\n\t\t\t// doesn't get updated\n\t\t\tscriptArgs = append(scriptArgs, rawJSON) // ARGV[2]\n\t\t}\n\n\t\tif runAt != nil { // Scheduled job so different job queue with additional arg\n\t\t\tscriptArgs[0] = redisKeyScheduled(e.Namespace) // KEY[1]\n\t\t\tscriptArgs = append(scriptArgs, *runAt)        // ARGV[3]\n\n\t\t\tscript = e.enqueueUniqueInScript\n\t\t}\n\n\t\treturn redis.String(script.Do(conn, scriptArgs...))\n\t}\n\n\treturn enqueueFn, job, nil\n}\n"
        },
        {
          "name": "enqueue_test.go",
          "type": "blob",
          "size": 11.6259765625,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestEnqueue(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\tjob, err := enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.Nil(t, err)\n\tassert.Equal(t, \"wat\", job.Name)\n\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\tassert.Equal(t, \"cool\", job.ArgString(\"b\"))\n\tassert.EqualValues(t, 1, job.ArgInt64(\"a\"))\n\tassert.NoError(t, job.ArgError())\n\n\t// Make sure \"wat\" is in the known jobs\n\tassert.EqualValues(t, []string{\"wat\"}, knownJobs(pool, redisKeyKnownJobs(ns)))\n\n\t// Make sure the cache is set\n\texpiresAt := enqueuer.knownJobs[\"wat\"]\n\tassert.True(t, expiresAt > (time.Now().Unix()+290))\n\n\t// Make sure the length of the queue is 1\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobs(ns, \"wat\")))\n\n\t// Get the job\n\tj := jobOnQueue(pool, redisKeyJobs(ns, \"wat\"))\n\tassert.Equal(t, \"wat\", j.Name)\n\tassert.True(t, len(j.ID) > 10)                        // Something is in it\n\tassert.True(t, j.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\tassert.True(t, j.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\tassert.Equal(t, \"cool\", j.ArgString(\"b\"))\n\tassert.EqualValues(t, 1, j.ArgInt64(\"a\"))\n\tassert.NoError(t, j.ArgError())\n\n\t// Now enqueue another job, make sure that we can enqueue multiple\n\t_, err = enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\t_, err = enqueuer.Enqueue(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.Nil(t, err)\n\tassert.EqualValues(t, 2, listSize(pool, redisKeyJobs(ns, \"wat\")))\n}\n\nfunc TestEnqueueIn(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\n\t// Set to expired value to make sure we update the set of known jobs\n\tenqueuer.knownJobs[\"wat\"] = 4\n\n\tjob, err := enqueuer.EnqueueIn(\"wat\", 300, Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.Nil(t, err)\n\tif assert.NotNil(t, job) {\n\t\tassert.Equal(t, \"wat\", job.Name)\n\t\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\t\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\t\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\t\tassert.Equal(t, \"cool\", job.ArgString(\"b\"))\n\t\tassert.EqualValues(t, 1, job.ArgInt64(\"a\"))\n\t\tassert.NoError(t, job.ArgError())\n\t\tassert.EqualValues(t, job.EnqueuedAt+300, job.RunAt)\n\t}\n\n\t// Make sure \"wat\" is in the known jobs\n\tassert.EqualValues(t, []string{\"wat\"}, knownJobs(pool, redisKeyKnownJobs(ns)))\n\n\t// Make sure the cache is set\n\texpiresAt := enqueuer.knownJobs[\"wat\"]\n\tassert.True(t, expiresAt > (time.Now().Unix()+290))\n\n\t// Make sure the length of the scheduled job queue is 1\n\tassert.EqualValues(t, 1, zsetSize(pool, redisKeyScheduled(ns)))\n\n\t// Get the job\n\tscore, j := jobOnZset(pool, redisKeyScheduled(ns))\n\n\tassert.True(t, score > time.Now().Unix()+290)\n\tassert.True(t, score <= time.Now().Unix()+300)\n\n\tassert.Equal(t, \"wat\", j.Name)\n\tassert.True(t, len(j.ID) > 10)                        // Something is in it\n\tassert.True(t, j.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\tassert.True(t, j.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\tassert.Equal(t, \"cool\", j.ArgString(\"b\"))\n\tassert.EqualValues(t, 1, j.ArgInt64(\"a\"))\n\tassert.NoError(t, j.ArgError())\n}\n\nfunc TestEnqueueUnique(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\tvar mutex = &sync.Mutex{}\n\tjob, err := enqueuer.EnqueueUnique(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.NoError(t, err)\n\tif assert.NotNil(t, job) {\n\t\tassert.Equal(t, \"wat\", job.Name)\n\t\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\t\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\t\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\t\tassert.Equal(t, \"cool\", job.ArgString(\"b\"))\n\t\tassert.EqualValues(t, 1, job.ArgInt64(\"a\"))\n\t\tassert.NoError(t, job.ArgError())\n\t}\n\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", Q{\"a\": 1, \"b\": \"coolio\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", nil)\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\tjob, err = enqueuer.EnqueueUnique(\"taw\", nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\t// Process the queues. Ensure the right number of jobs were processed\n\tvar wats, taws int64\n\twp := NewWorkerPool(TestContext{}, 3, ns, pool)\n\twp.JobWithOptions(\"wat\", JobOptions{Priority: 1, MaxFails: 1}, func(job *Job) error {\n\t\tmutex.Lock()\n\t\twats++\n\t\tmutex.Unlock()\n\t\treturn nil\n\t})\n\twp.JobWithOptions(\"taw\", JobOptions{Priority: 1, MaxFails: 1}, func(job *Job) error {\n\t\tmutex.Lock()\n\t\ttaws++\n\t\tmutex.Unlock()\n\t\treturn fmt.Errorf(\"ohno\")\n\t})\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n\n\tassert.EqualValues(t, 3, wats)\n\tassert.EqualValues(t, 1, taws)\n\n\t// Enqueue again. Ensure we can.\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUnique(\"wat\", Q{\"a\": 1, \"b\": \"coolio\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\t// Even though taw resulted in an error, we should still be able to re-queue it.\n\t// This could result in multiple taws enqueued at the same time in a production system.\n\tjob, err = enqueuer.EnqueueUnique(\"taw\", nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n}\n\nfunc TestEnqueueUniqueIn(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\n\t// Enqueue two unique jobs -- ensure one job sticks.\n\tjob, err := enqueuer.EnqueueUniqueIn(\"wat\", 300, Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.NoError(t, err)\n\tif assert.NotNil(t, job) {\n\t\tassert.Equal(t, \"wat\", job.Name)\n\t\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\t\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\t\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\t\tassert.Equal(t, \"cool\", job.ArgString(\"b\"))\n\t\tassert.EqualValues(t, 1, job.ArgInt64(\"a\"))\n\t\tassert.NoError(t, job.ArgError())\n\t\tassert.EqualValues(t, job.EnqueuedAt+300, job.RunAt)\n\t}\n\n\tjob, err = enqueuer.EnqueueUniqueIn(\"wat\", 10, Q{\"a\": 1, \"b\": \"cool\"})\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\t// Get the job\n\tscore, j := jobOnZset(pool, redisKeyScheduled(ns))\n\n\tassert.True(t, score > time.Now().Unix()+290) // We don't want to overwrite the time\n\tassert.True(t, score <= time.Now().Unix()+300)\n\n\tassert.Equal(t, \"wat\", j.Name)\n\tassert.True(t, len(j.ID) > 10)                        // Something is in it\n\tassert.True(t, j.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\tassert.True(t, j.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\tassert.Equal(t, \"cool\", j.ArgString(\"b\"))\n\tassert.EqualValues(t, 1, j.ArgInt64(\"a\"))\n\tassert.NoError(t, j.ArgError())\n\tassert.True(t, j.Unique)\n\n\t// Now try to enqueue more stuff and ensure it\n\tjob, err = enqueuer.EnqueueUniqueIn(\"wat\", 300, Q{\"a\": 1, \"b\": \"coolio\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueIn(\"wat\", 300, nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueIn(\"wat\", 300, nil)\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueIn(\"taw\", 300, nil)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n}\n\nfunc TestEnqueueUniqueByKey(t *testing.T) {\n\tvar arg3 string\n\tvar arg4 string\n\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\tvar mutex = &sync.Mutex{}\n\tjob, err := enqueuer.EnqueueUniqueByKey(\"wat\", Q{\"a\": 3, \"b\": \"foo\"}, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tif assert.NotNil(t, job) {\n\t\tassert.Equal(t, \"wat\", job.Name)\n\t\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\t\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\t\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\t\tassert.Equal(t, \"foo\", job.ArgString(\"b\"))\n\t\tassert.EqualValues(t, 3, job.ArgInt64(\"a\"))\n\t\tassert.NoError(t, job.ArgError())\n\t}\n\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"wat\", Q{\"a\": 3, \"b\": \"bar\"}, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"wat\", Q{\"a\": 4, \"b\": \"baz\"}, Q{\"key\": \"124\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"taw\", nil, Q{\"key\": \"125\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\t// Process the queues. Ensure the right number of jobs were processed\n\tvar wats, taws int64\n\twp := NewWorkerPool(TestContext{}, 3, ns, pool)\n\twp.JobWithOptions(\"wat\", JobOptions{Priority: 1, MaxFails: 1}, func(job *Job) error {\n\t\tmutex.Lock()\n\t\targA := job.Args[\"a\"].(float64)\n\t\targB := job.Args[\"b\"].(string)\n\t\tif argA == 3 {\n\t\t\targ3 = argB\n\t\t}\n\t\tif argA == 4 {\n\t\t\targ4 = argB\n\t\t}\n\n\t\twats++\n\t\tmutex.Unlock()\n\t\treturn nil\n\t})\n\twp.JobWithOptions(\"taw\", JobOptions{Priority: 1, MaxFails: 1}, func(job *Job) error {\n\t\tmutex.Lock()\n\t\ttaws++\n\t\tmutex.Unlock()\n\t\treturn fmt.Errorf(\"ohno\")\n\t})\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n\n\tassert.EqualValues(t, 2, wats)\n\tassert.EqualValues(t, 1, taws)\n\n\t// Check that arguments got updated to new value\n\tassert.EqualValues(t, \"bar\", arg3)\n\tassert.EqualValues(t, \"baz\", arg4)\n\n\t// Enqueue again. Ensure we can.\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"wat\", Q{\"a\": 1, \"b\": \"cool\"}, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"wat\", Q{\"a\": 1, \"b\": \"coolio\"}, Q{\"key\": \"124\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n\n\t// Even though taw resulted in an error, we should still be able to re-queue it.\n\t// This could result in multiple taws enqueued at the same time in a production system.\n\tjob, err = enqueuer.EnqueueUniqueByKey(\"taw\", nil, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tassert.NotNil(t, job)\n}\n\nfunc EnqueueUniqueInByKey(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\n\t// Enqueue two unique jobs -- ensure one job sticks.\n\tjob, err := enqueuer.EnqueueUniqueInByKey(\"wat\", 300, Q{\"a\": 1, \"b\": \"cool\"}, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tif assert.NotNil(t, job) {\n\t\tassert.Equal(t, \"wat\", job.Name)\n\t\tassert.True(t, len(job.ID) > 10)                        // Something is in it\n\t\tassert.True(t, job.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\t\tassert.True(t, job.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\t\tassert.Equal(t, \"cool\", job.ArgString(\"b\"))\n\t\tassert.EqualValues(t, 1, job.ArgInt64(\"a\"))\n\t\tassert.NoError(t, job.ArgError())\n\t\tassert.EqualValues(t, job.EnqueuedAt+300, job.RunAt)\n\t}\n\n\tjob, err = enqueuer.EnqueueUniqueInByKey(\"wat\", 10, Q{\"a\": 1, \"b\": \"cool\"}, Q{\"key\": \"123\"})\n\tassert.NoError(t, err)\n\tassert.Nil(t, job)\n\n\t// Get the job\n\tscore, j := jobOnZset(pool, redisKeyScheduled(ns))\n\n\tassert.True(t, score > time.Now().Unix()+290) // We don't want to overwrite the time\n\tassert.True(t, score <= time.Now().Unix()+300)\n\n\tassert.Equal(t, \"wat\", j.Name)\n\tassert.True(t, len(j.ID) > 10)                        // Something is in it\n\tassert.True(t, j.EnqueuedAt > (time.Now().Unix()-10)) // Within 10 seconds\n\tassert.True(t, j.EnqueuedAt < (time.Now().Unix()+10)) // Within 10 seconds\n\tassert.Equal(t, \"cool\", j.ArgString(\"b\"))\n\tassert.EqualValues(t, 1, j.ArgInt64(\"a\"))\n\tassert.NoError(t, j.ArgError())\n\tassert.True(t, j.Unique)\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.2998046875,
          "content": "module github.com/gocraft/work\n\ngo 1.14\n\nrequire (\n\tgithub.com/albrow/jobs v0.4.2\n\tgithub.com/benmanns/goworker v0.1.3\n\tgithub.com/bitly/go-simplejson v0.5.0 // indirect\n\tgithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869 // indirect\n\tgithub.com/braintree/manners v0.0.0-20160418043613-82a8879fc5fd\n\tgithub.com/cihub/seelog v0.0.0-20170130134532-f561c5e57575 // indirect\n\tgithub.com/customerio/gospec v0.0.0-20130710230057-a5cc0e48aa39 // indirect\n\tgithub.com/dchest/uniuri v0.0.0-20200228104902-7aecb25e1fe5 // indirect\n\tgithub.com/dustin/go-humanize v1.0.0 // indirect\n\tgithub.com/garyburd/redigo v1.6.0 // indirect\n\tgithub.com/gocraft/health v0.0.0-20170925182251-8675af27fef0\n\tgithub.com/gocraft/web v0.0.0-20190207150652-9707327fb69b\n\tgithub.com/gocraft/work v0.5.1\n\tgithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b // indirect\n\tgithub.com/gomodule/redigo v2.0.0+incompatible\n\tgithub.com/jrallison/go-workers v0.0.0-20180112190529-dbf81d0b75bb\n\tgithub.com/kr/pretty v0.2.0 // indirect\n\tgithub.com/orfjackal/nanospec.go v0.0.0-20120727230329-de4694c1d701 // indirect\n\tgithub.com/robfig/cron v1.2.0 // indirect\n\tgithub.com/robfig/cron/v3 v3.0.1\n\tgithub.com/stretchr/testify v1.5.1\n\tgithub.com/youtube/vitess v2.1.1+incompatible // indirect\n\tgolang.org/x/net v0.0.0-20200425230154-ff2c4b7c35a0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 5.8603515625,
          "content": "github.com/albrow/jobs v0.4.2 h1:AhhNNgtnOz3h+Grt6uuRJP+uj/AVq+ZhIBY8Mzkf4TM=\ngithub.com/albrow/jobs v0.4.2/go.mod h1:e4sWh7D1DxPbpxrzJhNo/cMARAljpTYF/osgh2j3+r8=\ngithub.com/benmanns/goworker v0.1.3 h1:ekwn7WiKsn8oUOKfbHDqsA6g5bXz/uEZ9AdnKgtAECY=\ngithub.com/benmanns/goworker v0.1.3/go.mod h1:Gj3m7lTyCswE3+Kta7c79CMOmm5rHJmj2qh/GAmojJ4=\ngithub.com/bitly/go-simplejson v0.5.0 h1:6IH+V8/tVMab511d5bn4M7EwGXZf9Hj6i2xSwkNEM+Y=\ngithub.com/bitly/go-simplejson v0.5.0/go.mod h1:cXHtHw4XUPsvGaxgjIAn8PhEWG9NfngEKAMDJEczWVA=\ngithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869 h1:DDGfHa7BWjL4YnC6+E63dPcxHo2sUxDIu8g3QgEJdRY=\ngithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869/go.mod h1:Ekp36dRnpXw/yCqJaO+ZrUyxD+3VXMFFr56k5XYrpB4=\ngithub.com/braintree/manners v0.0.0-20160418043613-82a8879fc5fd h1:ePesaBzdTmoMQjwqRCLP2jY+jjWMBpwws/LEQdt1fMM=\ngithub.com/braintree/manners v0.0.0-20160418043613-82a8879fc5fd/go.mod h1:TNehV1AhBwtT7Bd+rh8G6MoGDbBLNs/sKdk3nvr4Yzg=\ngithub.com/cihub/seelog v0.0.0-20170130134532-f561c5e57575 h1:kHaBemcxl8o/pQ5VM1c8PVE1PubbNx3mjUr09OqWGCs=\ngithub.com/cihub/seelog v0.0.0-20170130134532-f561c5e57575/go.mod h1:9d6lWj8KzO/fd/NrVaLscBKmPigpZpn5YawRPw+e3Yo=\ngithub.com/customerio/gospec v0.0.0-20130710230057-a5cc0e48aa39 h1:O0YTztXI3XeJXlFhSo4wNb0VBVqSgT+hi/CjNWKvMnY=\ngithub.com/customerio/gospec v0.0.0-20130710230057-a5cc0e48aa39/go.mod h1:OzYUFhPuL2JbjwFwrv6CZs23uBawekc6OZs+g19F0mY=\ngithub.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dchest/uniuri v0.0.0-20200228104902-7aecb25e1fe5 h1:RAV05c0xOkJ3dZGS0JFybxFKZ2WMLabgx3uXnd7rpGs=\ngithub.com/dchest/uniuri v0.0.0-20200228104902-7aecb25e1fe5/go.mod h1:GgB8SF9nRG+GqaDtLcwJZsQFhcogVCJ79j4EdT0c2V4=\ngithub.com/dustin/go-humanize v1.0.0 h1:VSnTsYCnlFHaM2/igO1h6X3HA71jcobQuxemgkq4zYo=\ngithub.com/dustin/go-humanize v1.0.0/go.mod h1:HtrtbFcZ19U5GC7JDqmcUSB87Iq5E25KnS6fMYU6eOk=\ngithub.com/garyburd/redigo v1.6.0 h1:0VruCpn7yAIIu7pWVClQC8wxCJEcG3nyzpMSHKi1PQc=\ngithub.com/garyburd/redigo v1.6.0/go.mod h1:NR3MbYisc3/PwhQ00EMzDiPmrwpPxAn5GI05/YaO1SY=\ngithub.com/gocraft/health v0.0.0-20170925182251-8675af27fef0 h1:pKjeDsx7HGGbjr7VGI1HksxDJqSjaGED3cSw9GeSI98=\ngithub.com/gocraft/health v0.0.0-20170925182251-8675af27fef0/go.mod h1:rWibcVfwbUxi/QXW84U7vNTcIcZFd6miwbt8ritxh/Y=\ngithub.com/gocraft/web v0.0.0-20190207150652-9707327fb69b h1:g2Qcs0B+vOQE1L3a7WQ/JUUSzJnHbTz14qkJSqEWcF4=\ngithub.com/gocraft/web v0.0.0-20190207150652-9707327fb69b/go.mod h1:Ag7UMbZNGrnHwaXPJOUKJIVgx4QOWMOWZngrvsN6qak=\ngithub.com/gocraft/work v0.5.1 h1:3bRjMiOo6N4zcRgZWV3Y7uX7R22SF+A9bPTk4xRXr34=\ngithub.com/gocraft/work v0.5.1/go.mod h1:pc3n9Pb5FAESPPGfM0nL+7Q1xtgtRnF8rr/azzhQVlM=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b h1:VKtxabqXZkF25pY9ekfRL6a582T4P37/31XEstQ5p58=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/gomodule/redigo v2.0.0+incompatible h1:K/R+8tc58AaqLkqG2Ol3Qk+DR/TlNuhuh457pBFPtt0=\ngithub.com/gomodule/redigo v2.0.0+incompatible/go.mod h1:B4C85qUVwatsJoIUNIfCRsp7qO0iAmpGFZ4EELWSbC4=\ngithub.com/jrallison/go-workers v0.0.0-20180112190529-dbf81d0b75bb h1:y9LFhCM3gwK94Xz9/h7GcSVLteky9pFHEkP04AqQupA=\ngithub.com/jrallison/go-workers v0.0.0-20180112190529-dbf81d0b75bb/go.mod h1:ziQRRNHCWZe0wVNzF8y8kCWpso0VMpqHJjB19DSenbE=\ngithub.com/kr/pretty v0.2.0 h1:s5hAObm+yFO5uHYt5dYjxi2rXrsnmRpJx4OYvIWUaQs=\ngithub.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/orfjackal/nanospec.go v0.0.0-20120727230329-de4694c1d701 h1:yOXfzNV7qkZ3nf2NPqy4BMzlCmnQzIEbI1vuqKb2FkQ=\ngithub.com/orfjackal/nanospec.go v0.0.0-20120727230329-de4694c1d701/go.mod h1:VtBIF1XX0c1nKkeAPk8i4aXkYopqQgfDqolHUIHPwNI=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/robfig/cron v1.2.0 h1:ZjScXvvxeQ63Dbyxy76Fj3AT3Ut0aKsyd2/tl3DTMuQ=\ngithub.com/robfig/cron v1.2.0/go.mod h1:JGuDeoQd7Z6yL4zQhZ3OPEVHB7fL6Ka6skscFHfmt2k=\ngithub.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=\ngithub.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/youtube/vitess v2.1.1+incompatible h1:SE+P7DNX/jw5RHFs5CHRhZQjq402EJFCD33JhzQMdDw=\ngithub.com/youtube/vitess v2.1.1+incompatible/go.mod h1:hpMim5/30F1r+0P8GGtB29d0gWHr0IZ5unS+CG0zMx8=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/net v0.0.0-20200425230154-ff2c4b7c35a0 h1:Jcxah/M+oLZ/R4/z5RzfPzGbPXnVDPkEDtf2JnuxN+U=\ngolang.org/x/net v0.0.0-20200425230154-ff2c4b7c35a0/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n"
        },
        {
          "name": "heartbeater.go",
          "type": "blob",
          "size": 2.66015625,
          "content": "package work\n\nimport (\n\t\"os\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\nconst (\n\tbeatPeriod = 5 * time.Second\n)\n\ntype workerPoolHeartbeater struct {\n\tworkerPoolID string\n\tnamespace    string // eg, \"myapp-work\"\n\tpool         *redis.Pool\n\tbeatPeriod   time.Duration\n\tconcurrency  uint\n\tjobNames     string\n\tstartedAt    int64\n\tpid          int\n\thostname     string\n\tworkerIDs    string\n\n\tstopChan         chan struct{}\n\tdoneStoppingChan chan struct{}\n}\n\nfunc newWorkerPoolHeartbeater(namespace string, pool *redis.Pool, workerPoolID string, jobTypes map[string]*jobType, concurrency uint, workerIDs []string) *workerPoolHeartbeater {\n\th := &workerPoolHeartbeater{\n\t\tworkerPoolID:     workerPoolID,\n\t\tnamespace:        namespace,\n\t\tpool:             pool,\n\t\tbeatPeriod:       beatPeriod,\n\t\tconcurrency:      concurrency,\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\t}\n\n\tjobNames := make([]string, 0, len(jobTypes))\n\tfor k := range jobTypes {\n\t\tjobNames = append(jobNames, k)\n\t}\n\tsort.Strings(jobNames)\n\th.jobNames = strings.Join(jobNames, \",\")\n\n\tsort.Strings(workerIDs)\n\th.workerIDs = strings.Join(workerIDs, \",\")\n\n\th.pid = os.Getpid()\n\thost, err := os.Hostname()\n\tif err != nil {\n\t\tlogError(\"heartbeat.hostname\", err)\n\t\thost = \"hostname_errored\"\n\t}\n\th.hostname = host\n\n\treturn h\n}\n\nfunc (h *workerPoolHeartbeater) start() {\n\tgo h.loop()\n}\n\nfunc (h *workerPoolHeartbeater) stop() {\n\th.stopChan <- struct{}{}\n\t<-h.doneStoppingChan\n}\n\nfunc (h *workerPoolHeartbeater) loop() {\n\th.startedAt = nowEpochSeconds()\n\th.heartbeat() // do it right away\n\tticker := time.Tick(h.beatPeriod)\n\tfor {\n\t\tselect {\n\t\tcase <-h.stopChan:\n\t\t\th.removeHeartbeat()\n\t\t\th.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-ticker:\n\t\t\th.heartbeat()\n\t\t}\n\t}\n}\n\nfunc (h *workerPoolHeartbeater) heartbeat() {\n\tconn := h.pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(h.namespace)\n\theartbeatKey := redisKeyHeartbeat(h.namespace, h.workerPoolID)\n\n\tconn.Send(\"SADD\", workerPoolsKey, h.workerPoolID)\n\tconn.Send(\"HMSET\", heartbeatKey,\n\t\t\"heartbeat_at\", nowEpochSeconds(),\n\t\t\"started_at\", h.startedAt,\n\t\t\"job_names\", h.jobNames,\n\t\t\"concurrency\", h.concurrency,\n\t\t\"worker_ids\", h.workerIDs,\n\t\t\"host\", h.hostname,\n\t\t\"pid\", h.pid,\n\t)\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"heartbeat\", err)\n\t}\n}\n\nfunc (h *workerPoolHeartbeater) removeHeartbeat() {\n\tconn := h.pool.Get()\n\tdefer conn.Close()\n\n\tworkerPoolsKey := redisKeyWorkerPools(h.namespace)\n\theartbeatKey := redisKeyHeartbeat(h.namespace, h.workerPoolID)\n\n\tconn.Send(\"SREM\", workerPoolsKey, h.workerPoolID)\n\tconn.Send(\"DEL\", heartbeatKey)\n\n\tif err := conn.Flush(); err != nil {\n\t\tlogError(\"remove_heartbeat\", err)\n\t}\n}\n"
        },
        {
          "name": "heartbeater_test.go",
          "type": "blob",
          "size": 1.228515625,
          "content": "package work\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestHeartbeater(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\ttMock := int64(1425263409)\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\n\tjobTypes := map[string]*jobType{\n\t\t\"foo\": nil,\n\t\t\"bar\": nil,\n\t}\n\n\theart := newWorkerPoolHeartbeater(ns, pool, \"abcd\", jobTypes, 10, []string{\"ccc\", \"bbb\"})\n\theart.start()\n\n\ttime.Sleep(20 * time.Millisecond)\n\n\tassert.True(t, redisInSet(pool, redisKeyWorkerPools(ns), \"abcd\"))\n\n\th := readHash(pool, redisKeyHeartbeat(ns, \"abcd\"))\n\tassert.Equal(t, \"1425263409\", h[\"heartbeat_at\"])\n\tassert.Equal(t, \"1425263409\", h[\"started_at\"])\n\tassert.Equal(t, \"bar,foo\", h[\"job_names\"])\n\tassert.Equal(t, \"bbb,ccc\", h[\"worker_ids\"])\n\tassert.Equal(t, \"10\", h[\"concurrency\"])\n\n\tassert.True(t, h[\"pid\"] != \"\")\n\tassert.True(t, h[\"host\"] != \"\")\n\n\theart.stop()\n\n\tassert.False(t, redisInSet(pool, redisKeyWorkerPools(ns), \"abcd\"))\n}\n\nfunc redisInSet(pool *redis.Pool, key, member string) bool {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Bool(conn.Do(\"SISMEMBER\", key, member))\n\tif err != nil {\n\t\tpanic(\"could not delete retry/dead queue: \" + err.Error())\n\t}\n\treturn v\n}\n"
        },
        {
          "name": "identifier.go",
          "type": "blob",
          "size": 0.205078125,
          "content": "package work\n\nimport (\n\t\"crypto/rand\"\n\t\"fmt\"\n\t\"io\"\n)\n\nfunc makeIdentifier() string {\n\tb := make([]byte, 12)\n\t_, err := io.ReadFull(rand.Reader, b)\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\treturn fmt.Sprintf(\"%x\", b)\n}\n"
        },
        {
          "name": "identifier_test.go",
          "type": "blob",
          "size": 0.169921875,
          "content": "package work\n\nimport \"testing\"\n\nfunc TestMakeIdentifier(t *testing.T) {\n\tid := makeIdentifier()\n\tif len(id) < 10 {\n\t\tt.Errorf(\"expected a string of length 10 at least\")\n\t}\n}\n"
        },
        {
          "name": "job.go",
          "type": "blob",
          "size": 5.5771484375,
          "content": "package work\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"reflect\"\n)\n\n// Job represents a job.\ntype Job struct {\n\t// Inputs when making a new job\n\tName       string                 `json:\"name,omitempty\"`\n\tID         string                 `json:\"id\"`\n\tEnqueuedAt int64                  `json:\"t\"`\n\tArgs       map[string]interface{} `json:\"args\"`\n\tUnique     bool                   `json:\"unique,omitempty\"`\n\tUniqueKey  string                 `json:\"unique_key,omitempty\"`\n\n\t// Inputs when retrying\n\tFails    int64  `json:\"fails,omitempty\"` // number of times this job has failed\n\tLastErr  string `json:\"err,omitempty\"`\n\tFailedAt int64  `json:\"failed_at,omitempty\"`\n\n\trawJSON      []byte\n\tdequeuedFrom []byte\n\tinProgQueue  []byte\n\targError     error\n\tobserver     *observer\n}\n\n// Q is a shortcut to easily specify arguments for jobs when enqueueing them.\n// Example: e.Enqueue(\"send_email\", work.Q{\"addr\": \"test@example.com\", \"track\": true})\ntype Q map[string]interface{}\n\nfunc newJob(rawJSON, dequeuedFrom, inProgQueue []byte) (*Job, error) {\n\tvar job Job\n\terr := json.Unmarshal(rawJSON, &job)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tjob.rawJSON = rawJSON\n\tjob.dequeuedFrom = dequeuedFrom\n\tjob.inProgQueue = inProgQueue\n\treturn &job, nil\n}\n\nfunc (j *Job) serialize() ([]byte, error) {\n\treturn json.Marshal(j)\n}\n\n// setArg sets a single named argument on the job.\nfunc (j *Job) setArg(key string, val interface{}) {\n\tif j.Args == nil {\n\t\tj.Args = make(map[string]interface{})\n\t}\n\tj.Args[key] = val\n}\n\nfunc (j *Job) failed(err error) {\n\tj.Fails++\n\tj.LastErr = err.Error()\n\tj.FailedAt = nowEpochSeconds()\n}\n\n// Checkin will update the status of the executing job to the specified messages. This message is visible within the web UI. This is useful for indicating some sort of progress on very long running jobs. For instance, on a job that has to process a million records over the course of an hour, the job could call Checkin with the current job number every 10k jobs.\nfunc (j *Job) Checkin(msg string) {\n\tif j.observer != nil {\n\t\tj.observer.observeCheckin(j.Name, j.ID, msg)\n\t}\n}\n\n// ArgString returns j.Args[key] typed to a string. If the key is missing or of the wrong type, it sets an argument error\n// on the job. This function is meant to be used in the body of a job handling function while extracting arguments,\n// followed by a single call to j.ArgError().\nfunc (j *Job) ArgString(key string) string {\n\tv, ok := j.Args[key]\n\tif ok {\n\t\ttypedV, ok := v.(string)\n\t\tif ok {\n\t\t\treturn typedV\n\t\t}\n\t\tj.argError = typecastError(\"string\", key, v)\n\t} else {\n\t\tj.argError = missingKeyError(\"string\", key)\n\t}\n\treturn \"\"\n}\n\n// ArgInt64 returns j.Args[key] typed to an int64. If the key is missing or of the wrong type, it sets an argument error\n// on the job. This function is meant to be used in the body of a job handling function while extracting arguments,\n// followed by a single call to j.ArgError().\nfunc (j *Job) ArgInt64(key string) int64 {\n\tv, ok := j.Args[key]\n\tif ok {\n\t\trVal := reflect.ValueOf(v)\n\t\tif isIntKind(rVal) {\n\t\t\treturn rVal.Int()\n\t\t} else if isUintKind(rVal) {\n\t\t\tvUint := rVal.Uint()\n\t\t\tif vUint <= math.MaxInt64 {\n\t\t\t\treturn int64(vUint)\n\t\t\t}\n\t\t} else if isFloatKind(rVal) {\n\t\t\tvFloat64 := rVal.Float()\n\t\t\tvInt64 := int64(vFloat64)\n\t\t\tif vFloat64 == math.Trunc(vFloat64) && vInt64 <= 9007199254740892 && vInt64 >= -9007199254740892 {\n\t\t\t\treturn vInt64\n\t\t\t}\n\t\t}\n\t\tj.argError = typecastError(\"int64\", key, v)\n\t} else {\n\t\tj.argError = missingKeyError(\"int64\", key)\n\t}\n\treturn 0\n}\n\n// ArgFloat64 returns j.Args[key] typed to a float64. If the key is missing or of the wrong type, it sets an argument error\n// on the job. This function is meant to be used in the body of a job handling function while extracting arguments,\n// followed by a single call to j.ArgError().\nfunc (j *Job) ArgFloat64(key string) float64 {\n\tv, ok := j.Args[key]\n\tif ok {\n\t\trVal := reflect.ValueOf(v)\n\t\tif isIntKind(rVal) {\n\t\t\treturn float64(rVal.Int())\n\t\t} else if isUintKind(rVal) {\n\t\t\treturn float64(rVal.Uint())\n\t\t} else if isFloatKind(rVal) {\n\t\t\treturn rVal.Float()\n\t\t}\n\t\tj.argError = typecastError(\"float64\", key, v)\n\t} else {\n\t\tj.argError = missingKeyError(\"float64\", key)\n\t}\n\treturn 0.0\n}\n\n// ArgBool returns j.Args[key] typed to a bool. If the key is missing or of the wrong type, it sets an argument error\n// on the job. This function is meant to be used in the body of a job handling function while extracting arguments,\n// followed by a single call to j.ArgError().\nfunc (j *Job) ArgBool(key string) bool {\n\tv, ok := j.Args[key]\n\tif ok {\n\t\ttypedV, ok := v.(bool)\n\t\tif ok {\n\t\t\treturn typedV\n\t\t}\n\t\tj.argError = typecastError(\"bool\", key, v)\n\t} else {\n\t\tj.argError = missingKeyError(\"bool\", key)\n\t}\n\treturn false\n}\n\n// ArgError returns the last error generated when extracting typed params. Returns nil if extracting the args went fine.\nfunc (j *Job) ArgError() error {\n\treturn j.argError\n}\n\nfunc isIntKind(v reflect.Value) bool {\n\tk := v.Kind()\n\treturn k == reflect.Int || k == reflect.Int8 || k == reflect.Int16 || k == reflect.Int32 || k == reflect.Int64\n}\n\nfunc isUintKind(v reflect.Value) bool {\n\tk := v.Kind()\n\treturn k == reflect.Uint || k == reflect.Uint8 || k == reflect.Uint16 || k == reflect.Uint32 || k == reflect.Uint64\n}\n\nfunc isFloatKind(v reflect.Value) bool {\n\tk := v.Kind()\n\treturn k == reflect.Float32 || k == reflect.Float64\n}\n\nfunc missingKeyError(jsonType, key string) error {\n\treturn fmt.Errorf(\"looking for a %s in job.Arg[%s] but key wasn't found\", jsonType, key)\n}\n\nfunc typecastError(jsonType, key string, v interface{}) error {\n\tactualType := reflect.TypeOf(v)\n\treturn fmt.Errorf(\"looking for a %s in job.Arg[%s] but value wasn't right type: %v(%v)\", jsonType, key, actualType, v)\n}\n"
        },
        {
          "name": "job_test.go",
          "type": "blob",
          "size": 5.1884765625,
          "content": "package work\n\nimport (\n\t\"math\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestJobArgumentExtraction(t *testing.T) {\n\tj := Job{}\n\tj.setArg(\"str1\", \"bar\")\n\n\tj.setArg(\"int1\", int64(77))\n\tj.setArg(\"int2\", 77)\n\tj.setArg(\"int3\", uint64(77))\n\tj.setArg(\"int4\", float64(77.0))\n\n\tj.setArg(\"bool1\", true)\n\n\tj.setArg(\"float1\", 3.14)\n\n\t//\n\t// Success cases:\n\t//\n\tvString := j.ArgString(\"str1\")\n\tassert.Equal(t, vString, \"bar\")\n\tassert.NoError(t, j.ArgError())\n\n\tvInt64 := j.ArgInt64(\"int1\")\n\tassert.EqualValues(t, vInt64, 77)\n\tassert.NoError(t, j.ArgError())\n\n\tvInt64 = j.ArgInt64(\"int2\")\n\tassert.EqualValues(t, vInt64, 77)\n\tassert.NoError(t, j.ArgError())\n\n\tvInt64 = j.ArgInt64(\"int3\")\n\tassert.EqualValues(t, vInt64, 77)\n\tassert.NoError(t, j.ArgError())\n\n\tvInt64 = j.ArgInt64(\"int4\")\n\tassert.EqualValues(t, vInt64, 77)\n\tassert.NoError(t, j.ArgError())\n\n\tvBool := j.ArgBool(\"bool1\")\n\tassert.Equal(t, vBool, true)\n\tassert.NoError(t, j.ArgError())\n\n\tvFloat := j.ArgFloat64(\"float1\")\n\tassert.Equal(t, vFloat, 3.14)\n\tassert.NoError(t, j.ArgError())\n\n\t// Missing key results in error:\n\tvString = j.ArgString(\"str_missing\")\n\tassert.Equal(t, vString, \"\")\n\tassert.Error(t, j.ArgError())\n\tj.argError = nil\n\tassert.NoError(t, j.ArgError())\n\n\tvInt64 = j.ArgInt64(\"int_missing\")\n\tassert.EqualValues(t, vInt64, 0)\n\tassert.Error(t, j.ArgError())\n\tj.argError = nil\n\tassert.NoError(t, j.ArgError())\n\n\tvBool = j.ArgBool(\"bool_missing\")\n\tassert.Equal(t, vBool, false)\n\tassert.Error(t, j.ArgError())\n\tj.argError = nil\n\tassert.NoError(t, j.ArgError())\n\n\tvFloat = j.ArgFloat64(\"float_missing\")\n\tassert.Equal(t, vFloat, 0.0)\n\tassert.Error(t, j.ArgError())\n\tj.argError = nil\n\tassert.NoError(t, j.ArgError())\n\n\t// Missing string; Make sure we don't reset it with successes after\n\tvString = j.ArgString(\"str_missing\")\n\tassert.Equal(t, vString, \"\")\n\tassert.Error(t, j.ArgError())\n\t_ = j.ArgString(\"str1\")\n\t_ = j.ArgInt64(\"int1\")\n\t_ = j.ArgBool(\"bool1\")\n\t_ = j.ArgFloat64(\"float1\")\n\tassert.Error(t, j.ArgError())\n}\n\nfunc TestJobArgumentExtractionBadString(t *testing.T) {\n\tvar testCases = []struct {\n\t\tkey  string\n\t\tval  interface{}\n\t\tgood bool\n\t}{\n\t\t{\"a\", 1, false},\n\t\t{\"b\", false, false},\n\t\t{\"c\", \"yay\", true},\n\t}\n\n\tj := Job{}\n\n\tfor _, tc := range testCases {\n\t\tj.setArg(tc.key, tc.val)\n\t}\n\n\tfor _, tc := range testCases {\n\t\tr := j.ArgString(tc.key)\n\t\terr := j.ArgError()\n\t\tif tc.good {\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; err = %v\\n\", tc, err)\n\t\t\t}\n\t\t\tif r != tc.val.(string) {\n\t\t\t\tt.Errorf(\"Failed test case: %v; r = %v\\n\", tc, r)\n\t\t\t}\n\t\t} else {\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but err was nil\\n\", tc)\n\t\t\t}\n\t\t\tif r != \"\" {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but r was %v\\n\", tc, r)\n\t\t\t}\n\t\t}\n\t\tj.argError = nil\n\t}\n}\n\nfunc TestJobArgumentExtractionBadBool(t *testing.T) {\n\tvar testCases = []struct {\n\t\tkey  string\n\t\tval  interface{}\n\t\tgood bool\n\t}{\n\t\t{\"a\", 1, false},\n\t\t{\"b\", \"boo\", false},\n\t\t{\"c\", true, true},\n\t\t{\"d\", false, true},\n\t}\n\n\tj := Job{}\n\n\tfor _, tc := range testCases {\n\t\tj.setArg(tc.key, tc.val)\n\t}\n\n\tfor _, tc := range testCases {\n\t\tr := j.ArgBool(tc.key)\n\t\terr := j.ArgError()\n\t\tif tc.good {\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; err = %v\\n\", tc, err)\n\t\t\t}\n\t\t\tif r != tc.val.(bool) {\n\t\t\t\tt.Errorf(\"Failed test case: %v; r = %v\\n\", tc, r)\n\t\t\t}\n\t\t} else {\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but err was nil\\n\", tc)\n\t\t\t}\n\t\t\tif r != false {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but r was %v\\n\", tc, r)\n\t\t\t}\n\t\t}\n\t\tj.argError = nil\n\t}\n}\n\nfunc TestJobArgumentExtractionBadInt(t *testing.T) {\n\tvar testCases = []struct {\n\t\tkey  string\n\t\tval  interface{}\n\t\tgood bool\n\t}{\n\t\t{\"a\", \"boo\", false},\n\t\t{\"b\", true, false},\n\t\t{\"c\", 1.1, false},\n\t\t{\"d\", 19007199254740892.0, false},\n\t\t{\"e\", -19007199254740892.0, false},\n\t\t{\"f\", uint64(math.MaxInt64) + 1, false},\n\n\t\t{\"z\", 0, true},\n\t\t{\"y\", 9007199254740892, true},\n\t\t{\"x\", 9007199254740892.0, true},\n\t\t{\"w\", 573839921, true},\n\t\t{\"v\", -573839921, true},\n\t\t{\"u\", uint64(math.MaxInt64), true},\n\t}\n\n\tj := Job{}\n\n\tfor _, tc := range testCases {\n\t\tj.setArg(tc.key, tc.val)\n\t}\n\n\tfor _, tc := range testCases {\n\t\tr := j.ArgInt64(tc.key)\n\t\terr := j.ArgError()\n\t\tif tc.good {\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; err = %v\\n\", tc, err)\n\t\t\t}\n\t\t} else {\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but err was nil\\n\", tc)\n\t\t\t}\n\t\t\tif r != 0 {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but r was %v\\n\", tc, r)\n\t\t\t}\n\t\t}\n\t\tj.argError = nil\n\t}\n}\n\nfunc TestJobArgumentExtractionBadFloat(t *testing.T) {\n\tvar testCases = []struct {\n\t\tkey  string\n\t\tval  interface{}\n\t\tgood bool\n\t}{\n\t\t{\"a\", \"boo\", false},\n\t\t{\"b\", true, false},\n\n\t\t{\"z\", 0, true},\n\t\t{\"y\", 9007199254740892, true},\n\t\t{\"x\", 9007199254740892.0, true},\n\t\t{\"w\", 573839921, true},\n\t\t{\"v\", -573839921, true},\n\t\t{\"u\", math.MaxFloat64, true},\n\t\t{\"t\", math.SmallestNonzeroFloat64, true},\n\t}\n\n\tj := Job{}\n\n\tfor _, tc := range testCases {\n\t\tj.setArg(tc.key, tc.val)\n\t}\n\n\tfor _, tc := range testCases {\n\t\tr := j.ArgFloat64(tc.key)\n\t\terr := j.ArgError()\n\t\tif tc.good {\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; err = %v\\n\", tc, err)\n\t\t\t}\n\t\t} else {\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but err was nil\\n\", tc)\n\t\t\t}\n\t\t\tif r != 0 {\n\t\t\t\tt.Errorf(\"Failed test case: %v; but r was %v\\n\", tc, r)\n\t\t\t}\n\t\t}\n\t\tj.argError = nil\n\t}\n}\n"
        },
        {
          "name": "log.go",
          "type": "blob",
          "size": 0.1162109375,
          "content": "package work\n\nimport \"fmt\"\n\nfunc logError(key string, err error) {\n\tfmt.Printf(\"ERROR: %s - %s\\n\", key, err.Error())\n}\n"
        },
        {
          "name": "observer.go",
          "type": "blob",
          "size": 5.65625,
          "content": "package work\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\n// An observer observes a single worker. Each worker has its own observer.\ntype observer struct {\n\tnamespace string\n\tworkerID  string\n\tpool      *redis.Pool\n\n\t// nil: worker isn't doing anything that we know of\n\t// not nil: the last started observation that we received on the channel.\n\t// if we get an checkin, we'll just update the existing observation\n\tcurrentStartedObservation *observation\n\n\t// version of the data that we wrote to redis.\n\t// each observation we get, we'll update version. When we flush it to redis, we'll update lastWrittenVersion.\n\t// This will keep us from writing to redis unless necessary\n\tversion, lastWrittenVersion int64\n\n\tobservationsChan chan *observation\n\n\tstopChan         chan struct{}\n\tdoneStoppingChan chan struct{}\n\n\tdrainChan        chan struct{}\n\tdoneDrainingChan chan struct{}\n}\n\ntype observationKind int\n\nconst (\n\tobservationKindStarted observationKind = iota\n\tobservationKindDone\n\tobservationKindCheckin\n)\n\ntype observation struct {\n\tkind observationKind\n\n\t// These fields always need to be set\n\tjobName string\n\tjobID   string\n\n\t// These need to be set when starting a job\n\tstartedAt int64\n\targuments map[string]interface{}\n\n\t// If we're done w/ the job, err will indicate the success/failure of it\n\terr error // nil: success. not nil: the error we got when running the job\n\n\t// If this is a checkin, set these.\n\tcheckin   string\n\tcheckinAt int64\n}\n\nconst observerBufferSize = 1024\n\nfunc newObserver(namespace string, pool *redis.Pool, workerID string) *observer {\n\treturn &observer{\n\t\tnamespace:        namespace,\n\t\tworkerID:         workerID,\n\t\tpool:             pool,\n\t\tobservationsChan: make(chan *observation, observerBufferSize),\n\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\n\t\tdrainChan:        make(chan struct{}),\n\t\tdoneDrainingChan: make(chan struct{}),\n\t}\n}\n\nfunc (o *observer) start() {\n\tgo o.loop()\n}\n\nfunc (o *observer) stop() {\n\to.stopChan <- struct{}{}\n\t<-o.doneStoppingChan\n}\n\nfunc (o *observer) drain() {\n\to.drainChan <- struct{}{}\n\t<-o.doneDrainingChan\n}\n\nfunc (o *observer) observeStarted(jobName, jobID string, arguments map[string]interface{}) {\n\to.observationsChan <- &observation{\n\t\tkind:      observationKindStarted,\n\t\tjobName:   jobName,\n\t\tjobID:     jobID,\n\t\tstartedAt: nowEpochSeconds(),\n\t\targuments: arguments,\n\t}\n}\n\nfunc (o *observer) observeDone(jobName, jobID string, err error) {\n\to.observationsChan <- &observation{\n\t\tkind:    observationKindDone,\n\t\tjobName: jobName,\n\t\tjobID:   jobID,\n\t\terr:     err,\n\t}\n}\n\nfunc (o *observer) observeCheckin(jobName, jobID, checkin string) {\n\to.observationsChan <- &observation{\n\t\tkind:      observationKindCheckin,\n\t\tjobName:   jobName,\n\t\tjobID:     jobID,\n\t\tcheckin:   checkin,\n\t\tcheckinAt: nowEpochSeconds(),\n\t}\n}\n\nfunc (o *observer) loop() {\n\t// Every tick we'll update redis if necessary\n\t// We don't update it on every job because the only purpose of this data is for humans to inspect the system,\n\t// and a fast worker could move onto new jobs every few ms.\n\tticker := time.Tick(1000 * time.Millisecond)\n\n\tfor {\n\t\tselect {\n\t\tcase <-o.stopChan:\n\t\t\to.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-o.drainChan:\n\t\tDRAIN_LOOP:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase obv := <-o.observationsChan:\n\t\t\t\t\to.process(obv)\n\t\t\t\tdefault:\n\t\t\t\t\tif err := o.writeStatus(o.currentStartedObservation); err != nil {\n\t\t\t\t\t\tlogError(\"observer.write\", err)\n\t\t\t\t\t}\n\t\t\t\t\to.doneDrainingChan <- struct{}{}\n\t\t\t\t\tbreak DRAIN_LOOP\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-ticker:\n\t\t\tif o.lastWrittenVersion != o.version {\n\t\t\t\tif err := o.writeStatus(o.currentStartedObservation); err != nil {\n\t\t\t\t\tlogError(\"observer.write\", err)\n\t\t\t\t}\n\t\t\t\to.lastWrittenVersion = o.version\n\t\t\t}\n\t\tcase obv := <-o.observationsChan:\n\t\t\to.process(obv)\n\t\t}\n\t}\n}\n\nfunc (o *observer) process(obv *observation) {\n\tif obv.kind == observationKindStarted {\n\t\to.currentStartedObservation = obv\n\t} else if obv.kind == observationKindDone {\n\t\to.currentStartedObservation = nil\n\t} else if obv.kind == observationKindCheckin {\n\t\tif (o.currentStartedObservation != nil) && (obv.jobID == o.currentStartedObservation.jobID) {\n\t\t\to.currentStartedObservation.checkin = obv.checkin\n\t\t\to.currentStartedObservation.checkinAt = obv.checkinAt\n\t\t} else {\n\t\t\tlogError(\"observer.checkin_mismatch\", fmt.Errorf(\"got checkin but mismatch on job ID or no job\"))\n\t\t}\n\t}\n\to.version++\n\n\t// If this is the version observation we got, just go ahead and write it.\n\tif o.version == 1 {\n\t\tif err := o.writeStatus(o.currentStartedObservation); err != nil {\n\t\t\tlogError(\"observer.first_write\", err)\n\t\t}\n\t\to.lastWrittenVersion = o.version\n\t}\n}\n\nfunc (o *observer) writeStatus(obv *observation) error {\n\tconn := o.pool.Get()\n\tdefer conn.Close()\n\n\tkey := redisKeyWorkerObservation(o.namespace, o.workerID)\n\n\tif obv == nil {\n\t\tif _, err := conn.Do(\"DEL\", key); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// hash:\n\t\t// job_name -> obv.Name\n\t\t// job_id -> obv.jobID\n\t\t// started_at -> obv.startedAt\n\t\t// args -> json.Encode(obv.arguments)\n\t\t// checkin -> obv.checkin\n\t\t// checkin_at -> obv.checkinAt\n\n\t\tvar argsJSON []byte\n\t\tif len(obv.arguments) == 0 {\n\t\t\targsJSON = []byte(\"\")\n\t\t} else {\n\t\t\tvar err error\n\t\t\targsJSON, err = json.Marshal(obv.arguments)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\targs := make([]interface{}, 0, 13)\n\t\targs = append(args,\n\t\t\tkey,\n\t\t\t\"job_name\", obv.jobName,\n\t\t\t\"job_id\", obv.jobID,\n\t\t\t\"started_at\", obv.startedAt,\n\t\t\t\"args\", argsJSON,\n\t\t)\n\n\t\tif (obv.checkin != \"\") && (obv.checkinAt > 0) {\n\t\t\targs = append(args,\n\t\t\t\t\"checkin\", obv.checkin,\n\t\t\t\t\"checkin_at\", obv.checkinAt,\n\t\t\t)\n\t\t}\n\n\t\tconn.Send(\"HMSET\", args...)\n\t\tconn.Send(\"EXPIRE\", key, 60*60*24)\n\t\tif err := conn.Flush(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "observer_test.go",
          "type": "blob",
          "size": 3.0859375,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestObserverStarted(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\ttMock := int64(1425263401)\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\n\tobserver := newObserver(ns, pool, \"abcd\")\n\tobserver.start()\n\tobserver.observeStarted(\"foo\", \"bar\", Q{\"a\": 1, \"b\": \"wat\"})\n\t//observer.observeDone(\"foo\", \"bar\", nil)\n\tobserver.drain()\n\tobserver.stop()\n\n\th := readHash(pool, redisKeyWorkerObservation(ns, \"abcd\"))\n\tassert.Equal(t, \"foo\", h[\"job_name\"])\n\tassert.Equal(t, \"bar\", h[\"job_id\"])\n\tassert.Equal(t, fmt.Sprint(tMock), h[\"started_at\"])\n\tassert.Equal(t, `{\"a\":1,\"b\":\"wat\"}`, h[\"args\"])\n}\n\nfunc TestObserverStartedDone(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\ttMock := int64(1425263401)\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\n\tobserver := newObserver(ns, pool, \"abcd\")\n\tobserver.start()\n\tobserver.observeStarted(\"foo\", \"bar\", Q{\"a\": 1, \"b\": \"wat\"})\n\tobserver.observeDone(\"foo\", \"bar\", nil)\n\tobserver.drain()\n\tobserver.stop()\n\n\th := readHash(pool, redisKeyWorkerObservation(ns, \"abcd\"))\n\tassert.Equal(t, 0, len(h))\n}\n\nfunc TestObserverCheckin(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\tobserver := newObserver(ns, pool, \"abcd\")\n\tobserver.start()\n\n\ttMock := int64(1425263401)\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\tobserver.observeStarted(\"foo\", \"bar\", Q{\"a\": 1, \"b\": \"wat\"})\n\n\ttMockCheckin := int64(1425263402)\n\tsetNowEpochSecondsMock(tMockCheckin)\n\tobserver.observeCheckin(\"foo\", \"bar\", \"doin it\")\n\tobserver.drain()\n\tobserver.stop()\n\n\th := readHash(pool, redisKeyWorkerObservation(ns, \"abcd\"))\n\tassert.Equal(t, \"foo\", h[\"job_name\"])\n\tassert.Equal(t, \"bar\", h[\"job_id\"])\n\tassert.Equal(t, fmt.Sprint(tMock), h[\"started_at\"])\n\tassert.Equal(t, `{\"a\":1,\"b\":\"wat\"}`, h[\"args\"])\n\tassert.Equal(t, \"doin it\", h[\"checkin\"])\n\tassert.Equal(t, fmt.Sprint(tMockCheckin), h[\"checkin_at\"])\n}\n\nfunc TestObserverCheckinFromJob(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\n\tobserver := newObserver(ns, pool, \"abcd\")\n\tobserver.start()\n\n\ttMock := int64(1425263401)\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\tobserver.observeStarted(\"foo\", \"barbar\", Q{\"a\": 1, \"b\": \"wat\"})\n\n\ttMockCheckin := int64(1425263402)\n\tsetNowEpochSecondsMock(tMockCheckin)\n\n\tj := &Job{Name: \"foo\", ID: \"barbar\", observer: observer}\n\tj.Checkin(\"sup\")\n\n\tobserver.drain()\n\tobserver.stop()\n\n\th := readHash(pool, redisKeyWorkerObservation(ns, \"abcd\"))\n\tassert.Equal(t, \"foo\", h[\"job_name\"])\n\tassert.Equal(t, \"barbar\", h[\"job_id\"])\n\tassert.Equal(t, fmt.Sprint(tMock), h[\"started_at\"])\n\tassert.Equal(t, \"sup\", h[\"checkin\"])\n\tassert.Equal(t, fmt.Sprint(tMockCheckin), h[\"checkin_at\"])\n}\n\nfunc readHash(pool *redis.Pool, key string) map[string]string {\n\tm := make(map[string]string)\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Strings(conn.Do(\"HGETALL\", key))\n\tif err != nil {\n\t\tpanic(\"could not delete retry/dead queue: \" + err.Error())\n\t}\n\n\tfor i, l := 0, len(v); i < l; i += 2 {\n\t\tm[v[i]] = v[i+1]\n\t}\n\n\treturn m\n}\n"
        },
        {
          "name": "periodic_enqueuer.go",
          "type": "blob",
          "size": 3.3125,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/robfig/cron/v3\"\n)\n\nconst (\n\tperiodicEnqueuerSleep   = 2 * time.Minute\n\tperiodicEnqueuerHorizon = 4 * time.Minute\n)\n\ntype periodicEnqueuer struct {\n\tnamespace             string\n\tpool                  *redis.Pool\n\tperiodicJobs          []*periodicJob\n\tscheduledPeriodicJobs []*scheduledPeriodicJob\n\tstopChan              chan struct{}\n\tdoneStoppingChan      chan struct{}\n}\n\ntype periodicJob struct {\n\tjobName  string\n\tspec     string\n\tschedule cron.Schedule\n}\n\ntype scheduledPeriodicJob struct {\n\tscheduledAt      time.Time\n\tscheduledAtEpoch int64\n\t*periodicJob\n}\n\nfunc newPeriodicEnqueuer(namespace string, pool *redis.Pool, periodicJobs []*periodicJob) *periodicEnqueuer {\n\treturn &periodicEnqueuer{\n\t\tnamespace:        namespace,\n\t\tpool:             pool,\n\t\tperiodicJobs:     periodicJobs,\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\t}\n}\n\nfunc (pe *periodicEnqueuer) start() {\n\tgo pe.loop()\n}\n\nfunc (pe *periodicEnqueuer) stop() {\n\tpe.stopChan <- struct{}{}\n\t<-pe.doneStoppingChan\n}\n\nfunc (pe *periodicEnqueuer) loop() {\n\t// Begin reaping periodically\n\ttimer := time.NewTimer(periodicEnqueuerSleep + time.Duration(rand.Intn(30))*time.Second)\n\tdefer timer.Stop()\n\n\tif pe.shouldEnqueue() {\n\t\terr := pe.enqueue()\n\t\tif err != nil {\n\t\t\tlogError(\"periodic_enqueuer.loop.enqueue\", err)\n\t\t}\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-pe.stopChan:\n\t\t\tpe.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t\ttimer.Reset(periodicEnqueuerSleep + time.Duration(rand.Intn(30))*time.Second)\n\t\t\tif pe.shouldEnqueue() {\n\t\t\t\terr := pe.enqueue()\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogError(\"periodic_enqueuer.loop.enqueue\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (pe *periodicEnqueuer) enqueue() error {\n\tnow := nowEpochSeconds()\n\tnowTime := time.Unix(now, 0)\n\thorizon := nowTime.Add(periodicEnqueuerHorizon)\n\n\tconn := pe.pool.Get()\n\tdefer conn.Close()\n\n\tfor _, pj := range pe.periodicJobs {\n\t\tfor t := pj.schedule.Next(nowTime); t.Before(horizon); t = pj.schedule.Next(t) {\n\t\t\tepoch := t.Unix()\n\t\t\tid := makeUniquePeriodicID(pj.jobName, pj.spec, epoch)\n\n\t\t\tjob := &Job{\n\t\t\t\tName: pj.jobName,\n\t\t\t\tID:   id,\n\n\t\t\t\t// This is technically wrong, but this lets the bytes be identical for the same periodic job instance. If we don't do this, we'd need to use a different approach -- probably giving each periodic job its own history of the past 100 periodic jobs, and only scheduling a job if it's not in the history.\n\t\t\t\tEnqueuedAt: epoch,\n\t\t\t\tArgs:       nil,\n\t\t\t}\n\n\t\t\trawJSON, err := job.serialize()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t_, err = conn.Do(\"ZADD\", redisKeyScheduled(pe.namespace), epoch, rawJSON)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t_, err := conn.Do(\"SET\", redisKeyLastPeriodicEnqueue(pe.namespace), now)\n\n\treturn err\n}\n\nfunc (pe *periodicEnqueuer) shouldEnqueue() bool {\n\tconn := pe.pool.Get()\n\tdefer conn.Close()\n\n\tlastEnqueue, err := redis.Int64(conn.Do(\"GET\", redisKeyLastPeriodicEnqueue(pe.namespace)))\n\tif err == redis.ErrNil {\n\t\treturn true\n\t} else if err != nil {\n\t\tlogError(\"periodic_enqueuer.should_enqueue\", err)\n\t\treturn true\n\t}\n\n\treturn lastEnqueue < (nowEpochSeconds() - int64(periodicEnqueuerSleep/time.Minute))\n}\n\nfunc makeUniquePeriodicID(name, spec string, epoch int64) string {\n\treturn fmt.Sprintf(\"periodic:%s:%s:%d\", name, spec, epoch)\n}\n"
        },
        {
          "name": "periodic_enqueuer_test.go",
          "type": "blob",
          "size": 4.279296875,
          "content": "package work\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/robfig/cron/v3\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestPeriodicEnqueuer(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tvar pjs []*periodicJob\n\tpjs = appendPeriodicJob(pjs, \"0/29 * * * * *\", \"foo\") // Every 29 seconds\n\tpjs = appendPeriodicJob(pjs, \"3/49 * * * * *\", \"bar\") // Every 49 seconds\n\tpjs = appendPeriodicJob(pjs, \"* * * 2 * *\", \"baz\")    // Every 2nd of the month seconds\n\n\tsetNowEpochSecondsMock(1468359453)\n\tdefer resetNowEpochSecondsMock()\n\n\tpe := newPeriodicEnqueuer(ns, pool, pjs)\n\terr := pe.enqueue()\n\tassert.NoError(t, err)\n\n\tc := NewClient(ns, pool)\n\tscheduledJobs, count, err := c.ScheduledJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 20, count)\n\n\texpected := []struct {\n\t\tname         string\n\t\tid           string\n\t\tscheduledFor int64\n\t}{\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359472\", scheduledFor: 1468359472},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359478\", scheduledFor: 1468359478},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359480\", scheduledFor: 1468359480},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359483\", scheduledFor: 1468359483},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359509\", scheduledFor: 1468359509},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359532\", scheduledFor: 1468359532},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359538\", scheduledFor: 1468359538},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359540\", scheduledFor: 1468359540},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359543\", scheduledFor: 1468359543},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359569\", scheduledFor: 1468359569},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359592\", scheduledFor: 1468359592},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359598\", scheduledFor: 1468359598},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359600\", scheduledFor: 1468359600},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359603\", scheduledFor: 1468359603},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359629\", scheduledFor: 1468359629},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359652\", scheduledFor: 1468359652},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359658\", scheduledFor: 1468359658},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359660\", scheduledFor: 1468359660},\n\t\t{name: \"bar\", id: \"periodic:bar:3/49 * * * * *:1468359663\", scheduledFor: 1468359663},\n\t\t{name: \"foo\", id: \"periodic:foo:0/29 * * * * *:1468359689\", scheduledFor: 1468359689},\n\t}\n\n\tfor i, e := range expected {\n\t\tassert.EqualValues(t, scheduledJobs[i].RunAt, scheduledJobs[i].EnqueuedAt)\n\t\tassert.Nil(t, scheduledJobs[i].Args)\n\n\t\tassert.Equal(t, e.name, scheduledJobs[i].Name)\n\t\tassert.Equal(t, e.id, scheduledJobs[i].ID)\n\t\tassert.Equal(t, e.scheduledFor, scheduledJobs[i].RunAt)\n\t}\n\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\t// Make sure the last periodic enqueued was set\n\tlastEnqueue, err := redis.Int64(conn.Do(\"GET\", redisKeyLastPeriodicEnqueue(ns)))\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 1468359453, lastEnqueue)\n\n\tsetNowEpochSecondsMock(1468359454)\n\n\t// Now do it again, and make sure nothing happens!\n\terr = pe.enqueue()\n\tassert.NoError(t, err)\n\n\t_, count, err = c.ScheduledJobs(1)\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 20, count)\n\n\t// Make sure the last periodic enqueued was set\n\tlastEnqueue, err = redis.Int64(conn.Do(\"GET\", redisKeyLastPeriodicEnqueue(ns)))\n\tassert.NoError(t, err)\n\tassert.EqualValues(t, 1468359454, lastEnqueue)\n\n\tassert.False(t, pe.shouldEnqueue())\n\n\tsetNowEpochSecondsMock(1468359454 + int64(periodicEnqueuerSleep/time.Minute) + 10)\n\n\tassert.True(t, pe.shouldEnqueue())\n}\n\nfunc TestPeriodicEnqueuerSpawn(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\tpe := newPeriodicEnqueuer(ns, pool, nil)\n\tpe.start()\n\tpe.stop()\n}\n\nfunc appendPeriodicJob(pjs []*periodicJob, spec, jobName string) []*periodicJob {\n\tp := cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)\n\n\tsched, err := p.Parse(spec)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tpj := &periodicJob{jobName: jobName, spec: spec, schedule: sched}\n\treturn append(pjs, pj)\n}\n"
        },
        {
          "name": "priority_sampler.go",
          "type": "blob",
          "size": 2.30078125,
          "content": "package work\n\nimport (\n\t\"math/rand\"\n)\n\ntype prioritySampler struct {\n\tsum     uint\n\tsamples []sampleItem\n}\n\ntype sampleItem struct {\n\tpriority uint\n\n\t// payload:\n\tredisJobs               string\n\tredisJobsInProg         string\n\tredisJobsPaused         string\n\tredisJobsLock           string\n\tredisJobsLockInfo       string\n\tredisJobsMaxConcurrency string\n}\n\nfunc (s *prioritySampler) add(priority uint, redisJobs, redisJobsInProg, redisJobsPaused, redisJobsLock, redisJobsLockInfo, redisJobsMaxConcurrency string) {\n\tsample := sampleItem{\n\t\tpriority:                priority,\n\t\tredisJobs:               redisJobs,\n\t\tredisJobsInProg:         redisJobsInProg,\n\t\tredisJobsPaused:         redisJobsPaused,\n\t\tredisJobsLock:           redisJobsLock,\n\t\tredisJobsLockInfo:       redisJobsLockInfo,\n\t\tredisJobsMaxConcurrency: redisJobsMaxConcurrency,\n\t}\n\ts.samples = append(s.samples, sample)\n\ts.sum += priority\n}\n\n// sample re-sorts s.samples, modifying it in-place. Higher weighted things will tend to go towards the beginning.\n// NOTE: as written currently makes 0 allocations.\n// NOTE2: this is an O(n^2 algorithm) that is:\n//     5492ns for 50 jobs (50 is a large number of unique jobs in my experience)\n//     54966ns for 200 jobs\n//     ~1ms for 1000 jobs\n//     ~4ms for 2000 jobs\nfunc (s *prioritySampler) sample() []sampleItem {\n\tlenSamples := len(s.samples)\n\tremaining := lenSamples\n\tsumRemaining := s.sum\n\tlastValidIdx := 0\n\n\t// Algorithm is as follows:\n\t// Loop until we sort everything. We're going to sort it in-place, probabilistically moving the highest weights to the front of the slice.\n\t//   Pick a random number\n\t//   Move backwards through the slice on each iteration,\n\t//     and see where the random number fits in the continuum.\n\t//     If we find where it fits, sort the item to the next slot towards the front of the slice.\n\tfor remaining > 1 {\n\t\t// rn from [0 to sumRemaining)\n\t\trn := uint(rand.Uint32()) % sumRemaining\n\n\t\tprevSum := uint(0)\n\t\tfor i := lenSamples - 1; i >= lastValidIdx; i-- {\n\t\t\tsample := s.samples[i]\n\t\t\tif rn < (sample.priority + prevSum) {\n\t\t\t\t// move the sample to the beginning\n\t\t\t\ts.samples[i], s.samples[lastValidIdx] = s.samples[lastValidIdx], s.samples[i]\n\n\t\t\t\tsumRemaining -= sample.priority\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\tprevSum += sample.priority\n\t\t\t}\n\t\t}\n\n\t\tlastValidIdx++\n\t\tremaining--\n\t}\n\n\treturn s.samples\n}\n"
        },
        {
          "name": "priority_sampler_test.go",
          "type": "blob",
          "size": 1.4404296875,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestPrioritySampler(t *testing.T) {\n\tps := prioritySampler{}\n\n\tps.add(5, \"jobs.5\", \"jobsinprog.5\", \"jobspaused.5\", \"jobslock.5\", \"jobslockinfo.5\", \"jobsconcurrency.5\")\n\tps.add(2, \"jobs.2a\", \"jobsinprog.2a\", \"jobspaused.2a\", \"jobslock.2a\", \"jobslockinfo.2a\", \"jobsconcurrency.2a\")\n\tps.add(1, \"jobs.1b\", \"jobsinprog.1b\", \"jobspaused.1b\", \"jobslock.1b\", \"jobslockinfo.1b\", \"jobsconcurrency.1b\")\n\n\tvar c5 = 0\n\tvar c2 = 0\n\tvar c1 = 0\n\tvar c1end = 0\n\tvar total = 200\n\tfor i := 0; i < total; i++ {\n\t\tret := ps.sample()\n\t\tif ret[0].priority == 5 {\n\t\t\tc5++\n\t\t} else if ret[0].priority == 2 {\n\t\t\tc2++\n\t\t} else if ret[0].priority == 1 {\n\t\t\tc1++\n\t\t}\n\t\tif ret[2].priority == 1 {\n\t\t\tc1end++\n\t\t}\n\t}\n\n\t// make sure these numbers are roughly correct. note that probability is a thing.\n\tassert.True(t, c5 > (2*c2))\n\tassert.True(t, float64(c2) > (1.5*float64(c1)))\n\tassert.True(t, c1 >= (total/13), fmt.Sprintf(\"c1 = %d total = %d total/13=%d\", c1, total, total/13))\n\tassert.True(t, float64(c1end) > (float64(total)*0.50))\n}\n\nfunc BenchmarkPrioritySampler(b *testing.B) {\n\tps := prioritySampler{}\n\tfor i := 0; i < 200; i++ {\n\t\tps.add(uint(i)+1,\n\t\t\t\"jobs.\"+fmt.Sprint(i),\n\t\t\t\"jobsinprog.\"+fmt.Sprint(i),\n\t\t\t\"jobspaused.\"+fmt.Sprint(i),\n\t\t\t\"jobslock.\"+fmt.Sprint(i),\n\t\t\t\"jobslockinfo.\"+fmt.Sprint(i),\n\t\t\t\"jobsmaxconcurrency.\"+fmt.Sprint(i))\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tps.sample()\n\t}\n}\n"
        },
        {
          "name": "redis.go",
          "type": "blob",
          "size": 11.0712890625,
          "content": "package work\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n)\n\nfunc redisNamespacePrefix(namespace string) string {\n\tl := len(namespace)\n\tif (l > 0) && (namespace[l-1] != ':') {\n\t\tnamespace = namespace + \":\"\n\t}\n\treturn namespace\n}\n\nfunc redisKeyKnownJobs(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"known_jobs\"\n}\n\n// returns \"<namespace>:jobs:\"\n// so that we can just append the job name and be good to go\nfunc redisKeyJobsPrefix(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"jobs:\"\n}\n\nfunc redisKeyJobs(namespace, jobName string) string {\n\treturn redisKeyJobsPrefix(namespace) + jobName\n}\n\nfunc redisKeyJobsInProgress(namespace, poolID, jobName string) string {\n\treturn fmt.Sprintf(\"%s:%s:inprogress\", redisKeyJobs(namespace, jobName), poolID)\n}\n\nfunc redisKeyRetry(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"retry\"\n}\n\nfunc redisKeyDead(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"dead\"\n}\n\nfunc redisKeyScheduled(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"scheduled\"\n}\n\nfunc redisKeyWorkerObservation(namespace, workerID string) string {\n\treturn redisNamespacePrefix(namespace) + \"worker:\" + workerID\n}\n\nfunc redisKeyWorkerPools(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"worker_pools\"\n}\n\nfunc redisKeyHeartbeat(namespace, workerPoolID string) string {\n\treturn redisNamespacePrefix(namespace) + \"worker_pools:\" + workerPoolID\n}\n\nfunc redisKeyJobsPaused(namespace, jobName string) string {\n\treturn redisKeyJobs(namespace, jobName) + \":paused\"\n}\n\nfunc redisKeyJobsLock(namespace, jobName string) string {\n\treturn redisKeyJobs(namespace, jobName) + \":lock\"\n}\n\nfunc redisKeyJobsLockInfo(namespace, jobName string) string {\n\treturn redisKeyJobs(namespace, jobName) + \":lock_info\"\n}\n\nfunc redisKeyJobsConcurrency(namespace, jobName string) string {\n\treturn redisKeyJobs(namespace, jobName) + \":max_concurrency\"\n}\n\nfunc redisKeyUniqueJob(namespace, jobName string, args map[string]interface{}) (string, error) {\n\tvar buf bytes.Buffer\n\n\tbuf.WriteString(redisNamespacePrefix(namespace))\n\tbuf.WriteString(\"unique:\")\n\tbuf.WriteString(jobName)\n\tbuf.WriteRune(':')\n\n\tif args != nil {\n\t\terr := json.NewEncoder(&buf).Encode(args)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\treturn buf.String(), nil\n}\n\nfunc redisKeyLastPeriodicEnqueue(namespace string) string {\n\treturn redisNamespacePrefix(namespace) + \"last_periodic_enqueue\"\n}\n\n// Used to fetch the next job to run\n//\n// KEYS[1] = the 1st job queue we want to try, eg, \"work:jobs:emails\"\n// KEYS[2] = the 1st job queue's in prog queue, eg, \"work:jobs:emails:97c84119d13cb54119a38743:inprogress\"\n// KEYS[3] = the 2nd job queue...\n// KEYS[4] = the 2nd job queue's in prog queue...\n// ...\n// KEYS[N] = the last job queue...\n// KEYS[N+1] = the last job queue's in prog queue...\n// ARGV[1] = job queue's workerPoolID\nvar redisLuaFetchJob = fmt.Sprintf(`\nlocal function acquireLock(lockKey, lockInfoKey, workerPoolID)\n  redis.call('incr', lockKey)\n  redis.call('hincrby', lockInfoKey, workerPoolID, 1)\nend\n\nlocal function haveJobs(jobQueue)\n  return redis.call('llen', jobQueue) > 0\nend\n\nlocal function isPaused(pauseKey)\n  return redis.call('get', pauseKey)\nend\n\nlocal function canRun(lockKey, maxConcurrency)\n  local activeJobs = tonumber(redis.call('get', lockKey))\n  if (not maxConcurrency or maxConcurrency == 0) or (not activeJobs or activeJobs < maxConcurrency) then\n    -- default case: maxConcurrency not defined or set to 0 means no cap on concurrent jobs OR\n    -- maxConcurrency set, but lock does not yet exist OR\n    -- maxConcurrency set, lock is set, but not yet at max concurrency\n    return true\n  else\n    -- we are at max capacity for running jobs\n    return false\n  end\nend\n\nlocal res, jobQueue, inProgQueue, pauseKey, lockKey, maxConcurrency, workerPoolID, concurrencyKey, lockInfoKey\nlocal keylen = #KEYS\nworkerPoolID = ARGV[1]\n\nfor i=1,keylen,%d do\n  jobQueue = KEYS[i]\n  inProgQueue = KEYS[i+1]\n  pauseKey = KEYS[i+2]\n  lockKey = KEYS[i+3]\n  lockInfoKey = KEYS[i+4]\n  concurrencyKey = KEYS[i+5]\n\n  maxConcurrency = tonumber(redis.call('get', concurrencyKey))\n\n  if haveJobs(jobQueue) and not isPaused(pauseKey) and canRun(lockKey, maxConcurrency) then\n    acquireLock(lockKey, lockInfoKey, workerPoolID)\n    res = redis.call('rpoplpush', jobQueue, inProgQueue)\n    return {res, jobQueue, inProgQueue}\n  end\nend\nreturn nil`, fetchKeysPerJobType)\n\n// Used by the reaper to re-enqueue jobs that were in progress\n//\n// KEYS[1] = the 1st job's in progress queue\n// KEYS[2] = the 1st job's job queue\n// KEYS[3] = the 2nd job's in progress queue\n// KEYS[4] = the 2nd job's job queue\n// ...\n// KEYS[N] = the last job's in progress queue\n// KEYS[N+1] = the last job's job queue\n// ARGV[1] = workerPoolID for job queue\nvar redisLuaReenqueueJob = fmt.Sprintf(`\nlocal function releaseLock(lockKey, lockInfoKey, workerPoolID)\n  redis.call('decr', lockKey)\n  redis.call('hincrby', lockInfoKey, workerPoolID, -1)\nend\n\nlocal keylen = #KEYS\nlocal res, jobQueue, inProgQueue, workerPoolID, lockKey, lockInfoKey\nworkerPoolID = ARGV[1]\n\nfor i=1,keylen,%d do\n  inProgQueue = KEYS[i]\n  jobQueue = KEYS[i+1]\n  lockKey = KEYS[i+2]\n  lockInfoKey = KEYS[i+3]\n  res = redis.call('rpoplpush', inProgQueue, jobQueue)\n  if res then\n    releaseLock(lockKey, lockInfoKey, workerPoolID)\n    return {res, inProgQueue, jobQueue}\n  end\nend\nreturn nil`, requeueKeysPerJob)\n\n// Used by the reaper to clean up stale locks\n//\n// KEYS[1] = the 1st job's lock\n// KEYS[2] = the 1st job's lock info hash\n// KEYS[3] = the 2nd job's lock\n// KEYS[4] = the 2nd job's lock info hash\n// ...\n// KEYS[N] = the last job's lock\n// KEYS[N+1] = the last job's lock info haash\n// ARGV[1] = the dead worker pool id\nvar redisLuaReapStaleLocks = `\nlocal keylen = #KEYS\nlocal lock, lockInfo, deadLockCount\nlocal deadPoolID = ARGV[1]\n\nfor i=1,keylen,2 do\n  lock = KEYS[i]\n  lockInfo = KEYS[i+1]\n  deadLockCount = tonumber(redis.call('hget', lockInfo, deadPoolID))\n\n  if deadLockCount then\n    redis.call('decrby', lock, deadLockCount)\n    redis.call('hdel', lockInfo, deadPoolID)\n\n    if tonumber(redis.call('get', lock)) < 0 then\n      redis.call('set', lock, 0)\n    end\n  end\nend\nreturn nil\n`\n\n// KEYS[1] = zset of jobs (retry or scheduled), eg work:retry\n// KEYS[2] = zset of dead, eg work:dead. If we don't know the jobName of a job, we'll put it in dead.\n// KEYS[3...] = known job queues, eg [\"work:jobs:create_watch\", \"work:jobs:send_email\", ...]\n// ARGV[1] = jobs prefix, eg, \"work:jobs:\". We'll take that and append the job name from the JSON object in order to queue up a job\n// ARGV[2] = current time in epoch seconds\nvar redisLuaZremLpushCmd = `\nlocal res, j, queue\nres = redis.call('zrangebyscore', KEYS[1], '-inf', ARGV[2], 'LIMIT', 0, 1)\nif #res > 0 then\n  j = cjson.decode(res[1])\n  redis.call('zrem', KEYS[1], res[1])\n  queue = ARGV[1] .. j['name']\n  for _,v in pairs(KEYS) do\n    if v == queue then\n      j['t'] = tonumber(ARGV[2])\n      redis.call('lpush', queue, cjson.encode(j))\n      return 'ok'\n    end\n  end\n  j['err'] = 'unknown job when requeueing'\n  j['failed_at'] = tonumber(ARGV[2])\n  redis.call('zadd', KEYS[2], ARGV[2], cjson.encode(j))\n  return 'dead' -- put on dead queue\nend\nreturn nil\n`\n\n// KEYS[1] = zset of (dead|scheduled|retry), eg, work:dead\n// ARGV[1] = died at. The z rank of the job.\n// ARGV[2] = job ID to requeue\n// Returns:\n// - number of jobs deleted (typically 1 or 0)\n// - job bytes (last job only)\nvar redisLuaDeleteSingleCmd = `\nlocal jobs, i, j, deletedCount, jobBytes\njobs = redis.call('zrangebyscore', KEYS[1], ARGV[1], ARGV[1])\nlocal jobCount = #jobs\njobBytes = ''\ndeletedCount = 0\nfor i=1,jobCount do\n  j = cjson.decode(jobs[i])\n  if j['id'] == ARGV[2] then\n    redis.call('zrem', KEYS[1], jobs[i])\n    deletedCount = deletedCount + 1\n    jobBytes = jobs[i]\n  end\nend\nreturn {deletedCount, jobBytes}\n`\n\n// KEYS[1] = zset of dead jobs, eg, work:dead\n// KEYS[2...] = known job queues, eg [\"work:jobs:create_watch\", \"work:jobs:send_email\", ...]\n// ARGV[1] = jobs prefix, eg, \"work:jobs:\". We'll take that and append the job name from the JSON object in order to queue up a job\n// ARGV[2] = current time in epoch seconds\n// ARGV[3] = died at. The z rank of the job.\n// ARGV[4] = job ID to requeue\n// Returns: number of jobs requeued (typically 1 or 0)\nvar redisLuaRequeueSingleDeadCmd = `\nlocal jobs, i, j, queue, found, requeuedCount\njobs = redis.call('zrangebyscore', KEYS[1], ARGV[3], ARGV[3])\nlocal jobCount = #jobs\nrequeuedCount = 0\nfor i=1,jobCount do\n  j = cjson.decode(jobs[i])\n  if j['id'] == ARGV[4] then\n    redis.call('zrem', KEYS[1], jobs[i])\n    queue = ARGV[1] .. j['name']\n    found = false\n    for _,v in pairs(KEYS) do\n      if v == queue then\n        j['t'] = tonumber(ARGV[2])\n        j['fails'] = nil\n        j['failed_at'] = nil\n        j['err'] = nil\n        redis.call('lpush', queue, cjson.encode(j))\n        requeuedCount = requeuedCount + 1\n        found = true\n        break\n      end\n    end\n    if not found then\n      j['err'] = 'unknown job when requeueing'\n      j['failed_at'] = tonumber(ARGV[2])\n      redis.call('zadd', KEYS[1], ARGV[2] + 5, cjson.encode(j))\n    end\n  end\nend\nreturn requeuedCount\n`\n\n// KEYS[1] = zset of dead jobs, eg work:dead\n// KEYS[2...] = known job queues, eg [\"work:jobs:create_watch\", \"work:jobs:send_email\", ...]\n// ARGV[1] = jobs prefix, eg, \"work:jobs:\". We'll take that and append the job name from the JSON object in order to queue up a job\n// ARGV[2] = current time in epoch seconds\n// ARGV[3] = max number of jobs to requeue\n// Returns: number of jobs requeued\nvar redisLuaRequeueAllDeadCmd = `\nlocal jobs, i, j, queue, found, requeuedCount\njobs = redis.call('zrangebyscore', KEYS[1], '-inf', ARGV[2], 'LIMIT', 0, ARGV[3])\nlocal jobCount = #jobs\nrequeuedCount = 0\nfor i=1,jobCount do\n  j = cjson.decode(jobs[i])\n  redis.call('zrem', KEYS[1], jobs[i])\n  queue = ARGV[1] .. j['name']\n  found = false\n  for _,v in pairs(KEYS) do\n    if v == queue then\n      j['t'] = tonumber(ARGV[2])\n      j['fails'] = nil\n      j['failed_at'] = nil\n      j['err'] = nil\n      redis.call('lpush', queue, cjson.encode(j))\n      requeuedCount = requeuedCount + 1\n      found = true\n      break\n    end\n  end\n  if not found then\n    j['err'] = 'unknown job when requeueing'\n    j['failed_at'] = tonumber(ARGV[2])\n    redis.call('zadd', KEYS[1], ARGV[2] + 5, cjson.encode(j))\n  end\nend\nreturn requeuedCount\n`\n\n// KEYS[1] = job queue to push onto\n// KEYS[2] = Unique job's key. Test for existence and set if we push.\n// ARGV[1] = job\n// ARGV[2] = updated job or just a 1 if arguments don't update\nvar redisLuaEnqueueUnique = `\nif redis.call('set', KEYS[2], ARGV[2], 'NX', 'EX', '86400') then\n  redis.call('lpush', KEYS[1], ARGV[1])\n  return 'ok'\nelse\n  redis.call('set', KEYS[2], ARGV[2], 'EX', '86400')\nend\nreturn 'dup'\n`\n\n// KEYS[1] = scheduled job queue\n// KEYS[2] = Unique job's key. Test for existence and set if we push.\n// ARGV[1] = job\n// ARGV[2] = updated job or just a 1 if arguments don't update\n// ARGV[3] = epoch seconds for job to be run at\nvar redisLuaEnqueueUniqueIn = `\nif redis.call('set', KEYS[2], ARGV[2], 'NX', 'EX', '86400') then\n  redis.call('zadd', KEYS[1], ARGV[3], ARGV[1])\n  return 'ok'\nelse\n  redis.call('set', KEYS[2], ARGV[2], 'EX', '86400')\nend\nreturn 'dup'\n`\n"
        },
        {
          "name": "requeuer.go",
          "type": "blob",
          "size": 2.4248046875,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\ntype requeuer struct {\n\tnamespace string\n\tpool      *redis.Pool\n\n\tredisRequeueScript *redis.Script\n\tredisRequeueArgs   []interface{}\n\n\tstopChan         chan struct{}\n\tdoneStoppingChan chan struct{}\n\n\tdrainChan        chan struct{}\n\tdoneDrainingChan chan struct{}\n}\n\nfunc newRequeuer(namespace string, pool *redis.Pool, requeueKey string, jobNames []string) *requeuer {\n\targs := make([]interface{}, 0, len(jobNames)+2+2)\n\targs = append(args, requeueKey)              // KEY[1]\n\targs = append(args, redisKeyDead(namespace)) // KEY[2]\n\tfor _, jobName := range jobNames {\n\t\targs = append(args, redisKeyJobs(namespace, jobName)) // KEY[3, 4, ...]\n\t}\n\targs = append(args, redisKeyJobsPrefix(namespace)) // ARGV[1]\n\targs = append(args, 0)                             // ARGV[2] -- NOTE: We're going to change this one on every call\n\n\treturn &requeuer{\n\t\tnamespace: namespace,\n\t\tpool:      pool,\n\n\t\tredisRequeueScript: redis.NewScript(len(jobNames)+2, redisLuaZremLpushCmd),\n\t\tredisRequeueArgs:   args,\n\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\n\t\tdrainChan:        make(chan struct{}),\n\t\tdoneDrainingChan: make(chan struct{}),\n\t}\n}\n\nfunc (r *requeuer) start() {\n\tgo r.loop()\n}\n\nfunc (r *requeuer) stop() {\n\tr.stopChan <- struct{}{}\n\t<-r.doneStoppingChan\n}\n\nfunc (r *requeuer) drain() {\n\tr.drainChan <- struct{}{}\n\t<-r.doneDrainingChan\n}\n\nfunc (r *requeuer) loop() {\n\t// Just do this simple thing for now.\n\t// If we have 100 processes all running requeuers,\n\t// there's probably too much hitting redis.\n\t// So later on we'l have to implement exponential backoff\n\tticker := time.Tick(1000 * time.Millisecond)\n\n\tfor {\n\t\tselect {\n\t\tcase <-r.stopChan:\n\t\t\tr.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-r.drainChan:\n\t\t\tfor r.process() {\n\t\t\t}\n\t\t\tr.doneDrainingChan <- struct{}{}\n\t\tcase <-ticker:\n\t\t\tfor r.process() {\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (r *requeuer) process() bool {\n\tconn := r.pool.Get()\n\tdefer conn.Close()\n\n\tr.redisRequeueArgs[len(r.redisRequeueArgs)-1] = nowEpochSeconds()\n\n\tres, err := redis.String(r.redisRequeueScript.Do(conn, r.redisRequeueArgs...))\n\tif err == redis.ErrNil {\n\t\treturn false\n\t} else if err != nil {\n\t\tlogError(\"requeuer.process\", err)\n\t\treturn false\n\t}\n\n\tif res == \"\" {\n\t\treturn false\n\t} else if res == \"dead\" {\n\t\tlogError(\"requeuer.process.dead\", fmt.Errorf(\"no job name\"))\n\t\treturn true\n\t} else if res == \"ok\" {\n\t\treturn true\n\t}\n\n\treturn false\n}\n"
        },
        {
          "name": "requeuer_test.go",
          "type": "blob",
          "size": 2.1982421875,
          "content": "package work\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestRequeue(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\ttMock := nowEpochSeconds() - 10\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.EnqueueIn(\"wat\", -9, nil)\n\tassert.NoError(t, err)\n\t_, err = enqueuer.EnqueueIn(\"wat\", -9, nil)\n\tassert.NoError(t, err)\n\t_, err = enqueuer.EnqueueIn(\"foo\", 10, nil)\n\tassert.NoError(t, err)\n\t_, err = enqueuer.EnqueueIn(\"foo\", 14, nil)\n\tassert.NoError(t, err)\n\t_, err = enqueuer.EnqueueIn(\"bar\", 19, nil)\n\tassert.NoError(t, err)\n\n\tresetNowEpochSecondsMock()\n\n\tre := newRequeuer(ns, pool, redisKeyScheduled(ns), []string{\"wat\", \"foo\", \"bar\"})\n\tre.start()\n\tre.drain()\n\tre.stop()\n\n\tassert.EqualValues(t, 2, listSize(pool, redisKeyJobs(ns, \"wat\")))\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobs(ns, \"foo\")))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, \"bar\")))\n\tassert.EqualValues(t, 2, zsetSize(pool, redisKeyScheduled(ns)))\n\n\tj := jobOnQueue(pool, redisKeyJobs(ns, \"foo\"))\n\tassert.Equal(t, j.Name, \"foo\")\n\n\t// Because we mocked time to 10 seconds ago above, the job was put on the zset with t=10 secs ago\n\t// We want to ensure it's requeued with t=now.\n\t// On boundary conditions with the VM, nowEpochSeconds() might be 1 or 2 secs ahead of EnqueuedAt\n\tassert.True(t, (j.EnqueuedAt+2) >= nowEpochSeconds())\n\n}\n\nfunc TestRequeueUnknown(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\n\ttMock := nowEpochSeconds() - 10\n\tsetNowEpochSecondsMock(tMock)\n\tdefer resetNowEpochSecondsMock()\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.EnqueueIn(\"wat\", -9, nil)\n\tassert.NoError(t, err)\n\n\tnowish := nowEpochSeconds()\n\tsetNowEpochSecondsMock(nowish)\n\n\tre := newRequeuer(ns, pool, redisKeyScheduled(ns), []string{\"bar\"})\n\tre.start()\n\tre.drain()\n\tre.stop()\n\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyScheduled(ns)))\n\tassert.EqualValues(t, 1, zsetSize(pool, redisKeyDead(ns)))\n\n\trank, job := jobOnZset(pool, redisKeyDead(ns))\n\n\tassert.Equal(t, nowish, rank)\n\tassert.Equal(t, nowish, job.FailedAt)\n\tassert.Equal(t, \"unknown job when requeueing\", job.LastErr)\n}\n"
        },
        {
          "name": "run.go",
          "type": "blob",
          "size": 1.400390625,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// returns an error if the job fails, or there's a panic, or we couldn't reflect correctly.\n// if we return an error, it signals we want the job to be retried.\nfunc runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) {\n\treturnCtx = reflect.New(ctxType)\n\tcurrentMiddleware := 0\n\tmaxMiddleware := len(middleware)\n\n\tvar next NextMiddlewareFunc\n\tnext = func() error {\n\t\tif currentMiddleware < maxMiddleware {\n\t\t\tmw := middleware[currentMiddleware]\n\t\t\tcurrentMiddleware++\n\t\t\tif mw.IsGeneric {\n\t\t\t\treturn mw.GenericMiddlewareHandler(job, next)\n\t\t\t}\n\t\t\tres := mw.DynamicMiddleware.Call([]reflect.Value{returnCtx, reflect.ValueOf(job), reflect.ValueOf(next)})\n\t\t\tx := res[0].Interface()\n\t\t\tif x == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn x.(error)\n\t\t}\n\t\tif jt.IsGeneric {\n\t\t\treturn jt.GenericHandler(job)\n\t\t}\n\t\tres := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)})\n\t\tx := res[0].Interface()\n\t\tif x == nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn x.(error)\n\t}\n\n\tdefer func() {\n\t\tif panicErr := recover(); panicErr != nil {\n\t\t\t// err turns out to be interface{}, of actual type \"runtime.errorCString\"\n\t\t\t// Luckily, the err sprints nicely via fmt.\n\t\t\terrorishError := fmt.Errorf(\"%v\", panicErr)\n\t\t\tlogError(\"runJob.panic\", errorishError)\n\t\t\treturnError = errorishError\n\t\t}\n\t}()\n\n\treturnError = next()\n\n\treturn\n}\n"
        },
        {
          "name": "run_test.go",
          "type": "blob",
          "size": 3.337890625,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestRunBasicMiddleware(t *testing.T) {\n\tmw1 := func(j *Job, next NextMiddlewareFunc) error {\n\t\tj.setArg(\"mw1\", \"mw1\")\n\t\treturn next()\n\t}\n\n\tmw2 := func(c *tstCtx, j *Job, next NextMiddlewareFunc) error {\n\t\tc.record(j.Args[\"mw1\"].(string))\n\t\tc.record(\"mw2\")\n\t\treturn next()\n\t}\n\n\tmw3 := func(c *tstCtx, j *Job, next NextMiddlewareFunc) error {\n\t\tc.record(\"mw3\")\n\t\treturn next()\n\t}\n\n\th1 := func(c *tstCtx, j *Job) error {\n\t\tc.record(\"h1\")\n\t\tc.record(j.Args[\"a\"].(string))\n\t\treturn nil\n\t}\n\n\tmiddleware := []*middlewareHandler{\n\t\t{IsGeneric: true, GenericMiddlewareHandler: mw1},\n\t\t{IsGeneric: false, DynamicMiddleware: reflect.ValueOf(mw2)},\n\t\t{IsGeneric: false, DynamicMiddleware: reflect.ValueOf(mw3)},\n\t}\n\n\tjt := &jobType{\n\t\tName:           \"foo\",\n\t\tIsGeneric:      false,\n\t\tDynamicHandler: reflect.ValueOf(h1),\n\t}\n\n\tjob := &Job{\n\t\tName: \"foo\",\n\t\tArgs: map[string]interface{}{\"a\": \"foo\"},\n\t}\n\n\tv, err := runJob(job, tstCtxType, middleware, jt)\n\tassert.NoError(t, err)\n\tc := v.Interface().(*tstCtx)\n\tassert.Equal(t, \"mw1mw2mw3h1foo\", c.String())\n}\n\nfunc TestRunHandlerError(t *testing.T) {\n\tmw1 := func(j *Job, next NextMiddlewareFunc) error {\n\t\treturn next()\n\t}\n\th1 := func(c *tstCtx, j *Job) error {\n\t\tc.record(\"h1\")\n\t\treturn fmt.Errorf(\"h1_err\")\n\t}\n\n\tmiddleware := []*middlewareHandler{\n\t\t{IsGeneric: true, GenericMiddlewareHandler: mw1},\n\t}\n\n\tjt := &jobType{\n\t\tName:           \"foo\",\n\t\tIsGeneric:      false,\n\t\tDynamicHandler: reflect.ValueOf(h1),\n\t}\n\n\tjob := &Job{\n\t\tName: \"foo\",\n\t}\n\n\tv, err := runJob(job, tstCtxType, middleware, jt)\n\tassert.Error(t, err)\n\tassert.Equal(t, \"h1_err\", err.Error())\n\n\tc := v.Interface().(*tstCtx)\n\tassert.Equal(t, \"h1\", c.String())\n}\n\nfunc TestRunMwError(t *testing.T) {\n\tmw1 := func(j *Job, next NextMiddlewareFunc) error {\n\t\treturn fmt.Errorf(\"mw1_err\")\n\t}\n\th1 := func(c *tstCtx, j *Job) error {\n\t\tc.record(\"h1\")\n\t\treturn fmt.Errorf(\"h1_err\")\n\t}\n\n\tmiddleware := []*middlewareHandler{\n\t\t{IsGeneric: true, GenericMiddlewareHandler: mw1},\n\t}\n\n\tjt := &jobType{\n\t\tName:           \"foo\",\n\t\tIsGeneric:      false,\n\t\tDynamicHandler: reflect.ValueOf(h1),\n\t}\n\n\tjob := &Job{\n\t\tName: \"foo\",\n\t}\n\n\t_, err := runJob(job, tstCtxType, middleware, jt)\n\tassert.Error(t, err)\n\tassert.Equal(t, \"mw1_err\", err.Error())\n}\n\nfunc TestRunHandlerPanic(t *testing.T) {\n\tmw1 := func(j *Job, next NextMiddlewareFunc) error {\n\t\treturn next()\n\t}\n\th1 := func(c *tstCtx, j *Job) error {\n\t\tc.record(\"h1\")\n\n\t\tpanic(\"dayam\")\n\t}\n\n\tmiddleware := []*middlewareHandler{\n\t\t{IsGeneric: true, GenericMiddlewareHandler: mw1},\n\t}\n\n\tjt := &jobType{\n\t\tName:           \"foo\",\n\t\tIsGeneric:      false,\n\t\tDynamicHandler: reflect.ValueOf(h1),\n\t}\n\n\tjob := &Job{\n\t\tName: \"foo\",\n\t}\n\n\t_, err := runJob(job, tstCtxType, middleware, jt)\n\tassert.Error(t, err)\n\tassert.Equal(t, \"dayam\", err.Error())\n}\n\nfunc TestRunMiddlewarePanic(t *testing.T) {\n\tmw1 := func(j *Job, next NextMiddlewareFunc) error {\n\t\tpanic(\"dayam\")\n\t}\n\th1 := func(c *tstCtx, j *Job) error {\n\t\tc.record(\"h1\")\n\t\treturn nil\n\t}\n\n\tmiddleware := []*middlewareHandler{\n\t\t{IsGeneric: true, GenericMiddlewareHandler: mw1},\n\t}\n\n\tjt := &jobType{\n\t\tName:           \"foo\",\n\t\tIsGeneric:      false,\n\t\tDynamicHandler: reflect.ValueOf(h1),\n\t}\n\n\tjob := &Job{\n\t\tName: \"foo\",\n\t}\n\n\t_, err := runJob(job, tstCtxType, middleware, jt)\n\tassert.Error(t, err)\n\tassert.Equal(t, \"dayam\", err.Error())\n}\n"
        },
        {
          "name": "time.go",
          "type": "blob",
          "size": 0.345703125,
          "content": "package work\n\nimport \"time\"\n\nvar nowMock int64\n\nfunc nowEpochSeconds() int64 {\n\tif nowMock != 0 {\n\t\treturn nowMock\n\t}\n\treturn time.Now().Unix()\n}\n\nfunc setNowEpochSecondsMock(t int64) {\n\tnowMock = t\n}\n\nfunc resetNowEpochSecondsMock() {\n\tnowMock = 0\n}\n\n// convert epoch seconds to a time\nfunc epochSecondsToTime(t int64) time.Time {\n\treturn time.Time{}\n}\n"
        },
        {
          "name": "todo.txt",
          "type": "blob",
          "size": 1.0283203125,
          "content": "IDEAS/TODO:\n----\n - zero context each time -- see if that affects performance\n - benchmarks for memory allocations\n - benchmarks for runJob\n - investigate changing runJob to use a shared context, and zero'ing it's value each time\n - revisit the retry backoff\n - generally, look into process scalability. Eg, if we have 30 processes, each with concurrency=25, that's a lot of pinging redis\n - thought: what if we *scale up* to max workers if some are idle, should we shut them down?\n   - thing we're guarding against: 100 goroutines all polling redis\n   - alt: some clever mechanism to only check redis if we are busy?\n - is there some way to detect redis contention, or overall, just measure the latency of redis\n   - both lock contention (not enuf redis pool)\n   - enuf pool, but redis itself is overloaded\n - It could be cool to provide an API for that redis stuff.\n   - latencies\n   - lock contention\n   - number of redis connections used by work\n   - overall redis stuff: mem, avail, cxns\n - it might be nice to have an overall counter like sidekiq\n"
        },
        {
          "name": "webui",
          "type": "tree",
          "content": null
        },
        {
          "name": "worker.go",
          "type": "blob",
          "size": 8.5732421875,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n)\n\nconst fetchKeysPerJobType = 6\n\ntype worker struct {\n\tworkerID      string\n\tpoolID        string\n\tnamespace     string\n\tpool          *redis.Pool\n\tjobTypes      map[string]*jobType\n\tsleepBackoffs []int64\n\tmiddleware    []*middlewareHandler\n\tcontextType   reflect.Type\n\n\tredisFetchScript *redis.Script\n\tsampler          prioritySampler\n\t*observer\n\n\tstopChan         chan struct{}\n\tdoneStoppingChan chan struct{}\n\n\tdrainChan        chan struct{}\n\tdoneDrainingChan chan struct{}\n}\n\nfunc newWorker(namespace string, poolID string, pool *redis.Pool, contextType reflect.Type, middleware []*middlewareHandler, jobTypes map[string]*jobType, sleepBackoffs []int64) *worker {\n\tworkerID := makeIdentifier()\n\tob := newObserver(namespace, pool, workerID)\n\n\tif len(sleepBackoffs) == 0 {\n\t\tsleepBackoffs = sleepBackoffsInMilliseconds\n\t}\n\n\tw := &worker{\n\t\tworkerID:      workerID,\n\t\tpoolID:        poolID,\n\t\tnamespace:     namespace,\n\t\tpool:          pool,\n\t\tcontextType:   contextType,\n\t\tsleepBackoffs: sleepBackoffs,\n\n\t\tobserver: ob,\n\n\t\tstopChan:         make(chan struct{}),\n\t\tdoneStoppingChan: make(chan struct{}),\n\n\t\tdrainChan:        make(chan struct{}),\n\t\tdoneDrainingChan: make(chan struct{}),\n\t}\n\n\tw.updateMiddlewareAndJobTypes(middleware, jobTypes)\n\n\treturn w\n}\n\n// note: can't be called while the thing is started\nfunc (w *worker) updateMiddlewareAndJobTypes(middleware []*middlewareHandler, jobTypes map[string]*jobType) {\n\tw.middleware = middleware\n\tsampler := prioritySampler{}\n\tfor _, jt := range jobTypes {\n\t\tsampler.add(jt.Priority,\n\t\t\tredisKeyJobs(w.namespace, jt.Name),\n\t\t\tredisKeyJobsInProgress(w.namespace, w.poolID, jt.Name),\n\t\t\tredisKeyJobsPaused(w.namespace, jt.Name),\n\t\t\tredisKeyJobsLock(w.namespace, jt.Name),\n\t\t\tredisKeyJobsLockInfo(w.namespace, jt.Name),\n\t\t\tredisKeyJobsConcurrency(w.namespace, jt.Name))\n\t}\n\tw.sampler = sampler\n\tw.jobTypes = jobTypes\n\tw.redisFetchScript = redis.NewScript(len(jobTypes)*fetchKeysPerJobType, redisLuaFetchJob)\n}\n\nfunc (w *worker) start() {\n\tgo w.loop()\n\tgo w.observer.start()\n}\n\nfunc (w *worker) stop() {\n\tw.stopChan <- struct{}{}\n\t<-w.doneStoppingChan\n\tw.observer.drain()\n\tw.observer.stop()\n}\n\nfunc (w *worker) drain() {\n\tw.drainChan <- struct{}{}\n\t<-w.doneDrainingChan\n\tw.observer.drain()\n}\n\nvar sleepBackoffsInMilliseconds = []int64{0, 10, 100, 1000, 5000}\n\nfunc (w *worker) loop() {\n\tvar drained bool\n\tvar consequtiveNoJobs int64\n\n\t// Begin immediately. We'll change the duration on each tick with a timer.Reset()\n\ttimer := time.NewTimer(0)\n\tdefer timer.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-w.stopChan:\n\t\t\tw.doneStoppingChan <- struct{}{}\n\t\t\treturn\n\t\tcase <-w.drainChan:\n\t\t\tdrained = true\n\t\t\ttimer.Reset(0)\n\t\tcase <-timer.C:\n\t\t\tjob, err := w.fetchJob()\n\t\t\tif err != nil {\n\t\t\t\tlogError(\"worker.fetch\", err)\n\t\t\t\ttimer.Reset(10 * time.Millisecond)\n\t\t\t} else if job != nil {\n\t\t\t\tw.processJob(job)\n\t\t\t\tconsequtiveNoJobs = 0\n\t\t\t\ttimer.Reset(0)\n\t\t\t} else {\n\t\t\t\tif drained {\n\t\t\t\t\tw.doneDrainingChan <- struct{}{}\n\t\t\t\t\tdrained = false\n\t\t\t\t}\n\t\t\t\tconsequtiveNoJobs++\n\t\t\t\tidx := consequtiveNoJobs\n\t\t\t\tif idx >= int64(len(w.sleepBackoffs)) {\n\t\t\t\t\tidx = int64(len(w.sleepBackoffs)) - 1\n\t\t\t\t}\n\t\t\t\ttimer.Reset(time.Duration(w.sleepBackoffs[idx]) * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (w *worker) fetchJob() (*Job, error) {\n\t// resort queues\n\t// NOTE: we could optimize this to only resort every second, or something.\n\tw.sampler.sample()\n\tnumKeys := len(w.sampler.samples) * fetchKeysPerJobType\n\tvar scriptArgs = make([]interface{}, 0, numKeys+1)\n\n\tfor _, s := range w.sampler.samples {\n\t\tscriptArgs = append(scriptArgs, s.redisJobs, s.redisJobsInProg, s.redisJobsPaused, s.redisJobsLock, s.redisJobsLockInfo, s.redisJobsMaxConcurrency) // KEYS[1-6 * N]\n\t}\n\tscriptArgs = append(scriptArgs, w.poolID) // ARGV[1]\n\tconn := w.pool.Get()\n\tdefer conn.Close()\n\n\tvalues, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...))\n\tif err == redis.ErrNil {\n\t\treturn nil, nil\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(values) != 3 {\n\t\treturn nil, fmt.Errorf(\"need 3 elements back\")\n\t}\n\n\trawJSON, ok := values[0].([]byte)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"response msg not bytes\")\n\t}\n\n\tdequeuedFrom, ok := values[1].([]byte)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"response queue not bytes\")\n\t}\n\n\tinProgQueue, ok := values[2].([]byte)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"response in prog not bytes\")\n\t}\n\n\tjob, err := newJob(rawJSON, dequeuedFrom, inProgQueue)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn job, nil\n}\n\nfunc (w *worker) processJob(job *Job) {\n\tif job.Unique {\n\t\tupdatedJob := w.getAndDeleteUniqueJob(job)\n\t\t// This is to support the old way of doing it, where we used the job off the queue and just deleted the unique key\n\t\t// Going forward the job on the queue will always be just a placeholder, and we will be replacing it with the\n\t\t// updated job extracted here\n\t\tif updatedJob != nil {\n\t\t\tjob = updatedJob\n\t\t}\n\t}\n\tvar runErr error\n\tjt := w.jobTypes[job.Name]\n\tif jt == nil {\n\t\trunErr = fmt.Errorf(\"stray job: no handler\")\n\t\tlogError(\"process_job.stray\", runErr)\n\t} else {\n\t\tw.observeStarted(job.Name, job.ID, job.Args)\n\t\tjob.observer = w.observer // for Checkin\n\t\t_, runErr = runJob(job, w.contextType, w.middleware, jt)\n\t\tw.observeDone(job.Name, job.ID, runErr)\n\t}\n\n\tfate := terminateOnly\n\tif runErr != nil {\n\t\tjob.failed(runErr)\n\t\tfate = w.jobFate(jt, job)\n\t}\n\tw.removeJobFromInProgress(job, fate)\n}\n\nfunc (w *worker) getAndDeleteUniqueJob(job *Job) *Job {\n\tvar uniqueKey string\n\tvar err error\n\n\tif job.UniqueKey != \"\" {\n\t\tuniqueKey = job.UniqueKey\n\t} else { // For jobs put in queue prior to this change. In the future this can be deleted as there will always be a UniqueKey\n\t\tuniqueKey, err = redisKeyUniqueJob(w.namespace, job.Name, job.Args)\n\t\tif err != nil {\n\t\t\tlogError(\"worker.delete_unique_job.key\", err)\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tconn := w.pool.Get()\n\tdefer conn.Close()\n\n\trawJSON, err := redis.Bytes(conn.Do(\"GET\", uniqueKey))\n\tif err != nil {\n\t\tlogError(\"worker.delete_unique_job.get\", err)\n\t\treturn nil\n\t}\n\n\t_, err = conn.Do(\"DEL\", uniqueKey)\n\tif err != nil {\n\t\tlogError(\"worker.delete_unique_job.del\", err)\n\t\treturn nil\n\t}\n\n\t// Previous versions did not support updated arguments and just set key to 1, so in these cases we should do nothing.\n\t// In the future this can be deleted, as we will always be getting arguments from here\n\tif string(rawJSON) == \"1\" {\n\t\treturn nil\n\t}\n\n\t// The job pulled off the queue was just a placeholder with no args, so replace it\n\tjobWithArgs, err := newJob(rawJSON, job.dequeuedFrom, job.inProgQueue)\n\tif err != nil {\n\t\tlogError(\"worker.delete_unique_job.updated_job\", err)\n\t\treturn nil\n\t}\n\n\treturn jobWithArgs\n}\n\nfunc (w *worker) removeJobFromInProgress(job *Job, fate terminateOp) {\n\tconn := w.pool.Get()\n\tdefer conn.Close()\n\n\tconn.Send(\"MULTI\")\n\tconn.Send(\"LREM\", job.inProgQueue, 1, job.rawJSON)\n\tconn.Send(\"DECR\", redisKeyJobsLock(w.namespace, job.Name))\n\tconn.Send(\"HINCRBY\", redisKeyJobsLockInfo(w.namespace, job.Name), w.poolID, -1)\n\tfate(conn)\n\tif _, err := conn.Do(\"EXEC\"); err != nil {\n\t\tlogError(\"worker.remove_job_from_in_progress.lrem\", err)\n\t}\n}\n\ntype terminateOp func(conn redis.Conn)\n\nfunc terminateOnly(_ redis.Conn) { return }\nfunc terminateAndRetry(w *worker, jt *jobType, job *Job) terminateOp {\n\trawJSON, err := job.serialize()\n\tif err != nil {\n\t\tlogError(\"worker.terminate_and_retry.serialize\", err)\n\t\treturn terminateOnly\n\t}\n\treturn func(conn redis.Conn) {\n\t\tconn.Send(\"ZADD\", redisKeyRetry(w.namespace), nowEpochSeconds()+jt.calcBackoff(job), rawJSON)\n\t}\n}\nfunc terminateAndDead(w *worker, job *Job) terminateOp {\n\trawJSON, err := job.serialize()\n\tif err != nil {\n\t\tlogError(\"worker.terminate_and_dead.serialize\", err)\n\t\treturn terminateOnly\n\t}\n\treturn func(conn redis.Conn) {\n\t\t// NOTE: sidekiq limits the # of jobs: only keep jobs for 6 months, and only keep a max # of jobs\n\t\t// The max # of jobs seems really horrible. Seems like operations should be on top of it.\n\t\t// conn.Send(\"ZREMRANGEBYSCORE\", redisKeyDead(w.namespace), \"-inf\", now - keepInterval)\n\t\t// conn.Send(\"ZREMRANGEBYRANK\", redisKeyDead(w.namespace), 0, -maxJobs)\n\n\t\tconn.Send(\"ZADD\", redisKeyDead(w.namespace), nowEpochSeconds(), rawJSON)\n\t}\n}\n\nfunc (w *worker) jobFate(jt *jobType, job *Job) terminateOp {\n\tif jt != nil {\n\t\tfailsRemaining := int64(jt.MaxFails) - job.Fails\n\t\tif failsRemaining > 0 {\n\t\t\treturn terminateAndRetry(w, jt, job)\n\t\t}\n\t\tif jt.SkipDead {\n\t\t\treturn terminateOnly\n\t\t}\n\t}\n\treturn terminateAndDead(w, job)\n}\n\n// Default algorithm returns an fastly increasing backoff counter which grows in an unbounded fashion\nfunc defaultBackoffCalculator(job *Job) int64 {\n\tfails := job.Fails\n\treturn (fails * fails * fails * fails) + 15 + (rand.Int63n(30) * (fails + 1))\n}\n"
        },
        {
          "name": "worker_pool.go",
          "type": "blob",
          "size": 13.5,
          "content": "package work\n\nimport (\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// WorkerPool represents a pool of workers. It forms the primary API of gocraft/work. WorkerPools provide the public API of gocraft/work. You can attach jobs and middlware to them. You can start and stop them. Based on their concurrency setting, they'll spin up N worker goroutines.\ntype WorkerPool struct {\n\tworkerPoolID  string\n\tconcurrency   uint\n\tnamespace     string // eg, \"myapp-work\"\n\tpool          *redis.Pool\n\tsleepBackoffs []int64\n\n\tcontextType  reflect.Type\n\tjobTypes     map[string]*jobType\n\tmiddleware   []*middlewareHandler\n\tstarted      bool\n\tperiodicJobs []*periodicJob\n\n\tworkers          []*worker\n\theartbeater      *workerPoolHeartbeater\n\tretrier          *requeuer\n\tscheduler        *requeuer\n\tdeadPoolReaper   *deadPoolReaper\n\tperiodicEnqueuer *periodicEnqueuer\n}\n\ntype jobType struct {\n\tName string\n\tJobOptions\n\n\tIsGeneric      bool\n\tGenericHandler GenericHandler\n\tDynamicHandler reflect.Value\n}\n\nfunc (jt *jobType) calcBackoff(j *Job) int64 {\n\tif jt.Backoff == nil {\n\t\treturn defaultBackoffCalculator(j)\n\t}\n\treturn jt.Backoff(j)\n}\n\n// You may provide your own backoff function for retrying failed jobs or use the builtin one.\n// Returns the number of seconds to wait until the next attempt.\n//\n// The builtin backoff calculator provides an exponentially increasing wait function.\ntype BackoffCalculator func(job *Job) int64\n\n// JobOptions can be passed to JobWithOptions.\ntype JobOptions struct {\n\tPriority       uint              // Priority from 1 to 10000\n\tMaxFails       uint              // 1: send straight to dead (unless SkipDead)\n\tSkipDead       bool              // If true, don't send failed jobs to the dead queue when retries are exhausted.\n\tMaxConcurrency uint              // Max number of jobs to keep in flight (default is 0, meaning no max)\n\tBackoff        BackoffCalculator // If not set, uses the default backoff algorithm\n}\n\n// WorkerPoolOptions can be passed to NewWorkerPoolWithOptions.\ntype WorkerPoolOptions struct {\n\tSleepBackoffs []int64 // Sleep backoffs in milliseconds\n}\n\n// GenericHandler is a job handler without any custom context.\ntype GenericHandler func(*Job) error\n\n// GenericMiddlewareHandler is a middleware without any custom context.\ntype GenericMiddlewareHandler func(*Job, NextMiddlewareFunc) error\n\n// NextMiddlewareFunc is a function type (whose instances are named 'next') that you call to advance to the next middleware.\ntype NextMiddlewareFunc func() error\n\ntype middlewareHandler struct {\n\tIsGeneric                bool\n\tDynamicMiddleware        reflect.Value\n\tGenericMiddlewareHandler GenericMiddlewareHandler\n}\n\n// NewWorkerPool creates a new worker pool. ctx should be a struct literal whose type will be used for middleware and handlers.\n// concurrency specifies how many workers to spin up - each worker can process jobs concurrently.\nfunc NewWorkerPool(ctx interface{}, concurrency uint, namespace string, pool *redis.Pool) *WorkerPool {\n\treturn NewWorkerPoolWithOptions(ctx, concurrency, namespace, pool, WorkerPoolOptions{})\n}\n\n// NewWorkerPoolWithOptions creates a new worker pool as per the NewWorkerPool function, but permits you to specify\n// additional options such as sleep backoffs.\nfunc NewWorkerPoolWithOptions(ctx interface{}, concurrency uint, namespace string, pool *redis.Pool, workerPoolOpts WorkerPoolOptions) *WorkerPool {\n\tif pool == nil {\n\t\tpanic(\"NewWorkerPool needs a non-nil *redis.Pool\")\n\t}\n\n\tctxType := reflect.TypeOf(ctx)\n\tvalidateContextType(ctxType)\n\twp := &WorkerPool{\n\t\tworkerPoolID:  makeIdentifier(),\n\t\tconcurrency:   concurrency,\n\t\tnamespace:     namespace,\n\t\tpool:          pool,\n\t\tsleepBackoffs: workerPoolOpts.SleepBackoffs,\n\t\tcontextType:   ctxType,\n\t\tjobTypes:      make(map[string]*jobType),\n\t}\n\n\tfor i := uint(0); i < wp.concurrency; i++ {\n\t\tw := newWorker(wp.namespace, wp.workerPoolID, wp.pool, wp.contextType, nil, wp.jobTypes, wp.sleepBackoffs)\n\t\twp.workers = append(wp.workers, w)\n\t}\n\n\treturn wp\n}\n\n// Middleware appends the specified function to the middleware chain. The fn can take one of these forms:\n// (*ContextType).func(*Job, NextMiddlewareFunc) error, (ContextType matches the type of ctx specified when creating a pool)\n// func(*Job, NextMiddlewareFunc) error, for the generic middleware format.\nfunc (wp *WorkerPool) Middleware(fn interface{}) *WorkerPool {\n\tvfn := reflect.ValueOf(fn)\n\tvalidateMiddlewareType(wp.contextType, vfn)\n\n\tmw := &middlewareHandler{\n\t\tDynamicMiddleware: vfn,\n\t}\n\n\tif gmh, ok := fn.(func(*Job, NextMiddlewareFunc) error); ok {\n\t\tmw.IsGeneric = true\n\t\tmw.GenericMiddlewareHandler = gmh\n\t}\n\n\twp.middleware = append(wp.middleware, mw)\n\n\tfor _, w := range wp.workers {\n\t\tw.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes)\n\t}\n\n\treturn wp\n}\n\n// Job registers the job name to the specified handler fn. For instance, when workers pull jobs from the name queue they'll be processed by the specified handler function.\n// fn can take one of these forms:\n// (*ContextType).func(*Job) error, (ContextType matches the type of ctx specified when creating a pool)\n// func(*Job) error, for the generic handler format.\nfunc (wp *WorkerPool) Job(name string, fn interface{}) *WorkerPool {\n\treturn wp.JobWithOptions(name, JobOptions{}, fn)\n}\n\n// JobWithOptions adds a handler for 'name' jobs as per the Job function, but permits you specify additional options\n// such as a job's priority, retry count, and whether to send dead jobs to the dead job queue or trash them.\nfunc (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool {\n\tjobOpts = applyDefaultsAndValidate(jobOpts)\n\n\tvfn := reflect.ValueOf(fn)\n\tvalidateHandlerType(wp.contextType, vfn)\n\tjt := &jobType{\n\t\tName:           name,\n\t\tDynamicHandler: vfn,\n\t\tJobOptions:     jobOpts,\n\t}\n\tif gh, ok := fn.(func(*Job) error); ok {\n\t\tjt.IsGeneric = true\n\t\tjt.GenericHandler = gh\n\t}\n\n\twp.jobTypes[name] = jt\n\n\tfor _, w := range wp.workers {\n\t\tw.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes)\n\t}\n\n\treturn wp\n}\n\n// PeriodicallyEnqueue will periodically enqueue jobName according to the cron-based spec.\n// The spec format is based on https://godoc.org/github.com/robfig/cron, which is a relatively standard cron format.\n// Note that the first value is the seconds!\n// If you have multiple worker pools on different machines, they'll all coordinate and only enqueue your job once.\nfunc (wp *WorkerPool) PeriodicallyEnqueue(spec string, jobName string) *WorkerPool {\n\tp := cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)\n\n\tschedule, err := p.Parse(spec)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\twp.periodicJobs = append(wp.periodicJobs, &periodicJob{jobName: jobName, spec: spec, schedule: schedule})\n\n\treturn wp\n}\n\n// Start starts the workers and associated processes.\nfunc (wp *WorkerPool) Start() {\n\tif wp.started {\n\t\treturn\n\t}\n\twp.started = true\n\n\t// TODO: we should cleanup stale keys on startup from previously registered jobs\n\twp.writeConcurrencyControlsToRedis()\n\tgo wp.writeKnownJobsToRedis()\n\n\tfor _, w := range wp.workers {\n\t\tgo w.start()\n\t}\n\n\twp.heartbeater = newWorkerPoolHeartbeater(wp.namespace, wp.pool, wp.workerPoolID, wp.jobTypes, wp.concurrency, wp.workerIDs())\n\twp.heartbeater.start()\n\twp.startRequeuers()\n\twp.periodicEnqueuer = newPeriodicEnqueuer(wp.namespace, wp.pool, wp.periodicJobs)\n\twp.periodicEnqueuer.start()\n}\n\n// Stop stops the workers and associated processes.\nfunc (wp *WorkerPool) Stop() {\n\tif !wp.started {\n\t\treturn\n\t}\n\twp.started = false\n\n\twg := sync.WaitGroup{}\n\tfor _, w := range wp.workers {\n\t\twg.Add(1)\n\t\tgo func(w *worker) {\n\t\t\tw.stop()\n\t\t\twg.Done()\n\t\t}(w)\n\t}\n\twg.Wait()\n\twp.heartbeater.stop()\n\twp.retrier.stop()\n\twp.scheduler.stop()\n\twp.deadPoolReaper.stop()\n\twp.periodicEnqueuer.stop()\n}\n\n// Drain drains all jobs in the queue before returning. Note that if jobs are added faster than we can process them, this function wouldn't return.\nfunc (wp *WorkerPool) Drain() {\n\twg := sync.WaitGroup{}\n\tfor _, w := range wp.workers {\n\t\twg.Add(1)\n\t\tgo func(w *worker) {\n\t\t\tw.drain()\n\t\t\twg.Done()\n\t\t}(w)\n\t}\n\twg.Wait()\n}\n\nfunc (wp *WorkerPool) startRequeuers() {\n\tjobNames := make([]string, 0, len(wp.jobTypes))\n\tfor k := range wp.jobTypes {\n\t\tjobNames = append(jobNames, k)\n\t}\n\twp.retrier = newRequeuer(wp.namespace, wp.pool, redisKeyRetry(wp.namespace), jobNames)\n\twp.scheduler = newRequeuer(wp.namespace, wp.pool, redisKeyScheduled(wp.namespace), jobNames)\n\twp.deadPoolReaper = newDeadPoolReaper(wp.namespace, wp.pool, jobNames)\n\twp.retrier.start()\n\twp.scheduler.start()\n\twp.deadPoolReaper.start()\n}\n\nfunc (wp *WorkerPool) workerIDs() []string {\n\twids := make([]string, 0, len(wp.workers))\n\tfor _, w := range wp.workers {\n\t\twids = append(wids, w.workerID)\n\t}\n\tsort.Strings(wids)\n\treturn wids\n}\n\nfunc (wp *WorkerPool) writeKnownJobsToRedis() {\n\tif len(wp.jobTypes) == 0 {\n\t\treturn\n\t}\n\n\tconn := wp.pool.Get()\n\tdefer conn.Close()\n\tkey := redisKeyKnownJobs(wp.namespace)\n\tjobNames := make([]interface{}, 0, len(wp.jobTypes)+1)\n\tjobNames = append(jobNames, key)\n\tfor k := range wp.jobTypes {\n\t\tjobNames = append(jobNames, k)\n\t}\n\n\tif _, err := conn.Do(\"SADD\", jobNames...); err != nil {\n\t\tlogError(\"write_known_jobs\", err)\n\t}\n}\n\nfunc (wp *WorkerPool) writeConcurrencyControlsToRedis() {\n\tif len(wp.jobTypes) == 0 {\n\t\treturn\n\t}\n\n\tconn := wp.pool.Get()\n\tdefer conn.Close()\n\tfor jobName, jobType := range wp.jobTypes {\n\t\tif _, err := conn.Do(\"SET\", redisKeyJobsConcurrency(wp.namespace, jobName), jobType.MaxConcurrency); err != nil {\n\t\t\tlogError(\"write_concurrency_controls_max_concurrency\", err)\n\t\t}\n\t}\n}\n\n// validateContextType will panic if context is invalid\nfunc validateContextType(ctxType reflect.Type) {\n\tif ctxType.Kind() != reflect.Struct {\n\t\tpanic(\"work: Context needs to be a struct type\")\n\t}\n}\n\nfunc validateHandlerType(ctxType reflect.Type, vfn reflect.Value) {\n\tif !isValidHandlerType(ctxType, vfn) {\n\t\tpanic(instructiveMessage(vfn, \"a handler\", \"handler\", \"job *work.Job\", ctxType))\n\t}\n}\n\nfunc validateMiddlewareType(ctxType reflect.Type, vfn reflect.Value) {\n\tif !isValidMiddlewareType(ctxType, vfn) {\n\t\tpanic(instructiveMessage(vfn, \"middleware\", \"middleware\", \"job *work.Job, next NextMiddlewareFunc\", ctxType))\n\t}\n}\n\n// Since it's easy to pass the wrong method as a middleware/handler, and since the user can't rely on static type checking since we use reflection,\n// lets be super helpful about what they did and what they need to do.\n// Arguments:\n//  - vfn is the failed method\n//  - addingType is for \"You are adding {addingType} to a worker pool...\". Eg, \"middleware\" or \"a handler\"\n//  - yourType is for \"Your {yourType} function can have...\". Eg, \"middleware\" or \"handler\" or \"error handler\"\n//  - args is like \"rw web.ResponseWriter, req *web.Request, next web.NextMiddlewareFunc\"\n//    - NOTE: args can be calculated if you pass in each type. BUT, it doesn't have example argument name, so it has less copy/paste value.\nfunc instructiveMessage(vfn reflect.Value, addingType string, yourType string, args string, ctxType reflect.Type) string {\n\t// Get context type without package.\n\tctxString := ctxType.String()\n\tsplitted := strings.Split(ctxString, \".\")\n\tif len(splitted) <= 1 {\n\t\tctxString = splitted[0]\n\t} else {\n\t\tctxString = splitted[1]\n\t}\n\n\tstr := \"\\n\" + strings.Repeat(\"*\", 120) + \"\\n\"\n\tstr += \"* You are adding \" + addingType + \" to a worker pool with context type '\" + ctxString + \"'\\n\"\n\tstr += \"*\\n*\\n\"\n\tstr += \"* Your \" + yourType + \" function can have one of these signatures:\\n\"\n\tstr += \"*\\n\"\n\tstr += \"* // If you don't need context:\\n\"\n\tstr += \"* func YourFunctionName(\" + args + \") error\\n\"\n\tstr += \"*\\n\"\n\tstr += \"* // If you want your \" + yourType + \" to accept a context:\\n\"\n\tstr += \"* func (c *\" + ctxString + \") YourFunctionName(\" + args + \") error  // or,\\n\"\n\tstr += \"* func YourFunctionName(c *\" + ctxString + \", \" + args + \") error\\n\"\n\tstr += \"*\\n\"\n\tstr += \"* Unfortunately, your function has this signature: \" + vfn.Type().String() + \"\\n\"\n\tstr += \"*\\n\"\n\tstr += strings.Repeat(\"*\", 120) + \"\\n\"\n\n\treturn str\n}\n\nfunc isValidHandlerType(ctxType reflect.Type, vfn reflect.Value) bool {\n\tfnType := vfn.Type()\n\n\tif fnType.Kind() != reflect.Func {\n\t\treturn false\n\t}\n\n\tnumIn := fnType.NumIn()\n\tnumOut := fnType.NumOut()\n\n\tif numOut != 1 {\n\t\treturn false\n\t}\n\n\toutType := fnType.Out(0)\n\tvar e *error\n\n\tif outType != reflect.TypeOf(e).Elem() {\n\t\treturn false\n\t}\n\n\tvar j *Job\n\tif numIn == 1 {\n\t\tif fnType.In(0) != reflect.TypeOf(j) {\n\t\t\treturn false\n\t\t}\n\t} else if numIn == 2 {\n\t\tif fnType.In(0) != reflect.PtrTo(ctxType) {\n\t\t\treturn false\n\t\t}\n\t\tif fnType.In(1) != reflect.TypeOf(j) {\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc isValidMiddlewareType(ctxType reflect.Type, vfn reflect.Value) bool {\n\tfnType := vfn.Type()\n\n\tif fnType.Kind() != reflect.Func {\n\t\treturn false\n\t}\n\n\tnumIn := fnType.NumIn()\n\tnumOut := fnType.NumOut()\n\n\tif numOut != 1 {\n\t\treturn false\n\t}\n\n\toutType := fnType.Out(0)\n\tvar e *error\n\n\tif outType != reflect.TypeOf(e).Elem() {\n\t\treturn false\n\t}\n\n\tvar j *Job\n\tvar nfn NextMiddlewareFunc\n\tif numIn == 2 {\n\t\tif fnType.In(0) != reflect.TypeOf(j) {\n\t\t\treturn false\n\t\t}\n\t\tif fnType.In(1) != reflect.TypeOf(nfn) {\n\t\t\treturn false\n\t\t}\n\t} else if numIn == 3 {\n\t\tif fnType.In(0) != reflect.PtrTo(ctxType) {\n\t\t\treturn false\n\t\t}\n\t\tif fnType.In(1) != reflect.TypeOf(j) {\n\t\t\treturn false\n\t\t}\n\t\tif fnType.In(2) != reflect.TypeOf(nfn) {\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc applyDefaultsAndValidate(jobOpts JobOptions) JobOptions {\n\tif jobOpts.Priority == 0 {\n\t\tjobOpts.Priority = 1\n\t}\n\n\tif jobOpts.MaxFails == 0 {\n\t\tjobOpts.MaxFails = 4\n\t}\n\n\tif jobOpts.Priority > 100000 {\n\t\tpanic(\"work: JobOptions.Priority must be between 1 and 100000\")\n\t}\n\n\treturn jobOpts\n}\n"
        },
        {
          "name": "worker_pool_test.go",
          "type": "blob",
          "size": 7.705078125,
          "content": "package work\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype tstCtx struct {\n\ta int\n\tbytes.Buffer\n}\n\nfunc (c *tstCtx) record(s string) {\n\t_, _ = c.WriteString(s)\n}\n\nvar tstCtxType = reflect.TypeOf(tstCtx{})\n\nfunc TestWorkerPoolHandlerValidations(t *testing.T) {\n\tvar cases = []struct {\n\t\tfn   interface{}\n\t\tgood bool\n\t}{\n\t\t{func(j *Job) error { return nil }, true},\n\t\t{func(c *tstCtx, j *Job) error { return nil }, true},\n\t\t{func(c *tstCtx, j *Job) {}, false},\n\t\t{func(c *tstCtx, j *Job) string { return \"\" }, false},\n\t\t{func(c *tstCtx, j *Job) (error, string) { return nil, \"\" }, false},\n\t\t{func(c *tstCtx) error { return nil }, false},\n\t\t{func(c tstCtx, j *Job) error { return nil }, false},\n\t\t{func() error { return nil }, false},\n\t\t{func(c *tstCtx, j *Job, wat string) error { return nil }, false},\n\t}\n\n\tfor i, testCase := range cases {\n\t\tr := isValidHandlerType(tstCtxType, reflect.ValueOf(testCase.fn))\n\t\tif testCase.good != r {\n\t\t\tt.Errorf(\"idx %d: should return %v but returned %v\", i, testCase.good, r)\n\t\t}\n\t}\n}\n\nfunc TestWorkerPoolMiddlewareValidations(t *testing.T) {\n\tvar cases = []struct {\n\t\tfn   interface{}\n\t\tgood bool\n\t}{\n\t\t{func(j *Job, n NextMiddlewareFunc) error { return nil }, true},\n\t\t{func(c *tstCtx, j *Job, n NextMiddlewareFunc) error { return nil }, true},\n\t\t{func(c *tstCtx, j *Job) error { return nil }, false},\n\t\t{func(c *tstCtx, j *Job, n NextMiddlewareFunc) {}, false},\n\t\t{func(c *tstCtx, j *Job, n NextMiddlewareFunc) string { return \"\" }, false},\n\t\t{func(c *tstCtx, j *Job, n NextMiddlewareFunc) (error, string) { return nil, \"\" }, false},\n\t\t{func(c *tstCtx, n NextMiddlewareFunc) error { return nil }, false},\n\t\t{func(c tstCtx, j *Job, n NextMiddlewareFunc) error { return nil }, false},\n\t\t{func() error { return nil }, false},\n\t\t{func(c *tstCtx, j *Job, wat string) error { return nil }, false},\n\t\t{func(c *tstCtx, j *Job, n NextMiddlewareFunc, wat string) error { return nil }, false},\n\t}\n\n\tfor i, testCase := range cases {\n\t\tr := isValidMiddlewareType(tstCtxType, reflect.ValueOf(testCase.fn))\n\t\tif testCase.good != r {\n\t\t\tt.Errorf(\"idx %d: should return %v but returned %v\", i, testCase.good, r)\n\t\t}\n\t}\n}\n\nfunc TestWorkerPoolStartStop(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Start()\n\twp.Start()\n\twp.Stop()\n\twp.Stop()\n\twp.Start()\n\twp.Stop()\n}\n\nfunc TestWorkerPoolValidations(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\n\tfunc() {\n\t\tdefer func() {\n\t\t\tif panicErr := recover(); panicErr != nil {\n\t\t\t\tassert.Regexp(t, \"Your middleware function can have one of these signatures\", fmt.Sprintf(\"%v\", panicErr))\n\t\t\t} else {\n\t\t\t\tt.Errorf(\"expected a panic when using bad middleware\")\n\t\t\t}\n\t\t}()\n\n\t\twp.Middleware(TestWorkerPoolValidations)\n\t}()\n\n\tfunc() {\n\t\tdefer func() {\n\t\t\tif panicErr := recover(); panicErr != nil {\n\t\t\t\tassert.Regexp(t, \"Your handler function can have one of these signatures\", fmt.Sprintf(\"%v\", panicErr))\n\t\t\t} else {\n\t\t\t\tt.Errorf(\"expected a panic when using a bad handler\")\n\t\t\t}\n\t\t}()\n\n\t\twp.Job(\"wat\", TestWorkerPoolValidations)\n\t}()\n}\n\nfunc TestWorkersPoolRunSingleThreaded(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tnumJobs, concurrency, sleepTime := 5, 5, 2\n\twp := setupTestWorkerPool(pool, ns, job1, concurrency, JobOptions{Priority: 1, MaxConcurrency: 1})\n\twp.Start()\n\t// enqueue some jobs\n\tenqueuer := NewEnqueuer(ns, pool)\n\tfor i := 0; i < numJobs; i++ {\n\t\t_, err := enqueuer.Enqueue(job1, Q{\"sleep\": sleepTime})\n\t\tassert.Nil(t, err)\n\t}\n\n\t// make sure we've enough jobs queued up to make an interesting test\n\tjobsQueued := listSize(pool, redisKeyJobs(ns, job1))\n\tassert.True(t, jobsQueued >= 3, \"should be at least 3 jobs queued up, but only found %v\", jobsQueued)\n\n\t// now make sure the during the duration of job execution there is never > 1 job in flight\n\tstart := time.Now()\n\ttotalRuntime := time.Duration(sleepTime*numJobs) * time.Millisecond\n\ttime.Sleep(10 * time.Millisecond)\n\tfor time.Since(start) < totalRuntime {\n\t\t// jobs in progress, lock count for the job and lock info for the pool should never exceed 1\n\t\tjobsInProgress := listSize(pool, redisKeyJobsInProgress(ns, wp.workerPoolID, job1))\n\t\tassert.True(t, jobsInProgress <= 1, \"jobsInProgress should never exceed 1: actual=%d\", jobsInProgress)\n\n\t\tjobLockCount := getInt64(pool, redisKeyJobsLock(ns, job1))\n\t\tassert.True(t, jobLockCount <= 1, \"global lock count for job should never exceed 1, got: %v\", jobLockCount)\n\t\twpLockCount := hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), wp.workerPoolID)\n\t\tassert.True(t, wpLockCount <= 1, \"lock count for the worker pool should never exceed 1: actual=%v\", wpLockCount)\n\t\ttime.Sleep(time.Duration(sleepTime) * time.Millisecond)\n\t}\n\twp.Drain()\n\twp.Stop()\n\n\t// At this point it should all be empty.\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, wp.workerPoolID, job1)))\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), wp.workerPoolID))\n}\n\nfunc TestWorkerPoolPauseSingleThreadedJobs(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns, job1 := \"work\", \"job1\"\n\tnumJobs, concurrency, sleepTime := 5, 5, 2\n\twp := setupTestWorkerPool(pool, ns, job1, concurrency, JobOptions{Priority: 1, MaxConcurrency: 1})\n\twp.Start()\n\t// enqueue some jobs\n\tenqueuer := NewEnqueuer(ns, pool)\n\tfor i := 0; i < numJobs; i++ {\n\t\t_, err := enqueuer.Enqueue(job1, Q{\"sleep\": sleepTime})\n\t\tassert.Nil(t, err)\n\t}\n\t// provide time for jobs to process\n\ttime.Sleep(10 * time.Millisecond)\n\n\t// pause work, provide time for outstanding jobs to finish and queue up another job\n\tpauseJobs(ns, job1, pool)\n\ttime.Sleep(2 * time.Millisecond)\n\t_, err := enqueuer.Enqueue(job1, Q{\"sleep\": sleepTime})\n\tassert.Nil(t, err)\n\n\t// check that we still have some jobs to process\n\tassert.True(t, listSize(pool, redisKeyJobs(ns, job1)) >= 1)\n\n\t// now make sure no jobs get started until we unpause\n\tstart := time.Now()\n\ttotalRuntime := time.Duration(sleepTime*numJobs) * time.Millisecond\n\tfor time.Since(start) < totalRuntime {\n\t\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, wp.workerPoolID, job1)))\n\t\t// lock count for the job and lock info for the pool should both be at 1 while job is running\n\t\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\t\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), wp.workerPoolID))\n\t\ttime.Sleep(time.Duration(sleepTime) * time.Millisecond)\n\t}\n\n\t// unpause work and get past the backoff time\n\tunpauseJobs(ns, job1, pool)\n\ttime.Sleep(10 * time.Millisecond)\n\n\twp.Drain()\n\twp.Stop()\n\n\t// At this point it should all be empty.\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, wp.workerPoolID, job1)))\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), wp.workerPoolID))\n}\n\n// Test Helpers\nfunc (t *TestContext) SleepyJob(job *Job) error {\n\tsleepTime := time.Duration(job.ArgInt64(\"sleep\"))\n\ttime.Sleep(sleepTime * time.Millisecond)\n\treturn nil\n}\n\nfunc setupTestWorkerPool(pool *redis.Pool, namespace, jobName string, concurrency int, jobOpts JobOptions) *WorkerPool {\n\tdeleteQueue(pool, namespace, jobName)\n\tdeleteRetryAndDead(pool, namespace)\n\tdeletePausedAndLockedKeys(namespace, jobName, pool)\n\n\twp := NewWorkerPool(TestContext{}, uint(concurrency), namespace, pool)\n\twp.JobWithOptions(jobName, jobOpts, (*TestContext).SleepyJob)\n\t// reset the backoff times to help with testing\n\tsleepBackoffsInMilliseconds = []int64{10, 10, 10, 10, 10}\n\treturn wp\n}\n"
        },
        {
          "name": "worker_test.go",
          "type": "blob",
          "size": 17.01953125,
          "content": "package work\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gomodule/redigo/redis\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestWorkerBasics(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tjob2 := \"job2\"\n\tjob3 := \"job3\"\n\n\tcleanKeyspace(ns, pool)\n\n\tvar arg1 float64\n\tvar arg2 float64\n\tvar arg3 float64\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\targ1 = job.Args[\"a\"].(float64)\n\t\t\treturn nil\n\t\t},\n\t}\n\tjobTypes[job2] = &jobType{\n\t\tName:       job2,\n\t\tJobOptions: JobOptions{Priority: 1},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\targ2 = job.Args[\"a\"].(float64)\n\t\t\treturn nil\n\t\t},\n\t}\n\tjobTypes[job3] = &jobType{\n\t\tName:       job3,\n\t\tJobOptions: JobOptions{Priority: 1},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\targ3 = job.Args[\"a\"].(float64)\n\t\t\treturn nil\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, Q{\"a\": 1})\n\tassert.Nil(t, err)\n\t_, err = enqueuer.Enqueue(job2, Q{\"a\": 2})\n\tassert.Nil(t, err)\n\t_, err = enqueuer.Enqueue(job3, Q{\"a\": 3})\n\tassert.Nil(t, err)\n\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\tw.start()\n\tw.drain()\n\tw.stop()\n\n\t// make sure the jobs ran (side effect of setting these variables to the job arguments)\n\tassert.EqualValues(t, 1.0, arg1)\n\tassert.EqualValues(t, 2.0, arg2)\n\tassert.EqualValues(t, 3.0, arg3)\n\n\t// nothing in retries or dead\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyRetry(ns)))\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyDead(ns)))\n\n\t// Nothing in the queues or in-progress queues\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job2)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job3)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job2)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job3)))\n\n\t// nothing in the worker status\n\th := readHash(pool, redisKeyWorkerObservation(ns, w.workerID))\n\tassert.EqualValues(t, 0, len(h))\n}\n\nfunc TestWorkerInProgress(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tdeleteQueue(pool, ns, job1)\n\tdeleteRetryAndDead(pool, ns)\n\tdeletePausedAndLockedKeys(ns, job1, pool)\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\ttime.Sleep(30 * time.Millisecond)\n\t\t\treturn nil\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, Q{\"a\": 1})\n\tassert.Nil(t, err)\n\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\tw.start()\n\n\t// instead of w.forceIter(), we'll wait for 10 milliseconds to let the job start\n\t// The job will then sleep for 30ms. In that time, we should be able to see something in the in-progress queue.\n\ttime.Sleep(10 * time.Millisecond)\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\tassert.EqualValues(t, 1, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\tassert.EqualValues(t, 1, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), w.poolID))\n\n\t// nothing in the worker status\n\tw.observer.drain()\n\th := readHash(pool, redisKeyWorkerObservation(ns, w.workerID))\n\tassert.Equal(t, job1, h[\"job_name\"])\n\tassert.Equal(t, `{\"a\":1}`, h[\"args\"])\n\t// NOTE: we could check for job_id and started_at, but it's a PITA and it's tested in observer_test.\n\n\tw.drain()\n\tw.stop()\n\n\t// At this point, it should all be empty.\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\n\t// nothing in the worker status\n\th = readHash(pool, redisKeyWorkerObservation(ns, w.workerID))\n\tassert.EqualValues(t, 0, len(h))\n}\n\nfunc TestWorkerRetry(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tdeleteQueue(pool, ns, job1)\n\tdeleteRetryAndDead(pool, ns)\n\tdeletePausedAndLockedKeys(ns, job1, pool)\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1, MaxFails: 3},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\treturn fmt.Errorf(\"sorry kid\")\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, Q{\"a\": 1})\n\tassert.Nil(t, err)\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\tw.start()\n\tw.drain()\n\tw.stop()\n\n\t// Ensure the right stuff is in our queues:\n\tassert.EqualValues(t, 1, zsetSize(pool, redisKeyRetry(ns)))\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyDead(ns)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), w.poolID))\n\n\t// Get the job on the retry queue\n\tts, job := jobOnZset(pool, redisKeyRetry(ns))\n\n\tassert.True(t, ts > nowEpochSeconds())      // enqueued in the future\n\tassert.True(t, ts < (nowEpochSeconds()+80)) // but less than a minute from now (first failure)\n\n\tassert.Equal(t, job1, job.Name) // basics are preserved\n\tassert.EqualValues(t, 1, job.Fails)\n\tassert.Equal(t, \"sorry kid\", job.LastErr)\n\tassert.True(t, (nowEpochSeconds()-job.FailedAt) <= 2)\n}\n\n// Check if a custom backoff function functions functionally.\nfunc TestWorkerRetryWithCustomBackoff(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tdeleteQueue(pool, ns, job1)\n\tdeleteRetryAndDead(pool, ns)\n\tcalledCustom := 0\n\n\tcustombo := func(job *Job) int64 {\n\t\tcalledCustom++\n\t\treturn 5 // Always 5 seconds\n\t}\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1, MaxFails: 3, Backoff: custombo},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\treturn fmt.Errorf(\"sorry kid\")\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, Q{\"a\": 1})\n\tassert.Nil(t, err)\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\tw.start()\n\tw.drain()\n\tw.stop()\n\n\t// Ensure the right stuff is in our queues:\n\tassert.EqualValues(t, 1, zsetSize(pool, redisKeyRetry(ns)))\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyDead(ns)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\n\t// Get the job on the retry queue\n\tts, job := jobOnZset(pool, redisKeyRetry(ns))\n\n\tassert.True(t, ts > nowEpochSeconds())      // enqueued in the future\n\tassert.True(t, ts < (nowEpochSeconds()+10)) // but less than ten secs in\n\n\tassert.Equal(t, job1, job.Name) // basics are preserved\n\tassert.EqualValues(t, 1, job.Fails)\n\tassert.Equal(t, \"sorry kid\", job.LastErr)\n\tassert.True(t, (nowEpochSeconds()-job.FailedAt) <= 2)\n\tassert.Equal(t, 1, calledCustom)\n}\n\nfunc TestWorkerDead(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tjob2 := \"job2\"\n\tdeleteQueue(pool, ns, job1)\n\tdeleteQueue(pool, ns, job2)\n\tdeleteRetryAndDead(pool, ns)\n\tdeletePausedAndLockedKeys(ns, job1, pool)\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1, MaxFails: 0},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\treturn fmt.Errorf(\"sorry kid1\")\n\t\t},\n\t}\n\tjobTypes[job2] = &jobType{\n\t\tName:       job2,\n\t\tJobOptions: JobOptions{Priority: 1, MaxFails: 0, SkipDead: true},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\treturn fmt.Errorf(\"sorry kid2\")\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, nil)\n\tassert.Nil(t, err)\n\t_, err = enqueuer.Enqueue(job2, nil)\n\tassert.Nil(t, err)\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\tw.start()\n\tw.drain()\n\tw.stop()\n\n\t// Ensure the right stuff is in our queues:\n\tassert.EqualValues(t, 0, zsetSize(pool, redisKeyRetry(ns)))\n\tassert.EqualValues(t, 1, zsetSize(pool, redisKeyDead(ns)))\n\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job1)))\n\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job1), w.poolID))\n\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job2)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job2)))\n\tassert.EqualValues(t, 0, getInt64(pool, redisKeyJobsLock(ns, job2)))\n\tassert.EqualValues(t, 0, hgetInt64(pool, redisKeyJobsLockInfo(ns, job2), w.poolID))\n\n\t// Get the job on the dead queue\n\tts, job := jobOnZset(pool, redisKeyDead(ns))\n\n\tassert.True(t, ts <= nowEpochSeconds())\n\n\tassert.Equal(t, job1, job.Name) // basics are preserved\n\tassert.EqualValues(t, 1, job.Fails)\n\tassert.Equal(t, \"sorry kid1\", job.LastErr)\n\tassert.True(t, (nowEpochSeconds()-job.FailedAt) <= 2)\n}\n\nfunc TestWorkersPaused(t *testing.T) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tjob1 := \"job1\"\n\tdeleteQueue(pool, ns, job1)\n\tdeleteRetryAndDead(pool, ns)\n\tdeletePausedAndLockedKeys(ns, job1, pool)\n\n\tjobTypes := make(map[string]*jobType)\n\tjobTypes[job1] = &jobType{\n\t\tName:       job1,\n\t\tJobOptions: JobOptions{Priority: 1},\n\t\tIsGeneric:  true,\n\t\tGenericHandler: func(job *Job) error {\n\t\t\ttime.Sleep(30 * time.Millisecond)\n\t\t\treturn nil\n\t\t},\n\t}\n\n\tenqueuer := NewEnqueuer(ns, pool)\n\t_, err := enqueuer.Enqueue(job1, Q{\"a\": 1})\n\tassert.Nil(t, err)\n\n\tw := newWorker(ns, \"1\", pool, tstCtxType, nil, jobTypes, nil)\n\t// pause the jobs prior to starting\n\terr = pauseJobs(ns, job1, pool)\n\tassert.Nil(t, err)\n\t// reset the backoff times to help with testing\n\tsleepBackoffsInMilliseconds = []int64{10, 10, 10, 10, 10}\n\tw.start()\n\n\t// make sure the jobs stay in the still in the run queue and not moved to in progress\n\tfor i := 0; i < 2; i++ {\n\t\ttime.Sleep(10 * time.Millisecond)\n\t\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobs(ns, job1)))\n\t\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\t}\n\n\t// now unpause the jobs and check that they start\n\terr = unpauseJobs(ns, job1, pool)\n\tassert.Nil(t, err)\n\t// sleep through 2 backoffs to make sure we allow enough time to start running\n\ttime.Sleep(20 * time.Millisecond)\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 1, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\n\tw.observer.drain()\n\th := readHash(pool, redisKeyWorkerObservation(ns, w.workerID))\n\tassert.Equal(t, job1, h[\"job_name\"])\n\tassert.Equal(t, `{\"a\":1}`, h[\"args\"])\n\tw.drain()\n\tw.stop()\n\n\t// At this point, it should all be empty.\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobs(ns, job1)))\n\tassert.EqualValues(t, 0, listSize(pool, redisKeyJobsInProgress(ns, \"1\", job1)))\n\n\t// nothing in the worker status\n\th = readHash(pool, redisKeyWorkerObservation(ns, w.workerID))\n\tassert.EqualValues(t, 0, len(h))\n}\n\n// Test that in the case of an unavailable Redis server,\n// the worker loop exits in the case of a WorkerPool.Stop\nfunc TestStop(t *testing.T) {\n\tredisPool := &redis.Pool{\n\t\tDial: func() (redis.Conn, error) {\n\t\t\tc, err := redis.Dial(\"tcp\", \"notworking:6379\", redis.DialConnectTimeout(1*time.Second))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn c, nil\n\t\t},\n\t}\n\twp := NewWorkerPool(TestContext{}, 10, \"work\", redisPool)\n\twp.Start()\n\twp.Stop()\n}\n\nfunc BenchmarkJobProcessing(b *testing.B) {\n\tpool := newTestPool(\":6379\")\n\tns := \"work\"\n\tcleanKeyspace(ns, pool)\n\tenqueuer := NewEnqueuer(ns, pool)\n\n\tfor i := 0; i < b.N; i++ {\n\t\t_, err := enqueuer.Enqueue(\"wat\", nil)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\twp := NewWorkerPool(TestContext{}, 10, ns, pool)\n\twp.Job(\"wat\", func(c *TestContext, job *Job) error {\n\t\treturn nil\n\t})\n\n\tb.ResetTimer()\n\n\twp.Start()\n\twp.Drain()\n\twp.Stop()\n}\n\nfunc newTestPool(addr string) *redis.Pool {\n\treturn &redis.Pool{\n\t\tMaxActive:   10,\n\t\tMaxIdle:     10,\n\t\tIdleTimeout: 240 * time.Second,\n\t\tDial: func() (redis.Conn, error) {\n\t\t\tc, err := redis.Dial(\"tcp\", addr)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn c, nil\n\t\t},\n\t\tWait: true,\n\t}\n}\n\nfunc deleteQueue(pool *redis.Pool, namespace, jobName string) {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\t_, err := conn.Do(\"DEL\", redisKeyJobs(namespace, jobName), redisKeyJobsInProgress(namespace, \"1\", jobName))\n\tif err != nil {\n\t\tpanic(\"could not delete queue: \" + err.Error())\n\t}\n}\n\nfunc deleteRetryAndDead(pool *redis.Pool, namespace string) {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\t_, err := conn.Do(\"DEL\", redisKeyRetry(namespace), redisKeyDead(namespace))\n\tif err != nil {\n\t\tpanic(\"could not delete retry/dead queue: \" + err.Error())\n\t}\n}\n\nfunc zsetSize(pool *redis.Pool, key string) int64 {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Int64(conn.Do(\"ZCARD\", key))\n\tif err != nil {\n\t\tpanic(\"could not get ZSET size: \" + err.Error())\n\t}\n\treturn v\n}\n\nfunc listSize(pool *redis.Pool, key string) int64 {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Int64(conn.Do(\"LLEN\", key))\n\tif err != nil {\n\t\tpanic(\"could not get list length: \" + err.Error())\n\t}\n\treturn v\n}\n\nfunc getInt64(pool *redis.Pool, key string) int64 {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Int64(conn.Do(\"GET\", key))\n\tif err != nil {\n\t\tpanic(\"could not GET int64: \" + err.Error())\n\t}\n\treturn v\n}\n\nfunc hgetInt64(pool *redis.Pool, redisKey, hashKey string) int64 {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := redis.Int64(conn.Do(\"HGET\", redisKey, hashKey))\n\tif err != nil {\n\t\tpanic(\"could not HGET int64: \" + err.Error())\n\t}\n\treturn v\n}\n\nfunc jobOnZset(pool *redis.Pool, key string) (int64, *Job) {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tv, err := conn.Do(\"ZRANGE\", key, 0, 0, \"WITHSCORES\")\n\tif err != nil {\n\t\tpanic(\"ZRANGE error: \" + err.Error())\n\t}\n\n\tvv := v.([]interface{})\n\n\tjob, err := newJob(vv[0].([]byte), nil, nil)\n\tif err != nil {\n\t\tpanic(\"couldn't get job: \" + err.Error())\n\t}\n\n\tscore := vv[1].([]byte)\n\tscoreInt, err := strconv.ParseInt(string(score), 10, 64)\n\tif err != nil {\n\t\tpanic(\"couldn't parse int: \" + err.Error())\n\t}\n\n\treturn scoreInt, job\n}\n\nfunc jobOnQueue(pool *redis.Pool, key string) *Job {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\trawJSON, err := redis.Bytes(conn.Do(\"RPOP\", key))\n\tif err != nil {\n\t\tpanic(\"could RPOP from job queue: \" + err.Error())\n\t}\n\n\tjob, err := newJob(rawJSON, nil, nil)\n\tif err != nil {\n\t\tpanic(\"couldn't get job: \" + err.Error())\n\t}\n\n\treturn job\n}\n\nfunc knownJobs(pool *redis.Pool, key string) []string {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tjobNames, err := redis.Strings(conn.Do(\"SMEMBERS\", key))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn jobNames\n}\n\nfunc cleanKeyspace(namespace string, pool *redis.Pool) {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tkeys, err := redis.Strings(conn.Do(\"KEYS\", namespace+\"*\"))\n\tif err != nil {\n\t\tpanic(\"could not get keys: \" + err.Error())\n\t}\n\tfor _, k := range keys {\n\t\tif _, err := conn.Do(\"DEL\", k); err != nil {\n\t\t\tpanic(\"could not del: \" + err.Error())\n\t\t}\n\t}\n}\n\nfunc pauseJobs(namespace, jobName string, pool *redis.Pool) error {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tif _, err := conn.Do(\"SET\", redisKeyJobsPaused(namespace, jobName), \"1\"); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc unpauseJobs(namespace, jobName string, pool *redis.Pool) error {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tif _, err := conn.Do(\"DEL\", redisKeyJobsPaused(namespace, jobName)); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc deletePausedAndLockedKeys(namespace, jobName string, pool *redis.Pool) error {\n\tconn := pool.Get()\n\tdefer conn.Close()\n\n\tif _, err := conn.Do(\"DEL\", redisKeyJobsPaused(namespace, jobName)); err != nil {\n\t\treturn err\n\t}\n\tif _, err := conn.Do(\"DEL\", redisKeyJobsLock(namespace, jobName)); err != nil {\n\t\treturn err\n\t}\n\tif _, err := conn.Do(\"DEL\", redisKeyJobsLockInfo(namespace, jobName)); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\ntype emptyCtx struct{}\n\n// Starts up a pool with two workers emptying it as fast as they can\n// The pool is Stop()ped while jobs are still going on.  Tests that the\n// pool processing is really stopped and that it's not first completely\n// drained before returning.\n// https://github.com/gocraft/work/issues/24\nfunc TestWorkerPoolStop(t *testing.T) {\n\tns := \"will_it_end\"\n\tpool := newTestPool(\":6379\")\n\tvar started, stopped int32\n\tnum_iters := 30\n\n\twp := NewWorkerPool(emptyCtx{}, 2, ns, pool)\n\n\twp.Job(\"sample_job\", func(c *emptyCtx, job *Job) error {\n\t\tatomic.AddInt32(&started, 1)\n\t\ttime.Sleep(1 * time.Second)\n\t\tatomic.AddInt32(&stopped, 1)\n\t\treturn nil\n\t})\n\n\tvar enqueuer = NewEnqueuer(ns, pool)\n\n\tfor i := 0; i <= num_iters; i++ {\n\t\tenqueuer.Enqueue(\"sample_job\", Q{})\n\t}\n\n\t// Start the pool and quit before it has had a chance to complete\n\t// all the jobs.\n\twp.Start()\n\ttime.Sleep(5 * time.Second)\n\twp.Stop()\n\n\tif started != stopped {\n\t\tt.Errorf(\"Expected that jobs were finished and not killed while processing (started=%d, stopped=%d)\", started, stopped)\n\t}\n\n\tif started >= int32(num_iters) {\n\t\tt.Errorf(\"Expected that jobs queue was not completely emptied.\")\n\t}\n}\n"
        }
      ]
    }
  ]
}