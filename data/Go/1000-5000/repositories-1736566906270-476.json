{
  "metadata": {
    "timestamp": 1736566906270,
    "page": 476,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "buraksezer/olric",
      "stars": 3174,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.353515625,
          "content": "# Binaries for programs and plugins\n*.exe\n*.dll\n*.so\n*.dylib\n\n# Vim creates this\n*.swp\n\n# Test binary, build with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n*.out\n\n# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736\n.glide/\n\n# GoLand creates this\n.idea/\n\n# OSX creates this\n.DS_Store\n\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3671875,
          "content": "FROM golang:latest as build\nWORKDIR /src/\nCOPY . /src/\nRUN go mod download\nRUN CGO_ENABLED=1 go build -ldflags=\"-s -w\" -o /usr/bin/olricd /src/cmd/olricd\n\nFROM gcr.io/distroless/base-debian11\nCOPY --from=build /usr/bin/olricd /usr/bin/olricd\nCOPY --from=build /src/olricd-docker.yaml /etc/olricd.yaml\n\nEXPOSE 3320 3322\nENTRYPOINT [\"/usr/bin/olricd\", \"-c\", \"/etc/olricd.yaml\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 10.5126953125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        https://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   Copyright 2018-2024 Burak Sezer.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 61.4833984375,
          "content": "# Olric [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Olric%3A+Distributed+and+in-memory+key%2Fvalue+database.+It+can+be+used+both+as+an+embedded+Go+library+and+as+a+language-independent+service.+&url=https://github.com/buraksezer/olric&hashtags=golang,distributed,database)\n\n[![Go Reference](https://pkg.go.dev/badge/github.com/buraksezer/olric.svg)](https://pkg.go.dev/github.com/buraksezer/olric) [![Coverage Status](https://coveralls.io/repos/github/buraksezer/olric/badge.svg?branch=master)](https://coveralls.io/github/buraksezer/olric?branch=master) [![Build Status](https://travis-ci.org/buraksezer/olric.svg?branch=master)](https://travis-ci.org/buraksezer/olric) [![Go Report Card](https://goreportcard.com/badge/github.com/buraksezer/olric)](https://goreportcard.com/report/github.com/buraksezer/olric) [![Discord](https://img.shields.io/discord/721708998021087273.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/ahK7Vjr8We) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nOlric is a distributed, in-memory key/value store and cache. It's designed from the ground up to be distributed, and it can be \nused both as an embedded Go library and as a language-independent service.\n\nWith Olric, you can instantly create a fast, scalable, shared pool of RAM across a cluster of computers.\n\nOlric is implemented in [Go](https://go.dev/) and uses the [Redis serialization protocol](https://redis.io/topics/protocol). So Olric has client implementations in all major programming \nlanguages.\n\nOlric is highly scalable and available. Distributed applications can use it for distributed caching, clustering and \npublish-subscribe messaging.\n\nIt is designed to scale out to hundreds of members and thousands of clients. When you add new members, they automatically \ndiscover the cluster and linearly increase the memory capacity. Olric offers simple scalability, partitioning (sharding), \nand re-balancing out-of-the-box. It does not require any extra coordination processes. With Olric, when you start another \nprocess to add more capacity, data and backups are automatically and evenly balanced. \n\nSee [Docker](#docker) and [Samples](#samples) sections to get started! \n\nJoin our [Discord server!](https://discord.gg/ahK7Vjr8We)\n\nThe current production version is [v0.5.7](https://github.com/buraksezer/olric/tree/release/v0.5.0#olric-)\n\n### About versions\n\nOlric v0.4 and previous versions use *Olric Binary Protocol*, v0.5.x and later use [Redis serialization protocol](https://redis.io/docs/reference/protocol-spec/) for communication and the API was significantly changed.\nOlric v0.4.x tree is going to receive bug fixes and security updates forever, but I would recommend considering an upgrade to the new version.\n\nThis document only covers `v0.5.x` and later. See v0.4.x documents [here](https://github.com/buraksezer/olric/tree/release/v0.4.0#olric-).\n\n## At a glance\n\n* Designed to share some transient, approximate, fast-changing data between servers,\n* Uses Redis serialization protocol,\n* Implements a distributed hash table,\n* Provides a drop-in replacement for Redis Publish/Subscribe messaging system,\n* Supports both programmatic and declarative configuration, \n* Embeddable but can be used as a language-independent service with *olricd*,\n* Supports different eviction algorithms (including LRU and TTL),\n* Highly available and horizontally scalable,\n* Provides best-effort consistency guarantees without being a complete CP (indeed PA/EC) solution,\n* Supports replication by default (with sync and async options),\n* Quorum-based voting for replica control (Read/Write quorums),\n* Supports atomic operations,\n* Provides an iterator on distributed maps,\n* Provides a plugin interface for service discovery daemons,\n* Provides a locking primitive which inspired by [SETNX of Redis](https://redis.io/commands/setnx#design-pattern-locking-with-codesetnxcode),\n\n## Possible Use Cases\n\nOlric is an eventually consistent, unordered key/value data store. It supports various eviction mechanisms for distributed caching implementations. Olric \nalso provides publish-subscribe messaging, data replication, failure detection and simple anti-entropy services. \n\nIt's good at distributed caching and publish/subscribe messaging.\n\n## Table of Contents\n\n* [Features](#features)\n* [Support](#support)\n* [Installing](#installing)\n  * [Docker](#docker)\n  * [Kubernetes](#kubernetes)\n  * [Working with Docker Compose](#working-with-docker-compose)\n* [Getting Started](#getting-started)\n  * [Operation Modes](#operation-modes)\n    * [Embedded Member](#embedded-member)\n    * [Client-Server](#client-server)\n* [Golang Client](#golang-client)\n* [Cluster Events](#cluster-events)\n* [Commands](#commands)\n  * [Distributed Map](#distributed-map)\n    * [DM.PUT](#dmput)\n    * [DM.GET](#dmget)\n    * [DM.DEL](#dmdel)\n    * [DM.EXPIRE](#dmexpire)\n    * [DM.PEXPIRE](#dmpexpire)\n    * [DM.DESTROY](#dmdestroy)\n    * [Atomic Operations](#atomic-operations)\n      * [DM.INCR](#dmincr)\n      * [DM.DECR](#dmdecr)\n      * [DM.GETPUT](#dmgetput)\n      * [DM.INCRBYFLOAT](#dmincrbyfloat)\n    * [Locking](#locking)\n      * [DM.LOCK](#dmlock)\n      * [DM.UNLOCK](#dmunlock)\n      * [DM.LOCKLEASE](#dmlocklease)\n      * [DM.PLOCKLEASE](#dmplocklease)\n    * [DM.SCAN](#dmscan)\n  * [Publish-Subscribe](#publish-subscribe)\n    * [SUBSCRIBE](#subscribe)\n    * [PSUBSCRIBE](#psubscribe)\n    * [UNSUBSCRIBE](#unsubscribe)\n    * [PUNSUBSCRIBE](#punsubscribe)\n    * [PUBSUB CHANNELS](#pubsub-channels)\n    * [PUBSUB NUMPAT](#pubsub-numpat)\n    * [PUBSUB NUMSUB](#pubsub-numsub)\n    * [QUIT](#quit)\n    * [PING](#ping)\n  * [Cluster](#cluster)\n    * [CLUSTER.ROUTINGTABLE](#clusterroutingtable)\n    * [CLUSTER.MEMBERS](#clustermembers)\n  * [Others](#others)\n    * [PING](#ping)\n    * [STATS](#stats)\n* [Configuration](#configuration)\n    * [Embedded Member Mode](#embedded-member-mode)\n      * [Manage the configuration in YAML format](#manage-the-configuration-in-yaml-format)\n    * [Client-Server Mode](#client-server-mode)\n    * [Network Configuration](#network-configuration)\n    * [Service discovery](#service-discovery)\n    * [Timeouts](#timeouts)\n* [Architecture](#architecture)\n  * [Overview](#overview)\n  * [Consistency and Replication Model](#consistency-and-replication-model)\n    * [Last-write-wins conflict resolution](#last-write-wins-conflict-resolution)\n    * [PACELC Theorem](#pacelc-theorem)\n    * [Read-Repair on DMaps](#read-repair-on-dmaps)\n    * [Quorum-based Replica Control](#quorum-based-replica-control)\n    * [Simple Split-Brain Protection](#simple-split-brain-protection)\n  * [Eviction](#eviction)\n    * [Expire with TTL](#expire-with-ttl)\n    * [Expire with MaxIdleDuration](#expire-with-maxidleduration)\n    * [Expire with LRU](#expire-with-lru)\n  * [Lock Implementation](#lock-implementation)\n  * [Storage Engine](#storage-engine)\n* [Samples](#samples)\n* [Contributions](#contributions)\n* [License](#license)\n* [About the name](#about-the-name)\n\n\n## Features\n\n* Designed to share some transient, approximate, fast-changing data between servers,\n* Accepts arbitrary types as value,\n* Only in-memory,\n* Uses Redis protocol,\n* Compatible with existing Redis clients,\n* Embeddable but can be used as a language-independent service with olricd,\n* GC-friendly storage engine,\n* O(1) running time for lookups,\n* Supports atomic operations,\n* Provides a lock implementation which can be used for non-critical purposes,\n* Different eviction policies: LRU, MaxIdleDuration and Time-To-Live (TTL),\n* Highly available,\n* Horizontally scalable,\n* Provides best-effort consistency guarantees without being a complete CP (indeed PA/EC) solution,\n* Distributes load fairly among cluster members with a [consistent hash function](https://github.com/buraksezer/consistent),\n* Supports replication by default (with sync and async options),\n* Quorum-based voting for replica control,\n* Thread-safe by default,\n* Provides an iterator on distributed maps,\n* Provides a plugin interface for service discovery daemons and cloud providers,\n* Provides a locking primitive which inspired by [SETNX of Redis](https://redis.io/commands/setnx#design-pattern-locking-with-codesetnxcode),\n* Provides a drop-in replacement of Redis' Publish-Subscribe messaging feature.\n\nSee [Architecture](#architecture) section to see details.\n\n## Support\n\nWe have a few communication channels: \n\n* [Issue Tracker](https://github.com/buraksezer/olric/issues)\n* [Discord server](https://discord.gg/ahK7Vjr8We)\n\nYou should know that the issue tracker is only intended for bug reports and feature requests.\n\nSoftware doesn't maintain itself. If you need support on complex topics or request new features, please consider [sponsoring Olric](https://github.com/sponsors/buraksezer).\n\n## Installing\n\nWith a correctly configured Golang environment:\n\n```\ngo install github.com/buraksezer/olric/cmd/olricd@v0.5.7\n```\n\nNow you can start using Olric:\n\n```\nolricd -c cmd/olricd/olricd-local.yaml\n```\n\nSee [Configuration](#configuration) section to create your cluster properly.\n\n### Docker\n\nYou can launch `olricd` Docker container by running the following command. \n\n```bash\ndocker run -p 3320:3320 olricio/olricd:v0.5.4\n``` \n\nThis command will pull olricd Docker image and run a new Olric Instance. You should know that the container exposes \n`3320` and `3322` ports. \n\nNow, you can access an Olric cluster using any Redis client including `redis-cli`:\n\n```bash\nredis-cli -p 3320\n127.0.0.1:3320> DM.PUT my-dmap my-key \"Olric Rocks!\"\nOK\n127.0.0.1:3320> DM.GET my-dmap my-key\n\"Olric Rocks!\"\n127.0.0.1:3320>\n```\n\n## Getting Started\n\nWith olricd, you can create an Olric cluster with a few commands. This is how to install olricd:\n\n```bash\ngo install github.com/buraksezer/olric/cmd/olricd@v0.5.7\n```\n\nLet's create a cluster with the following:\n\n```\nolricd -c <YOUR_CONFIG_FILE_PATH>\n```\n\nYou can find the sample configuration file under `cmd/olricd/olricd-local.yaml`. It can perfectly run with single node. \nolricd also supports `OLRICD_CONFIG` environment variable to set configuration. Just like that: \n\n```\nOLRICD_CONFIG=<YOUR_CONFIG_FILE_PATH> olricd\n```\n\nOlric uses [hashicorp/memberlist](https://github.com/hashicorp/memberlist) for failure detection and cluster membership. \nCurrently, there are different ways to discover peers in a cluster. You can use a static list of nodes in your configuration. \nIt's ideal for development and test environments. Olric also supports Consul, Kubernetes and all well-known cloud providers\nfor service discovery. Please take a look at [Service Discovery](#service-discovery) section for further information.\n\nSee [Client-Server](#client-server) section to get more information about this deployment scenario.\n\n#### Maintaining a list of peers manually\n\nBasically, there is a list of nodes under `memberlist` block in the configuration file. In order to create an Olric cluster, \nyou just need to add `Host:Port` pairs of the other nodes. Please note that the `Port` is the memberlist port of the peer.\nIt is `3322` by default. \n\n```yaml\nmemberlist:\n  peers:\n    - \"localhost:3322\"\n```\n\nThanks to [hashicorp/memberlist](https://github.com/hashicorp/memberlist), Olric nodes can share the full list of members \nwith each other. So an Olric node can discover the whole cluster by using a single member address.\n\n#### Embedding into your Go application.\n\nSee [Samples](#samples) section to learn how to embed Olric into your existing Golang application.\n\n### Operation Modes\n\nOlric has two different operation modes.\n\n#### Embedded Member\n\nIn Embedded Member Mode, members include both the application and Olric data and services. The advantage of the Embedded\nMember Mode is having a low-latency data access and locality.\n\n#### Client-Server\n\nIn Client-Server Mode, Olric data and services are centralized in one or more servers, and they are accessed by the \napplication through clients. You can have a cluster of servers that can be independently created and scaled. Your clients \ncommunicate with these members to reach to Olric data and services on them.\n\nClient-Server deployment has advantages including more predictable and reliable performance, easier identification\nof problem causes and, most importantly, better scalability. When you need to scale in this deployment type, just add more\nOlric server members. You can address client and server scalability concerns separately.\n\n## Golang Client\n\nThe official Golang client is defined by the `Client` interface. There are two different implementations of that interface in \nthis repository. `EmbeddedClient` provides a client implementation for [embedded-member](#embedded-member) scenario, \n`ClusterClient` provides an implementation of the same interface for [client-server](#client-server) deployment scenario. \nObviously, you can use `ClusterClient` for your embedded-member deployments. But it's good to use `EmbeddedClient` provides \na better performance due to localization of the queries.\n\nSee the client documentation on [pkg.go.dev](https://pkg.go.dev/github.com/buraksezer/olric@v0.5.7)\n\n## Cluster Events\n\nOlric can send push cluster events to `cluster.events` channel. Available cluster events:\n\n* node-join-event\n* node-left-event\n* fragment-migration-event\n* fragment-received-even\n\nIf you want to receive these events, set `true` to `EnableClusterEventsChannel` and subscribe to `cluster.events` channel. \nThe default is `false`.\n\nSee [events/cluster_events.go](events/cluster_events.go) file to get more information about events.\n\n## Commands\n\nOlric uses Redis protocol and supports Redis-style commands to query the database. You can use any Redis client, including\n`redis-cli`. The official Go client is a thin layer around [go-redis/redis](https://github.com/go-redis/redis) package. \nSee [Golang Client](#golang-client) section for the documentation.\n\n### Distributed Map\n\n#### DM.PUT \n\nDM.PUT sets the value for the given key. It overwrites any previous value for that key.\n\n```\nDM.PUT dmap key value [ EX seconds | PX milliseconds | EXAT unix-time-seconds | PXAT unix-time-milliseconds ] [ NX | XX]\n```\n\n**Example:**\n```\n127.0.0.1:3320> DM.PUT my-dmap my-key value\nOK\n```\n\n**Options:**\n\nThe DM.PUT command supports a set of options that modify its behavior:\n\n* **EX** *seconds* -- Set the specified expire time, in seconds.\n* **PX** *milliseconds* -- Set the specified expire time, in milliseconds.\n* **EXAT** *timestamp-seconds* -- Set the specified Unix time at which the key will expire, in seconds.\n* **PXAT** *timestamp-milliseconds* -- Set the specified Unix time at which the key will expire, in milliseconds.\n* **NX** -- Only set the key if it does not already exist.\n* **XX** -- Only set the key if it already exist.\n\n**Return:**\n\n* **Simple string reply:** OK if DM.PUT was executed correctly.\n* **KEYFOUND:** (error) if the DM.PUT operation was not performed because the user specified the NX option but the condition was not met.\n* **KEYNOTFOUND:** (error) if the DM.PUT operation was not performed because the user specified the XX option but the condition was not met.\n\n#### DM.GET\n\nDM.GET gets the value for the given key. It returns (error)`KEYNOTFOUND` if the key doesn't exist. \n\n```\nDM.GET dmap key\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.GET dmap key\n\"value\"\n```\n\n**Return:**\n\n**Bulk string reply**: the value of key, or (error)`KEYNOTFOUND` when key does not exist.\n\n#### DM.DEL\n\nDM.DEL deletes values for the given keys. It doesn't return any error if the key does not exist.\n\n```\nDM.DEL dmap key [key...]\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.DEL dmap key1 key2\n(integer) 2\n```\n\n**Return:**\n\n* **Integer reply**: The number of keys that were removed.\n\n#### DM.EXPIRE\n\nDM.EXPIRE updates or sets the timeout for the given key. It returns `KEYNOTFOUND` if the key doesn't exist. After the timeout has expired, \nthe key will automatically be deleted. \n\nThe timeout will only be cleared by commands that delete or overwrite the contents of the key, including DM.DEL, DM.PUT, DM.GETPUT.\n\n```\nDM.EXPIRE dmap key seconds\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.EXPIRE dmap key 1\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK if DM.EXPIRE was executed correctly.\n* **KEYNOTFOUND:** (error) when key does not exist.\n\n#### DM.PEXPIRE\n\nDM.PEXPIRE updates or sets the timeout for the given key. It returns `KEYNOTFOUND` if the key doesn't exist. After the timeout has expired,\nthe key will automatically be deleted.\n\nThe timeout will only be cleared by commands that delete or overwrite the contents of the key, including DM.DEL, DM.PUT, DM.GETPUT.\n\n```\nDM.PEXPIRE dmap key milliseconds\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.PEXPIRE dmap key 1000\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK if DM.EXPIRE was executed correctly.\n* **KEYNOTFOUND:** (error) when key does not exist.\n\n#### DM.DESTROY\n\nDM.DESTROY flushes the given DMap on the cluster. You should know that there is no global lock on DMaps. DM.PUT and DM.DESTROY commands\nmay run concurrently on the same DMap. \n\n```\nDM.DESTROY dmap\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.DESTROY dmap\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK, if DM.DESTROY was executed correctly.\n\n### Atomic Operations\n\nOperations on key/value pairs are performed by the partition owner. In addition, atomic operations are guarded by a lock implementation which can be found under `internal/locker`. It means that\nOlric guaranties consistency of atomic operations, if there is no network partition. Basic flow for `DM.INCR`:\n\n* Acquire the lock for the given key,\n* Call `DM.GET` to retrieve the current value,\n* Calculate the new value,\n* Call `DM.PUT` to set the new value,\n* Release the lock.\n\nIt's important to know that if you call `DM.PUT` and `DM.GETPUT` concurrently on the same key, this will break the atomicity.\n\n`internal/locker` package is provided by [Docker](https://github.com/moby/moby).\n\n**Important note about consistency:**\n\nYou should know that Olric is a PA/EC (see [Consistency and Replication Model](#consistency-and-replication-model)) product. So if your network is stable, all the operations on key/value\npairs are performed by a single cluster member. It means that you can be sure about the consistency when the cluster is stable. It's important to know that computer networks fail\noccasionally, processes crash and random GC pauses may happen. Many factors can lead a network partitioning. If you cannot tolerate losing strong consistency under network partitioning,\nyou need to use a different tool for atomic operations.\n\nSee [Hazelcast and the Mythical PA/EC System](https://dbmsmusings.blogspot.com/2017/10/hazelcast-and-mythical-paec-system.html) and [Jepsen Analysis on Hazelcast 3.8.3](https://hazelcast.com/blog/jepsen-analysis-hazelcast-3-8-3/) for more insight on this topic.\n\n\n#### DM.INCR\n\nDM.INCR atomically increments the number stored at key by delta. The return value is the new value after being incremented or an error.\n\n```\nDM.INCR dmap key delta\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.INCR dmap key 10\n(integer) 10\n```\n\n**Return:**\n\n* **Integer reply:** the value of key after the increment.\n\n#### DM.DECR\n\nDM.DECR atomically decrements the number stored at key by delta. The return value is the new value after being incremented or an error.\n\n```\nDM.DECR dmap key delta\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.DECR dmap key 10\n(integer) 0\n```\n\n**Return:**\n\n* **Integer reply:** the value of key after the increment.\n\n#### DM.GETPUT\n\nDM.GETPUT atomically sets key to value and returns the old value stored at the key.\n\n```\nDM.GETPUT dmap key value\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.GETPUT dmap key value-1\n(nil)\n127.0.0.1:3320> DM.GETPUT dmap key value-2\n\"value-1\"\n```\n\n**Return:**\n\n* **Bulk string reply**: the old value stored at the key.\n\n#### DM.INCRBYFLOAT\n\nDM.INCRBYFLOAT atomically increments the number stored at key by delta. The return value is the new value after being incremented or an error.\n\n```\nDM.INCRBYFLOAT dmap key delta\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.PUT dmap key 10.50\nOK\n127.0.0.1:3320> DM.INCRBYFLOAT dmap key 0.1\n\"10.6\"\n127.0.0.1:3320> DM.PUT dmap key 5.0e3\nOK\n127.0.0.1:3320> DM.INCRBYFLOAT dmap key 2.0e2\n\"5200\"\n```\n\n**Return:**\n\n* **Bulk string reply**: the value of key after the increment.\n\n\n### Locking\n\n**Important:** The lock provided by DMap implementation is approximate and only to be used for non-critical purposes.\n\nThe DMap implementation is already thread-safe to meet your thread safety requirements. When you want to have more control on the\nconcurrency, you can use **DM.LOCK** command. Olric borrows the locking algorithm from Redis. Redis authors propose\nthe following algorithm:\n\n> The command <SET resource-name anystring NX EX max-lock-time> is a simple way to implement a locking system with Redis.\n>\n> A client can acquire the lock if the above command returns OK (or retry after some time if the command returns Nil), and remove the lock just using DEL.\n>\n> The lock will be auto-released after the expire time is reached.\n>\n> It is possible to make this system more robust modifying the unlock schema as follows:\n>\n> Instead of setting a fixed string, set a non-guessable large random string, called token.\n> Instead of releasing the lock with DEL, send a script that only removes the key if the value matches.\n> This avoids that a client will try to release the lock after the expire time deleting the key created by another client that acquired the lock later.\n\nEquivalent of `SETNX` command in Olric is `DM.PUT dmap key value NX`. DM.LOCK command are properly implements\nthe algorithm which is proposed above.\n\nYou should know that this implementation is subject to the clustering algorithm. So there is no guarantee about reliability in the case of network partitioning. I recommend the lock implementation to be used for\nefficiency purposes in general, instead of correctness.\n\n**Important note about consistency:**\n\nYou should know that Olric is a PA/EC (see [Consistency and Replication Model](#consistency-and-replication-model)) product. So if your network is stable, all the operations on key/value\npairs are performed by a single cluster member. It means that you can be sure about the consistency when the cluster is stable. It's important to know that computer networks fail\noccasionally, processes crash and random GC pauses may happen. Many factors can lead a network partitioning. If you cannot tolerate losing strong consistency under network partitioning,\nyou need to use a different tool for locking.\n\nSee [Hazelcast and the Mythical PA/EC System](https://dbmsmusings.blogspot.com/2017/10/hazelcast-and-mythical-paec-system.html) and [Jepsen Analysis on Hazelcast 3.8.3](https://hazelcast.com/blog/jepsen-analysis-hazelcast-3-8-3/) for more insight on this topic.\n\n#### DM.LOCK\n\nDM.LOCK sets a lock for the given key. The acquired lock is only valid for the key in this DMap.\nIt returns immediately if it acquires the lock for the given key. Otherwise, it waits until deadline.\n\nDM.LOCK returns a token. You must keep that token to unlock the key. Using prefixed keys is highly recommended.\nIf the key does already exist in the DMap, DM.LOCK will wait until the deadline is exceeded.\n\n```\nDM.LOCK dmap key seconds [ EX seconds | PX milliseconds ]\n```\n\n**Options:**\n\n* **EX** *seconds* -- Set the specified expire time, in seconds.\n* **PX** *milliseconds* -- Set the specified expire time, in milliseconds.\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.LOCK dmap lock.key 10\n2363ec600be286cb10fbb35181efb029\n```\n\n**Return:**\n\n* **Simple string reply:** a token to unlock or lease the lock.\n* **NOSUCHLOCK**: (error) returned when the requested lock does not exist.\n* **LOCKNOTACQUIRED**: (error) returned when the requested lock could not be acquired.\n\n#### DM.UNLOCK\n\nDM.UNLOCK releases an acquired lock for the given key. It returns `NOSUCHLOCK` if there is no lock for the given key.\n\n```\nDM.UNLOCK dmap key token\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.UNLOCK dmap key 2363ec600be286cb10fbb35181efb029\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK if DM.UNLOCK was executed correctly.\n* **NOSUCHLOCK**: (error) returned when the lock does not exist.\n\n#### DM.LOCKLEASE\n\nDM.LOCKLEASE sets or updates the timeout of the acquired lock for the given key. It returns `NOSUCHLOCK` if there is no lock for the given key.\n\nDM.LOCKLEASE accepts seconds as timeout.\n\n```\nDM.LOCKLEASE dmap key token seconds\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.LOCKLEASE dmap key 2363ec600be286cb10fbb35181efb029 100\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK if DM.UNLOCK was executed correctly.\n* **NOSUCHLOCK**: (error) returned when the lock does not exist.\n\n#### DM.PLOCKLEASE\n\nDM.PLOCKLEASE sets or updates the timeout of the acquired lock for the given key. It returns `NOSUCHLOCK` if there is no lock for the given key.\n\nDM.PLOCKLEASE accepts milliseconds as timeout.\n\n```\nDM.LOCKLEASE dmap key token milliseconds\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.PLOCKLEASE dmap key 2363ec600be286cb10fbb35181efb029 1000\nOK\n```\n\n**Return:**\n\n* **Simple string reply:** OK if DM.PLOCKLEASE was executed correctly.\n* **NOSUCHLOCK**: (error) returned when the lock does not exist.\n\n#### DM.SCAN\n\nDM.SCAN is a cursor based iterator. This means that at every call of the command, the server returns an updated cursor \nthat the user needs to use as the cursor argument in the next call.\n\nAn iteration starts when the cursor is set to 0, and terminates when the cursor returned by the server is 0. The iterator runs\nlocally on every partition. So you need to know the partition count. If the returned cursor is 0 for a particular partition,\nyou have to start scanning the next partition. \n\n```\nDM.SCAN partID dmap cursor [ MATCH pattern | COUNT count ]\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> DM.SCAN 3 bench 0\n1) \"96990\"\n2)  1) \"memtier-2794837\"\n    2) \"memtier-8630933\"\n    3) \"memtier-6415429\"\n    4) \"memtier-7808686\"\n    5) \"memtier-3347072\"\n    6) \"memtier-4247791\"\n    7) \"memtier-3931982\"\n    8) \"memtier-7164719\"\n    9) \"memtier-4710441\"\n   10) \"memtier-8892916\"\n127.0.0.1:3320> DM.SCAN 3 bench 96990\n1) \"193499\"\n2)  1) \"memtier-429905\"\n    2) \"memtier-1271812\"\n    3) \"memtier-7835776\"\n    4) \"memtier-2717575\"\n    5) \"memtier-95312\"\n    6) \"memtier-2155214\"\n    7) \"memtier-123931\"\n    8) \"memtier-2902510\"\n    9) \"memtier-2632291\"\n   10) \"memtier-1938450\"\n```\n### Publish-Subscribe\n\n**SUBSCRIBE**, **UNSUBSCRIBE** and **PUBLISH** implement the Publish/Subscribe messaging paradigm where \nsenders are not programmed to send their messages to specific receivers. Rather, published messages are characterized \ninto channels, without knowledge of what (if any) subscribers there may be. Subscribers express interest in one or more \nchannels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are. \nThis decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.\n\n**Important note:** In an Olric cluster, clients can subscribe to every node, and can also publish to every other node. The cluster\nwill make sure that published messages are forwarded as needed.\n\n*Source of this section: [https://redis.io/commands/?group=pubsub](https://redis.io/commands/?group=pubsub)*\n\n#### SUBSCRIBE\n\nSubscribes the client to the specified channels.\n\n```\nSUBSCRIBE channel [channel...]\n```\n\nOnce the client enters the subscribed state it is not supposed to issue any other commands, except for additional **SUBSCRIBE**, \n**PSUBSCRIBE**, **UNSUBSCRIBE**, **PUNSUBSCRIBE**, **PING**, and **QUIT** commands.\n\n#### PSUBSCRIBE\n\nSubscribes the client to the given patterns.\n\n```\nPSUBSCRIBE pattern [ pattern ...]\n```\n\nSupported glob-style patterns:\n\n* `h?llo` subscribes to hello, hallo and hxllo\n* `h*llo` subscribes to hllo and heeeello\n* `h[ae]llo` subscribes to hello and hallo, but not hillo\n* Use **\\\\** to escape special characters if you want to match them verbatim.\n\n#### UNSUBSCRIBE\n\nUnsubscribes the client from the given channels, or from all of them if none is given.\n\n```\nUNSUBSCRIBE [channel [channel ...]]\n```\n\nWhen no channels are specified, the client is unsubscribed from all the previously subscribed channels. In this case, \na message for every unsubscribed channel will be sent to the client.\n\n#### PUNSUBSCRIBE\n\nUnsubscribes the client from the given patterns, or from all of them if none is given.\n\n```\nPUNSUBSCRIBE [pattern [pattern ...]]\n```\n\nWhen no patterns are specified, the client is unsubscribed from all the previously subscribed patterns. In this case, \na message for every unsubscribed pattern will be sent to the client.\n\n#### PUBSUB CHANNELS\n\nLists the currently active channels.\n\n```\nPUBSUB CHANNELS [pattern]\n```\n\nAn active channel is a Pub/Sub channel with one or more subscribers (excluding clients subscribed to patterns).\n\nIf no pattern is specified, all the channels are listed, otherwise if pattern is specified only channels matching the \nspecified glob-style pattern are listed.\n\n#### PUBSUB NUMPAT\n\nReturns the number of unique patterns that are subscribed to by clients (that are performed using the PSUBSCRIBE command).\n\n```\nPUBSUB NUMPAT\n```\n\nNote that this isn't the count of clients subscribed to patterns, but the total number of unique patterns all the clients are subscribed to.\n\n**Important note**: In an Olric cluster, clients can subscribe to every node, and can also publish to every other node. The cluster \nwill make sure that published messages are forwarded as needed. That said, PUBSUB's replies in a cluster only report information \nfrom the node's Pub/Sub context, rather than the entire cluster.\n\n#### PUBSUB NUMSUB\n\nReturns the number of subscribers (exclusive of clients subscribed to patterns) for the specified channels.\n\n```\nPUBSUB NUMSUB [channel [channel ...]]\n```\nNote that it is valid to call this command without channels. In this case it will just return an empty list.\n\n**Important note**: In an Olric cluster, clients can subscribe to every node, and can also publish to every other node. The cluster \nwill make sure that published messages are forwarded as needed. That said, PUBSUB's replies in a cluster only report information \nfrom the node's Pub/Sub context, rather than the entire cluster.\n\n#### QUIT\n\nAsk the server to close the connection. The connection is closed as soon as all pending replies have been written to the client.\n\n```\nQUIT\n```\n### Cluster\n\n#### CLUSTER.ROUTINGTABLE\n\nCLUSTER.ROUTINGTABLE returns the latest view of the routing table. Simply, it's a data structure that maps\npartitions to members.\n\n```\nCLUSTER.ROUTINGTABLE\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> CLUSTER.ROUTINGTABLE\n 1) 1) (integer) 0\n     2) 1) \"127.0.0.1:3320\"\n     3) (empty array)\n  2) 1) (integer) 1\n     2) 1) \"127.0.0.1:3320\"\n     3) (empty array)\n  3) 1) (integer) 2\n     2) 1) \"127.0.0.1:3320\"\n     3) (empty array)\n```\n\nIt returns an array of arrays. \n\n**Fields:**\n\n```\n1) (integer) 0 <- Partition ID\n  2) 1) \"127.0.0.1:3320\" <- Array of the current and previous primary owners\n  3) (empty array) <- Array of backup owners. \n```\n\n#### CLUSTER.MEMBERS\n\nCLUSTER.MEMBERS returns an array of known members by the server.\n\n```\nCLUSTER.MEMBERS\n```\n\n**Example:**\n\n```\n127.0.0.1:3320> CLUSTER.MEMBERS\n1) 1) \"127.0.0.1:3320\"\n   2) (integer) 1652619388427137000\n   3) \"true\"\n```\n\n**Fields:**\n\n```\n1) 1) \"127.0.0.1:3320\" <- Member's name in the cluster\n   2) (integer) 1652619388427137000 <-Member's birthedate\n   3) \"true\" <- Is cluster coordinator (the oldest node)\n```\n\n### Others\n\n#### PING\n\nReturns PONG if no argument is provided, otherwise return a copy of the argument as a bulk. This command is often used to\ntest if a connection is still alive, or to measure latency.\n\n```\nPING\n```\n\n#### STATS\n\nThe STATS command returns information and statistics about the server in JSON format. See `stats/stats.go` file.\n\n## Configuration\n\nOlric supports both declarative and programmatic configurations. You can choose one of them depending on your needs.\nYou should feel free to ask any questions about configuration and integration. Please see [Support](#support) section.\n\n### Embedded-Member Mode\n\n#### Programmatic Configuration\nOlric provides a function to generate default configuration to use in embedded-member mode:\n\n```go\nimport \"github.com/buraksezer/olric/config\"\n...\nc := config.New(\"local\")\n```\n\nThe `New` function takes a parameter called `env`. It denotes the network environment and consumed by [hashicorp/memberlist](https://github.com/hashicorp/memberlist). \nDefault configuration is good enough for distributed caching scenario. In order to see all configuration parameters, please take a look at [this](https://godoc.org/github.com/buraksezer/olric/config).\n\nSee [Sample Code](#sample-code) section for an introduction.\n\n#### Declarative configuration with YAML format\n\nYou can also import configuration from a YAML file by using the `Load` function:\n\n```go\nc, err := config.Load(path/to/olric.yaml)\n```\n\nA sample configuration file in YAML format can be found [here](https://github.com/buraksezer/olric/blob/master/cmd/olricd/olricd.yaml). This may be the most appropriate way to manage the Olric configuration.\n\n\n### Client-Server Mode\n\nOlric provides **olricd** to implement client-server mode. olricd gets a YAML file for the configuration. The most basic  functionality of olricd is that \ntranslating YAML configuration into Olric's configuration struct. A sample `olricd.yaml` file  is being provided [here](https://github.com/buraksezer/olric/blob/master/cmd/olricd/olricd.yaml).\n\n### Network Configuration\n\nIn an Olric instance, there are two different TCP servers. One for Olric, and the other one is for memberlist. `BindAddr` is very\ncritical to deploy a healthy Olric node. There are different scenarios:\n\n* You can freely set a domain name or IP address as `BindAddr` for both Olric and memberlist. Olric will resolve and use it to bind.\n* You can freely set `localhost`, `127.0.0.1` or `::1` as `BindAddr` in development environment for both Olric and memberlist.\n* You can freely set `0.0.0.0` as `BindAddr` for both Olric and memberlist. Olric will pick an IP address, if there is any.\n* If you don't set `BindAddr`, hostname will be used, and it will be resolved to get a valid IP address.\n* You can set a network interface by using `Config.Interface` and `Config.MemberlistInterface` fields. Olric will find an appropriate IP address for the given interfaces, if there is any.\n* You can set both `BindAddr` and interface parameters. In this case Olric will ensure that `BindAddr` is available on the given interface.\n\nYou should know that Olric needs a single and stable IP address to function properly. If you don't know the IP address of the host at the deployment time, \nyou can set `BindAddr` as `0.0.0.0`. Olric will very likely to find an IP address for you.\n\n### Service Discovery\n\nOlric provides a service discovery interface which can be used to implement plugins. \n\nWe currently have a bunch of service discovery plugins for automatic peer discovery on cloud environments:\n\n* [buraksezer/olric-consul-plugin](https://github.com/buraksezer/olric-consul-plugin) provides a plugin using Consul.\n* [buraksezer/olric-cloud-plugin](https://github.com/buraksezer/olric-cloud-plugin) provides a plugin for well-known cloud providers. Including Kubernetes.\n* [justinfx/olric-nats-plugin](https://github.com/justinfx/olric-nats-plugin) provides a plugin using nats.io\n\nIn order to get more info about installation and configuration of the plugins, see their GitHub page. \n\n### Timeouts\n\nOlric nodes supports setting `KeepAlivePeriod` on TCP sockets. \n\n**Server-side:**\n\n##### config.KeepAlivePeriod \n\nKeepAlivePeriod denotes whether the operating system should send keep-alive messages on the connection.\n\n**Client-side:**\n \n##### config.DialTimeout\n\nTimeout for TCP dial. The timeout includes name resolution, if required. When using TCP, and the host in the address \nparameter resolves to multiple IP addresses, the timeout is spread over each consecutive dial, such that each is\ngiven an appropriate fraction of the time to connect.\n\n##### config.ReadTimeout\n\nTimeout for socket reads. If reached, commands will fail with a timeout instead of blocking. Use value -1 for no \ntimeout and 0 for default. The default is config.DefaultReadTimeout\n\n##### config.WriteTimeout\n\nTimeout for socket writes. If reached, commands will fail with a timeout instead of blocking. The default is config.DefaultWriteTimeout\n\n## Architecture\n\n### Overview\n\nOlric uses:\n* [hashicorp/memberlist](https://github.com/hashicorp/memberlist) for cluster membership and failure detection,\n* [buraksezer/consistent](https://github.com/buraksezer/consistent) for consistent hashing and load balancing,\n* [Redis Serialization Protocol](https://github.com/tidwall/redcon) for communication.\n\nOlric distributes data among partitions. Every partition is being owned by a cluster member and may have one or more backups for redundancy. \nWhen you read or write a DMap entry, you transparently talk to the partition owner. Each request hits the most up-to-date version of a\nparticular data entry in a stable cluster.\n\nIn order to find the partition which the key belongs to, Olric hashes the key and mod it with the number of partitions:\n\n```\npartID = MOD(hash result, partition count)\n```\n\nThe partitions are being distributed among cluster members by using a consistent hashing algorithm. In order to get details, please see [buraksezer/consistent](https://github.com/buraksezer/consistent). \n\nWhen a new cluster is created, one of the instances is elected as the **cluster coordinator**. It manages the partition table: \n\n* When a node joins or leaves, it distributes the partitions and their backups among the members again,\n* Removes empty previous owners from the partition owners list,\n* Pushes the new partition table to all the members,\n* Pushes the partition table to the cluster periodically.\n\nMembers propagate their birthdate(POSIX time in nanoseconds) to the cluster. The coordinator is the oldest member in the cluster.\nIf the coordinator leaves the cluster, the second oldest member gets elected as the coordinator.\n\nOlric has a component called **rebalancer** which is responsible for keeping underlying data structures consistent:\n\n* Works on every node,\n* When a node joins or leaves, the cluster coordinator pushes the new partition table. Then, the **rebalancer** runs immediately and moves the partitions and backups to their new hosts,\n* Merges fragmented partitions.\n\nPartitions have a concept called **owners list**. When a node joins or leaves the cluster, a new primary owner may be assigned by the \ncoordinator. At any time, a partition may have one or more partition owners. If a partition has two or more owners, this is called **fragmented partition**. \nThe last added owner is called **primary owner**. Write operation is only done by the primary owner. The previous owners are only used for read and delete.\n\nWhen you read a key, the primary owner tries to find the key on itself, first. Then, queries the previous owners and backups, respectively.\nThe delete operation works the same way.\n\nThe data(distributed map objects) in the fragmented partition is moved slowly to the primary owner by the **rebalancer**. Until the move is done,\nthe data remains available on the previous owners. The DMap methods use this list to query data on the cluster.\n\n*Please note that, 'multiple partition owners' is an undesirable situation and the **rebalancer** component is designed to fix that in a short time.*\n\n### Consistency and Replication Model\n\n**Olric is an AP product** in the context of [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), which employs the combination of primary-copy \nand [optimistic replication](https://en.wikipedia.org/wiki/Optimistic_replication) techniques. With optimistic replication, when the partition owner \nreceives a write or delete operation for a key, applies it locally, and propagates it to the backup owners.\n\nThis technique enables Olric clusters to offer high throughput. However, due to temporary situations in the system, such as network\nfailure, backup owners can miss some updates and diverge from the primary owner. If a partition owner crashes while there is an\ninconsistency between itself and the backups, strong consistency of the data can be lost.\n\nTwo types of backup replication are available: **sync** and **async**. Both types are still implementations of the optimistic replication\nmodel.\n\n* **sync**: Blocks until write/delete operation is applied by backup owners.\n* **async**: Just fire & forget.\n\n#### Last-write-wins conflict resolution\n\nEvery time a piece of data is written to Olric, a timestamp is attached by the client. Then, when Olric has to deal with conflict data in the case \nof network partitioning, it simply chooses the data with the most recent timestamp. This called LWW conflict resolution policy.\n\n#### PACELC Theorem\n\nFrom Wikipedia:\n\n> In theoretical computer science, the [PACELC theorem](https://en.wikipedia.org/wiki/PACELC_theorem) is an extension to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem). It states that in case of network partitioning (P) in a \n> distributed computer system, one has to choose between availability (A) and consistency (C) (as per the CAP theorem), but else (E), even when the system is \n> running normally in the absence of partitions, one has to choose between latency (L) and consistency (C).\n\nIn the context of PACELC theorem, Olric is a **PA/EC** product. It means that Olric is considered to be **consistent** data store if the network is stable. \nBecause the key space is divided between partitions and every partition is controlled by its primary owner. All operations on DMaps are redirected to the \npartition owner. \n\nIn the case of network partitioning, Olric chooses **availability** over consistency. So that you can still access some parts of the cluster when the network is unreliable, \nbut the cluster may return inconsistent results.  \n\nOlric implements read-repair and quorum based voting system to deal with inconsistencies in the DMaps. \n\nReadings on PACELC theorem:\n* [Please stop calling databases CP or AP](https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html)\n* [Problems with CAP, and Yahoo’s little known NoSQL system](https://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html)\n* [A Critique of the CAP Theorem](https://arxiv.org/abs/1509.05393)\n* [Hazelcast and the Mythical PA/EC System](https://dbmsmusings.blogspot.com/2017/10/hazelcast-and-mythical-paec-system.html)\n\n#### Read-Repair on DMaps\n\nRead repair is a feature that allows for inconsistent data to be fixed at query time. Olric tracks every write operation with a timestamp value and assumes \nthat the latest write operation is the valid one. When you want to access a key/value pair, the partition owner retrieves all available copies for that pair\nand compares the timestamp values. The latest one is the winner. If there is some outdated version of the requested pair, the primary owner propagates the latest\nversion of the pair. \n\nRead-repair is disabled by default for the sake of performance. If you have a use case that requires a more strict consistency control than a distributed caching \nscenario, you can enable read-repair via the configuration. \n\n#### Quorum-based replica control\n\nOlric implements Read/Write quorum to keep the data in a consistent state. When you start a write operation on the cluster and write quorum (W) is 2, \nthe partition owner tries to write the given key/value pair on its own data storage and on the replica nodes. If the number of successful write operations \nis below W, the primary owner returns `ErrWriteQuorum`. The read flow is the same: if you have R=2 and the owner only access one of the replicas, \nit returns `ErrReadQuorum`.\n\n#### Simple Split-Brain Protection\n\nOlric implements a technique called *majority quorum* to manage split-brain conditions. If a network partitioning occurs, and some members\nlost the connection to rest of the cluster, they immediately stops functioning and return an error to incoming requests. This behaviour is controlled by\n`MemberCountQuorum` parameter. It's default `1`. \n\nWhen the network healed, the stopped nodes joins again the cluster and fragmented partitions is merged by their primary owners in accordance with \n*LWW policy*. Olric also implements an *ownership report* mechanism to fix inconsistencies in partition distribution after a partitioning event. \n\n### Eviction\nOlric supports different policies to evict keys from distributed maps. \n\n#### Expire with TTL\nOlric implements TTL eviction policy. It shares the same algorithm with [Redis](https://redis.io/commands/expire#appendix-redis-expires):\n\n> Periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.\n>\n> Specifically this is what Redis does 10 times per second:\n>\n> * Test 20 random keys from the set of keys with an associated expire.\n> * Delete all the keys found expired.\n> * If more than 25% of keys were expired, start again from step 1.\n>\n> This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%\n\nWhen a client tries to access a key, Olric returns `ErrKeyNotFound` if the key is found to be timed out. A background task evicts keys with the algorithm described above.\n\n#### Expire with MaxIdleDuration\n\nMaximum time for each entry to stay idle in the DMap. It limits the lifetime of the entries relative to the time of the last read \nor write access performed on them. The entries whose idle period exceeds this limit are expired and evicted automatically. \nAn entry is idle if no Get, Put, PutEx, Expire, PutIf, PutIfEx on it. Configuration of MaxIdleDuration feature varies by \npreferred deployment method. \n\n#### Expire with LRU\n\nOlric implements LRU eviction method on DMaps. Approximated LRU algorithm is borrowed from Redis. The Redis authors proposes the following algorithm:\n\n> It is important to understand that the eviction process works like this:\n> \n> * A client runs a new command, resulting in more data added.\n> * Redis checks the memory usage, and if it is greater than the maxmemory limit , it evicts keys according to the policy.\n> * A new command is executed, and so forth.\n>\n> So we continuously cross the boundaries of the memory limit, by going over it, and then by evicting keys to return back under the limits.\n>\n> If a command results in a lot of memory being used (like a big set intersection stored into a new key) for some time the memory \n> limit can be surpassed by a noticeable amount. \n>\n> **Approximated LRU algorithm**\n>\n> Redis LRU algorithm is not an exact implementation. This means that Redis is not able to pick the best candidate for eviction, \n> that is, the access that was accessed the most in the past. Instead it will try to run an approximation of the LRU algorithm, \n> by sampling a small number of keys, and evicting the one that is the best (with the oldest access time) among the sampled keys.\n\nOlric tracks access time for every DMap instance. Then it picks and sorts some configurable amount of keys to select keys for eviction.\nEvery node runs this algorithm independently. The access log is moved along with the partition when a network partition is occured.\n\n#### Configuration of eviction mechanisms\n\nHere is a simple configuration block for `olricd.yaml`: \n\n```\ncache:\n  numEvictionWorkers: 1\n  maxIdleDuration: \"\"\n  ttlDuration: \"100s\"\n  maxKeys: 100000\n  maxInuse: 1000000 # in bytes\n  lRUSamples: 10\n  evictionPolicy: \"LRU\" # NONE/LRU\n```\n\nYou can also set cache configuration per DMap. Here is a simple configuration for a DMap named `foobar`:\n\n```\ndmaps:\n  foobar:\n    maxIdleDuration: \"60s\"\n    ttlDuration: \"300s\"\n    maxKeys: 500000 # in-bytes\n    lRUSamples: 20\n    evictionPolicy: \"NONE\" # NONE/LRU\n```\n\nIf you prefer embedded-member deployment scenario, please take a look at [config#CacheConfig](https://godoc.org/github.com/buraksezer/olric/config#CacheConfig) and [config#DMapCacheConfig](https://godoc.org/github.com/buraksezer/olric/config#DMapCacheConfig) for the configuration.\n\n\n### Lock Implementation\n\nThe DMap implementation is already thread-safe to meet your thread safety requirements. When you want to have more control on the\nconcurrency, you can use **LockWithTimeout** and **Lock** methods. Olric borrows the locking algorithm from Redis. Redis authors propose\nthe following algorithm:\n\n> The command <SET resource-name anystring NX EX max-lock-time> is a simple way to implement a locking system with Redis.\n>\n> A client can acquire the lock if the above command returns OK (or retry after some time if the command returns Nil), and remove the lock just using DEL.\n>\n> The lock will be auto-released after the expire time is reached.\n>\n> It is possible to make this system more robust modifying the unlock schema as follows:\n>\n> Instead of setting a fixed string, set a non-guessable large random string, called token.\n> Instead of releasing the lock with DEL, send a script that only removes the key if the value matches.\n> This avoids that a client will try to release the lock after the expire time deleting the key created by another client that acquired the lock later.\n\nEquivalent of`SETNX` command in Olric is `PutIf(key, value, IfNotFound)`. Lock and LockWithTimeout commands are properly implements\nthe algorithm which is proposed above. \n\nYou should know that this implementation is subject to the clustering algorithm. So there is no guarantee about reliability in the case of network partitioning. I recommend the lock implementation to be used for \nefficiency purposes in general, instead of correctness.\n\n**Important note about consistency:**\n\nYou should know that Olric is a PA/EC (see [Consistency and Replication Model](#consistency-and-replication-model)) product. So if your network is stable, all the operations on key/value \npairs are performed by a single cluster member. It means that you can be sure about the consistency when the cluster is stable. It's important to know that computer networks fail \noccasionally, processes crash and random GC pauses may happen. Many factors can lead a network partitioning. If you cannot tolerate losing strong consistency under network partitioning, \nyou need to use a different tool for locking.\n\nSee [Hazelcast and the Mythical PA/EC System](https://dbmsmusings.blogspot.com/2017/10/hazelcast-and-mythical-paec-system.html) and [Jepsen Analysis on Hazelcast 3.8.3](https://hazelcast.com/blog/jepsen-analysis-hazelcast-3-8-3/) for more insight on this topic.\n             \n### Storage Engine\n\nOlric implements a GC-friendly storage engine to store large amounts of data on RAM. Basically, it applies an append-only log file approach with indexes. \nOlric inserts key/value pairs into pre-allocated byte slices (table in Olric terminology) and indexes that memory region by using Golang's built-in map. \nThe data type of this map is `map[uint64]uint64`. When a pre-allocated byte slice is full Olric allocates a new one and continues inserting the new data into it. \nThis design greatly reduces the write latency.\n\nWhen you want to read a key/value pair from the Olric cluster, it scans the related DMap fragment by iterating over the indexes(implemented by the built-in map). \nThe number of allocated byte slices should be small. So Olric would find the key immediately but technically, the read performance depends on the number of keys in the fragment. \nThe effect of this design on the read performance is negligible.\n\nThe size of the pre-allocated byte slices is configurable.\n\n## Samples\n\nIn this section, you can find code snippets for various scenarios.\n\n### Embedded-member scenario\n#### Distributed map\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n\n  \"github.com/buraksezer/olric\"\n  \"github.com/buraksezer/olric/config\"\n)\n\nfunc main() {\n  // Sample for Olric v0.5.x\n\n  // Deployment scenario: embedded-member\n  // This creates a single-node Olric cluster. It's good enough for experimenting.\n\n  // config.New returns a new config.Config with sane defaults. Available values for env:\n  // local, lan, wan\n  c := config.New(\"local\")\n\n  // Callback function. It's called when this node is ready to accept connections.\n  ctx, cancel := context.WithCancel(context.Background())\n  c.Started = func() {\n    defer cancel()\n    log.Println(\"[INFO] Olric is ready to accept connections\")\n  }\n\n  // Create a new Olric instance.\n  db, err := olric.New(c)\n  if err != nil {\n    log.Fatalf(\"Failed to create Olric instance: %v\", err)\n  }\n\n  // Start the instance. It will form a single-node cluster.\n  go func() {\n    // Call Start at background. It's a blocker call.\n    err = db.Start()\n    if err != nil {\n      log.Fatalf(\"olric.Start returned an error: %v\", err)\n    }\n  }()\n\n  <-ctx.Done()\n\n  // In embedded-member scenario, you can use the EmbeddedClient. It implements\n  // the Client interface.\n  e := db.NewEmbeddedClient()\n\n  dm, err := e.NewDMap(\"bucket-of-arbitrary-items\")\n  if err != nil {\n    log.Fatalf(\"olric.NewDMap returned an error: %v\", err)\n  }\n\n  ctx, cancel = context.WithCancel(context.Background())\n\n  // Magic starts here!\n  fmt.Println(\"##\")\n  fmt.Println(\"Simple Put/Get on a DMap instance:\")\n  err = dm.Put(ctx, \"my-key\", \"Olric Rocks!\")\n  if err != nil {\n    log.Fatalf(\"Failed to call Put: %v\", err)\n  }\n\n  gr, err := dm.Get(ctx, \"my-key\")\n  if err != nil {\n    log.Fatalf(\"Failed to call Get: %v\", err)\n  }\n\n  // Olric uses the Redis serialization format.\n  value, err := gr.String()\n  if err != nil {\n    log.Fatalf(\"Failed to read Get response: %v\", err)\n  }\n\n  fmt.Println(\"Response for my-key:\", value)\n  fmt.Println(\"##\")\n\n  // Don't forget the call Shutdown when you want to leave the cluster.\n  ctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n  defer cancel()\n\n  err = db.Shutdown(ctx)\n  if err != nil {\n    log.Printf(\"Failed to shutdown Olric: %v\", err)\n  }\n}\n```\n\n#### Publish-Subscribe\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n\n  \"github.com/buraksezer/olric\"\n  \"github.com/buraksezer/olric/config\"\n)\n\nfunc main() {\n  // Sample for Olric v0.5.x\n\n  // Deployment scenario: embedded-member\n  // This creates a single-node Olric cluster. It's good enough for experimenting.\n\n  // config.New returns a new config.Config with sane defaults. Available values for env:\n  // local, lan, wan\n  c := config.New(\"local\")\n\n  // Callback function. It's called when this node is ready to accept connections.\n  ctx, cancel := context.WithCancel(context.Background())\n  c.Started = func() {\n    defer cancel()\n    log.Println(\"[INFO] Olric is ready to accept connections\")\n  }\n\n  // Create a new Olric instance.\n  db, err := olric.New(c)\n  if err != nil {\n    log.Fatalf(\"Failed to create Olric instance: %v\", err)\n  }\n\n  // Start the instance. It will form a single-node cluster.\n  go func() {\n    // Call Start at background. It's a blocker call.\n    err = db.Start()\n    if err != nil {\n      log.Fatalf(\"olric.Start returned an error: %v\", err)\n    }\n  }()\n\n  <-ctx.Done()\n\n  // In embedded-member scenario, you can use the EmbeddedClient. It implements\n  // the Client interface.\n  e := db.NewEmbeddedClient()\n\n  ps, err := e.NewPubSub()\n  if err != nil {\n    log.Fatalf(\"olric.NewPubSub returned an error: %v\", err)\n  }\n\n  ctx, cancel = context.WithCancel(context.Background())\n\n  // Olric implements a drop-in replacement of Redis Publish-Subscribe messaging\n  // system. PubSub client is just a thin layer around go-redis/redis.\n  rps := ps.Subscribe(ctx, \"my-channel\")\n\n  // Get a message to read messages from my-channel\n  msg := rps.Channel()\n\n  go func() {\n    // Publish a message here.\n    _, err := ps.Publish(ctx, \"my-channel\", \"Olric Rocks!\")\n    if err != nil {\n      log.Fatalf(\"PubSub.Publish returned an error: %v\", err)\n    }\n  }()\n\n  // Consume messages\n  rm := <-msg\n\n  fmt.Printf(\"Received message: \\\"%s\\\" from \\\"%s\\\"\", rm.Channel, rm.Payload)\n\n  // Don't forget the call Shutdown when you want to leave the cluster.\n  ctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n  defer cancel()\n\n  err = e.Close(ctx)\n  if err != nil {\n    log.Printf(\"Failed to close EmbeddedClient: %v\", err)\n  }\n}\n```\n\n### Client-Server scenario\n#### Distributed map\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n\n  \"github.com/buraksezer/olric\"\n)\n\nfunc main() {\n  // Sample for Olric v0.5.x\n\n  // Deployment scenario: client-server\n\n  // NewClusterClient takes a list of the nodes. This list may only contain a\n  // load balancer address. Please note that Olric nodes will calculate the partition owner\n  // and proxy the incoming requests.\n  c, err := olric.NewClusterClient([]string{\"localhost:3320\"})\n  if err != nil {\n    log.Fatalf(\"olric.NewClusterClient returned an error: %v\", err)\n  }\n\n  // In client-server scenario, you can use the ClusterClient. It implements\n  // the Client interface.\n  dm, err := c.NewDMap(\"bucket-of-arbitrary-items\")\n  if err != nil {\n    log.Fatalf(\"olric.NewDMap returned an error: %v\", err)\n  }\n\n  ctx, cancel := context.WithCancel(context.Background())\n\n  // Magic starts here!\n  fmt.Println(\"##\")\n  fmt.Println(\"Simple Put/Get on a DMap instance:\")\n  err = dm.Put(ctx, \"my-key\", \"Olric Rocks!\")\n  if err != nil {\n    log.Fatalf(\"Failed to call Put: %v\", err)\n  }\n\n  gr, err := dm.Get(ctx, \"my-key\")\n  if err != nil {\n    log.Fatalf(\"Failed to call Get: %v\", err)\n  }\n\n  // Olric uses the Redis serialization format.\n  value, err := gr.String()\n  if err != nil {\n    log.Fatalf(\"Failed to read Get response: %v\", err)\n  }\n\n  fmt.Println(\"Response for my-key:\", value)\n  fmt.Println(\"##\")\n\n  // Don't forget the call Shutdown when you want to leave the cluster.\n  ctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n  defer cancel()\n\n  err = c.Close(ctx)\n  if err != nil {\n    log.Printf(\"Failed to close ClusterClient: %v\", err)\n  }\n}\n```\n\n### SCAN on DMaps\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric\"\n\t\"github.com/buraksezer/olric/config\"\n)\n\nfunc main() {\n\t// Sample for Olric v0.5.x\n\n\t// Deployment scenario: embedded-member\n\t// This creates a single-node Olric cluster. It's good enough for experimenting.\n\n\t// config.New returns a new config.Config with sane defaults. Available values for env:\n\t// local, lan, wan\n\tc := config.New(\"local\")\n\n\t// Callback function. It's called when this node is ready to accept connections.\n\tctx, cancel := context.WithCancel(context.Background())\n\tc.Started = func() {\n\t\tdefer cancel()\n\t\tlog.Println(\"[INFO] Olric is ready to accept connections\")\n\t}\n\n\t// Create a new Olric instance.\n\tdb, err := olric.New(c)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create Olric instance: %v\", err)\n\t}\n\n\t// Start the instance. It will form a single-node cluster.\n\tgo func() {\n\t\t// Call Start at background. It's a blocker call.\n\t\terr = db.Start()\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"olric.Start returned an error: %v\", err)\n\t\t}\n\t}()\n\n\t<-ctx.Done()\n\n\t// In embedded-member scenario, you can use the EmbeddedClient. It implements\n\t// the Client interface.\n\te := db.NewEmbeddedClient()\n\n\tdm, err := e.NewDMap(\"bucket-of-arbitrary-items\")\n\tif err != nil {\n\t\tlog.Fatalf(\"olric.NewDMap returned an error: %v\", err)\n\t}\n\n\tctx, cancel = context.WithCancel(context.Background())\n\n\t// Magic starts here!\n\tfmt.Println(\"##\")\n\tfmt.Println(\"Insert 10 keys\")\n\tvar key string\n\tfor i := 0; i < 10; i++ {\n\t\tif i%2 == 0 {\n\t\t\tkey = fmt.Sprintf(\"even:%d\", i)\n\t\t} else {\n\t\t\tkey = fmt.Sprintf(\"odd:%d\", i)\n\t\t}\n\t\terr = dm.Put(ctx, key, nil)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Failed to call Put: %v\", err)\n\t\t}\n\t}\n\n\ti, err := dm.Scan(ctx)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to call Scan: %v\", err)\n\t}\n\n\tfmt.Println(\"Iterate over all the keys\")\n\tfor i.Next() {\n\t\tfmt.Println(\">> Key\", i.Key())\n\t}\n\n\ti.Close()\n\n\ti, err = dm.Scan(ctx, olric.Match(\"^even:\"))\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to call Scan: %v\", err)\n\t}\n\n\tfmt.Println(\"\\n\\nScan with regex: ^even:\")\n\tfor i.Next() {\n\t\tfmt.Println(\">> Key\", i.Key())\n\t}\n\n\ti.Close()\n\n\t// Don't forget the call Shutdown when you want to leave the cluster.\n\tctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\n\terr = db.Shutdown(ctx)\n\tif err != nil {\n\t\tlog.Printf(\"Failed to shutdown Olric: %v\", err)\n\t}\n}\n```\n\n#### Publish-Subscribe\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n\n  \"github.com/buraksezer/olric\"\n)\n\nfunc main() {\n  // Sample for Olric v0.5.x\n\n  // Deployment scenario: client-server\n\n  // NewClusterClient takes a list of the nodes. This list may only contain a\n  // load balancer address. Please note that Olric nodes will calculate the partition owner\n  // and proxy the incoming requests.\n  c, err := olric.NewClusterClient([]string{\"localhost:3320\"})\n  if err != nil {\n    log.Fatalf(\"olric.NewClusterClient returned an error: %v\", err)\n  }\n\n  // In client-server scenario, you can use the ClusterClient. It implements\n  // the Client interface.\n  ps, err := c.NewPubSub()\n  if err != nil {\n    log.Fatalf(\"olric.NewPubSub returned an error: %v\", err)\n  }\n\n  ctx, cancel := context.WithCancel(context.Background())\n\n  // Olric implements a drop-in replacement of Redis Publish-Subscribe messaging\n  // system. PubSub client is just a thin layer around go-redis/redis.\n  rps := ps.Subscribe(ctx, \"my-channel\")\n\n  // Get a message to read messages from my-channel\n  msg := rps.Channel()\n\n  go func() {\n    // Publish a message here.\n    _, err := ps.Publish(ctx, \"my-channel\", \"Olric Rocks!\")\n    if err != nil {\n      log.Fatalf(\"PubSub.Publish returned an error: %v\", err)\n    }\n  }()\n\n  // Consume messages\n  rm := <-msg\n\n  fmt.Printf(\"Received message: \\\"%s\\\" from \\\"%s\\\"\", rm.Channel, rm.Payload)\n\n  // Don't forget the call Shutdown when you want to leave the cluster.\n  ctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n  defer cancel()\n\n  err = c.Close(ctx)\n  if err != nil {\n    log.Printf(\"Failed to close ClusterClient: %v\", err)\n  }\n}\n\n```\n\n## Contributions\n\nPlease don't hesitate to fork the project and send a pull request or just e-mail me to ask questions and share ideas.\n\n## License\n\nThe Apache License, Version 2.0 - see LICENSE for more details.\n\n## About the name\n\nThe inner voice of Turgut Özben who is the main character of [Oğuz Atay's masterpiece -The Disconnected-](https://www.themodernnovel.org/asia/other-asia/turkey/oguz-atay/the-disconnected/).\n"
        },
        {
          "name": "client.go",
          "type": "blob",
          "size": 10.6435546875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/pkg/storage\"\n\t\"github.com/buraksezer/olric/stats\"\n)\n\nconst DefaultScanCount = 10\n\n// Member denotes a member of the Olric cluster.\ntype Member struct {\n\t// Member name in the cluster. It's also host:port of the node.\n\tName string\n\n\t// ID of the Member in the cluster. Hash of Name and Birthdate of the member\n\tID uint64\n\n\t// Birthdate of the member in nanoseconds.\n\tBirthdate int64\n\n\t// Role of the member in the cluster. There is only one coordinator member\n\t// in a healthy cluster.\n\tCoordinator bool\n}\n\n// Iterator defines an interface to implement iterators on the distributed maps.\ntype Iterator interface {\n\t// Next returns true if there is more key in the iterator implementation.\n\t// Otherwise, it returns false.\n\tNext() bool\n\n\t// Key returns a key name from the distributed map.\n\tKey() string\n\n\t// Close stops the iteration and releases allocated resources.\n\tClose()\n}\n\n// LockContext interface defines methods to manage locks on distributed maps.\ntype LockContext interface {\n\t// Unlock releases an acquired lock for the given key. It returns ErrNoSuchLock\n\t// if there is no lock for the given key.\n\tUnlock(ctx context.Context) error\n\n\t// Lease sets or updates the timeout of the acquired lock for the given key.\n\t// It returns ErrNoSuchLock if there is no lock for the given key.\n\tLease(ctx context.Context, duration time.Duration) error\n}\n\n// PutOption is a function for define options to control behavior of the Put command.\ntype PutOption func(*dmap.PutConfig)\n\n// EX sets the specified expire time, in seconds.\nfunc EX(ex time.Duration) PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasEX = true\n\t\tcfg.EX = ex\n\t}\n}\n\n// PX sets the specified expire time, in milliseconds.\nfunc PX(px time.Duration) PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasPX = true\n\t\tcfg.PX = px\n\t}\n}\n\n// EXAT sets the specified Unix time at which the key will expire, in seconds.\nfunc EXAT(exat time.Duration) PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasEXAT = true\n\t\tcfg.EXAT = exat\n\t}\n}\n\n// PXAT sets the specified Unix time at which the key will expire, in milliseconds.\nfunc PXAT(pxat time.Duration) PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasPXAT = true\n\t\tcfg.PXAT = pxat\n\t}\n}\n\n// NX only sets the key if it does not already exist.\nfunc NX() PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasNX = true\n\t}\n}\n\n// XX only sets the key if it already exists.\nfunc XX() PutOption {\n\treturn func(cfg *dmap.PutConfig) {\n\t\tcfg.HasXX = true\n\t}\n}\n\ntype dmapConfig struct {\n\tstorageEntryImplementation func() storage.Entry\n}\n\n// DMapOption is a function for defining options to control behavior of distributed map instances.\ntype DMapOption func(*dmapConfig)\n\n// StorageEntryImplementation sets and encoder/decoder implementation for your choice of storage engine.\nfunc StorageEntryImplementation(e func() storage.Entry) DMapOption {\n\treturn func(cfg *dmapConfig) {\n\t\tcfg.storageEntryImplementation = e\n\t}\n}\n\n// ScanOption is a function for defining options to control behavior of the SCAN command.\ntype ScanOption func(*dmap.ScanConfig)\n\n// Count is the user specified the amount of work that should be done at every call in order to\n// retrieve elements from the distributed map. This is just a hint for the implementation,\n// however generally speaking this is what you could expect most of the time from the implementation.\n// The default value is 10.\nfunc Count(c int) ScanOption {\n\treturn func(cfg *dmap.ScanConfig) {\n\t\tcfg.HasCount = true\n\t\tcfg.Count = c\n\t}\n}\n\n// Match is used for using regular expressions on keys. See https://pkg.go.dev/regexp\nfunc Match(s string) ScanOption {\n\treturn func(cfg *dmap.ScanConfig) {\n\t\tcfg.HasMatch = true\n\t\tcfg.Match = s\n\t}\n}\n\n// DMap defines methods to access and manipulate distributed maps.\ntype DMap interface {\n\t// Name exposes name of the DMap.\n\tName() string\n\n\t// Put sets the value for the given key. It overwrites any previous value for\n\t// that key, and it's thread-safe. The key has to be a string. value type is arbitrary.\n\t// It is safe to modify the contents of the arguments after Put returns but not before.\n\tPut(ctx context.Context, key string, value interface{}, options ...PutOption) error\n\n\t// Get gets the value for the given key. It returns ErrKeyNotFound if the DB\n\t// does not contain the key. It's thread-safe. It is safe to modify the contents\n\t// of the returned value. See GetResponse for the details.\n\tGet(ctx context.Context, key string) (*GetResponse, error)\n\n\t// Delete deletes values for the given keys. Delete will not return error\n\t// if key doesn't exist. It's thread-safe. It is safe to modify the contents\n\t// of the argument after Delete returns.\n\tDelete(ctx context.Context, keys ...string) (int, error)\n\n\t// Incr atomically increments the key by delta. The return value is the new value\n\t// after being incremented or an error.\n\tIncr(ctx context.Context, key string, delta int) (int, error)\n\n\t// Decr atomically decrements the key by delta. The return value is the new value\n\t// after being decremented or an error.\n\tDecr(ctx context.Context, key string, delta int) (int, error)\n\n\t// GetPut atomically sets the key to value and returns the old value stored at key. It returns nil if there is no\n\t// previous value.\n\tGetPut(ctx context.Context, key string, value interface{}) (*GetResponse, error)\n\n\t// IncrByFloat atomically increments the key by delta. The return value is the new value\n\t// after being incremented or an error.\n\tIncrByFloat(ctx context.Context, key string, delta float64) (float64, error)\n\n\t// Expire updates the expiry for the given key. It returns ErrKeyNotFound if\n\t// the DB does not contain the key. It's thread-safe.\n\tExpire(ctx context.Context, key string, timeout time.Duration) error\n\n\t// Lock sets a lock for the given key. Acquired lock is only for the key in\n\t// this dmap.\n\t//\n\t// It returns immediately if it acquires the lock for the given key. Otherwise,\n\t// it waits until deadline.\n\t//\n\t// You should know that the locks are approximate, and only to be used for\n\t// non-critical purposes.\n\tLock(ctx context.Context, key string, deadline time.Duration) (LockContext, error)\n\n\t// LockWithTimeout sets a lock for the given key. If the lock is still unreleased\n\t// the end of given period of time,\n\t// it automatically releases the lock. Acquired lock is only for the key in\n\t// this dmap.\n\t//\n\t// It returns immediately if it acquires the lock for the given key. Otherwise,\n\t// it waits until deadline.\n\t//\n\t// You should know that the locks are approximate, and only to be used for\n\t// non-critical purposes.\n\tLockWithTimeout(ctx context.Context, key string, timeout, deadline time.Duration) (LockContext, error)\n\n\t// Scan returns an iterator to loop over the keys.\n\t//\n\t// Available scan options:\n\t//\n\t// * Count\n\t// * Match\n\tScan(ctx context.Context, options ...ScanOption) (Iterator, error)\n\n\t// Destroy flushes the given DMap on the cluster. You should know that there\n\t// is no global lock on DMaps. So if you call Put/PutEx and Destroy methods\n\t// concurrently on the cluster, Put call may set new values to the DMap.\n\tDestroy(ctx context.Context) error\n\n\t// Pipeline is a mechanism to realise Redis Pipeline technique.\n\t//\n\t// Pipelining is a technique to extremely speed up processing by packing\n\t// operations to batches, send them at once to Redis and read a replies in a\n\t// singe step.\n\t// See https://redis.io/topics/pipelining\n\t//\n\t// Pay attention, that Pipeline is not a transaction, so you can get unexpected\n\t// results in case of big pipelines and small read/write timeouts.\n\t// Redis client has retransmission logic in case of timeouts, pipeline\n\t// can be retransmitted and commands can be executed more than once.\n\tPipeline(opts ...PipelineOption) (*DMapPipeline, error)\n}\n\n// PipelineOption is a function for defining options to control behavior of the Pipeline command.\ntype PipelineOption func(pipeline *DMapPipeline)\n\n// PipelineConcurrency is a PipelineOption controlling the number of concurrent goroutines.\nfunc PipelineConcurrency(concurrency int) PipelineOption {\n\treturn func(dp *DMapPipeline) {\n\t\tdp.concurrency = concurrency\n\t}\n}\n\ntype statsConfig struct {\n\tCollectRuntime bool\n}\n\n// StatsOption is a function for defining options to control behavior of the STATS command.\ntype StatsOption func(*statsConfig)\n\n// CollectRuntime is a StatsOption for collecting Go runtime statistics from a cluster member.\nfunc CollectRuntime() StatsOption {\n\treturn func(cfg *statsConfig) {\n\t\tcfg.CollectRuntime = true\n\t}\n}\n\ntype pubsubConfig struct {\n\tAddress string\n}\n\n// ToAddress is a PubSubOption for using a specific cluster member to publish messages to a channel.\nfunc ToAddress(addr string) PubSubOption {\n\treturn func(cfg *pubsubConfig) {\n\t\tcfg.Address = addr\n\t}\n}\n\n// PubSubOption is a function for defining options to control behavior of the Publish-Subscribe service.\ntype PubSubOption func(option *pubsubConfig)\n\n// Client is an interface that denotes an Olric client.\ntype Client interface {\n\t// NewDMap returns a new DMap client with the given options.\n\tNewDMap(name string, options ...DMapOption) (DMap, error)\n\n\t// NewPubSub returns a new PubSub client with the given options.\n\tNewPubSub(options ...PubSubOption) (*PubSub, error)\n\n\t// Stats returns stats.Stats with the given options.\n\tStats(ctx context.Context, address string, options ...StatsOption) (stats.Stats, error)\n\n\t// Ping sends a ping message to an Olric node. Returns PONG if message is empty,\n\t// otherwise return a copy of the message as a bulk. This command is often used to test\n\t// if a connection is still alive, or to measure latency.\n\tPing(ctx context.Context, address, message string) (string, error)\n\n\t// RoutingTable returns the latest version of the routing table.\n\tRoutingTable(ctx context.Context) (RoutingTable, error)\n\n\t// Members returns a thread-safe list of cluster members.\n\tMembers(ctx context.Context) ([]Member, error)\n\n\t// RefreshMetadata fetches a list of available members and the latest routing\n\t// table version. It also closes stale clients, if there are any.\n\tRefreshMetadata(ctx context.Context) error\n\n\t// Close stops background routines and frees allocated resources.\n\tClose(ctx context.Context) error\n}\n"
        },
        {
          "name": "cluster.go",
          "type": "blob",
          "size": 5.0419921875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/tidwall/redcon\"\n)\n\ntype Route struct {\n\tPrimaryOwners []string\n\tReplicaOwners []string\n}\n\ntype RoutingTable map[uint64]Route\n\nfunc mapToRoutingTable(slice []interface{}) (RoutingTable, error) {\n\trt := make(RoutingTable)\n\tfor _, raw := range slice {\n\t\titem := raw.([]interface{})\n\t\trawPartID, rawPrimaryOwners, rawReplicaOwners := item[0], item[1], item[2]\n\t\tvar partID uint64\n\t\tswitch rawPartID.(type) {\n\t\tcase int64:\n\t\t\tpartID = uint64(rawPartID.(int64))\n\t\tcase string:\n\t\t\traw, err := strconv.ParseUint(rawPartID.(string), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid partition id: %v: %w\", rawPartID, err)\n\t\t\t}\n\t\t\tpartID = raw\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"invalid partition id: %v\", rawPartID)\n\t\t}\n\n\t\tr := Route{}\n\t\tprimaryOwners, ok := rawPrimaryOwners.([]interface{})\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid primary owners: %v\", rawPrimaryOwners)\n\t\t}\n\t\tfor _, rawOwner := range primaryOwners {\n\t\t\towner, ok := rawOwner.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid owner: %v\", owner)\n\t\t\t}\n\t\t\tr.PrimaryOwners = append(r.PrimaryOwners, owner)\n\t\t}\n\n\t\treplicaOwners, ok := rawReplicaOwners.([]interface{})\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid replica owners: %v\", rawPrimaryOwners)\n\t\t}\n\t\tfor _, rawOwner := range replicaOwners {\n\t\t\towner, ok := rawOwner.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid owner: %v\", owner)\n\t\t\t}\n\t\t\tr.ReplicaOwners = append(r.ReplicaOwners, owner)\n\t\t}\n\t\trt[partID] = r\n\t}\n\treturn rt, nil\n}\n\nfunc (db *Olric) clusterRoutingTableCommandHandler(conn redcon.Conn, cmd redcon.Command) {\n\t_, err := protocol.ParseClusterRoutingTable(cmd)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\tcoordinator := db.rt.Discovery().GetCoordinator()\n\tif coordinator.CompareByID(db.rt.This()) {\n\t\tconn.WriteArray(int(db.config.PartitionCount))\n\t\trt := db.fillRoutingTable()\n\t\tfor partID := uint64(0); partID < db.config.PartitionCount; partID++ {\n\t\t\tconn.WriteArray(3)\n\t\t\tconn.WriteUint64(partID)\n\n\t\t\tr := rt[partID]\n\t\t\tprimaryOwners := r.PrimaryOwners\n\t\t\tconn.WriteArray(len(primaryOwners))\n\t\t\tfor _, owner := range primaryOwners {\n\t\t\t\tconn.WriteBulkString(owner)\n\t\t\t}\n\n\t\t\treplicaOwners := r.ReplicaOwners\n\t\t\tconn.WriteArray(len(replicaOwners))\n\t\t\tfor _, owner := range replicaOwners {\n\t\t\t\tconn.WriteBulkString(owner)\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\t// Redirect to the cluster coordinator\n\trtCmd := protocol.NewClusterRoutingTable().Command(db.ctx)\n\trc := db.client.Get(coordinator.String())\n\terr = rc.Process(db.ctx, rtCmd)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\tslice, err := rtCmd.Slice()\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\tconn.WriteAny(slice)\n}\n\nfunc (db *Olric) fillRoutingTable() RoutingTable {\n\trt := make(RoutingTable)\n\tfor partID := uint64(0); partID < db.config.PartitionCount; partID++ {\n\t\tr := Route{}\n\t\tprimaryOwners := db.primary.PartitionOwnersByID(partID)\n\t\tfor _, owner := range primaryOwners {\n\t\t\tr.PrimaryOwners = append(r.PrimaryOwners, owner.String())\n\t\t}\n\t\treplicaOwners := db.backup.PartitionOwnersByID(partID)\n\t\tfor _, owner := range replicaOwners {\n\t\t\tr.ReplicaOwners = append(r.ReplicaOwners, owner.String())\n\t\t}\n\t\trt[partID] = r\n\t}\n\treturn rt\n}\n\nfunc (db *Olric) routingTable(ctx context.Context) (RoutingTable, error) {\n\tcoordinator := db.rt.Discovery().GetCoordinator()\n\tif coordinator.CompareByID(db.rt.This()) {\n\t\treturn db.fillRoutingTable(), nil\n\t}\n\n\trtCmd := protocol.NewClusterRoutingTable().Command(ctx)\n\trc := db.client.Get(coordinator.String())\n\terr := rc.Process(ctx, rtCmd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tslice, err := rtCmd.Slice()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn mapToRoutingTable(slice)\n}\n\nfunc (db *Olric) clusterMembersCommandHandler(conn redcon.Conn, cmd redcon.Command) {\n\t_, err := protocol.ParseClusterMembers(cmd)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\n\tcoordinator := db.rt.Discovery().GetCoordinator()\n\tmembers := db.rt.Discovery().GetMembers()\n\tconn.WriteArray(len(members))\n\tfor _, member := range members {\n\t\tconn.WriteArray(3)\n\t\tconn.WriteBulkString(member.Name)\n\t\t// go-redis/redis package cannot handle uint64. At the time of this writing,\n\t\t// there is no solution for this, and I don't want to use a soft fork to repair it.\n\t\t//conn.WriteUint64(member.ID)\n\t\tconn.WriteInt64(member.Birthdate)\n\t\tif coordinator.CompareByID(member) {\n\t\t\tconn.WriteBulkString(\"true\")\n\t\t} else {\n\t\t\tconn.WriteBulkString(\"false\")\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "cluster_client.go",
          "type": "blob",
          "size": 21.6171875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/config\"\n\t\"github.com/buraksezer/olric/hasher\"\n\t\"github.com/buraksezer/olric/internal/bufpool\"\n\t\"github.com/buraksezer/olric/internal/cluster/partitions\"\n\t\"github.com/buraksezer/olric/internal/discovery\"\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/kvstore/entry\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/resp\"\n\t\"github.com/buraksezer/olric/internal/server\"\n\t\"github.com/buraksezer/olric/pkg/storage\"\n\t\"github.com/buraksezer/olric/stats\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nvar pool = bufpool.New()\n\n// DefaultRoutingTableFetchInterval is the default value of RoutingTableFetchInterval. ClusterClient implementation\n// fetches the routing table from the cluster to route requests to the right partition.\nconst DefaultRoutingTableFetchInterval = time.Minute\n\ntype ClusterLockContext struct {\n\tkey   string\n\ttoken string\n\tdm    *ClusterDMap\n}\n\n// ClusterDMap implements a client for DMaps.\ntype ClusterDMap struct {\n\tname          string\n\tnewEntry      func() storage.Entry\n\tconfig        *dmapConfig\n\tclient        *server.Client\n\tclusterClient *ClusterClient\n}\n\n// Name exposes name of the DMap.\nfunc (dm *ClusterDMap) Name() string {\n\treturn dm.name\n}\n\nfunc processProtocolError(err error) error {\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif err == redis.Nil {\n\t\treturn ErrKeyNotFound\n\t}\n\tif errors.Is(err, syscall.ECONNREFUSED) {\n\t\topErr := err.(*net.OpError)\n\t\treturn fmt.Errorf(\"%s %s %s: %w\", opErr.Op, opErr.Net, opErr.Addr, ErrConnRefused)\n\t}\n\treturn convertDMapError(protocol.ConvertError(err))\n}\n\nfunc (dm *ClusterDMap) writePutCommand(c *dmap.PutConfig, key string, value []byte) *protocol.Put {\n\tcmd := protocol.NewPut(dm.name, key, value)\n\tswitch {\n\tcase c.HasEX:\n\t\tcmd.SetEX(c.EX.Seconds())\n\tcase c.HasPX:\n\t\tcmd.SetPX(c.PX.Milliseconds())\n\tcase c.HasEXAT:\n\t\tcmd.SetEXAT(c.EXAT.Seconds())\n\tcase c.HasPXAT:\n\t\tcmd.SetPXAT(c.PXAT.Milliseconds())\n\t}\n\n\tswitch {\n\tcase c.HasNX:\n\t\tcmd.SetNX()\n\tcase c.HasXX:\n\t\tcmd.SetXX()\n\t}\n\n\treturn cmd\n}\n\nfunc (cl *ClusterClient) clientByPartID(partID uint64) (*redis.Client, error) {\n\traw := cl.routingTable.Load()\n\tif raw == nil {\n\t\treturn nil, fmt.Errorf(\"routing table is empty\")\n\t}\n\n\troutingTable, ok := raw.(RoutingTable)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"routing table is corrupt\")\n\t}\n\n\troute := routingTable[partID]\n\tif len(route.PrimaryOwners) == 0 {\n\t\treturn nil, fmt.Errorf(\"primary owners list for %d is empty\", partID)\n\t}\n\n\tprimaryOwner := route.PrimaryOwners[len(route.PrimaryOwners)-1]\n\treturn cl.client.Get(primaryOwner), nil\n}\n\nfunc (cl *ClusterClient) smartPick(dmap, key string) (*redis.Client, error) {\n\thkey := partitions.HKey(dmap, key)\n\tpartID := hkey % cl.partitionCount\n\treturn cl.clientByPartID(partID)\n}\n\n// Put sets the value for the given key. It overwrites any previous value for\n// that key, and it's thread-safe. The key has to be a string. value type is arbitrary.\n// It is safe to modify the contents of the arguments after Put returns but not before.\nfunc (dm *ClusterDMap) Put(ctx context.Context, key string, value interface{}, options ...PutOption) error {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvalueBuf := pool.Get()\n\tdefer pool.Put(valueBuf)\n\n\tenc := resp.New(valueBuf)\n\terr = enc.Encode(value)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar pc dmap.PutConfig\n\tfor _, opt := range options {\n\t\topt(&pc)\n\t}\n\tputCmd := dm.writePutCommand(&pc, key, valueBuf.Bytes())\n\tcmd := putCmd.Command(ctx)\n\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn processProtocolError(err)\n\t}\n\treturn processProtocolError(cmd.Err())\n}\n\nfunc (dm *ClusterDMap) makeGetResponse(cmd *redis.StringCmd) (*GetResponse, error) {\n\traw, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\n\te := dm.newEntry()\n\te.Decode(raw)\n\treturn &GetResponse{\n\t\tentry: e,\n\t}, nil\n}\n\n// Get gets the value for the given key. It returns ErrKeyNotFound if the DB\n// does not contain the key. It's thread-safe. It is safe to modify the contents\n// of the returned value. See GetResponse for the details.\nfunc (dm *ClusterDMap) Get(ctx context.Context, key string) (*GetResponse, error) {\n\tcmd := protocol.NewGet(dm.name, key).SetRaw().Command(ctx)\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\treturn dm.makeGetResponse(cmd)\n}\n\n// Delete deletes values for the given keys. Delete will not return error\n// if key doesn't exist. It's thread-safe. It is safe to modify the contents\n// of the argument after Delete returns.\nfunc (dm *ClusterDMap) Delete(ctx context.Context, keys ...string) (int, error) {\n\trc, err := dm.client.Pick()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tcmd := protocol.NewDel(dm.name, keys...).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn 0, processProtocolError(err)\n\t}\n\n\tres, err := cmd.Uint64()\n\tif err != nil {\n\t\treturn 0, processProtocolError(cmd.Err())\n\t}\n\treturn int(res), nil\n}\n\n// Incr atomically increments the key by delta. The return value is the new value\n// after being incremented or an error.\nfunc (dm *ClusterDMap) Incr(ctx context.Context, key string, delta int) (int, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tcmd := protocol.NewIncr(dm.name, key, delta).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn 0, processProtocolError(err)\n\t}\n\tres, err := cmd.Uint64()\n\tif err != nil {\n\t\treturn 0, processProtocolError(cmd.Err())\n\t}\n\treturn int(res), nil\n}\n\n// Decr atomically decrements the key by delta. The return value is the new value\n// after being decremented or an error.\nfunc (dm *ClusterDMap) Decr(ctx context.Context, key string, delta int) (int, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tcmd := protocol.NewDecr(dm.name, key, delta).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn 0, processProtocolError(err)\n\t}\n\tres, err := cmd.Uint64()\n\tif err != nil {\n\t\treturn 0, processProtocolError(cmd.Err())\n\t}\n\treturn int(res), nil\n}\n\n// GetPut atomically sets the key to value and returns the old value stored at key. It returns nil if there is no\n// previous value.\nfunc (dm *ClusterDMap) GetPut(ctx context.Context, key string, value interface{}) (*GetResponse, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalueBuf := pool.Get()\n\tdefer pool.Put(valueBuf)\n\n\tenc := resp.New(valueBuf)\n\terr = enc.Encode(value)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcmd := protocol.NewGetPut(dm.name, key, valueBuf.Bytes()).SetRaw().Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\terr = processProtocolError(err)\n\tif err != nil {\n\t\t// First try to set a key/value with GetPut\n\t\tif err == ErrKeyNotFound {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\traw, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\n\te := dm.newEntry()\n\te.Decode(raw)\n\treturn &GetResponse{\n\t\tentry: e,\n\t}, nil\n}\n\n// IncrByFloat atomically increments the key by delta. The return value is the new value\n// after being incremented or an error.\nfunc (dm *ClusterDMap) IncrByFloat(ctx context.Context, key string, delta float64) (float64, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tcmd := protocol.NewIncrByFloat(dm.name, key, delta).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn 0, processProtocolError(err)\n\t}\n\tres, err := cmd.Result()\n\tif err != nil {\n\t\treturn 0, processProtocolError(cmd.Err())\n\t}\n\treturn res, nil\n}\n\n// Expire updates the expiry for the given key. It returns ErrKeyNotFound if\n// the DB does not contain the key. It's thread-safe.\nfunc (dm *ClusterDMap) Expire(ctx context.Context, key string, timeout time.Duration) error {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcmd := protocol.NewExpire(dm.name, key, timeout).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn processProtocolError(err)\n\t}\n\treturn processProtocolError(cmd.Err())\n}\n\n// Lock sets a lock for the given key. Acquired lock is only for the key in\n// this dmap.\n//\n// It returns immediately if it acquires the lock for the given key. Otherwise,\n// it waits until deadline.\n//\n// You should know that the locks are approximate, and only to be used for\n// non-critical purposes.\nfunc (dm *ClusterDMap) Lock(ctx context.Context, key string, deadline time.Duration) (LockContext, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcmd := protocol.NewLock(dm.name, key, deadline.Seconds()).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\n\ttoken, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\treturn &ClusterLockContext{\n\t\tkey:   key,\n\t\ttoken: string(token),\n\t\tdm:    dm,\n\t}, nil\n}\n\n// LockWithTimeout sets a lock for the given key. If the lock is still unreleased\n// the end of given period of time, it automatically releases the lock.\n// Acquired lock is only for the key in this DMap.\n//\n// It returns immediately if it acquires the lock for the given key. Otherwise,\n// it waits until deadline.\n//\n// You should know that the locks are approximate, and only to be used for\n// non-critical purposes.\nfunc (dm *ClusterDMap) LockWithTimeout(ctx context.Context, key string, timeout, deadline time.Duration) (LockContext, error) {\n\trc, err := dm.clusterClient.smartPick(dm.name, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcmd := protocol.NewLock(dm.name, key, deadline.Seconds()).SetPX(timeout.Milliseconds()).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\n\ttoken, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn nil, processProtocolError(err)\n\t}\n\n\treturn &ClusterLockContext{\n\t\tkey:   key,\n\t\ttoken: string(token),\n\t\tdm:    dm,\n\t}, nil\n}\n\nfunc (c *ClusterLockContext) Unlock(ctx context.Context) error {\n\trc, err := c.dm.clusterClient.smartPick(c.dm.name, c.key)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcmd := protocol.NewUnlock(c.dm.name, c.key, c.token).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn processProtocolError(err)\n\t}\n\treturn processProtocolError(cmd.Err())\n}\n\nfunc (c *ClusterLockContext) Lease(ctx context.Context, duration time.Duration) error {\n\trc, err := c.dm.clusterClient.smartPick(c.dm.name, c.key)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcmd := protocol.NewLockLease(c.dm.name, c.key, c.token, duration.Seconds()).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn processProtocolError(err)\n\t}\n\treturn processProtocolError(cmd.Err())\n}\n\n// Scan returns an iterator to loop over the keys.\n//\n// Available scan options:\n//\n// * Count\n// * Match\nfunc (dm *ClusterDMap) Scan(ctx context.Context, options ...ScanOption) (Iterator, error) {\n\tvar sc dmap.ScanConfig\n\tfor _, opt := range options {\n\t\topt(&sc)\n\t}\n\tif sc.Count == 0 {\n\t\tsc.Count = DefaultScanCount\n\t}\n\n\tictx, cancel := context.WithCancel(ctx)\n\ti := &ClusterIterator{\n\t\tdm:            dm,\n\t\tclusterClient: dm.clusterClient,\n\t\tconfig:        &sc,\n\t\tlogger:        dm.clusterClient.logger,\n\t\tpartitionKeys: make(map[string]struct{}),\n\t\tcursors:       make(map[uint64]map[string]*currentCursor),\n\t\tctx:           ictx,\n\t\tcancel:        cancel,\n\t}\n\n\t// Embedded iterator uses a slightly different scan function.\n\ti.scanner = i.scanOnOwners\n\n\tif err := i.fetchRoutingTable(); err != nil {\n\t\treturn nil, err\n\t}\n\t// Load the route for the first partition (0) to scan.\n\ti.loadRoute()\n\n\ti.wg.Add(1)\n\tgo i.fetchRoutingTablePeriodically()\n\n\treturn i, nil\n}\n\n// Destroy flushes the given DMap on the cluster. You should know that there\n// is no global lock on DMaps. So if you call Put/PutEx and Destroy methods\n// concurrently on the cluster, Put call may set new values to the DMap.\nfunc (dm *ClusterDMap) Destroy(ctx context.Context) error {\n\trc, err := dm.client.Pick()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcmd := protocol.NewDestroy(dm.name).Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn processProtocolError(err)\n\t}\n\n\treturn processProtocolError(cmd.Err())\n}\n\ntype ClusterClient struct {\n\tclient         *server.Client\n\tconfig         *clusterClientConfig\n\tlogger         *log.Logger\n\troutingTable   atomic.Value\n\tpartitionCount uint64\n\twg             sync.WaitGroup\n\tctx            context.Context\n\tcancel         context.CancelFunc\n}\n\n// Ping sends a ping message to an Olric node. Returns PONG if message is empty,\n// otherwise return a copy of the message as a bulk. This command is often used to test\n// if a connection is still alive, or to measure latency.\nfunc (cl *ClusterClient) Ping(ctx context.Context, addr, message string) (string, error) {\n\tpingCmd := protocol.NewPing()\n\tif message != \"\" {\n\t\tpingCmd.SetMessage(message)\n\t}\n\tcmd := pingCmd.Command(ctx)\n\n\trc := cl.client.Get(addr)\n\terr := rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn \"\", processProtocolError(err)\n\t}\n\terr = processProtocolError(cmd.Err())\n\tif err != nil {\n\t\treturn \"\", nil\n\t}\n\n\treturn cmd.Result()\n}\n\n// RoutingTable returns the latest version of the routing table.\nfunc (cl *ClusterClient) RoutingTable(ctx context.Context) (RoutingTable, error) {\n\tcmd := protocol.NewClusterRoutingTable().Command(ctx)\n\trc, err := cl.client.Pick()\n\tif err != nil {\n\t\treturn RoutingTable{}, err\n\t}\n\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn RoutingTable{}, processProtocolError(err)\n\t}\n\n\tif err = cmd.Err(); err != nil {\n\t\treturn RoutingTable{}, processProtocolError(err)\n\t}\n\n\tresult, err := cmd.Slice()\n\tif err != nil {\n\t\treturn RoutingTable{}, processProtocolError(err)\n\n\t}\n\treturn mapToRoutingTable(result)\n}\n\n// Stats returns stats.Stats with the given options.\nfunc (cl *ClusterClient) Stats(ctx context.Context, address string, options ...StatsOption) (stats.Stats, error) {\n\tvar cfg statsConfig\n\tfor _, opt := range options {\n\t\topt(&cfg)\n\t}\n\n\tstatsCmd := protocol.NewStats()\n\tif cfg.CollectRuntime {\n\t\tstatsCmd.SetCollectRuntime()\n\t}\n\n\tcmd := statsCmd.Command(ctx)\n\trc := cl.client.Get(address)\n\n\terr := rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\n\tif err = cmd.Err(); err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\tdata, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\tvar s stats.Stats\n\terr = json.Unmarshal(data, &s)\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\treturn s, nil\n}\n\n// Members returns a thread-safe list of cluster members.\nfunc (cl *ClusterClient) Members(ctx context.Context) ([]Member, error) {\n\trc, err := cl.client.Pick()\n\tif err != nil {\n\t\treturn []Member{}, err\n\t}\n\n\tcmd := protocol.NewClusterMembers().Command(ctx)\n\terr = rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn []Member{}, processProtocolError(err)\n\t}\n\n\tif err = cmd.Err(); err != nil {\n\t\treturn []Member{}, processProtocolError(err)\n\t}\n\n\titems, err := cmd.Slice()\n\tif err != nil {\n\t\treturn []Member{}, processProtocolError(err)\n\t}\n\tvar members []Member\n\tfor _, rawItem := range items {\n\t\tm := Member{}\n\t\titem := rawItem.([]interface{})\n\t\tm.Name = item[0].(string)\n\t\tm.Birthdate = item[1].(int64)\n\n\t\t// go-redis/redis package cannot handle uint64 type. At the time of this writing,\n\t\t// there is no solution for this, and I don't want to use a soft fork to repair it.\n\t\tm.ID = discovery.MemberID(m.Name, m.Birthdate)\n\n\t\tif item[2] == \"true\" {\n\t\t\tm.Coordinator = true\n\t\t}\n\t\tmembers = append(members, m)\n\t}\n\treturn members, nil\n}\n\n// RefreshMetadata fetches a list of available members and the latest routing\n// table version. It also closes stale clients, if there are any.\nfunc (cl *ClusterClient) RefreshMetadata(ctx context.Context) error {\n\t// Fetch a list of currently available cluster members.\n\tvar members []Member\n\tvar err error\n\tfor {\n\t\tmembers, err = cl.Members(ctx)\n\t\tif errors.Is(err, ErrConnRefused) {\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbreak\n\t}\n\t// Use a map for fast access.\n\taddresses := make(map[string]struct{})\n\tfor _, member := range members {\n\t\taddresses[member.Name] = struct{}{}\n\t}\n\n\t// Clean stale client connections\n\tfor addr := range cl.client.Addresses() {\n\t\tif _, ok := addresses[addr]; !ok {\n\t\t\t// Gone\n\t\t\tif err := cl.client.Close(addr); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Re-fetch the routing table, we should use the latest routing table version.\n\treturn cl.fetchRoutingTable()\n}\n\n// Close stops background routines and frees allocated resources.\nfunc (cl *ClusterClient) Close(ctx context.Context) error {\n\tselect {\n\tcase <-cl.ctx.Done():\n\t\treturn nil\n\tdefault:\n\t}\n\n\tcl.cancel()\n\n\t// Wait for the background workers:\n\t// * fetchRoutingTablePeriodically\n\tcl.wg.Wait()\n\n\t// Close the underlying TCP sockets gracefully.\n\treturn cl.client.Shutdown(ctx)\n}\n\n// NewPubSub returns a new PubSub client with the given options.\nfunc (cl *ClusterClient) NewPubSub(options ...PubSubOption) (*PubSub, error) {\n\treturn newPubSub(cl.client, options...)\n}\n\n// NewDMap returns a new DMap client with the given options.\nfunc (cl *ClusterClient) NewDMap(name string, options ...DMapOption) (DMap, error) {\n\tvar dc dmapConfig\n\tfor _, opt := range options {\n\t\topt(&dc)\n\t}\n\n\tif dc.storageEntryImplementation == nil {\n\t\tdc.storageEntryImplementation = func() storage.Entry {\n\t\t\treturn entry.New()\n\t\t}\n\t}\n\n\treturn &ClusterDMap{name: name,\n\t\tconfig:        &dc,\n\t\tnewEntry:      dc.storageEntryImplementation,\n\t\tclient:        cl.client,\n\t\tclusterClient: cl,\n\t}, nil\n}\n\ntype ClusterClientOption func(c *clusterClientConfig)\n\ntype clusterClientConfig struct {\n\tlogger                    *log.Logger\n\tconfig                    *config.Client\n\thasher                    hasher.Hasher\n\troutingTableFetchInterval time.Duration\n}\n\nfunc WithHasher(h hasher.Hasher) ClusterClientOption {\n\treturn func(cfg *clusterClientConfig) {\n\t\tcfg.hasher = h\n\t}\n}\n\nfunc WithLogger(l *log.Logger) ClusterClientOption {\n\treturn func(cfg *clusterClientConfig) {\n\t\tcfg.logger = l\n\t}\n}\n\nfunc WithConfig(c *config.Client) ClusterClientOption {\n\treturn func(cfg *clusterClientConfig) {\n\t\tcfg.config = c\n\t}\n}\n\n// WithRoutingTableFetchInterval is used to set a custom value to routingTableFetchInterval. ClusterClient implementation\n// retrieves the routing table from the cluster to route requests to the partition owners.\nfunc WithRoutingTableFetchInterval(interval time.Duration) ClusterClientOption {\n\treturn func(cfg *clusterClientConfig) {\n\t\tcfg.routingTableFetchInterval = interval\n\t}\n}\n\nfunc (cl *ClusterClient) fetchRoutingTable() error {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\troutingTable, err := cl.RoutingTable(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error while loading the routing table: %w\", err)\n\t}\n\n\tprevious := cl.routingTable.Load()\n\tif previous == nil {\n\t\t// First run. Partition count is a constant, actually. It has to be greater than zero.\n\t\tcl.partitionCount = uint64(len(routingTable))\n\t}\n\tcl.routingTable.Store(routingTable)\n\treturn nil\n}\n\nfunc (cl *ClusterClient) fetchRoutingTablePeriodically() {\n\tdefer cl.wg.Done()\n\n\tticker := time.NewTicker(cl.config.routingTableFetchInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-cl.ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\terr := cl.fetchRoutingTable()\n\t\t\tif err != nil {\n\t\t\t\tcl.logger.Printf(\"[ERROR] Failed to fetch the latest version of the routing table: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// NewClusterClient creates a new Client instance. It needs one node address at least to discover the whole cluster.\nfunc NewClusterClient(addresses []string, options ...ClusterClientOption) (*ClusterClient, error) {\n\tif len(addresses) == 0 {\n\t\treturn nil, fmt.Errorf(\"addresses cannot be empty\")\n\t}\n\n\tvar cc clusterClientConfig\n\tfor _, opt := range options {\n\t\topt(&cc)\n\t}\n\n\tif cc.hasher == nil {\n\t\tcc.hasher = hasher.NewDefaultHasher()\n\t}\n\n\tif cc.logger == nil {\n\t\tcc.logger = log.New(os.Stderr, \"logger: \", log.Lshortfile)\n\t}\n\n\tif cc.config == nil {\n\t\tcc.config = config.NewClient()\n\t}\n\n\tif cc.routingTableFetchInterval <= 0 {\n\t\tcc.routingTableFetchInterval = DefaultRoutingTableFetchInterval\n\t}\n\n\tif err := cc.config.Sanitize(); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := cc.config.Validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tcl := &ClusterClient{\n\t\tclient: server.NewClient(cc.config),\n\t\tconfig: &cc,\n\t\tlogger: cc.logger,\n\t\tctx:    ctx,\n\t\tcancel: cancel,\n\t}\n\n\t// Initialize clients for the given cluster members.\n\tfor _, address := range addresses {\n\t\tcl.client.Get(address)\n\t}\n\n\t// Discover all cluster members\n\tmembers, err := cl.Members(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error while discovering the cluster members: %w\", err)\n\t}\n\tfor _, member := range members {\n\t\tcl.client.Get(member.Name)\n\t}\n\n\t// Hash function is required to target primary owners instead of random cluster members.\n\tpartitions.SetHashFunc(cc.hasher)\n\n\t// Initial fetch. ClusterClient targets the primary owners for a smooth and quick operation.\n\tif err := cl.fetchRoutingTable(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Refresh the routing table in every 15 seconds.\n\tcl.wg.Add(1)\n\tgo cl.fetchRoutingTablePeriodically()\n\n\treturn cl, nil\n}\n\nvar (\n\t_ Client = (*ClusterClient)(nil)\n\t_ DMap   = (*ClusterDMap)(nil)\n)\n"
        },
        {
          "name": "cluster_client_test.go",
          "type": "blob",
          "size": 18.2099609375,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/config\"\n\t\"github.com/buraksezer/olric/hasher\"\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/buraksezer/olric/stats\"\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestClusterClient_Ping(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tcluster.addMember(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tresponse, err := c.Ping(ctx, db.rt.This().String(), \"\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, DefaultPingResponse, response)\n}\n\nfunc TestClusterClient_Ping_WithMessage(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tcluster.addMember(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tmessage := \"Olric is the best!\"\n\tresult, err := c.Ping(ctx, db.rt.This().String(), message)\n\trequire.NoError(t, err)\n\trequire.Equal(t, message, result)\n}\n\nfunc TestClusterClient_RoutingTable(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\trt, err := c.RoutingTable(ctx)\n\trequire.NoError(t, err)\n\n\trequire.Len(t, rt, int(db.config.PartitionCount))\n}\n\nfunc TestClusterClient_RoutingTable_Cluster(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tcluster.addMember(t) // Cluster coordinator\n\t<-time.After(250 * time.Millisecond)\n\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\trt, err := c.RoutingTable(ctx)\n\trequire.NoError(t, err)\n\n\trequire.Len(t, rt, int(db.config.PartitionCount))\n}\n\nfunc TestClusterClient_Put(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n}\n\nfunc TestClusterClient_Get(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\tgr, err := dm.Get(ctx, \"mykey\")\n\trequire.NoError(t, err)\n\n\tres, err := gr.String()\n\trequire.NoError(t, err)\n\n\trequire.Equal(t, res, \"myvalue\")\n}\n\nfunc TestClusterClient_Delete(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\tcount, err := dm.Delete(ctx, \"mykey\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, count)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Delete_Many_Keys(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tvar keys []string\n\tfor i := 0; i < 10; i++ {\n\t\tkey := testutil.ToKey(i)\n\t\terr = dm.Put(context.Background(), key, \"myvalue\")\n\t\trequire.NoError(t, err)\n\t\tkeys = append(keys, key)\n\t}\n\n\tcount, err := dm.Delete(context.Background(), keys...)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 10, count)\n}\n\nfunc TestClusterClient_Destroy(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\terr = dm.Destroy(ctx)\n\trequire.NoError(t, err)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Incr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 10; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.Incr(ctx, \"mykey\", 1)\n\t\t\treturn err\n\t\t})\n\t}\n\n\trequire.NoError(t, errGr.Wait())\n\n\tresult, err := dm.Incr(ctx, \"mykey\", 1)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 11, result)\n}\n\nfunc TestClusterClient_IncrByFloat(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 10; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.IncrByFloat(ctx, \"mykey\", 1.2)\n\t\t\treturn err\n\t\t})\n\t}\n\n\trequire.NoError(t, errGr.Wait())\n\n\tresult, err := dm.IncrByFloat(ctx, \"mykey\", 1.2)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 13.199999999999998, result)\n}\n\nfunc TestClusterClient_Decr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", 11)\n\trequire.NoError(t, err)\n\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 10; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.Decr(ctx, \"mykey\", 1)\n\t\t\treturn err\n\t\t})\n\t}\n\n\trequire.NoError(t, errGr.Wait())\n\n\tresult, err := dm.Decr(ctx, \"mykey\", 1)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 0, result)\n}\n\nfunc TestClusterClient_GetPut(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tgr, err := dm.GetPut(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\trequire.Nil(t, gr)\n\n\tgr, err = dm.GetPut(ctx, \"mykey\", \"myvalue-2\")\n\trequire.NoError(t, err)\n\n\tvalue, err := gr.String()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"myvalue\", value)\n}\n\nfunc TestClusterClient_Expire(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\terr = dm.Expire(ctx, \"mykey\", time.Millisecond)\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Lock_Unlock(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tlx, err := dm.Lock(ctx, \"lock.foo.key\", time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n}\n\nfunc TestClusterClient_Lock_Lease(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tlx, err := dm.Lock(ctx, \"lock.foo.key\", time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Lease(ctx, time.Millisecond)\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestClusterClient_Lock_ErrLockNotAcquired(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\t_, err = dm.Lock(ctx, \"lock.foo.key\", time.Second)\n\trequire.NoError(t, err)\n\n\t_, err = dm.Lock(ctx, \"lock.foo.key\", time.Millisecond)\n\trequire.ErrorIs(t, err, ErrLockNotAcquired)\n}\n\nfunc TestClusterClient_LockWithTimeout(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tlx, err := dm.LockWithTimeout(ctx, \"lock.foo.key\", time.Hour, time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n}\n\nfunc TestClusterClient_LockWithTimeout_ErrNoSuchLock(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tlx, err := dm.LockWithTimeout(ctx, \"lock.foo.key\", time.Millisecond, time.Second)\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestClusterClient_LockWithTimeout_Then_Lease(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tlx, err := dm.LockWithTimeout(ctx, \"lock.foo.key\", 50*time.Millisecond, time.Second)\n\trequire.NoError(t, err)\n\n\t// Expand its timeout value\n\terr = lx.Lease(ctx, time.Hour)\n\trequire.NoError(t, err)\n\n\t<-time.After(100 * time.Millisecond)\n\n\t_, err = dm.Lock(ctx, \"lock.foo.key\", time.Millisecond)\n\trequire.ErrorIs(t, err, ErrLockNotAcquired)\n}\n\nfunc TestClusterClient_LockWithTimeout_ErrLockNotAcquired(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\t_, err = dm.LockWithTimeout(ctx, \"lock.foo.key\", time.Hour, time.Second)\n\trequire.NoError(t, err)\n\n\t_, err = dm.Lock(ctx, \"lock.foo.key\", time.Millisecond)\n\trequire.Equal(t, err, ErrLockNotAcquired)\n}\n\nfunc TestClusterClient_Put_Ex(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", EX(time.Second))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Second)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Put_PX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", PX(time.Millisecond))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Put_EXAT(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", EXAT(time.Duration(time.Now().Add(time.Second).UnixNano())))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Second)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Put_PXAT(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", PXAT(time.Duration(time.Now().Add(time.Millisecond).UnixNano())))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Put_NX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue-2\", NX())\n\trequire.ErrorIs(t, err, ErrKeyFound)\n\n\tgr, err := dm.Get(ctx, \"mykey\")\n\trequire.NoError(t, err)\n\n\tvalue, err := gr.String()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"myvalue\", value)\n}\n\nfunc TestClusterClient_Put_XX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue-2\", XX())\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestClusterClient_Stats(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tvar empty stats.Stats\n\ts, err := c.Stats(ctx, db.rt.This().String())\n\trequire.NoError(t, err)\n\trequire.Nil(t, s.Runtime)\n\trequire.NotEqual(t, empty, s)\n}\n\nfunc TestClusterClient_Stats_Cluster(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\tdb2 := cluster.addMember(t)\n\n\t<-time.After(250 * time.Millisecond)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tvar empty stats.Stats\n\ts, err := c.Stats(ctx, db2.rt.This().String())\n\trequire.NoError(t, err)\n\trequire.Nil(t, s.Runtime)\n\trequire.NotEqual(t, empty, s)\n\trequire.Equal(t, db2.rt.This().String(), s.Member.String())\n}\n\nfunc TestClusterClient_Stats_CollectRuntime(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tvar empty stats.Stats\n\ts, err := c.Stats(ctx, db.rt.This().String(), CollectRuntime())\n\trequire.NoError(t, err)\n\trequire.NotNil(t, s.Runtime)\n\trequire.NotEqual(t, empty, s)\n}\n\nfunc TestClusterClient_Set_Options(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\n\tlg := log.New(os.Stderr, \"logger: \", log.Lshortfile)\n\tcfg := config.NewClient()\n\tc, err := NewClusterClient([]string{db.name}, WithConfig(cfg), WithLogger(lg))\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\trequire.Equal(t, cfg, c.config.config)\n\trequire.Equal(t, lg, c.config.logger)\n}\n\nfunc TestClusterClient_Members(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tcluster.addMember(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tmembers, err := c.Members(ctx)\n\trequire.NoError(t, err)\n\trequire.Len(t, members, 2)\n\n\tcoordinator := db.rt.Discovery().GetCoordinator()\n\tfor _, member := range members {\n\t\trequire.NotEqual(t, \"\", member.Name)\n\t\trequire.NotEqual(t, 0, member.ID)\n\t\trequire.NotEqual(t, 0, member.Birthdate)\n\t\tif coordinator.ID == member.ID {\n\t\t\trequire.True(t, member.Coordinator)\n\t\t} else {\n\t\t\trequire.False(t, member.Coordinator)\n\t\t}\n\t}\n}\n\nfunc TestClusterClient_smartPick(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb1 := cluster.addMember(t)\n\tdb2 := cluster.addMember(t)\n\tdb3 := cluster.addMember(t)\n\tdb4 := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient(\n\t\t[]string{db1.name, db2.name, db3.name, db4.name},\n\t\tWithHasher(hasher.NewDefaultHasher()),\n\t)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tclients := make(map[string]struct{})\n\tfor i := 0; i < 1000; i++ {\n\t\trc, err := c.smartPick(\"mydmap\", testutil.ToKey(i))\n\t\trequire.NoError(t, err)\n\t\tclients[rc.String()] = struct{}{}\n\t}\n\trequire.Len(t, clients, 4)\n}\n"
        },
        {
          "name": "cluster_iterator.go",
          "type": "blob",
          "size": 6.70703125,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n)\n\ntype currentCursor struct {\n\tprimary uint64\n\treplica uint64\n}\n\n// ClusterIterator implements distributed query on DMaps.\ntype ClusterIterator struct {\n\tmtx             sync.Mutex // protects pos and page\n\troutingTableMtx sync.Mutex // protects routingTable and partitionCount\n\n\tlogger         *log.Logger\n\tdm             *ClusterDMap\n\tclusterClient  *ClusterClient\n\tpos            int\n\tpage           []string\n\troute          *Route\n\tpartitionKeys  map[string]struct{}\n\tcursors        map[uint64]map[string]*currentCursor\n\tpartID         uint64 // current partition id\n\troutingTable   RoutingTable\n\tpartitionCount uint64\n\tconfig         *dmap.ScanConfig\n\tscanner        func() error\n\twg             sync.WaitGroup\n\tctx            context.Context\n\tcancel         context.CancelFunc\n}\n\nfunc (i *ClusterIterator) loadRoute() {\n\ti.routingTableMtx.Lock()\n\tdefer i.routingTableMtx.Unlock()\n\n\troute, ok := i.routingTable[i.partID]\n\tif !ok {\n\t\tpanic(\"partID: could not be found in the routing table\")\n\t}\n\ti.route = &route\n}\n\nfunc (i *ClusterIterator) updateCursor(owner string, cursor uint64) {\n\tif _, ok := i.cursors[i.partID]; !ok {\n\t\ti.cursors[i.partID] = make(map[string]*currentCursor)\n\t}\n\tcc, ok := i.cursors[i.partID][owner]\n\tif !ok {\n\t\tcc = &currentCursor{}\n\t\tif i.config.Replica {\n\t\t\tcc.replica = cursor\n\t\t} else {\n\t\t\tcc.primary = cursor\n\t\t}\n\t\ti.cursors[i.partID][owner] = cc\n\t\treturn\n\t}\n\n\tif i.config.Replica {\n\t\tcc.replica = cursor\n\t} else {\n\t\tcc.primary = cursor\n\t}\n\ti.cursors[i.partID][owner] = cc\n}\n\nfunc (i *ClusterIterator) loadCursor(owner string) uint64 {\n\tif _, ok := i.cursors[i.partID]; !ok {\n\t\treturn 0\n\t}\n\tcc, ok := i.cursors[i.partID][owner]\n\tif !ok {\n\t\treturn 0\n\t}\n\tif i.config.Replica {\n\t\treturn cc.replica\n\t}\n\treturn cc.primary\n}\n\nfunc (i *ClusterIterator) updateIterator(keys []string, cursor uint64, owner string) {\n\tfor _, key := range keys {\n\t\tif _, ok := i.partitionKeys[key]; !ok {\n\t\t\ti.page = append(i.page, key)\n\t\t\ti.partitionKeys[key] = struct{}{}\n\t\t}\n\t}\n\ti.updateCursor(owner, cursor)\n}\n\nfunc (i *ClusterIterator) getOwners() []string {\n\tvar raw []string\n\tif i.config.Replica {\n\t\traw = i.routingTable[i.partID].ReplicaOwners\n\t} else {\n\t\traw = i.routingTable[i.partID].PrimaryOwners\n\t}\n\tvar owners []string\n\t// Make a safe copy of the raw.\n\tfor _, owner := range raw {\n\t\towners = append(owners, owner)\n\t}\n\treturn owners\n}\n\nfunc (i *ClusterIterator) removeScannedOwner(idx int) {\n\tif i.config.Replica {\n\t\tif len(i.route.ReplicaOwners) > 0 && len(i.route.ReplicaOwners) > idx {\n\t\t\ti.route.ReplicaOwners = append(i.route.ReplicaOwners[:idx], i.route.ReplicaOwners[idx+1:]...)\n\t\t}\n\t} else {\n\t\tif len(i.route.PrimaryOwners) > 0 && len(i.route.PrimaryOwners) > idx {\n\t\t\ti.route.PrimaryOwners = append(i.route.PrimaryOwners[:idx], i.route.PrimaryOwners[idx+1:]...)\n\t\t}\n\t}\n}\n\nfunc (i *ClusterIterator) scanOnOwners() error {\n\towners := i.getOwners()\n\n\tfor idx, owner := range owners {\n\t\tcursor := i.loadCursor(owner)\n\n\t\t// Build a scan command here\n\t\ts := protocol.NewScan(i.partID, i.dm.Name(), cursor)\n\t\tif i.config.HasCount {\n\t\t\ts.SetCount(i.config.Count)\n\t\t}\n\t\tif i.config.HasMatch {\n\t\t\ts.SetMatch(i.config.Match)\n\t\t}\n\t\tif i.config.Replica {\n\t\t\ts.SetReplica()\n\t\t}\n\n\t\tscanCmd := s.Command(i.ctx)\n\t\t// Fetch a Redis client for the given owner.\n\t\trc := i.clusterClient.client.Get(owner)\n\t\terr := rc.Process(i.ctx, scanCmd)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tkeys, newCursor, err := scanCmd.Result()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ti.updateIterator(keys, newCursor, owner)\n\t\tif newCursor == 0 {\n\t\t\ti.removeScannedOwner(idx)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (i *ClusterIterator) resetPage() {\n\tif len(i.page) != 0 {\n\t\ti.page = []string{}\n\t}\n\ti.pos = 0\n}\n\nfunc (i *ClusterIterator) fetchData() error {\n\ti.config.Replica = false\n\tif err := i.scanner(); err != nil {\n\t\treturn err\n\t}\n\n\ti.config.Replica = true\n\treturn i.scanner()\n}\n\nfunc (i *ClusterIterator) reset() {\n\ti.partitionKeys = make(map[string]struct{})\n\ti.resetPage()\n\ti.loadRoute()\n}\n\nfunc (i *ClusterIterator) next() bool {\n\tif len(i.page) != 0 {\n\t\ti.pos++\n\t\tif i.pos <= len(i.page) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\ti.resetPage()\n\n\tfor {\n\t\tif err := i.fetchData(); err != nil {\n\t\t\ti.logger.Printf(\"[ERROR] Failed to fetch data: %s\", err)\n\t\t\treturn false\n\t\t}\n\t\tif len(i.page) != 0 {\n\t\t\t// We have data on the page to read. Stop the iteration.\n\t\t\tbreak\n\t\t}\n\n\t\tif len(i.route.PrimaryOwners) == 0 && len(i.route.ReplicaOwners) == 0 {\n\t\t\t// We completed scanning all the owners. Stop the iteration.\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif len(i.page) == 0 && len(i.route.PrimaryOwners) == 0 && len(i.route.ReplicaOwners) == 0 {\n\t\ti.partID++\n\t\tif i.partID >= i.partitionCount {\n\t\t\treturn false\n\t\t}\n\t\ti.reset()\n\t\treturn i.next()\n\t}\n\ti.pos = 1\n\treturn true\n}\n\n// Next returns true if there is more key in the iterator implementation.\n// Otherwise, it returns false\nfunc (i *ClusterIterator) Next() bool {\n\ti.mtx.Lock()\n\tdefer i.mtx.Unlock()\n\n\tselect {\n\tcase <-i.ctx.Done():\n\t\treturn false\n\tdefault:\n\t}\n\n\treturn i.next()\n}\n\n// Key returns a key name from the distributed map.\nfunc (i *ClusterIterator) Key() string {\n\ti.mtx.Lock()\n\tdefer i.mtx.Unlock()\n\n\tvar key string\n\tif i.pos > 0 && i.pos <= len(i.page) {\n\t\tkey = i.page[i.pos-1]\n\t}\n\treturn key\n}\n\nfunc (i *ClusterIterator) fetchRoutingTablePeriodically() {\n\tdefer i.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-i.ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(time.Second):\n\t\t\tif err := i.fetchRoutingTable(); err != nil {\n\t\t\t\ti.logger.Printf(\"[ERROR] Failed to fetch the latest version of the routing table: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (i *ClusterIterator) fetchRoutingTable() error {\n\troutingTable, err := i.clusterClient.RoutingTable(i.ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ti.routingTableMtx.Lock()\n\tdefer i.routingTableMtx.Unlock()\n\n\t// Partition count is a constant, actually. It has to be greater than zero.\n\ti.partitionCount = uint64(len(routingTable))\n\ti.routingTable = routingTable\n\treturn nil\n}\n\n// Close stops the iteration and releases allocated resources.\nfunc (i *ClusterIterator) Close() {\n\tselect {\n\tcase <-i.ctx.Done():\n\t\treturn\n\tdefault:\n\t}\n\n\ti.cancel()\n\n\t// await for routing table updater\n\ti.wg.Wait()\n}\n"
        },
        {
          "name": "cluster_iterator_test.go",
          "type": "blob",
          "size": 2.1962890625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestClusterClient_ScanMatch(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tevenKeys := make(map[string]bool)\n\tfor i := 0; i < 100; i++ {\n\t\tvar key string\n\t\tif i%2 == 0 {\n\t\t\tkey = fmt.Sprintf(\"even:%s\", testutil.ToKey(i))\n\t\t\tevenKeys[key] = false\n\t\t} else {\n\t\t\tkey = fmt.Sprintf(\"odd:%s\", testutil.ToKey(i))\n\t\t}\n\t\terr = dm.Put(ctx, key, i)\n\t\trequire.NoError(t, err)\n\t}\n\ti, err := dm.Scan(ctx, Match(\"^even:\"))\n\trequire.NoError(t, err)\n\tvar count int\n\tdefer i.Close()\n\n\tfor i.Next() {\n\t\tcount++\n\t\trequire.Contains(t, evenKeys, i.Key())\n\t}\n\trequire.Equal(t, 50, count)\n}\n\nfunc TestClusterClient_Scan(t *testing.T) {\n\tcl := newTestOlricCluster(t)\n\tdb := cl.addMember(t)\n\tcl.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tallKeys := make(map[string]bool)\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), i)\n\t\trequire.NoError(t, err)\n\t\tallKeys[testutil.ToKey(i)] = false\n\t}\n\n\ti, err := dm.Scan(ctx)\n\trequire.NoError(t, err)\n\n\tvar count int\n\tdefer i.Close()\n\n\tfor i.Next() {\n\t\tcount++\n\t\trequire.Contains(t, allKeys, i.Key())\n\t}\n\trequire.Equal(t, 100, count)\n}\n"
        },
        {
          "name": "cluster_test.go",
          "type": "blob",
          "size": 1.77734375,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestOlric_ClusterRoutingTable_clusterRoutingTableCommandHandler(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\trtCmd := protocol.NewClusterRoutingTable().Command(db.ctx)\n\trc := db.client.Get(db.rt.This().String())\n\terr := rc.Process(db.ctx, rtCmd)\n\trequire.NoError(t, err)\n\tslice, err := rtCmd.Slice()\n\trequire.NoError(t, err)\n\n\trt, err := mapToRoutingTable(slice)\n\trequire.NoError(t, err)\n\trequire.Len(t, rt, int(db.config.PartitionCount))\n\tfor _, route := range rt {\n\t\trequire.Len(t, route.PrimaryOwners, 1)\n\t\trequire.Equal(t, db.rt.This().String(), route.PrimaryOwners[0])\n\t\trequire.Len(t, route.ReplicaOwners, 0)\n\t}\n}\n\nfunc TestOlric_RoutingTable_Standalone(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\trt, err := db.routingTable(context.Background())\n\trequire.NoError(t, err)\n\trequire.Len(t, rt, int(db.config.PartitionCount))\n\tfor _, route := range rt {\n\t\trequire.Len(t, route.PrimaryOwners, 1)\n\t\trequire.Equal(t, db.rt.This().String(), route.PrimaryOwners[0])\n\t\trequire.Len(t, route.ReplicaOwners, 0)\n\t}\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "cover.sh",
          "type": "blob",
          "size": 0.400390625,
          "content": "#!/bin/bash\n\nTMP=$(mktemp /tmp/olric-coverage-XXXXX.txt)\n\nBUILD=$1\nOUT=$2\n\nset -e\n\n# create coverage output\necho 'mode: atomic' > $OUT\nfor PKG in $(go list ./...| grep -v -E 'vendor'|grep -v -E 'hasher'|grep -v -E 'internal/bufpool'|\ngrep -v -E 'internal/flog'|grep -v -E 'serializer'|grep -v -E 'stats'|grep -v -E 'cmd'); do\n  go test -covermode=atomic -coverprofile=$TMP $PKG\n  tail -n +2 $TMP >> $OUT\ndone\n\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "embedded_client.go",
          "type": "blob",
          "size": 11.83984375,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/discovery\"\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/util\"\n\t\"github.com/buraksezer/olric/stats\"\n)\n\n// EmbeddedLockContext is returned by Lock and LockWithTimeout methods.\n// It should be stored in a proper way to release the lock.\ntype EmbeddedLockContext struct {\n\tkey   string\n\ttoken []byte\n\tdm    *EmbeddedDMap\n}\n\n// Unlock releases the lock.\nfunc (l *EmbeddedLockContext) Unlock(ctx context.Context) error {\n\terr := l.dm.dm.Unlock(ctx, l.key, l.token)\n\treturn convertDMapError(err)\n}\n\n// Lease takes the duration to update the expiry for the given Lock.\nfunc (l *EmbeddedLockContext) Lease(ctx context.Context, duration time.Duration) error {\n\terr := l.dm.dm.Lease(ctx, l.key, l.token, duration)\n\treturn convertDMapError(err)\n}\n\n// EmbeddedClient is an Olric client implementation for embedded-member scenario.\ntype EmbeddedClient struct {\n\tdb *Olric\n}\n\n// EmbeddedDMap is an DMap client implementation for embedded-member scenario.\ntype EmbeddedDMap struct {\n\tmtx           sync.RWMutex\n\tclusterClient *ClusterClient\n\tconfig        *dmapConfig\n\tmember        discovery.Member\n\tdm            *dmap.DMap\n\tclient        *EmbeddedClient\n\tname          string\n}\n\nfunc (dm *EmbeddedDMap) setOrGetClusterClient() (Client, error) {\n\t// Acquire the read lock and try to access the cluster client, if any.\n\tdm.mtx.RLock()\n\tif dm.clusterClient != nil {\n\t\tdm.mtx.RUnlock()\n\t\treturn dm.clusterClient, nil\n\t}\n\tdm.mtx.RUnlock()\n\n\t// The cluster client is unset, try to create a new one.\n\tdm.mtx.Lock()\n\tdefer dm.mtx.Unlock()\n\n\t// Check the existing value last time. There can be another running instances\n\t// of this function.\n\tif dm.clusterClient != nil {\n\t\treturn dm.clusterClient, nil\n\t}\n\n\t// Create a new cluster client here.\n\tc, err := NewClusterClient([]string{dm.client.db.rt.This().String()})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdm.clusterClient = c\n\n\treturn dm.clusterClient, nil\n}\n\n// Pipeline is a mechanism to realise Redis Pipeline technique.\n//\n// Pipelining is a technique to extremely speed up processing by packing\n// operations to batches, send them at once to Redis and read a replies in a\n// singe step.\n// See https://redis.io/topics/pipelining\n//\n// Pay attention, that Pipeline is not a transaction, so you can get unexpected\n// results in case of big pipelines and small read/write timeouts.\n// Redis client has retransmission logic in case of timeouts, pipeline\n// can be retransmitted and commands can be executed more than once.\nfunc (dm *EmbeddedDMap) Pipeline(opts ...PipelineOption) (*DMapPipeline, error) {\n\tcc, err := dm.setOrGetClusterClient()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tclusterDMap, err := cc.NewDMap(dm.name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn clusterDMap.Pipeline(opts...)\n}\n\n// RefreshMetadata fetches a list of available members and the latest routing\n// table version. It also closes stale clients, if there are any. EmbeddedClient has\n// this method to implement the Client interface. It doesn't need to refresh metadata manually.\nfunc (e *EmbeddedClient) RefreshMetadata(_ context.Context) error {\n\t// EmbeddedClient already has the latest metadata.\n\treturn nil\n}\n\n// Scan returns an iterator to loop over the keys.\n//\n// Available scan options:\n//\n// * Count\n// * Match\nfunc (dm *EmbeddedDMap) Scan(ctx context.Context, options ...ScanOption) (Iterator, error) {\n\tcc, err := NewClusterClient([]string{dm.client.db.rt.This().String()})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcdm, err := cc.NewDMap(dm.name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ti, err := cdm.Scan(ctx, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\te := &EmbeddedIterator{\n\t\tclient: dm.client,\n\t\tdm:     dm.dm,\n\t}\n\n\tclusterIterator := i.(*ClusterIterator)\n\tclusterIterator.scanner = e.scanOnOwners\n\te.clusterIterator = clusterIterator\n\treturn e, nil\n}\n\n// Lock sets a lock for the given key. Acquired lock is only for the key in\n// this dmap.\n//\n// It returns immediately if it acquires the lock for the given key. Otherwise,\n// it waits until deadline.\n//\n// You should know that the locks are approximate, and only to be used for\n// non-critical purposes.\nfunc (dm *EmbeddedDMap) Lock(ctx context.Context, key string, deadline time.Duration) (LockContext, error) {\n\ttoken, err := dm.dm.Lock(ctx, key, 0*time.Second, deadline)\n\tif err != nil {\n\t\treturn nil, convertDMapError(err)\n\t}\n\treturn &EmbeddedLockContext{\n\t\tkey:   key,\n\t\ttoken: token,\n\t\tdm:    dm,\n\t}, nil\n}\n\n// LockWithTimeout sets a lock for the given key. If the lock is still unreleased\n// the end of given period of time,\n// it automatically releases the lock. Acquired lock is only for the key in\n// this dmap.\n//\n// It returns immediately if it acquires the lock for the given key. Otherwise,\n// it waits until deadline.\n//\n// You should know that the locks are approximate, and only to be used for\n// non-critical purposes.\nfunc (dm *EmbeddedDMap) LockWithTimeout(ctx context.Context, key string, timeout, deadline time.Duration) (LockContext, error) {\n\ttoken, err := dm.dm.Lock(ctx, key, timeout, deadline)\n\tif err != nil {\n\t\treturn nil, convertDMapError(err)\n\t}\n\treturn &EmbeddedLockContext{\n\t\tkey:   key,\n\t\ttoken: token,\n\t\tdm:    dm,\n\t}, nil\n}\n\n// Destroy flushes the given DMap on the cluster. You should know that there\n// is no global lock on DMaps. So if you call Put/PutEx and Destroy methods\n// concurrently on the cluster, Put call may set new values to the DMap.\nfunc (dm *EmbeddedDMap) Destroy(ctx context.Context) error {\n\treturn dm.dm.Destroy(ctx)\n}\n\n// Expire updates the expiry for the given key. It returns ErrKeyNotFound if\n// the DB does not contain the key. It's thread-safe.\nfunc (dm *EmbeddedDMap) Expire(ctx context.Context, key string, timeout time.Duration) error {\n\treturn dm.dm.Expire(ctx, key, timeout)\n}\n\n// Name exposes name of the DMap.\nfunc (dm *EmbeddedDMap) Name() string {\n\treturn dm.name\n}\n\n// GetPut atomically sets the key to value and returns the old value stored at key. It returns nil if there is no\n// previous value.\nfunc (dm *EmbeddedDMap) GetPut(ctx context.Context, key string, value interface{}) (*GetResponse, error) {\n\te, err := dm.dm.GetPut(ctx, key, value)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &GetResponse{\n\t\tentry: e,\n\t}, nil\n}\n\n// Decr atomically decrements the key by delta. The return value is the new value\n// after being decremented or an error.\nfunc (dm *EmbeddedDMap) Decr(ctx context.Context, key string, delta int) (int, error) {\n\treturn dm.dm.Decr(ctx, key, delta)\n}\n\n// Incr atomically increments the key by delta. The return value is the new value\n// after being incremented or an error.\nfunc (dm *EmbeddedDMap) Incr(ctx context.Context, key string, delta int) (int, error) {\n\treturn dm.dm.Incr(ctx, key, delta)\n}\n\n// IncrByFloat atomically increments the key by delta. The return value is the new value after being incremented or an error.\nfunc (dm *EmbeddedDMap) IncrByFloat(ctx context.Context, key string, delta float64) (float64, error) {\n\treturn dm.dm.IncrByFloat(ctx, key, delta)\n}\n\n// Delete deletes values for the given keys. Delete will not return error\n// if key doesn't exist. It's thread-safe. It is safe to modify the contents\n// of the argument after Delete returns.\nfunc (dm *EmbeddedDMap) Delete(ctx context.Context, keys ...string) (int, error) {\n\treturn dm.dm.Delete(ctx, keys...)\n}\n\n// Get gets the value for the given key. It returns ErrKeyNotFound if the DB\n// does not contain the key. It's thread-safe. It is safe to modify the contents\n// of the returned value. See GetResponse for the details.\nfunc (dm *EmbeddedDMap) Get(ctx context.Context, key string) (*GetResponse, error) {\n\tresult, err := dm.dm.Get(ctx, key)\n\tif err != nil {\n\t\treturn nil, convertDMapError(err)\n\t}\n\n\treturn &GetResponse{\n\t\tentry: result,\n\t}, nil\n}\n\n// Put sets the value for the given key. It overwrites any previous value for\n// that key, and it's thread-safe. The key has to be a string. value type is arbitrary.\n// It is safe to modify the contents of the arguments after Put returns but not before.\nfunc (dm *EmbeddedDMap) Put(ctx context.Context, key string, value interface{}, options ...PutOption) error {\n\tvar pc dmap.PutConfig\n\tfor _, opt := range options {\n\t\topt(&pc)\n\t}\n\terr := dm.dm.Put(ctx, key, value, &pc)\n\tif err != nil {\n\t\treturn convertDMapError(err)\n\t}\n\treturn nil\n}\n\nfunc (e *EmbeddedClient) NewDMap(name string, options ...DMapOption) (DMap, error) {\n\tdm, err := e.db.dmap.NewDMap(name)\n\tif err != nil {\n\t\treturn nil, convertDMapError(err)\n\t}\n\n\tvar dc dmapConfig\n\tfor _, opt := range options {\n\t\topt(&dc)\n\t}\n\n\treturn &EmbeddedDMap{\n\t\tconfig: &dc,\n\t\tdm:     dm,\n\t\tname:   name,\n\t\tclient: e,\n\t\tmember: e.db.rt.This(),\n\t}, nil\n}\n\n// Stats exposes some useful metrics to monitor an Olric node.\nfunc (e *EmbeddedClient) Stats(ctx context.Context, address string, options ...StatsOption) (stats.Stats, error) {\n\tif err := e.db.isOperable(); err != nil {\n\t\t// this node is not bootstrapped yet.\n\t\treturn stats.Stats{}, err\n\t}\n\tvar cfg statsConfig\n\tfor _, opt := range options {\n\t\topt(&cfg)\n\t}\n\n\tif address == e.db.rt.This().String() {\n\t\treturn e.db.stats(cfg), nil\n\t}\n\n\tstatsCmd := protocol.NewStats()\n\tif cfg.CollectRuntime {\n\t\tstatsCmd.SetCollectRuntime()\n\t}\n\tcmd := statsCmd.Command(ctx)\n\trc := e.db.client.Get(address)\n\terr := rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\n\tif err = cmd.Err(); err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\tdata, err := cmd.Bytes()\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\tvar s stats.Stats\n\terr = json.Unmarshal(data, &s)\n\tif err != nil {\n\t\treturn stats.Stats{}, processProtocolError(err)\n\t}\n\treturn s, nil\n}\n\n// Close stops background routines and frees allocated resources.\nfunc (e *EmbeddedClient) Close(_ context.Context) error {\n\treturn nil\n}\n\n// Ping sends a ping message to an Olric node. Returns PONG if message is empty,\n// otherwise return a copy of the message as a bulk. This command is often used to test\n// if a connection is still alive, or to measure latency.\nfunc (e *EmbeddedClient) Ping(ctx context.Context, addr, message string) (string, error) {\n\tresponse, err := e.db.ping(ctx, addr, message)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn util.BytesToString(response), nil\n}\n\n// RoutingTable returns the latest version of the routing table.\nfunc (e *EmbeddedClient) RoutingTable(ctx context.Context) (RoutingTable, error) {\n\treturn e.db.routingTable(ctx)\n}\n\n// Members returns a thread-safe list of cluster members.\nfunc (e *EmbeddedClient) Members(_ context.Context) ([]Member, error) {\n\tmembers := e.db.rt.Discovery().GetMembers()\n\tcoordinator := e.db.rt.Discovery().GetCoordinator()\n\tvar result []Member\n\tfor _, member := range members {\n\t\tm := Member{\n\t\t\tName:      member.Name,\n\t\t\tID:        member.ID,\n\t\t\tBirthdate: member.Birthdate,\n\t\t}\n\t\tif coordinator.ID == member.ID {\n\t\t\tm.Coordinator = true\n\t\t}\n\t\tresult = append(result, m)\n\t}\n\treturn result, nil\n}\n\n// NewPubSub returns a new PubSub client with the given options.\nfunc (e *EmbeddedClient) NewPubSub(options ...PubSubOption) (*PubSub, error) {\n\treturn newPubSub(e.db.client, options...)\n}\n\n// NewEmbeddedClient creates and returns a new EmbeddedClient instance.\nfunc (db *Olric) NewEmbeddedClient() *EmbeddedClient {\n\treturn &EmbeddedClient{db: db}\n}\n\nvar (\n\t_ Client = (*EmbeddedClient)(nil)\n\t_ DMap   = (*EmbeddedDMap)(nil)\n)\n"
        },
        {
          "name": "embedded_client_test.go",
          "type": "blob",
          "size": 14.783203125,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestEmbeddedClient_NewDMap(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\t_, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n}\n\nfunc TestEmbeddedClient_DMap_Put(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(context.Background(), \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n}\n\nfunc TestEmbeddedClient_DMap_Put_EX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", EX(time.Second))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Second)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Put_PX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", PX(time.Millisecond))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Put_EXAT(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", EXAT(time.Duration(time.Now().Add(time.Second).UnixNano())))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Second)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Put_PXAT(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", PXAT(time.Duration(time.Now().Add(time.Millisecond).UnixNano())))\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Put_NX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", NX())\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\t_, err = dm.Get(ctx, \"mykey\")\n\trequire.NoError(t, err)\n}\n\nfunc TestEmbeddedClient_DMap_Put_XX(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\", XX())\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Get(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(context.Background(), \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\tgr, err := dm.Get(context.Background(), \"mykey\")\n\trequire.NoError(t, err)\n\n\tvalue, err := gr.String()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"myvalue\", value)\n}\n\nfunc TestEmbeddedClient_DMap_Delete(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\terr = dm.Put(context.Background(), \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\tcount, err := dm.Delete(context.Background(), \"mykey\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, count)\n\n\t_, err = dm.Get(context.Background(), \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Delete_Many_Keys(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tvar keys []string\n\tfor i := 0; i < 10; i++ {\n\t\tkey := testutil.ToKey(i)\n\t\terr = dm.Put(context.Background(), key, \"myvalue\")\n\t\trequire.NoError(t, err)\n\t\tkeys = append(keys, key)\n\t}\n\n\tcount, err := dm.Delete(context.Background(), keys...)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 10, count)\n}\n\nfunc TestEmbeddedClient_DMap_Atomic_Incr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 100; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.Incr(ctx, \"mykey\", 1)\n\t\t\treturn err\n\t\t})\n\t}\n\trequire.NoError(t, errGr.Wait())\n\n\tgr, err := dm.Get(context.Background(), \"mykey\")\n\tres, err := gr.Int()\n\trequire.NoError(t, err)\n\trequire.Equal(t, 100, res)\n}\n\nfunc TestEmbeddedClient_DMap_Atomic_Decr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\terr = dm.Put(ctx, \"mykey\", 100)\n\trequire.NoError(t, err)\n\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 100; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.Decr(ctx, \"mykey\", 1)\n\t\t\treturn err\n\t\t})\n\t}\n\trequire.NoError(t, errGr.Wait())\n\n\tgr, err := dm.Get(context.Background(), \"mykey\")\n\tres, err := gr.Int()\n\trequire.NoError(t, err)\n\trequire.Equal(t, 0, res)\n}\n\nfunc TestEmbeddedClient_DMap_GetPut(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tgr, err := dm.GetPut(context.Background(), \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\t_, err = gr.String()\n\trequire.ErrorIs(t, err, ErrNilResponse)\n\n\tgr, err = dm.GetPut(context.Background(), \"mykey\", \"myvalue-2\")\n\trequire.NoError(t, err)\n\n\tvalue, err := gr.String()\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"myvalue\", value)\n}\n\nfunc TestEmbeddedClient_DMap_Atomic_IncrByFloat(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tvar errGr errgroup.Group\n\tfor i := 0; i < 100; i++ {\n\t\terrGr.Go(func() error {\n\t\t\t_, err = dm.IncrByFloat(ctx, \"mykey\", 1.2)\n\t\t\treturn err\n\t\t})\n\t}\n\trequire.NoError(t, errGr.Wait())\n\n\tgr, err := dm.Get(context.Background(), \"mykey\")\n\tres, err := gr.Float64()\n\trequire.NoError(t, err)\n\trequire.Equal(t, 120.0000000000002, res)\n}\n\nfunc TestEmbeddedClient_DMap_Expire(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\terr = dm.Put(ctx, \"mykey\", \"myvalue\")\n\trequire.NoError(t, err)\n\n\terr = dm.Expire(ctx, \"mykey\", time.Millisecond)\n\trequire.NoError(t, err)\n\n\t<-time.After(2 * time.Millisecond)\n\n\t_, err = dm.Get(context.Background(), \"mykey\")\n\trequire.ErrorIs(t, err, ErrKeyNotFound)\n}\n\nfunc TestEmbeddedClient_DMap_Destroy(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\terr = dm.Destroy(ctx)\n\trequire.NoError(t, err)\n\n\t// Destroy is an async command. Wait for some time to see its effect.\n\t<-time.After(100 * time.Millisecond)\n\n\tstats, err := e.Stats(ctx, e.db.rt.This().String())\n\trequire.NoError(t, err)\n\tvar total int\n\tfor _, part := range stats.Partitions {\n\t\ttotal += part.Length\n\t}\n\trequire.Greater(t, 100, total)\n}\n\nfunc TestEmbeddedClient_DMap_Lock(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.Lock(ctx, key, time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n}\n\nfunc TestEmbeddedClient_DMap_Lock_ErrLockNotAcquired(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\t_, err = dm.Lock(ctx, key, time.Second)\n\trequire.NoError(t, err)\n\n\t_, err = dm.Lock(ctx, key, time.Millisecond)\n\trequire.ErrorIs(t, err, ErrLockNotAcquired)\n}\n\nfunc TestEmbeddedClient_DMap_Lock_ErrNoSuchLock(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.Lock(ctx, key, time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.LockWithTimeout(ctx, key, 5*time.Second, time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout_Timeout(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.LockWithTimeout(ctx, key, time.Millisecond, time.Second)\n\trequire.NoError(t, err)\n\n\t<-time.After(2 * time.Millisecond)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout_ErrLockNotAcquired(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\t_, err = dm.LockWithTimeout(ctx, key, 10*time.Second, time.Second)\n\trequire.NoError(t, err)\n\n\t_, err = dm.LockWithTimeout(ctx, key, 10*time.Second, time.Millisecond)\n\trequire.ErrorIs(t, err, ErrLockNotAcquired)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout_ErrNoSuchLock(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.LockWithTimeout(ctx, key, time.Second, time.Second)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.NoError(t, err)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout_ErrNoSuchLock_Timeout(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.LockWithTimeout(ctx, key, time.Millisecond, time.Second)\n\trequire.NoError(t, err)\n\n\t<-time.After(time.Millisecond)\n\n\terr = lx.Unlock(ctx)\n\trequire.ErrorIs(t, err, ErrNoSuchLock)\n}\n\nfunc TestEmbeddedClient_DMap_LockWithTimeout_Then_Lease(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tkey := \"lock.key.test\"\n\n\tlx, err := dm.LockWithTimeout(ctx, key, 50*time.Millisecond, time.Second)\n\trequire.NoError(t, err)\n\n\t// Expand its timeout value\n\terr = lx.Lease(ctx, time.Hour)\n\trequire.NoError(t, err)\n\n\t<-time.After(100 * time.Millisecond)\n\n\t_, err = dm.Lock(ctx, key, time.Millisecond)\n\trequire.ErrorIs(t, err, ErrLockNotAcquired)\n}\n\nfunc TestEmbeddedClient_RoutingTable_Standalone(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\trt, err := e.RoutingTable(context.Background())\n\trequire.NoError(t, err)\n\trequire.Len(t, rt, int(db.config.PartitionCount))\n\tfor _, route := range rt {\n\t\trequire.Len(t, route.PrimaryOwners, 1)\n\t\trequire.Equal(t, db.rt.This().String(), route.PrimaryOwners[0])\n\t\trequire.Len(t, route.ReplicaOwners, 0)\n\t}\n}\n\nfunc TestEmbeddedClient_RoutingTable_Cluster(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\n\tcluster.addMember(t) // Cluster coordinator\n\t<-time.After(250 * time.Millisecond)\n\n\tcluster.addMember(t)\n\tdb2 := cluster.addMember(t)\n\n\te := db2.NewEmbeddedClient()\n\trt, err := e.RoutingTable(context.Background())\n\trequire.NoError(t, err)\n\trequire.Len(t, rt, int(db2.config.PartitionCount))\n\towners := make(map[string]struct{})\n\tfor _, route := range rt {\n\t\tfor _, owner := range route.PrimaryOwners {\n\t\t\towners[owner] = struct{}{}\n\t\t}\n\t}\n\trequire.Len(t, owners, 3)\n}\n\nfunc TestEmbeddedClient_Member(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\tcluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tmembers, err := e.Members(context.Background())\n\trequire.NoError(t, err)\n\trequire.Len(t, members, 2)\n\tcoordinator := db.rt.Discovery().GetCoordinator()\n\tfor _, member := range members {\n\t\trequire.NotEqual(t, \"\", member.Name)\n\t\trequire.NotEqual(t, 0, member.ID)\n\t\trequire.NotEqual(t, 0, member.Birthdate)\n\t\tif coordinator.ID == member.ID {\n\t\t\trequire.True(t, member.Coordinator)\n\t\t} else {\n\t\t\trequire.False(t, member.Coordinator)\n\t\t}\n\t}\n}\n\nfunc TestEmbeddedClient_Ping(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tctx := context.Background()\n\tresponse, err := e.Ping(ctx, db.rt.This().String(), \"\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, DefaultPingResponse, response)\n}\n\nfunc TestEmbeddedClient_Ping_WithMessage(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tctx := context.Background()\n\tmessage := \"Olric is the best\"\n\tresponse, err := e.Ping(ctx, db.rt.This().String(), message)\n\trequire.NoError(t, err)\n\trequire.Equal(t, message, response)\n}\n"
        },
        {
          "name": "embedded_iterator.go",
          "type": "blob",
          "size": 2.6455078125,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"sync\"\n\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n)\n\n// EmbeddedIterator implements distributed query on DMaps.\ntype EmbeddedIterator struct {\n\tmtx sync.Mutex\n\n\tclient          *EmbeddedClient\n\tdm              *dmap.DMap\n\tclusterIterator *ClusterIterator\n}\n\nfunc (e *EmbeddedIterator) scanOnOwners() error {\n\towners := e.clusterIterator.getOwners()\n\n\tfor idx, owner := range owners {\n\t\tcursor := e.clusterIterator.loadCursor(owner)\n\n\t\tif e.client.db.rt.This().String() == owner {\n\t\t\tkeys, newCursor, err := e.dm.Scan(e.clusterIterator.partID, cursor, e.clusterIterator.config)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\te.clusterIterator.updateIterator(keys, newCursor, owner)\n\t\t\tif newCursor == 0 {\n\t\t\t\te.clusterIterator.removeScannedOwner(idx)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Build a scan command here\n\t\ts := protocol.NewScan(e.clusterIterator.partID, e.clusterIterator.dm.Name(), cursor)\n\t\tif e.clusterIterator.config.HasCount {\n\t\t\ts.SetCount(e.clusterIterator.config.Count)\n\t\t}\n\t\tif e.clusterIterator.config.HasMatch {\n\t\t\ts.SetMatch(e.clusterIterator.config.Match)\n\t\t}\n\t\tif e.clusterIterator.config.Replica {\n\t\t\ts.SetReplica()\n\t\t}\n\n\t\tscanCmd := s.Command(e.clusterIterator.ctx)\n\t\t// Fetch a Redis client for the given owner.\n\t\trc := e.clusterIterator.clusterClient.client.Get(owner)\n\t\terr := rc.Process(e.clusterIterator.ctx, scanCmd)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tkeys, newCursor, err := scanCmd.Result()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\te.clusterIterator.updateIterator(keys, newCursor, owner)\n\t\tif newCursor == 0 {\n\t\t\te.clusterIterator.removeScannedOwner(idx)\n\t\t}\n\t}\n\treturn nil\n}\n\n// Next returns true if there is more key in the iterator implementation.\n// Otherwise, it returns false.\nfunc (e *EmbeddedIterator) Next() bool {\n\treturn e.clusterIterator.Next()\n}\n\n// Key returns a key name from the distributed map.\nfunc (e *EmbeddedIterator) Key() string {\n\treturn e.clusterIterator.Key()\n}\n\n// Close stops the iteration and releases allocated resources.\nfunc (e *EmbeddedIterator) Close() {\n\te.clusterIterator.Close()\n}\n"
        },
        {
          "name": "embedded_iterator_test.go",
          "type": "blob",
          "size": 2.0087890625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestEmbeddedClient_ScanMatch(t *testing.T) {\n\tcl := newTestOlricCluster(t)\n\tdb := cl.addMember(t)\n\tcl.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\n\tevenKeys := make(map[string]bool)\n\tfor i := 0; i < 100; i++ {\n\t\tvar key string\n\t\tif i%2 == 0 {\n\t\t\tkey = fmt.Sprintf(\"even:%s\", testutil.ToKey(i))\n\t\t\tevenKeys[key] = false\n\t\t} else {\n\t\t\tkey = fmt.Sprintf(\"odd:%s\", testutil.ToKey(i))\n\t\t}\n\t\terr = dm.Put(ctx, key, i)\n\t\trequire.NoError(t, err)\n\t}\n\ti, err := dm.Scan(ctx, Match(\"^even:\"))\n\trequire.NoError(t, err)\n\tvar count int\n\tdefer i.Close()\n\n\tfor i.Next() {\n\t\tcount++\n\t\trequire.Contains(t, evenKeys, i.Key())\n\t}\n\trequire.Equal(t, 50, count)\n}\n\nfunc TestEmbeddedClient_Scan(t *testing.T) {\n\tcl := newTestOlricCluster(t)\n\tdb := cl.addMember(t)\n\tcl.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\tdm, err := e.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\tallKeys := make(map[string]bool)\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), i)\n\t\trequire.NoError(t, err)\n\t\tallKeys[testutil.ToKey(i)] = false\n\t}\n\ti, err := dm.Scan(ctx)\n\trequire.NoError(t, err)\n\tvar count int\n\tdefer i.Close()\n\n\tfor i.Next() {\n\t\tcount++\n\t\trequire.Contains(t, allKeys, i.Key())\n\t}\n\trequire.Equal(t, 100, count)\n}\n"
        },
        {
          "name": "events",
          "type": "tree",
          "content": null
        },
        {
          "name": "get_response.go",
          "type": "blob",
          "size": 3.4072265625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"errors\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/resp\"\n\t\"github.com/buraksezer/olric/pkg/storage\"\n)\n\nvar ErrNilResponse = errors.New(\"storage entry is nil\")\n\ntype GetResponse struct {\n\tentry storage.Entry\n}\n\nfunc (g *GetResponse) Scan(v interface{}) error {\n\tif g.entry == nil {\n\t\treturn ErrNilResponse\n\t}\n\treturn resp.Scan(g.entry.Value(), v)\n}\n\nfunc (g *GetResponse) Int() (int, error) {\n\tv := new(int)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) String() (string, error) {\n\tv := new(string)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Int8() (int8, error) {\n\tv := new(int8)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Int16() (int16, error) {\n\tv := new(int16)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Int32() (int32, error) {\n\tv := new(int32)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Int64() (int64, error) {\n\tv := new(int64)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Uint() (uint, error) {\n\tv := new(uint)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Uint8() (uint8, error) {\n\tv := new(uint8)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Uint16() (uint16, error) {\n\tv := new(uint16)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Uint32() (uint32, error) {\n\tv := new(uint32)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Uint64() (uint64, error) {\n\tv := new(uint64)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Float32() (float32, error) {\n\tv := new(float32)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Float64() (float64, error) {\n\tv := new(float64)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Bool() (bool, error) {\n\tv := new(bool)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Time() (time.Time, error) {\n\tv := new(time.Time)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn time.Time{}, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Duration() (time.Duration, error) {\n\tv := new(time.Duration)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) Byte() ([]byte, error) {\n\tv := new([]byte)\n\terr := g.Scan(v)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn *v, nil\n}\n\nfunc (g *GetResponse) TTL() int64 {\n\treturn g.entry.TTL()\n}\n\nfunc (g *GetResponse) Timestamp() int64 {\n\treturn g.entry.Timestamp()\n}\n"
        },
        {
          "name": "get_response_test.go",
          "type": "blob",
          "size": 9.04296875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/resp\"\n\t\"github.com/buraksezer/olric/internal/testcluster\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDMap_Get_GetResponse(t *testing.T) {\n\tcluster := testcluster.New(dmap.NewService)\n\ts := cluster.AddMember(nil).(*dmap.Service)\n\tdefer cluster.Shutdown()\n\n\tctx := context.Background()\n\tdm, err := s.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tt.Run(\"Scan\", func(t *testing.T) {\n\t\tvar value = 100\n\t\terr = dm.Put(ctx, \"mykey-scan\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-scan\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue := new(int)\n\t\terr = gr.Scan(scannedValue)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, *scannedValue)\n\t})\n\n\tt.Run(\"Byte\", func(t *testing.T) {\n\t\tvar value = []byte(\"olric\")\n\t\terr = dm.Put(ctx, \"mykey-byte\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-byte\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Byte()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"TTL\", func(t *testing.T) {\n\t\tvar value = []byte(\"olric\")\n\t\terr = dm.Put(ctx, \"mykey-byte\", value, &dmap.PutConfig{\n\t\t\tHasEX: true,\n\t\t\tEX:    time.Second,\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-byte\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tttl := gr.TTL()\n\t\trequire.Greater(t, ttl, int64(0))\n\t})\n\n\tt.Run(\"Timestamp\", func(t *testing.T) {\n\t\tvar value = []byte(\"olric\")\n\t\terr = dm.Put(ctx, \"mykey-byte\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-byte\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\ttimestamp := gr.Timestamp()\n\t\trequire.Greater(t, timestamp, int64(0))\n\t})\n\n\tt.Run(\"Int\", func(t *testing.T) {\n\t\tvar value = 100\n\t\terr = dm.Put(ctx, \"mykey-Int\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"String\", func(t *testing.T) {\n\t\tvar value = \"olric\"\n\t\terr = dm.Put(ctx, \"mykey-String\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-String\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.String()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Int8\", func(t *testing.T) {\n\t\tvar value int8 = 10\n\t\terr = dm.Put(ctx, \"mykey-Int8\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int8\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int8()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Int16\", func(t *testing.T) {\n\t\tvar value int16 = 10\n\t\terr = dm.Put(ctx, \"mykey-Int16\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int16\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int16()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Int32\", func(t *testing.T) {\n\t\tvar value int32 = 10\n\t\terr = dm.Put(ctx, \"mykey-Int32\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int32\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int32()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Int64\", func(t *testing.T) {\n\t\tvar value int64 = 10\n\t\terr = dm.Put(ctx, \"mykey-Int64\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int64\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int64()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Int64\", func(t *testing.T) {\n\t\tvar value int64 = 10\n\t\terr = dm.Put(ctx, \"mykey-Int64\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Int64\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Int64()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Uint\", func(t *testing.T) {\n\t\tvar value uint = 10\n\t\terr = dm.Put(ctx, \"mykey-Uint\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Uint\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Uint()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Uint8\", func(t *testing.T) {\n\t\tvar value uint8 = 10\n\t\terr = dm.Put(ctx, \"mykey-Uint8\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Uint8\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Uint8()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Uint16\", func(t *testing.T) {\n\t\tvar value uint16 = 10\n\t\terr = dm.Put(ctx, \"mykey-Uint16\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Uint16\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Uint16()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Uint32\", func(t *testing.T) {\n\t\tvar value uint32 = 10\n\t\terr = dm.Put(ctx, \"mykey-Uint32\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Uint32\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Uint32()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Uint64\", func(t *testing.T) {\n\t\tvar value uint64 = 10\n\t\terr = dm.Put(ctx, \"mykey-Uint64\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Uint64\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Uint64()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Float32\", func(t *testing.T) {\n\t\tvar value float32 = 10.12\n\t\terr = dm.Put(ctx, \"mykey-Float32\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Float32\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Float32()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Float64\", func(t *testing.T) {\n\t\tvar value = 10.12\n\t\terr = dm.Put(ctx, \"mykey-Float64\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Float64\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Float64()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, scannedValue)\n\t})\n\n\tt.Run(\"Bool\", func(t *testing.T) {\n\t\terr = dm.Put(ctx, \"mykey-Bool\", true, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-Bool\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\t\tscannedValue, err := gr.Bool()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, true, scannedValue)\n\t})\n\n\tt.Run(\"time.Time\", func(t *testing.T) {\n\t\tvar value = time.Now()\n\t\terr = dm.Put(ctx, \"mykey-time.Time\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-time.Time\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tenc := resp.New(buf)\n\t\terr = enc.Encode(value)\n\t\trequire.NoError(t, err)\n\n\t\texpectedValue := new(time.Time)\n\t\terr = resp.Scan(buf.Bytes(), expectedValue)\n\t\trequire.NoError(t, err)\n\n\t\tscannedValue, err := gr.Time()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, *expectedValue, scannedValue)\n\t})\n\n\tt.Run(\"time.Duration\", func(t *testing.T) {\n\t\tvar value = time.Second\n\t\terr = dm.Put(ctx, \"mykey-time.Duration\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-time.Duration\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tenc := resp.New(buf)\n\t\terr = enc.Encode(value)\n\t\trequire.NoError(t, err)\n\n\t\texpectedValue := new(time.Duration)\n\t\terr = resp.Scan(buf.Bytes(), expectedValue)\n\t\trequire.NoError(t, err)\n\n\t\tscannedValue, err := gr.Duration()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, *expectedValue, scannedValue)\n\t})\n\n\tt.Run(\"BinaryUnmarshaler\", func(t *testing.T) {\n\t\tvar value = &myType{\n\t\t\tDatabase: \"olric\",\n\t\t}\n\t\terr = dm.Put(ctx, \"mykey-BinaryUnmarshaler\", value, nil)\n\t\trequire.NoError(t, err)\n\n\t\te, err := dm.Get(ctx, \"mykey-BinaryUnmarshaler\")\n\t\trequire.NoError(t, err)\n\n\t\tgr := &GetResponse{entry: e}\n\n\t\tv := myType{}\n\t\terr = gr.Scan(&v)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, &v)\n\t})\n}\n\ntype myType struct {\n\tDatabase string\n}\n\nfunc (mt *myType) MarshalBinary() ([]byte, error) {\n\treturn json.Marshal(mt)\n}\n\nfunc (mt *myType) UnmarshalBinary(data []byte) error {\n\treturn json.Unmarshal(data, &mt)\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.7548828125,
          "content": "module github.com/buraksezer/olric\n\ngo 1.13\n\nrequire (\n\tgithub.com/RoaringBitmap/roaring v1.6.0\n\tgithub.com/buraksezer/consistent v0.10.0\n\tgithub.com/cespare/xxhash/v2 v2.2.0\n\tgithub.com/hashicorp/go-multierror v1.1.1\n\tgithub.com/hashicorp/go-sockaddr v1.0.6\n\tgithub.com/hashicorp/logutils v1.0.0\n\tgithub.com/hashicorp/memberlist v0.5.1\n\tgithub.com/miekg/dns v1.1.45 // indirect\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/redis/go-redis/v9 v9.2.1\n\tgithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529\n\tgithub.com/stretchr/testify v1.8.4\n\tgithub.com/tidwall/btree v1.7.0\n\tgithub.com/tidwall/match v1.1.1\n\tgithub.com/tidwall/redcon v1.6.2\n\tgithub.com/vmihailenco/msgpack/v5 v5.4.1\n\tgolang.org/x/net v0.29.0 // indirect\n\tgolang.org/x/sync v0.8.0\n\tgopkg.in/yaml.v2 v2.4.0\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 22.6650390625,
          "content": "github.com/DataDog/zstd v1.5.2/go.mod h1:g4AWEaM3yOg3HYfnJ3YIawPnVdXJh9QME85blwSAmyw=\ngithub.com/Masterminds/goutils v1.1.1/go.mod h1:8cTjp+g8YejhMuvIA5y2vz3BpJxksy863GQaJW2MFNU=\ngithub.com/Masterminds/semver/v3 v3.1.1/go.mod h1:VPu/7SZ7ePZ3QOrcuXROw5FAcLl4a0cBrbBpGY/8hQs=\ngithub.com/Masterminds/sprig/v3 v3.2.1/go.mod h1:UoaO7Yp8KlPnJIYWTFkMaqPUYKTfGFPhxNuwnnxkKlk=\ngithub.com/RoaringBitmap/roaring v1.6.0 h1:dc7kRiroETgJcHhWX6BerXkZz2b3JgLGg9nTURJL/og=\ngithub.com/RoaringBitmap/roaring v1.6.0/go.mod h1:plvDsJQpxOC5bw8LRteu/MLWHsHez/3y6cubLI4/1yE=\ngithub.com/Sereal/Sereal/Go/sereal v0.0.0-20231009093132-b9187f1a92c6/go.mod h1:JwrycNnC8+sZPDyzM3MQ86LvaGzSpfxg885KOOwFRW4=\ngithub.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da h1:8GUt8eRujhVEGZFFEjBj46YV4rDjvGrNxb0KMWYkL2I=\ngithub.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da/go.mod h1:Q73ZrmVTwzkszR9V5SSuryQ31EELlFMUz1kKyl939pY=\ngithub.com/armon/go-radix v0.0.0-20180808171621-7fddfc383310/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\ngithub.com/bgentry/speakeasy v0.1.0/go.mod h1:+zsyZBPWlz7T6j88CTgSN5bM796AkVf0kBD4zp0CCIs=\ngithub.com/bits-and-blooms/bitset v1.2.0 h1:Kn4yilvwNtMACtf1eYDlG8H77R07mZSPbMjLyS07ChA=\ngithub.com/bits-and-blooms/bitset v1.2.0/go.mod h1:gIdJ4wp64HaoK2YrL1Q5/N7Y16edYb8uY+O0FJTyyDA=\ngithub.com/bsm/ginkgo/v2 v2.12.0 h1:Ny8MWAHyOepLGlLKYmXG4IEkioBysk6GpaRTLC8zwWs=\ngithub.com/bsm/ginkgo/v2 v2.12.0/go.mod h1:SwYbGRRDovPVboqFv0tPTcG1sN61LM1Z4ARdbAV9g4c=\ngithub.com/bsm/gomega v1.27.10 h1:yeMWxP2pV2fG3FgAODIY8EiRE3dy0aeFYt4l7wh6yKA=\ngithub.com/bsm/gomega v1.27.10/go.mod h1:JyEr/xRbxbtgWNi8tIEVPUYZ5Dzef52k01W3YH0H+O0=\ngithub.com/buraksezer/consistent v0.10.0 h1:hqBgz1PvNLC5rkWcEBVAL9dFMBWz6I0VgUCW25rrZlU=\ngithub.com/buraksezer/consistent v0.10.0/go.mod h1:6BrVajWq7wbKZlTOUPs/XVfR8c0maujuPowduSpZqmw=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-xdr v0.0.0-20161123171359-e6a2ba005892/go.mod h1:CTDl0pzVzE5DEzZhPfvhY/9sPFMQIxaJ9VAMs9AagrE=\ngithub.com/dchest/siphash v1.2.3/go.mod h1:0NvQU092bT0ipiFN++/rXm69QG9tVxLAlQHIXMPAkHc=\ngithub.com/dgryski/go-ddmin v0.0.0-20210904190556-96a6d69f1034/go.mod h1:zz4KxBkcXUWKjIcrc+uphJ1gPh/t18ymGm3PmQ+VGTk=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\ngithub.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c h1:964Od4U6p2jUkFxvCydnIczKteheJEzHRToSGK3Bnlw=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-immutable-radix v1.0.0 h1:AKDB1HM5PWEA7i4nhcpwOrO2byshxBjXVn/J/3+z5/0=\ngithub.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-msgpack/v2 v2.1.1 h1:xQEY9yB2wnHitoSzk/B9UjXWRQ67QKu5AOm8aFp8N3I=\ngithub.com/hashicorp/go-msgpack/v2 v2.1.1/go.mod h1:upybraOAblm4S7rx0+jeNy+CWWhzywQsSRV5033mMu4=\ngithub.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/go-sockaddr v1.0.0/go.mod h1:7Xibr9yA9JjQq1JpNB2Vw7kxv8xerXegt+ozgdvDeDU=\ngithub.com/hashicorp/go-sockaddr v1.0.6 h1:RSG8rKU28VTUTvEKghe5gIhIQpv8evvNpnDEyqO4u9I=\ngithub.com/hashicorp/go-sockaddr v1.0.6/go.mod h1:uoUUmtwU7n9Dv3O4SNLeFvg0SxQ3lyjsj6+CCykpaxI=\ngithub.com/hashicorp/go-uuid v1.0.0 h1:RS8zrF7PhGwyNPOtxSClXXj9HA8feRnJzgnI1RJCSnM=\ngithub.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/golang-lru v0.5.0 h1:CL2msUPvZTLb5O648aiLNJw3hnBxN2+1Jq8rCOH9wdo=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/logutils v1.0.0 h1:dLEQVugN8vlakKOUE3ihGLTZJRB4j+M2cdTm/ORI65Y=\ngithub.com/hashicorp/logutils v1.0.0/go.mod h1:QIAnNjmIWmVIIkWDTG1z5v++HQmx9WQRO+LraFDTW64=\ngithub.com/hashicorp/memberlist v0.5.1 h1:mk5dRuzeDNis2bi6LLoQIXfMH7JQvAzt3mQD0vNZZUo=\ngithub.com/hashicorp/memberlist v0.5.1/go.mod h1:zGDXV6AqbDTKTM6yxW0I4+JtFzZAJVoIPvss4hV8F24=\ngithub.com/huandu/xstrings v1.3.1/go.mod h1:y5/lhBue+AyNmUVz9RLU9xbLR0o4KIIExikq4ovT0aE=\ngithub.com/huandu/xstrings v1.3.2/go.mod h1:y5/lhBue+AyNmUVz9RLU9xbLR0o4KIIExikq4ovT0aE=\ngithub.com/imdario/mergo v0.3.11/go.mod h1:jmQim1M+e3UYxmgPu/WyfjB3N3VflVyUjjjwH0dnCYA=\ngithub.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/kr/pretty v0.2.1 h1:Fmg33tUaq4/8ym9TJN1x7sLJnHVwhP33CNkpYV/7rwI=\ngithub.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=\ngithub.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\ngithub.com/mattn/go-isatty v0.0.3/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\ngithub.com/miekg/dns v1.1.26/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=\ngithub.com/miekg/dns v1.1.45 h1:g5fRIhm9nx7g8osrAvgb16QJfmyMsyOCb+J7LSv+Qzk=\ngithub.com/miekg/dns v1.1.45/go.mod h1:e3IlAVfNqAllflbibAZEWOXOQ+Ynzk/dDozDxY7XnME=\ngithub.com/mitchellh/cli v1.1.5/go.mod h1:v8+iFts2sPIKUV1ltktPXMCC8fumSKFItNcD2cLtRR4=\ngithub.com/mitchellh/copystructure v1.0.0/go.mod h1:SNtv71yrdKgLRyLFxmLdkAbkKEFWgYaq1OVrnRcwhnw=\ngithub.com/mitchellh/go-wordwrap v1.0.1/go.mod h1:R62XHJLzvMFRBbcrT7m7WgmE1eOyTSsCt+hzestvNj0=\ngithub.com/mitchellh/reflectwalk v1.0.0/go.mod h1:mSTlrgnPZtwu0c4WaC2kGObEpuNDbx0jmZXqmk4esnw=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/mschoch/smat v0.2.0 h1:8imxQsjDm8yFEAVBe7azKmKSgzSkZXDuKkSq9374khM=\ngithub.com/mschoch/smat v0.2.0/go.mod h1:kc9mz7DoBKqDyiRL7VZN8KvXQMWeTaVnttLRXOlotKw=\ngithub.com/pascaldekloe/goe v0.0.0-20180627143212-57f6aae5913c h1:Lgl0gzECD8GnQ5QCWA8o6BtfL6mDH5rQgM4/fX3avOs=\ngithub.com/pascaldekloe/goe v0.0.0-20180627143212-57f6aae5913c/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/philhofer/fwd v1.1.2/go.mod h1:qkPdfjR2SIEbspLqpe1tO4n5yICnr2DY7mqEx2tUTP0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/posener/complete v1.1.1/go.mod h1:em0nMJCgc9GFtwrmVmEMR/ZL6WyhyjMBndrE9hABlRI=\ngithub.com/pquerna/ffjson v0.0.0-20190930134022-aa0246cd15f7/go.mod h1:YARuvh7BUWHNhzDq2OM5tzR2RiCcN2D7sapiKyCel/M=\ngithub.com/redis/go-redis/v9 v9.2.1 h1:WlYJg71ODF0dVspZZCpYmoF1+U1Jjk9Rwd7pq6QmlCg=\ngithub.com/redis/go-redis/v9 v9.2.1/go.mod h1:hdY0cQFCN4fnSYT6TkisLufl/4W5UIXyv0b/CLO2V2M=\ngithub.com/ryanuber/columnize v2.1.2+incompatible/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529 h1:nn5Wsu0esKSJiIVhscUtVbo7ada43DJhG55ua/hjS5I=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529/go.mod h1:DxrIzT+xaE7yg65j358z/aeFdxmN0P9QXhEzd20vsDc=\ngithub.com/shopspring/decimal v1.2.0/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=\ngithub.com/spf13/cast v1.3.1/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.2/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/stretchr/testify v1.8.4 h1:CcVxjf3Q8PM0mHUKJCdn+eZZtm5yQwehR5yeSVQQcUk=\ngithub.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=\ngithub.com/tidwall/btree v1.1.0/go.mod h1:TzIRzen6yHbibdSfK6t8QimqbUnoxUSrZfeW7Uob0q4=\ngithub.com/tidwall/btree v1.7.0 h1:L1fkJH/AuEh5zBnnBbmTwQ5Lt+bRJ5A8EWecslvo9iI=\ngithub.com/tidwall/btree v1.7.0/go.mod h1:twD9XRA5jj9VUQGELzDO4HPQTNJsoWWfYEL+EUQ2cKY=\ngithub.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=\ngithub.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\ngithub.com/tidwall/redcon v1.6.2 h1:5qfvrrybgtO85jnhSravmkZyC0D+7WstbfCs3MmPhow=\ngithub.com/tidwall/redcon v1.6.2/go.mod h1:p5Wbsgeyi2VSTBWOcA5vRXrOb9arFTcU2+ZzFjqV75Y=\ngithub.com/tinylib/msgp v1.1.8/go.mod h1:qkpG+2ldGg4xRFmx+jfTvZPxfGFhi64BcnL9vkCm/Tw=\ngithub.com/vmihailenco/msgpack/v5 v5.4.1 h1:cQriyiUvjTwOHg8QZaPihLWeRAAVoCpE00IUPn0Bjt8=\ngithub.com/vmihailenco/msgpack/v5 v5.4.1/go.mod h1:GaZTsDaehaPpQVyxrf5mtQlH+pc21PIudVV/E3rRQok=\ngithub.com/vmihailenco/tagparser/v2 v2.0.0 h1:y09buUbR+b5aycVFQs/g70pqKVZNBmxwAhO7/IwNM9g=\ngithub.com/vmihailenco/tagparser/v2 v2.0.0/go.mod h1:Wri+At7QHww0WTrCBeu4J6bNtoV6mEfg5OIWRZA9qds=\ngithub.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB+9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200414173820-0848c9571904/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20200820211705-5c72a883971a/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.13.0/go.mod h1:y6Z2r+Rw4iayiXXAIxJIDAJ1zMW4yaTpebo8fPOliYc=\ngolang.org/x/crypto v0.14.0/go.mod h1:MVFd36DqK4CsrnJYDkBA3VC4m2GkXAM0PvzMCn4JQf4=\ngolang.org/x/crypto v0.19.0/go.mod h1:Iy9bg/ha4yyC70EfRS8jz+B6ybOBKMaSxLj6P6oBDfU=\ngolang.org/x/crypto v0.23.0/go.mod h1:CKFgDieR+mRhux2Lsu27y0fO304Db0wZe70UKqHu0v8=\ngolang.org/x/crypto v0.27.0/go.mod h1:1Xngt8kV6Dvbssa53Ziq6Eqn0HqbZi5Z6R0ZpwQzt70=\ngolang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/mod v0.7.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\ngolang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\ngolang.org/x/mod v0.12.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\ngolang.org/x/mod v0.13.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/mod v0.15.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/mod v0.17.0 h1:zY54UmvipHiNd+pm+m0x9KhZ9hl1/7QNMyxXbc6ICqA=\ngolang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\ngolang.org/x/net v0.0.0-20210726213435-c6fcb2dbf985/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.3.0/go.mod h1:MBQ8lrhLObU/6UmLb4fmbmk5OcyYmqtbGd/9yIeKjEE=\ngolang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=\ngolang.org/x/net v0.15.0/go.mod h1:idbUs1IY1+zTqbi8yxTbhexhEEk5ur9LInksu6HrEpk=\ngolang.org/x/net v0.16.0/go.mod h1:NxSsAGuq816PNPmqtQdLE42eU2Fs7NoRIZrHJAlaCOE=\ngolang.org/x/net v0.21.0/go.mod h1:bIjVDfnllIU7BJ2DNgfnXvpSvtn8VRwhlsaeUTyUS44=\ngolang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=\ngolang.org/x/net v0.29.0 h1:5ORfpBpCs4HzDYoodCDBbwHzdR5UrLBZ3sOnUJmFoHo=\ngolang.org/x/net v0.29.0/go.mod h1:gLkgy8jTGERgjzMic6DS9+SP0ajcu6Xu3Orq/SpETg0=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sync v0.4.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sync v0.6.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sync v0.8.0 h1:3NFvSEYkUoMifnESzZl15y791HH1qU2xm6eCJU5ZPXQ=\ngolang.org/x/sync v0.8.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.3.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/sys v0.20.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/sys v0.25.0 h1:r+8e+loiHxRqhXVl6ML1nO3l1+oFoWbnlu2Ehimmi34=\ngolang.org/x/sys v0.25.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/telemetry v0.0.0-20240228155512-f48c80bd79b2/go.mod h1:TeRTkGYfJXctD9OcfyVLyj2J3IxLnKwHJR8f4D8a3YE=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.3.0/go.mod h1:q750SLmJuPmVoN1blW3UFBPREJfb1KmY3vwxfr+nFDA=\ngolang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\ngolang.org/x/term v0.8.0/go.mod h1:xPskH00ivmX89bAKVGSKKtLOWNx2+17Eiy94tnKShWo=\ngolang.org/x/term v0.12.0/go.mod h1:owVbMEjm3cBLCHdkQu9b1opXd4ETQWc3BhuQGKgXgvU=\ngolang.org/x/term v0.13.0/go.mod h1:LTmsnFJwVN6bCy1rVCoS+qHT1HhALEFxKncY3WNNh4U=\ngolang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=\ngolang.org/x/term v0.20.0/go.mod h1:8UkIAJTvZgivsXaD6/pH6U9ecQzZ45awqEOzuCvwpFY=\ngolang.org/x/term v0.24.0/go.mod h1:lOBK/LVxemqiMij05LGJ0tzNr8xlmwBRJ81PX6wVLH8=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.5.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\ngolang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\ngolang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\ngolang.org/x/text v0.15.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\ngolang.org/x/text v0.18.0/go.mod h1:BuEKDfySbSR4drPmRPG/7iBdf8hvFMuRexcpahXilzY=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.1.6-0.20210726203631-07bc1bf47fb2/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/tools v0.4.0/go.mod h1:UE5sM2OK9E/d67R0ANs2xJizIymRP5gJU295PvKXxjQ=\ngolang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=\ngolang.org/x/tools v0.13.0/go.mod h1:HvlwmtVNQAhOuCjW7xxvovg8wbNq7LwfXh/k7wXUl58=\ngolang.org/x/tools v0.14.0/go.mod h1:uYBEerGOWcJyEORxN+Ek8+TT266gXkNlHdJBwexUsBg=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.28.1/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/mgo.v2 v2.0.0-20190816093944-a6b53ec6cb22/go.mod h1:yeKp02qBN3iKW1OzL3MGk2IdtZzaj7SFntXj72NppTA=\ngopkg.in/vmihailenco/msgpack.v2 v2.9.2/go.mod h1:/3Dn1Npt9+MYyLpYYXjInO/5jvMLamn+AEGwNEOatn8=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "hasher",
          "type": "tree",
          "content": null
        },
        {
          "name": "integration_test.go",
          "type": "blob",
          "size": 13.447265625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/config\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestIntegration_NodesJoinOrLeftDuringQuery(t *testing.T) {\n\t// TODO: https://github.com/buraksezer/olric/issues/227\n\tt.Skip(\"TestIntegration_NodesJoinOrLeftDuringQuery: flaky test\")\n\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 2\n\t\tc.WriteQuorum = 1\n\t\tc.ReadRepair = true\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\tdb2 := cluster.addMemberWithConfig(t, newConfig())\n\n\tt.Log(\"Wait for 1 second before inserting keys\")\n\t<-time.After(time.Second)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < 100000; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t\tif i == 5999 {\n\t\t\tgo cluster.addMemberWithConfig(t, newConfig())\n\t\t}\n\t}\n\n\tgo cluster.addMemberWithConfig(t, newConfig())\n\n\tt.Log(\"Fetch all keys\")\n\n\tfor i := 0; i < 100000; i++ {\n\t\t_, err = dm.Get(context.Background(), fmt.Sprintf(\"mykey-%d\", i))\n\t\tif errors.Is(err, ErrConnRefused) {\n\t\t\t// Rewind\n\t\t\ti--\n\t\t\trequire.NoError(t, c.RefreshMetadata(context.Background()))\n\t\t\tcontinue\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tif i == 5999 {\n\t\t\terr = c.client.Close(db2.name)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tt.Logf(\"Shutdown one of the nodes: %s\", db2.name)\n\t\t\trequire.NoError(t, db2.Shutdown(ctx))\n\n\t\t\tgo cluster.addMemberWithConfig(t, newConfig())\n\n\t\t\tt.Log(\"Wait for \\\"NodeLeave\\\" event propagation\")\n\t\t\t<-time.After(time.Second)\n\t\t}\n\t}\n\n\tfor i := 0; i < 100000; i++ {\n\t\t_, err = dm.Get(context.Background(), fmt.Sprintf(\"mykey-%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc TestIntegration_DMap_Cache_Eviction_LRU_MaxKeys(t *testing.T) {\n\tmaxKeys := 100000\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 1\n\t\tc.WriteQuorum = 1\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.DMaps.MaxKeys = maxKeys\n\t\tc.DMaps.EvictionPolicy = config.LRUEviction\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\tvar total int\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\", NX())\n\t\tif err == ErrKeyFound {\n\t\t\terr = nil\n\t\t} else {\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Greater(t, total, 0)\n\tt.Logf(\"number of misses: %d, utilization rate: %f\", total, float64(100)-(float64(total*100))/float64(maxKeys))\n}\n\nfunc TestIntegration_DMap_Cache_Eviction_MaxKeys(t *testing.T) {\n\tmaxKeys := 100000\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 1\n\t\tc.WriteQuorum = 1\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.DMaps.MaxKeys = maxKeys\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\tvar total int\n\tfor i := maxKeys; i < 2*maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\", NX())\n\t\tif err == ErrKeyFound {\n\t\t\terr = nil\n\t\t} else {\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\t_, err = dm.Get(ctx, fmt.Sprintf(\"mykey-%d\", i))\n\t\tif err == ErrKeyNotFound {\n\t\t\terr = nil\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Equal(t, maxKeys, total)\n}\n\nfunc TestIntegration_DMap_Cache_Eviction_MaxIdleDuration(t *testing.T) {\n\tmaxKeys := 100000\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 1\n\t\tc.WriteQuorum = 1\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.DMaps.MaxIdleDuration = 100 * time.Millisecond\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\t<-time.After(250 * time.Millisecond)\n\n\tvar total int\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\t_, err = dm.Get(ctx, fmt.Sprintf(\"mykey-%d\", i))\n\t\tif err == ErrKeyNotFound {\n\t\t\terr = nil\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Greater(t, total, 0)\n}\n\nfunc TestIntegration_DMap_Cache_Eviction_TTLDuration(t *testing.T) {\n\tmaxKeys := 100000\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 1\n\t\tc.WriteQuorum = 1\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.DMaps.TTLDuration = 100 * time.Millisecond\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\t<-time.After(250 * time.Millisecond)\n\n\tvar total int\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\t_, err := dm.Get(ctx, fmt.Sprintf(\"mykey-%d\", i))\n\t\tif err == ErrKeyNotFound {\n\t\t\terr = nil\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Equal(t, maxKeys, total)\n}\n\nfunc TestIntegration_DMap_Cache_Eviction_LRU_MaxInuse(t *testing.T) {\n\tmaxKeys := 100000\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 1\n\t\tc.WriteQuorum = 1\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.DMaps.MaxInuse = 100 // bytes\n\t\tc.DMaps.EvictionPolicy = \"LRU\"\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\t<-time.After(250 * time.Millisecond)\n\n\tvar total int\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\t_, err = dm.Get(ctx, fmt.Sprintf(\"mykey-%d\", i))\n\t\tif err == ErrKeyNotFound {\n\t\t\terr = nil\n\t\t\ttotal++\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Greater(t, total, 0)\n}\n\nfunc TestIntegration_Kill_Nodes_During_Operation(t *testing.T) {\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 3\n\t\tc.WriteQuorum = 1\n\t\tc.ReadRepair = true\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\n\tcluster.addMemberWithConfig(t, newConfig())\n\tdb3 := cluster.addMemberWithConfig(t, newConfig())\n\tcluster.addMemberWithConfig(t, newConfig())\n\tdb5 := cluster.addMemberWithConfig(t, newConfig())\n\n\tt.Log(\"Wait for 1 second before inserting keys\")\n\t<-time.After(time.Second)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\trequire.NoError(t, err)\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tt.Log(\"Insert keys\")\n\n\tfor i := 0; i < 100000; i++ {\n\t\terr = dm.Put(ctx, fmt.Sprintf(\"mykey-%d\", i), \"myvalue\")\n\t\trequire.NoError(t, err)\n\t}\n\n\tt.Log(\"Fetch all keys\")\n\n\tfor i := 0; i < 100000; i++ {\n\t\t_, err = dm.Get(ctx, fmt.Sprintf(\"mykey-%d\", i))\n\t\tif err == ErrKeyNotFound {\n\t\t\terr = nil\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n\n\tt.Logf(\"Terminate %s\", db3.rt.This())\n\trequire.NoError(t, db3.Shutdown(context.Background()))\n\n\tt.Logf(\"Terminate %s\", db5.rt.This())\n\trequire.NoError(t, db5.Shutdown(context.Background()))\n\n\tt.Log(\"Wait for \\\"NodeLeave\\\" event propagation\")\n\t<-time.After(time.Second)\n\n\tfor i := 0; i < 100000; i++ {\n\t\t_, err = dm.Get(context.Background(), fmt.Sprintf(\"mykey-%d\", i))\n\t\tif errors.Is(err, ErrConnRefused) {\n\t\t\ti--\n\t\t\tfmt.Println(c.RefreshMetadata(context.Background()))\n\t\t\tcontinue\n\t\t}\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc scanIntegrationTestCommon(t *testing.T, embedded bool, keyFunc func(i int) string, options ...ScanOption) []map[string]struct{} {\n\tnewConfig := func() *config.Config {\n\t\tc := config.New(\"local\")\n\t\tc.PartitionCount = config.DefaultPartitionCount\n\t\tc.ReplicaCount = 2\n\t\tc.WriteQuorum = 1\n\t\tc.ReadRepair = false\n\t\tc.ReadQuorum = 1\n\t\tc.LogOutput = io.Discard\n\t\tc.TriggerBalancerInterval = time.Millisecond\n\t\trequire.NoError(t, c.Sanitize())\n\t\trequire.NoError(t, c.Validate())\n\t\treturn c\n\t}\n\n\tcluster := newTestOlricCluster(t)\n\n\tdb := cluster.addMemberWithConfig(t, newConfig())\n\tdb2 := cluster.addMemberWithConfig(t, newConfig())\n\t_ = cluster.addMemberWithConfig(t, newConfig())\n\n\tt.Log(\"Wait for 1 second before inserting keys\")\n\t<-time.After(time.Second)\n\n\tctx := context.Background()\n\tvar c Client\n\tvar err error\n\n\tif embedded {\n\t\tc = db.NewEmbeddedClient()\n\t} else {\n\t\tc, err = NewClusterClient([]string{db.name})\n\t\trequire.NoError(t, err)\n\t}\n\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tpassOne := make(map[string]struct{})\n\tpassTwo := make(map[string]struct{})\n\tfor i := 0; i < 10000; i++ {\n\t\tkey := keyFunc(i)\n\t\terr = dm.Put(ctx, key, \"myvalue\")\n\t\trequire.NoError(t, err)\n\t\tpassOne[key] = struct{}{}\n\t\tpassTwo[key] = struct{}{}\n\t}\n\n\tt.Logf(\"Shutdown one of the nodes: %s\", db2.name)\n\trequire.NoError(t, db2.Shutdown(ctx))\n\n\tt.Log(\"Wait for \\\"NodeLeave\\\" event propagation\")\n\t<-time.After(time.Second)\n\n\tt.Log(\"First pass\")\n\n\ts, err := dm.Scan(context.Background(), options...)\n\trequire.NoError(t, err)\n\tfor s.Next() {\n\t\tdelete(passOne, s.Key())\n\t}\n\ts.Close()\n\n\tdb3 := cluster.addMemberWithConfig(t, newConfig())\n\tt.Logf(\"Add a new member: %s\", db3.rt.This())\n\n\t<-time.After(time.Second)\n\n\tt.Log(\"Second pass\")\n\ts, err = dm.Scan(context.Background(), options...)\n\trequire.NoError(t, err)\n\n\tfor s.Next() {\n\t\tdelete(passTwo, s.Key())\n\t}\n\n\treturn []map[string]struct{}{passOne, passTwo}\n}\n\nfunc TestIntegration_Network_Partitioning_Cluster_DM_SCAN(t *testing.T) {\n\tkeyGenerator := func(i int) string {\n\t\treturn fmt.Sprintf(\"mykey-%d\", i)\n\t}\n\tresult := scanIntegrationTestCommon(t, false, keyGenerator)\n\tpassOne, passTwo := result[0], result[1]\n\trequire.Empty(t, passOne)\n\trequire.Empty(t, passTwo)\n}\n\nfunc TestIntegration_Network_Partitioning_Cluster_DM_SCAN_Match(t *testing.T) {\n\tvar oddNumbers int\n\tkeyGenerator := func(i int) string {\n\t\tif i%2 == 0 {\n\t\t\treturn fmt.Sprintf(\"even:%d\", i)\n\t\t}\n\t\toddNumbers++\n\t\treturn fmt.Sprintf(\"odd:%d\", i)\n\t}\n\tresult := scanIntegrationTestCommon(t, false, keyGenerator, Match(\"^even:\"))\n\tpassOne, passTwo := result[0], result[1]\n\trequire.Len(t, passOne, oddNumbers)\n\trequire.Len(t, passTwo, oddNumbers)\n}\n\nfunc TestIntegration_Network_Partitioning_Embedded_DM_SCAN(t *testing.T) {\n\tkeyGenerator := func(i int) string {\n\t\treturn fmt.Sprintf(\"mykey-%d\", i)\n\t}\n\tresult := scanIntegrationTestCommon(t, true, keyGenerator)\n\tpassOne, passTwo := result[0], result[1]\n\trequire.Empty(t, passOne)\n\trequire.Empty(t, passTwo)\n}\n\nfunc TestIntegration_Network_Partitioning_Embedded_DM_SCAN_Match(t *testing.T) {\n\tvar oddNumbers int\n\tkeyGenerator := func(i int) string {\n\t\tif i%2 == 0 {\n\t\t\treturn fmt.Sprintf(\"even:%d\", i)\n\t\t}\n\t\toddNumbers++\n\t\treturn fmt.Sprintf(\"odd:%d\", i)\n\t}\n\tresult := scanIntegrationTestCommon(t, true, keyGenerator, Match(\"^even:\"))\n\tpassOne, passTwo := result[0], result[1]\n\trequire.Len(t, passOne, oddNumbers)\n\trequire.Len(t, passTwo, oddNumbers)\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "olric.go",
          "type": "blob",
          "size": 13.3193359375,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/*\nPackage olric provides a distributed cache and in-memory key/value data store.\nIt can be used both as an embedded Go library and as a language-independent\nservice.\n\nWith Olric, you can instantly create a fast, scalable, shared pool of RAM across\na cluster of computers.\n\nOlric is designed to be a distributed cache. But it also provides Publish/Subscribe,\ndata replication, failure detection and simple anti-entropy services.\nSo it can be used as an ordinary key/value data store to scale your cloud\napplication.\n*/\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/config\"\n\t\"github.com/buraksezer/olric/hasher\"\n\t\"github.com/buraksezer/olric/internal/checkpoint\"\n\t\"github.com/buraksezer/olric/internal/cluster/balancer\"\n\t\"github.com/buraksezer/olric/internal/cluster/partitions\"\n\t\"github.com/buraksezer/olric/internal/cluster/routingtable\"\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/environment\"\n\t\"github.com/buraksezer/olric/internal/locker\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/pubsub\"\n\t\"github.com/buraksezer/olric/internal/server\"\n\t\"github.com/buraksezer/olric/pkg/flog\"\n\t\"github.com/hashicorp/logutils\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/tidwall/redcon\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// ReleaseVersion is the current stable version of Olric\nconst ReleaseVersion string = \"0.6.0-alpha.1\"\n\nvar (\n\t// ErrOperationTimeout is returned when an operation times out.\n\tErrOperationTimeout = errors.New(\"operation timeout\")\n\n\t// ErrServerGone means that a cluster member is closed unexpectedly.\n\tErrServerGone = errors.New(\"server is gone\")\n\n\t// ErrKeyNotFound means that returned when a key could not be found.\n\tErrKeyNotFound = errors.New(\"key not found\")\n\n\t// ErrKeyFound means that the requested key found in the cluster.\n\tErrKeyFound = errors.New(\"key found\")\n\n\t// ErrWriteQuorum means that write quorum cannot be reached to operate.\n\tErrWriteQuorum = errors.New(\"write quorum cannot be reached\")\n\n\t// ErrReadQuorum means that read quorum cannot be reached to operate.\n\tErrReadQuorum = errors.New(\"read quorum cannot be reached\")\n\n\t// ErrLockNotAcquired is returned when the requested lock could not be acquired\n\tErrLockNotAcquired = errors.New(\"lock not acquired\")\n\n\t// ErrNoSuchLock is returned when the requested lock does not exist\n\tErrNoSuchLock = errors.New(\"no such lock\")\n\n\t// ErrClusterQuorum means that the cluster could not reach a healthy numbers of members to operate.\n\tErrClusterQuorum = errors.New(\"failed to find enough peers to create quorum\")\n\n\t// ErrKeyTooLarge means that the given key is too large to process.\n\t// Maximum length of a key is 256 bytes.\n\tErrKeyTooLarge = errors.New(\"key too large\")\n\n\t// ErrEntryTooLarge returned if the required space for an entry is bigger than table size.\n\tErrEntryTooLarge = errors.New(\"entry too large for the configured table size\")\n\n\t// ErrConnRefused returned if the target node refused a connection request.\n\t// It is good to call RefreshMetadata to update the underlying data structures.\n\tErrConnRefused = errors.New(\"connection refused\")\n)\n\n// Olric implements a distributed cache and in-memory key/value data store.\n// It can be used both as an embedded Go library and as a language-independent\n// service.\ntype Olric struct {\n\t// name is BindAddr:BindPort. It defines servers unique name in the cluster.\n\tname     string\n\tenv      *environment.Environment\n\tconfig   *config.Config\n\tlog      *flog.Logger\n\thashFunc hasher.Hasher\n\n\t// Logical units to store data\n\tprimary *partitions.Partitions\n\tbackup  *partitions.Partitions\n\n\t// RESP server and clients.\n\tserver *server.Server\n\tclient *server.Client\n\n\trt       *routingtable.RoutingTable\n\tbalancer *balancer.Balancer\n\n\tpubsub *pubsub.Service\n\tdmap   *dmap.Service\n\n\t// Structures for flow control\n\tctx    context.Context\n\tcancel context.CancelFunc\n\twg     sync.WaitGroup\n\n\t// Callback function. Olric calls this after\n\t// the server is ready to accept new connections.\n\tstarted func()\n}\n\nfunc prepareConfig(c *config.Config) (*config.Config, error) {\n\tif c == nil {\n\t\treturn nil, fmt.Errorf(\"config cannot be nil\")\n\t}\n\n\terr := c.Sanitize()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = c.Validate()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = c.SetupNetworkConfig()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tc.MemberlistConfig.Name = net.JoinHostPort(c.BindAddr,\n\t\tstrconv.Itoa(c.BindPort))\n\n\tfilter := &logutils.LevelFilter{\n\t\tLevels:   []logutils.LogLevel{\"DEBUG\", \"WARN\", \"ERROR\", \"INFO\"},\n\t\tMinLevel: logutils.LogLevel(strings.ToUpper(c.LogLevel)),\n\t\tWriter:   c.Logger.Writer(),\n\t}\n\tc.Logger.SetOutput(filter)\n\n\treturn c, nil\n}\n\nfunc initializeServices(db *Olric) error {\n\tdb.rt = routingtable.New(db.env)\n\tdb.env.Set(\"routingtable\", db.rt)\n\n\tdb.balancer = balancer.New(db.env)\n\n\t// Add Services\n\tdt, err := pubsub.NewService(db.env)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdb.pubsub = dt.(*pubsub.Service)\n\n\tdm, err := dmap.NewService(db.env)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdb.dmap = dm.(*dmap.Service)\n\n\treturn nil\n}\n\n// New creates a new Olric instance, otherwise returns an error.\nfunc New(c *config.Config) (*Olric, error) {\n\tvar err error\n\tc, err = prepareConfig(c)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\te := environment.New()\n\te.Set(\"config\", c)\n\n\t// Set the hash function. Olric distributes keys over partitions by hashing.\n\tpartitions.SetHashFunc(c.Hasher)\n\n\tflogger := flog.New(c.Logger)\n\tflogger.SetLevel(c.LogVerbosity)\n\tif c.LogLevel == \"DEBUG\" {\n\t\tflogger.ShowLineNumber(1)\n\t}\n\te.Set(\"logger\", flogger)\n\n\tclient := server.NewClient(c.Client)\n\te.Set(\"client\", client)\n\te.Set(\"primary\", partitions.New(c.PartitionCount, partitions.PRIMARY))\n\te.Set(\"backup\", partitions.New(c.PartitionCount, partitions.BACKUP))\n\te.Set(\"locker\", locker.New())\n\tctx, cancel := context.WithCancel(context.Background())\n\tdb := &Olric{\n\t\tname:     c.MemberlistConfig.Name,\n\t\tenv:      e,\n\t\tlog:      flogger,\n\t\tconfig:   c,\n\t\thashFunc: c.Hasher,\n\t\tclient:   client,\n\t\tprimary:  e.Get(\"primary\").(*partitions.Partitions),\n\t\tbackup:   e.Get(\"backup\").(*partitions.Partitions),\n\t\tstarted:  c.Started,\n\t\tctx:      ctx,\n\t\tcancel:   cancel,\n\t}\n\n\t// Create a Redcon server instance\n\trc := &server.Config{\n\t\tBindAddr:        c.BindAddr,\n\t\tBindPort:        c.BindPort,\n\t\tKeepAlivePeriod: c.KeepAlivePeriod,\n\t}\n\tsrv := server.New(rc, flogger)\n\tsrv.SetPreConditionFunc(db.preconditionFunc)\n\n\tdb.server = srv\n\te.Set(\"server\", srv)\n\n\terr = initializeServices(db)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdb.registerCommandHandlers()\n\n\treturn db, nil\n}\n\nfunc (db *Olric) preconditionFunc(conn redcon.Conn, _ redcon.Command) bool {\n\terr := db.isOperable()\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (db *Olric) registerCommandHandlers() {\n\tdb.server.ServeMux().HandleFunc(protocol.Generic.Ping, db.pingCommandHandler)\n\tdb.server.ServeMux().HandleFunc(protocol.Cluster.RoutingTable, db.clusterRoutingTableCommandHandler)\n\tdb.server.ServeMux().HandleFunc(protocol.Generic.Stats, db.statsCommandHandler)\n\tdb.server.ServeMux().HandleFunc(protocol.Cluster.Members, db.clusterMembersCommandHandler)\n}\n\n// callStartedCallback checks passed checkpoint count and calls the callback\n// function.\nfunc (db *Olric) callStartedCallback() {\n\tdefer db.wg.Done()\n\n\ttimer := time.NewTimer(10 * time.Millisecond)\n\tdefer timer.Stop()\n\n\tfor {\n\t\ttimer.Reset(10 * time.Millisecond)\n\t\tselect {\n\t\tcase <-timer.C:\n\t\t\tif checkpoint.AllPassed() {\n\t\t\t\tif db.started != nil {\n\t\t\t\t\tdb.started()\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-db.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc convertClusterError(err error) error {\n\tswitch {\n\tcase errors.Is(err, routingtable.ErrClusterQuorum):\n\t\treturn ErrClusterQuorum\n\tcase errors.Is(err, routingtable.ErrServerGone):\n\t\treturn ErrServerGone\n\tcase errors.Is(err, routingtable.ErrOperationTimeout):\n\t\treturn ErrOperationTimeout\n\tdefault:\n\t\treturn err\n\t}\n}\n\n// isOperable controls bootstrapping status and cluster quorum to prevent split-brain syndrome.\nfunc (db *Olric) isOperable() error {\n\tif err := db.rt.CheckMemberCountQuorum(); err != nil {\n\t\treturn convertClusterError(err)\n\t}\n\t// An Olric node has to be bootstrapped to function properly.\n\treturn db.rt.CheckBootstrap()\n}\n\n// Start starts background servers and joins the cluster. You still must call Shutdown\n// method if Start function returns an early error.\nfunc (db *Olric) Start() error {\n\tdb.log.V(1).Printf(\"[INFO] Olric %s on %s/%s %s\", ReleaseVersion, runtime.GOOS, runtime.GOARCH, runtime.Version())\n\n\t// This error group is responsible to run the TCP server at background and report errors.\n\terrGr, ctx := errgroup.WithContext(context.Background())\n\terrGr.Go(func() error {\n\t\treturn db.server.ListenAndServe()\n\t})\n\n\tselect {\n\tcase <-db.server.StartedCtx.Done():\n\t\t// TCP server has been started\n\tcase <-ctx.Done():\n\t\t// TCP server could not be started due to an error. There is no need to run\n\t\t// Olric.Shutdown here because we could not start anything.\n\t\treturn errGr.Wait()\n\t}\n\n\t// Balancer works periodically to balance partition data across the cluster.\n\tif err := db.balancer.Start(); err != nil {\n\t\tif err != nil {\n\t\t\tdb.log.V(2).Printf(\"[ERROR] Failed to run the balancer subsystem: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\n\t// First, we need to join the cluster. Then, the routing table has been started.\n\tif err := db.rt.Join(); err != nil {\n\t\tif err != nil {\n\t\t\tdb.log.V(2).Printf(\"[ERROR] Failed to join the Olric cluster: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\t// Start routing table service and member discovery subsystem.\n\tif err := db.rt.Start(); err != nil {\n\t\tif err != nil {\n\t\t\tdb.log.V(2).Printf(\"[ERROR] Failed to run the routing table subsystem: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\n\t// Start publish-subscribe service\n\tif err := db.pubsub.Start(); err != nil {\n\t\tif err != nil {\n\t\t\tdb.log.V(2).Printf(\"[ERROR] Failed to run the Publish-Subscribe service: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\n\t// Start distributed map service\n\tif err := db.dmap.Start(); err != nil {\n\t\tif err != nil {\n\t\t\tdb.log.V(2).Printf(\"[ERROR] Failed to run the Distributed Map service: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\n\t// Warn the user about his/her choice of configuration\n\tif db.config.ReplicationMode == config.AsyncReplicationMode && db.config.WriteQuorum > 1 {\n\t\tdb.log.V(2).\n\t\t\tPrintf(\"[WARN] Olric is running in async replication mode. WriteQuorum (%d) is ineffective\",\n\t\t\t\tdb.config.WriteQuorum)\n\t}\n\n\tif db.started != nil {\n\t\tdb.wg.Add(1)\n\t\tgo db.callStartedCallback()\n\t}\n\n\tdb.log.V(2).Printf(\"[INFO] Node name in the cluster: %s\",\n\t\tdb.name)\n\tif db.config.Interface != \"\" {\n\t\tdb.log.V(2).Printf(\"[INFO] Olric uses interface: %s\",\n\t\t\tdb.config.Interface)\n\t}\n\tdb.log.V(2).Printf(\"[INFO] Olric bindAddr: %s, bindPort: %d\",\n\t\tdb.config.BindAddr, db.config.BindPort)\n\tdb.log.V(2).Printf(\"[INFO] Replication count is %d\", db.config.ReplicaCount)\n\n\t// Wait for the TCP server.\n\treturn errGr.Wait()\n}\n\n// Shutdown stops background servers and leaves the cluster.\nfunc (db *Olric) Shutdown(ctx context.Context) error {\n\tselect {\n\tcase <-db.ctx.Done():\n\t\t// Shutdown only once.\n\t\treturn nil\n\tdefault:\n\t}\n\n\tdb.cancel()\n\n\tvar latestError error\n\n\tif err := db.pubsub.Shutdown(ctx); err != nil {\n\t\tdb.log.V(2).Printf(\"[ERROR] Failed to shutdown PubSub service: %v\", err)\n\t\tlatestError = err\n\t}\n\n\tif err := db.dmap.Shutdown(ctx); err != nil {\n\t\tdb.log.V(2).Printf(\"[ERROR] Failed to shutdown DMap service: %v\", err)\n\t\tlatestError = err\n\t}\n\n\tif err := db.balancer.Shutdown(ctx); err != nil {\n\t\tdb.log.V(2).Printf(\"[ERROR] Failed to shutdown balancer service: %v\", err)\n\t\tlatestError = err\n\t}\n\n\tif err := db.rt.Shutdown(ctx); err != nil {\n\t\tdb.log.V(2).Printf(\"[ERROR] Failed to shutdown routing table service: %v\", err)\n\t\tlatestError = err\n\t}\n\n\t// Shutdown Redcon server\n\tif err := db.server.Shutdown(ctx); err != nil {\n\t\tdb.log.V(2).Printf(\"[ERROR] Failed to shutdown RESP server: %v\", err)\n\t\tlatestError = err\n\t}\n\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer func() {\n\t\t\tclose(done)\n\t\t}()\n\t\tdb.wg.Wait()\n\t}()\n\n\tselect {\n\tcase <-ctx.Done():\n\tcase <-done:\n\t}\n\n\t// db.name will be shown as empty string, if the program is killed before\n\t// bootstrapping.\n\tdb.log.V(2).Printf(\"[INFO] %s is gone\", db.name)\n\treturn latestError\n}\n\nfunc convertDMapError(err error) error {\n\tswitch {\n\tcase errors.Is(err, dmap.ErrKeyFound):\n\t\treturn ErrKeyFound\n\tcase errors.Is(err, dmap.ErrKeyNotFound):\n\t\treturn ErrKeyNotFound\n\tcase errors.Is(err, dmap.ErrDMapNotFound):\n\t\treturn ErrKeyNotFound\n\tcase errors.Is(err, dmap.ErrLockNotAcquired):\n\t\treturn ErrLockNotAcquired\n\tcase errors.Is(err, dmap.ErrNoSuchLock):\n\t\treturn ErrNoSuchLock\n\tcase errors.Is(err, dmap.ErrReadQuorum):\n\t\treturn ErrReadQuorum\n\tcase errors.Is(err, dmap.ErrWriteQuorum):\n\t\treturn ErrWriteQuorum\n\tcase errors.Is(err, dmap.ErrServerGone):\n\t\treturn ErrServerGone\n\tcase errors.Is(err, dmap.ErrKeyTooLarge):\n\t\treturn ErrKeyTooLarge\n\tcase errors.Is(err, dmap.ErrEntryTooLarge):\n\t\treturn ErrEntryTooLarge\n\tdefault:\n\t\treturn convertClusterError(err)\n\t}\n}\n"
        },
        {
          "name": "olric_test.go",
          "type": "blob",
          "size": 3.4462890625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/config\"\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/buraksezer/olric/stats\"\n\t\"github.com/hashicorp/memberlist\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// newTestOlricWithConfig creates a new Olric instance with the given configuration.\n// This function is intended for internal use. Please use testOlricCluster and its\n// methods to form a cluster in tests.\nfunc newTestOlricWithConfig(t *testing.T, c *config.Config) *Olric {\n\tport, err := testutil.GetFreePort()\n\trequire.NoError(t, err)\n\n\tif c.MemberlistConfig == nil {\n\t\tc.MemberlistConfig = memberlist.DefaultLocalConfig()\n\t}\n\tc.MemberlistConfig.BindPort = 0\n\n\tc.BindAddr = \"127.0.0.1\"\n\tc.BindPort = port\n\n\terr = c.Sanitize()\n\trequire.NoError(t, err)\n\n\terr = c.Validate()\n\trequire.NoError(t, err)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tc.Started = func() {\n\t\tcancel()\n\t}\n\n\tdb, err := New(c)\n\trequire.NoError(t, err)\n\n\tgo func() {\n\t\tif err := db.Start(); err != nil {\n\t\t\tpanic(fmt.Sprintf(\"Failed to run Olric: %v\", err))\n\t\t}\n\t}()\n\n\tselect {\n\tcase <-time.After(time.Second):\n\t\tt.Fatalf(\"Olric cannot be started in one second\")\n\tcase <-ctx.Done():\n\t\t// everything is fine\n\t}\n\n\treturn db\n}\n\ntype testOlricCluster struct {\n\tmtx     sync.Mutex\n\tmembers map[string]*Olric\n}\n\nfunc newTestOlricCluster(t *testing.T) *testOlricCluster {\n\tcl := &testOlricCluster{members: make(map[string]*Olric)}\n\tt.Cleanup(func() {\n\t\tcl.mtx.Lock()\n\t\tdefer cl.mtx.Unlock()\n\t\tfor _, member := range cl.members {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\t\t\terr := member.Shutdown(ctx)\n\t\t\tcancel()\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t})\n\treturn cl\n}\n\nfunc (cl *testOlricCluster) addMemberWithConfig(t *testing.T, c *config.Config) *Olric {\n\tcl.mtx.Lock()\n\tdefer cl.mtx.Unlock()\n\n\tif c == nil {\n\t\tc = testutil.NewConfig()\n\t}\n\n\tfor _, member := range cl.members {\n\t\tc.Peers = append(c.Peers, member.rt.Discovery().LocalNode().Address())\n\t}\n\n\tdb := newTestOlricWithConfig(t, c)\n\tcl.members[db.rt.This().String()] = db\n\tt.Logf(\"A new cluster member has been created: %s\", db.rt.This())\n\treturn db\n}\n\nfunc (cl *testOlricCluster) addMember(t *testing.T) *Olric {\n\treturn cl.addMemberWithConfig(t, nil)\n}\n\nfunc TestOlric_StartAndShutdown(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\terr := db.Shutdown(context.Background())\n\trequire.NoError(t, err)\n}\n\nfunc TestOlricCluster_StartAndShutdown(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tcluster.addMember(t)\n\tdb := cluster.addMember(t)\n\trequire.Len(t, cluster.members, 2)\n\n\te := db.NewEmbeddedClient()\n\tst, err := e.Stats(context.Background(), db.rt.This().String())\n\trequire.NoError(t, err)\n\trequire.Len(t, st.ClusterMembers, 2)\n\tfor _, member := range cluster.members {\n\t\trequire.Contains(t, st.ClusterMembers, stats.MemberID(member.rt.This().ID))\n\t}\n}\n"
        },
        {
          "name": "olricd-docker.yaml",
          "type": "blob",
          "size": 8.9248046875,
          "content": "olricd:\n  # BindAddr denotes the address that Olric will bind to for communication\n  # with other Olric nodes.\n  bindAddr: 0.0.0.0\n\n  # BindPort denotes the address that Olric will bind to for communication\n  # with other Olric nodes.\n  bindPort: 3320\n\n  # KeepAlivePeriod denotes whether the operating system should send\n  # keep-alive messages on the connection.\n  keepAlivePeriod: 300s\n\n  # IdleClose will automatically close idle connections after the specified duration.\n  # Use zero to disable this feature.\n  # idleClose: 300s\n\n  # Timeout for bootstrap control\n  #\n  # An Olric node checks operation status before taking any action for the\n  # cluster events, responding incoming requests and running API functions.\n  # Bootstrapping status is one of the most important checkpoints for an\n  # \"operable\" Olric node. BootstrapTimeout sets a deadline to check\n  # bootstrapping status without blocking indefinitely.\n  bootstrapTimeout: 5s\n\n  # PartitionCount is 271, by default.\n  partitionCount: 271\n\n  # ReplicaCount is 1, by default.\n  replicaCount: 1\n\n  # Minimum number of successful writes to return a response for a write request.\n  writeQuorum: 1\n\n  # Minimum number of successful reads to return a response for a read request.\n  readQuorum: 1\n\n  # Switch to control read-repair algorithm which helps to reduce entropy.\n  readRepair: false\n\n  # Default value is SyncReplicationMode.\n  replicationMode: 0 # sync mode. for async, set 1\n\n  # Minimum number of members to form a cluster and run any query on the cluster.\n  memberCountQuorum: 1\n\n  # Coordinator member pushes the routing table to cluster members in the case of\n  # node join or left events. It also pushes the table periodically. routingTablePushInterval\n  # is the interval between subsequent calls. Default is 1 minute.\n  routingTablePushInterval: 1m\n\n  # Olric can send push cluster events to cluster.events channel. Available cluster events:\n  #\n  # * node-join-event\n  # * node-left-event\n  # * fragment-migration-event\n  # * fragment-received-event\n  #\n  # If you want to receive these events, set true to EnableClusterEventsChannel and subscribe to\n  # cluster.events channel. Default is false.\n  enableClusterEventsChannel: true\n\nclient:\n  # Timeout for TCP dial.\n  #\n  # The timeout includes name resolution, if required. When using TCP, and the host in the address parameter\n  # resolves to multiple IP addresses, the timeout is spread over each consecutive dial, such that each is\n  # given an appropriate fraction of the time to connect.\n  dialTimeout: 5s\n\n  # Timeout for socket reads. If reached, commands will fail\n  # with a timeout instead of blocking. Use value -1 for no timeout and 0 for default.\n  # Default is DefaultReadTimeout\n  readTimeout: 3s\n\n  # Timeout for socket writes. If reached, commands will fail\n  # with a timeout instead of blocking.\n  # Default is DefaultWriteTimeout\n  writeTimeout: 3s\n\n  # Maximum number of retries before giving up.\n  # Default is 3 retries; -1 (not 0) disables retries.\n  #maxRetries: 3\n\n  # Minimum backoff between each retry.\n  # Default is 8 milliseconds; -1 disables backoff.\n  #minRetryBackoff: 8ms\n\n  # Maximum backoff between each retry.\n  # Default is 512 milliseconds; -1 disables backoff.\n  #maxRetryBackoff: 512ms\n\n  # Type of connection pool.\n  # true for FIFO pool, false for LIFO pool.\n  # Note that fifo has higher overhead compared to lifo.\n  #poolFIFO: false\n\n  # Maximum number of socket connections.\n  # Default is 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n  #poolSize: 0\n\n  # Minimum number of idle connections which is useful when establishing\n  # new connection is slow.\n  #minIdleConns:\n\n  # Connection age at which client retires (closes) the connection.\n  # Default is to not close aged connections.\n  #maxConnAge:\n\n  # Amount of time client waits for connection if all connections are busy before\n  # returning an error. Default is ReadTimeout + 1 second.\n  #poolTimeout: 3s\n\n  # Amount of time after which client closes idle connections.\n  # Should be less than server's timeout.\n  # Default is 5 minutes. -1 disables idle timeout check.\n  idleTimeout: 5m\n\n  # Frequency of idle checks made by idle connections reaper.\n  # Default is 1 minute. -1 disables idle connections reaper,\n  # but idle connections are still discarded by the client\n  # if IdleTimeout is set.\n  idleCheckFrequency: 1m\n\n\nlogging:\n  # DefaultLogVerbosity denotes default log verbosity level.\n  #\n  # * 1 - Generally useful for this to ALWAYS be visible to an operator\n  #   * Programmer errors\n  #   * Logging extra info about a panic\n  #   * CLI argument handling\n  # * 2 - A reasonable default log level if you don't want verbosity.\n  #   * Information about config (listening on X, watching Y)\n  #   * Errors that repeat frequently that relate to conditions that can be\n  #     corrected\n  # * 3 - Useful steady state information about the service and\n  #     important log messages that may correlate to\n  #   significant changes in the system.  This is the recommended default log\n  #     level for most systems.\n  #   * Logging HTTP requests and their exit code\n  #   * System state changing\n  #   * Controller state change events\n  #   * Scheduler log messages\n  # * 4 - Extended information about changes\n  #   * More info about system state changes\n  # * 5 - Debug level verbosity\n  #   * Logging in particularly thorny parts of code where you may want to come\n  #     back later and check it\n  # * 6 - Trace level verbosity\n  #   * Context to understand the steps leading up to neterrors and warnings\n  #   * More information for troubleshooting reported issues\n  verbosity: 3\n\n  # Default LogLevel is DEBUG. Available levels: \"DEBUG\", \"WARN\", \"ERROR\", \"INFO\"\n  level: INFO\n  output: stderr\n\nmemberlist:\n  environment: lan\n\n  # Configuration related to what address to bind to and ports to\n  # listen on. The port is used for both UDP and TCP gossip. It is\n  # assumed other nodes are running on this port, but they do not need\n  # to.\n  bindAddr: 0.0.0.0\n  bindPort: 3322\n\n  # EnableCompression is used to control message compression. This can\n  # be used to reduce bandwidth usage at the cost of slightly more CPU\n  # utilization. This is only available starting at protocol version 1.\n  enableCompression: false\n\n  # JoinRetryInterval is the time gap between attempts to join an existing\n  # cluster.\n  joinRetryInterval: 1ms\n\n  # MaxJoinAttempts denotes the maximum number of attemps to join an existing\n  # cluster before forming a new one.\n  maxJoinAttempts: 1\n\n  # See service discovery plugins\n  #peers:\n  #  - \"localhost:3325\"\n\n  #advertiseAddr: \"\"\n  #advertisePort: 3322\n  #suspicionMaxTimeoutMult: 6\n  #disableTCPPings: false\n  #awarenessMaxMultiplier: 8\n  #gossipNodes: 3\n  #gossipVerifyIncoming: true\n  #gossipVerifyOutgoing: true\n  #dnsConfigPath: \"/etc/resolv.conf\"\n  #handoffQueueDepth: 1024\n  #udpBufferSize: 1400\n\ndmaps:\n  engine:\n    name: kvstore\n    config:\n      tableSize: 524288 # bytes\n#  checkEmptyFragmentsInterval: 1m\n#  triggerCompactionInterval: 10m\n#  numEvictionWorkers: 1\n#  maxIdleDuration: \"\"\n#  ttlDuration: \"100s\"\n#  maxKeys: 100000\n#  maxInuse: 1000000\n#  lRUSamples: 10\n#  evictionPolicy: \"LRU\"\n#  custom:\n#   foobar:\n#      maxIdleDuration: \"60s\"\n#      ttlDuration: \"300s\"\n#      maxKeys: 500000\n#      lRUSamples: 20\n#      evictionPolicy: \"NONE\"\n\n\n#serviceDiscovery:\n#  # path is a required property and used by Olric. It has to be a full path.\n#  path: \"/home/burak/go/src/github.com/buraksezer/olric-consul-plugin/consul.so\"\n#\n#  # provider is just informal,\n#  provider: \"consul\"\n#\n#  # Plugin specific configuration\n#  # Consul server, used by the plugin. It's required\n#  address: \"http://127.0.0.1:8500\"\n#\n#  # Specifies that the server should return only nodes with all checks in the passing state.\n#  passingOnly: true\n#\n#  # Missing health checks from the request will be deleted from the agent. Using this parameter\n#  # allows to idempotently register a service and its checks without having to manually deregister\n#  # checks.\n#  replaceExistingChecks: true\n#\n#  # InsecureSkipVerify controls whether a client verifies the\n#  # server's certificate chain and host name.\n#  # If InsecureSkipVerify is true, TLS accepts any certificate\n#  # presented by the server and any host name in that certificate.\n#  # In this mode, TLS is susceptible to man-in-the-middle attacks.\n#  # This should be used only for testing.\n#  insecureSkipVerify: true\n#\n#  # service record\n#  payload: '\n#      {\n#          \"Name\": \"olric-cluster\",\n#          \"ID\": \"olric-node-1\",\n#          \"Tags\": [\n#            \"primary\",\n#            \"v1\"\n#          ],\n#          \"Address\": \"localhost\",\n#          \"Port\": 3322,\n#          \"EnableTagOverride\": false,\n#          \"check\": {\n#            \"name\": \"Olric node on 3322\",\n#            \"tcp\": \"0.0.0.0:3322\",\n#            \"interval\": \"10s\",\n#            \"timeout\": \"1s\"\n#          }\n#      }\n#'\n#\n#\n#serviceDiscovery:\n#  provider: \"k8s\"\n#  path: \"/Users/buraksezer/go/src/github.com/buraksezer/olric-cloud-plugin/olric-cloud-plugin.so\"\n#  args: 'label_selector=\"app = olricd-server\"'\n"
        },
        {
          "name": "ping.go",
          "type": "blob",
          "size": 1.3916015625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"strings\"\n\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/tidwall/redcon\"\n)\n\nconst DefaultPingResponse = \"PONG\"\n\nfunc (db *Olric) ping(ctx context.Context, addr, message string) ([]byte, error) {\n\tmessage = strings.TrimSpace(message)\n\n\tpingCmd := protocol.NewPing()\n\tif message != \"\" {\n\t\tpingCmd = pingCmd.SetMessage(message)\n\t}\n\n\tcmd := pingCmd.Command(ctx)\n\trc := db.client.Get(addr)\n\terr := rc.Process(ctx, cmd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn cmd.Bytes()\n}\n\nfunc (db *Olric) pingCommandHandler(conn redcon.Conn, cmd redcon.Command) {\n\tpingCmd, err := protocol.ParsePingCommand(cmd)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\n\tif pingCmd.Message != \"\" {\n\t\tconn.WriteString(pingCmd.Message)\n\t\treturn\n\t}\n\tconn.WriteString(DefaultPingResponse)\n}\n"
        },
        {
          "name": "ping_test.go",
          "type": "blob",
          "size": 1.1884765625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestOlric_Ping(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tresult, err := db.ping(context.Background(), db.rt.This().String(), \"\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, []byte(DefaultPingResponse), result)\n}\n\nfunc TestOlric_PingWithMessage(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tmsg := \"Olric rocks!\"\n\tresponse, err := db.ping(context.Background(), db.rt.This().String(), msg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, []byte(msg), response)\n}\n"
        },
        {
          "name": "pipeline.go",
          "type": "blob",
          "size": 18.2998046875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/cluster/partitions\"\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/resp\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"golang.org/x/sync/errgroup\"\n\t\"golang.org/x/sync/semaphore\"\n)\n\nvar (\n\t// ErrNotReady denotes that the Future instance you hold is not ready to read the response yet.\n\tErrNotReady = errors.New(\"not ready yet\")\n\n\t// ErrPipelineClosed denotes that the underlying pipeline is closed, and it's impossible to operate.\n\tErrPipelineClosed = errors.New(\"pipeline is closed\")\n\n\t// ErrPipelineExecuted denotes that Exec was already called on the underlying pipeline.\n\tErrPipelineExecuted = errors.New(\"pipeline already executed\")\n)\n\n// DMapPipeline implements a pipeline for the following methods of the DMap API:\n//\n// * Put\n// * Get\n// * Delete\n// * Incr\n// * Decr\n// * GetPut\n// * IncrByFloat\n//\n// DMapPipeline enables batch operations on DMap data.\ntype DMapPipeline struct {\n\tmtx          sync.Mutex\n\tdm           *ClusterDMap\n\tcommands     map[uint64][]redis.Cmder\n\tresult       map[uint64][]redis.Cmder\n\tctx          context.Context\n\tcancel       context.CancelFunc\n\tclosedCtx    context.Context // used to detect if the pipeline is closed / discarded\n\tclosedCancel context.CancelFunc\n\n\tconcurrency int // defaults to runtime.NumCPU()\n}\n\nfunc (dp *DMapPipeline) addCommand(key string, cmd redis.Cmder) (uint64, int) {\n\tdp.mtx.Lock()\n\tdefer dp.mtx.Unlock()\n\n\thkey := partitions.HKey(dp.dm.name, key)\n\tpartID := hkey % dp.dm.clusterClient.partitionCount\n\n\tcmds, ok := dp.commands[partID]\n\tif !ok {\n\t\t// if there are no existing commands, get a new slice from the pool\n\t\tcmds = getPipelineCmdsFromPool()\n\t}\n\tdp.commands[partID] = append(cmds, cmd)\n\n\treturn partID, len(dp.commands[partID]) - 1\n}\n\n// FuturePut is used to read the result of a pipelined Put command.\ntype FuturePut struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Put command.\nfunc (f *FuturePut) Result() error {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\treturn processProtocolError(cmd.Err())\n\tdefault:\n\t\treturn ErrNotReady\n\t}\n}\n\n// Put queues a Put command. The parameters are identical to the DMap.Put,\n// but it returns FuturePut to read the batched response.\nfunc (dp *DMapPipeline) Put(ctx context.Context, key string, value interface{}, options ...PutOption) (*FuturePut, error) {\n\tbuf := bytes.NewBuffer(nil)\n\n\tenc := resp.New(buf)\n\terr := enc.Encode(value)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar pc dmap.PutConfig\n\tfor _, opt := range options {\n\t\topt(&pc)\n\t}\n\n\tcmd := dp.dm.writePutCommand(&pc, key, buf.Bytes()).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FuturePut{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\n// FutureGet is used to read result of a pipelined Get command.\ntype FutureGet struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Get command.\nfunc (f *FutureGet) Result() (*GetResponse, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn nil, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() != nil {\n\t\t\treturn nil, processProtocolError(cmd.Err())\n\t\t}\n\t\tstringCmd := redis.NewStringCmd(context.Background(), cmd.Args()...)\n\t\tstringCmd.SetVal(cmd.(*redis.Cmd).Val().(string))\n\t\treturn f.dp.dm.makeGetResponse(stringCmd)\n\tdefault:\n\t\treturn nil, ErrNotReady\n\t}\n}\n\n// Get queues a Get command. The parameters are identical to the DMap.Get,\n// but it returns FutureGet to read the batched response.\nfunc (dp *DMapPipeline) Get(ctx context.Context, key string) *FutureGet {\n\tcmd := protocol.NewGet(dp.dm.name, key).SetRaw().Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureGet{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}\n}\n\n// FutureDelete is used to read the result of a pipelined Delete command.\ntype FutureDelete struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Delete command.\nfunc (f *FutureDelete) Result() (int, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn 0, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() != nil {\n\t\t\treturn 0, processProtocolError(cmd.Err())\n\t\t}\n\t\treturn int(cmd.(*redis.Cmd).Val().(int64)), nil\n\tdefault:\n\t\treturn 0, ErrNotReady\n\t}\n}\n\n// Delete queues a Delete command. The parameters are identical to the DMap.Delete,\n// but it returns FutureDelete to read the batched response.\nfunc (dp *DMapPipeline) Delete(ctx context.Context, key string) *FutureDelete {\n\tcmd := protocol.NewDel(dp.dm.name, []string{key}...).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureDelete{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}\n}\n\n// FutureExpire is used to read the result of a pipelined Expire command.\ntype FutureExpire struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Expire command.\nfunc (f *FutureExpire) Result() error {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\treturn processProtocolError(cmd.Err())\n\tdefault:\n\t\treturn ErrNotReady\n\t}\n}\n\n// Expire queues an Expire command. The parameters are identical to the DMap.Expire,\n// but it returns FutureExpire to read the batched response.\nfunc (dp *DMapPipeline) Expire(ctx context.Context, key string, timeout time.Duration) (*FutureExpire, error) {\n\tcmd := protocol.NewExpire(dp.dm.name, key, timeout).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureExpire{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\n// FutureIncr is used to read the result of a pipelined Incr command.\ntype FutureIncr struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Incr command.\nfunc (f *FutureIncr) Result() (int, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn 0, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() != nil {\n\t\t\treturn 0, processProtocolError(cmd.Err())\n\t\t}\n\t\treturn int(cmd.(*redis.Cmd).Val().(int64)), nil\n\tdefault:\n\t\treturn 0, ErrNotReady\n\t}\n}\n\n// Incr queues an Incr command. The parameters are identical to the DMap.Incr,\n// but it returns FutureIncr to read the batched response.\nfunc (dp *DMapPipeline) Incr(ctx context.Context, key string, delta int) (*FutureIncr, error) {\n\tcmd := protocol.NewIncr(dp.dm.name, key, delta).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureIncr{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\n// FutureDecr is used to read the result of a pipelined Decr command.\ntype FutureDecr struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined Decr command.\nfunc (f *FutureDecr) Result() (int, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn 0, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() != nil {\n\t\t\treturn 0, processProtocolError(cmd.Err())\n\t\t}\n\t\treturn int(cmd.(*redis.Cmd).Val().(int64)), nil\n\tdefault:\n\t\treturn 0, ErrNotReady\n\t}\n}\n\n// Decr queues a Decr command. The parameters are identical to the DMap.Decr,\n// but it returns FutureDecr to read the batched response.\nfunc (dp *DMapPipeline) Decr(ctx context.Context, key string, delta int) (*FutureDecr, error) {\n\tcmd := protocol.NewDecr(dp.dm.name, key, delta).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureDecr{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\n// FutureGetPut is used to read the result of a pipelined GetPut command.\ntype FutureGetPut struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined GetPut command.\nfunc (f *FutureGetPut) Result() (*GetResponse, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn nil, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() == redis.Nil {\n\t\t\t// This should be the first run.\n\t\t\treturn nil, nil\n\t\t}\n\t\tif cmd.Err() != nil {\n\t\t\treturn nil, processProtocolError(cmd.Err())\n\t\t}\n\t\tstringCmd := redis.NewStringCmd(context.Background(), cmd.Args()...)\n\t\tstringCmd.SetVal(cmd.(*redis.Cmd).Val().(string))\n\t\treturn f.dp.dm.makeGetResponse(stringCmd)\n\tdefault:\n\t\treturn nil, ErrNotReady\n\t}\n}\n\n// GetPut queues a GetPut command. The parameters are identical to the DMap.GetPut,\n// but it returns FutureGetPut to read the batched response.\nfunc (dp *DMapPipeline) GetPut(ctx context.Context, key string, value interface{}) (*FutureGetPut, error) {\n\tbuf := bytes.NewBuffer(nil)\n\n\tenc := resp.New(buf)\n\terr := enc.Encode(value)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcmd := protocol.NewGetPut(dp.dm.name, key, buf.Bytes()).SetRaw().Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureGetPut{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\n// FutureIncrByFloat is used to read the result of a pipelined IncrByFloat command.\ntype FutureIncrByFloat struct {\n\tdp        *DMapPipeline\n\tpartID    uint64\n\tindex     int\n\tctx       context.Context\n\tclosedCtx context.Context\n}\n\n// Result returns a response for the pipelined IncrByFloat command.\nfunc (f *FutureIncrByFloat) Result() (float64, error) {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-f.closedCtx.Done():\n\t\treturn 0, ErrPipelineClosed\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-f.ctx.Done():\n\t\tcmd := f.dp.result[f.partID][f.index]\n\t\tif cmd.Err() != nil {\n\t\t\treturn 0, processProtocolError(cmd.Err())\n\t\t}\n\t\tstringRes := cmd.(*redis.Cmd).Val().(string)\n\t\treturn strconv.ParseFloat(stringRes, 64)\n\tdefault:\n\t\treturn 0, ErrNotReady\n\t}\n}\n\n// IncrByFloat queues an IncrByFloat command. The parameters are identical to the DMap.IncrByFloat,\n// but it returns FutureIncrByFloat to read the batched response.\nfunc (dp *DMapPipeline) IncrByFloat(ctx context.Context, key string, delta float64) (*FutureIncrByFloat, error) {\n\tcmd := protocol.NewIncrByFloat(dp.dm.name, key, delta).Command(ctx)\n\tpartID, index := dp.addCommand(key, cmd)\n\treturn &FutureIncrByFloat{\n\t\tdp:        dp,\n\t\tpartID:    partID,\n\t\tindex:     index,\n\t\tctx:       dp.ctx,\n\t\tclosedCtx: dp.closedCtx,\n\t}, nil\n}\n\nfunc (dp *DMapPipeline) execOnPartition(ctx context.Context, partID uint64) error {\n\trc, err := dp.dm.clusterClient.clientByPartID(partID)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// There is no need to protect dp.commands map and its content.\n\t// It's already filled before running Exec, and it's now a read-only\n\t// data structure\n\tcommands := dp.commands[partID]\n\tpipe := rc.Pipeline()\n\n\tfor _, cmd := range commands {\n\t\tpipe.Do(ctx, cmd.Args()...)\n\t}\n\n\t// Exec executes all previously queued commands using one\n\t// client-server roundtrip.\n\t//\n\t// Exec always returns list of commands and error of the first failed\n\t// command if any.\n\tresult, _ := pipe.Exec(ctx)\n\tdp.mtx.Lock()\n\tdp.result[partID] = result\n\tdp.mtx.Unlock()\n\treturn nil\n}\n\n// Exec executes all queued commands using one client-server roundtrip per partition.\nfunc (dp *DMapPipeline) Exec(ctx context.Context) error {\n\t// this select is separate from the one below on purpose, since select is non-deterministic if multiple\n\t// cases are available, and we need to guarantee this check first.\n\tselect {\n\tcase <-dp.closedCtx.Done():\n\t\treturn ErrPipelineClosed\n\tdefault:\n\t}\n\n\t// this checks to see if Exec has already run. While Exec should only be called once, it is possible that\n\t// the user could call Exec multiple times. If we stored the result of errGr.Wait on the pipeline, we could\n\t// return that error and make Exec idempotent.\n\tselect {\n\tcase <-dp.ctx.Done():\n\t\treturn ErrPipelineExecuted\n\tdefault:\n\t}\n\n\tdefer dp.cancel()\n\n\tvar errGr errgroup.Group\n\tsem := semaphore.NewWeighted(int64(dp.concurrency))\n\tfor i := uint64(0); i < dp.dm.clusterClient.partitionCount; i++ {\n\t\terr := sem.Acquire(ctx, 1)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpartID := i\n\t\terrGr.Go(func() error {\n\t\t\tdefer sem.Release(1)\n\t\t\t// If execOnPartition returns an error, it will eventually stop\n\t\t\t// all flush operation.\n\t\t\treturn dp.execOnPartition(ctx, partID)\n\t\t})\n\t}\n\n\treturn errGr.Wait()\n}\n\n// Discard discards the pipelined commands and resets all internal states.\n// A pipeline can be reused after calling Discard.\nfunc (dp *DMapPipeline) Discard() error {\n\tselect {\n\tcase <-dp.closedCtx.Done():\n\t\treturn ErrPipelineClosed\n\tdefault:\n\t}\n\n\tdp.closedCancel()\n\n\tdp.mtx.Lock()\n\tdefer dp.mtx.Unlock()\n\n\t// return all command slices to the pool\n\n\tfor _, v := range dp.commands {\n\t\tputPipelineCmdsIntoPool(v)\n\t}\n\n\tfor _, v := range dp.result {\n\t\tputPipelineCmdsIntoPool(v)\n\t}\n\n\t// the deletes below are purposefully not combined with the loops above, as these are recognized and optimized\n\t// by the compiler. https://go-review.googlesource.com/c/go/+/110055\n\n\tfor k := range dp.commands {\n\t\tdelete(dp.commands, k)\n\t}\n\n\tfor k := range dp.result {\n\t\tdelete(dp.result, k)\n\t}\n\n\tdp.initContexts()\n\n\treturn nil\n}\n\n// Close closes the pipeline and frees the allocated resources. You shouldn't try to\n// reuse a closed pipeline.\nfunc (dp *DMapPipeline) Close() {\n\tdp.closedCancel()\n}\n\n// Pipeline is a mechanism to realise Redis Pipeline technique.\n//\n// Pipelining is a technique to extremely speed up processing by packing\n// operations to batches, send them at once to Redis and read a replies in a\n// singe step.\n// See https://redis.io/topics/pipelining\n//\n// Pay attention, that Pipeline is not a transaction, so you can get unexpected\n// results in case of big pipelines and small read/write timeouts.\n// Redis client has retransmission logic in case of timeouts, pipeline\n// can be retransmitted and commands can be executed more than once.\nfunc (dm *ClusterDMap) Pipeline(opts ...PipelineOption) (*DMapPipeline, error) {\n\tdp := &DMapPipeline{\n\t\tdm:       dm,\n\t\tcommands: make(map[uint64][]redis.Cmder),\n\t\tresult:   make(map[uint64][]redis.Cmder),\n\n\t\tconcurrency: runtime.NumCPU(),\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(dp)\n\t}\n\n\tdp.initContexts()\n\n\treturn dp, nil\n}\n\n// initContexts sets up chained contexts for the pipeline. The base is closedCtx, which is closed either in\n// Close or Discard. ctx is a child of closedCtx, as we want to cancel the pipeline if it is closed. It is\n// canceled in Exec, and used to block FutureXXX.Result() calls until Exec has completed.\nfunc (dp *DMapPipeline) initContexts() {\n\tdp.closedCtx, dp.closedCancel = context.WithCancel(context.Background())\n\tdp.ctx, dp.cancel = context.WithCancel(dp.closedCtx)\n}\n\n// This stores a slice of commands for each partition. There is a possibility that a single\n// large slice could be allocated with an unusually large number of commands in a single pipeline that\n// are very unbalanced across partitions, but that is unlikely to be a problem in practice.\n//\n// It does not store a pointer to the slice as recommended by staticcheck because that is harder to reason\n// about, and a single allocation is not a big deal compared to the slices we're able to reuse.\n// https://staticcheck.io/docs/checks#SA6002\n// https://github.com/dominikh/go-tools/issues/1336#issuecomment-1331206290\nvar pipelineCmdPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn make([]redis.Cmder, 0)\n\t},\n}\n\nfunc getPipelineCmdsFromPool() []redis.Cmder {\n\treturn pipelineCmdPool.Get().([]redis.Cmder)\n}\n\nfunc putPipelineCmdsIntoPool(cmds []redis.Cmder) {\n\t// remove references to underlying commands so they can be GCed\n\tfor i := range cmds {\n\t\tcmds[i] = nil\n\t}\n\tcmds = cmds[:0]\n\tpipelineCmdPool.Put(cmds)\n}\n"
        },
        {
          "name": "pipeline_test.go",
          "type": "blob",
          "size": 11.396484375,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDMapPipeline_Put(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FuturePut)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfp, err := pipe.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fp\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fp := range futures {\n\t\trequire.NoError(t, fp.Result())\n\t}\n\n\tfor i := 0; i < 100; i++ {\n\t\tkey := testutil.ToKey(i)\n\t\tgr, err := dm.Get(ctx, key)\n\t\trequire.NoError(t, err)\n\n\t\tvalue, err := gr.Byte()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testutil.ToVal(i), value)\n\t}\n}\n\nfunc TestDMapPipeline_Get(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t}\n\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfutures := make(map[int]*FutureGet)\n\tfor i := 0; i < 100; i++ {\n\t\tfg := pipe.Get(ctx, testutil.ToKey(i))\n\t\tfutures[i] = fg\n\t}\n\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor i, fg := range futures {\n\t\tgr, err := fg.Result()\n\t\trequire.NoError(t, err)\n\n\t\tvalue, err := gr.Byte()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testutil.ToVal(i), value)\n\t}\n}\n\nfunc TestDMapPipeline_Delete(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t}\n\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfutures := make(map[int]*FutureDelete)\n\tfor i := 0; i < 100; i++ {\n\t\tfd := pipe.Delete(ctx, testutil.ToKey(i))\n\t\tfutures[i] = fd\n\t}\n\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fd := range futures {\n\t\tnum, err := fd.Result()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, num)\n\t}\n}\n\nfunc TestDMapPipeline_Expire(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t}\n\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfutures := make(map[int]*FutureExpire)\n\tfor i := 0; i < 100; i++ {\n\t\tfd, err := pipe.Expire(ctx, testutil.ToKey(i), time.Hour)\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fd\n\t}\n\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fd := range futures {\n\t\terr := fd.Result()\n\t\trequire.NoError(t, err)\n\t}\n\n\tfor i := 0; i < 100; i++ {\n\t\tgr, err := dm.Get(ctx, testutil.ToKey(i))\n\t\trequire.NoError(t, err)\n\t\trequire.NotEqual(t, int64(0), gr.TTL())\n\t}\n}\n\nfunc TestDMapPipeline_Incr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FutureIncr)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfi, err := pipe.Incr(ctx, \"mykey\", 1)\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fi\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor i, fp := range futures {\n\t\tnum, err := fp.Result()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, i+1, num)\n\t}\n}\n\nfunc TestDMapPipeline_Decr(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FutureDecr)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfi, err := pipe.Decr(ctx, \"mykey\", 1)\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fi\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor i, fp := range futures {\n\t\tnum, err := fp.Result()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, -1*(i+1), num)\n\t}\n}\n\nfunc TestDMapPipeline_GetPut(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FutureGetPut)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfi, err := pipe.GetPut(ctx, \"key\", testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fi\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fp := range futures {\n\t\tgr, err := fp.Result()\n\t\trequire.NoError(t, err)\n\t\tif gr != nil {\n\t\t\tfmt.Println(gr.String())\n\t\t}\n\t}\n}\n\nfunc TestDMapPipeline_IncrByFloat(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FutureIncrByFloat)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfi, err := pipe.IncrByFloat(ctx, \"mykey\", 1.2)\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fi\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fp := range futures {\n\t\t_, err := fp.Result()\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc TestDMapPipeline_Discard(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FuturePut)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < 100; i++ {\n\t\tfp, err := pipe.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fp\n\t}\n\n\t// Discard all pipelined DM.PUT requests.\n\terr = pipe.Discard()\n\trequire.NoError(t, err)\n\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < 100; i++ {\n\t\tkey := testutil.ToKey(i)\n\t\t_, err := dm.Get(ctx, key)\n\t\trequire.ErrorIs(t, err, ErrKeyNotFound)\n\t}\n}\n\nfunc TestDMapPipeline_Close(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FuturePut)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < 100; i++ {\n\t\tfp, err := pipe.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fp\n\t}\n\n\tpipe.Close()\n\n\terr = pipe.Exec(ctx)\n\trequire.ErrorIs(t, err, ErrPipelineClosed)\n}\n\nfunc TestDMapPipeline_ErrNotReady(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tt.Run(\"Put\", func(t *testing.T) {\n\t\tfp, err := pipe.Put(ctx, \"key\", \"value\")\n\t\trequire.NoError(t, err)\n\t\trequire.ErrorIs(t, ErrNotReady, fp.Result())\n\t})\n\n\tt.Run(\"Get\", func(t *testing.T) {\n\t\tfp := pipe.Get(ctx, \"key\")\n\t\t_, err := fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"Delete\", func(t *testing.T) {\n\t\tfp := pipe.Delete(ctx, \"key\")\n\t\t_, err := fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"Expire\", func(t *testing.T) {\n\t\tfp, err := pipe.Expire(ctx, \"key\", time.Second)\n\t\trequire.NoError(t, err)\n\t\terr = fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"Incr\", func(t *testing.T) {\n\t\tfp, err := pipe.Incr(ctx, \"key\", 1)\n\t\trequire.NoError(t, err)\n\t\t_, err = fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"Decr\", func(t *testing.T) {\n\t\tfp, err := pipe.Decr(ctx, \"key\", 1)\n\t\trequire.NoError(t, err)\n\t\t_, err = fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"GetPut\", func(t *testing.T) {\n\t\tfp, err := pipe.GetPut(ctx, \"key\", \"value\")\n\t\trequire.NoError(t, err)\n\t\t_, err = fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n\n\tt.Run(\"IncrByFloat\", func(t *testing.T) {\n\t\tfp, err := pipe.IncrByFloat(ctx, \"key\", 1)\n\t\trequire.NoError(t, err)\n\t\t_, err = fp.Result()\n\t\trequire.ErrorIs(t, ErrNotReady, err)\n\t})\n}\n\nfunc TestDMapPipeline_EmbeddedClient(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc := db.NewEmbeddedClient()\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tfutures := make(map[int]*FuturePut)\n\tpipe, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipe.Close()\n\n\tfor i := 0; i < 100; i++ {\n\t\tfp, err := pipe.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t\tfutures[i] = fp\n\t}\n\terr = pipe.Exec(ctx)\n\trequire.NoError(t, err)\n\n\tfor _, fp := range futures {\n\t\trequire.NoError(t, fp.Result())\n\t}\n\n\tfor i := 0; i < 100; i++ {\n\t\tkey := testutil.ToKey(i)\n\t\tgr, err := dm.Get(ctx, key)\n\t\trequire.NoError(t, err)\n\n\t\tvalue, err := gr.Byte()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testutil.ToVal(i), value)\n\t}\n}\n\nfunc TestDMapPipeline_setOrGetClusterClient(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc := db.NewEmbeddedClient()\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tdm, err := c.NewDMap(\"mydmap\")\n\trequire.NoError(t, err)\n\n\tpipeOne, err := dm.Pipeline()\n\trequire.NoError(t, err)\n\tdefer pipeOne.Close()\n\n\trequire.NotNil(t, dm.(*EmbeddedDMap).clusterClient)\n}\n"
        },
        {
          "name": "pkg",
          "type": "tree",
          "content": null
        },
        {
          "name": "pubsub.go",
          "type": "blob",
          "size": 2.0244140625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"strings\"\n\n\t\"github.com/buraksezer/olric/internal/server\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\ntype PubSub struct {\n\tconfig *pubsubConfig\n\trc     *redis.Client\n\tclient *server.Client\n}\n\nfunc newPubSub(client *server.Client, options ...PubSubOption) (*PubSub, error) {\n\tvar (\n\t\terr error\n\t\trc  *redis.Client\n\t\tpc  pubsubConfig\n\t)\n\tfor _, opt := range options {\n\t\topt(&pc)\n\t}\n\n\taddr := strings.Trim(pc.Address, \" \")\n\tif addr != \"\" {\n\t\trc = client.Get(addr)\n\t} else {\n\t\trc, err = client.Pick()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn &PubSub{\n\t\tconfig: &pc,\n\t\trc:     rc,\n\t\tclient: client,\n\t}, nil\n}\n\nfunc (ps *PubSub) Subscribe(ctx context.Context, channels ...string) *redis.PubSub {\n\treturn ps.rc.Subscribe(ctx, channels...)\n}\n\nfunc (ps *PubSub) PSubscribe(ctx context.Context, channels ...string) *redis.PubSub {\n\treturn ps.rc.PSubscribe(ctx, channels...)\n}\n\nfunc (ps *PubSub) Publish(ctx context.Context, channel string, message interface{}) (int64, error) {\n\treturn ps.rc.Publish(ctx, channel, message).Result()\n}\n\nfunc (ps *PubSub) PubSubChannels(ctx context.Context, pattern string) ([]string, error) {\n\treturn ps.rc.PubSubChannels(ctx, pattern).Result()\n}\n\nfunc (ps *PubSub) PubSubNumSub(ctx context.Context, channels ...string) (map[string]int64, error) {\n\treturn ps.rc.PubSubNumSub(ctx, channels...).Result()\n}\n\nfunc (ps *PubSub) PubSubNumPat(ctx context.Context) (int64, error) {\n\treturn ps.rc.PubSubNumPat(ctx).Result()\n}\n"
        },
        {
          "name": "pubsub_test.go",
          "type": "blob",
          "size": 6.3564453125,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc pubsubTestRunner(t *testing.T, ps *PubSub, kind, channel string) {\n\tctx := context.Background()\n\tvar rp *redis.PubSub\n\tswitch kind {\n\tcase \"subscribe\":\n\t\trp = ps.Subscribe(ctx, channel)\n\tcase \"psubscribe\":\n\t\trp = ps.PSubscribe(ctx, channel)\n\t}\n\n\tdefer func() {\n\t\trequire.NoError(t, rp.Close())\n\t}()\n\n\t// Wait for confirmation that subscription is created before publishing anything.\n\tmsgi, err := rp.ReceiveTimeout(ctx, time.Second)\n\trequire.NoError(t, err)\n\n\tsubs := msgi.(*redis.Subscription)\n\trequire.Equal(t, kind, subs.Kind)\n\trequire.Equal(t, channel, subs.Channel)\n\trequire.Equal(t, 1, subs.Count)\n\n\t// Go channel which receives messages.\n\tch := rp.Channel()\n\n\texpected := make(map[string]struct{})\n\tfor i := 0; i < 10; i++ {\n\t\tmsg := fmt.Sprintf(\"my-message-%d\", i)\n\t\tcount, err := ps.Publish(ctx, \"my-channel\", msg)\n\t\trequire.Equal(t, int64(1), count)\n\t\trequire.NoError(t, err)\n\t\texpected[msg] = struct{}{}\n\t}\n\n\tconsumed := make(map[string]struct{})\nL:\n\tfor {\n\t\tselect {\n\t\tcase msg := <-ch:\n\t\t\trequire.Equal(t, \"my-channel\", msg.Channel)\n\t\t\tconsumed[msg.Payload] = struct{}{}\n\t\t\tif len(consumed) == 10 {\n\t\t\t\t// It would be OK\n\t\t\t\tbreak L\n\t\t\t}\n\t\tcase <-time.After(5 * time.Second):\n\t\t\t// Enough. Break it and check the consumed items.\n\t\t\tbreak L\n\t\t}\n\t}\n\n\trequire.Equal(t, expected, consumed)\n}\n\nfunc TestPubSub_Publish_Subscribe(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps, err := c.NewPubSub(ToAddress(db.rt.This().String()))\n\trequire.NoError(t, err)\n\n\tpubsubTestRunner(t, ps, \"subscribe\", \"my-channel\")\n}\n\nfunc TestPubSub_Publish_PSubscribe(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps, err := c.NewPubSub(ToAddress(db.rt.This().String()))\n\trequire.NoError(t, err)\n\tpubsubTestRunner(t, ps, \"psubscribe\", \"my-*\")\n}\n\nfunc TestPubSub_PubSubChannels(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps, err := c.NewPubSub(ToAddress(db.rt.This().String()))\n\trequire.NoError(t, err)\n\n\trp := ps.Subscribe(ctx, \"my-channel\")\n\n\tdefer func() {\n\t\trequire.NoError(t, rp.Close())\n\t}()\n\n\t// Wait for confirmation that subscription is created before publishing anything.\n\t_, err = rp.ReceiveTimeout(ctx, time.Second)\n\trequire.NoError(t, err)\n\n\tchannels, err := ps.PubSubChannels(ctx, \"my-*\")\n\trequire.NoError(t, err)\n\n\trequire.Equal(t, []string{\"my-channel\"}, channels)\n}\n\nfunc TestPubSub_PubSubNumSub(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps, err := c.NewPubSub(ToAddress(db.rt.This().String()))\n\trequire.NoError(t, err)\n\n\trp := ps.Subscribe(ctx, \"my-channel\")\n\n\tdefer func() {\n\t\trequire.NoError(t, rp.Close())\n\t}()\n\n\t// Wait for confirmation that subscription is created before publishing anything.\n\t_, err = rp.ReceiveTimeout(ctx, time.Second)\n\trequire.NoError(t, err)\n\n\tnumsub, err := ps.PubSubNumSub(ctx, \"my-channel\", \"foobar\")\n\trequire.NoError(t, err)\n\n\texpected := map[string]int64{\n\t\t\"foobar\":     0,\n\t\t\"my-channel\": 1,\n\t}\n\trequire.Equal(t, expected, numsub)\n}\n\nfunc TestPubSub_PubSubNumPat(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps, err := c.NewPubSub(ToAddress(db.rt.This().String()))\n\trequire.NoError(t, err)\n\n\trp := ps.PSubscribe(ctx, \"my-*\")\n\n\tdefer func() {\n\t\trequire.NoError(t, rp.Close())\n\t}()\n\n\t// Wait for confirmation that subscription is created before publishing anything.\n\t_, err = rp.ReceiveTimeout(ctx, time.Second)\n\trequire.NoError(t, err)\n\n\tnumpat, err := ps.PubSubNumPat(ctx)\n\trequire.NoError(t, err)\n\trequire.Equal(t, int64(1), numpat)\n}\n\nfunc TestPubSub_Cluster(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb1 := cluster.addMember(t)\n\tdb2 := cluster.addMember(t)\n\n\t// Create a subscriber\n\tctx := context.Background()\n\tc, err := NewClusterClient([]string{db1.name})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, c.Close(ctx))\n\t}()\n\n\tps1, err := c.NewPubSub(ToAddress(db1.rt.This().String()))\n\trequire.NoError(t, err)\n\n\trp := ps1.Subscribe(ctx, \"my-channel\")\n\tdefer func() {\n\t\trequire.NoError(t, rp.Close())\n\t}()\n\t// Wait for confirmation that subscription is created before publishing anything.\n\t_, err = rp.ReceiveTimeout(ctx, time.Second)\n\trequire.NoError(t, err)\n\treceiveChan := rp.Channel()\n\n\t// Create a publisher\n\n\te := db2.NewEmbeddedClient()\n\tps2, err := e.NewPubSub(ToAddress(db2.rt.This().String()))\n\trequire.NoError(t, err)\n\texpected := make(map[string]struct{})\n\tfor i := 0; i < 10; i++ {\n\t\tmsg := fmt.Sprintf(\"my-message-%d\", i)\n\t\tcount, err := ps2.Publish(ctx, \"my-channel\", msg)\n\t\trequire.Equal(t, int64(1), count)\n\t\trequire.NoError(t, err)\n\t\texpected[msg] = struct{}{}\n\t}\n\n\tconsumed := make(map[string]struct{})\nL:\n\tfor {\n\t\tselect {\n\t\tcase msg := <-receiveChan:\n\t\t\trequire.Equal(t, \"my-channel\", msg.Channel)\n\t\t\tconsumed[msg.Payload] = struct{}{}\n\t\t\tif len(consumed) == 10 {\n\t\t\t\t// It would be OK\n\t\t\t\tbreak L\n\t\t\t}\n\t\tcase <-time.After(5 * time.Second):\n\t\t\t// Enough. Break it and check the consumed items.\n\t\t\tbreak L\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "stats.go",
          "type": "blob",
          "size": 4.9697265625,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"encoding/json\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/buraksezer/olric/internal/cluster/partitions\"\n\t\"github.com/buraksezer/olric/internal/discovery\"\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/pubsub\"\n\t\"github.com/buraksezer/olric/internal/server\"\n\t\"github.com/buraksezer/olric/stats\"\n\t\"github.com/tidwall/redcon\"\n)\n\nfunc toMember(member discovery.Member) stats.Member {\n\treturn stats.Member{\n\t\tName:      member.Name,\n\t\tID:        member.ID,\n\t\tBirthdate: member.Birthdate,\n\t}\n}\n\nfunc toMembers(members []discovery.Member) []stats.Member {\n\tvar _stats []stats.Member\n\tfor _, m := range members {\n\t\t_stats = append(_stats, toMember(m))\n\t}\n\treturn _stats\n}\n\nfunc (db *Olric) collectPartitionMetrics(partID uint64, part *partitions.Partition) stats.Partition {\n\towners := part.Owners()\n\tp := stats.Partition{\n\t\tBackups: toMembers(db.backup.PartitionOwnersByID(partID)),\n\t\tLength:  part.Length(),\n\t\tDMaps:   make(map[string]stats.DMap),\n\t}\n\tif len(owners) > 0 {\n\t\tp.PreviousOwners = toMembers(owners[:len(owners)-1])\n\t}\n\tpart.Map().Range(func(name, item interface{}) bool {\n\t\tf := item.(partitions.Fragment)\n\t\tst := f.Stats()\n\t\ttmp := stats.DMap{\n\t\t\tLength:    st.Length,\n\t\t\tNumTables: st.NumTables,\n\t\t}\n\t\ttmp.SlabInfo.Allocated = st.Allocated\n\t\ttmp.SlabInfo.Garbage = st.Garbage\n\t\ttmp.SlabInfo.Inuse = st.Inuse\n\t\tdmapName := strings.TrimPrefix(name.(string), \"dmap.\")\n\t\tp.DMaps[dmapName] = tmp\n\t\treturn true\n\t})\n\treturn p\n}\n\nfunc (db *Olric) checkPartitionOwnership(part *partitions.Partition) bool {\n\towners := part.Owners()\n\tfor _, owner := range owners {\n\t\tif owner.CompareByID(db.rt.This()) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (db *Olric) stats(cfg statsConfig) stats.Stats {\n\ts := stats.Stats{\n\t\tCmdline:            os.Args,\n\t\tReleaseVersion:     ReleaseVersion,\n\t\tUptimeSeconds:      discovery.UptimeSeconds.Read(),\n\t\tClusterCoordinator: toMember(db.rt.Discovery().GetCoordinator()),\n\t\tMember:             toMember(db.rt.This()),\n\t\tPartitions:         make(map[stats.PartitionID]stats.Partition),\n\t\tBackups:            make(map[stats.PartitionID]stats.Partition),\n\t\tClusterMembers:     make(map[stats.MemberID]stats.Member),\n\t\tNetwork: stats.Network{\n\t\t\tConnectionsTotal:   server.ConnectionsTotal.Read(),\n\t\t\tCurrentConnections: server.CurrentConnections.Read(),\n\t\t\tWrittenBytesTotal:  server.WrittenBytesTotal.Read(),\n\t\t\tReadBytesTotal:     server.ReadBytesTotal.Read(),\n\t\t\tCommandsTotal:      server.CommandsTotal.Read(),\n\t\t},\n\t\tDMaps: stats.DMaps{\n\t\t\tEntriesTotal: dmap.EntriesTotal.Read(),\n\t\t\tDeleteHits:   dmap.DeleteHits.Read(),\n\t\t\tDeleteMisses: dmap.DeleteMisses.Read(),\n\t\t\tGetMisses:    dmap.GetMisses.Read(),\n\t\t\tGetHits:      dmap.GetHits.Read(),\n\t\t\tEvictedTotal: dmap.EvictedTotal.Read(),\n\t\t},\n\t\tPubSub: stats.PubSub{\n\t\t\tPublishedTotal:      pubsub.PublishedTotal.Read(),\n\t\t\tCurrentSubscribers:  pubsub.CurrentSubscribers.Read(),\n\t\t\tSubscribersTotal:    pubsub.SubscribersTotal.Read(),\n\t\t\tCurrentPSubscribers: pubsub.CurrentPSubscribers.Read(),\n\t\t\tPSubscribersTotal:   pubsub.PSubscribersTotal.Read(),\n\t\t},\n\t}\n\n\tif cfg.CollectRuntime {\n\t\ts.Runtime = &stats.Runtime{\n\t\t\tGOOS:         runtime.GOOS,\n\t\t\tGOARCH:       runtime.GOARCH,\n\t\t\tVersion:      runtime.Version(),\n\t\t\tNumCPU:       runtime.NumCPU(),\n\t\t\tNumGoroutine: runtime.NumGoroutine(),\n\t\t}\n\t\truntime.ReadMemStats(&s.Runtime.MemStats)\n\t}\n\n\tdb.rt.RLock()\n\tdefer db.rt.RUnlock()\n\n\tdb.rt.Members().Range(func(id uint64, member discovery.Member) bool {\n\t\ts.ClusterMembers[stats.MemberID(id)] = toMember(member)\n\t\treturn true\n\t})\n\n\tfor partID := uint64(0); partID < db.config.PartitionCount; partID++ {\n\t\tprimary := db.primary.PartitionByID(partID)\n\t\tif db.checkPartitionOwnership(primary) {\n\t\t\ts.Partitions[stats.PartitionID(partID)] = db.collectPartitionMetrics(partID, primary)\n\t\t}\n\t\tbackup := db.backup.PartitionByID(partID)\n\t\tif db.checkPartitionOwnership(backup) {\n\t\t\ts.Backups[stats.PartitionID(partID)] = db.collectPartitionMetrics(partID, backup)\n\t\t}\n\t}\n\n\treturn s\n}\n\nfunc (db *Olric) statsCommandHandler(conn redcon.Conn, cmd redcon.Command) {\n\tstatsCmd, err := protocol.ParseStatsCommand(cmd)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\n\tsc := statsConfig{}\n\tif statsCmd.CollectRuntime {\n\t\tsc.CollectRuntime = true\n\t}\n\tmemberStats := db.stats(sc)\n\tdata, err := json.Marshal(memberStats)\n\tif err != nil {\n\t\tprotocol.WriteError(conn, err)\n\t\treturn\n\t}\n\tconn.WriteBulk(data)\n}\n"
        },
        {
          "name": "stats",
          "type": "tree",
          "content": null
        },
        {
          "name": "stats_test.go",
          "type": "blob",
          "size": 8.294921875,
          "content": "// Copyright 2018-2024 Burak Sezer\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage olric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/buraksezer/olric/internal/dmap\"\n\t\"github.com/buraksezer/olric/internal/protocol\"\n\t\"github.com/buraksezer/olric/internal/pubsub\"\n\t\"github.com/buraksezer/olric/internal/testutil\"\n\t\"github.com/buraksezer/olric/stats\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc resetPubSubStats() {\n\tpubsub.SubscribersTotal.Reset()\n\tpubsub.CurrentPSubscribers.Reset()\n\tpubsub.CurrentSubscribers.Reset()\n\tpubsub.PSubscribersTotal.Reset()\n\tpubsub.PublishedTotal.Reset()\n}\n\nfunc TestOlric_Stats(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\tc := db.NewEmbeddedClient()\n\tdm, err := c.NewDMap(\"mymap\")\n\trequire.NoError(t, err)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tfor i := 0; i < 100; i++ {\n\t\terr = dm.Put(ctx, testutil.ToKey(i), testutil.ToVal(i))\n\t\trequire.NoError(t, err)\n\t}\n\n\ts, err := c.Stats(ctx, db.rt.This().String())\n\trequire.NoError(t, err)\n\n\tif s.ClusterCoordinator.ID != db.rt.This().ID {\n\t\tt.Fatalf(\"Expected cluster coordinator: %v. Got: %v\", db.rt.This(), s.ClusterCoordinator)\n\t}\n\n\trequire.Equal(t, s.Member.Name, db.rt.This().Name)\n\trequire.Equal(t, s.Member.ID, db.rt.This().ID)\n\trequire.Equal(t, s.Member.Birthdate, db.rt.This().Birthdate)\n\tif s.Runtime != nil {\n\t\tt.Error(\"Runtime stats must not be collected by default:\", s.Runtime)\n\t}\n\n\tvar total int\n\tfor partID, part := range s.Partitions {\n\t\ttotal += part.Length\n\t\tif _, ok := part.DMaps[\"mymap\"]; !ok {\n\t\t\tt.Fatalf(\"Expected dmap check result is true. Got false\")\n\t\t}\n\t\tif len(part.PreviousOwners) != 0 {\n\t\t\tt.Fatalf(\"Expected PreviosOwners list is empty. \"+\n\t\t\t\t\"Got: %v for PartID: %d\", part.PreviousOwners, partID)\n\t\t}\n\t\tif part.Length <= 0 {\n\t\t\tt.Fatalf(\"Unexpected Length: %d\", part.Length)\n\t\t}\n\t}\n\tif total != 100 {\n\t\tt.Fatalf(\"Expected total length of partition in stats is 100. Got: %d\", total)\n\t}\n\t_, ok := s.ClusterMembers[stats.MemberID(db.rt.This().ID)]\n\tif !ok {\n\t\tt.Fatalf(\"Expected member ID: %d could not be found in ClusterMembers\", db.rt.This().ID)\n\t}\n}\n\nfunc TestOlric_Stats_CollectRuntime(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\ts, err := e.Stats(context.Background(), db.rt.This().String(), CollectRuntime())\n\trequire.NoError(t, err)\n\n\tif s.Runtime == nil {\n\t\tt.Fatal(\"Runtime stats must be collected by default:\", s.Runtime)\n\t}\n}\n\nfunc TestOlric_Stats_Cluster(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\tdb2 := cluster.addMember(t)\n\n\te := db.NewEmbeddedClient()\n\ts, err := e.Stats(context.Background(), db2.rt.This().String())\n\trequire.NoError(t, err)\n\trequire.Nil(t, s.Runtime)\n\trequire.Equal(t, s.Member.String(), db2.rt.This().String())\n}\n\nfunc TestStats_PubSub(t *testing.T) {\n\tresetPubSubStats()\n\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\trc := redis.NewClient(&redis.Options{Addr: db.rt.This().String()})\n\tctx := context.Background()\n\n\tt.Run(\"Subscribe\", func(t *testing.T) {\n\t\tdefer func() {\n\t\t\tresetPubSubStats()\n\t\t}()\n\n\t\tvar subscribers []*redis.PubSub\n\t\tfor i := 0; i < 5; i++ {\n\t\t\tps := rc.Subscribe(ctx, \"my-channel\")\n\t\t\t// Wait for confirmation that subscription is created before publishing anything.\n\t\t\t_, err := ps.Receive(ctx)\n\t\t\trequire.NoError(t, err)\n\t\t\tsubscribers = append(subscribers, ps)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := rc.Publish(ctx, \"my-channel\", fmt.Sprintf(\"message-%d\", i))\n\t\t\tres, err := cmd.Result()\n\t\t\trequire.Equal(t, int64(5), res)\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t\trequire.Equal(t, int64(50), pubsub.PublishedTotal.Read())\n\t\trequire.Equal(t, int64(5), pubsub.SubscribersTotal.Read())\n\t\trequire.Equal(t, int64(5), pubsub.CurrentSubscribers.Read())\n\t\trequire.Equal(t, int64(0), pubsub.PSubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentPSubscribers.Read())\n\n\t\t// Unsubscribe\n\t\tfor _, s := range subscribers {\n\t\t\terr := s.Unsubscribe(ctx, \"my-channel\")\n\t\t\trequire.NoError(t, err)\n\t\t\t<-time.After(100 * time.Millisecond)\n\t\t}\n\n\t\trequire.Equal(t, int64(5), pubsub.SubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentSubscribers.Read())\n\t\trequire.Equal(t, int64(0), pubsub.PSubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentPSubscribers.Read())\n\t})\n\n\tt.Run(\"PSubscribe\", func(t *testing.T) {\n\t\tdefer func() {\n\t\t\tresetPubSubStats()\n\t\t}()\n\n\t\tps := rc.PSubscribe(ctx, \"h?llo\")\n\t\t// Wait for confirmation that subscription is created before publishing anything.\n\t\t_, err := ps.Receive(ctx)\n\t\trequire.NoError(t, err)\n\n\t\tcmd := rc.Publish(ctx, \"hxllo\", \"message\")\n\t\tres, err := cmd.Result()\n\t\trequire.Equal(t, int64(1), res)\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, int64(1), pubsub.PublishedTotal.Read())\n\t\trequire.Equal(t, int64(1), pubsub.PSubscribersTotal.Read())\n\t\trequire.Equal(t, int64(1), pubsub.CurrentPSubscribers.Read())\n\n\t\trequire.Equal(t, int64(0), pubsub.SubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentSubscribers.Read())\n\n\t\terr = ps.PUnsubscribe(ctx, \"h?llo\")\n\t\trequire.NoError(t, err)\n\n\t\t<-time.After(100 * time.Millisecond)\n\t\trequire.Equal(t, int64(1), pubsub.PSubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentPSubscribers.Read())\n\t\trequire.Equal(t, int64(0), pubsub.SubscribersTotal.Read())\n\t\trequire.Equal(t, int64(0), pubsub.CurrentSubscribers.Read())\n\t})\n}\n\nfunc TestStats_DMap(t *testing.T) {\n\tcluster := newTestOlricCluster(t)\n\tdb := cluster.addMember(t)\n\n\trc := redis.NewClient(&redis.Options{Addr: db.rt.This().String()})\n\tctx := context.Background()\n\n\tt.Run(\"DMap stats without eviction\", func(t *testing.T) {\n\t\t// EntriesTotal\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewPut(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i), []byte(\"myvalue\")).Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, cmd.Err())\n\t\t}\n\n\t\t// GetHits\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewGet(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i)).Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, cmd.Err())\n\t\t}\n\n\t\t// DeleteHits\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewDel(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i)).Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, cmd.Err())\n\t\t}\n\n\t\t// GetMisses\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewGet(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i)).Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\terr = protocol.ConvertError(err)\n\t\t\trequire.ErrorIs(t, err, dmap.ErrKeyNotFound)\n\t\t}\n\n\t\t// DeleteMisses\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewDel(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i)).Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, cmd.Err())\n\t\t}\n\n\t\trequire.GreaterOrEqual(t, dmap.EntriesTotal.Read(), int64(10))\n\t\trequire.GreaterOrEqual(t, dmap.GetMisses.Read(), int64(10))\n\t\trequire.GreaterOrEqual(t, dmap.GetHits.Read(), int64(10))\n\t\trequire.GreaterOrEqual(t, dmap.DeleteHits.Read(), int64(10))\n\t\trequire.GreaterOrEqual(t, dmap.DeleteMisses.Read(), int64(10))\n\t})\n\n\tt.Run(\"DMap eviction stats\", func(t *testing.T) {\n\t\t// EntriesTotal, EvictedTotal\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.\n\t\t\t\tNewPut(\"mydmap\", fmt.Sprintf(\"mykey-%d\", i), []byte(\"myvalue\")).\n\t\t\t\tSetPX(time.Millisecond.Milliseconds()).\n\t\t\t\tCommand(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, cmd.Err())\n\t\t}\n\t\t<-time.After(100 * time.Millisecond)\n\n\t\t// GetMisses\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tcmd := protocol.NewGet(\"mydmap\", \"mykey\").Command(ctx)\n\t\t\terr := rc.Process(ctx, cmd)\n\t\t\terr = protocol.ConvertError(err)\n\t\t\trequire.ErrorIs(t, err, dmap.ErrKeyNotFound)\n\t\t}\n\n\t\trequire.Greater(t, dmap.DeleteHits.Read(), int64(0))\n\t\trequire.Greater(t, dmap.EvictedTotal.Read(), int64(0))\n\t\trequire.GreaterOrEqual(t, dmap.EntriesTotal.Read(), int64(10))\n\t})\n}\n"
        }
      ]
    }
  ]
}