{
  "metadata": {
    "timestamp": 1736566573210,
    "page": 123,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mholt/archiver",
      "stars": 4438,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0224609375,
          "content": "_gitignore\n__debug_bin\n"
        },
        {
          "name": ".goreleaser.yml",
          "type": "blob",
          "size": 0.755859375,
          "content": "# This is an example goreleaser.yaml file with some sane defaults.\n# Make sure to check the documentation at http://goreleaser.com\nproject_name: arc\nbefore:\n  hooks:\n    # You may remove this if you don't use go modules.\n    - go mod download\n    # you may remove this if you don't need go generate\n    - go generate ./...\nbuilds:\n  -\n    env:\n      - CGO_ENABLED=0\n    main: ./cmd/arc\n    goos:\n      - linux\n      - windows\n      - darwin\n    goarch:\n      - 386\n      - amd64\n      - arm\n      - arm64\n    goarm:\n      - 6\n      - 7\narchives:\n  -\n    format: binary\n    replacements:\n      darwin: mac\nchecksum:\n  name_template: 'checksums.txt'\nsnapshot:\n  name_template: \"{{ .Tag }}-next\"\nchangelog:\n  sort: asc\n  filters:\n    exclude:\n      - '^docs:'\n      - '^test:'\n"
        },
        {
          "name": "7z.go",
          "type": "blob",
          "size": 3.341796875,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"path\"\n\t\"strings\"\n\n\t\"github.com/bodgit/sevenzip\"\n)\n\nfunc init() {\n\tRegisterFormat(SevenZip{})\n\n\t// looks like the sevenzip package registers a lot of decompressors for us automatically:\n\t// https://github.com/bodgit/sevenzip/blob/46c5197162c784318b98b9a3f80289a9aa1ca51a/register.go#L38-L61\n}\n\ntype SevenZip struct {\n\t// If true, errors encountered during reading or writing\n\t// a file within an archive will be logged and the\n\t// operation will continue on remaining files.\n\tContinueOnError bool\n\n\t// The password, if dealing with an encrypted archive.\n\tPassword string\n}\n\nfunc (z SevenZip) Extension() string { return \".7z\" }\n\nfunc (z SevenZip) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), z.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(sevenZipHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, sevenZipHeader)\n\n\treturn mr, nil\n}\n\n// Archive is not implemented for 7z because I do not know of a pure-Go 7z writer.\n\n// Extract extracts files from z, implementing the Extractor interface. Uniquely, however,\n// sourceArchive must be an io.ReaderAt and io.Seeker, which are oddly disjoint interfaces\n// from io.Reader which is what the method signature requires. We chose this signature for\n// the interface because we figure you can Read() from anything you can ReadAt() or Seek()\n// with. Due to the nature of the zip archive format, if sourceArchive is not an io.Seeker\n// and io.ReaderAt, an error is returned.\nfunc (z SevenZip) Extract(ctx context.Context, sourceArchive io.Reader, handleFile FileHandler) error {\n\tsra, ok := sourceArchive.(seekReaderAt)\n\tif !ok {\n\t\treturn fmt.Errorf(\"input type must be an io.ReaderAt and io.Seeker because of zip format constraints\")\n\t}\n\n\tsize, err := streamSizeBySeeking(sra)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"determining stream size: %w\", err)\n\t}\n\n\tzr, err := sevenzip.NewReaderWithPassword(sra, size, z.Password)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// important to initialize to non-nil, empty value due to how fileIsIncluded works\n\tskipDirs := skipList{}\n\n\tfor i, f := range zr.File {\n\t\tf := f // make a copy for the Open closure\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\n\t\tif fileIsIncluded(skipDirs, f.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfi := f.FileInfo()\n\t\tfile := FileInfo{\n\t\t\tFileInfo:      fi,\n\t\t\tHeader:        f.FileHeader,\n\t\t\tNameInArchive: f.Name,\n\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\topenedFile, err := f.Open()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn fileInArchive{openedFile, fi}, nil\n\t\t\t},\n\t\t}\n\n\t\terr := handleFile(ctx, file)\n\t\tif errors.Is(err, fs.SkipDir) {\n\t\t\t// if a directory, skip this path; if a file, skip the folder path\n\t\t\tdirPath := f.Name\n\t\t\tif !file.IsDir() {\n\t\t\t\tdirPath = path.Dir(f.Name) + \"/\"\n\t\t\t}\n\t\t\tskipDirs.add(dirPath)\n\t\t} else if err != nil {\n\t\t\tif z.ContinueOnError {\n\t\t\t\tlog.Printf(\"[ERROR] %s: %v\", f.Name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"handling file %d: %s: %w\", i, f.Name, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// https://py7zr.readthedocs.io/en/latest/archive_format.html#signature\nvar sevenZipHeader = []byte(\"7z\\xBC\\xAF\\x27\\x1C\")\n\n// Interface guard\nvar _ Extractor = SevenZip{}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2016 Matthew Holt\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 0.466796875,
          "content": "# archiver v4 alpha (DEPRECATED)\n\nThis package is no longer developed; it has been superceded by **[mholt/archives](https://github.com/mholt/archives)** with an improved, more conventional exported API, significant performance improvements, and new features.\n\nThank you to [all 71 contributors](https://github.com/mholt/archiver/graphs/contributors) over the 8 years this version of the project was developed! It lives at **[mholt/archives](https://github.com/mholt/archives)**."
        },
        {
          "name": "archiver.go",
          "type": "blob",
          "size": 10.251953125,
          "content": "package archiver\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n)\n\n// FileInfo is a virtualized, generalized file abstraction for interacting with archives.\ntype FileInfo struct {\n\tfs.FileInfo\n\n\t// The file header as used/provided by the archive format.\n\t// Typically, you do not need to set this field when creating\n\t// an archive.\n\tHeader any\n\n\t// The path of the file as it appears in the archive.\n\t// This is equivalent to Header.Name (for most Header\n\t// types). We require it to be specified here because\n\t// it is such a common field and we want to preserve\n\t// format-agnosticism (no type assertions) for basic\n\t// operations.\n\t//\n\t// When extracting, this name or path may not have\n\t// been sanitized; it should not be trusted at face\n\t// value. Consider using path.Clean() before using.\n\t//\n\t// EXPERIMENTAL: If inserting a file into an archive,\n\t// and this is left blank, the implementation of the\n\t// archive format can default to using the file's base\n\t// name.\n\tNameInArchive string\n\n\t// For symbolic and hard links, the target of the link.\n\t// Not supported by all archive formats.\n\tLinkTarget string\n\n\t// A callback function that opens the file to read its\n\t// contents. The file must be closed when reading is\n\t// complete.\n\tOpen func() (fs.File, error)\n}\n\nfunc (f FileInfo) Stat() (fs.FileInfo, error) { return f.FileInfo, nil }\n\n// FilesFromDisk returns a list of files by walking the directories in the\n// given filenames map. The keys are the names on disk, and the values are\n// their associated names in the archive.\n//\n// Map keys that specify directories on disk will be walked and added to the\n// archive recursively, rooted at the named directory. They should use the\n// platform's path separator (backslash on Windows; slash on everything else).\n// For convenience, map keys that end in a separator ('/', or '\\' on Windows)\n// will enumerate contents only without adding the folder itself to the archive.\n//\n// Map values should typically use slash ('/') as the separator regardless of\n// the platform, as most archive formats standardize on that rune as the\n// directory separator for filenames within an archive. For convenience, map\n// values that are empty string are interpreted as the base name of the file\n// (sans path) in the root of the archive; and map values that end in a slash\n// will use the base name of the file in that folder of the archive.\n//\n// File gathering will adhere to the settings specified in options.\n//\n// This function is used primarily when preparing a list of files to add to\n// an archive.\nfunc FilesFromDisk(options *FromDiskOptions, filenames map[string]string) ([]FileInfo, error) {\n\tvar files []FileInfo\n\tfor rootOnDisk, rootInArchive := range filenames {\n\t\twalkErr := filepath.WalkDir(rootOnDisk, func(filename string, d fs.DirEntry, err error) error {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tinfo, err := d.Info()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tnameInArchive := nameOnDiskToNameInArchive(filename, rootOnDisk, rootInArchive)\n\t\t\t// this is the root folder and we are adding its contents to target rootInArchive\n\t\t\tif info.IsDir() && nameInArchive == \"\" {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// handle symbolic links\n\t\t\tvar linkTarget string\n\t\t\tif isSymlink(info) {\n\t\t\t\tif options != nil && options.FollowSymlinks {\n\t\t\t\t\t// dereference symlinks\n\t\t\t\t\tfilename, err = os.Readlink(filename)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"%s: readlink: %w\", filename, err)\n\t\t\t\t\t}\n\t\t\t\t\tinfo, err = os.Stat(filename)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"%s: statting dereferenced symlink: %w\", filename, err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// preserve symlinks\n\t\t\t\t\tlinkTarget, err = os.Readlink(filename)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"%s: readlink: %w\", filename, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// handle file attributes\n\t\t\tif options != nil && options.ClearAttributes {\n\t\t\t\tinfo = noAttrFileInfo{info}\n\t\t\t}\n\n\t\t\tfile := FileInfo{\n\t\t\t\tFileInfo:      info,\n\t\t\t\tNameInArchive: nameInArchive,\n\t\t\t\tLinkTarget:    linkTarget,\n\t\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\t\treturn os.Open(filename)\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tfiles = append(files, file)\n\t\t\treturn nil\n\t\t})\n\t\tif walkErr != nil {\n\t\t\treturn nil, walkErr\n\t\t}\n\t}\n\treturn files, nil\n}\n\n// nameOnDiskToNameInArchive converts a filename from disk to a name in an archive,\n// respecting rules defined by FilesFromDisk. nameOnDisk is the full filename on disk\n// which is expected to be prefixed by rootOnDisk (according to fs.WalkDirFunc godoc)\n// and which will be placed into a folder rootInArchive in the archive.\nfunc nameOnDiskToNameInArchive(nameOnDisk, rootOnDisk, rootInArchive string) string {\n\t// These manipulations of rootInArchive could be done just once instead of on\n\t// every walked file since they don't rely on nameOnDisk which is the only\n\t// variable that changes during the walk, but combining all the logic into this\n\t// one function is easier to reason about and test. I suspect the performance\n\t// penalty is insignificant.\n\tif strings.HasSuffix(rootOnDisk, string(filepath.Separator)) {\n\t\trootInArchive = trimTopDir(rootInArchive)\n\t} else if rootInArchive == \"\" {\n\t\trootInArchive = filepath.Base(rootOnDisk)\n\t}\n\tif strings.HasSuffix(rootInArchive, \"/\") {\n\t\trootInArchive += filepath.Base(rootOnDisk)\n\t}\n\ttruncPath := strings.TrimPrefix(nameOnDisk, rootOnDisk)\n\treturn path.Join(rootInArchive, filepath.ToSlash(truncPath))\n}\n\n// trimTopDir strips the top or first directory from the path.\n// It expects a forward-slashed path.\n//\n// For example, \"a/b/c\" => \"b/c\".\nfunc trimTopDir(dir string) string {\n\tif len(dir) > 0 && dir[0] == '/' {\n\t\tdir = dir[1:]\n\t}\n\tif pos := strings.Index(dir, \"/\"); pos >= 0 {\n\t\treturn dir[pos+1:]\n\t}\n\treturn dir\n}\n\n// topDir returns the top or first directory in the path.\n// It expects a forward-slashed path.\n//\n// For example, \"a/b/c\" => \"a\".\nfunc topDir(dir string) string {\n\tif len(dir) > 0 && dir[0] == '/' {\n\t\tdir = dir[1:]\n\t}\n\tif pos := strings.Index(dir, \"/\"); pos >= 0 {\n\t\treturn dir[:pos]\n\t}\n\treturn dir\n}\n\n// noAttrFileInfo is used to zero out some file attributes (issue #280).\ntype noAttrFileInfo struct{ fs.FileInfo }\n\n// Mode preserves only the type and permission bits.\nfunc (no noAttrFileInfo) Mode() fs.FileMode {\n\treturn no.FileInfo.Mode() & (fs.ModeType | fs.ModePerm)\n}\nfunc (noAttrFileInfo) ModTime() time.Time { return time.Time{} }\nfunc (noAttrFileInfo) Sys() any           { return nil }\n\n// FromDiskOptions specifies various options for gathering files from disk.\ntype FromDiskOptions struct {\n\t// If true, symbolic links will be dereferenced, meaning that\n\t// the link will not be added as a link, but what the link\n\t// points to will be added as a file.\n\tFollowSymlinks bool\n\n\t// If true, some file attributes will not be preserved.\n\t// Name, size, type, and permissions will still be preserved.\n\tClearAttributes bool\n}\n\n// FileHandler is a callback function that is used to handle files as they are read\n// from an archive; it is kind of like fs.WalkDirFunc. Handler functions that open\n// their files must not overlap or run concurrently, as files may be read from the\n// same sequential stream; always close the file before returning.\n//\n// If the special error value fs.SkipDir is returned, the directory of the file\n// (or the file itself if it is a directory) will not be walked. Note that because\n// archive contents are not necessarily ordered, skipping directories requires\n// memory, and skipping lots of directories may run up your memory bill.\n//\n// Any other returned error will terminate a walk and be returned to the caller.\ntype FileHandler func(ctx context.Context, info FileInfo) error\n\n// openAndCopyFile opens file for reading, copies its\n// contents to w, then closes file.\nfunc openAndCopyFile(file FileInfo, w io.Writer) error {\n\tfileReader, err := file.Open()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer fileReader.Close()\n\t// When file is in use and size is being written to, creating the compressed\n\t// file will fail with \"archive/tar: write too long.\" Using CopyN gracefully\n\t// handles this.\n\t_, err = io.Copy(w, fileReader)\n\tif err != nil && err != io.EOF {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// fileIsIncluded returns true if filename is included according to\n// filenameList; meaning it is in the list, its parent folder/path\n// is in the list, or the list is nil.\nfunc fileIsIncluded(filenameList []string, filename string) bool {\n\t// include all files if there is no specific list\n\tif filenameList == nil {\n\t\treturn true\n\t}\n\tfor _, fn := range filenameList {\n\t\t// exact matches are of course included\n\t\tif filename == fn {\n\t\t\treturn true\n\t\t}\n\t\t// also consider the file included if its parent folder/path is in the list\n\t\tif strings.HasPrefix(filename, strings.TrimSuffix(fn, \"/\")+\"/\") {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc isSymlink(info fs.FileInfo) bool {\n\treturn info.Mode()&os.ModeSymlink != 0\n}\n\n// streamSizeBySeeking determines the size of the stream by\n// seeking to the end, then back again, so the resulting\n// seek position upon returning is the same as when called\n// (assuming no errors).\nfunc streamSizeBySeeking(s io.Seeker) (int64, error) {\n\tcurrentPosition, err := s.Seek(0, io.SeekCurrent)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"getting current offset: %w\", err)\n\t}\n\tmaxPosition, err := s.Seek(0, io.SeekEnd)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"fast-forwarding to end: %w\", err)\n\t}\n\t_, err = s.Seek(currentPosition, io.SeekStart)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"returning to prior offset %d: %w\", currentPosition, err)\n\t}\n\treturn maxPosition, nil\n}\n\n// skipList keeps a list of non-intersecting paths\n// as long as its add method is used. Identical\n// elements are rejected, more specific paths are\n// replaced with broader ones, and more specific\n// paths won't be added when a broader one already\n// exists in the list. Trailing slashes are ignored.\ntype skipList []string\n\nfunc (s *skipList) add(dir string) {\n\ttrimmedDir := strings.TrimSuffix(dir, \"/\")\n\tvar dontAdd bool\n\tfor i := 0; i < len(*s); i++ {\n\t\ttrimmedElem := strings.TrimSuffix((*s)[i], \"/\")\n\t\tif trimmedDir == trimmedElem {\n\t\t\treturn\n\t\t}\n\t\t// don't add dir if a broader path already exists in the list\n\t\tif strings.HasPrefix(trimmedDir, trimmedElem+\"/\") {\n\t\t\tdontAdd = true\n\t\t\tcontinue\n\t\t}\n\t\t// if dir is broader than a path in the list, remove more specific path in list\n\t\tif strings.HasPrefix(trimmedElem, trimmedDir+\"/\") {\n\t\t\t*s = append((*s)[:i], (*s)[i+1:]...)\n\t\t\ti--\n\t\t}\n\t}\n\tif !dontAdd {\n\t\t*s = append(*s, dir)\n\t}\n}\n"
        },
        {
          "name": "archiver_test.go",
          "type": "blob",
          "size": 5.712890625,
          "content": "package archiver\n\nimport (\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestTrimTopDir(t *testing.T) {\n\tfor _, tc := range []struct {\n\t\tinput string\n\t\twant  string\n\t}{\n\t\t{input: \"a/b/c\", want: \"b/c\"},\n\t\t{input: \"a\", want: \"a\"},\n\t\t{input: \"abc/def\", want: \"def\"},\n\t\t{input: \"/abc/def\", want: \"def\"},\n\t} {\n\t\ttc := tc\n\t\tt.Run(tc.input, func(t *testing.T) {\n\t\t\tgot := trimTopDir(tc.input)\n\t\t\tif got != tc.want {\n\t\t\t\tt.Errorf(\"want: '%s', got: '%s')\", tc.want, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTopDir(t *testing.T) {\n\tfor _, tc := range []struct {\n\t\tinput string\n\t\twant  string\n\t}{\n\t\t{input: \"a/b/c\", want: \"a\"},\n\t\t{input: \"a\", want: \"a\"},\n\t\t{input: \"abc/def\", want: \"abc\"},\n\t\t{input: \"/abc/def\", want: \"abc\"},\n\t} {\n\t\ttc := tc\n\t\tt.Run(tc.input, func(t *testing.T) {\n\t\t\tgot := topDir(tc.input)\n\t\t\tif got != tc.want {\n\t\t\t\tt.Errorf(\"want: '%s', got: '%s')\", tc.want, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFileIsIncluded(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tincluded  []string\n\t\tcandidate string\n\t\texpect    bool\n\t}{\n\t\t{\n\t\t\tincluded:  []string{\"a\"},\n\t\t\tcandidate: \"a\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\", \"b\", \"a/b\"},\n\t\t\tcandidate: \"b\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\", \"b\", \"c/d\"},\n\t\t\tcandidate: \"c/d/e\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\"},\n\t\t\tcandidate: \"a/b/c\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\"},\n\t\t\tcandidate: \"aa/b/c\",\n\t\t\texpect:    false,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\", \"b\", \"c/d\"},\n\t\t\tcandidate: \"b/c\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a/\"},\n\t\t\tcandidate: \"a\",\n\t\t\texpect:    false,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a/\"},\n\t\t\tcandidate: \"a/\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a\"},\n\t\t\tcandidate: \"a/\",\n\t\t\texpect:    true,\n\t\t},\n\t\t{\n\t\t\tincluded:  []string{\"a/b\"},\n\t\t\tcandidate: \"a/\",\n\t\t\texpect:    false,\n\t\t},\n\t} {\n\t\tactual := fileIsIncluded(tc.included, tc.candidate)\n\t\tif actual != tc.expect {\n\t\t\tt.Errorf(\"Test %d (included=%v candidate=%v): expected %t but got %t\",\n\t\t\t\ti, tc.included, tc.candidate, tc.expect, actual)\n\t\t}\n\t}\n}\n\nfunc TestSkipList(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tstart  skipList\n\t\tadd    string\n\t\texpect skipList\n\t}{\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b\", \"c\"},\n\t\t\tadd:    \"d\",\n\t\t\texpect: skipList{\"a\", \"b\", \"c\", \"d\"},\n\t\t},\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b\", \"c\"},\n\t\t\tadd:    \"b\",\n\t\t\texpect: skipList{\"a\", \"b\", \"c\"},\n\t\t},\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b\", \"c\"},\n\t\t\tadd:    \"b/c\", // don't add because b implies b/c\n\t\t\texpect: skipList{\"a\", \"b\", \"c\"},\n\t\t},\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b\", \"c\"},\n\t\t\tadd:    \"b/c/\", // effectively same as above\n\t\t\texpect: skipList{\"a\", \"b\", \"c\"},\n\t\t},\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b/\", \"c\"},\n\t\t\tadd:    \"b\", // effectively same as b/\n\t\t\texpect: skipList{\"a\", \"b/\", \"c\"},\n\t\t},\n\t\t{\n\t\t\tstart:  skipList{\"a\", \"b/c\", \"c\"},\n\t\t\tadd:    \"b\", // replace b/c because b is broader\n\t\t\texpect: skipList{\"a\", \"c\", \"b\"},\n\t\t},\n\t} {\n\t\tstart := make(skipList, len(tc.start))\n\t\tcopy(start, tc.start)\n\n\t\ttc.start.add(tc.add)\n\n\t\tif !reflect.DeepEqual(tc.start, tc.expect) {\n\t\t\tt.Errorf(\"Test %d (start=%v add=%v): expected %v but got %v\",\n\t\t\t\ti, start, tc.add, tc.expect, tc.start)\n\t\t}\n\t}\n}\n\nfunc TestNameOnDiskToNameInArchive(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\twindows       bool   // only run this test on Windows\n\t\trootOnDisk    string // user says they want to archive this file/folder\n\t\tnameOnDisk    string // the walk encounters a file with this name (with rootOnDisk as a prefix)\n\t\trootInArchive string // file should be placed in this dir within the archive (rootInArchive becomes a prefix)\n\t\texpect        string // final filename in archive\n\t}{\n\t\t{\n\t\t\trootOnDisk:    \"a\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"\",\n\t\t\texpect:        \"a/b/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/b\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"\",\n\t\t\texpect:        \"b/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/b/\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"\",\n\t\t\texpect:        \"c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/b/\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \".\",\n\t\t\texpect:        \"c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/b/c\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"\",\n\t\t\texpect:        \"c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/b\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"foo\",\n\t\t\texpect:        \"foo/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"foo\",\n\t\t\texpect:        \"foo/b/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"foo/\",\n\t\t\texpect:        \"foo/a/b/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"foo\",\n\t\t\texpect:        \"foo/b/c\",\n\t\t},\n\t\t{\n\t\t\trootOnDisk:    \"a/\",\n\t\t\tnameOnDisk:    \"a/b/c\",\n\t\t\trootInArchive: \"foo\",\n\t\t\texpect:        \"foo/b/c\",\n\t\t},\n\t\t{\n\t\t\twindows:       true,\n\t\t\trootOnDisk:    `C:\\foo`,\n\t\t\tnameOnDisk:    `C:\\foo\\bar`,\n\t\t\trootInArchive: \"\",\n\t\t\texpect:        \"foo/bar\",\n\t\t},\n\t\t{\n\t\t\twindows:       true,\n\t\t\trootOnDisk:    `C:\\foo`,\n\t\t\tnameOnDisk:    `C:\\foo\\bar`,\n\t\t\trootInArchive: \"subfolder\",\n\t\t\texpect:        \"subfolder/bar\",\n\t\t},\n\t} {\n\t\tif !strings.HasPrefix(tc.nameOnDisk, tc.rootOnDisk) {\n\t\t\tt.Errorf(\"Test %d: Invalid test case! Filename (on disk) will have rootOnDisk as a prefix according to the fs.WalkDirFunc godoc.\", i)\n\t\t\tcontinue\n\t\t}\n\t\tif tc.windows && runtime.GOOS != \"windows\" {\n\t\t\tt.Logf(\"Test %d: Skipping test that is only compatible with Windows\", i)\n\t\t\tcontinue\n\t\t}\n\t\tif !tc.windows && runtime.GOOS == \"windows\" {\n\t\t\tt.Logf(\"Test %d: Skipping test that is not compatible with Windows\", i)\n\t\t\tcontinue\n\t\t}\n\n\t\tactual := nameOnDiskToNameInArchive(tc.nameOnDisk, tc.rootOnDisk, tc.rootInArchive)\n\t\tif actual != tc.expect {\n\t\t\tt.Errorf(\"Test %d: Got '%s' but expected '%s' (nameOnDisk=%s rootOnDisk=%s rootInArchive=%s)\",\n\t\t\t\ti, actual, tc.expect, tc.nameOnDisk, tc.rootOnDisk, tc.rootInArchive)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "brotli.go",
          "type": "blob",
          "size": 1.119140625,
          "content": "package archiver\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/andybalholm/brotli\"\n)\n\nfunc init() {\n\tRegisterFormat(Brotli{})\n}\n\n// Brotli facilitates brotli compression.\ntype Brotli struct {\n\tQuality int\n}\n\nfunc (Brotli) Extension() string { return \".br\" }\n\nfunc (br Brotli) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), br.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// brotli does not have well-defined file headers or a magic number;\n\t// the best way to match the stream is probably to try decoding part\n\t// of it, but we'll just have to guess a large-enough size that is\n\t// still small enough for the smallest streams we'll encounter\n\tr := brotli.NewReader(stream)\n\tbuf := make([]byte, 16)\n\tif _, err := io.ReadFull(r, buf); err == nil {\n\t\tmr.ByStream = true\n\t}\n\n\treturn mr, nil\n}\n\nfunc (br Brotli) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\treturn brotli.NewWriterLevel(w, br.Quality), nil\n}\n\nfunc (Brotli) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\treturn io.NopCloser(brotli.NewReader(r)), nil\n}\n"
        },
        {
          "name": "bz2.go",
          "type": "blob",
          "size": 0.9267578125,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/dsnet/compress/bzip2\"\n)\n\nfunc init() {\n\tRegisterFormat(Bz2{})\n}\n\n// Bz2 facilitates bzip2 compression.\ntype Bz2 struct {\n\tCompressionLevel int\n}\n\nfunc (Bz2) Extension() string { return \".bz2\" }\n\nfunc (bz Bz2) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), bz.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(bzip2Header))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, bzip2Header)\n\n\treturn mr, nil\n}\n\nfunc (bz Bz2) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\treturn bzip2.NewWriter(w, &bzip2.WriterConfig{\n\t\tLevel: bz.CompressionLevel,\n\t})\n}\n\nfunc (Bz2) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\treturn bzip2.NewReader(r, nil)\n}\n\nvar bzip2Header = []byte(\"BZh\")\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "formats.go",
          "type": "blob",
          "size": 12.845703125,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\n// RegisterFormat registers a format. It should be called during init.\n// Duplicate formats by name are not allowed and will panic.\nfunc RegisterFormat(format Format) {\n\tname := strings.Trim(strings.ToLower(format.Extension()), \".\")\n\tif _, ok := formats[name]; ok {\n\t\tpanic(\"format \" + name + \" is already registered\")\n\t}\n\tformats[name] = format\n}\n\n// Identify iterates the registered formats and returns the one that\n// matches the given filename and/or stream. It is capable of identifying\n// compressed files (.gz, .xz...), archive files (.tar, .zip...), and\n// compressed archive files (tar.gz, tar.bz2...). The returned Format\n// value can be type-asserted to ascertain its capabilities.\n//\n// If no matching formats were found, special error ErrNoMatch is returned.\n//\n// If stream is nil then it will only match on file name and the\n// returned io.Reader will be nil.\n//\n// If stream is non-nil then the returned io.Reader will always be\n// non-nil and will read from the same point as the reader which was\n// passed in. If the input stream is not an io.Seeker, the returned\n// io.Reader value should be used in place of the input stream after\n// calling Identify() because it preserves and re-reads the bytes that\n// were already read during the identification process.\n//\n// If the input stream is an io.Seeker, Seek() must work, and the\n// original input value will be returned instead of a wrapper value.\nfunc Identify(ctx context.Context, filename string, stream io.Reader) (Format, io.Reader, error) {\n\tvar compression Compression\n\tvar archival Archival\n\tvar extraction Extraction\n\n\trewindableStream, err := newRewindReader(stream)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// try compression format first, since that's the outer \"layer\" if combined\n\tfor name, format := range formats {\n\t\tcf, isCompression := format.(Compression)\n\t\tif !isCompression {\n\t\t\tcontinue\n\t\t}\n\n\t\tmatchResult, err := identifyOne(ctx, format, filename, rewindableStream, nil)\n\t\tif err != nil {\n\t\t\treturn nil, rewindableStream.reader(), fmt.Errorf(\"matching %s: %w\", name, err)\n\t\t}\n\n\t\t// if matched, wrap input stream with decompression\n\t\t// so we can see if it contains an archive within\n\t\tif matchResult.Matched() {\n\t\t\tcompression = cf\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// try archival and extraction format next\n\tfor name, format := range formats {\n\t\tar, isArchive := format.(Archival)\n\t\tex, isExtract := format.(Extraction)\n\t\tif !isArchive && !isExtract {\n\t\t\tcontinue\n\t\t}\n\n\t\tmatchResult, err := identifyOne(ctx, format, filename, rewindableStream, compression)\n\t\tif err != nil {\n\t\t\treturn nil, rewindableStream.reader(), fmt.Errorf(\"matching %s: %w\", name, err)\n\t\t}\n\n\t\tif matchResult.Matched() {\n\t\t\tarchival = ar\n\t\t\textraction = ex\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// the stream should be rewound by identifyOne; then return the most specific type of match\n\tbufferedStream := rewindableStream.reader()\n\tswitch {\n\tcase compression != nil && archival == nil && extraction == nil:\n\t\treturn compression, bufferedStream, nil\n\tcase compression == nil && archival != nil && extraction == nil:\n\t\treturn archival, bufferedStream, nil\n\tcase compression == nil && archival == nil && extraction != nil:\n\t\treturn extraction, bufferedStream, nil\n\tcase archival != nil || extraction != nil:\n\t\treturn Archive{compression, archival, extraction}, bufferedStream, nil\n\tdefault:\n\t\treturn nil, bufferedStream, NoMatch\n\t}\n}\n\nfunc identifyOne(ctx context.Context, format Format, filename string, stream *rewindReader, comp Compression) (mr MatchResult, err error) {\n\tdefer stream.rewind()\n\n\tif filename == \".\" {\n\t\tfilename = \"\"\n\t}\n\n\t// if looking within a compressed format, wrap the stream in a\n\t// reader that can decompress it so we can match the \"inner\" format\n\t// (yes, we have to make a new reader every time we do a match,\n\t// because we reset/seek the stream each time and that can mess up\n\t// the compression reader's state if we don't discard it also)\n\tif comp != nil && stream != nil {\n\t\tdecompressedStream, openErr := comp.OpenReader(stream)\n\t\tif openErr != nil {\n\t\t\treturn MatchResult{}, openErr\n\t\t}\n\t\tdefer decompressedStream.Close()\n\t\tmr, err = format.Match(ctx, filename, decompressedStream)\n\t} else {\n\t\t// Make sure we pass a nil io.Reader not a *rewindReader(nil)\n\t\tvar r io.Reader\n\t\tif stream != nil {\n\t\t\tr = stream\n\t\t}\n\t\tmr, err = format.Match(ctx, filename, r)\n\t}\n\n\t// if the error is EOF, we can just ignore it.\n\t// Just means we have a small input file.\n\tif errors.Is(err, io.EOF) {\n\t\terr = nil\n\t}\n\treturn mr, err\n}\n\n// readAtMost reads at most n bytes from the stream. A nil, empty, or short\n// stream is not an error. The returned slice of bytes may have length < n\n// without an error.\nfunc readAtMost(stream io.Reader, n int) ([]byte, error) {\n\tif stream == nil || n <= 0 {\n\t\treturn []byte{}, nil\n\t}\n\n\tbuf := make([]byte, n)\n\tnr, err := io.ReadFull(stream, buf)\n\n\t// Return the bytes read if there was no error OR if the\n\t// error was EOF (stream was empty) or UnexpectedEOF (stream\n\t// had less than n). We ignore those errors because we aren't\n\t// required to read the full n bytes; so an empty or short\n\t// stream is not actually an error.\n\tif err == nil ||\n\t\terrors.Is(err, io.EOF) ||\n\t\terrors.Is(err, io.ErrUnexpectedEOF) {\n\t\treturn buf[:nr], nil\n\t}\n\n\treturn nil, err\n}\n\n// Archive represents an archive which may be compressed at the outer layer.\n// It combines a compression format on top of an archive/extraction\n// format (e.g. \".tar.gz\") and provides both functionalities in a single\n// type. It ensures that archival functions are wrapped by compressors and\n// decompressors. However, compressed archives have some limitations; for\n// example, files cannot be inserted/appended because of complexities with\n// modifying existing compression state (perhaps this could be overcome,\n// but I'm not about to try it).\n//\n// The embedded Archival and Extraction values are used for writing and\n// reading, respectively. Compression is optional and is only needed if the\n// format is compressed externally (for example, tar archives).\ntype Archive struct {\n\tCompression\n\tArchival\n\tExtraction\n}\n\n// Name returns a concatenation of the archive and compression format extensions.\nfunc (ar Archive) Extension() string {\n\tvar name string\n\tif ar.Archival != nil {\n\t\tname += ar.Archival.Extension()\n\t} else if ar.Extraction != nil {\n\t\tname += ar.Extraction.Extension()\n\t}\n\tif ar.Compression != nil {\n\t\tname += ar.Compression.Extension()\n\t}\n\treturn name\n}\n\n// Match matches if the input matches both the compression and archival/extraction format.\nfunc (ar Archive) Match(ctx context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar conglomerate MatchResult\n\n\tif ar.Compression != nil {\n\t\tmatchResult, err := ar.Compression.Match(ctx, filename, stream)\n\t\tif err != nil {\n\t\t\treturn MatchResult{}, err\n\t\t}\n\t\tif !matchResult.Matched() {\n\t\t\treturn matchResult, nil\n\t\t}\n\n\t\t// wrap the reader with the decompressor so we can\n\t\t// attempt to match the archive by reading the stream\n\t\trc, err := ar.Compression.OpenReader(stream)\n\t\tif err != nil {\n\t\t\treturn matchResult, err\n\t\t}\n\t\tdefer rc.Close()\n\t\tstream = rc\n\n\t\tconglomerate = matchResult\n\t}\n\n\tif ar.Archival != nil {\n\t\tmatchResult, err := ar.Archival.Match(ctx, filename, stream)\n\t\tif err != nil {\n\t\t\treturn MatchResult{}, err\n\t\t}\n\t\tif !matchResult.Matched() {\n\t\t\treturn matchResult, nil\n\t\t}\n\t\tconglomerate.ByName = conglomerate.ByName || matchResult.ByName\n\t\tconglomerate.ByStream = conglomerate.ByStream || matchResult.ByStream\n\t}\n\n\treturn conglomerate, nil\n}\n\n// Archive adds files to the output archive while compressing the result.\nfunc (ar Archive) Archive(ctx context.Context, output io.Writer, files []FileInfo) error {\n\tif ar.Archival == nil {\n\t\treturn fmt.Errorf(\"no archival format\")\n\t}\n\tif ar.Compression != nil {\n\t\twc, err := ar.Compression.OpenWriter(output)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer wc.Close()\n\t\toutput = wc\n\t}\n\treturn ar.Archival.Archive(ctx, output, files)\n}\n\n// ArchiveAsync adds files to the output archive while compressing the result asynchronously.\nfunc (ar Archive) ArchiveAsync(ctx context.Context, output io.Writer, jobs <-chan ArchiveAsyncJob) error {\n\tif ar.Archival == nil {\n\t\treturn fmt.Errorf(\"no archival format\")\n\t}\n\tdo, ok := ar.Archival.(ArchiverAsync)\n\tif !ok {\n\t\treturn fmt.Errorf(\"%T archive does not support async writing\", ar.Archival)\n\t}\n\tif ar.Compression != nil {\n\t\twc, err := ar.Compression.OpenWriter(output)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer wc.Close()\n\t\toutput = wc\n\t}\n\treturn do.ArchiveAsync(ctx, output, jobs)\n}\n\n// Extract reads files out of an archive while decompressing the results.\nfunc (ar Archive) Extract(ctx context.Context, sourceArchive io.Reader, handleFile FileHandler) error {\n\tif ar.Extraction == nil {\n\t\treturn fmt.Errorf(\"no extraction format\")\n\t}\n\tif ar.Compression != nil {\n\t\trc, err := ar.Compression.OpenReader(sourceArchive)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer rc.Close()\n\t\tsourceArchive = rc\n\t}\n\treturn ar.Extraction.Extract(ctx, sourceArchive, handleFile)\n}\n\n// MatchResult returns true if the format was matched either\n// by name, stream, or both. Name usually refers to matching\n// by file extension, and stream usually refers to reading\n// the first few bytes of the stream (its header). A stream\n// match is generally stronger, as filenames are not always\n// indicative of their contents if they even exist at all.\ntype MatchResult struct {\n\tByName, ByStream bool\n}\n\n// Matched returns true if a match was made by either name or stream.\nfunc (mr MatchResult) Matched() bool { return mr.ByName || mr.ByStream }\n\n// rewindReader is a Reader that can be rewound (reset) to re-read what\n// was already read and then continue to read more from the underlying\n// stream. When no more rewinding is necessary, call reader() to get a\n// new reader that first reads the buffered bytes, then continues to\n// read from the stream. This is useful for \"peeking\" a stream an\n// arbitrary number of bytes. Loosely based on the Connection type\n// from https://github.com/mholt/caddy-l4.\n//\n// If the reader is also an io.Seeker, no buffer is used, and instead\n// the stream seeks back to the starting position.\ntype rewindReader struct {\n\tio.Reader\n\tstart     int64\n\tbuf       *bytes.Buffer\n\tbufReader io.Reader\n}\n\nfunc newRewindReader(r io.Reader) (*rewindReader, error) {\n\tif r == nil {\n\t\treturn nil, nil\n\t}\n\n\trr := &rewindReader{Reader: r}\n\n\t// avoid buffering if we have a seeker we can use\n\tif seeker, ok := r.(io.Seeker); ok {\n\t\tvar err error\n\t\trr.start, err = seeker.Seek(0, io.SeekCurrent)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"seek to determine current position: %w\", err)\n\t\t}\n\t} else {\n\t\trr.buf = new(bytes.Buffer)\n\t}\n\n\treturn rr, nil\n}\n\nfunc (rr *rewindReader) Read(p []byte) (n int, err error) {\n\tif rr == nil {\n\t\tpanic(\"reading from nil rewindReader\")\n\t}\n\n\t// if there is a buffer we should read from, start\n\t// with that; we only read from the underlying stream\n\t// after the buffer has been \"depleted\"\n\tif rr.bufReader != nil {\n\t\tn, err = rr.bufReader.Read(p)\n\t\tif err == io.EOF {\n\t\t\trr.bufReader = nil\n\t\t\terr = nil\n\t\t}\n\t\tif n == len(p) {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// buffer has been depleted or we are not using one,\n\t// so read from underlying stream\n\tnr, err := rr.Reader.Read(p[n:])\n\n\t// anything that was read needs to be written to\n\t// the buffer (if used), even if there was an error\n\tif nr > 0 && rr.buf != nil {\n\t\tif nw, errw := rr.buf.Write(p[n : n+nr]); errw != nil {\n\t\t\treturn nw, errw\n\t\t}\n\t}\n\n\t// up to now, n was how many bytes were read from\n\t// the buffer, and nr was how many bytes were read\n\t// from the stream; add them to return total count\n\tn += nr\n\n\treturn\n}\n\n// rewind resets the stream to the beginning by causing\n// Read() to start reading from the beginning of the\n// stream, or, if buffering, the buffered bytes.\nfunc (rr *rewindReader) rewind() {\n\tif rr == nil {\n\t\treturn\n\t}\n\tif ras, ok := rr.Reader.(io.Seeker); ok {\n\t\tif _, err := ras.Seek(rr.start, io.SeekStart); err == nil {\n\t\t\treturn\n\t\t}\n\t}\n\trr.bufReader = bytes.NewReader(rr.buf.Bytes())\n}\n\n// reader returns a reader that reads first from the buffered\n// bytes (if buffering), then from the underlying stream; if a\n// Seeker, the stream will be seeked back to the start. After\n// calling this, no more rewinding is allowed since reads from\n// the stream are not recorded, so rewinding properly is impossible.\n// If the underlying reader implements io.Seeker, then the\n// underlying reader will be used directly.\nfunc (rr *rewindReader) reader() io.Reader {\n\tif rr == nil {\n\t\treturn nil\n\t}\n\tif ras, ok := rr.Reader.(io.Seeker); ok {\n\t\tif _, err := ras.Seek(rr.start, io.SeekStart); err == nil {\n\t\t\treturn rr.Reader\n\t\t}\n\t}\n\treturn io.MultiReader(bytes.NewReader(rr.buf.Bytes()), rr.Reader)\n}\n\n// NoMatch is a special error returned if there are no matching formats.\nvar NoMatch = fmt.Errorf(\"no formats matched\")\n\n// Registered formats.\nvar formats = make(map[string]Format)\n\n// Interface guards\nvar (\n\t_ Format        = (*Archive)(nil)\n\t_ Archiver      = (*Archive)(nil)\n\t_ ArchiverAsync = (*Archive)(nil)\n\t_ Extractor     = (*Archive)(nil)\n)\n"
        },
        {
          "name": "formats_test.go",
          "type": "blob",
          "size": 12.630859375,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"io/fs\"\n\t\"math/rand\"\n\t\"os\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestRewindReader(t *testing.T) {\n\tdata := \"the header\\nthe body\\n\"\n\n\tr, err := newRewindReader(strings.NewReader(data))\n\tif err != nil {\n\t\tt.Errorf(\"creating rewindReader: %v\", err)\n\t}\n\n\tbuf := make([]byte, 10) // enough for 'the header'\n\n\t// test rewinding reads\n\tfor i := 0; i < 10; i++ {\n\t\tr.rewind()\n\t\tn, err := r.Read(buf)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Read failed: %s\", err)\n\t\t}\n\t\tif string(buf[:n]) != \"the header\" {\n\t\t\tt.Errorf(\"iteration %d: expected 'the header' but got '%s' (n=%d)\", i, string(buf[:n]), n)\n\t\t}\n\t}\n\n\t// get the reader from header reader and make sure we can read all of the data out\n\tr.rewind()\n\tfinalReader := r.reader()\n\tbuf = make([]byte, len(data))\n\tn, err := io.ReadFull(finalReader, buf)\n\tif err != nil {\n\t\tt.Errorf(\"ReadFull failed: %s (n=%d)\", err, n)\n\t}\n\tif string(buf) != data {\n\t\tt.Errorf(\"expected '%s' but got '%s'\", string(data), string(buf))\n\t}\n}\n\nfunc TestCompression(t *testing.T) {\n\tseed := time.Now().UnixNano()\n\tt.Logf(\"seed: %d\", seed)\n\tr := rand.New(rand.NewSource(seed))\n\n\tcontents := make([]byte, 1024)\n\tr.Read(contents)\n\n\tcompressed := new(bytes.Buffer)\n\n\ttestOK := func(t *testing.T, comp Compression, testFilename string) {\n\t\t// compress into buffer\n\t\tcompressed.Reset()\n\t\twc, err := comp.OpenWriter(compressed)\n\t\tcheckErr(t, err, \"opening writer\")\n\t\t_, err = wc.Write(contents)\n\t\tcheckErr(t, err, \"writing contents\")\n\t\tcheckErr(t, wc.Close(), \"closing writer\")\n\n\t\t// make sure Identify correctly chooses this compression method\n\t\tformat, stream, err := Identify(context.Background(), testFilename, compressed)\n\t\tcheckErr(t, err, \"identifying\")\n\t\tif format.Extension() != comp.Extension() {\n\t\t\tt.Errorf(\"expected format %s but got %s\", comp.Extension(), format.Extension())\n\t\t}\n\n\t\t// read the contents back out and compare\n\t\tdecompReader, err := format.(Decompressor).OpenReader(stream)\n\t\tcheckErr(t, err, \"opening with decompressor '%s'\", format.Extension())\n\t\tdata, err := io.ReadAll(decompReader)\n\t\tcheckErr(t, err, \"reading decompressed data\")\n\t\tcheckErr(t, decompReader.Close(), \"closing decompressor\")\n\t\tif !bytes.Equal(data, contents) {\n\t\t\tt.Errorf(\"not equal to original\")\n\t\t}\n\t}\n\n\tvar cannotIdentifyFromStream = map[string]bool{Brotli{}.Extension(): true}\n\n\tfor _, f := range formats {\n\t\t// only test compressors\n\t\tcomp, ok := f.(Compression)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tt.Run(f.Extension()+\"_with_extension\", func(t *testing.T) {\n\t\t\ttestOK(t, comp, \"file\"+f.Extension())\n\t\t})\n\t\tif !cannotIdentifyFromStream[f.Extension()] {\n\t\t\tt.Run(f.Extension()+\"_without_extension\", func(t *testing.T) {\n\t\t\t\ttestOK(t, comp, \"\")\n\t\t\t})\n\t\t}\n\t}\n}\n\nfunc checkErr(t *testing.T, err error, msgFmt string, args ...any) {\n\tt.Helper()\n\tif err == nil {\n\t\treturn\n\t}\n\targs = append(args, err)\n\tt.Fatalf(msgFmt+\": %s\", args...)\n}\n\nfunc TestIdentifyDoesNotMatchContentFromTrimmedKnownHeaderHaving0Suffix(t *testing.T) {\n\t// Using the outcome of `n, err := io.ReadFull(stream, buf)` without minding n\n\t// may lead to a mis-characterization for cases with known header ending with 0x0\n\t// because the default byte value in a declared array is 0.\n\t// This test guards against those cases.\n\ttests := []struct {\n\t\tname   string\n\t\theader []byte\n\t}{\n\t\t{\n\t\t\tname:   \"rar_v5.0\",\n\t\t\theader: rarHeaderV5_0,\n\t\t},\n\t\t{\n\t\t\tname:   \"rar_v1.5\",\n\t\t\theader: rarHeaderV1_5,\n\t\t},\n\t\t{\n\t\t\tname:   \"xz\",\n\t\t\theader: xzHeader,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\theaderLen := len(tt.header)\n\t\t\tif headerLen == 0 || tt.header[headerLen-1] != 0 {\n\t\t\t\tt.Errorf(\"header expected to end with 0: header=%v\", tt.header)\n\t\t\t\treturn\n\t\t\t}\n\t\t\theaderTrimmed := tt.header[:headerLen-1]\n\t\t\tstream := bytes.NewReader(headerTrimmed)\n\t\t\tgot, _, err := Identify(context.Background(), \"\", stream)\n\t\t\tif got != nil {\n\t\t\t\tt.Errorf(\"no Format expected for trimmed know %s header: found Format= %v\", tt.name, got.Extension())\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !errors.Is(err, NoMatch) {\n\t\t\t\tt.Errorf(\"NoMatch expected for for trimmed know %s header: err :=%#v\", tt.name, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t})\n\t}\n}\n\nfunc TestIdentifyCanAssessSmallOrNoContent(t *testing.T) {\n\ttype args struct {\n\t\tstream io.ReadSeeker\n\t}\n\ttests := []struct {\n\t\tname string\n\t\targs args\n\t}{\n\t\t{\n\t\t\tname: \"should return nomatch for an empty stream\",\n\t\t\targs: args{\n\t\t\t\tstream: bytes.NewReader([]byte{}),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"should return nomatch for a stream with content size less than known header\",\n\t\t\targs: args{\n\t\t\t\tstream: bytes.NewReader([]byte{'a'}),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"should return nomatch for a stream with content size greater then known header size and not supported format\",\n\t\t\targs: args{\n\t\t\t\tstream: bytes.NewReader([]byte(strings.Repeat(\"this is a txt content\", 2))),\n\t\t\t},\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, _, err := Identify(context.Background(), \"\", tt.args.stream)\n\t\t\tif got != nil {\n\t\t\t\tt.Errorf(\"no Format expected for non archive and not compressed stream: found Format=%#v\", got)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !errors.Is(err, NoMatch) {\n\t\t\t\tt.Errorf(\"NoMatch expected for non archive and not compressed stream: %#v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t})\n\t}\n}\n\nfunc compress(\n\tt *testing.T, compName string, content []byte,\n\topenwriter func(w io.Writer) (io.WriteCloser, error),\n) []byte {\n\tbuf := bytes.NewBuffer(make([]byte, 0, 128))\n\tcwriter, err := openwriter(buf)\n\tif err != nil {\n\t\tt.Errorf(\"fail to open compression writer: compression-name=%s, err=%#v\", compName, err)\n\t\treturn nil\n\t}\n\t_, err = cwriter.Write(content)\n\tif err != nil {\n\t\tcerr := cwriter.Close()\n\t\tt.Errorf(\n\t\t\t\"fail to write using compression writer: compression-name=%s, err=%#v, close-err=%#v\",\n\t\t\tcompName, err, cerr)\n\t\treturn nil\n\t}\n\terr = cwriter.Close()\n\tif err != nil {\n\t\tt.Errorf(\"fail to close compression writer: compression-name=%s, err=%#v\", compName, err)\n\t\treturn nil\n\t}\n\treturn buf.Bytes()\n}\n\nfunc archive(t *testing.T, arch Archiver, fname string, fileInfo fs.FileInfo) []byte {\n\tfiles := []FileInfo{\n\t\t{FileInfo: fileInfo, NameInArchive: \"tmp.txt\",\n\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\treturn os.Open(fname)\n\t\t\t}},\n\t}\n\tbuf := bytes.NewBuffer(make([]byte, 0, 128))\n\terr := arch.Archive(context.TODO(), buf, files)\n\tif err != nil {\n\t\tt.Errorf(\"fail to create archive: err=%#v\", err)\n\t\treturn nil\n\t}\n\treturn buf.Bytes()\n\n}\n\ntype writeNopCloser struct{ io.Writer }\n\nfunc (wnc writeNopCloser) Close() error { return nil }\n\nfunc newWriteNopCloser(w io.Writer) (io.WriteCloser, error) {\n\treturn writeNopCloser{w}, nil\n}\n\nfunc newTmpTextFile(t *testing.T, content string) (string, fs.FileInfo) {\n\ttmpTxtFile, err := os.CreateTemp(\"\", \"TestIdentifyFindFormatByStreamContent-tmp-*.txt\")\n\tif err != nil {\n\t\tt.Errorf(\"fail to create tmp test file for archive tests: err=%v\", err)\n\t\treturn \"\", nil\n\t}\n\tfname := tmpTxtFile.Name()\n\n\tif _, err = tmpTxtFile.Write([]byte(content)); err != nil {\n\t\tt.Errorf(\"fail to write content to tmp-txt-file: err=%#v\", err)\n\t\treturn \"\", nil\n\t}\n\tif err = tmpTxtFile.Close(); err != nil {\n\t\tt.Errorf(\"fail to close tmp-txt-file: err=%#v\", err)\n\t\treturn \"\", nil\n\t}\n\tfi, err := os.Stat(fname)\n\tif err != nil {\n\t\tt.Errorf(\"fail to get tmp-txt-file stats: err=%v\", err)\n\t\treturn \"\", nil\n\t}\n\n\treturn fname, fi\n}\n\nfunc TestIdentifyFindFormatByStreamContent(t *testing.T) {\n\ttmpTxtFileName, tmpTxtFileInfo := newTmpTextFile(t, \"this is text that has to be long enough for brotli to match\")\n\tt.Cleanup(func() {\n\t\tos.RemoveAll(tmpTxtFileName)\n\t})\n\n\ttests := []struct {\n\t\tname                  string\n\t\tcontent               []byte\n\t\topenCompressionWriter func(w io.Writer) (io.WriteCloser, error)\n\t\tcompressorName        string\n\t\twantFormatName        string\n\t}{\n\t\t{\n\t\t\tname:                  \"should recognize brotli\",\n\t\t\topenCompressionWriter: Brotli{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text, but it has to be long enough to match brotli which doesn't have a magic number\"),\n\t\t\tcompressorName:        \".br\",\n\t\t\twantFormatName:        \".br\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize bz2\",\n\t\t\topenCompressionWriter: Bz2{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".bz2\",\n\t\t\twantFormatName:        \".bz2\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize gz\",\n\t\t\topenCompressionWriter: Gz{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".gz\",\n\t\t\twantFormatName:        \".gz\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize lz4\",\n\t\t\topenCompressionWriter: Lz4{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".lz4\",\n\t\t\twantFormatName:        \".lz4\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize lz\",\n\t\t\topenCompressionWriter: Lzip{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".lz\",\n\t\t\twantFormatName:        \".lz\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize sz\",\n\t\t\topenCompressionWriter: Sz{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".sz\",\n\t\t\twantFormatName:        \".sz\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize xz\",\n\t\t\topenCompressionWriter: Xz{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".xz\",\n\t\t\twantFormatName:        \".xz\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize zst\",\n\t\t\topenCompressionWriter: Zstd{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".zst\",\n\t\t\twantFormatName:        \".zst\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize tar\",\n\t\t\topenCompressionWriter: newWriteNopCloser,\n\t\t\tcontent:               archive(t, Tar{}, tmpTxtFileName, tmpTxtFileInfo),\n\t\t\tcompressorName:        \"\",\n\t\t\twantFormatName:        \".tar\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize tar.gz\",\n\t\t\topenCompressionWriter: Gz{}.OpenWriter,\n\t\t\tcontent:               archive(t, Tar{}, tmpTxtFileName, tmpTxtFileInfo),\n\t\t\tcompressorName:        \".gz\",\n\t\t\twantFormatName:        \".tar.gz\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize zip\",\n\t\t\topenCompressionWriter: newWriteNopCloser,\n\t\t\tcontent:               archive(t, Zip{}, tmpTxtFileName, tmpTxtFileInfo),\n\t\t\tcompressorName:        \"\",\n\t\t\twantFormatName:        \".zip\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize rar by v5.0 header\",\n\t\t\topenCompressionWriter: newWriteNopCloser,\n\t\t\tcontent:               rarHeaderV5_0[:],\n\t\t\tcompressorName:        \"\",\n\t\t\twantFormatName:        \".rar\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize rar by v1.5 header\",\n\t\t\topenCompressionWriter: newWriteNopCloser,\n\t\t\tcontent:               rarHeaderV1_5[:],\n\t\t\tcompressorName:        \"\",\n\t\t\twantFormatName:        \".rar\",\n\t\t},\n\t\t{\n\t\t\tname:                  \"should recognize zz\",\n\t\t\topenCompressionWriter: Zlib{}.OpenWriter,\n\t\t\tcontent:               []byte(\"this is text\"),\n\t\t\tcompressorName:        \".zz\",\n\t\t\twantFormatName:        \".zz\",\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tstream := bytes.NewReader(compress(t, tt.compressorName, tt.content, tt.openCompressionWriter))\n\t\t\tgot, _, err := Identify(context.Background(), \"\", stream)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"should have found a corresponding Format, but got err=%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif tt.wantFormatName != got.Extension() {\n\t\t\t\tt.Errorf(\"unexpected format found: expected=%s actual=%s\", tt.wantFormatName, got.Extension())\n\t\t\t\treturn\n\t\t\t}\n\n\t\t})\n\t}\n}\n\nfunc TestIdentifyAndOpenZip(t *testing.T) {\n\tf, err := os.Open(\"testdata/test.zip\")\n\tcheckErr(t, err, \"opening zip\")\n\tdefer f.Close()\n\n\tformat, reader, err := Identify(context.Background(), \"test.zip\", f)\n\tcheckErr(t, err, \"identifying zip\")\n\tif format.Extension() != \".zip\" {\n\t\tt.Errorf(\"unexpected format found: expected=.zip actual=%s\", format.Extension())\n\t}\n\n\terr = format.(Extractor).Extract(context.Background(), reader, func(ctx context.Context, f FileInfo) error {\n\t\trc, err := f.Open()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer rc.Close()\n\t\t_, err = io.ReadAll(rc)\n\t\treturn err\n\t})\n\tcheckErr(t, err, \"extracting zip\")\n}\n\nfunc TestIdentifyASCIIFileStartingWithX(t *testing.T) {\n\t// Create a temporary file starting with the letter 'x'\n\ttmpFile, err := os.CreateTemp(\"\", \"TestIdentifyASCIIFileStartingWithX-tmp-*.txt\")\n\tif err != nil {\n\t\tt.Errorf(\"fail to create tmp test file for archive tests: err=%v\", err)\n\t}\n\tdefer os.Remove(tmpFile.Name())\n\n\t_, err = tmpFile.Write([]byte(\"xThis is a test file\"))\n\tif err != nil {\n\t\tt.Errorf(\"Failed to write to temp file: %v\", err)\n\t}\n\ttmpFile.Close()\n\n\t// Open the file and use the Identify function\n\tfile, err := os.Open(tmpFile.Name())\n\tif err != nil {\n\t\tt.Errorf(\"Failed to open temp file: %v\", err)\n\t}\n\tdefer file.Close()\n\n\t_, _, err = Identify(context.Background(), tmpFile.Name(), file)\n\tif !errors.Is(err, NoMatch) {\n\t\tt.Errorf(\"Identify failed: %v\", err)\n\t}\n\n}\n"
        },
        {
          "name": "fs.go",
          "type": "blob",
          "size": 27.4892578125,
          "content": "package archiver\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"slices\"\n\t\"strings\"\n\t\"time\"\n)\n\n// FileSystem identifies the format of the input and returns a read-only file system.\n// The input can be a filename, stream, or both.\n//\n// If only a filename is specified, it may be a path to a directory, archive file,\n// compressed archive file, compressed regular file, or any other regular file on\n// disk. If the filename is a directory, its contents are accessed directly from\n// the device's file system. If the filename is an archive file, the contents can\n// be accessed like a normal directory; compressed archive files are transparently\n// decompressed as contents are accessed. And if the filename is any other file, it\n// is the only file in the returned file system; if the file is compressed, it is\n// transparently decompressed when read from.\n//\n// If a stream is specified, the filename (if available) is used as a hint to help\n// identify its format. Streams of archive files must be able to be made into an\n// io.SectionReader (for safe concurrency) which requires io.ReaderAt and io.Seeker\n// (to efficiently determine size). The automatic format identification requires\n// io.Reader and will use io.Seeker if supported to avoid buffering.\n//\n// Whether the data comes from disk or a stream, it is peeked at to automatically\n// detect which format to use.\n//\n// This function essentially offers uniform read access to various kinds of files:\n// directories, archives, compressed archives, individual files, and file streams\n// are all treated the same way.\n//\n// NOTE: The performance of compressed tar archives is not great due to overhead\n// with decompression. However, the fs.WalkDir() use case has been optimized to\n// create an index on first call to ReadDir().\nfunc FileSystem(ctx context.Context, filename string, stream ReaderAtSeeker) (fs.FS, error) {\n\tif filename == \"\" && stream == nil {\n\t\treturn nil, errors.New(\"no input\")\n\t}\n\n\t// if an input stream is specified, we'll use that for identification\n\t// and for ArchiveFS (if it's an archive); but if not, we'll open the\n\t// file and read it for identification, but in that case we won't want\n\t// to also use it for the ArchiveFS (because we need to close what we\n\t// opened, and ArchiveFS opens its own files), hence this separate var\n\tidStream := stream\n\n\t// if input is only a filename (no stream), check if it's a directory;\n\t// if not, open it so we can determine which format to use (filename\n\t// is not always a good indicator of file format)\n\tif filename != \"\" && stream == nil {\n\t\tinfo, err := os.Stat(filename)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// real folders can be accessed easily\n\t\tif info.IsDir() {\n\t\t\treturn os.DirFS(filename), nil\n\t\t}\n\n\t\t// if any archive formats recognize this file, access it like a folder\n\t\tfile, err := os.Open(filename)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer file.Close()\n\t\tidStream = file // use file for format identification only\n\t}\n\n\t// normally, callers should use the Reader value returned from Identify, but\n\t// our input is a Seeker, so we know the original input value gets returned\n\tformat, _, err := Identify(ctx, filepath.Base(filename), idStream)\n\tif errors.Is(err, NoMatch) {\n\t\treturn FileFS{Path: filename}, nil // must be an ordinary file\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"identify format: %w\", err)\n\t}\n\n\tswitch fileFormat := format.(type) {\n\tcase Extractor:\n\t\t// if no stream was input, return an ArchiveFS that relies on the filepath\n\t\tif stream == nil {\n\t\t\treturn &ArchiveFS{Path: filename, Format: fileFormat, Context: ctx}, nil\n\t\t}\n\n\t\t// otherwise, if a stream was input, return an ArchiveFS that relies on that\n\n\t\t// determine size -- we know that the stream value we get back from\n\t\t// Identify is the same type as what we input because it is a Seeker\n\t\tsize, err := streamSizeBySeeking(stream)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"seeking for size: %w\", err)\n\t\t}\n\n\t\tsr := io.NewSectionReader(stream, 0, size)\n\n\t\treturn &ArchiveFS{Stream: sr, Format: fileFormat, Context: ctx}, nil\n\n\tcase Compression:\n\t\treturn FileFS{Path: filename, Compression: fileFormat}, nil\n\t}\n\n\treturn nil, fmt.Errorf(\"unable to create file system rooted at %s due to unsupported file or folder type\", filename)\n}\n\n// ReaderAtSeeker is a type that can read, read at, and seek.\n// os.File and io.SectionReader both implement this interface.\ntype ReaderAtSeeker interface {\n\tio.Reader\n\tio.ReaderAt\n\tio.Seeker\n}\n\n// FileFS allows accessing a file on disk using a consistent file system interface.\n// The value should be the path to a regular file, not a directory. This file will\n// be the only entry in the file system and will be at its root. It can be accessed\n// within the file system by the name of \".\" or the filename.\n//\n// If the file is compressed, set the Compression field so that reads from the\n// file will be transparently decompressed.\ntype FileFS struct {\n\t// The path to the file on disk.\n\tPath string\n\n\t// If file is compressed, setting this field will\n\t// transparently decompress reads.\n\tCompression Decompressor\n}\n\n// Open opens the named file, which must be the file used to create the file system.\nfunc (f FileFS) Open(name string) (fs.File, error) {\n\tif err := f.checkName(name, \"open\"); err != nil {\n\t\treturn nil, err\n\t}\n\tfile, err := os.Open(f.Path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif f.Compression == nil {\n\t\treturn file, nil\n\t}\n\tr, err := f.Compression.OpenReader(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn compressedFile{r, closeBoth{file, r}}, nil\n}\n\n// Stat stats the named file, which must be the file used to create the file system.\nfunc (f FileFS) Stat(name string) (fs.FileInfo, error) {\n\tif err := f.checkName(name, \"stat\"); err != nil {\n\t\treturn nil, err\n\t}\n\treturn os.Stat(f.Path)\n}\n\n// ReadDir returns a directory listing with the file as the singular entry.\nfunc (f FileFS) ReadDir(name string) ([]fs.DirEntry, error) {\n\tif err := f.checkName(name, \"stat\"); err != nil {\n\t\treturn nil, err\n\t}\n\tinfo, err := f.Stat(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn []fs.DirEntry{fs.FileInfoToDirEntry(info)}, nil\n}\n\n// checkName ensures the name is a valid path and also, in the case of\n// the FileFS, that it is either \".\", the filename originally passed in\n// to create the FileFS, or the base of the filename (name without path).\n// Other names do not make sense for a FileFS since the FS is only 1 file.\nfunc (f FileFS) checkName(name, op string) error {\n\tif name == f.Path {\n\t\treturn nil\n\t}\n\tif !fs.ValidPath(name) {\n\t\treturn &fs.PathError{Op: op, Path: name, Err: fs.ErrInvalid}\n\t}\n\tif name != \".\" && name != filepath.Base(f.Path) {\n\t\treturn &fs.PathError{Op: op, Path: name, Err: fs.ErrNotExist}\n\t}\n\treturn nil\n}\n\n// compressedFile is an fs.File that specially reads\n// from a decompression reader, and which closes both\n// that reader and the underlying file.\ntype compressedFile struct {\n\tio.Reader // decompressor\n\tcloseBoth // file and decompressor\n}\n\n// ArchiveFS allows reading an archive (or a compressed archive) using a\n// consistent file system interface. Essentially, it allows traversal and\n// reading of archive contents the same way as any normal directory on disk.\n// The contents of compressed archives are transparently decompressed.\n//\n// A valid ArchiveFS value must set either Path or Stream, but not both.\n// If Path is set, a literal file will be opened from the disk.\n// If Stream is set, new SectionReaders will be implicitly created to\n// access the stream, enabling safe, concurrent access.\n//\n// NOTE: Due to Go's file system APIs (see package io/fs), the performance\n// of ArchiveFS can suffer when using fs.WalkDir(). To mitigate this,\n// an optimized fs.ReadDirFS has been implemented that indexes the entire\n// archive on the first call to ReadDir() (since the entire archive needs\n// to be walked for every call to ReadDir() anyway, as archive contents are\n// often unordered). The first call to ReadDir(), i.e. near the start of the\n// walk, will be slow for large archives, but should be instantaneous after.\n// If you don't care about walking a file system in directory order, consider\n// calling Extract() on the underlying archive format type directly, which\n// walks the archive in entry order, without needing to do any sorting.\n//\n// Note that fs.FS implementations, including this one, reject paths starting\n// with \"./\". This can be problematic sometimes, as it is not uncommon for\n// tarballs to contain a top-level/root directory literally named \".\", which\n// can happen if a tarball is created in the same directory it is archiving.\n// The underlying Extract() calls are faithful to entries with this name,\n// but file systems have certain semantics around \".\" that restrict its use.\n// For example, a file named \".\" cannot be created on a real file system\n// because it is a special name that means \"current directory\".\n//\n// We had to decide whether to honor the true name in the archive, or honor\n// file system semantics. Given that this is a virtual file system and other\n// code using the fs.FS APIs will trip over a literal directory named \".\",\n// we choose to honor file system semantics. Files named \".\" are ignored;\n// directories with this name are effectively transparent; their contents\n// get promoted up a directory/level. This means a file at \"./x\" where \".\"\n// is a literal directory name, its name will be passed in as \"x\" in\n// WalkDir callbacks. If you need the raw, uninterpeted values from an\n// archive, use the formats' Extract() method directly. See\n// https://github.com/golang/go/issues/70155 for a little more background.\n//\n// This does have one negative edge case... a tar containing contents like\n// [x . ./x] will have a conflict on the file named \"x\" because \"./x\" will\n// also be accessed with the name of \"x\".\ntype ArchiveFS struct {\n\t// set one of these\n\tPath   string            // path to the archive file on disk, or...\n\tStream *io.SectionReader // ...stream from which to read archive\n\n\tFormat  Extractor       // the archive format\n\tPrefix  string          // optional subdirectory in which to root the fs\n\tContext context.Context // optional; mainly for cancellation\n\n\t// amortizing cache speeds up walks (esp. ReadDir)\n\tcontents map[string]fs.FileInfo\n\tdirs     map[string][]fs.DirEntry\n}\n\n// context always return a context, preferring f.Context if not nil.\nfunc (f ArchiveFS) context() context.Context {\n\tif f.Context != nil {\n\t\treturn f.Context\n\t}\n\treturn context.Background()\n}\n\n// Open opens the named file from within the archive. If name is \".\" then\n// the archive file itself will be opened as a directory file.\nfunc (f ArchiveFS) Open(name string) (fs.File, error) {\n\tif !fs.ValidPath(name) {\n\t\treturn nil, &fs.PathError{Op: \"open\", Path: name, Err: fmt.Errorf(\"%w: %s\", fs.ErrInvalid, name)}\n\t}\n\n\t// apply prefix if fs is rooted in a subtree\n\tname = path.Join(f.Prefix, name)\n\n\t// if we've already indexed the archive, we can know quickly if the file doesn't exist,\n\t// and we can also return directory files with their entries instantly\n\tif f.contents != nil {\n\t\tif info, found := f.contents[name]; found {\n\t\t\tif info.IsDir() {\n\t\t\t\tif entries, ok := f.dirs[name]; ok {\n\t\t\t\t\treturn &dirFile{info: info, entries: entries}, nil\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif entries, found := f.dirs[name]; found {\n\t\t\t\treturn &dirFile{info: implicitDirInfo{implicitDirEntry{name}}, entries: entries}, nil\n\t\t\t}\n\t\t\treturn nil, &fs.PathError{Op: \"open\", Path: name, Err: fmt.Errorf(\"open %s: %w\", name, fs.ErrNotExist)}\n\t\t}\n\t}\n\n\t// if a filename is specified, open the archive file\n\tvar archiveFile *os.File\n\tvar err error\n\tif f.Stream == nil {\n\t\tarchiveFile, err = os.Open(f.Path)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer func() {\n\t\t\t// close the archive file if extraction failed; we can only\n\t\t\t// count on the user/caller closing it if they successfully\n\t\t\t// got the handle to the extracted file\n\t\t\tif err != nil {\n\t\t\t\tarchiveFile.Close()\n\t\t\t}\n\t\t}()\n\t} else if f.Stream == nil {\n\t\treturn nil, fmt.Errorf(\"no input; one of Path or Stream must be set\")\n\t}\n\n\t// handle special case of opening the archive root\n\tif name == \".\" {\n\t\tvar archiveInfo fs.FileInfo\n\t\tif archiveFile != nil {\n\t\t\tarchiveInfo, err = archiveFile.Stat()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else {\n\t\t\tarchiveInfo = implicitDirInfo{\n\t\t\t\timplicitDirEntry{\".\"},\n\t\t\t}\n\t\t}\n\t\tvar entries []fs.DirEntry\n\t\tentries, err = f.ReadDir(name)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := archiveFile.Close(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &dirFile{\n\t\t\tinfo:    dirFileInfo{archiveInfo},\n\t\t\tentries: entries,\n\t\t}, nil\n\t}\n\n\tvar inputStream io.Reader\n\tif f.Stream == nil {\n\t\tinputStream = archiveFile\n\t} else {\n\t\tinputStream = io.NewSectionReader(f.Stream, 0, f.Stream.Size())\n\t}\n\n\tvar decompressor io.ReadCloser\n\tif decomp, ok := f.Format.(Decompressor); ok {\n\t\tdecompressor, err = decomp.OpenReader(inputStream)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tinputStream = decompressor\n\t}\n\n\t// prepare the handler that we'll need if we have to iterate the\n\t// archive to find the file being requested\n\tvar fsFile fs.File\n\thandler := func(ctx context.Context, file FileInfo) error {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// paths in archives can't necessarily be trusted; also clean up any \"./\" prefix\n\t\tfile.NameInArchive = path.Clean(file.NameInArchive)\n\n\t\tif !strings.HasPrefix(file.NameInArchive, name) {\n\t\t\treturn nil\n\t\t}\n\n\t\t// if this is the requested file, and it's a directory, set up the dirFile,\n\t\t// which will include a listing of all its contents as we continue the walk\n\t\tif file.NameInArchive == name && file.IsDir() {\n\t\t\tfsFile = &dirFile{info: file} // will fill entries slice as we continue the walk\n\t\t\treturn nil\n\t\t}\n\n\t\t// if the named file was a directory and we are filling its entries,\n\t\t// add this entry to the list\n\t\tif df, ok := fsFile.(*dirFile); ok {\n\t\t\tdf.entries = append(df.entries, fs.FileInfoToDirEntry(file))\n\n\t\t\t// don't traverse into subfolders\n\t\t\tif file.IsDir() {\n\t\t\t\treturn fs.SkipDir\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\tinnerFile, err := file.Open()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfsFile = closeBoth{File: innerFile, c: archiveFile}\n\n\t\tif decompressor != nil {\n\t\t\tfsFile = closeBoth{fsFile, decompressor}\n\t\t}\n\n\t\treturn fs.SkipAll\n\t}\n\n\t// when we start the walk, we pass in a nil list of files to extract, since\n\t// files may have a \".\" component in them, and the underlying format doesn't\n\t// know about our file system semantics, so we need to filter ourselves (it's\n\t// not significantly less efficient).\n\tif ar, ok := f.Format.(Archive); ok {\n\t\t// bypass the CompressedArchive format's opening of the decompressor, since\n\t\t// we already did it because we need to keep it open after returning.\n\t\t// \"I BYPASSED THE COMPRESSOR!\" -Rey\n\t\terr = ar.Extraction.Extract(f.context(), inputStream, handler)\n\t} else {\n\t\terr = f.Format.Extract(f.context(), inputStream, handler)\n\t}\n\tif err != nil {\n\t\treturn nil, &fs.PathError{Op: \"open\", Path: name, Err: fmt.Errorf(\"extract: %w\", err)}\n\t}\n\tif fsFile == nil {\n\t\treturn nil, &fs.PathError{Op: \"open\", Path: name, Err: fmt.Errorf(\"open %s: %w\", name, fs.ErrNotExist)}\n\t}\n\n\treturn fsFile, nil\n}\n\n// Stat stats the named file from within the archive. If name is \".\" then\n// the archive file itself is statted and treated as a directory file.\nfunc (f ArchiveFS) Stat(name string) (fs.FileInfo, error) {\n\tif !fs.ValidPath(name) {\n\t\treturn nil, &fs.PathError{Op: \"stat\", Path: name, Err: fmt.Errorf(\"%s: %w\", name, fs.ErrInvalid)}\n\t}\n\n\tif name == \".\" {\n\t\tif f.Path != \"\" {\n\t\t\tfileInfo, err := os.Stat(f.Path)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn dirFileInfo{fileInfo}, nil\n\t\t} else if f.Stream != nil {\n\t\t\treturn implicitDirInfo{implicitDirEntry{name}}, nil\n\t\t}\n\t}\n\n\t// apply prefix if fs is rooted in a subtree\n\tname = path.Join(f.Prefix, name)\n\n\t// if archive has already been indexed, simply use it\n\tif f.contents != nil {\n\t\tif info, ok := f.contents[name]; ok {\n\t\t\treturn info, nil\n\t\t}\n\t\treturn nil, &fs.PathError{Op: \"stat\", Path: name, Err: fmt.Errorf(\"stat %s: %w\", name, fs.ErrNotExist)}\n\t}\n\n\tvar archiveFile *os.File\n\tvar err error\n\tif f.Stream == nil {\n\t\tarchiveFile, err = os.Open(f.Path)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer archiveFile.Close()\n\t}\n\n\tvar result FileInfo\n\thandler := func(ctx context.Context, file FileInfo) error {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif path.Clean(file.NameInArchive) == name {\n\t\t\tresult = file\n\t\t\treturn fs.SkipAll\n\t\t}\n\t\treturn nil\n\t}\n\tvar inputStream io.Reader = archiveFile\n\tif f.Stream != nil {\n\t\tinputStream = io.NewSectionReader(f.Stream, 0, f.Stream.Size())\n\t}\n\terr = f.Format.Extract(f.context(), inputStream, handler)\n\tif err != nil && result.FileInfo == nil {\n\t\treturn nil, err\n\t}\n\tif result.FileInfo == nil {\n\t\treturn nil, fs.ErrNotExist\n\t}\n\treturn result.FileInfo, nil\n}\n\n// ReadDir reads the named directory from within the archive. If name is \".\"\n// then the root of the archive content is listed.\nfunc (f *ArchiveFS) ReadDir(name string) ([]fs.DirEntry, error) {\n\tif !fs.ValidPath(name) {\n\t\treturn nil, &fs.PathError{Op: \"readdir\", Path: name, Err: fs.ErrInvalid}\n\t}\n\n\t// apply prefix if fs is rooted in a subtree\n\tname = path.Join(f.Prefix, name)\n\n\t// fs.WalkDir() calls ReadDir() once per directory, and for archives with\n\t// lots of directories, that is very slow, since we have to traverse the\n\t// entire archive in order to ensure that we got all the entries for a\n\t// directory -- so we can fast-track this lookup if we've done the\n\t// traversal already\n\tif len(f.dirs) > 0 {\n\t\treturn f.dirs[name], nil\n\t}\n\n\tf.contents = make(map[string]fs.FileInfo)\n\tf.dirs = make(map[string][]fs.DirEntry)\n\n\tvar archiveFile *os.File\n\tvar err error\n\tif f.Stream == nil {\n\t\tarchiveFile, err = os.Open(f.Path)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer archiveFile.Close()\n\t}\n\n\thandler := func(ctx context.Context, file FileInfo) error {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// can't always trust path names\n\t\tfile.NameInArchive = path.Clean(file.NameInArchive)\n\n\t\t// avoid infinite walk; apparently, creating a tar file in the target\n\t\t// directory may result in an entry called \".\" in the archive; see #384\n\t\tif file.NameInArchive == \".\" {\n\t\t\treturn nil\n\t\t}\n\n\t\t// if the name being requested isn't a directory, return an error similar to\n\t\t// what most OSes return from the readdir system call when given a non-dir\n\t\tif file.NameInArchive == name && !file.IsDir() {\n\t\t\treturn &fs.PathError{Op: \"readdir\", Path: name, Err: errors.New(\"not a directory\")}\n\t\t}\n\n\t\t// index this file info for quick access\n\t\tf.contents[file.NameInArchive] = file\n\n\t\t// amortize the DirEntry list per directory, and prefer the real entry's DirEntry over an implicit/fake\n\t\t// one we may have created earlier; first try to find if it exists, and if so, replace the value;\n\t\t// otherwise insert it in sorted position\n\t\tdir := path.Dir(file.NameInArchive)\n\t\tdirEntry := fs.FileInfoToDirEntry(file)\n\t\tidx, found := slices.BinarySearchFunc(f.dirs[dir], dirEntry, func(a, b fs.DirEntry) int {\n\t\t\treturn strings.Compare(a.Name(), b.Name())\n\t\t})\n\t\tif found {\n\t\t\tf.dirs[dir][idx] = dirEntry\n\t\t} else {\n\t\t\tf.dirs[dir] = slices.Insert(f.dirs[dir], idx, dirEntry)\n\t\t}\n\n\t\t// this loop looks like an abomination, but it's really quite simple: we're\n\t\t// just iterating the directories of the path up to the root; i.e. we lob off\n\t\t// the base (last component) of the path until no separators remain, i.e. only\n\t\t// one component remains -- then loop again to make sure it's not a duplicate\n\t\t// (start without the base, since we know the full filename is an actual entry\n\t\t// in the archive, we don't need to create an implicit directory entry for it)\n\t\tstartingPath := path.Dir(file.NameInArchive)\n\t\tfor dir, base := path.Dir(startingPath), path.Base(startingPath); base != \".\"; dir, base = path.Dir(dir), path.Base(dir) {\n\t\t\tif err := ctx.Err(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tvar dirInfo fs.DirEntry = implicitDirInfo{implicitDirEntry{base}}\n\n\t\t\t// we are \"filling in\" any directories that could potentially be only implicit,\n\t\t\t// and since a nested directory can have more than 1 item, we need to prevent\n\t\t\t// duplication; for example: given a/b/c and a/b/d, we need to avoid adding\n\t\t\t// an entry for \"b\" twice within \"a\" -- hence we search for it first, and if\n\t\t\t// it doesn't already exist, we insert it in sorted position\n\t\t\tidx, found := slices.BinarySearchFunc(f.dirs[dir], dirInfo, func(a, b fs.DirEntry) int {\n\t\t\t\treturn strings.Compare(a.Name(), b.Name())\n\t\t\t})\n\t\t\tif !found {\n\t\t\t\tf.dirs[dir] = slices.Insert(f.dirs[dir], idx, dirInfo)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tvar inputStream io.Reader = archiveFile\n\tif f.Stream != nil {\n\t\tinputStream = io.NewSectionReader(f.Stream, 0, f.Stream.Size())\n\t}\n\n\terr = f.Format.Extract(f.context(), inputStream, handler)\n\tif err != nil {\n\t\t// these being non-nil implies that we have indexed the archive,\n\t\t// but if an error occurred, we likely only got part of the way\n\t\t// through and our index is incomplete, and we'd have to re-walk\n\t\t// the whole thing anyway; so reset these to nil to avoid bugs\n\t\tf.dirs = nil\n\t\tf.contents = nil\n\t\treturn nil, fmt.Errorf(\"extract: %w\", err)\n\t}\n\n\treturn f.dirs[name], nil\n}\n\n// Sub returns an FS corresponding to the subtree rooted at dir.\nfunc (f *ArchiveFS) Sub(dir string) (fs.FS, error) {\n\tif !fs.ValidPath(dir) {\n\t\treturn nil, &fs.PathError{Op: \"sub\", Path: dir, Err: fs.ErrInvalid}\n\t}\n\tinfo, err := f.Stat(dir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !info.IsDir() {\n\t\treturn nil, fmt.Errorf(\"%s is not a directory\", dir)\n\t}\n\t// result is the same as what we're starting with, except\n\t// we indicate a path prefix to be used for all operations;\n\t// the reason we don't append to the Path field directly\n\t// is because the input might be a stream rather than a\n\t// path on disk, and the Prefix field is applied on both\n\tresult := f\n\tresult.Prefix = dir\n\treturn result, nil\n}\n\n// TopDirOpen is a special Open() function that may be useful if\n// a file system root was created by extracting an archive.\n//\n// It first tries the file name as given, but if that returns an\n// error, it tries the name without the first element of the path.\n// In other words, if \"a/b/c\" returns an error, then \"b/c\" will\n// be tried instead.\n//\n// Consider an archive that contains a file \"a/b/c\". When the\n// archive is extracted, the contents may be created without a\n// new parent/root folder to contain them, and the path of the\n// same file outside the archive may be lacking an exclusive root\n// or parent container. Thus it is likely for a file system\n// created for the same files extracted to disk to be rooted at\n// one of the top-level files/folders from the archive instead of\n// a parent folder. For example, the file known as \"a/b/c\" when\n// rooted at the archive becomes \"b/c\" after extraction when rooted\n// at \"a\" on disk (because no new, exclusive top-level folder was\n// created). This difference in paths can make it difficult to use\n// archives and directories uniformly. Hence these TopDir* functions\n// which attempt to smooth over the difference.\n//\n// Some extraction utilities do create a container folder for\n// archive contents when extracting, in which case the user\n// may give that path as the root. In that case, these TopDir*\n// functions are not necessary (but aren't harmful either). They\n// are primarily useful if you are not sure whether the root is\n// an archive file or is an extracted archive file, as they will\n// work with the same filename/path inputs regardless of the\n// presence of a top-level directory.\nfunc TopDirOpen(fsys fs.FS, name string) (fs.File, error) {\n\tfile, err := fsys.Open(name)\n\tif err == nil {\n\t\treturn file, nil\n\t}\n\treturn fsys.Open(pathWithoutTopDir(name))\n}\n\n// TopDirStat is like TopDirOpen but for Stat.\nfunc TopDirStat(fsys fs.FS, name string) (fs.FileInfo, error) {\n\tinfo, err := fs.Stat(fsys, name)\n\tif err == nil {\n\t\treturn info, nil\n\t}\n\treturn fs.Stat(fsys, pathWithoutTopDir(name))\n}\n\n// TopDirReadDir is like TopDirOpen but for ReadDir.\nfunc TopDirReadDir(fsys fs.FS, name string) ([]fs.DirEntry, error) {\n\tentries, err := fs.ReadDir(fsys, name)\n\tif err == nil {\n\t\treturn entries, nil\n\t}\n\treturn fs.ReadDir(fsys, pathWithoutTopDir(name))\n}\n\nfunc pathWithoutTopDir(fpath string) string {\n\tslashIdx := strings.Index(fpath, \"/\")\n\tif slashIdx < 0 {\n\t\treturn fpath\n\t}\n\treturn fpath[slashIdx+1:]\n}\n\n// dirFile implements the fs.ReadDirFile interface.\ntype dirFile struct {\n\tinfo        fs.FileInfo\n\tentries     []fs.DirEntry\n\tentriesRead int // used for paging with ReadDir(n)\n}\n\nfunc (dirFile) Read([]byte) (int, error)      { return 0, errors.New(\"cannot read a directory file\") }\nfunc (df dirFile) Stat() (fs.FileInfo, error) { return df.info, nil }\nfunc (dirFile) Close() error                  { return nil }\n\n// ReadDir implements [fs.ReadDirFile].\nfunc (df *dirFile) ReadDir(n int) ([]fs.DirEntry, error) {\n\tif n <= 0 {\n\t\treturn df.entries, nil\n\t}\n\tif df.entriesRead >= len(df.entries) {\n\t\treturn nil, io.EOF\n\t}\n\tif df.entriesRead+n > len(df.entries) {\n\t\tn = len(df.entries) - df.entriesRead\n\t}\n\tentries := df.entries[df.entriesRead : df.entriesRead+n]\n\tdf.entriesRead += n\n\treturn entries, nil\n}\n\n// dirFileInfo is an implementation of fs.FileInfo that\n// is only used for files that are directories. It always\n// returns 0 size, directory bit set in the mode, and\n// true for IsDir. It is often used as the FileInfo for\n// dirFile values.\ntype dirFileInfo struct {\n\tfs.FileInfo\n}\n\nfunc (dirFileInfo) Size() int64            { return 0 }\nfunc (info dirFileInfo) Mode() fs.FileMode { return info.FileInfo.Mode() | fs.ModeDir }\nfunc (dirFileInfo) IsDir() bool            { return true }\n\n// fileInArchive represents a file that is opened from within an archive.\n// It implements fs.File.\ntype fileInArchive struct {\n\tio.ReadCloser\n\tinfo fs.FileInfo\n}\n\nfunc (af fileInArchive) Stat() (fs.FileInfo, error) { return af.info, nil }\n\n// closeBoth closes both the file and an associated\n// closer, such as a (de)compressor that wraps the\n// reading/writing of the file. See issue #365. If a\n// better solution is found, I'd probably prefer that.\ntype closeBoth struct {\n\tfs.File\n\tc io.Closer // usually the archive or the decompressor\n}\n\n// Close closes both the file and the associated closer. It always calls\n// Close() on both, but if multiple errors occur they are wrapped together.\nfunc (dc closeBoth) Close() error {\n\tvar err error\n\tif dc.File != nil {\n\t\tif err2 := dc.File.Close(); err2 != nil {\n\t\t\terr = fmt.Errorf(\"closing file: %w\", err2)\n\t\t}\n\t}\n\tif dc.c != nil {\n\t\tif err2 := dc.c.Close(); err2 != nil {\n\t\t\tif err == nil {\n\t\t\t\terr = fmt.Errorf(\"closing closer: %w\", err2)\n\t\t\t} else {\n\t\t\t\terr = fmt.Errorf(\"%w; additionally, closing closer: %w\", err, err2)\n\t\t\t}\n\t\t}\n\t}\n\treturn err\n}\n\n// implicitDirEntry represents a directory that does\n// not actually exist in the archive but is inferred\n// from the paths of actual files in the archive.\ntype implicitDirEntry struct{ name string }\n\nfunc (e implicitDirEntry) Name() string    { return e.name }\nfunc (implicitDirEntry) IsDir() bool       { return true }\nfunc (implicitDirEntry) Type() fs.FileMode { return fs.ModeDir }\nfunc (e implicitDirEntry) Info() (fs.FileInfo, error) {\n\treturn implicitDirInfo{e}, nil\n}\n\n// implicitDirInfo is a fs.FileInfo for an implicit directory\n// (implicitDirEntry) value. This is used when an archive may\n// not contain actual entries for a directory, but we need to\n// pretend it exists so its contents can be discovered and\n// traversed.\ntype implicitDirInfo struct{ implicitDirEntry }\n\nfunc (d implicitDirInfo) Name() string      { return d.name }\nfunc (implicitDirInfo) Size() int64         { return 0 }\nfunc (d implicitDirInfo) Mode() fs.FileMode { return d.Type() }\nfunc (implicitDirInfo) ModTime() time.Time  { return time.Time{} }\nfunc (implicitDirInfo) Sys() any            { return nil }\n\n// Interface guards\nvar (\n\t_ fs.ReadDirFS = (*FileFS)(nil)\n\t_ fs.StatFS    = (*FileFS)(nil)\n\n\t_ fs.ReadDirFS = (*ArchiveFS)(nil)\n\t_ fs.StatFS    = (*ArchiveFS)(nil)\n\t_ fs.SubFS     = (*ArchiveFS)(nil)\n)\n"
        },
        {
          "name": "fs_test.go",
          "type": "blob",
          "size": 4.6455078125,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t_ \"embed\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"reflect\"\n\t\"sort\"\n\t\"testing\"\n)\n\nfunc TestPathWithoutTopDir(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tinput, expect string\n\t}{\n\t\t{\n\t\t\tinput:  \"a/b/c\",\n\t\t\texpect: \"b/c\",\n\t\t},\n\t\t{\n\t\t\tinput:  \"b/c\",\n\t\t\texpect: \"c\",\n\t\t},\n\t\t{\n\t\t\tinput:  \"c\",\n\t\t\texpect: \"c\",\n\t\t},\n\t\t{\n\t\t\tinput:  \"\",\n\t\t\texpect: \"\",\n\t\t},\n\t} {\n\t\tif actual := pathWithoutTopDir(tc.input); actual != tc.expect {\n\t\t\tt.Errorf(\"Test %d (input=%s): Expected '%s' but got '%s'\", i, tc.input, tc.expect, actual)\n\t\t}\n\t}\n}\n\n//go:generate zip testdata/test.zip go.mod\n//go:generate zip -qr9 testdata/nodir.zip archiver.go go.mod cmd/arc/main.go .github/ISSUE_TEMPLATE/bug_report.md .github/FUNDING.yml README.md .github/workflows/ubuntu-latest.yml\n\nvar (\n\t//go:embed testdata/test.zip\n\ttestZIP []byte\n\t//go:embed testdata/nodir.zip\n\tnodirZIP []byte\n\t//go:embed testdata/unordered.zip\n\tunorderZip []byte\n)\n\nfunc TestSelfTar(t *testing.T) {\n\tfn := \"testdata/self-tar.tar\"\n\tfh, err := os.Open(fn)\n\tif err != nil {\n\t\tt.Errorf(\"Could not load test tar: %v\", fn)\n\t}\n\tfstat, err := os.Stat(fn)\n\tif err != nil {\n\t\tt.Errorf(\"Could not stat test tar: %v\", fn)\n\t}\n\tfsys := &ArchiveFS{\n\t\tStream: io.NewSectionReader(fh, 0, fstat.Size()),\n\t\tFormat: Tar{},\n\t}\n\tvar count int\n\terr = fs.WalkDir(fsys, \".\", func(path string, d fs.DirEntry, err error) error {\n\t\tif count > 10 {\n\t\t\tt.Error(\"walking test tar appears to be recursing in error\")\n\t\t\treturn fmt.Errorf(\"recursing tar: %v\", fn)\n\t\t}\n\t\tcount++\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n}\n\nfunc ExampleArchiveFS_Stream() {\n\tfsys := &ArchiveFS{\n\t\tStream: io.NewSectionReader(bytes.NewReader(testZIP), 0, int64(len(testZIP))),\n\t\tFormat: Zip{},\n\t}\n\t// You can serve the contents in a web server:\n\thttp.Handle(\"/static\", http.StripPrefix(\"/static\",\n\t\thttp.FileServer(http.FS(fsys))))\n\n\t// Or read the files using fs functions:\n\tdis, err := fsys.ReadDir(\".\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfor _, di := range dis {\n\t\tfmt.Println(di.Name())\n\t\tb, err := fs.ReadFile(fsys, path.Join(\".\", di.Name()))\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tfmt.Println(bytes.Contains(b, []byte(\"require (\")))\n\t}\n\t// Output:\n\t// go.mod\n\t// true\n}\n\nfunc TestArchiveFS_ReadDir(t *testing.T) {\n\tfor _, tc := range []struct {\n\t\tname    string\n\t\tarchive ArchiveFS\n\t\twant    map[string][]string\n\t}{\n\t\t{\n\t\t\tname: \"test.zip\",\n\t\t\tarchive: ArchiveFS{\n\t\t\t\tStream: io.NewSectionReader(bytes.NewReader(testZIP), 0, int64(len(testZIP))),\n\t\t\t\tFormat: Zip{},\n\t\t\t},\n\t\t\t// unzip -l testdata/test.zip\n\t\t\twant: map[string][]string{\n\t\t\t\t\".\": {\"go.mod\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"nodir.zip\",\n\t\t\tarchive: ArchiveFS{\n\t\t\t\tStream: io.NewSectionReader(bytes.NewReader(nodirZIP), 0, int64(len(nodirZIP))),\n\t\t\t\tFormat: Zip{},\n\t\t\t},\n\t\t\t// unzip -l testdata/nodir.zip\n\t\t\twant: map[string][]string{\n\t\t\t\t\".\":       {\".github\", \"README.md\", \"archiver.go\", \"cmd\", \"go.mod\"},\n\t\t\t\t\".github\": {\"FUNDING.yml\", \"ISSUE_TEMPLATE\", \"workflows\"},\n\t\t\t\t\"cmd\":     {\"arc\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"unordered.zip\",\n\t\t\tarchive: ArchiveFS{\n\t\t\t\tStream: io.NewSectionReader(bytes.NewReader(unorderZip), 0, int64(len(unorderZip))),\n\t\t\t\tFormat: Zip{},\n\t\t\t},\n\t\t\t// unzip -l testdata/unordered.zip, note entry 1/1 and 1/2 are separated by contents of directory 2\n\t\t\twant: map[string][]string{\n\t\t\t\t\".\": {\"1\", \"2\"},\n\t\t\t\t\"1\": {\"1\", \"2\"},\n\t\t\t\t\"2\": {\"1\"},\n\t\t\t},\n\t\t},\n\t} {\n\t\ttc := tc\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\t\t\tfsys := tc.archive\n\t\t\tfor baseDir, wantLS := range tc.want {\n\t\t\t\tt.Run(fmt.Sprintf(\"ReadDir(%q)\", baseDir), func(t *testing.T) {\n\t\t\t\t\tdis, err := fsys.ReadDir(baseDir)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Error(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tdirs := []string{}\n\t\t\t\t\tfor _, di := range dis {\n\t\t\t\t\t\tdirs = append(dirs, di.Name())\n\t\t\t\t\t}\n\n\t\t\t\t\t// Stabilize the sort order\n\t\t\t\t\tsort.Strings(dirs)\n\n\t\t\t\t\tif !reflect.DeepEqual(wantLS, dirs) {\n\t\t\t\t\t\tt.Errorf(\"ReadDir() got: %v, want: %v\", dirs, wantLS)\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t\t// Uncomment to reproduce https://github.com/mholt/archiver/issues/340.\n\t\t\t\tt.Run(fmt.Sprintf(\"Open(%s)\", baseDir), func(t *testing.T) {\n\t\t\t\t\tf, err := fsys.Open(baseDir)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Errorf(\"fsys.Open(%q): %#v %s\", baseDir, err, err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\trdf, ok := f.(fs.ReadDirFile)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tt.Errorf(\"fsys.Open(%q) did not return a fs.ReadDirFile, got: %#v\", baseDir, f)\n\t\t\t\t\t}\n\n\t\t\t\t\tdis, err := rdf.ReadDir(-1)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Error(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tdirs := []string{}\n\t\t\t\t\tfor _, di := range dis {\n\t\t\t\t\t\tdirs = append(dirs, di.Name())\n\t\t\t\t\t}\n\n\t\t\t\t\t// Stabilize the sort order\n\t\t\t\t\tsort.Strings(dirs)\n\n\t\t\t\t\tif !reflect.DeepEqual(wantLS, dirs) {\n\t\t\t\t\t\tt.Errorf(\"Open().ReadDir(-1) got: %v, want: %v\", dirs, wantLS)\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.884765625,
          "content": "module github.com/mholt/archiver/v4\n\ngo 1.22.2\n\ntoolchain go1.23.2\n\nrequire (\n\tgithub.com/andybalholm/brotli v1.1.1\n\tgithub.com/dsnet/compress v0.0.2-0.20230904184137-39efe44ab707\n\tgithub.com/klauspost/compress v1.17.11\n\tgithub.com/klauspost/pgzip v1.2.6\n\tgithub.com/nwaples/rardecode/v2 v2.0.0-beta.4.0.20241112120701-034e449c6e78\n\tgithub.com/therootcompany/xz v1.0.1\n\tgithub.com/ulikunitz/xz v0.5.12\n)\n\nrequire (\n\tgithub.com/STARRY-S/zip v0.2.1\n\tgithub.com/bodgit/sevenzip v1.6.0\n\tgithub.com/pierrec/lz4/v4 v4.1.21\n\tgithub.com/sorairolake/lzip-go v0.3.5\n\tgolang.org/x/text v0.20.0\n)\n\nrequire (\n\tgithub.com/bodgit/plumbing v1.3.0 // indirect\n\tgithub.com/bodgit/windows v1.0.1 // indirect\n\tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n\tgithub.com/hashicorp/go-multierror v1.1.1 // indirect\n\tgithub.com/hashicorp/golang-lru/v2 v2.0.7 // indirect\n\tgo4.org v0.0.0-20230225012048-214862532bf5 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 28.3466796875,
          "content": "cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\ncloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\ncloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\ncloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\ncloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\ncloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\ncloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\ncloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\ncloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\ncloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\ncloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\ncloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\ncloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\ncloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/STARRY-S/zip v0.2.1 h1:pWBd4tuSGm3wtpoqRZZ2EAwOmcHK6XFf7bU9qcJXyFg=\ngithub.com/STARRY-S/zip v0.2.1/go.mod h1:xNvshLODWtC4EJ702g7cTYn13G53o1+X9BWnPFpcWV4=\ngithub.com/andybalholm/brotli v1.1.1 h1:PR2pgnyFznKEugtsUo0xLdDop5SKXd5Qf5ysW+7XdTA=\ngithub.com/andybalholm/brotli v1.1.1/go.mod h1:05ib4cKhjx3OQYUY22hTVd34Bc8upXjOLL2rKwwZBoA=\ngithub.com/bodgit/plumbing v1.3.0 h1:pf9Itz1JOQgn7vEOE7v7nlEfBykYqvUYioC61TwWCFU=\ngithub.com/bodgit/plumbing v1.3.0/go.mod h1:JOTb4XiRu5xfnmdnDJo6GmSbSbtSyufrsyZFByMtKEs=\ngithub.com/bodgit/sevenzip v1.6.0 h1:a4R0Wu6/P1o1pP/3VV++aEOcyeBxeO/xE2Y9NSTrr6A=\ngithub.com/bodgit/sevenzip v1.6.0/go.mod h1:zOBh9nJUof7tcrlqJFv1koWRrhz3LbDbUNngkuZxLMc=\ngithub.com/bodgit/windows v1.0.1 h1:tF7K6KOluPYygXa3Z2594zxlkbKPAOvqr97etrGNIz4=\ngithub.com/bodgit/windows v1.0.1/go.mod h1:a6JLwrB4KrTR5hBpp8FI9/9W9jJfeQ2h4XDXU74ZCdM=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\ngithub.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\ngithub.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dsnet/compress v0.0.2-0.20230904184137-39efe44ab707 h1:2tV76y6Q9BB+NEBasnqvs7e49aEBFI8ejC89PSnWH+4=\ngithub.com/dsnet/compress v0.0.2-0.20230904184137-39efe44ab707/go.mod h1:qssHWj60/X5sZFNxpG4HBPDHVqxNm4DfnCKgrbZOT+s=\ngithub.com/dsnet/golib v0.0.0-20171103203638-1ea166775780/go.mod h1:Lj+Z9rebOhdfkVLjJ8T6VcRQv3SXugXy999NBtR9aFY=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\ngithub.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\ngithub.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\ngithub.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\ngithub.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7 h1:a+bsQ5rvGLjzHuww6tVxozPZFVghXaHOwFs4luLUK2k=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7/go.mod h1:QeFd9opnmA6QUJc5vARoKUSoFhyfM2/ZepoAG6RGpeM=\ngithub.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\ngithub.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/klauspost/compress v1.4.1/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\ngithub.com/klauspost/compress v1.17.11 h1:In6xLpyWOi1+C7tXUUWv2ot1QvBjxevKAaI6IXrJmUc=\ngithub.com/klauspost/compress v1.17.11/go.mod h1:pMDklpSncoRMuLFrf1W9Ss9KT+0rH90U12bZKk7uwG0=\ngithub.com/klauspost/cpuid v1.2.0/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\ngithub.com/klauspost/pgzip v1.2.6 h1:8RXeL5crjEUFnR2/Sn6GJNWtSQ3Dk8pq4CL3jvdDyjU=\ngithub.com/klauspost/pgzip v1.2.6/go.mod h1:Ch1tH69qFZu15pkjo5kYi6mth2Zzwzt50oCQKQE9RUs=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/nwaples/rardecode/v2 v2.0.0-beta.4.0.20241112120701-034e449c6e78 h1:MYzLheyVx1tJVDqfu3YnN4jtnyALNzLvwl+f58TcvQY=\ngithub.com/nwaples/rardecode/v2 v2.0.0-beta.4.0.20241112120701-034e449c6e78/go.mod h1:yntwv/HfMc/Hbvtq9I19D1n58te3h6KsqCf3GxyfBGY=\ngithub.com/pierrec/lz4/v4 v4.1.21 h1:yOVMLb6qSIDP67pl/5F7RepeKYu/VmTyEXvuMI5d9mQ=\ngithub.com/pierrec/lz4/v4 v4.1.21/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rwcarlsen/goexif v0.0.0-20190401172101-9e8deecbddbd/go.mod h1:hPqNNc0+uJM6H+SuU8sEs5K5IQeKccPqeSjfgcKGgPk=\ngithub.com/sorairolake/lzip-go v0.3.5 h1:ms5Xri9o1JBIWvOFAorYtUNik6HI3HgBTkISiqu0Cwg=\ngithub.com/sorairolake/lzip-go v0.3.5/go.mod h1:N0KYq5iWrMXI0ZEXKXaS9hCyOjZUQdBDEIbXfoUwbdk=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=\ngithub.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/therootcompany/xz v1.0.1 h1:CmOtsn1CbtmyYiusbfmhmkpAAETj0wBIH6kCYaX+xzw=\ngithub.com/therootcompany/xz v1.0.1/go.mod h1:3K3UH1yCKgBneZYhuQUvJ9HPD19UEXEI0BWbMn8qNMY=\ngithub.com/ulikunitz/xz v0.5.8/go.mod h1:nbz6k7qbPmH4IRqmfOplQw/tblSgqTqBwxkY0oWt/14=\ngithub.com/ulikunitz/xz v0.5.12 h1:37Nm15o69RwBkXM0J6A5OlE67RZTfzUxTj8fB3dfcsc=\ngithub.com/ulikunitz/xz v0.5.12/go.mod h1:nbz6k7qbPmH4IRqmfOplQw/tblSgqTqBwxkY0oWt/14=\ngithub.com/xyproto/randomstring v1.0.5 h1:YtlWPoRdgMu3NZtP45drfy1GKoojuR7hmRcnhZqKjWU=\ngithub.com/xyproto/randomstring v1.0.5/go.mod h1:rgmS5DeNXLivK7YprL0pY+lTuhNQW3iGxZ18UQApw/E=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngo.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\ngo.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\ngo.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo4.org v0.0.0-20230225012048-214862532bf5 h1:nifaUDeh+rPaBCMPMQHZmvJf+QdpLFnuQPwx+LxVmtc=\ngo4.org v0.0.0-20230225012048-214862532bf5/go.mod h1:F57wTi5Lrj6WLyswp5EYV1ncrEbFGHD4hhz6S1ZYeaU=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\ngolang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\ngolang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\ngolang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.7.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.9.0 h1:fEo0HyrW1GIgZdpbhCRO0PkJajUS5H9IFUztCgEo2jQ=\ngolang.org/x/sync v0.9.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.20.0 h1:gK/Kv2otX8gz+wn7Rmb3vT96ZwuoxnQlY+HlJVj7Qug=\ngolang.org/x/text v0.20.0/go.mod h1:D4IsuqiFMhST5bX19pQ9ikHC2GsaKyk/oF+pn3ducp4=\ngolang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\ngoogle.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\ngoogle.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\ngoogle.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\ngoogle.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\ngoogle.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\nrsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\nrsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\nrsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\n"
        },
        {
          "name": "gz.go",
          "type": "blob",
          "size": 2.03515625,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/klauspost/compress/gzip\"\n\t\"github.com/klauspost/pgzip\"\n)\n\nfunc init() {\n\tRegisterFormat(Gz{})\n}\n\n// Gz facilitates gzip compression.\ntype Gz struct {\n\t// Gzip compression level. See https://pkg.go.dev/compress/flate#pkg-constants\n\t// for some predefined constants. If 0, DefaultCompression is assumed rather\n\t// than no compression.\n\tCompressionLevel int\n\n\t// DisableMultistream controls whether the reader supports multistream files.\n\t// See https://pkg.go.dev/compress/gzip#example-Reader.Multistream\n\tDisableMultistream bool\n\n\t// Use a fast parallel Gzip implementation. This is only\n\t// effective for large streams (about 1 MB or greater).\n\tMultithreaded bool\n}\n\nfunc (Gz) Extension() string { return \".gz\" }\n\nfunc (gz Gz) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), gz.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(gzHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, gzHeader)\n\n\treturn mr, nil\n}\n\nfunc (gz Gz) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\t// assume default compression level if 0, rather than no\n\t// compression, since no compression on a gzipped file\n\t// doesn't make any sense in our use cases\n\tlevel := gz.CompressionLevel\n\tif level == 0 {\n\t\tlevel = gzip.DefaultCompression\n\t}\n\n\tvar wc io.WriteCloser\n\tvar err error\n\tif gz.Multithreaded {\n\t\twc, err = pgzip.NewWriterLevel(w, level)\n\t} else {\n\t\twc, err = gzip.NewWriterLevel(w, level)\n\t}\n\treturn wc, err\n}\n\nfunc (gz Gz) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\tif gz.Multithreaded {\n\t\tgzR, err := pgzip.NewReader(r)\n\t\tif gzR != nil && gz.DisableMultistream {\n\t\t\tgzR.Multistream(false)\n\t\t}\n\t\treturn gzR, err\n\t}\n\n\tgzR, err := gzip.NewReader(r)\n\tif gzR != nil && gz.DisableMultistream {\n\t\tgzR.Multistream(false)\n\t}\n\treturn gzR, err\n}\n\n// magic number at the beginning of gzip files\nvar gzHeader = []byte{0x1f, 0x8b}\n"
        },
        {
          "name": "interfaces.go",
          "type": "blob",
          "size": 3.5888671875,
          "content": "package archiver\n\nimport (\n\t\"context\"\n\t\"io\"\n)\n\n// Format represents a way of getting data out of something else.\n// A format usually represents compression or an archive (or both).\ntype Format interface {\n\t// Extension returns the conventional file extension for this\n\t// format.\n\tExtension() string\n\n\t// Match returns true if the given name/stream is recognized.\n\t// One of the arguments is optional: filename might be empty\n\t// if working with an unnamed stream, or stream might be\n\t// empty if only working with a filename. The filename should\n\t// consist only of the base name, not a path component, and is\n\t// typically used for matching by file extension. However,\n\t// matching by reading the stream is preferred. Match reads\n\t// only as many bytes as needed to determine a match. To\n\t// preserve the stream through matching, you should either\n\t// buffer what is read by Match, or seek to the last position\n\t// before Match was called.\n\tMatch(ctx context.Context, filename string, stream io.Reader) (MatchResult, error)\n}\n\n// Compression is a compression format with both compress and decompress methods.\ntype Compression interface {\n\tFormat\n\tCompressor\n\tDecompressor\n}\n\n// Archival is an archival format that can create/write archives.\ntype Archival interface {\n\tFormat\n\tArchiver\n}\n\n// Extraction is an archival format that extract from (read) archives.\ntype Extraction interface {\n\tFormat\n\tExtractor\n}\n\n// Compressor can compress data by wrapping a writer.\ntype Compressor interface {\n\t// OpenWriter wraps w with a new writer that compresses what is written.\n\t// The writer must be closed when writing is finished.\n\tOpenWriter(w io.Writer) (io.WriteCloser, error)\n}\n\n// Decompressor can decompress data by wrapping a reader.\ntype Decompressor interface {\n\t// OpenReader wraps r with a new reader that decompresses what is read.\n\t// The reader must be closed when reading is finished.\n\tOpenReader(r io.Reader) (io.ReadCloser, error)\n}\n\n// Archiver can create a new archive.\ntype Archiver interface {\n\t// Archive writes an archive file to output with the given files.\n\t//\n\t// Context cancellation must be honored.\n\tArchive(ctx context.Context, output io.Writer, files []FileInfo) error\n}\n\n// ArchiveAsyncJob contains a File to be archived and a channel that\n// the result of the archiving should be returned on.\ntype ArchiveAsyncJob struct {\n\tFile   FileInfo\n\tResult chan<- error\n}\n\n// ArchiverAsync is an Archiver that can also create archives\n// asynchronously by pumping files into a channel as they are\n// discovered.\ntype ArchiverAsync interface {\n\tArchiver\n\n\t// Use ArchiveAsync if you can't pre-assemble a list of all\n\t// the files for the archive. Close the jobs channel after\n\t// all the files have been sent.\n\t//\n\t// This won't return until the channel is closed.\n\tArchiveAsync(ctx context.Context, output io.Writer, jobs <-chan ArchiveAsyncJob) error\n}\n\n// Extractor can extract files from an archive.\ntype Extractor interface {\n\t// Extract walks entries in the archive and calls handleFile for each\n\t// entry in the archive.\n\t//\n\t// Any files opened in the FileHandler should be closed when it returns,\n\t// as there is no guarantee the files can be read outside the handler\n\t// or after the walk has proceeded to the next file.\n\t//\n\t// Context cancellation must be honored.\n\tExtract(ctx context.Context, archive io.Reader, handleFile FileHandler) error\n}\n\n// Inserter can insert files into an existing archive.\n// EXPERIMENTAL: This API is subject to change.\ntype Inserter interface {\n\t// Insert inserts the files into archive.\n\t//\n\t// Context cancellation must be honored.\n\tInsert(ctx context.Context, archive io.ReadWriteSeeker, files []FileInfo) error\n}\n"
        },
        {
          "name": "lz4.go",
          "type": "blob",
          "size": 1.068359375,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/pierrec/lz4/v4\"\n)\n\nfunc init() {\n\tRegisterFormat(Lz4{})\n}\n\n// Lz4 facilitates LZ4 compression.\ntype Lz4 struct {\n\tCompressionLevel int\n}\n\nfunc (Lz4) Extension() string { return \".lz4\" }\n\nfunc (lz Lz4) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), lz.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(lz4Header))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, lz4Header)\n\n\treturn mr, nil\n}\n\nfunc (lz Lz4) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\tlzw := lz4.NewWriter(w)\n\toptions := []lz4.Option{\n\t\tlz4.CompressionLevelOption(lz4.CompressionLevel(lz.CompressionLevel)),\n\t}\n\tif err := lzw.Apply(options...); err != nil {\n\t\treturn nil, err\n\t}\n\treturn lzw, nil\n}\n\nfunc (Lz4) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\treturn io.NopCloser(lz4.NewReader(r)), nil\n}\n\nvar lz4Header = []byte{0x04, 0x22, 0x4d, 0x18}\n"
        },
        {
          "name": "lzip.go",
          "type": "blob",
          "size": 1.0478515625,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/sorairolake/lzip-go\"\n)\n\nfunc init() {\n\tRegisterFormat(Lzip{})\n}\n\n// Lzip facilitates lzip compression.\ntype Lzip struct{}\n\nfunc (Lzip) Extension() string { return \".lz\" }\n\nfunc (lz Lzip) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif filepath.Ext(strings.ToLower(filename)) == lz.Extension() {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(lzipHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, lzipHeader)\n\n\treturn mr, nil\n}\n\nfunc (Lzip) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\treturn lzip.NewWriter(w), nil\n}\n\nfunc (Lzip) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\tlzr, err := lzip.NewReader(r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn io.NopCloser(lzr), err\n}\n\n// magic number at the beginning of lzip files\n// https://datatracker.ietf.org/doc/html/draft-diaz-lzip-09#section-2\nvar lzipHeader = []byte(\"LZIP\")\n"
        },
        {
          "name": "rar.go",
          "type": "blob",
          "size": 3.2744140625,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/nwaples/rardecode/v2\"\n)\n\nfunc init() {\n\tRegisterFormat(Rar{})\n}\n\ntype Rar struct {\n\t// If true, errors encountered during reading or writing\n\t// a file within an archive will be logged and the\n\t// operation will continue on remaining files.\n\tContinueOnError bool\n\n\t// Password to open archives.\n\tPassword string\n}\n\nfunc (Rar) Extension() string { return \".rar\" }\n\nfunc (r Rar) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), r.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header (there are two versions; allocate buffer for larger one)\n\tbuf, err := readAtMost(stream, len(rarHeaderV5_0))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\n\tmatchedV1_5 := len(buf) >= len(rarHeaderV1_5) &&\n\t\tbytes.Equal(rarHeaderV1_5, buf[:len(rarHeaderV1_5)])\n\tmatchedV5_0 := len(buf) >= len(rarHeaderV5_0) &&\n\t\tbytes.Equal(rarHeaderV5_0, buf[:len(rarHeaderV5_0)])\n\n\tmr.ByStream = matchedV1_5 || matchedV5_0\n\n\treturn mr, nil\n}\n\n// Archive is not implemented for RAR because it is patent-encumbered.\n\nfunc (r Rar) Extract(ctx context.Context, sourceArchive io.Reader, handleFile FileHandler) error {\n\tvar options []rardecode.Option\n\tif r.Password != \"\" {\n\t\toptions = append(options, rardecode.Password(r.Password))\n\t}\n\n\trr, err := rardecode.NewReader(sourceArchive, options...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// important to initialize to non-nil, empty value due to how fileIsIncluded works\n\tskipDirs := skipList{}\n\n\tfor {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\n\t\thdr, err := rr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\tif r.ContinueOnError {\n\t\t\t\tlog.Printf(\"[ERROR] Advancing to next file in rar archive: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif fileIsIncluded(skipDirs, hdr.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tinfo := rarFileInfo{hdr}\n\t\tfile := FileInfo{\n\t\t\tFileInfo:      info,\n\t\t\tHeader:        hdr,\n\t\t\tNameInArchive: hdr.Name,\n\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\treturn fileInArchive{io.NopCloser(rr), info}, nil\n\t\t\t},\n\t\t}\n\n\t\terr = handleFile(ctx, file)\n\t\tif errors.Is(err, fs.SkipAll) {\n\t\t\tbreak\n\t\t} else if errors.Is(err, fs.SkipDir) {\n\t\t\t// if a directory, skip this path; if a file, skip the folder path\n\t\t\tdirPath := hdr.Name\n\t\t\tif !hdr.IsDir {\n\t\t\t\tdirPath = path.Dir(hdr.Name) + \"/\"\n\t\t\t}\n\t\t\tskipDirs.add(dirPath)\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"handling file: %s: %w\", hdr.Name, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// rarFileInfo satisfies the fs.FileInfo interface for RAR entries.\ntype rarFileInfo struct {\n\tfh *rardecode.FileHeader\n}\n\nfunc (rfi rarFileInfo) Name() string       { return path.Base(rfi.fh.Name) }\nfunc (rfi rarFileInfo) Size() int64        { return rfi.fh.UnPackedSize }\nfunc (rfi rarFileInfo) Mode() os.FileMode  { return rfi.fh.Mode() }\nfunc (rfi rarFileInfo) ModTime() time.Time { return rfi.fh.ModificationTime }\nfunc (rfi rarFileInfo) IsDir() bool        { return rfi.fh.IsDir }\nfunc (rfi rarFileInfo) Sys() any           { return nil }\n\nvar (\n\trarHeaderV1_5 = []byte(\"Rar!\\x1a\\x07\\x00\")     // v1.5\n\trarHeaderV5_0 = []byte(\"Rar!\\x1a\\x07\\x01\\x00\") // v5.0\n)\n\n// Interface guard\nvar _ Extractor = Rar{}\n"
        },
        {
          "name": "sz.go",
          "type": "blob",
          "size": 3.4091796875,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/klauspost/compress/s2\"\n)\n\nfunc init() {\n\tRegisterFormat(Sz{})\n}\n\n// Sz facilitates Snappy compression. It uses S2\n// for reading and writing, but by default will\n// write Snappy-compatible data.\ntype Sz struct {\n\t// Configurable S2 extension.\n\tS2 S2\n}\n\n// S2 is an extension of Snappy that can read Snappy\n// streams and write Snappy-compatible streams, but\n// can also be configured to write Snappy-incompatible\n// streams for greater gains. See\n// https://pkg.go.dev/github.com/klauspost/compress/s2\n// for details and the documentation for each option.\ntype S2 struct {\n\t// reader options\n\tMaxBlockSize           int\n\tAllocBlock             int\n\tIgnoreStreamIdentifier bool\n\tIgnoreCRC              bool\n\n\t// writer options\n\tAddIndex           bool\n\tCompression        S2Level\n\tBlockSize          int\n\tConcurrency        int\n\tFlushOnWrite       bool\n\tPadding            int\n\tSnappyIncompatible bool\n}\n\nfunc (sz Sz) Extension() string { return \".sz\" }\n\nfunc (sz Sz) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), sz.Extension()) ||\n\t\tstrings.Contains(strings.ToLower(filename), \".s2\") {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(snappyHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, snappyHeader)\n\n\treturn mr, nil\n}\n\nfunc (sz Sz) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\tvar opts []s2.WriterOption\n\tif sz.S2.AddIndex {\n\t\topts = append(opts, s2.WriterAddIndex())\n\t}\n\tswitch sz.S2.Compression {\n\tcase S2LevelNone:\n\t\topts = append(opts, s2.WriterUncompressed())\n\tcase S2LevelBetter:\n\t\topts = append(opts, s2.WriterBetterCompression())\n\tcase S2LevelBest:\n\t\topts = append(opts, s2.WriterBestCompression())\n\t}\n\tif sz.S2.BlockSize != 0 {\n\t\topts = append(opts, s2.WriterBlockSize(sz.S2.BlockSize))\n\t}\n\tif sz.S2.Concurrency != 0 {\n\t\topts = append(opts, s2.WriterConcurrency(sz.S2.Concurrency))\n\t}\n\tif sz.S2.FlushOnWrite {\n\t\topts = append(opts, s2.WriterFlushOnWrite())\n\t}\n\tif sz.S2.Padding != 0 {\n\t\topts = append(opts, s2.WriterPadding(sz.S2.Padding))\n\t}\n\tif !sz.S2.SnappyIncompatible {\n\t\t// this option is inverted because by default we should\n\t\t// probably write Snappy-compatible streams\n\t\topts = append(opts, s2.WriterSnappyCompat())\n\t}\n\treturn s2.NewWriter(w, opts...), nil\n}\n\nfunc (sz Sz) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\tvar opts []s2.ReaderOption\n\tif sz.S2.AllocBlock != 0 {\n\t\topts = append(opts, s2.ReaderAllocBlock(sz.S2.AllocBlock))\n\t}\n\tif sz.S2.IgnoreCRC {\n\t\topts = append(opts, s2.ReaderIgnoreCRC())\n\t}\n\tif sz.S2.IgnoreStreamIdentifier {\n\t\topts = append(opts, s2.ReaderIgnoreStreamIdentifier())\n\t}\n\tif sz.S2.MaxBlockSize != 0 {\n\t\topts = append(opts, s2.ReaderMaxBlockSize(sz.S2.MaxBlockSize))\n\t}\n\treturn io.NopCloser(s2.NewReader(r, opts...)), nil\n}\n\n// Compression level for S2 (Snappy/Sz extension).\n// EXPERIMENTAL: May be changed or removed without a major version bump.\ntype S2Level int\n\n// Compression levels for S2.\n// EXPERIMENTAL: May be changed or removed without a major version bump.\nconst (\n\tS2LevelNone   S2Level = 0\n\tS2LevelFast   S2Level = 1\n\tS2LevelBetter S2Level = 2\n\tS2LevelBest   S2Level = 3\n)\n\n// https://github.com/google/snappy/blob/master/framing_format.txt - contains \"sNaPpY\"\nvar snappyHeader = []byte{0xff, 0x06, 0x00, 0x00, 0x73, 0x4e, 0x61, 0x50, 0x70, 0x59}\n"
        },
        {
          "name": "tar.go",
          "type": "blob",
          "size": 7.216796875,
          "content": "package archiver\n\nimport (\n\t\"archive/tar\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"path\"\n\t\"strings\"\n)\n\nfunc init() {\n\tRegisterFormat(Tar{})\n}\n\ntype Tar struct {\n\t// If true, preserve only numeric user and group id\n\tNumericUIDGID bool\n\n\t// If true, errors encountered during reading or writing\n\t// a file within an archive will be logged and the\n\t// operation will continue on remaining files.\n\tContinueOnError bool\n}\n\nfunc (Tar) Extension() string { return \".tar\" }\n\nfunc (t Tar) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), t.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tif stream != nil {\n\t\tr := tar.NewReader(stream)\n\t\t_, err := r.Next()\n\t\tmr.ByStream = err == nil\n\t}\n\n\treturn mr, nil\n}\n\nfunc (t Tar) Archive(ctx context.Context, output io.Writer, files []FileInfo) error {\n\ttw := tar.NewWriter(output)\n\tdefer tw.Close()\n\n\tfor _, file := range files {\n\t\tif err := t.writeFileToArchive(ctx, tw, file); err != nil {\n\t\t\tif t.ContinueOnError && ctx.Err() == nil { // context errors should always abort\n\t\t\t\tlog.Printf(\"[ERROR] %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (t Tar) ArchiveAsync(ctx context.Context, output io.Writer, jobs <-chan ArchiveAsyncJob) error {\n\ttw := tar.NewWriter(output)\n\tdefer tw.Close()\n\n\tfor job := range jobs {\n\t\tjob.Result <- t.writeFileToArchive(ctx, tw, job.File)\n\t}\n\n\treturn nil\n}\n\nfunc (t Tar) writeFileToArchive(ctx context.Context, tw *tar.Writer, file FileInfo) error {\n\tif err := ctx.Err(); err != nil {\n\t\treturn err // honor context cancellation\n\t}\n\n\thdr, err := tar.FileInfoHeader(file, file.LinkTarget)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"file %s: creating header: %w\", file.NameInArchive, err)\n\t}\n\thdr.Name = file.NameInArchive // complete path, since FileInfoHeader() only has base name\n\tif hdr.Name == \"\" {\n\t\thdr.Name = file.Name() // assume base name of file I guess\n\t}\n\tif t.NumericUIDGID {\n\t\thdr.Uname = \"\"\n\t\thdr.Gname = \"\"\n\t}\n\n\tif err := tw.WriteHeader(hdr); err != nil {\n\t\treturn fmt.Errorf(\"file %s: writing header: %w\", file.NameInArchive, err)\n\t}\n\n\t// only proceed to write a file body if there is actually a body\n\t// (for example, directories and links don't have a body)\n\tif hdr.Typeflag != tar.TypeReg {\n\t\treturn nil\n\t}\n\n\tif err := openAndCopyFile(file, tw); err != nil {\n\t\treturn fmt.Errorf(\"file %s: writing data: %w\", file.NameInArchive, err)\n\t}\n\n\treturn nil\n}\n\nfunc (t Tar) Insert(ctx context.Context, into io.ReadWriteSeeker, files []FileInfo) error {\n\t// Tar files may end with some, none, or a lot of zero-byte padding. The spec says\n\t// it should end with two 512-byte trailer records consisting solely of null/0\n\t// bytes: https://www.gnu.org/software/tar/manual/html_node/Standard.html. However,\n\t// in my experiments using the `tar` command, I've found that is not the case,\n\t// and Colin Percival (author of tarsnap) confirmed this:\n\t// - https://twitter.com/cperciva/status/1476774314623913987\n\t// - https://twitter.com/cperciva/status/1476776999758663680\n\t// So while this solution on Stack Overflow makes sense if you control the\n\t// writer: https://stackoverflow.com/a/18330903/1048862 - and I did get it\n\t// to work in that case -- it is not a general solution. Seems that the only\n\t// reliable thing to do is scan the entire archive to find the last file,\n\t// read its size, then use that to compute the end of content and thus the\n\t// true length of end-of-archive padding. This is slightly more complex than\n\t// just adding the size of the last file to the current stream/seek position,\n\t// because we have to align to 512-byte blocks precisely. I don't actually\n\t// fully know why this works, but in my testing on a few different files it\n\t// did work, whereas other solutions only worked on 1 specific file. *shrug*\n\t//\n\t// Another option is to scan the file for the last contiguous series of 0s,\n\t// without interpreting the tar format at all, and to find the nearest\n\t// blocksize-offset and start writing there. Problem is that you wouldn't\n\t// know if you just overwrote some of the last file if it ends with all 0s.\n\t// Sigh.\n\tvar lastFileSize, lastStreamPos int64\n\ttr := tar.NewReader(into)\n\tfor {\n\t\thdr, err := tr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlastStreamPos, err = into.Seek(0, io.SeekCurrent)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlastFileSize = hdr.Size\n\t}\n\n\t// we can now compute the precise location to write the new file to (I think)\n\tconst blockSize = 512 // (as of Go 1.17, this is also a hard-coded const in the archive/tar package)\n\tnewOffset := lastStreamPos + lastFileSize\n\tnewOffset += blockSize - (newOffset % blockSize) // shift to next-nearest block boundary\n\t_, err := into.Seek(newOffset, io.SeekStart)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttw := tar.NewWriter(into)\n\tdefer tw.Close()\n\n\tfor i, file := range files {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\t\terr = t.writeFileToArchive(ctx, tw, file)\n\t\tif err != nil {\n\t\t\tif t.ContinueOnError && ctx.Err() == nil {\n\t\t\t\tlog.Printf(\"[ERROR] appending file %d into archive: %s: %v\", i, file.Name(), err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"appending file %d into archive: %s: %w\", i, file.Name(), err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (t Tar) Extract(ctx context.Context, sourceArchive io.Reader, handleFile FileHandler) error {\n\ttr := tar.NewReader(sourceArchive)\n\n\t// important to initialize to non-nil, empty value due to how fileIsIncluded works\n\tskipDirs := skipList{}\n\n\tfor {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\n\t\thdr, err := tr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\tif t.ContinueOnError && ctx.Err() == nil {\n\t\t\t\tlog.Printf(\"[ERROR] Advancing to next file in tar archive: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif fileIsIncluded(skipDirs, hdr.Name) {\n\t\t\tcontinue\n\t\t}\n\t\tif hdr.Typeflag == tar.TypeXGlobalHeader {\n\t\t\t// ignore the pax global header from git-generated tarballs\n\t\t\tcontinue\n\t\t}\n\n\t\tinfo := hdr.FileInfo()\n\t\tfile := FileInfo{\n\t\t\tFileInfo:      info,\n\t\t\tHeader:        hdr,\n\t\t\tNameInArchive: hdr.Name,\n\t\t\tLinkTarget:    hdr.Linkname,\n\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\treturn fileInArchive{io.NopCloser(tr), info}, nil\n\t\t\t},\n\t\t}\n\n\t\terr = handleFile(ctx, file)\n\t\tif errors.Is(err, fs.SkipAll) {\n\t\t\t// At first, I wasn't sure if fs.SkipAll implied that the rest of the entries\n\t\t\t// should still be iterated and just \"skipped\" (i.e. no-ops) or if the walk\n\t\t\t// should stop; both have the same net effect, one is just less efficient...\n\t\t\t// apparently the name of fs.StopWalk was the preferred name, but it still\n\t\t\t// became fs.SkipAll because of semantics with documentation; see\n\t\t\t// https://github.com/golang/go/issues/47209 -- anyway, the walk should stop.\n\t\t\tbreak\n\t\t} else if errors.Is(err, fs.SkipDir) {\n\t\t\t// if a directory, skip this path; if a file, skip the folder path\n\t\t\tdirPath := hdr.Name\n\t\t\tif hdr.Typeflag != tar.TypeDir {\n\t\t\t\tdirPath = path.Dir(hdr.Name) + \"/\"\n\t\t\t}\n\t\t\tskipDirs.add(dirPath)\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"handling file: %s: %w\", hdr.Name, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Interface guards\nvar (\n\t_ Archiver      = (*Tar)(nil)\n\t_ ArchiverAsync = (*Tar)(nil)\n\t_ Extractor     = (*Tar)(nil)\n\t_ Inserter      = (*Tar)(nil)\n)\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "xz.go",
          "type": "blob",
          "size": 1.0615234375,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\tfastxz \"github.com/therootcompany/xz\"\n\t\"github.com/ulikunitz/xz\"\n)\n\nfunc init() {\n\tRegisterFormat(Xz{})\n}\n\n// Xz facilitates xz compression.\ntype Xz struct{}\n\nfunc (Xz) Extension() string { return \".xz\" }\n\nfunc (x Xz) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), x.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(xzHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, xzHeader)\n\n\treturn mr, nil\n}\n\nfunc (Xz) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\treturn xz.NewWriter(w)\n}\n\nfunc (Xz) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\txr, err := fastxz.NewReader(r, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn io.NopCloser(xr), err\n}\n\n// magic number at the beginning of xz files; see section 2.1.1.1\n// of https://tukaani.org/xz/xz-file-format.txt\nvar xzHeader = []byte{0xfd, 0x37, 0x7a, 0x58, 0x5a, 0x00}\n"
        },
        {
          "name": "zip.go",
          "type": "blob",
          "size": 12.0166015625,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"path\"\n\t\"strings\"\n\n\tszip \"github.com/STARRY-S/zip\"\n\n\t\"github.com/dsnet/compress/bzip2\"\n\t\"github.com/klauspost/compress/zip\"\n\t\"github.com/klauspost/compress/zstd\"\n\t\"github.com/ulikunitz/xz\"\n\t\"golang.org/x/text/encoding\"\n\t\"golang.org/x/text/encoding/charmap\"\n\t\"golang.org/x/text/encoding/japanese\"\n\t\"golang.org/x/text/encoding/korean\"\n\t\"golang.org/x/text/encoding/simplifiedchinese\"\n\t\"golang.org/x/text/encoding/traditionalchinese\"\n\t\"golang.org/x/text/encoding/unicode\"\n)\n\nfunc init() {\n\tRegisterFormat(Zip{})\n\n\t// TODO: What about custom flate levels too\n\tzip.RegisterCompressor(ZipMethodBzip2, func(out io.Writer) (io.WriteCloser, error) {\n\t\treturn bzip2.NewWriter(out, &bzip2.WriterConfig{ /*TODO: Level: z.CompressionLevel*/ })\n\t})\n\tzip.RegisterCompressor(ZipMethodZstd, func(out io.Writer) (io.WriteCloser, error) {\n\t\treturn zstd.NewWriter(out)\n\t})\n\tzip.RegisterCompressor(ZipMethodXz, func(out io.Writer) (io.WriteCloser, error) {\n\t\treturn xz.NewWriter(out)\n\t})\n\n\tzip.RegisterDecompressor(ZipMethodBzip2, func(r io.Reader) io.ReadCloser {\n\t\tbz2r, err := bzip2.NewReader(r, nil)\n\t\tif err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn bz2r\n\t})\n\tzip.RegisterDecompressor(ZipMethodZstd, func(r io.Reader) io.ReadCloser {\n\t\tzr, err := zstd.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn zr.IOReadCloser()\n\t})\n\tzip.RegisterDecompressor(ZipMethodXz, func(r io.Reader) io.ReadCloser {\n\t\txr, err := xz.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn io.NopCloser(xr)\n\t})\n}\n\ntype Zip struct {\n\t// Only compress files which are not already in a\n\t// compressed format (determined simply by examining\n\t// file extension).\n\tSelectiveCompression bool\n\n\t// The method or algorithm for compressing stored files.\n\tCompression uint16\n\n\t// If true, errors encountered during reading or writing\n\t// a file within an archive will be logged and the\n\t// operation will continue on remaining files.\n\tContinueOnError bool\n\n\t// For files in zip archives that do not have UTF-8\n\t// encoded filenames and comments, specify the character\n\t// encoding here.\n\tTextEncoding string\n}\n\nfunc (z Zip) Extension() string { return \".zip\" }\n\nfunc (z Zip) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), z.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(zipHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, zipHeader)\n\n\treturn mr, nil\n}\n\nfunc (z Zip) Archive(ctx context.Context, output io.Writer, files []FileInfo) error {\n\tzw := zip.NewWriter(output)\n\tdefer zw.Close()\n\n\tfor i, file := range files {\n\t\tif err := z.archiveOneFile(ctx, zw, i, file); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (z Zip) ArchiveAsync(ctx context.Context, output io.Writer, jobs <-chan ArchiveAsyncJob) error {\n\tzw := zip.NewWriter(output)\n\tdefer zw.Close()\n\n\tvar i int\n\tfor job := range jobs {\n\t\tjob.Result <- z.archiveOneFile(ctx, zw, i, job.File)\n\t\ti++\n\t}\n\n\treturn nil\n}\n\nfunc (z Zip) archiveOneFile(ctx context.Context, zw *zip.Writer, idx int, file FileInfo) error {\n\tif err := ctx.Err(); err != nil {\n\t\treturn err // honor context cancellation\n\t}\n\n\thdr, err := zip.FileInfoHeader(file)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting info for file %d: %s: %w\", idx, file.Name(), err)\n\t}\n\thdr.Name = file.NameInArchive // complete path, since FileInfoHeader() only has base name\n\tif hdr.Name == \"\" {\n\t\thdr.Name = file.Name() // assume base name of file I guess\n\t}\n\n\t// customize header based on file properties\n\tif file.IsDir() {\n\t\tif !strings.HasSuffix(hdr.Name, \"/\") {\n\t\t\thdr.Name += \"/\" // required\n\t\t}\n\t\thdr.Method = zip.Store\n\t} else if z.SelectiveCompression {\n\t\t// only enable compression on compressable files\n\t\text := strings.ToLower(path.Ext(hdr.Name))\n\t\tif _, ok := compressedFormats[ext]; ok {\n\t\t\thdr.Method = zip.Store\n\t\t} else {\n\t\t\thdr.Method = z.Compression\n\t\t}\n\t} else {\n\t\thdr.Method = z.Compression\n\t}\n\n\tw, err := zw.CreateHeader(hdr)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"creating header for file %d: %s: %w\", idx, file.Name(), err)\n\t}\n\n\t// directories have no file body\n\tif file.IsDir() {\n\t\treturn nil\n\t}\n\tif err := openAndCopyFile(file, w); err != nil {\n\t\treturn fmt.Errorf(\"writing file %d: %s: %w\", idx, file.Name(), err)\n\t}\n\n\treturn nil\n}\n\n// Extract extracts files from z, implementing the Extractor interface. Uniquely, however,\n// sourceArchive must be an io.ReaderAt and io.Seeker, which are oddly disjoint interfaces\n// from io.Reader which is what the method signature requires. We chose this signature for\n// the interface because we figure you can Read() from anything you can ReadAt() or Seek()\n// with. Due to the nature of the zip archive format, if sourceArchive is not an io.Seeker\n// and io.ReaderAt, an error is returned.\nfunc (z Zip) Extract(ctx context.Context, sourceArchive io.Reader, handleFile FileHandler) error {\n\tsra, ok := sourceArchive.(seekReaderAt)\n\tif !ok {\n\t\treturn fmt.Errorf(\"input type must be an io.ReaderAt and io.Seeker because of zip format constraints\")\n\t}\n\n\tsize, err := streamSizeBySeeking(sra)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"determining stream size: %w\", err)\n\t}\n\n\tzr, err := zip.NewReader(sra, size)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// important to initialize to non-nil, empty value due to how fileIsIncluded works\n\tskipDirs := skipList{}\n\n\tfor i, f := range zr.File {\n\t\tf := f // make a copy for the Open closure\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\n\t\t// ensure filename and comment are UTF-8 encoded (issue #147 and PR #305)\n\t\tz.decodeText(&f.FileHeader)\n\n\t\tif fileIsIncluded(skipDirs, f.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tinfo := f.FileInfo()\n\t\tfile := FileInfo{\n\t\t\tFileInfo:      info,\n\t\t\tHeader:        f.FileHeader,\n\t\t\tNameInArchive: f.Name,\n\t\t\tOpen: func() (fs.File, error) {\n\t\t\t\topenedFile, err := f.Open()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn fileInArchive{openedFile, info}, nil\n\t\t\t},\n\t\t}\n\n\t\terr := handleFile(ctx, file)\n\t\tif errors.Is(err, fs.SkipAll) {\n\t\t\tbreak\n\t\t} else if errors.Is(err, fs.SkipDir) {\n\t\t\t// if a directory, skip this path; if a file, skip the folder path\n\t\t\tdirPath := f.Name\n\t\t\tif !file.IsDir() {\n\t\t\t\tdirPath = path.Dir(f.Name) + \"/\"\n\t\t\t}\n\t\t\tskipDirs.add(dirPath)\n\t\t} else if err != nil {\n\t\t\tif z.ContinueOnError {\n\t\t\t\tlog.Printf(\"[ERROR] %s: %v\", f.Name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"handling file %d: %s: %w\", i, f.Name, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// decodeText decodes the name and comment fields from hdr into UTF-8.\n// It is a no-op if the text is already UTF-8 encoded or if z.TextEncoding\n// is not specified.\nfunc (z Zip) decodeText(hdr *zip.FileHeader) {\n\tif hdr.NonUTF8 && z.TextEncoding != \"\" {\n\t\tfilename, err := decodeText(hdr.Name, z.TextEncoding)\n\t\tif err == nil {\n\t\t\thdr.Name = filename\n\t\t}\n\t\tif hdr.Comment != \"\" {\n\t\t\tcomment, err := decodeText(hdr.Comment, z.TextEncoding)\n\t\t\tif err == nil {\n\t\t\t\thdr.Comment = comment\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Insert appends the listed files into the provided Zip archive stream.\nfunc (z Zip) Insert(ctx context.Context, into io.ReadWriteSeeker, files []FileInfo) error {\n\t// following very simple example at https://github.com/STARRY-S/zip?tab=readme-ov-file#usage\n\tzu, err := szip.NewUpdater(into)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer zu.Close()\n\n\tfor idx, file := range files {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err // honor context cancellation\n\t\t}\n\n\t\thdr, err := szip.FileInfoHeader(file)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"getting info for file %d: %s: %w\", idx, file.NameInArchive, err)\n\t\t}\n\t\thdr.Name = file.NameInArchive // complete path, since FileInfoHeader() only has base name\n\t\tif hdr.Name == \"\" {\n\t\t\thdr.Name = file.Name() // assume base name of file I guess\n\t\t}\n\n\t\t// customize header based on file properties\n\t\tif file.IsDir() {\n\t\t\tif !strings.HasSuffix(hdr.Name, \"/\") {\n\t\t\t\thdr.Name += \"/\" // required\n\t\t\t}\n\t\t\thdr.Method = zip.Store\n\t\t} else if z.SelectiveCompression {\n\t\t\t// only enable compression on compressable files\n\t\t\text := strings.ToLower(path.Ext(hdr.Name))\n\t\t\tif _, ok := compressedFormats[ext]; ok {\n\t\t\t\thdr.Method = zip.Store\n\t\t\t} else {\n\t\t\t\thdr.Method = z.Compression\n\t\t\t}\n\t\t}\n\n\t\tw, err := zu.Append(hdr.Name, szip.APPEND_MODE_OVERWRITE)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"inserting file header: %d: %s: %w\", idx, file.Name(), err)\n\t\t}\n\n\t\t// directories have no file body\n\t\tif file.IsDir() {\n\t\t\treturn nil\n\t\t}\n\t\tif err := openAndCopyFile(file, w); err != nil {\n\t\t\tif z.ContinueOnError && ctx.Err() == nil {\n\t\t\t\tlog.Printf(\"[ERROR] appending file %d into archive: %s: %v\", idx, file.Name(), err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"copying inserted file %d: %s: %w\", idx, file.Name(), err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype seekReaderAt interface {\n\tio.ReaderAt\n\tio.Seeker\n}\n\n// Additional compression methods not offered by archive/zip.\n// See https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT section 4.4.5.\nconst (\n\tZipMethodBzip2 = 12\n\t// TODO: LZMA: Disabled - because 7z isn't able to unpack ZIP+LZMA ZIP+LZMA2 archives made this way - and vice versa.\n\t// ZipMethodLzma     = 14\n\tZipMethodZstd = 93\n\tZipMethodXz   = 95\n)\n\n// compressedFormats is a (non-exhaustive) set of lowercased\n// file extensions for formats that are typically already\n// compressed. Compressing files that are already compressed\n// is inefficient, so use this set of extensions to avoid that.\nvar compressedFormats = map[string]struct{}{\n\t\".7z\":   {},\n\t\".avi\":  {},\n\t\".br\":   {},\n\t\".bz2\":  {},\n\t\".cab\":  {},\n\t\".docx\": {},\n\t\".gif\":  {},\n\t\".gz\":   {},\n\t\".jar\":  {},\n\t\".jpeg\": {},\n\t\".jpg\":  {},\n\t\".lz\":   {},\n\t\".lz4\":  {},\n\t\".lzma\": {},\n\t\".m4v\":  {},\n\t\".mov\":  {},\n\t\".mp3\":  {},\n\t\".mp4\":  {},\n\t\".mpeg\": {},\n\t\".mpg\":  {},\n\t\".png\":  {},\n\t\".pptx\": {},\n\t\".rar\":  {},\n\t\".sz\":   {},\n\t\".tbz2\": {},\n\t\".tgz\":  {},\n\t\".tsz\":  {},\n\t\".txz\":  {},\n\t\".xlsx\": {},\n\t\".xz\":   {},\n\t\".zip\":  {},\n\t\".zipx\": {},\n}\n\nvar encodings = map[string]encoding.Encoding{\n\t\"ibm866\":            charmap.CodePage866,\n\t\"iso8859_2\":         charmap.ISO8859_2,\n\t\"iso8859_3\":         charmap.ISO8859_3,\n\t\"iso8859_4\":         charmap.ISO8859_4,\n\t\"iso8859_5\":         charmap.ISO8859_5,\n\t\"iso8859_6\":         charmap.ISO8859_6,\n\t\"iso8859_7\":         charmap.ISO8859_7,\n\t\"iso8859_8\":         charmap.ISO8859_8,\n\t\"iso8859_8I\":        charmap.ISO8859_8I,\n\t\"iso8859_10\":        charmap.ISO8859_10,\n\t\"iso8859_13\":        charmap.ISO8859_13,\n\t\"iso8859_14\":        charmap.ISO8859_14,\n\t\"iso8859_15\":        charmap.ISO8859_15,\n\t\"iso8859_16\":        charmap.ISO8859_16,\n\t\"koi8r\":             charmap.KOI8R,\n\t\"koi8u\":             charmap.KOI8U,\n\t\"macintosh\":         charmap.Macintosh,\n\t\"windows874\":        charmap.Windows874,\n\t\"windows1250\":       charmap.Windows1250,\n\t\"windows1251\":       charmap.Windows1251,\n\t\"windows1252\":       charmap.Windows1252,\n\t\"windows1253\":       charmap.Windows1253,\n\t\"windows1254\":       charmap.Windows1254,\n\t\"windows1255\":       charmap.Windows1255,\n\t\"windows1256\":       charmap.Windows1256,\n\t\"windows1257\":       charmap.Windows1257,\n\t\"windows1258\":       charmap.Windows1258,\n\t\"macintoshcyrillic\": charmap.MacintoshCyrillic,\n\t\"gbk\":               simplifiedchinese.GBK,\n\t\"gb18030\":           simplifiedchinese.GB18030,\n\t\"big5\":              traditionalchinese.Big5,\n\t\"eucjp\":             japanese.EUCJP,\n\t\"iso2022jp\":         japanese.ISO2022JP,\n\t\"shiftjis\":          japanese.ShiftJIS,\n\t\"euckr\":             korean.EUCKR,\n\t\"utf16be\":           unicode.UTF16(unicode.BigEndian, unicode.IgnoreBOM),\n\t\"utf16le\":           unicode.UTF16(unicode.LittleEndian, unicode.IgnoreBOM),\n}\n\n// decodeText returns UTF-8 encoded text from the given charset.\n// Thanks to @zxdvd for contributing non-UTF-8 encoding logic in\n// #149, and to @pashifika for helping in #305.\nfunc decodeText(input, charset string) (string, error) {\n\tif enc, ok := encodings[charset]; ok {\n\t\treturn enc.NewDecoder().String(input)\n\t}\n\treturn \"\", fmt.Errorf(\"unrecognized charset %s\", charset)\n}\n\nvar zipHeader = []byte(\"PK\\x03\\x04\") // NOTE: headers of empty zip files might end with 0x05,0x06 or 0x06,0x06 instead of 0x03,0x04\n\n// Interface guards\nvar (\n\t_ Archiver      = Zip{}\n\t_ ArchiverAsync = Zip{}\n\t_ Extractor     = Zip{}\n)\n"
        },
        {
          "name": "zlib.go",
          "type": "blob",
          "size": 1.84765625,
          "content": "package archiver\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/klauspost/compress/zlib\"\n)\n\nfunc init() {\n\tRegisterFormat(Zlib{})\n}\n\n// Zlib facilitates zlib compression.\ntype Zlib struct {\n\tCompressionLevel int\n}\n\nfunc (Zlib) Extension() string { return \".zz\" }\n\nfunc (zz Zlib) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), zz.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, 2)\n\t// If an error occurred or buf is not 2 bytes we can't check the header\n\tif err != nil || len(buf) < 2 {\n\t\treturn mr, err\n\t}\n\n\tmr.ByStream = isValidZlibHeader(buf[0], buf[1])\n\n\treturn mr, nil\n}\n\nfunc (zz Zlib) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\tlevel := zz.CompressionLevel\n\tif level == 0 {\n\t\tlevel = zlib.DefaultCompression\n\t}\n\treturn zlib.NewWriterLevel(w, level)\n}\n\nfunc (Zlib) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\treturn zlib.NewReader(r)\n}\n\nfunc isValidZlibHeader(first, second byte) bool {\n\t// Define all 32 valid zlib headers, see https://stackoverflow.com/questions/9050260/what-does-a-zlib-header-look-like/54915442#54915442\n\tvalidHeaders := map[uint16]struct{}{\n\t\t0x081D: {}, 0x085B: {}, 0x0899: {}, 0x08D7: {},\n\t\t0x1819: {}, 0x1857: {}, 0x1895: {}, 0x18D3: {},\n\t\t0x2815: {}, 0x2853: {}, 0x2891: {}, 0x28CF: {},\n\t\t0x3811: {}, 0x384F: {}, 0x388D: {}, 0x38CB: {},\n\t\t0x480D: {}, 0x484B: {}, 0x4889: {}, 0x48C7: {},\n\t\t0x5809: {}, 0x5847: {}, 0x5885: {}, 0x58C3: {},\n\t\t0x6805: {}, 0x6843: {}, 0x6881: {}, 0x68DE: {},\n\t\t0x7801: {}, 0x785E: {}, 0x789C: {}, 0x78DA: {},\n\t}\n\n\t// Combine the first and second bytes into a single 16-bit, big-endian value\n\theader := uint16(first)<<8 | uint16(second)\n\n\t// Check if the header is in the map of valid headers\n\t_, isValid := validHeaders[header]\n\treturn isValid\n}\n"
        },
        {
          "name": "zstd.go",
          "type": "blob",
          "size": 1.3232421875,
          "content": "package archiver\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"strings\"\n\n\t\"github.com/klauspost/compress/zstd\"\n)\n\nfunc init() {\n\tRegisterFormat(Zstd{})\n}\n\n// Zstd facilitates Zstandard compression.\ntype Zstd struct {\n\tEncoderOptions []zstd.EOption\n\tDecoderOptions []zstd.DOption\n}\n\nfunc (Zstd) Extension() string { return \".zst\" }\n\nfunc (zs Zstd) Match(_ context.Context, filename string, stream io.Reader) (MatchResult, error) {\n\tvar mr MatchResult\n\n\t// match filename\n\tif strings.Contains(strings.ToLower(filename), zs.Extension()) {\n\t\tmr.ByName = true\n\t}\n\n\t// match file header\n\tbuf, err := readAtMost(stream, len(zstdHeader))\n\tif err != nil {\n\t\treturn mr, err\n\t}\n\tmr.ByStream = bytes.Equal(buf, zstdHeader)\n\n\treturn mr, nil\n}\n\nfunc (zs Zstd) OpenWriter(w io.Writer) (io.WriteCloser, error) {\n\treturn zstd.NewWriter(w, zs.EncoderOptions...)\n}\n\nfunc (zs Zstd) OpenReader(r io.Reader) (io.ReadCloser, error) {\n\tzr, err := zstd.NewReader(r, zs.DecoderOptions...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn errorCloser{zr}, nil\n}\n\ntype errorCloser struct {\n\t*zstd.Decoder\n}\n\nfunc (ec errorCloser) Close() error {\n\tec.Decoder.Close()\n\treturn nil\n}\n\n// magic number at the beginning of Zstandard files\n// https://github.com/facebook/zstd/blob/6211bfee5ec24dc825c11751c33aa31d618b5f10/doc/zstd_compression_format.md\nvar zstdHeader = []byte{0x28, 0xb5, 0x2f, 0xfd}\n"
        }
      ]
    }
  ]
}