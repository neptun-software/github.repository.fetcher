{
  "metadata": {
    "timestamp": 1736566619590,
    "page": 176,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "superfly/litefs",
      "stars": 4169,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0302734375,
          "content": ".DS_Store\ngo.work\ncoverprofile\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.5078125,
          "content": "FROM golang:1.21.5 as builder\n\nWORKDIR /src/litefs\nCOPY . .\n\nARG LITEFS_VERSION=\nARG LITEFS_COMMIT=\n\nRUN --mount=type=cache,target=/root/.cache/go-build \\\n\t--mount=type=cache,target=/go/pkg \\\n\tgo build -ldflags \"-s -w -X 'main.Version=${LITEFS_VERSION}' -X 'main.Commit=${LITEFS_COMMIT}' -extldflags '-static'\" -tags osusergo,netgo,sqlite_omit_load_extension -o /usr/local/bin/litefs ./cmd/litefs\n\n\nFROM scratch\nCOPY --from=builder /usr/local/bin/litefs /usr/local/bin/litefs\nENTRYPOINT [\"/usr/local/bin/litefs\"]\nCMD []\n"
        },
        {
          "name": "Dockerfile.test",
          "type": "blob",
          "size": 1.0078125,
          "content": "FROM golang:1.21 as builder\n\nWORKDIR /src/litefs\nCOPY ../. .\n\nRUN --mount=type=cache,target=/root/.cache/go-build \\\n\t--mount=type=cache,target=/go/pkg \\\n\tgo build -o /usr/local/bin/litefs ./cmd/litefs\n\n\nFROM debian:buster-20221219-slim\nRUN set -ex \\\n  && apt-get update \\\n  && apt-get upgrade -y --no-install-recommends \\\n  && apt-get install -y curl ca-certificates unzip \\\n  && apt-get install -y build-essential tcl tcl-dev zlib1g-dev \\\n  && apt-get install -y kmod procps nano \\\n  && apt-get install -y fuse \n\nCOPY --from=builder /usr/local/bin/litefs /usr/local/bin/litefs\nADD tests/litefs.yml /etc/litefs.yml\nADD tests/test.sh /usr/bin/test.sh\nRUN chmod +x /usr/bin/test.sh\n\nRUN adduser --disabled-password --gecos \"\" build\nUSER build\nWORKDIR /home/build\n\nRUN curl -fsSLO --compressed --create-dirs \"https://sqlite.org/2022/sqlite-src-3400100.zip\"\nRUN unzip sqlite-src-3400100.zip && \\\n  mv sqlite-src-3400100 sqlite && \\\n  cd sqlite && ./configure && make testfixture\n\nENTRYPOINT [\"/usr/bin/test.sh\"]\nCMD [\"extraquick.test\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.080078125,
          "content": ".PHONY: default\ndefault:\n\n.PHONY: check\ncheck:\n\terrcheck ./...\n\tstaticcheck ./...\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.2294921875,
          "content": "LiteFS\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/superfly/litefs)\n![Status](https://img.shields.io/badge/status-beta-blue)\n![GitHub](https://img.shields.io/github/license/superfly/litefs)\n======\n\nLiteFS is a FUSE-based file system for replicating SQLite databases across a\ncluster of machines. It works as a passthrough file system that intercepts\nwrites to SQLite databases in order to detect transaction boundaries and record\nchanges on a per-transaction level in [LTX files](https://github.com/superfly/ltx).\n\nThis project is actively maintained but is currently in a beta state. Please\nreport any bugs as an issue on the GitHub repository.\n\nYou can find a [Getting Started guide](https://fly.io/docs/litefs/getting-started/)\non [LiteFS' documentation site](https://fly.io/docs/litefs/). Please see the\n[ARCHITECTURE.md](/docs/ARCHITECTURE.md) design document for details about how\nLiteFS works.\n\n\n## SQLite TCL Test Suite\n\nIt's a goal of LiteFS to pass the SQLite TCL test suite, however, this is\ncurrently a work in progress. LiteFS doesn't have database deletion implemented\nyet so that causes many tests to fail during teardown.\n\nTo run a test from the suite against LiteFS, you can use the `Dockerfile.test`\nto run it in isolation. First build the Dockerfile:\n\n```sh\ndocker build -t litefs-test -f Dockerfile.test .\n```\n\nThen run it with the filename of the test you want to run. In this case, we\nare running `select1.test`:\n\n```sh\ndocker run --device /dev/fuse --cap-add SYS_ADMIN -it litefs-test select1.test\n```\n\n\n## Contributing\n\nLiteFS contributions work a little different than most GitHub projects. If you\nhave a small bug fix or typo fix, please PR directly to this repository.\n\nIf you would like to contribute a feature, please follow these steps:\n\n1. Discuss the feature in an issue on this GitHub repository.\n2. Create a pull request to **your fork** of the repository.\n3. Post a link to your pull request in the issue for consideration.\n\nThis project has a roadmap and features are added and tested in a certain order.\nAdditionally, it's likely that code style, implementation details, and test\ncoverage will need to be tweaked so it's easier to for me to grab your\nimplementation as a starting point when implementing a feature.\n"
        },
        {
          "name": "backup_client.go",
          "type": "blob",
          "size": 6.6015625,
          "content": "package litefs\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"sync\"\n\n\t\"github.com/superfly/litefs/internal\"\n\t\"github.com/superfly/ltx\"\n)\n\ntype BackupClient interface {\n\t// URL of the backup service.\n\tURL() string\n\n\t// PosMap returns the replication position for all databases on the backup service.\n\tPosMap(ctx context.Context) (map[string]ltx.Pos, error)\n\n\t// WriteTx writes an LTX file to the backup service. The file must be\n\t// contiguous with the latest LTX file on the backup service or else it\n\t// will return an ltx.PosMismatchError.\n\t//\n\t// Returns the high-water mark that indicates it is safe to remove LTX files\n\t// before that transaction ID.\n\tWriteTx(ctx context.Context, name string, r io.Reader) (hwm ltx.TXID, err error)\n\n\t// FetchSnapshot requests a full snapshot of the database as it exists on\n\t// the backup service. This should be used if the LiteFS node has become\n\t// out of sync with the backup service.\n\tFetchSnapshot(ctx context.Context, name string) (io.ReadCloser, error)\n}\n\nvar _ BackupClient = (*FileBackupClient)(nil)\n\n// FileBackupClient is a reference implemenation for BackupClient.\n// This implementation is typically only used for testing.\ntype FileBackupClient struct {\n\tmu   sync.Mutex\n\tpath string\n}\n\n// NewFileBackupClient returns a new instance of FileBackupClient.\nfunc NewFileBackupClient(path string) *FileBackupClient {\n\treturn &FileBackupClient{path: path}\n}\n\n// Open validates & creates the path the client was initialized with.\nfunc (c *FileBackupClient) Open() (err error) {\n\t// Ensure root path exists and we can get the absolute path from it.\n\tif c.path == \"\" {\n\t\treturn fmt.Errorf(\"backup path required\")\n\t} else if c.path, err = filepath.Abs(c.path); err != nil {\n\t\treturn err\n\t}\n\n\tif err := os.MkdirAll(c.path, 0o777); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// URL of the backup service.\nfunc (c *FileBackupClient) URL() string {\n\treturn (&url.URL{\n\t\tScheme: \"file\",\n\t\tPath:   filepath.ToSlash(c.path),\n\t}).String()\n}\n\n// PosMap returns the replication position for all databases on the backup service.\nfunc (c *FileBackupClient) PosMap(ctx context.Context) (map[string]ltx.Pos, error) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tents, err := os.ReadDir(c.path)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tm := make(map[string]ltx.Pos)\n\tfor _, ent := range ents {\n\t\tif !ent.IsDir() {\n\t\t\tcontinue\n\t\t}\n\t\tpos, err := c.pos(ctx, ent.Name())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tm[ent.Name()] = pos\n\t}\n\n\treturn m, nil\n}\n\n// pos returns the replication position for a single database.\nfunc (c *FileBackupClient) pos(ctx context.Context, name string) (ltx.Pos, error) {\n\tdir := filepath.Join(c.path, name)\n\tents, err := os.ReadDir(dir)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn ltx.Pos{}, err\n\t}\n\n\tvar max string\n\tfor _, ent := range ents {\n\t\tif ent.IsDir() || filepath.Ext(ent.Name()) != \".ltx\" {\n\t\t\tcontinue\n\t\t}\n\t\tif max == \"\" || ent.Name() > max {\n\t\t\tmax = ent.Name()\n\t\t}\n\t}\n\n\t// No LTX files, return empty position.\n\tif max == \"\" {\n\t\treturn ltx.Pos{}, nil\n\t}\n\n\t// Determine position from last file in directory.\n\tf, err := os.Open(filepath.Join(dir, max))\n\tif err != nil {\n\t\treturn ltx.Pos{}, err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tdec := ltx.NewDecoder(f)\n\tif err := dec.Verify(); err != nil {\n\t\treturn ltx.Pos{}, err\n\t}\n\n\treturn ltx.Pos{\n\t\tTXID:              dec.Header().MaxTXID,\n\t\tPostApplyChecksum: dec.Trailer().PostApplyChecksum,\n\t}, nil\n}\n\n// WriteTx writes an LTX file to the backup service. The file must be\n// contiguous with the latest LTX file on the backup service or else it\n// will return an ltx.PosMismatchError.\nfunc (c *FileBackupClient) WriteTx(ctx context.Context, name string, r io.Reader) (hwm ltx.TXID, err error) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Determine the current position.\n\tpos, err := c.pos(ctx, name)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Peek at the LTX header.\n\tbuf := make([]byte, ltx.HeaderSize)\n\tvar hdr ltx.Header\n\tif _, err := io.ReadFull(r, buf); err != nil {\n\t\treturn 0, err\n\t} else if err := hdr.UnmarshalBinary(buf); err != nil {\n\t\treturn 0, err\n\t}\n\tr = io.MultiReader(bytes.NewReader(buf), r)\n\n\t// Ensure LTX file is contiguous with current replication position.\n\tif pos.TXID+1 != hdr.MinTXID || pos.PostApplyChecksum != hdr.PreApplyChecksum {\n\t\treturn 0, ltx.NewPosMismatchError(pos)\n\t}\n\n\t// Write file to a temporary file.\n\tfilename := filepath.Join(c.path, name, ltx.FormatFilename(hdr.MinTXID, hdr.MaxTXID))\n\ttempFilename := filename + \".tmp\"\n\tif err := os.MkdirAll(filepath.Dir(tempFilename), 0o777); err != nil {\n\t\treturn 0, err\n\t}\n\tf, err := os.Create(tempFilename)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\t// Copy & sync the file.\n\tif _, err := io.Copy(f, r); err != nil {\n\t\treturn 0, err\n\t} else if err := f.Sync(); err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Verify the contents of the file.\n\tif _, err := f.Seek(0, io.SeekStart); err != nil {\n\t\treturn 0, err\n\t} else if err := ltx.NewDecoder(f).Verify(); err != nil {\n\t\treturn 0, err\n\t}\n\tif err := f.Close(); err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Atomically rename the file.\n\tif err := os.Rename(tempFilename, filename); err != nil {\n\t\treturn 0, err\n\t} else if err := internal.Sync(filepath.Dir(filename)); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn hdr.MaxTXID, nil\n}\n\n// FetchSnapshot requests a full snapshot of the database as it exists on\n// the backup service. This should be used if the LiteFS node has become\n// out of sync with the backup service.\nfunc (c *FileBackupClient) FetchSnapshot(ctx context.Context, name string) (_ io.ReadCloser, retErr error) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tdir := filepath.Join(c.path, name)\n\tents, err := os.ReadDir(dir)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\t// Collect the filenames of all LTX files.\n\tvar filenames []string\n\tfor _, ent := range ents {\n\t\tif ent.IsDir() || filepath.Ext(ent.Name()) != \".ltx\" {\n\t\t\tcontinue\n\t\t}\n\t\tfilenames = append(filenames, ent.Name())\n\t}\n\n\t// Return an error if we have no LTX data for the database.\n\tif len(filenames) == 0 {\n\t\treturn nil, os.ErrNotExist\n\t}\n\tsort.Strings(filenames)\n\n\t// Build sorted reader list for compactor.\n\tvar rdrs []io.Reader\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tfor _, r := range rdrs {\n\t\t\t\t_ = r.(io.Closer).Close()\n\t\t\t}\n\t\t}\n\t}()\n\tfor _, filename := range filenames {\n\t\tf, err := os.Open(filepath.Join(dir, filename))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trdrs = append(rdrs, f)\n\t}\n\n\t// Send compaction through a pipe so we can convert it to an io.Reader.\n\tpr, pw := io.Pipe()\n\tgo func() {\n\t\tcompactor := ltx.NewCompactor(pw, rdrs)\n\t\tcompactor.HeaderFlags = ltx.HeaderFlagCompressLZ4\n\t\t_ = pw.CloseWithError(compactor.Compact(ctx))\n\t}()\n\treturn pr, nil\n}\n"
        },
        {
          "name": "backup_client_test.go",
          "type": "blob",
          "size": 8.630859375,
          "content": "package litefs_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/superfly/litefs\"\n\t\"github.com/superfly/ltx\"\n)\n\nfunc TestFileBackupClient_URL(t *testing.T) {\n\tc := litefs.NewFileBackupClient(\"/path/to/data\")\n\tif got, want := c.URL(), `file:///path/to/data`; got != want {\n\t\tt.Fatalf(\"URL=%s, want %s\", got, want)\n\t}\n}\n\nfunc TestFileBackupClient_WriteTx(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\n\t\t// Write several transaction files to the client.\n\t\tif hwm, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{1}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0xe4e4aaa102377eee},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := hwm, ltx.TXID(1); got != want {\n\t\t\tt.Fatalf(\"hwm=%s, want %s\", got, want)\n\t\t}\n\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 2, MinTXID: 2, MaxTXID: 2, PreApplyChecksum: 0xe4e4aaa102377eee},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 2}, Data: bytes.Repeat([]byte{2}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0x99b1d11ab98cc555},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 2, MinTXID: 3, MaxTXID: 4, PreApplyChecksum: 0x99b1d11ab98cc555},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{3}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0x8b87423eeeeeeeee},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Write to a different database.\n\t\tif _, err := c.WriteTx(context.Background(), \"db2\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{5}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0x99b1d11ab98cc555},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Read snapshot from backup service.\n\t\tvar other ltx.FileSpec\n\t\tif rc, err := c.FetchSnapshot(context.Background(), \"db\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := other.ReadFrom(rc); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := rc.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify contents of the snapshot.\n\t\tif got, want := &other, (&ltx.FileSpec{\n\t\t\tHeader: ltx.Header{Version: 1, Flags: ltx.HeaderFlagCompressLZ4, PageSize: 512, Commit: 2, MinTXID: 1, MaxTXID: 4},\n\t\t\tPages: []ltx.PageSpec{\n\t\t\t\t{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{3}, 512)},\n\t\t\t\t{Header: ltx.PageHeader{Pgno: 2}, Data: bytes.Repeat([]byte{2}, 512)},\n\t\t\t},\n\t\t\tTrailer: ltx.Trailer{\n\t\t\t\tPostApplyChecksum: 0x8b87423eeeeeeeee,\n\t\t\t\tFileChecksum:      0xb8e6a652b0ec8453,\n\t\t\t},\n\t\t}); !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"spec mismatch:\\ngot:  %#v\\nwant: %#v\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"ErrPosMismatch/TXID\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\n\t\t// Write the initial transaction.\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{1}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0xe4e4aaa102377eee},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Write a transaction that doesn't line up with the TXID.\n\t\tvar pmErr *ltx.PosMismatchError\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 2, MinTXID: 3, MaxTXID: 3, PreApplyChecksum: 0xe4e4aaa102377eee},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 2}, Data: bytes.Repeat([]byte{2}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: ltx.ChecksumFlag | 2000},\n\t\t})); !errors.As(err, &pmErr) {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t} else if got, want := pmErr.Pos, (ltx.Pos{TXID: 1, PostApplyChecksum: 0xe4e4aaa102377eee}); !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"pos=%s, want %s\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"ErrPosMismatch/PostApplyChecksum\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\n\t\t// Write the initial transaction.\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{1}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0xe4e4aaa102377eee},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Write a transaction that doesn't line up with the TXID.\n\t\tvar pmErr *ltx.PosMismatchError\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 2, MinTXID: 2, MaxTXID: 2, PreApplyChecksum: ltx.ChecksumFlag | 2000},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 2}, Data: bytes.Repeat([]byte{2}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: ltx.ChecksumFlag | 2000},\n\t\t})); !errors.As(err, &pmErr) {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t} else if got, want := pmErr.Pos, (ltx.Pos{TXID: 1, PostApplyChecksum: 0xe4e4aaa102377eee}); !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"pos=%s, want %s\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"ErrPosMismatch/FirstTx\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\n\t\tvar pmErr *ltx.PosMismatchError\n\t\tif _, err := c.WriteTx(context.Background(), \"db\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 2, MaxTXID: 2, PreApplyChecksum: ltx.ChecksumFlag | 2000},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{1}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: ltx.ChecksumFlag | 1000},\n\t\t})); !errors.As(err, &pmErr) {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t} else if got, want := pmErr.Pos, (ltx.Pos{}); !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"pos=%s, want %s\", got, want)\n\t\t}\n\t})\n}\n\nfunc TestFileBackupClient_PosMap(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\n\t\tif _, err := c.WriteTx(context.Background(), \"db1\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{1}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0xe4e4aaa102377eee},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.WriteTx(context.Background(), \"db1\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 2, MinTXID: 2, MaxTXID: 2, PreApplyChecksum: 0xe4e4aaa102377eee},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 2}, Data: bytes.Repeat([]byte{2}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: ltx.ChecksumFlag | 2000},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Write to a different database.\n\t\tif _, err := c.WriteTx(context.Background(), \"db2\", ltxFileSpecReader(t, &ltx.FileSpec{\n\t\t\tHeader:  ltx.Header{Version: 1, PageSize: 512, Commit: 1, MinTXID: 1, MaxTXID: 1},\n\t\t\tPages:   []ltx.PageSpec{{Header: ltx.PageHeader{Pgno: 1}, Data: bytes.Repeat([]byte{5}, 512)}},\n\t\t\tTrailer: ltx.Trailer{PostApplyChecksum: 0x99b1d11ab98cc555},\n\t\t})); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Read snapshot from backup service.\n\t\tif m, err := c.PosMap(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := m, map[string]ltx.Pos{\n\t\t\t\"db1\": {TXID: 0x2, PostApplyChecksum: 0x80000000000007d0},\n\t\t\t\"db2\": {TXID: 0x1, PostApplyChecksum: 0x99b1d11ab98cc555},\n\t\t}; !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"map=%#v, want %#v\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"NoDatabases\", func(t *testing.T) {\n\t\tc := newOpenFileBackupClient(t)\n\t\tif m, err := c.PosMap(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := m, map[string]ltx.Pos{}; !reflect.DeepEqual(got, want) {\n\t\t\tt.Fatalf(\"map=%#v, want %#v\", got, want)\n\t\t}\n\t})\n}\n\nfunc newOpenFileBackupClient(tb testing.TB) *litefs.FileBackupClient {\n\ttb.Helper()\n\tc := litefs.NewFileBackupClient(tb.TempDir())\n\tif err := c.Open(); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\treturn c\n}\n\n// ltxFileSpecReader returns a spec as an io.Reader of its serialized bytes.\nfunc ltxFileSpecReader(tb testing.TB, spec *ltx.FileSpec) io.Reader {\n\ttb.Helper()\n\tvar buf bytes.Buffer\n\tif _, err := spec.WriteTo(&buf); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\treturn bytes.NewReader(buf.Bytes())\n}\n"
        },
        {
          "name": "client.go",
          "type": "blob",
          "size": 8.19921875,
          "content": "package litefs\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/superfly/ltx\"\n)\n\n// Client represents a client for connecting to other LiteFS nodes.\ntype Client interface {\n\t// AcquireHaltLock attempts to acquire a remote halt lock on the primary node.\n\tAcquireHaltLock(ctx context.Context, primaryURL string, nodeID uint64, name string, lockID int64) (*HaltLock, error)\n\n\t// ReleaseHaltLock releases a previous held remote halt lock on the primary node.\n\tReleaseHaltLock(ctx context.Context, primaryURL string, nodeID uint64, name string, lockID int64) error\n\n\t// Commit sends an LTX file to the primary to be committed.\n\t// Must be holding the halt lock to be successful.\n\tCommit(ctx context.Context, primaryURL string, nodeID uint64, name string, lockID int64, r io.Reader) error\n\n\t// Stream starts a long-running connection to stream changes from another node.\n\t// If filter is specified, only those databases will be replicated.\n\tStream(ctx context.Context, primaryURL string, nodeID uint64, posMap map[string]ltx.Pos, filter []string) (Stream, error)\n}\n\n// Stream represents a stream of frames.\ntype Stream interface {\n\tio.ReadCloser\n\n\t// ClusterID of the primary node.\n\tClusterID() string\n}\n\ntype StreamFrameType uint32\n\nconst (\n\tStreamFrameTypeLTX       = StreamFrameType(1)\n\tStreamFrameTypeReady     = StreamFrameType(2)\n\tStreamFrameTypeEnd       = StreamFrameType(3)\n\tStreamFrameTypeDropDB    = StreamFrameType(4)\n\tStreamFrameTypeHandoff   = StreamFrameType(5)\n\tStreamFrameTypeHWM       = StreamFrameType(6)\n\tStreamFrameTypeHeartbeat = StreamFrameType(7)\n)\n\ntype StreamFrame interface {\n\tio.ReaderFrom\n\tio.WriterTo\n\tType() StreamFrameType\n}\n\n// ReadStreamFrame reads a the stream type & frame from the reader.\nfunc ReadStreamFrame(r io.Reader) (StreamFrame, error) {\n\tvar typ StreamFrameType\n\tif err := binary.Read(r, binary.BigEndian, &typ); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar f StreamFrame\n\tswitch typ {\n\tcase StreamFrameTypeLTX:\n\t\tf = &LTXStreamFrame{}\n\tcase StreamFrameTypeReady:\n\t\tf = &ReadyStreamFrame{}\n\tcase StreamFrameTypeEnd:\n\t\tf = &EndStreamFrame{}\n\tcase StreamFrameTypeDropDB:\n\t\tf = &DropDBStreamFrame{}\n\tcase StreamFrameTypeHandoff:\n\t\tf = &HandoffStreamFrame{}\n\tcase StreamFrameTypeHWM:\n\t\tf = &HWMStreamFrame{}\n\tcase StreamFrameTypeHeartbeat:\n\t\tf = &HeartbeatStreamFrame{}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"invalid stream frame type: 0x%02x\", typ)\n\t}\n\n\tif _, err := f.ReadFrom(r); err == io.EOF {\n\t\treturn nil, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\treturn f, nil\n}\n\n// WriteStreamFrame writes the stream type & frame to the writer.\nfunc WriteStreamFrame(w io.Writer, f StreamFrame) error {\n\tif err := binary.Write(w, binary.BigEndian, f.Type()); err != nil {\n\t\treturn err\n\t}\n\t_, err := f.WriteTo(w)\n\treturn err\n}\n\ntype LTXStreamFrame struct {\n\tSize int64  // payload size\n\tName string // database name\n}\n\n// Type returns the type of stream frame.\nfunc (*LTXStreamFrame) Type() StreamFrameType { return StreamFrameTypeLTX }\n\nfunc (f *LTXStreamFrame) ReadFrom(r io.Reader) (int64, error) {\n\tvar size uint64\n\tif err := binary.Read(r, binary.BigEndian, &size); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.Size = int64(size)\n\n\tvar nameN uint32\n\tif err := binary.Read(r, binary.BigEndian, &nameN); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\n\tname := make([]byte, nameN)\n\tif _, err := io.ReadFull(r, name); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.Name = string(name)\n\n\treturn 0, nil\n}\n\nfunc (f *LTXStreamFrame) WriteTo(w io.Writer) (int64, error) {\n\tif err := binary.Write(w, binary.BigEndian, uint64(f.Size)); err != nil {\n\t\treturn 0, err\n\t}\n\n\tif err := binary.Write(w, binary.BigEndian, uint32(len(f.Name))); err != nil {\n\t\treturn 0, err\n\t} else if _, err := w.Write([]byte(f.Name)); err != nil {\n\t\treturn 0, err\n\t}\n\treturn 0, nil\n}\n\ntype ReadyStreamFrame struct{}\n\nfunc (f *ReadyStreamFrame) Type() StreamFrameType               { return StreamFrameTypeReady }\nfunc (f *ReadyStreamFrame) ReadFrom(r io.Reader) (int64, error) { return 0, nil }\nfunc (f *ReadyStreamFrame) WriteTo(w io.Writer) (int64, error)  { return 0, nil }\n\ntype EndStreamFrame struct{}\n\nfunc (f *EndStreamFrame) Type() StreamFrameType               { return StreamFrameTypeEnd }\nfunc (f *EndStreamFrame) ReadFrom(r io.Reader) (int64, error) { return 0, nil }\nfunc (f *EndStreamFrame) WriteTo(w io.Writer) (int64, error)  { return 0, nil }\n\n// DropDBStreamFrame notifies replicas that a database has been deleted.\n// DEPRECATED: LTX files with a zero \"commit\" field now represent deletions.\ntype DropDBStreamFrame struct {\n\tName string // database name\n}\n\n// Type returns the type of stream frame.\nfunc (*DropDBStreamFrame) Type() StreamFrameType { return StreamFrameTypeDropDB }\n\nfunc (f *DropDBStreamFrame) ReadFrom(r io.Reader) (int64, error) {\n\tvar nameN uint32\n\tif err := binary.Read(r, binary.BigEndian, &nameN); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\n\tname := make([]byte, nameN)\n\tif _, err := io.ReadFull(r, name); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.Name = string(name)\n\n\treturn 0, nil\n}\n\nfunc (f *DropDBStreamFrame) WriteTo(w io.Writer) (int64, error) {\n\tif err := binary.Write(w, binary.BigEndian, uint32(len(f.Name))); err != nil {\n\t\treturn 0, err\n\t} else if _, err := w.Write([]byte(f.Name)); err != nil {\n\t\treturn 0, err\n\t}\n\treturn 0, nil\n}\n\ntype HandoffStreamFrame struct {\n\tLeaseID string\n}\n\n// Type returns the type of stream frame.\nfunc (*HandoffStreamFrame) Type() StreamFrameType { return StreamFrameTypeHandoff }\n\nfunc (f *HandoffStreamFrame) ReadFrom(r io.Reader) (int64, error) {\n\tvar n uint32\n\tif err := binary.Read(r, binary.BigEndian, &n); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\n\tleaseID := make([]byte, n)\n\tif _, err := io.ReadFull(r, leaseID); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.LeaseID = string(leaseID)\n\n\treturn 0, nil\n}\n\nfunc (f *HandoffStreamFrame) WriteTo(w io.Writer) (int64, error) {\n\tif err := binary.Write(w, binary.BigEndian, uint32(len(f.LeaseID))); err != nil {\n\t\treturn 0, err\n\t} else if _, err := w.Write([]byte(f.LeaseID)); err != nil {\n\t\treturn 0, err\n\t}\n\treturn 0, nil\n}\n\n// HWMStreamFrame propagates the high-water mark to replica nodes.\ntype HWMStreamFrame struct {\n\tTXID ltx.TXID // high-water mark TXID\n\tName string   // database name\n}\n\n// Type returns the type of stream frame.\nfunc (*HWMStreamFrame) Type() StreamFrameType { return StreamFrameTypeHWM }\n\nfunc (f *HWMStreamFrame) ReadFrom(r io.Reader) (int64, error) {\n\tvar txID uint64\n\tif err := binary.Read(r, binary.BigEndian, &txID); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.TXID = ltx.TXID(txID)\n\n\tvar nameN uint32\n\tif err := binary.Read(r, binary.BigEndian, &nameN); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\n\tname := make([]byte, nameN)\n\tif _, err := io.ReadFull(r, name); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\tf.Name = string(name)\n\n\treturn 0, nil\n}\n\nfunc (f *HWMStreamFrame) WriteTo(w io.Writer) (int64, error) {\n\tif err := binary.Write(w, binary.BigEndian, uint64(f.TXID)); err != nil {\n\t\treturn 0, err\n\t}\n\n\tif err := binary.Write(w, binary.BigEndian, uint32(len(f.Name))); err != nil {\n\t\treturn 0, err\n\t} else if _, err := w.Write([]byte(f.Name)); err != nil {\n\t\treturn 0, err\n\t}\n\treturn 0, nil\n}\n\n// HeartbeatStreamFrame informs replicas that there have been no recent transactions\ntype HeartbeatStreamFrame struct {\n\tTimestamp int64 // ms since unix epoch\n}\n\n// Type returns the type of stream frame.\nfunc (f *HeartbeatStreamFrame) Type() StreamFrameType { return StreamFrameTypeHeartbeat }\n\nfunc (f *HeartbeatStreamFrame) ReadFrom(r io.Reader) (int64, error) {\n\tif err := binary.Read(r, binary.BigEndian, &f.Timestamp); err == io.EOF {\n\t\treturn 0, io.ErrUnexpectedEOF\n\t} else if err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn 0, nil\n}\n\nfunc (f *HeartbeatStreamFrame) WriteTo(w io.Writer) (int64, error) {\n\tif err := binary.Write(w, binary.BigEndian, f.Timestamp); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn 0, nil\n}\n"
        },
        {
          "name": "client_test.go",
          "type": "blob",
          "size": 4.693359375,
          "content": "package litefs_test\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/superfly/litefs\"\n)\n\nfunc TestReadWriteStreamFrame(t *testing.T) {\n\tt.Run(\"LTXStreamFrame\", func(t *testing.T) {\n\t\tframe := &litefs.LTXStreamFrame{Size: 100, Name: \"test.db\"}\n\n\t\tvar buf bytes.Buffer\n\t\tif err := litefs.WriteStreamFrame(&buf, frame); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif other, err := litefs.ReadStreamFrame(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(frame, other) {\n\t\t\tt.Fatalf(\"got %#v, want %#v\", frame, other)\n\t\t}\n\t})\n\tt.Run(\"ReadyStreamFrame\", func(t *testing.T) {\n\t\tframe := &litefs.ReadyStreamFrame{}\n\n\t\tvar buf bytes.Buffer\n\t\tif err := litefs.WriteStreamFrame(&buf, frame); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif other, err := litefs.ReadStreamFrame(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(frame, other) {\n\t\t\tt.Fatalf(\"got %#v, want %#v\", frame, other)\n\t\t}\n\t})\n\n\tt.Run(\"ErrEOF\", func(t *testing.T) {\n\t\tif _, err := litefs.ReadStreamFrame(bytes.NewReader(nil)); err == nil || err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\tt.Run(\"ErrStreamTypeOnly\", func(t *testing.T) {\n\t\tif _, err := litefs.ReadStreamFrame(bytes.NewReader([]byte{0, 0, 0, 1})); err == nil || err != io.ErrUnexpectedEOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\tt.Run(\"ErrInvalidStreamType\", func(t *testing.T) {\n\t\tif _, err := litefs.ReadStreamFrame(bytes.NewReader([]byte{1, 2, 3, 4})); err == nil || err.Error() != `invalid stream frame type: 0x1020304` {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\tt.Run(\"ErrPartialPayload\", func(t *testing.T) {\n\t\tif _, err := litefs.ReadStreamFrame(bytes.NewReader([]byte{0, 0, 0, 1, 1, 2})); err == nil || err != io.ErrUnexpectedEOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\tt.Run(\"ErrWriteType\", func(t *testing.T) {\n\t\tif err := litefs.WriteStreamFrame(&errWriter{}, &litefs.LTXStreamFrame{}); err == nil || err.Error() != `write error occurred` {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n}\n\nfunc TestLTXStreamFrame_ReadFrom(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.LTXStreamFrame{Name: \"test.db\"}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := 0; i < buf.Len(); i++ {\n\t\t\tvar other litefs.LTXStreamFrame\n\t\t\tif _, err := other.ReadFrom(bytes.NewReader(buf.Bytes()[:i])); err != io.ErrUnexpectedEOF {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestLTXStreamFrame_WriteTo(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.LTXStreamFrame{Name: \"test.db\"}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor i := 0; i < buf.Len(); i++ {\n\t\t\tif _, err := frame.WriteTo(&errWriter{afterN: i}); err == nil || err.Error() != `write error occurred` {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestReadyStreamFrame_ReadFrom(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.ReadyStreamFrame{}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := 1; i < buf.Len(); i++ {\n\t\t\tvar other litefs.ReadyStreamFrame\n\t\t\tif _, err := other.ReadFrom(bytes.NewReader(buf.Bytes()[:i])); err != io.ErrUnexpectedEOF {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestReadyStreamFrame_WriteTo(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.ReadyStreamFrame{}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor i := 0; i < buf.Len(); i++ {\n\t\t\tif _, err := frame.WriteTo(&errWriter{afterN: i}); err == nil || err.Error() != `write error occurred` {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestHWMStreamFrame_ReadFrom(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.HWMStreamFrame{TXID: 1234, Name: \"test.db\"}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := 0; i < buf.Len(); i++ {\n\t\t\tvar other litefs.HWMStreamFrame\n\t\t\tif _, err := other.ReadFrom(bytes.NewReader(buf.Bytes()[:i])); err != io.ErrUnexpectedEOF {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestHWMStreamFrame_WriteTo(t *testing.T) {\n\tt.Run(\"ErrUnexpectedEOF\", func(t *testing.T) {\n\t\tframe := &litefs.HWMStreamFrame{TXID: 1234, Name: \"test.db\"}\n\t\tvar buf bytes.Buffer\n\t\tif _, err := frame.WriteTo(&buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor i := 0; i < buf.Len(); i++ {\n\t\t\tif _, err := frame.WriteTo(&errWriter{afterN: i}); err == nil || err.Error() != `write error occurred` {\n\t\t\t\tt.Fatalf(\"expected error at %d bytes: %s\", i, err)\n\t\t\t}\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "consul",
          "type": "tree",
          "content": null
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 122.798828125,
          "content": "package litefs\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"log\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/superfly/litefs/internal\"\n\t\"github.com/superfly/ltx\"\n)\n\n// WaitInterval is the time between checking if the DB has reached a position in DB.Wait().\nconst WaitInterval = 100 * time.Microsecond\n\n// DB represents a SQLite database.\ntype DB struct {\n\tstore     *Store        // parent store\n\tos        OS            // operating system interface (copied from store)\n\tname      string        // name of database\n\tpath      string        // full on-disk path\n\tpageSize  uint32        // database page size, if known\n\tpageN     atomic.Uint32 // database size, in pages\n\tpos       atomic.Value  // current tx position (Pos)\n\ttimestamp int64         // ms since epoch from last ltx\n\thwm       atomic.Uint64 // high-water mark\n\tmode      atomic.Value  // database journaling mode (rollback, wal)\n\n\t// Halt lock prevents writes or checkpoints on the primary so that\n\t// replica nodes can perform writes and send them back to the primary.\n\t//\n\thaltLockAndGuard atomic.Value // local halt lock & guard, if currently held\n\tremoteHaltLock   atomic.Value // remote halt lock, if currently held\n\n\tchksums struct { // database page checksums\n\t\tmu     sync.Mutex\n\t\tpages  []ltx.Checksum // individual database page checksums\n\t\tblocks []ltx.Checksum // aggregated database page checksums; grouped by ChecksumBlockSize\n\t}\n\n\tdirtyPageSet map[uint32]struct{}\n\n\twal struct {\n\t\toffset           int64                     // offset of the start of the transaction\n\t\tbyteOrder        binary.ByteOrder          // determine by WAL header magic\n\t\tsalt1, salt2     uint32                    // current WAL header salt values\n\t\tchksum1, chksum2 uint32                    // WAL checksum values at wal.offset\n\t\tframeOffsets     map[uint32]int64          // WAL frame offset of the last version of a given pgno before current tx\n\t\tchksums          map[uint32][]ltx.Checksum // wal page checksums\n\t}\n\tshmMu       sync.Mutex  // prevents updateSHM() from being called concurrently\n\tupdatingSHM atomic.Bool // marks when updateSHM is being called so SHM writes are prevented\n\n\t// Collection of outstanding guard sets, protected by a mutex.\n\tguardSets struct {\n\t\tmu sync.Mutex\n\t\tm  map[uint64]*GuardSet\n\t}\n\n\t// SQLite database locks\n\tpendingLock  RWMutex\n\tsharedLock   RWMutex\n\treservedLock RWMutex\n\n\t// SQLite WAL locks\n\twriteLock   RWMutex\n\tckptLock    RWMutex\n\trecoverLock RWMutex\n\tread0Lock   RWMutex\n\tread1Lock   RWMutex\n\tread2Lock   RWMutex\n\tread3Lock   RWMutex\n\tread4Lock   RWMutex\n\tdmsLock     RWMutex\n\n\t// Returns the current time. Used for mocking time in tests.\n\tNow func() time.Time\n}\n\n// NewDB returns a new instance of DB.\nfunc NewDB(store *Store, name string, path string) *DB {\n\tdb := &DB{\n\t\tstore: store,\n\t\tname:  name,\n\t\tpath:  path,\n\t\tos:    store.OS,\n\n\t\tdirtyPageSet: make(map[uint32]struct{}),\n\n\t\tNow: time.Now,\n\t}\n\tdb.pos.Store(ltx.Pos{})\n\tdb.mode.Store(DBModeRollback)\n\tdb.haltLockAndGuard.Store((*haltLockAndGuard)(nil))\n\tdb.remoteHaltLock.Store((*HaltLock)(nil))\n\tdb.wal.frameOffsets = make(map[uint32]int64)\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\tdb.guardSets.m = make(map[uint64]*GuardSet)\n\n\treturn db\n}\n\n// Name of the database name.\nfunc (db *DB) Name() string { return db.name }\n\n// Store returns the store that the database is a member of.\nfunc (db *DB) Store() *Store { return db.store }\n\n// Path of the database's data directory.\nfunc (db *DB) Path() string { return db.path }\n\n// LTXDir returns the path to the directory of LTX transaction files.\nfunc (db *DB) LTXDir() string { return filepath.Join(db.path, \"ltx\") }\n\n// LTXPath returns the path of an LTX file.\nfunc (db *DB) LTXPath(minTXID, maxTXID ltx.TXID) string {\n\treturn filepath.Join(db.LTXDir(), ltx.FormatFilename(minTXID, maxTXID))\n}\n\n// ReadLTXDir returns DirEntry for every LTX file.\nfunc (db *DB) ReadLTXDir() ([]fs.DirEntry, error) {\n\tents, err := db.os.ReadDir(\"READLTXDIR\", db.LTXDir())\n\tif os.IsNotExist(err) {\n\t\treturn nil, nil\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"readdir: %w\", err)\n\t}\n\n\tfor i := 0; i < len(ents); i++ {\n\t\tif _, _, err := ltx.ParseFilename(ents[i].Name()); err != nil {\n\t\t\tents, i = append(ents[:i], ents[i+1:]...), i-1\n\t\t}\n\t}\n\n\t// Ensure results are in sorted order.\n\tsort.Slice(ents, func(i, j int) bool { return ents[i].Name() < ents[j].Name() })\n\n\treturn ents, nil\n}\n\n// DatabasePath returns the path to the underlying database file.\nfunc (db *DB) DatabasePath() string { return filepath.Join(db.path, \"database\") }\n\n// JournalPath returns the path to the underlying journal file.\nfunc (db *DB) JournalPath() string { return filepath.Join(db.path, \"journal\") }\n\n// WALPath returns the path to the underlying WAL file.\nfunc (db *DB) WALPath() string { return filepath.Join(db.path, \"wal\") }\n\n// SHMPath returns the path to the underlying shared memory file.\nfunc (db *DB) SHMPath() string { return filepath.Join(db.path, \"shm\") }\n\n// PageN returns the number of pages in the database.\nfunc (db *DB) PageN() uint32 { return db.pageN.Load() }\n\n// Pos returns the current transaction position of the database.\nfunc (db *DB) Pos() ltx.Pos {\n\treturn db.pos.Load().(ltx.Pos)\n}\n\n// setPos sets the current transaction position of the database.\nfunc (db *DB) setPos(pos ltx.Pos, ts int64) error {\n\tdb.pos.Store(pos)\n\tatomic.StoreInt64(&db.timestamp, ts)\n\n\t// Invalidate page cache.\n\tif invalidator := db.store.Invalidator; invalidator != nil {\n\t\tif err := invalidator.InvalidatePos(db); err != nil {\n\t\t\treturn fmt.Errorf(\"invalidate pos: %w\", err)\n\t\t}\n\t}\n\n\t// Update metrics.\n\tdbTXIDMetricVec.WithLabelValues(db.name).Set(float64(pos.TXID))\n\n\treturn nil\n}\n\n// Timestamp is the timestamp from the last applied ltx.\nfunc (db *DB) Timestamp() time.Time {\n\treturn time.UnixMilli(atomic.LoadInt64(&db.timestamp))\n}\n\n// HWM returns the current high-water mark from the backup service.\nfunc (db *DB) HWM() ltx.TXID {\n\treturn ltx.TXID(db.hwm.Load())\n}\n\n// SetHWM sets the current high-water mark.\nfunc (db *DB) SetHWM(txID ltx.TXID) {\n\tdb.hwm.Store(uint64(txID))\n}\n\n// Mode returns the journaling mode for the database (DBModeWAL or DBModeRollback).\nfunc (db *DB) Mode() DBMode {\n\treturn db.mode.Load().(DBMode)\n}\n\n// AcquireHaltLock acquires the halt lock locally.\n// This implicitly acquires locks required for locking & performs a checkpoint.\nfunc (db *DB) AcquireHaltLock(ctx context.Context, lockID int64) (_ *HaltLock, retErr error) {\n\tif lockID == 0 {\n\t\treturn nil, fmt.Errorf(\"halt lock id required\")\n\t}\n\n\tvar msg string\n\tTraceLog.Printf(\"[AcquireHaltLock(%s)]: lockID=%d\", db.name, lockID)\n\tdefer func() {\n\t\tTraceLog.Printf(\"[AcquireHaltLock.Done(%s)]: lockID=%d msg=%q %s\", db.name, lockID, msg, errorKeyValue(retErr))\n\t}()\n\n\tacquireCtx, cancel := context.WithTimeout(ctx, db.store.HaltAcquireTimeout)\n\tdefer cancel()\n\n\t// Acquire a write lock before setting the halt lock. This can cause a race\n\t// by the replica when a FUSE call is interrupted and the call is retried.\n\t// We check for the same lockID already being acquired and releasing if that\n\t// is the case.\n\tvar currHaltLock HaltLock\n\tguardSet, err := db.AcquireWriteLock(acquireCtx, func() error {\n\t\tif curr := db.haltLockAndGuard.Load().(*haltLockAndGuard); curr != nil && curr.haltLock.ID == lockID {\n\t\t\tmsg = \"lock-already-acquired\"\n\t\t\tcurrHaltLock = *curr.haltLock\n\t\t\treturn errHaltLockAlreadyAcquired\n\t\t}\n\t\treturn nil\n\t})\n\tif err == errHaltLockAlreadyAcquired {\n\t\treturn &currHaltLock, nil\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tguardSet.Unlock()\n\t\t}\n\t}()\n\n\t// Perform a recovery to clear out journal & WAL files.\n\tif err := db.recover(ctx); err != nil {\n\t\treturn nil, fmt.Errorf(\"recovery: %w\", err)\n\t}\n\n\t// Generate a random identifier for the lock so it can be referenced by clients.\n\texpires := time.Now().Add(db.store.HaltLockTTL)\n\thaltLock := &HaltLock{\n\t\tID:      lockID,\n\t\tPos:     db.Pos(),\n\t\tExpires: &expires,\n\t}\n\n\t// There shouldn't be an existing halt lock but clear it just in case.\n\tprev := db.haltLockAndGuard.Load().(*haltLockAndGuard)\n\tif prev != nil {\n\t\tprev.guardSet.Unlock()\n\t}\n\n\t// Ensure we're swapping out the one we just unlocked so there's no race.\n\tif !db.haltLockAndGuard.CompareAndSwap(prev, &haltLockAndGuard{\n\t\thaltLock: haltLock,\n\t\tguardSet: guardSet,\n\t}) {\n\t\treturn nil, fmt.Errorf(\"halt lock conflict\")\n\t}\n\n\tother := *haltLock\n\treturn &other, nil\n}\n\n// This is a marker error and should not be propagated to the client.\nvar errHaltLockAlreadyAcquired = errors.New(\"litefs: halt lock already acquired\")\n\n// ReleaseHaltLock releases a halt lock by identifier. If the current halt lock\n// does not match the identifier then it has already been released.\nfunc (db *DB) ReleaseHaltLock(ctx context.Context, id int64) {\n\tTraceLog.Printf(\"[ReleaseHaltLock(%s)]:\", db.name)\n\n\tcurr := db.haltLockAndGuard.Load().(*haltLockAndGuard)\n\tif curr == nil {\n\t\tTraceLog.Printf(\"[ReleaseHaltLock.Done(%s)]: no-lock\", db.name)\n\t\treturn // no current lock\n\t} else if curr.haltLock.ID != id {\n\t\tTraceLog.Printf(\"[ReleaseHaltLock.Done(%s)]: not-current-lock\", db.name)\n\t\treturn // not the current lock\n\t}\n\n\t// Remove as the current halt lock. Ignore the swapped return since that\n\t// just means that a concurrent release already took care of it.\n\tdb.haltLockAndGuard.CompareAndSwap(curr, (*haltLockAndGuard)(nil))\n\n\t// Release the guard set so the database can write again.\n\tcurr.guardSet.Unlock()\n\n\tTraceLog.Printf(\"[ReleaseHaltLock.Done(%s)]:\", db.name)\n}\n\n// EnforceHaltLockExpiration unsets the HALT lock if it has expired.\nfunc (db *DB) EnforceHaltLockExpiration(ctx context.Context) {\n\tcurr := db.haltLockAndGuard.Load().(*haltLockAndGuard)\n\tif curr == nil {\n\t\treturn\n\t} else if curr.haltLock.Expires == nil || curr.haltLock.Expires.After(time.Now()) {\n\t\treturn\n\t}\n\n\tTraceLog.Printf(\"[ExpireHaltLock(%s)]: id=%d\", db.name, curr.haltLock.ID)\n\n\t// Clear lock & unlock its guards.\n\tdb.haltLockAndGuard.CompareAndSwap(curr, (*haltLockAndGuard)(nil))\n\tcurr.guardSet.Unlock()\n}\n\n// AcquireRemoteHaltLock acquires the remote lock and syncs the database to its\n// position before returning to the caller. Caller should provide a random lock\n// identifier so that the primary can deduplicate retry requests.\nfunc (db *DB) AcquireRemoteHaltLock(ctx context.Context, lockID int64) (_ *HaltLock, retErr error) {\n\tcctx, cancel := context.WithTimeout(ctx, db.store.HaltAcquireTimeout)\n\tdefer cancel()\n\n\tTraceLog.Printf(\"[AcquireRemoteHaltLock(%s)]: id=%d\", db.name, lockID)\n\tdefer func() {\n\t\tTraceLog.Printf(\"[AcquireRemoteHaltLock.Done(%s)]: id=%d %s\", db.name, lockID, errorKeyValue(retErr))\n\t}()\n\n\tif lockID == 0 {\n\t\treturn nil, fmt.Errorf(\"remote halt lock id required\")\n\t}\n\n\tisPrimary, info := db.store.PrimaryInfo()\n\tif isPrimary {\n\t\treturn nil, ErrNoHaltPrimary\n\t} else if info == nil {\n\t\treturn nil, fmt.Errorf(\"no primary available for remote transaction\")\n\t}\n\n\t// Request the remote lock from the primary node.\n\thaltLock, err := db.store.Client.AcquireHaltLock(cctx, info.AdvertiseURL, db.store.ID(), db.name, lockID)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"remote begin: %w\", err)\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif err := db.store.Client.ReleaseHaltLock(ctx, info.AdvertiseURL, db.store.ID(), db.name, haltLock.ID); err != nil {\n\t\t\t\tlog.Printf(\"cannot release remote halt lock after acquisition error: %s\", err)\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Store the remote lock so we can use it for commits. This may overwrite\n\t// but there should only be one halt lock at any time since there can only\n\t// be one primary. If a race condition occurs and the halt lock is replaced\n\t// with a dead one then the next commit will simply be rejected.\n\tdb.remoteHaltLock.Store(haltLock)\n\n\t// Wait for local node to catch up to remote position.\n\tif err := db.WaitPosExact(cctx, haltLock.Pos); err != nil {\n\t\treturn nil, fmt.Errorf(\"wait: %w\", err)\n\t}\n\n\tother := *haltLock\n\treturn &other, nil\n}\n\n// ReleaseRemoteHaltLock releases the current remote lock from the primary.\nfunc (db *DB) ReleaseRemoteHaltLock(ctx context.Context, lockID int64) (retErr error) {\n\tif err := db.UnsetRemoteHaltLock(ctx, lockID); err != nil {\n\t\treturn err\n\t}\n\n\tisPrimary, info := db.store.PrimaryInfo()\n\tif isPrimary {\n\t\treturn nil // no remote halting on primary\n\t} else if info == nil {\n\t\treturn fmt.Errorf(\"no primary available to release remote halt lock\")\n\t}\n\n\tif err := db.store.Client.ReleaseHaltLock(ctx, info.AdvertiseURL, db.store.ID(), db.name, lockID); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// UnsetRemoteHaltLock releases the current remote lock because of expiration.\n// This only removes the reference locally as it's assumed it has already been\n// removed on the primary.\nfunc (db *DB) UnsetRemoteHaltLock(ctx context.Context, lockID int64) (retErr error) {\n\tTraceLog.Printf(\"[UnsetRemoteHaltLock(%s)]:\", db.name)\n\n\thaltLock := db.remoteHaltLock.Load().(*HaltLock)\n\tif haltLock == nil {\n\t\tTraceLog.Printf(\"[UnsetRemoteHaltLock.Done(%s)]: id=%d no-lock\", db.name, lockID)\n\t\treturn nil\n\t} else if haltLock.ID != lockID {\n\t\tTraceLog.Printf(\"[UnsetRemoteHaltLock.Done(%s)]: id=%d curr=%d not-current-lock\", db.name, lockID, haltLock.ID)\n\t\treturn nil\n\t}\n\tdefer func() {\n\t\tTraceLog.Printf(\"[UnsetRemoteHaltLock.Done(%s)]: %s\", db.name, errorKeyValue(retErr))\n\t}()\n\n\t// Checkpoint when we release the remote lock.\n\tif err := db.Recover(ctx); err != nil {\n\t\treturn fmt.Errorf(\"recovery: %w\", err)\n\t}\n\n\t// Clear local reference before releasing from primary.\n\t// This avoids a race condition where the next LTX file comes in before release confirmation.\n\tdb.remoteHaltLock.CompareAndSwap(haltLock, (*HaltLock)(nil))\n\n\treturn nil\n}\n\n// WaitPosExact returns once db has reached the target position.\n// Returns an error if ctx is done, TXID is exceeded, or on checksum mismatch.\nfunc (db *DB) WaitPosExact(ctx context.Context, target ltx.Pos) error {\n\tticker := time.NewTicker(WaitInterval)\n\tdefer ticker.Stop()\n\n\t// TODO(fwd): Check for primary change.\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn context.Cause(ctx)\n\t\tcase <-ticker.C:\n\t\t\tpos := db.Pos()\n\t\t\tif pos.TXID < target.TXID {\n\t\t\t\tcontinue // not there yet, try again\n\t\t\t}\n\t\t\tif pos.TXID > target.TXID {\n\t\t\t\treturn fmt.Errorf(\"target transaction id exceeded: %s > %s\", pos.TXID.String(), target.TXID.String())\n\t\t\t}\n\t\t\tif pos.PostApplyChecksum != target.PostApplyChecksum {\n\t\t\t\treturn fmt.Errorf(\"target checksum mismatch: %s != %s\", pos.PostApplyChecksum, target.PostApplyChecksum)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// RemoteHaltLock returns a copy of the current remote lock, if any.\nfunc (db *DB) RemoteHaltLock() *HaltLock {\n\tvalue := db.remoteHaltLock.Load().(*HaltLock)\n\tif value == nil {\n\t\treturn nil\n\t}\n\tother := *value\n\treturn &other\n}\n\n// HasRemoteHaltLock returns true if the node currently has the remote lock acquired.\nfunc (db *DB) HasRemoteHaltLock() bool {\n\treturn db.remoteHaltLock.Load().(*HaltLock) != nil\n}\n\n// Writeable returns true if the node is the primary or if we've acquire the\n// HALT lock from the primary.\nfunc (db *DB) Writeable() bool {\n\treturn db.HasRemoteHaltLock() || db.store.IsPrimary()\n}\n\n// TXID returns the current transaction ID.\nfunc (db *DB) TXID() ltx.TXID { return db.Pos().TXID }\n\n// Open initializes the database from files in its data directory.\nfunc (db *DB) Open() error {\n\t// Read page size & page count from database file.\n\tif err := db.initFromDatabaseHeader(); err != nil {\n\t\treturn fmt.Errorf(\"init from database header: %w\", err)\n\t}\n\n\t// Ensure \"ltx\" directory exists.\n\tif err := db.os.MkdirAll(\"OPEN\", db.LTXDir(), 0o777); err != nil {\n\t\treturn err\n\t}\n\n\t// Remove all SHM files on start up.\n\tif err := db.os.Remove(\"OPEN:SHM\", db.SHMPath()); err != nil && !os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"remove shm: %w\", err)\n\t}\n\n\t// Determine the last LTX file to replay from, if any.\n\tltxFilename, err := db.maxLTXFile(context.Background())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"max ltx file: %w\", err)\n\t}\n\n\t// Sync up WAL and last LTX file, if they both exist.\n\tif ltxFilename != \"\" {\n\t\tif err := db.syncWALToLTX(context.Background(), ltxFilename); err != nil {\n\t\t\treturn fmt.Errorf(\"sync wal to ltx: %w\", err)\n\t\t}\n\t}\n\n\t// Reset the rollback journal and/or WAL.\n\tif err := db.recover(context.Background()); err != nil {\n\t\treturn fmt.Errorf(\"recover: %w\", err)\n\t}\n\n\t// Verify database header & initialize checksums.\n\tif err := db.initDatabaseFile(); err != nil {\n\t\treturn fmt.Errorf(\"init database file: %w\", err)\n\t}\n\n\t// Apply the last LTX file so our checksums match if there was a failure in\n\t// between the LTX commit and journal/WAL commit.\n\tif ltxFilename != \"\" {\n\t\tguard, err := db.AcquireWriteLock(context.Background(), nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer guard.Unlock()\n\n\t\tif err := db.ApplyLTXNoLock(ltxFilename, false); err != nil {\n\t\t\treturn fmt.Errorf(\"recover ltx: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// initFromDatabaseHeader reads the page size & page count from the database file header.\nfunc (db *DB) initFromDatabaseHeader() error {\n\tf, err := db.os.Open(\"INITDBHDR\", db.DatabasePath())\n\tif os.IsNotExist(err) {\n\t\treturn nil // no database file yet, skip\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\t// Read page size into memory.\n\thdr, _, err := readSQLiteDatabaseHeader(f)\n\tif err == io.EOF {\n\t\treturn nil\n\t} else if err == errInvalidDatabaseHeader { // invalid file\n\t\tlog.Printf(\"invalid database header on %q, clearing data files\", db.name)\n\t\tif err := db.clean(); err != nil {\n\t\t\treturn fmt.Errorf(\"clean: %w\", err)\n\t\t}\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdb.pageSize = hdr.PageSize\n\tdb.pageN.Store(hdr.PageN)\n\n\t// Initialize database mode.\n\tif hdr.WriteVersion == 2 && hdr.ReadVersion == 2 {\n\t\tdb.mode.Store(DBModeWAL)\n\t} else {\n\t\tdb.mode.Store(DBModeRollback)\n\t}\n\n\treturn nil\n}\n\n// Recover forces a rollback (journal) or checkpoint (wal).\nfunc (db *DB) Recover(ctx context.Context) error {\n\tguard, err := db.AcquireWriteLock(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer guard.Unlock()\n\treturn db.recover(ctx)\n}\n\nfunc (db *DB) recover(ctx context.Context) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[Recover(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\t// If a journal file exists, rollback the last transaction so that we\n\t// are in a consistent state before applying our last LTX file. Otherwise\n\t// we could have a partial transaction in our database, apply our LTX, and\n\t// then the SQLite client will recover from the journal and corrupt everything.\n\t//\n\t// See: https://github.com/superfly/litefs/issues/134\n\tif err := db.rollbackJournal(ctx); err != nil {\n\t\treturn fmt.Errorf(\"rollback journal: %w\", err)\n\t}\n\n\t// Copy the WAL file back to the main database. This ensures that we can\n\t// compute the checksum only using the database file instead of having to\n\t// first compute the latest page set from the WAL to overlay.\n\tif err := db.CheckpointNoLock(ctx); err != nil {\n\t\treturn fmt.Errorf(\"checkpoint: %w\", err)\n\t}\n\treturn nil\n}\n\n// rollbackJournal copies all the pages from an existing rollback journal back\n// to the database file. This is called on startup so that we can be in a\n// consistent state in order to verify our checksums.\nfunc (db *DB) rollbackJournal(ctx context.Context) error {\n\tjournalFile, err := db.os.OpenFile(\"ROLLBACKJOURNAL\", db.JournalPath(), os.O_RDWR, 0o666)\n\tif os.IsNotExist(err) {\n\t\treturn nil // no journal file, skip\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = journalFile.Close() }()\n\n\tdbFile, err := db.os.OpenFile(\"ROLLBACKJOURNALDB\", db.DatabasePath(), os.O_RDWR, 0o666)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = dbFile.Close() }()\n\n\t// Copy every journal page back into the main database file.\n\tr := NewJournalReader(journalFile, db.pageSize)\n\tfor i := 0; ; i++ {\n\t\tif err := r.Next(); err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"next segment(%d): %w\", i, err)\n\t\t}\n\t\tif err := db.rollbackJournalSegment(ctx, r, dbFile); err != nil {\n\t\t\treturn fmt.Errorf(\"segment(%d): %w\", i, err)\n\t\t}\n\t}\n\n\t// Resize database to size before journal transaction, if a valid header exists.\n\tif r.IsValid() {\n\t\tif err := db.truncateDatabase(dbFile, r.commit); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdb.pageN.Store(r.commit)\n\t}\n\n\tif err := dbFile.Sync(); err != nil {\n\t\treturn err\n\t} else if err := dbFile.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := journalFile.Close(); err != nil {\n\t\treturn err\n\t} else if err := db.os.Remove(\"ROLLBACKJOURNAL\", db.JournalPath()); err != nil {\n\t\treturn err\n\t}\n\n\tif invalidator := db.store.Invalidator; invalidator != nil {\n\t\tif err := invalidator.InvalidateEntry(db.name + \"-journal\"); err != nil {\n\t\t\treturn fmt.Errorf(\"invalidate journal: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) rollbackJournalSegment(ctx context.Context, r *JournalReader, dbFile *os.File) error {\n\tfor i := 0; ; i++ {\n\t\tpgno, data, err := r.ReadFrame()\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"read frame(%d): %w\", i, err)\n\t\t}\n\n\t\t// Write data to the database file.\n\t\tif err := db.writeDatabasePage(dbFile, pgno, data, true); err != nil {\n\t\t\treturn fmt.Errorf(\"write to database (pgno=%d): %w\", pgno, err)\n\t\t}\n\t}\n}\n\n// Checkpoint acquires locks and copies pages from the WAL into the database and truncates the WAL.\nfunc (db *DB) Checkpoint(ctx context.Context) (err error) {\n\tguard, err := db.AcquireWriteLock(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer guard.Unlock()\n\n\treturn db.CheckpointNoLock(ctx)\n}\n\n// CheckpointNoLock copies pages from the WAL into the database and truncates the WAL.\n// Appropriate locks must be held by the caller.\nfunc (db *DB) CheckpointNoLock(ctx context.Context) (err error) {\n\tTraceLog.Printf(\"[CheckpointBegin(%s)]\", db.name)\n\tdefer func() {\n\t\tTraceLog.Printf(\"[CheckpointDone(%s)] %v\", db.name, err)\n\t}()\n\n\t// Open the database file we'll checkpoint into. Skip if this hasn't been created.\n\tdbFile, err := db.os.OpenFile(\"CHECKPOINT:DB\", db.DatabasePath(), os.O_RDWR, 0o666)\n\tif os.IsNotExist(err) {\n\t\treturn nil // no database file yet, skip\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = dbFile.Close() }()\n\n\t// Open the WAL file that we'll copy from. Skip if it was cleanly closed and removed.\n\twalFile, err := db.os.Open(\"CHECKPOINT:WAL\", db.WALPath())\n\tif os.IsNotExist(err) {\n\t\treturn nil // no WAL file, skip\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = walFile.Close() }()\n\n\toffsets, commit, err := db.readWALPageOffsets(walFile)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"read wal page offsets: %w\", err)\n\t}\n\n\t// Copy pages from the WAL to the main database file & resize db file.\n\tif len(offsets) > 0 {\n\t\tbuf := make([]byte, db.pageSize)\n\t\tfor pgno, offset := range offsets {\n\t\t\tif _, err := walFile.Seek(offset+WALFrameHeaderSize, io.SeekStart); err != nil {\n\t\t\t\treturn fmt.Errorf(\"seek wal: %w\", err)\n\t\t\t} else if _, err := io.ReadFull(walFile, buf); err != nil {\n\t\t\t\treturn fmt.Errorf(\"read wal: %w\", err)\n\t\t\t}\n\n\t\t\tif err := db.writeDatabasePage(dbFile, pgno, buf, true); err != nil {\n\t\t\t\treturn fmt.Errorf(\"write db page %d: %w\", pgno, err)\n\t\t\t}\n\t\t}\n\n\t\tif err := db.truncateDatabase(dbFile, commit); err != nil {\n\t\t\treturn fmt.Errorf(\"truncate: %w\", err)\n\t\t}\n\n\t\t// Save the size of the database, in pages, based on last commit.\n\t\tdb.pageN.Store(commit)\n\t}\n\n\t// Remove WAL file.\n\tif err := db.TruncateWAL(ctx, 0); err != nil {\n\t\treturn fmt.Errorf(\"truncate wal: %w\", err)\n\t}\n\n\t// Clear per-page checksums within WAL.\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\t// Update the SHM file.\n\tif err := db.updateSHM(); err != nil {\n\t\treturn fmt.Errorf(\"update shm: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// readWALPageOffsets returns a map of the offsets of the last committed version\n// of each page in the WAL. Also returns the commit size of the last transaction.\nfunc (db *DB) readWALPageOffsets(f *os.File) (_ map[uint32]int64, lastCommit uint32, _ error) {\n\tr := NewWALReader(f)\n\tif err := r.ReadHeader(); err == io.EOF {\n\t\treturn nil, 0, nil\n\t}\n\n\t// Read the offset of the last version of each page in the WAL.\n\toffsets := make(map[uint32]int64)\n\ttxOffsets := make(map[uint32]int64)\n\tbuf := make([]byte, r.PageSize())\n\tfor {\n\t\tpgno, commit, err := r.ReadFrame(buf)\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\t// Save latest offset for each page version.\n\t\ttxOffsets[pgno] = r.Offset()\n\n\t\t// If this is not a committing frame, continue to next frame.\n\t\tif commit == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// At the end of each transaction, copy offsets to main map.\n\t\tlastCommit = commit\n\t\tfor k, v := range txOffsets {\n\t\t\toffsets[k] = v\n\t\t}\n\t\ttxOffsets = make(map[uint32]int64)\n\t}\n\n\treturn offsets, lastCommit, nil\n}\n\n// maxLTXFile returns the filename of the highest LTX file.\nfunc (db *DB) maxLTXFile(ctx context.Context) (string, error) {\n\tents, err := db.os.ReadDir(\"MAXLTX\", db.LTXDir())\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tvar max ltx.TXID\n\tvar filename string\n\tfor _, ent := range ents {\n\t\t_, maxTXID, err := ltx.ParseFilename(ent.Name())\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t} else if maxTXID <= max {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Save filename with the highest TXID.\n\t\tmax, filename = maxTXID, filepath.Join(db.LTXDir(), ent.Name())\n\t}\n\treturn filename, nil\n}\n\n// syncWALToLTX truncates the WAL file to the last LTX file if the WAL info\n// in the LTX header does not match. This protects against a hard shutdown\n// where a WAL file was sync'd past the last LTX file.\nfunc (db *DB) syncWALToLTX(ctx context.Context, ltxFilename string) error {\n\t// Open last LTX file.\n\tltxFile, err := db.os.Open(\"SYNCWAL:LTX\", ltxFilename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = ltxFile.Close() }()\n\n\t// Read header from LTX file to determine WAL fields.\n\t// This also validates the LTX file before it gets processed by ApplyLTX().\n\tdec := ltx.NewDecoder(ltxFile)\n\tif err := dec.Verify(); err != nil {\n\t\treturn fmt.Errorf(\"validate ltx: %w\", err)\n\t}\n\tltxWALSize := dec.Header().WALOffset + dec.Header().WALSize\n\n\t// Open WAL file, ignore if it doesn't exist.\n\twalFile, err := db.os.OpenFile(\"SYNCWAL:WAL\", db.WALPath(), os.O_RDWR, 0o666)\n\tif os.IsNotExist(err) {\n\t\tlog.Printf(\"wal-sync: no wal file exists on %q, skipping sync with ltx\", db.name)\n\t\treturn nil // no wal file, nothing to do\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = walFile.Close() }()\n\n\t// Determine WAL size.\n\tfi, err := walFile.Stat()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Read WAL header.\n\thdr := make([]byte, WALHeaderSize)\n\tif _, err := internal.ReadFullAt(walFile, hdr, 0); err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\tlog.Printf(\"wal-sync: short wal file exists on %q, skipping sync with ltx\", db.name)\n\t\treturn nil // short WAL header, skip\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\t// If WAL salt doesn't match the LTX WAL salt then the WAL has been\n\t// restarted and we need to remove it. We are just renaming it for now so\n\t// we can debug in case this happens.\n\tsalt1 := binary.BigEndian.Uint32(hdr[16:])\n\tsalt2 := binary.BigEndian.Uint32(hdr[20:])\n\tif salt1 != dec.Header().WALSalt1 || salt2 != dec.Header().WALSalt2 {\n\t\tlog.Printf(\"wal-sync: wal salt mismatch on %q, removing wal\", db.name)\n\t\tif err := db.os.Rename(\"SYNCWAL\", db.WALPath(), db.WALPath()+\".removed\"); err != nil {\n\t\t\treturn fmt.Errorf(\"wal-sync: rename wal file with salt mismatch: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// If the salt matches then we need to make sure we are at least up to the\n\t// start of the last LTX transaction.\n\tif fi.Size() < dec.Header().WALOffset {\n\t\treturn fmt.Errorf(\"wal-sync: short wal size (%d bytes) for %q, last ltx offset at %d bytes\", fi.Size(), db.name, dec.Header().WALOffset)\n\t}\n\n\t// Resize WAL back to size in the LTX file.\n\tif fi.Size() > ltxWALSize {\n\t\tlog.Printf(\"wal-sync: truncating wal of %q from %d bytes to %d bytes to match ltx\", db.name, fi.Size(), ltxWALSize)\n\t\tif err := walFile.Truncate(ltxWALSize); err != nil {\n\t\t\treturn fmt.Errorf(\"truncate wal: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\tlog.Printf(\"wal-sync: database %q has wal size of %d bytes within range of ltx file (@%d, %d bytes)\", db.name, fi.Size(), dec.Header().WALOffset, dec.Header().WALSize)\n\treturn nil\n}\n\n// initDatabaseFile opens and validates the database file, if it exists.\n// The journal & WAL should not exist at this point. The journal should be\n// rolled back and the WAL should be checkpointed.\nfunc (db *DB) initDatabaseFile() error {\n\tf, err := db.os.Open(\"INITDBFILE\", db.DatabasePath())\n\tif os.IsNotExist(err) {\n\t\tlog.Printf(\"database file does not exist on initialization: %s\", db.DatabasePath())\n\t\treturn nil // no database file yet\n\t} else if err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\thdr, _, err := readSQLiteDatabaseHeader(f)\n\tif err == io.EOF {\n\t\tlog.Printf(\"database file is zero length on initialization: %s\", db.DatabasePath())\n\t\treturn nil // no contents yet\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"cannot read database header: %w\", err)\n\t}\n\tdb.pageSize = hdr.PageSize\n\tdb.pageN.Store(hdr.PageN)\n\n\tassert(db.pageSize > 0, \"page size must be greater than zero\")\n\n\tdb.chksums.mu.Lock()\n\tdefer db.chksums.mu.Unlock()\n\n\t// Build per-page checksum map for existing pages. The database could be\n\t// short compared to the page count in the header so just checksum what we\n\t// can. The database may recover in applyLTX() so we'll do validation then.\n\tdb.chksums.pages = make([]ltx.Checksum, db.PageN())\n\tdb.chksums.blocks = make([]ltx.Checksum, pageChksumBlock(db.PageN()))\n\n\tlastGoodPage, err := ltx.ChecksumPages(db.DatabasePath(), db.pageSize, db.PageN(), 0, db.chksums.pages)\n\n\t// lastGoodPage tells us how far we got before the first error. Most likely\n\t// is that we got an EOF because the db was short, in which case no\n\t// subsequent checksums would have been written. To be cautious though,\n\t// zero out any checksums after the last good page.\n\tclear(db.chksums.pages[lastGoodPage:])\n\n\t// Always overwrite the lock page as a zero checksum.\n\tif lockPage := ltx.LockPgno(db.pageSize); len(db.chksums.pages) >= int(lockPage) {\n\t\tdb.chksums.pages[lockPage-1] = 0\n\t}\n\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\tlog.Printf(\"database checksum ending early at page %d of %d \", lastGoodPage, db.PageN())\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"read database page %d: %w\", lastGoodPage+1, err)\n\t}\n\n\treturn nil\n}\n\n// clean deletes and recreates the database data directory.\nfunc (db *DB) clean() error {\n\tif err := db.os.RemoveAll(\"CLEAN\", db.path); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn db.os.Mkdir(\"CLEAN\", db.path, 0o777)\n}\n\n// OpenLTXFile returns a file handle to an LTX file that contains the given TXID.\nfunc (db *DB) OpenLTXFile(txID ltx.TXID) (*os.File, error) {\n\treturn db.os.Open(\"OPENLTX\", db.LTXPath(txID, txID))\n}\n\n// OpenDatabase returns a handle for the database file.\nfunc (db *DB) OpenDatabase(ctx context.Context) (*os.File, error) {\n\tf, err := db.os.OpenFile(\"OPENDB\", db.DatabasePath(), os.O_RDWR, 0o666)\n\tTraceLog.Printf(\"[OpenDatabase(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// CloseDatabase closes a handle associated with the database file.\nfunc (db *DB) CloseDatabase(ctx context.Context, f *os.File, owner uint64) error {\n\terr := f.Close()\n\tTraceLog.Printf(\"[CloseDatabase(%s)]: owner=%d %s\", db.name, owner, errorKeyValue(err))\n\treturn err\n}\n\n// TruncateDatabase sets the size of the database file.\nfunc (db *DB) TruncateDatabase(ctx context.Context, size int64) (err error) {\n\t// Require the page size because we need to check against the page count & checksums.\n\tif db.pageSize == 0 {\n\t\treturn fmt.Errorf(\"page size required on database truncation\")\n\t} else if size%int64(db.pageSize) != 0 {\n\t\treturn fmt.Errorf(\"size must be page-aligned (%d bytes)\", db.pageSize)\n\t}\n\n\t// Verify new size matches the database size specified in the header.\n\tpageN := uint32(size / int64(db.pageSize))\n\tif pageN != db.PageN() {\n\t\treturn fmt.Errorf(\"truncation size (%d pages) does not match database header size (%d pages)\", pageN, db.PageN())\n\t}\n\n\t// Process the actual file system truncation.\n\tif f, err := db.os.OpenFile(\"TRUNCATE\", db.DatabasePath(), os.O_RDWR, 0o666); err != nil {\n\t\treturn err\n\t} else if err := db.truncateDatabase(f, pageN); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t} else if err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// truncateDatabase truncates the database to a given page count.\nfunc (db *DB) truncateDatabase(f *os.File, pageN uint32) (err error) {\n\tprevPageN := db.pageN.Load()\n\n\tdefer func() {\n\t\tTraceLog.Printf(\"[TruncateDatabase(%s)]: pageN=%d prevPageN=%d pageSize=%d %s\", db.name, pageN, prevPageN, db.pageSize, errorKeyValue(err))\n\t}()\n\n\tif err := f.Truncate(int64(pageN) * int64(db.pageSize)); err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\treturn err\n\t}\n\n\tdb.chksums.mu.Lock()\n\tdb.resetDatabasePageChecksumsAfter(pageN)\n\tdb.chksums.mu.Unlock()\n\n\treturn nil\n}\n\n// SyncDatabase fsync's the database file.\nfunc (db *DB) SyncDatabase(ctx context.Context) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[SyncDatabase(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\tf, err := db.os.Open(\"SYNCDB\", db.DatabasePath())\n\tif err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n\n// ReadDatabaseAt reads from the database at the specified index.\nfunc (db *DB) ReadDatabaseAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (int, error) {\n\tn, err := f.ReadAt(data, offset)\n\n\t// Compute checksum if page aligned.\n\tvar chksum string\n\tvar pgno uint32\n\tif db.pageSize != 0 && offset%int64(db.pageSize) == 0 && len(data) == int(db.pageSize) {\n\t\tpgno = uint32(offset/int64(db.pageSize)) + 1\n\t\tchksum = ltx.ChecksumPage(pgno, data).String()\n\t}\n\tTraceLog.Printf(\"[ReadDatabaseAt(%s)]: offset=%d size=%d pgno=%d chksum=%s owner=%d %s\", db.name, offset, len(data), pgno, chksum, owner, errorKeyValue(err))\n\n\treturn n, err\n}\n\n// WriteDatabaseAt writes data to the main database file at the given index.\nfunc (db *DB) WriteDatabaseAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) error {\n\t// Return an error if the current process is not the leader.\n\tif !db.Writeable() {\n\t\treturn ErrReadOnlyReplica\n\t} else if len(data) == 0 {\n\t\treturn nil\n\t}\n\n\t// Use page size from the write.\n\tif db.pageSize == 0 {\n\t\tif offset != 0 {\n\t\t\treturn fmt.Errorf(\"cannot determine page size, initial offset (%d) is non-zero\", offset)\n\t\t}\n\t\thdr, _, err := readSQLiteDatabaseHeader(bytes.NewReader(data))\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot read sqlite database header: %w\", err)\n\t\t}\n\t\tdb.pageSize = hdr.PageSize\n\t}\n\n\t// Require that writes are a single page and are page-aligned.\n\t// This allows us to track per-page checksums and detect errors on commit.\n\tif offset%int64(db.pageSize) != 0 {\n\t\treturn fmt.Errorf(\"database writes must be page-aligned (%d bytes)\", db.pageSize)\n\t} else if len(data) != int(db.pageSize) {\n\t\treturn fmt.Errorf(\"database write must be exactly one page (%d bytes)\", db.pageSize)\n\t}\n\n\t// Track dirty pages if we are using a rollback journal. This isn't\n\t// necessary with the write-ahead log (WAL) since pages are appended\n\t// instead of overwritten. We can determine the dirty set at commit-time.\n\tpgno := uint32(offset/int64(db.pageSize)) + 1\n\tif db.Mode() == DBModeRollback {\n\t\tdb.dirtyPageSet[pgno] = struct{}{}\n\t}\n\n\t// Perform write on handle.\n\tif err := db.writeDatabasePage(f, pgno, data, false); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// writeDatabasePage writes a page to the database file.\nfunc (db *DB) writeDatabasePage(f *os.File, pgno uint32, data []byte, invalidate bool) (err error) {\n\tvar prevChksum, newChksum ltx.Checksum\n\tdefer func() {\n\t\tTraceLog.Printf(\"[WriteDatabasePage(%s)]: pgno=%d chksum=%s prev=%s %s\", db.name, pgno, newChksum, prevChksum, errorKeyValue(err))\n\t}()\n\n\tassert(db.pageSize != 0, \"page size required\")\n\tif len(data) != int(db.pageSize) {\n\t\treturn fmt.Errorf(\"database write (%d bytes) must be a single page (%d bytes)\", len(data), db.pageSize)\n\t}\n\n\t// Issue write to database.\n\toffset := (int64(pgno) - 1) * int64(db.pageSize)\n\tif _, err := f.WriteAt(data, offset); err != nil {\n\t\treturn err\n\t}\n\tdbDatabaseWriteCountMetricVec.WithLabelValues(db.name).Inc()\n\n\t// Update in-memory checksum.\n\tnewChksum = ltx.ChecksumPage(pgno, data)\n\n\tdb.chksums.mu.Lock()\n\tprevChksum = db.databasePageChecksum(pgno)\n\tdb.setDatabasePageChecksum(pgno, newChksum)\n\tdb.chksums.mu.Unlock()\n\n\tif invalidator := db.store.Invalidator; invalidator != nil && invalidate {\n\t\tif err := invalidator.InvalidateDBRange(db, offset, int64(len(data))); err != nil {\n\t\t\treturn fmt.Errorf(\"invalidate db page: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// UnlockDatabase unlocks all locks from the database file.\nfunc (db *DB) UnlockDatabase(ctx context.Context, owner uint64) {\n\tguardSet := db.GuardSet(owner)\n\tif guardSet == nil {\n\t\treturn\n\t}\n\tTraceLog.Printf(\"[UnlockDatabase(%s)]: owner=%d\", db.name, owner)\n\tguardSet.UnlockDatabase()\n}\n\n// CreateJournal creates a new journal file on disk.\nfunc (db *DB) CreateJournal() (*os.File, error) {\n\tif !db.Writeable() {\n\t\tTraceLog.Printf(\"[CreateJournal(%s)]: %s\", db.name, errorKeyValue(ErrReadOnlyReplica))\n\t\treturn nil, ErrReadOnlyReplica\n\t}\n\n\tf, err := db.os.OpenFile(\"CREATEJOURNAL\", db.JournalPath(), os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_TRUNC, 0o666)\n\tTraceLog.Printf(\"[CreateJournal(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// OpenJournal returns a handle for the journal file.\nfunc (db *DB) OpenJournal(ctx context.Context) (*os.File, error) {\n\tf, err := db.os.OpenFile(\"OPENJOURNAL\", db.JournalPath(), os.O_RDWR, 0o666)\n\tTraceLog.Printf(\"[OpenJournal(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// CloseJournal closes a handle associated with the journal file.\nfunc (db *DB) CloseJournal(ctx context.Context, f *os.File, owner uint64) error {\n\terr := f.Close()\n\tTraceLog.Printf(\"[CloseJournal(%s)]: owner=%d %s\", db.name, owner, errorKeyValue(err))\n\treturn err\n}\n\n// TruncateJournal sets the size of the journal file.\nfunc (db *DB) TruncateJournal(ctx context.Context) error {\n\terr := db.CommitJournal(ctx, JournalModeTruncate)\n\tTraceLog.Printf(\"[TruncateJournal(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn err\n}\n\n// SyncJournal fsync's the journal file.\nfunc (db *DB) SyncJournal(ctx context.Context) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[SyncJournal(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\tf, err := db.os.Open(\"SYNCJOURNAL\", db.JournalPath())\n\tif err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n\n// RemoveJournal deletes the journal file from disk.\nfunc (db *DB) RemoveJournal(ctx context.Context) error {\n\terr := db.CommitJournal(ctx, JournalModeDelete)\n\tTraceLog.Printf(\"[RemoveJournal(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn err\n}\n\n// ReadJournalAt reads from the journal at the specified offset.\nfunc (db *DB) ReadJournalAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (int, error) {\n\tn, err := f.ReadAt(data, offset)\n\tTraceLog.Printf(\"[ReadJournalAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\treturn n, err\n}\n\n// WriteJournal writes data to the rollback journal file.\nfunc (db *DB) WriteJournalAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (err error) {\n\tdefer func() {\n\t\tvar buf []byte\n\t\tif len(data) == 4 { // pgno or chksum\n\t\t\tbuf = data\n\t\t} else if offset == 0 { // show initial header\n\t\t\tbuf = data\n\t\t\tif len(data) > SQLITE_JOURNAL_HEADER_SIZE {\n\t\t\t\tbuf = data[:SQLITE_JOURNAL_HEADER_SIZE]\n\t\t\t}\n\t\t}\n\n\t\tTraceLog.Printf(\"[WriteJournalAt(%s)]: offset=%d size=%d data=%x owner=%d %s\", db.name, offset, len(data), buf, owner, errorKeyValue(err))\n\t}()\n\n\tif !db.Writeable() {\n\t\treturn ErrReadOnlyReplica\n\t}\n\n\t// Set the page size on initial journal header write.\n\tif offset == 0 && len(data) >= SQLITE_JOURNAL_HEADER_SIZE && db.pageSize == 0 {\n\t\tdb.pageSize = binary.BigEndian.Uint32(data[24:])\n\t}\n\n\tdbJournalWriteCountMetricVec.WithLabelValues(db.name).Inc()\n\n\t// Assume this is a PERSIST commit if the initial header bytes are cleared.\n\tif offset == 0 && len(data) == SQLITE_JOURNAL_HEADER_SIZE && isByteSliceZero(data) {\n\t\tif err := db.CommitJournal(ctx, JournalModePersist); err != nil {\n\t\t\treturn fmt.Errorf(\"commit journal (PERSIST): %w\", err)\n\t\t}\n\t}\n\n\t// Passthrough write\n\t_, err = f.WriteAt(data, offset)\n\treturn err\n}\n\n// CreateWAL creates a new WAL file on disk.\nfunc (db *DB) CreateWAL() (*os.File, error) {\n\tf, err := db.os.OpenFile(\"CREATEWAL\", db.WALPath(), os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_TRUNC, 0o666)\n\tTraceLog.Printf(\"[CreateWAL(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// OpenWAL returns a handle for the write-ahead log file.\nfunc (db *DB) OpenWAL(ctx context.Context) (*os.File, error) {\n\tf, err := db.os.OpenFile(\"OPENWAL\", db.WALPath(), os.O_RDWR, 0o666)\n\tTraceLog.Printf(\"[OpenWAL(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// CloseWAL closes a handle associated with the WAL file.\nfunc (db *DB) CloseWAL(ctx context.Context, f *os.File, owner uint64) error {\n\terr := f.Close()\n\tTraceLog.Printf(\"[CloseWAL(%s)]: owner=%d %s\", db.name, owner, errorKeyValue(err))\n\treturn err\n}\n\n// TruncateWAL sets the size of the WAL file.\nfunc (db *DB) TruncateWAL(ctx context.Context, size int64) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[TruncateWAL(%s)]: size=%d %s\", db.name, size, errorKeyValue(err))\n\t}()\n\n\tif size != 0 {\n\t\treturn fmt.Errorf(\"wal can only be truncated to zero\")\n\t}\n\tif err := db.os.Truncate(\"TRUNCATEWAL\", db.WALPath(), size); err != nil {\n\t\treturn err\n\t}\n\n\t// Clear all per-page checksums for the WAL.\n\tdb.wal.frameOffsets = make(map[uint32]int64)\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\treturn nil\n}\n\n// RemoveWAL deletes the WAL file from disk.\nfunc (db *DB) RemoveWAL(ctx context.Context) (err error) {\n\tdefer func() { TraceLog.Printf(\"[RemoveWAL(%s)]: %s\", db.name, errorKeyValue(err)) }()\n\n\tif err := db.os.Remove(\"REMOVEWAL\", db.WALPath()); err != nil {\n\t\treturn err\n\t}\n\n\t// Clear all per-page checksums for the WAL.\n\tdb.wal.frameOffsets = make(map[uint32]int64)\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\treturn nil\n}\n\n// SyncWAL fsync's the WAL file.\nfunc (db *DB) SyncWAL(ctx context.Context) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[SyncWAL(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\tf, err := db.os.Open(\"SYNCWAL\", db.WALPath())\n\tif err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n\n// ReadWALAt reads from the WAL at the specified index.\nfunc (db *DB) ReadWALAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (int, error) {\n\tn, err := f.ReadAt(data, offset)\n\tTraceLog.Printf(\"[ReadWALAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\treturn n, err\n}\n\n// WriteWALAt writes data to the WAL file. On final commit write, an LTX file is\n// generated for the transaction.\nfunc (db *DB) WriteWALAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (err error) {\n\t// Return an error if the current process is not the leader.\n\tif !db.Writeable() {\n\t\tTraceLog.Printf(\"[WriteWALAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\t\treturn ErrReadOnlyReplica\n\t} else if len(data) == 0 {\n\t\tTraceLog.Printf(\"[WriteWALAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\t\treturn nil\n\t}\n\n\tassert(db.pageSize != 0, \"page size cannot be zero for wal write\")\n\n\tdbWALWriteCountMetricVec.WithLabelValues(db.name).Inc()\n\n\t// WAL header writes always start at a zero offset and are 32 bytes in size.\n\tframeSize := WALFrameHeaderSize + int64(db.pageSize)\n\tif offset == 0 {\n\t\tif err := db.writeWALHeader(ctx, f, data, offset, owner); err != nil {\n\t\t\treturn fmt.Errorf(\"wal header: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// WAL frame headers can be full 24-byte headers or partial ones.\n\tif frameOffset := (offset - WALHeaderSize) % frameSize; frameOffset < WALFrameHeaderSize {\n\t\tif err := db.writeWALFrameHeader(ctx, f, data, offset, frameOffset, owner); err != nil {\n\t\t\treturn fmt.Errorf(\"wal frame header: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Otherwise this is a write to the page data portion of the WAL frame.\n\tif err := db.writeWALFrameData(ctx, f, data, offset, owner); err != nil {\n\t\treturn fmt.Errorf(\"wal frame data: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (db *DB) writeWALHeader(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[WriteWALHeader(%s)]: offset=%d size=%d salt1=%08x salt2=%08x chksum1=%08x chksum2=%08x owner=%d %s\",\n\t\t\tdb.name, offset, len(data), db.wal.salt1, db.wal.salt2, db.wal.chksum1, db.wal.chksum2, owner, errorKeyValue(err))\n\t}()\n\n\tif len(data) != WALHeaderSize {\n\t\treturn fmt.Errorf(\"WAL header write must be 32 bytes in size, received %d\", len(data))\n\t}\n\n\t// Prevent transactions when the write lock has not been acquired (e.g. EXCLUSIVE lock)\n\tif db.writeLock.State() != RWMutexStateExclusive {\n\t\treturn fmt.Errorf(\"cannot write to WAL header without WRITE lock, exclusive locking not allowed\")\n\t}\n\n\t// Determine byte order of checksums.\n\tswitch magic := binary.BigEndian.Uint32(data[0:]); magic {\n\tcase 0x377f0682:\n\t\tdb.wal.byteOrder = binary.LittleEndian\n\tcase 0x377f0683:\n\t\tdb.wal.byteOrder = binary.BigEndian\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid magic: %x\", magic)\n\t}\n\n\t// Set remaining WAL fields.\n\tdb.wal.offset = WALHeaderSize\n\tdb.wal.salt1 = binary.BigEndian.Uint32(data[16:])\n\tdb.wal.salt2 = binary.BigEndian.Uint32(data[20:])\n\tdb.wal.chksum1 = binary.BigEndian.Uint32(data[24:])\n\tdb.wal.chksum2 = binary.BigEndian.Uint32(data[28:])\n\tdb.wal.frameOffsets = make(map[uint32]int64)\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\t// Passthrough write to underlying WAL file.\n\t_, err = f.WriteAt(data, offset)\n\treturn err\n}\n\nfunc (db *DB) writeWALFrameHeader(ctx context.Context, f *os.File, data []byte, offset, frameOffset int64, owner uint64) (err error) {\n\tvar pgno, commit, salt1, salt2, chksum1, chksum2 uint32\n\tvar hexdata string\n\tif frameOffset == 0 && len(data) == WALFrameHeaderSize {\n\t\tpgno = binary.BigEndian.Uint32(data[0:])\n\t\tcommit = binary.BigEndian.Uint32(data[4:])\n\t\tsalt1 = binary.BigEndian.Uint32(data[8:])\n\t\tsalt2 = binary.BigEndian.Uint32(data[12:])\n\t\tchksum1 = binary.BigEndian.Uint32(data[16:])\n\t\tchksum2 = binary.BigEndian.Uint32(data[20:])\n\t} else {\n\t\thexdata = fmt.Sprintf(\"%x\", data)\n\t}\n\n\tdefer func() {\n\t\tif hexdata != \"\" {\n\t\t\tTraceLog.Printf(\"[WriteWALFrameHeader(%s)]: offset=%d size=%d data=%s owner=%d %s\",\n\t\t\t\tdb.name, offset, len(data), hexdata, owner, errorKeyValue(err))\n\t\t} else {\n\t\t\tTraceLog.Printf(\"[WriteWALFrameHeader(%s)]: offset=%d size=%d pgno=%d commit=%d salt1=%08x salt2=%08x chksum1=%08x chksum2=%08x owner=%d %s\",\n\t\t\t\tdb.name, offset, len(data), pgno, commit, salt1, salt2, chksum1, chksum2, owner, errorKeyValue(err))\n\t\t}\n\t}()\n\n\t// Prevent transactions when the write lock has not been acquired (e.g. EXCLUSIVE lock)\n\tif db.writeLock.State() != RWMutexStateExclusive {\n\t\treturn fmt.Errorf(\"cannot write to WAL frame header without WRITE lock, exclusive locking not allowed\")\n\t}\n\n\t// Prevent SQLite from writing before the current WAL position.\n\tif offset < db.wal.offset {\n\t\treturn fmt.Errorf(\"cannot write wal frame header @%d before current WAL position @%d\", offset, db.wal.offset)\n\t}\n\n\t// Passthrough write to underlying WAL file.\n\t_, err = f.WriteAt(data, offset)\n\treturn err\n}\n\nfunc (db *DB) writeWALFrameData(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[WriteWALFrameData(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\t}()\n\n\t// Prevent transactions when the write lock has not been acquired (e.g. EXCLUSIVE lock)\n\tif db.writeLock.State() != RWMutexStateExclusive {\n\t\treturn fmt.Errorf(\"cannot write to WAL frame data without WRITE lock, exclusive locking not allowed\")\n\t}\n\n\t// Prevent SQLite from writing before the current WAL position.\n\tif offset < db.wal.offset {\n\t\treturn fmt.Errorf(\"cannot write wal frame data @%d before current WAL position @%d\", offset, db.wal.offset)\n\t}\n\n\t// Passthrough write to underlying WAL file.\n\t_, err = f.WriteAt(data, offset)\n\treturn err\n}\n\nfunc (db *DB) buildTxFrameOffsets(walFile *os.File) (_ map[uint32]int64, commit, chksum1, chksum2 uint32, endOffset int64, err error) {\n\tm := make(map[uint32]int64)\n\n\toffset := db.wal.offset\n\tchksum1, chksum2 = db.wal.chksum1, db.wal.chksum2\n\tframe := make([]byte, WALFrameHeaderSize+int64(db.pageSize))\n\tfor i := 0; ; i++ {\n\t\t// Read frame data & exit if we hit the end of file.\n\t\tif _, err := internal.ReadFullAt(walFile, frame, offset); err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\t\tTraceLog.Printf(\"[buildTxFrames(%s)]: msg=read-error offset=%d size=%d err=%q\",\n\t\t\t\tdb.name, offset, len(frame), err)\n\t\t\treturn nil, 0, 0, 0, 0, errNoTransaction\n\t\t} else if err != nil {\n\t\t\treturn nil, 0, 0, 0, 0, fmt.Errorf(\"read wal frame: %w\", err)\n\t\t}\n\n\t\t// If salt doesn't match then we've run into a previous WAL's data.\n\t\tsalt1 := binary.BigEndian.Uint32(frame[8:])\n\t\tsalt2 := binary.BigEndian.Uint32(frame[12:])\n\t\tif db.wal.salt1 != salt1 || db.wal.salt2 != salt2 {\n\t\t\tTraceLog.Printf(\"[buildTxFrames(%s)]: msg=salt-mismatch offset=%d hdr-salt1=%08x hdr-salt2=%08x frame-salt1=%08x frame-salt2=%08x\",\n\t\t\t\tdb.name, offset, db.wal.salt1, db.wal.salt2, salt1, salt2)\n\t\t\treturn nil, 0, 0, 0, 0, errNoTransaction\n\t\t}\n\n\t\t// Verify checksum\n\t\tfchksum1 := binary.BigEndian.Uint32(frame[16:])\n\t\tfchksum2 := binary.BigEndian.Uint32(frame[20:])\n\t\tchksum1, chksum2 = WALChecksum(db.wal.byteOrder, chksum1, chksum2, frame[:8])  // frame header\n\t\tchksum1, chksum2 = WALChecksum(db.wal.byteOrder, chksum1, chksum2, frame[24:]) // frame data\n\t\tif chksum1 != fchksum1 || chksum2 != fchksum2 {\n\t\t\tTraceLog.Printf(\"[buildTxFrames(%s)]: msg=chksum-mismatch offset=%d chksum1=%08x chksum2=%08x frame-chksum1=%08x frame-chksum2=%08x\",\n\t\t\t\tdb.name, offset, chksum1, chksum2, fchksum1, fchksum2)\n\t\t\treturn nil, 0, 0, 0, 0, errNoTransaction\n\t\t}\n\n\t\t// Save the offset for the last version of the page to a map.\n\t\tpgno := binary.BigEndian.Uint32(frame[0:])\n\t\tm[pgno] = offset\n\n\t\t// End of transaction, exit loop and return.\n\t\tif commit = binary.BigEndian.Uint32(frame[4:]); commit != 0 {\n\t\t\treturn m, commit, chksum1, chksum2, offset + int64(len(frame)), nil\n\t\t}\n\n\t\t// Move to the next frame.\n\t\toffset += int64(len(frame))\n\t}\n}\n\n// errNoTransaction is a marker error to indicate that a transaction could not\n// be found at the current WAL offset.\nvar errNoTransaction = errors.New(\"no transaction\")\n\n// CommitWAL is called when the client releases the WAL_WRITE_LOCK(120).\n// The transaction data is copied from the WAL into an LTX file and committed.\nfunc (db *DB) CommitWAL(ctx context.Context) (err error) {\n\tvar msg string\n\tvar commit uint32\n\tvar txPageCount int\n\tvar pos ltx.Pos\n\tprevPos := db.Pos()\n\tprevPageN := db.PageN()\n\tdefer func() {\n\t\tTraceLog.Printf(\"[CommitWAL(%s)]: pos=%s prevPos=%s pages=%d commit=%d prevPageN=%d pageSize=%d msg=%q %s\\n\\n\",\n\t\t\tdb.name, pos, prevPos, txPageCount, commit, prevPageN, db.pageSize, msg, errorKeyValue(err))\n\t}()\n\twalFrameSize := int64(WALFrameHeaderSize + db.pageSize)\n\n\tTraceLog.Printf(\"[CommitWALBegin(%s)]: prev=%s offset=%d salt1=%08x salt2=%08x chksum1=%08x chksum2=%08x remote=%v\",\n\t\tdb.name, prevPos, db.wal.offset, db.wal.salt1, db.wal.salt2, db.wal.chksum1, db.wal.chksum2, db.HasRemoteHaltLock())\n\n\t// WAL commit occurs when the write lock is released so returning an error\n\t// doesn't affect the SQLite client and it will continue operating as usual.\n\t// For now, we need to fatally stop LiteFS when we encounter an error in the\n\t// final commit phase as we don't have control of the SQLite clients. This\n\t// prevents SQLite and LiteFS from becoming out of sync. On next startup,\n\t// LiteFS will perform a recovery and sync the database & LTX files correctly.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tTraceLog.Printf(\"[FATAL(%s)]: err=%d\\n\", db.name, err)\n\t\t\tlog.Printf(\"fatal error occurred while committing WAL to %q: %s\\n\", db.name, err)\n\t\t\tdb.store.Exit(99)\n\t\t}\n\t}()\n\n\twalFile, err := db.os.Open(\"COMMITWAL:WAL\", db.WALPath())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"open wal file: %w\", err)\n\t}\n\tdefer func() { _ = walFile.Close() }()\n\n\t// Sync WAL to disk as this avoids data loss issues with SYNCHRONOUS=normal\n\tif err := walFile.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"sync wal: %w\", err)\n\t}\n\n\t// Build offset map for the last version of each page in the WAL transaction.\n\t// If txFrameOffsets has no entries then a transaction could not be found after db.wal.offset.\n\ttxFrameOffsets, commit, chksum1, chksum2, endOffset, err := db.buildTxFrameOffsets(walFile)\n\tif err == errNoTransaction {\n\t\tmsg = \"no transaction\"\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"build tx frame offsets: %w\", err)\n\t}\n\ttxPageCount = len(txFrameOffsets)\n\n\tdbFile, err := db.os.Open(\"COMMITWAL:DB\", db.DatabasePath())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot open database file: %w\", err)\n\t}\n\tdefer func() { _ = dbFile.Close() }()\n\n\t// Determine transaction ID of the in-process transaction.\n\ttxID := prevPos.TXID + 1\n\n\t// Open file descriptors for the header & page blocks for new LTX file.\n\tltxPath := db.LTXPath(txID, txID)\n\ttmpPath := ltxPath + \".tmp\"\n\t_ = db.os.Remove(\"COMMITWAL:LTX\", tmpPath)\n\n\tltxFile, err := db.os.Create(\"COMMITWAL:LTX\", tmpPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot create LTX file: %w\", err)\n\t}\n\tdefer func() { _ = ltxFile.Close() }()\n\n\tenc := ltx.NewEncoder(ltxFile)\n\tif err := enc.EncodeHeader(ltx.Header{\n\t\tVersion:          1,\n\t\tFlags:            db.store.ltxHeaderFlags(),\n\t\tPageSize:         db.pageSize,\n\t\tCommit:           commit,\n\t\tMinTXID:          txID,\n\t\tMaxTXID:          txID,\n\t\tTimestamp:        db.Now().UnixMilli(),\n\t\tPreApplyChecksum: prevPos.PostApplyChecksum,\n\t\tWALSalt1:         db.wal.salt1,\n\t\tWALSalt2:         db.wal.salt2,\n\t\tWALOffset:        db.wal.offset,\n\t\tWALSize:          endOffset - db.wal.offset,\n\t\tNodeID:           db.store.ID(),\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"cannot encode ltx header: %s\", err)\n\t}\n\n\t// Build sorted list of page numbers in current transaction.\n\tpgnos := make([]uint32, 0, len(txFrameOffsets))\n\tfor pgno := range txFrameOffsets {\n\t\tpgnos = append(pgnos, pgno)\n\t}\n\tsort.Slice(pgnos, func(i, j int) bool { return pgnos[i] < pgnos[j] })\n\n\tframe := make([]byte, walFrameSize)\n\tnewWALChksums := make(map[uint32]ltx.Checksum)\n\tlockPgno := ltx.LockPgno(db.pageSize)\n\tfor _, pgno := range pgnos {\n\t\tif pgno == lockPgno {\n\t\t\tTraceLog.Printf(\"[CommitWALPage(%s)]: pgno=%d SKIP(LOCK_PAGE)\\n\", db.name, pgno)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Read next frame from the WAL file.\n\t\toffset := txFrameOffsets[pgno]\n\t\tif _, err := internal.ReadFullAt(walFile, frame, offset); err != nil {\n\t\t\treturn fmt.Errorf(\"read next frame: %w\", err)\n\t\t}\n\t\tpgno := binary.BigEndian.Uint32(frame[0:4])\n\n\t\t// Copy page into LTX file.\n\t\tif err := enc.EncodePage(ltx.PageHeader{Pgno: pgno}, frame[WALFrameHeaderSize:]); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot encode ltx page: pgno=%d err=%w\", pgno, err)\n\t\t}\n\n\t\t// Update per-page checksum.\n\t\tdb.chksums.mu.Lock()\n\t\tprevPageChksum, _ := db.pageChecksum(pgno, db.PageN(), nil)\n\t\tdb.chksums.mu.Unlock()\n\t\tpageChksum := ltx.ChecksumPage(pgno, frame[WALFrameHeaderSize:])\n\t\tnewWALChksums[pgno] = pageChksum\n\n\t\tTraceLog.Printf(\"[CommitWALPage(%s)]: pgno=%d chksum=%s prev=%s\\n\", db.name, pgno, pageChksum, prevPageChksum)\n\t}\n\n\t// Remove checksum of truncated pages.\n\tpage := make([]byte, db.pageSize)\n\tfor pgno := commit + 1; pgno <= prevPageN; pgno++ {\n\t\tif pgno == lockPgno {\n\t\t\tTraceLog.Printf(\"[CommitWALRemovePage(%s)]: pgno=%d SKIP(LOCK_PAGE)\\n\", db.name, pgno)\n\t\t\tcontinue\n\t\t}\n\n\t\tif err := db.readPage(dbFile, walFile, pgno, page); err != nil {\n\t\t\treturn fmt.Errorf(\"read truncated page: pgno=%d err=%w\", pgno, err)\n\t\t}\n\n\t\t// Clear per-page checksum.\n\t\tdb.chksums.mu.Lock()\n\t\tprevPageChksum, _ := db.pageChecksum(pgno, db.PageN(), nil)\n\t\tdb.chksums.mu.Unlock()\n\t\tpageChksum := ltx.ChecksumPage(pgno, page)\n\t\tif pageChksum != prevPageChksum {\n\t\t\treturn fmt.Errorf(\"truncated page %d checksum mismatch: %s <> %s\", pgno, pageChksum, prevPageChksum)\n\t\t}\n\t\tnewWALChksums[pgno] = 0\n\n\t\tTraceLog.Printf(\"[CommitWALRemovePage(%s)]: pgno=%d prev=%s\\n\", db.name, pgno, prevPageChksum)\n\t}\n\n\t// Calculate checksum after commit.\n\tpostApplyChecksum, err := db.checksum(commit, newWALChksums)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"compute checksum: %w\", err)\n\t}\n\tenc.SetPostApplyChecksum(postApplyChecksum)\n\n\t// Finish page block to compute checksum and then finish header block.\n\tif err := enc.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx encoder: %s\", err)\n\t} else if err := ltxFile.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx file: %s\", err)\n\t}\n\n\t// If remote lock held, send LTX file to primary. Always set remote tx to nil.\n\thaltLock := db.RemoteHaltLock()\n\tif haltLock != nil {\n\t\t_, info := db.store.PrimaryInfo()\n\t\tif info == nil {\n\t\t\treturn fmt.Errorf(\"no primary available for remote transaction\")\n\t\t}\n\n\t\tif _, err := ltxFile.Seek(0, io.SeekStart); err != nil {\n\t\t\treturn fmt.Errorf(\"seek ltx file: %w\", err)\n\t\t} else if err := db.store.Client.Commit(ctx, info.AdvertiseURL, db.store.ID(), db.name, haltLock.ID, io.NopCloser(ltxFile)); err != nil {\n\t\t\treturn fmt.Errorf(\"remote commit: %w\", err)\n\t\t}\n\t}\n\n\tif err := ltxFile.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx file: %s\", err)\n\t}\n\n\t// Ensure node is still writable before final commit step.\n\tif !db.Writeable() {\n\t\treturn fmt.Errorf(\"node lost write access during transaction, rolling back\")\n\t}\n\n\t// Atomically rename the file\n\tif err := db.os.Rename(\"COMMITWAL:LTX\", tmpPath, ltxPath); err != nil {\n\t\treturn fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(ltxPath)); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\n\t// Copy page offsets on commit.\n\tfor pgno, off := range txFrameOffsets {\n\t\tdb.wal.frameOffsets[pgno] = off\n\t}\n\n\t// Append new checksums onto WAL set.\n\tfor pgno, chksum := range newWALChksums {\n\t\tdb.wal.chksums[pgno] = append(db.wal.chksums[pgno], chksum)\n\t}\n\n\t// Move the WAL position forward and reset the segment size.\n\tdb.pageN.Store(commit)\n\tdb.wal.offset = endOffset\n\tdb.wal.chksum1 = chksum1\n\tdb.wal.chksum2 = chksum2\n\n\t// Update transaction for database.\n\tpos = ltx.Pos{\n\t\tTXID:              enc.Header().MaxTXID,\n\t\tPostApplyChecksum: enc.Trailer().PostApplyChecksum,\n\t}\n\tif err := db.setPos(pos, enc.Header().Timestamp); err != nil {\n\t\treturn fmt.Errorf(\"set pos: %w\", err)\n\t}\n\n\t// Update metrics\n\tdbCommitCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXBytesMetricVec.WithLabelValues(db.name).Set(float64(enc.N()))\n\tdbLatencySecondsMetricVec.WithLabelValues(db.name).Set(0.0)\n\n\t// Notify store of database change.\n\tdb.store.MarkDirty(db.name)\n\n\t// Notify event stream subscribers of new transaction.\n\tdb.store.NotifyEvent(Event{\n\t\tType: EventTypeTx,\n\t\tDB:   db.name,\n\t\tData: TxEventData{\n\t\t\tTXID:              pos.TXID,\n\t\t\tPostApplyChecksum: pos.PostApplyChecksum,\n\t\t\tPageSize:          enc.Header().PageSize,\n\t\t\tCommit:            enc.Header().Commit,\n\t\t\tTimestamp:         time.UnixMilli(enc.Header().Timestamp).UTC(),\n\t\t},\n\t})\n\n\t// Perform full checksum verification, if set. For testing only.\n\tif db.store.StrictVerify {\n\t\tif chksum, err := db.onDiskChecksum(dbFile, walFile); err != nil {\n\t\t\treturn fmt.Errorf(\"checksum (wal): %w\", err)\n\t\t} else if chksum != postApplyChecksum {\n\t\t\treturn fmt.Errorf(\"verification failed (wal): %s <> %s\", chksum, postApplyChecksum)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// readPage reads the latest version of the page before the current transaction.\nfunc (db *DB) readPage(dbFile, walFile *os.File, pgno uint32, buf []byte) error {\n\t// Read from previous position in WAL, if available.\n\tif off, ok := db.wal.frameOffsets[pgno]; ok {\n\t\thdr := make([]byte, WALFrameHeaderSize)\n\t\tif _, err := internal.ReadFullAt(walFile, hdr, off); err != nil {\n\t\t\treturn fmt.Errorf(\"read wal page hdr: %w\", err)\n\t\t}\n\n\t\tif _, err := internal.ReadFullAt(walFile, buf, off+WALFrameHeaderSize); err != nil {\n\t\t\treturn fmt.Errorf(\"read wal page: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Otherwise read from the database file.\n\toffset := int64(pgno-1) * int64(db.pageSize)\n\tif _, err := internal.ReadFullAt(dbFile, buf, offset); err != nil {\n\t\treturn fmt.Errorf(\"read database page: %w\", err)\n\t}\n\treturn nil\n}\n\n// CreateSHM creates a new shared memory file on disk.\nfunc (db *DB) CreateSHM() (*os.File, error) {\n\tf, err := db.os.OpenFile(\"CREATESHM\", db.SHMPath(), os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_TRUNC, 0o666)\n\tTraceLog.Printf(\"[CreateSHM(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// OpenSHM returns a handle for the shared memory file.\nfunc (db *DB) OpenSHM(ctx context.Context) (*os.File, error) {\n\tf, err := db.os.OpenFile(\"OPENSHM\", db.SHMPath(), os.O_RDWR, 0o666)\n\tTraceLog.Printf(\"[OpenSHM(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn f, err\n}\n\n// CloseSHM closes a handle associated with the SHM file.\nfunc (db *DB) CloseSHM(ctx context.Context, f *os.File, owner uint64) error {\n\terr := f.Close()\n\tTraceLog.Printf(\"[CloseSHM(%s)]: owner=%d %s\", db.name, owner, errorKeyValue(err))\n\treturn err\n}\n\n// SyncSHM fsync's the shared memory file.\nfunc (db *DB) SyncSHM(ctx context.Context) (err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[SyncSHM(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\tf, err := db.os.Open(\"SYNCSHM\", db.SHMPath())\n\tif err != nil {\n\t\treturn err\n\t} else if err := f.Sync(); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n\n// TruncateSHM sets the size of the the SHM file.\nfunc (db *DB) TruncateSHM(ctx context.Context, size int64) error {\n\terr := db.os.Truncate(\"TRUNCATESHM\", db.SHMPath(), size)\n\tTraceLog.Printf(\"[TruncateSHM(%s)]: size=%d %s\", db.name, size, errorKeyValue(err))\n\treturn err\n}\n\n// RemoveSHM removes the SHM file from disk.\nfunc (db *DB) RemoveSHM(ctx context.Context) error {\n\terr := db.os.Remove(\"REMOVESHM\", db.SHMPath())\n\tTraceLog.Printf(\"[RemoveSHM(%s)]: %s\", db.name, errorKeyValue(err))\n\treturn err\n}\n\n// UnlockSHM unlocks all locks from the SHM file.\nfunc (db *DB) UnlockSHM(ctx context.Context, owner uint64) {\n\tguardSet := db.GuardSet(owner)\n\tif guardSet == nil {\n\t\treturn\n\t}\n\n\t// Process WAL if we have an exclusive lock on WAL_WRITE_LOCK.\n\tif guardSet.Write().State() == RWMutexStateExclusive {\n\t\tif err := db.CommitWAL(ctx); err != nil {\n\t\t\tlog.Printf(\"commit wal error(1): %s\", err)\n\t\t}\n\t}\n\n\tguardSet.UnlockSHM()\n\tTraceLog.Printf(\"[UnlockSHM(%s)]: owner=%d\", db.name, owner)\n}\n\n// ReadSHMAt reads from the shared memory at the specified offset.\nfunc (db *DB) ReadSHMAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (int, error) {\n\tn, err := f.ReadAt(data, offset)\n\tTraceLog.Printf(\"[ReadSHMAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\treturn n, err\n}\n\n// WriteSHMAt writes data to the SHM file.\nfunc (db *DB) WriteSHMAt(ctx context.Context, f *os.File, data []byte, offset int64, owner uint64) (int, error) {\n\t// Ignore writes that occur while the SHM is updating. This is a side effect\n\t// of SQLite using mmap() which can cause re-access to update it.\n\tif db.updatingSHM.Load() {\n\t\tTraceLog.Printf(\"[WriteSHMAt(%s)]: offset=%d size=%d [BLOCKED]\", db.name, offset, len(data))\n\t\treturn len(data), nil\n\t}\n\n\tdbSHMWriteCountMetricVec.WithLabelValues(db.name).Inc()\n\tn, err := f.WriteAt(data, offset)\n\tTraceLog.Printf(\"[WriteSHMAt(%s)]: offset=%d size=%d owner=%d %s\", db.name, offset, len(data), owner, errorKeyValue(err))\n\treturn n, err\n}\n\n// isByteSliceZero returns true if b only contains NULL bytes.\nfunc isByteSliceZero(b []byte) bool {\n\tfor _, v := range b {\n\t\tif v != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// CommitJournal deletes the journal file which commits or rolls back the transaction.\nfunc (db *DB) CommitJournal(ctx context.Context, mode JournalMode) (err error) {\n\tvar pos ltx.Pos\n\tprevPos := db.Pos()\n\tprevPageN := db.PageN()\n\tdefer func() {\n\t\tTraceLog.Printf(\"[CommitJournal(%s)]: pos=%s prevPos=%s pageN=%d prevPageN=%d mode=%s %s\\n\\n\",\n\t\t\tdb.name, pos, prevPos, db.PageN(), prevPageN, mode, errorKeyValue(err))\n\t}()\n\n\t// Return an error if the current process is not the leader.\n\tif !db.Writeable() {\n\t\treturn ErrReadOnlyReplica\n\t}\n\n\t// Read journal header to ensure it's valid.\n\tif ok, err := db.isJournalHeaderValid(); err != nil {\n\t\treturn err\n\t} else if !ok {\n\t\treturn db.invalidateJournal(mode) // rollback\n\t}\n\n\t// If there is no page size available then nothing has been written.\n\t// Continue with the invalidation without processing the journal.\n\tif db.pageSize == 0 {\n\t\tif err := db.invalidateJournal(mode); err != nil {\n\t\t\treturn fmt.Errorf(\"invalidate journal: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Determine transaction ID of the in-process transaction.\n\ttxID := prevPos.TXID + 1\n\n\tdbFile, err := db.os.Open(\"COMMITJOURNAL:DB\", db.DatabasePath())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot open database file: %w\", err)\n\t}\n\tdefer func() { _ = dbFile.Close() }()\n\n\tvar commit uint32\n\tif _, err := dbFile.Seek(SQLITE_DATABASE_SIZE_OFFSET, io.SeekStart); err != nil {\n\t\treturn fmt.Errorf(\"cannot seek to database size: %w\", err)\n\t} else if err := binary.Read(dbFile, binary.BigEndian, &commit); err != nil {\n\t\treturn fmt.Errorf(\"cannot read database size: %w\", err)\n\t}\n\n\t// Build sorted list of dirty page numbers.\n\tpgnos := make([]uint32, 0, len(db.dirtyPageSet))\n\tfor pgno := range db.dirtyPageSet {\n\t\tif pgno <= commit {\n\t\t\tpgnos = append(pgnos, pgno)\n\t\t}\n\t}\n\tsort.Slice(pgnos, func(i, j int) bool { return pgnos[i] < pgnos[j] })\n\n\t// Open file descriptors for the header & page blocks for new LTX file.\n\tltxPath := db.LTXPath(txID, txID)\n\ttmpPath := ltxPath + \".tmp\"\n\t_ = db.os.Remove(\"COMMITJOURNAL:LTX\", tmpPath)\n\n\tltxFile, err := db.os.Create(\"COMMITJOURNAL:LTX\", tmpPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot create LTX file: %w\", err)\n\t}\n\tdefer func() { _ = ltxFile.Close() }()\n\n\tenc := ltx.NewEncoder(ltxFile)\n\tif err := enc.EncodeHeader(ltx.Header{\n\t\tVersion:          1,\n\t\tFlags:            db.store.ltxHeaderFlags(),\n\t\tPageSize:         db.pageSize,\n\t\tCommit:           commit,\n\t\tMinTXID:          txID,\n\t\tMaxTXID:          txID,\n\t\tTimestamp:        db.Now().UnixMilli(),\n\t\tPreApplyChecksum: prevPos.PostApplyChecksum,\n\t\tNodeID:           db.store.ID(),\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"cannot encode ltx header: %s\", err)\n\t}\n\n\t// Remove WAL checksums. These shouldn't exist but remove them just in case.\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\t// Copy transactions from main database to the LTX file in sorted order.\n\tbuf := make([]byte, db.pageSize)\n\tdbMode := DBModeRollback\n\tlockPgno := ltx.LockPgno(db.pageSize)\n\tfor _, pgno := range pgnos {\n\t\tif pgno == lockPgno {\n\t\t\tTraceLog.Printf(\"[CommitJournalPage(%s)]: pgno=%d SKIP(LOCK_PAGE)\\n\", db.name, pgno)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Read page from database.\n\t\toffset := int64(pgno-1) * int64(db.pageSize)\n\t\tif _, err := internal.ReadFullAt(dbFile, buf, offset); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot read database page: pgno=%d err=%w\", pgno, err)\n\t\t}\n\n\t\t// Copy page into LTX file.\n\t\tif err := enc.EncodePage(ltx.PageHeader{Pgno: pgno}, buf); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot encode ltx page: pgno=%d err=%w\", pgno, err)\n\t\t}\n\n\t\t// Update the mode if this is the first page and the write/read versions as set to WAL (2).\n\t\tif pgno == 1 && buf[18] == 2 && buf[19] == 2 {\n\t\t\tdbMode = DBModeWAL\n\t\t}\n\n\t\t// Verify updated page matches in-memory checksum.\n\t\tdb.chksums.mu.Lock()\n\t\tpageChksum, ok := db.pageChecksum(pgno, commit, nil)\n\t\tdb.chksums.mu.Unlock()\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"updated page checksum not found: pgno=%d\", pgno)\n\t\t}\n\t\tbufChksum := ltx.ChecksumPage(pgno, buf)\n\t\tif bufChksum != pageChksum {\n\t\t\treturn fmt.Errorf(\"updated page (%d) does not match in-memory checksum: %s <> %s (%s)\", pgno, bufChksum, pageChksum, bufChksum^pageChksum)\n\t\t}\n\n\t\tTraceLog.Printf(\"[CommitJournalPage(%s)]: pgno=%d chksum=%s %s\", db.name, pgno, pageChksum, errorKeyValue(err))\n\t}\n\n\t// Remove all checksums after last page.\n\tfunc() {\n\t\tdb.chksums.mu.Lock()\n\t\tdefer db.chksums.mu.Unlock()\n\n\t\t// NOTE: The index in the checksum page slice is one less than the page\n\t\t// number so we are starting from the page after \"commit\" and clearing checksums.\n\t\tfor i := commit; i < uint32(len(db.chksums.pages)); i++ {\n\t\t\tpgno := i + 1\n\t\t\tif pgno == lockPgno {\n\t\t\t\tTraceLog.Printf(\"[CommitJournalRemovePage(%s)]: pgno=%d SKIP(LOCK_PAGE)\\n\", db.name, pgno)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tpageChksum, _ := db.pageChecksum(pgno, db.PageN(), nil)\n\t\t\tdb.setDatabasePageChecksum(pgno, 0)\n\t\t\tTraceLog.Printf(\"[CommitJournalRemovePage(%s)]: pgno=%d chksum=%s %s\", db.name, pgno, pageChksum, errorKeyValue(err))\n\t\t}\n\t}()\n\n\t// Compute new database checksum.\n\tpostApplyChecksum, err := db.checksum(commit, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"compute checksum: %w\", err)\n\t}\n\tenc.SetPostApplyChecksum(postApplyChecksum)\n\n\t// Finish page block to compute checksum and then finish header block.\n\tif err := enc.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx encoder: %s\", err)\n\t} else if err := ltxFile.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx file: %s\", err)\n\t}\n\n\t// If remote lock held, send LTX file to primary.\n\thaltLock := db.RemoteHaltLock()\n\tif haltLock != nil {\n\t\t_, info := db.store.PrimaryInfo()\n\t\tif info == nil {\n\t\t\treturn fmt.Errorf(\"no primary available for remote transaction\")\n\t\t}\n\n\t\tif _, err := ltxFile.Seek(0, io.SeekStart); err != nil {\n\t\t\treturn fmt.Errorf(\"seek ltx file: %w\", err)\n\t\t} else if err := db.store.Client.Commit(ctx, info.AdvertiseURL, db.store.ID(), db.name, haltLock.ID, io.NopCloser(ltxFile)); err != nil {\n\t\t\treturn fmt.Errorf(\"remote commit: %w\", err)\n\t\t}\n\t}\n\n\tif err := ltxFile.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx file: %s\", err)\n\t}\n\n\t// Atomically rename the file\n\tif err := db.os.Rename(\"COMMITJOURNAL:LTX\", tmpPath, ltxPath); err != nil {\n\t\treturn fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(ltxPath)); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\n\t// Ensure file is persisted to disk.\n\tif err := dbFile.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"cannot sync ltx file: %w\", err)\n\t}\n\n\tif err := db.invalidateJournal(mode); err != nil {\n\t\treturn fmt.Errorf(\"invalidate journal: %w\", err)\n\t}\n\n\t// Update database flags.\n\tdb.pageN.Store(commit)\n\tdb.mode.Store(dbMode)\n\n\t// Update transaction for database.\n\tpos = ltx.Pos{\n\t\tTXID:              enc.Header().MaxTXID,\n\t\tPostApplyChecksum: enc.Trailer().PostApplyChecksum,\n\t}\n\tif err := db.setPos(pos, enc.Header().Timestamp); err != nil {\n\t\treturn fmt.Errorf(\"set pos: %w\", err)\n\t}\n\n\t// Update metrics\n\tdbCommitCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXBytesMetricVec.WithLabelValues(db.name).Set(float64(enc.N()))\n\tdbLatencySecondsMetricVec.WithLabelValues(db.name).Set(0.0)\n\n\t// Notify store of database change.\n\tdb.store.MarkDirty(db.name)\n\n\t// Notify event stream subscribers of new transaction.\n\tdb.store.NotifyEvent(Event{\n\t\tType: EventTypeTx,\n\t\tDB:   db.name,\n\t\tData: TxEventData{\n\t\t\tTXID:              pos.TXID,\n\t\t\tPostApplyChecksum: pos.PostApplyChecksum,\n\t\t\tPageSize:          enc.Header().PageSize,\n\t\t\tCommit:            enc.Header().Commit,\n\t\t\tTimestamp:         time.UnixMilli(enc.Header().Timestamp).UTC(),\n\t\t},\n\t})\n\n\t// Calculate checksum for entire database.\n\tif db.store.StrictVerify {\n\t\tif chksum, err := db.onDiskChecksum(dbFile, nil); err != nil {\n\t\t\treturn fmt.Errorf(\"checksum (journal): %w\", err)\n\t\t} else if chksum != postApplyChecksum {\n\t\t\treturn fmt.Errorf(\"verification failed (journal): %s <> %s\", chksum, postApplyChecksum)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Drop writes a zero \"commit\" value to indicate that the database has been deleted.\nfunc (db *DB) Drop(ctx context.Context) (err error) {\n\tvar msg string\n\tvar commit uint32\n\tvar txPageCount int\n\tvar pos ltx.Pos\n\tprevPos := db.Pos()\n\tprevPageN := db.PageN()\n\ttxID := prevPos.TXID + 1\n\tdefer func() {\n\t\tTraceLog.Printf(\"[Drop(%s)]: pos=%s prevPos=%s pages=%d commit=%d prevPageN=%d pageSize=%d msg=%q %s\\n\\n\",\n\t\t\tdb.name, pos, prevPos, txPageCount, commit, prevPageN, db.pageSize, msg, errorKeyValue(err))\n\t}()\n\n\t// Open file descriptors for the header & page blocks for new LTX file.\n\tltxPath := db.LTXPath(txID, txID)\n\ttmpPath := ltxPath + \".tmp\"\n\tdefer func() { _ = db.os.Remove(\"DROP:LTX\", tmpPath) }()\n\n\tltxFile, err := db.os.Create(\"DROP:LTX\", tmpPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot create LTX file: %w\", err)\n\t}\n\tdefer func() { _ = ltxFile.Close() }()\n\n\tenc := ltx.NewEncoder(ltxFile)\n\tif err := enc.EncodeHeader(ltx.Header{\n\t\tVersion:          1,\n\t\tFlags:            db.store.ltxHeaderFlags(),\n\t\tPageSize:         db.pageSize,\n\t\tCommit:           commit,\n\t\tMinTXID:          txID,\n\t\tMaxTXID:          txID,\n\t\tTimestamp:        db.Now().UnixMilli(),\n\t\tPreApplyChecksum: prevPos.PostApplyChecksum,\n\t\tNodeID:           db.store.ID(),\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"cannot encode ltx header: %s\", err)\n\t}\n\n\tenc.SetPostApplyChecksum(ltx.ChecksumFlag)\n\tif err := enc.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx encoder: %s\", err)\n\t} else if err := ltxFile.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx file: %s\", err)\n\t}\n\n\t// If remote lock held, send LTX file to primary. Always set remote tx to nil.\n\thaltLock := db.RemoteHaltLock()\n\tif haltLock != nil {\n\t\t_, info := db.store.PrimaryInfo()\n\t\tif info == nil {\n\t\t\treturn fmt.Errorf(\"no primary available for remote drop transaction\")\n\t\t}\n\n\t\tif _, err := ltxFile.Seek(0, io.SeekStart); err != nil {\n\t\t\treturn fmt.Errorf(\"seek ltx file: %w\", err)\n\t\t} else if err := db.store.Client.Commit(ctx, info.AdvertiseURL, db.store.ID(), db.name, haltLock.ID, io.NopCloser(ltxFile)); err != nil {\n\t\t\treturn fmt.Errorf(\"remote commit: %w\", err)\n\t\t}\n\t}\n\n\tif err := ltxFile.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx file: %s\", err)\n\t}\n\n\t// Ensure node is still writable before final commit step.\n\tif !db.Writeable() {\n\t\treturn fmt.Errorf(\"node lost write access during drop, rolling back\")\n\t}\n\n\t// Atomically rename the file\n\tif err := db.os.Rename(\"DROP:LTX\", tmpPath, ltxPath); err != nil {\n\t\treturn fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(ltxPath)); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\n\t// Remove all related files.\n\tif err := db.os.Remove(\"DROP:DB\", db.DatabasePath()); err != nil && !os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"delete database file: %w\", err)\n\t}\n\tif err := db.os.Remove(\"DROP:JOURNAL\", db.JournalPath()); err != nil && !os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"delete journal file: %w\", err)\n\t}\n\tif err := db.os.Remove(\"DROP:WAL\", db.WALPath()); err != nil && !os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"delete wal file: %w\", err)\n\t}\n\tif err := db.os.Remove(\"DROP:SHM\", db.SHMPath()); err != nil && !os.IsNotExist(err) {\n\t\treturn fmt.Errorf(\"delete shm file: %w\", err)\n\t}\n\n\t// Reset database & WAL information.\n\tdb.mode.Store(DBModeRollback)\n\tdb.pageN.Store(0)\n\tdb.wal.offset = 0\n\tdb.wal.chksum1 = 0\n\tdb.wal.chksum2 = 0\n\tdb.wal.frameOffsets = make(map[uint32]int64)\n\tdb.wal.chksums = make(map[uint32][]ltx.Checksum)\n\n\t// Update transaction for database.\n\tpos = ltx.NewPos(enc.Header().MaxTXID, enc.Trailer().PostApplyChecksum)\n\tif err := db.setPos(pos, enc.Header().Timestamp); err != nil {\n\t\treturn fmt.Errorf(\"set pos: %w\", err)\n\t}\n\n\t// Update metrics\n\tdbCommitCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXCountMetricVec.WithLabelValues(db.name).Inc()\n\tdbLTXBytesMetricVec.WithLabelValues(db.name).Set(float64(enc.N()))\n\tdbLatencySecondsMetricVec.WithLabelValues(db.name).Set(0.0)\n\n\t// Notify store of database change.\n\tdb.store.MarkDirty(db.name)\n\n\t// Notify event stream subscribers of new transaction.\n\tdb.store.NotifyEvent(Event{\n\t\tType: EventTypeTx,\n\t\tDB:   db.name,\n\t\tData: TxEventData{\n\t\t\tTXID:              pos.TXID,\n\t\t\tPostApplyChecksum: pos.PostApplyChecksum,\n\t\t\tPageSize:          enc.Header().PageSize,\n\t\t\tCommit:            enc.Header().Commit,\n\t\t\tTimestamp:         time.UnixMilli(enc.Header().Timestamp).UTC(),\n\t\t},\n\t})\n\n\treturn nil\n}\n\n// onDiskChecksum calculates the LTX checksum directly from the on-disk database & WAL.\nfunc (db *DB) onDiskChecksum(dbFile, walFile *os.File) (chksum ltx.Checksum, err error) {\n\tif db.pageSize == 0 {\n\t\treturn 0, fmt.Errorf(\"page size required for checksum\")\n\t} else if db.PageN() == 0 {\n\t\treturn 0, fmt.Errorf(\"page count required for checksum\")\n\t}\n\n\t// Compute the lock page once and skip it during checksumming.\n\tlockPgno := ltx.LockPgno(db.pageSize)\n\n\tdata := make([]byte, db.pageSize)\n\tfor pgno := uint32(1); pgno <= db.PageN(); pgno++ {\n\t\tif pgno == lockPgno {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Read from either the database file or the WAL depending if the page exists in the WAL.\n\t\tif offset, ok := db.wal.frameOffsets[pgno]; !ok {\n\t\t\tif _, err := internal.ReadFullAt(dbFile, data, int64(pgno-1)*int64(db.pageSize)); err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"db read (pgno=%d): %w\", pgno, err)\n\t\t\t}\n\t\t} else {\n\t\t\tif _, err := internal.ReadFullAt(walFile, data, offset+WALFrameHeaderSize); err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"wal read (pgno=%d): %w\", pgno, err)\n\t\t\t}\n\t\t}\n\n\t\t// Add the page to the rolling checksum.\n\t\tchksum = ltx.ChecksumFlag | (chksum ^ ltx.ChecksumPage(pgno, data))\n\t}\n\n\treturn chksum, nil\n}\n\n// isJournalHeaderValid returns true if the journal starts with the journal magic.\nfunc (db *DB) isJournalHeaderValid() (bool, error) {\n\tf, err := db.os.Open(\"ISJOURNALHDRVALID\", db.JournalPath())\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tbuf := make([]byte, len(SQLITE_JOURNAL_HEADER_STRING))\n\tif _, err := io.ReadFull(f, buf); err != nil {\n\t\treturn false, err\n\t}\n\treturn string(buf) == SQLITE_JOURNAL_HEADER_STRING, nil\n}\n\n// invalidateJournal invalidates the journal file based on the journal mode.\nfunc (db *DB) invalidateJournal(mode JournalMode) error {\n\tswitch mode {\n\tcase JournalModeDelete:\n\t\tif err := db.os.Remove(\"INVALIDATEJOURNAL:DELETE\", db.JournalPath()); err != nil {\n\t\t\treturn fmt.Errorf(\"remove journal file: %w\", err)\n\t\t}\n\n\tcase JournalModeTruncate:\n\t\tif err := db.os.Truncate(\"INVALIDATEJOURNAL:TRUNCATE\", db.JournalPath(), 0); err != nil {\n\t\t\treturn fmt.Errorf(\"truncate: %w\", err)\n\t\t} else if err := internal.Sync(db.JournalPath()); err != nil {\n\t\t\treturn fmt.Errorf(\"sync journal: %w\", err)\n\t\t}\n\n\tcase JournalModePersist:\n\t\tf, err := db.os.OpenFile(\"INVALIDATEJOURNAL:PERSIST\", db.JournalPath(), os.O_RDWR, 0o666)\n\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"open journal: %w\", err)\n\t\t} else if err == nil {\n\t\t\tdefer func() { _ = f.Close() }()\n\n\t\t\tif _, err := f.Write(make([]byte, SQLITE_JOURNAL_HEADER_SIZE)); err != nil {\n\t\t\t\treturn fmt.Errorf(\"clear journal header: %w\", err)\n\t\t\t} else if err := f.Sync(); err != nil {\n\t\t\t\treturn fmt.Errorf(\"sync journal: %w\", err)\n\t\t\t}\n\t\t}\n\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid journal: %q\", mode)\n\t}\n\n\t// Sync the underlying directory.\n\tif err := internal.Sync(db.path); err != nil {\n\t\treturn fmt.Errorf(\"sync database directory: %w\", err)\n\t}\n\n\tdb.dirtyPageSet = make(map[uint32]struct{})\n\n\treturn nil\n}\n\n// WriteLTXFileAt atomically writes r to the database's LTX directory but does\n// not apply the file. That should be done after the file is written.\n//\n// If file is a snapshot, then all other LTX files are removed.\n//\n// Returns the path of the new LTX file on success.\nfunc (db *DB) WriteLTXFileAt(ctx context.Context, r io.Reader) (string, error) {\n\t// Read & parse initial header.\n\tbuf := make([]byte, ltx.HeaderSize)\n\tvar hdr ltx.Header\n\tif n, err := io.ReadFull(r, buf); err != nil {\n\t\treturn \"\", fmt.Errorf(\"read ltx header: n=%d %w\", n, err)\n\t} else if err := hdr.UnmarshalBinary(buf); err != nil {\n\t\treturn \"\", fmt.Errorf(\"decode ltx header: %w\", err)\n\t}\n\n\t// Validate TXID/preApplyChecksum before renaming.\n\tprevPos := db.Pos()\n\tif !hdr.IsSnapshot() {\n\t\tif got, want := hdr.MinTXID, prevPos.TXID+1; got != want {\n\t\t\treturn \"\", fmt.Errorf(\"non-sequential header minimum txid %s, expecting %s\", got.String(), want.String())\n\t\t}\n\t\tif got, want := hdr.PreApplyChecksum, prevPos.PostApplyChecksum; got != want {\n\t\t\treturn \"\", fmt.Errorf(\"pre-apply checksum mismatch: %s, expecting %s\", got, want)\n\t\t}\n\t}\n\n\t// Write LTX file to a temporary file.\n\tpath := db.LTXPath(hdr.MinTXID, hdr.MaxTXID)\n\ttmpPath := path + \".tmp\"\n\tdefer func() { _ = db.os.Remove(\"WRITELTX\", tmpPath) }()\n\n\tf, err := db.os.Create(\"WRITELTX\", tmpPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"cannot create temp ltx file: %w\", err)\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tif _, err := io.Copy(f, io.MultiReader(bytes.NewReader(buf), r)); err != nil {\n\t\treturn \"\", fmt.Errorf(\"write ltx file: %w\", err)\n\t} else if err := f.Sync(); err != nil {\n\t\treturn \"\", fmt.Errorf(\"fsync ltx file: %w\", err)\n\t}\n\n\t// Validate file with an LTX decoder before renaming.\n\tdec := ltx.NewDecoder(f)\n\tif _, err := f.Seek(0, io.SeekStart); err != nil {\n\t\treturn \"\", fmt.Errorf(\"seek for validation: %w\", err)\n\t} else if err := dec.Verify(); err != nil {\n\t\treturn \"\", fmt.Errorf(\"ltx validation error: %w\", err)\n\t}\n\n\t// If this is a snapshot, remove all other files before rename.\n\tif hdr.IsSnapshot() {\n\t\tdir, file := filepath.Split(tmpPath)\n\t\tlog.Printf(\"snapshot received for %q, removing other ltx files except: %s\", db.Name(), file)\n\t\tif err := removeFilesExcept(db.os, dir, file); err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"remove ltx except snapshot: %w\", err)\n\t\t}\n\t}\n\n\t// Atomically rename file.\n\tif err := db.os.Rename(\"WRITELTX\", tmpPath, path); err != nil {\n\t\treturn \"\", fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(path)); err != nil {\n\t\treturn \"\", fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\treturn path, nil\n}\n\n// ApplyLTXNoLock applies an LTX file to the database.\nfunc (db *DB) ApplyLTXNoLock(path string, fatalOnError bool) (retErr error) {\n\tvar hdr ltx.Header\n\tvar trailer ltx.Trailer\n\tprevDBMode := db.Mode()\n\tdefer func() {\n\t\tTraceLog.Printf(\"[ApplyLTX(%s)]: txid=%s-%s chksum=%s-%s commit=%d pageSize=%d timestamp=%s mode=(%s%s) path=%s\",\n\t\t\tdb.name, hdr.MinTXID.String(), hdr.MaxTXID.String(), hdr.PreApplyChecksum, trailer.PostApplyChecksum, hdr.Commit, db.pageSize,\n\t\t\ttime.UnixMilli(hdr.Timestamp).UTC().Format(time.RFC3339), prevDBMode, db.Mode(), filepath.Base(path))\n\t}()\n\n\t// Open LTX header reader.\n\thf, err := db.os.Open(\"APPLYLTX:LTX\", path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"open file: %w\", err)\n\t}\n\tdefer func() { _ = hf.Close() }()\n\n\tdec := ltx.NewDecoder(hf)\n\tif err := dec.DecodeHeader(); err != nil {\n\t\treturn fmt.Errorf(\"decode ltx header: %s\", err)\n\t}\n\thdr = dec.Header()\n\tif db.pageSize == 0 {\n\t\tdb.pageSize = dec.Header().PageSize\n\t}\n\n\t// Delete database files if this has a zero \"commit\" field.\n\tdbMode := db.Mode()\n\tvar dbFile *os.File\n\tif dec.Header().Commit > 0 {\n\t\tif dbFile, err = db.os.OpenFile(\"APPLYLTX:DB\", db.DatabasePath(), os.O_RDWR|os.O_CREATE, 0o666); err != nil {\n\t\t\treturn fmt.Errorf(\"open database file: %w\", err)\n\t\t}\n\t\tdefer func() { _ = dbFile.Close() }()\n\t}\n\n\t// After this point, a partial failure will result in a partially written\n\t// database. We don't have the ability to signal to the client that a failure\n\t// occurred so we need to exit.\n\tdefer func() {\n\t\tif fatalOnError && retErr != nil {\n\t\t\tTraceLog.Printf(\"[FATAL(%s)]: err=%d\\n\", db.name, retErr)\n\t\t\tlog.Printf(\"fatal error occurred while applying ltx to %q, exiting: %s\\n\", db.name, retErr)\n\t\t\tdb.store.Exit(99)\n\t\t}\n\t}()\n\n\tpageBuf := make([]byte, dec.Header().PageSize)\n\tfor i := 0; ; i++ {\n\t\t// Read pgno & page data from LTX file.\n\t\tvar phdr ltx.PageHeader\n\t\tif err := dec.DecodePage(&phdr, pageBuf); err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"decode ltx page[%d]: %w\", i, err)\n\t\t}\n\n\t\t// Update the mode if this is the first page and the write/read versions as set to WAL (2).\n\t\tif phdr.Pgno == 1 && pageBuf[18] == 2 && pageBuf[19] == 2 {\n\t\t\tdbMode = DBModeWAL\n\t\t}\n\n\t\t// Copy to database file.\n\t\tif err := db.writeDatabasePage(dbFile, phdr.Pgno, pageBuf, true); err != nil {\n\t\t\treturn fmt.Errorf(\"write to database file: %w\", err)\n\t\t}\n\t}\n\n\t// Close the reader so we can verify file integrity.\n\tif err := dec.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close ltx decode: %w\", err)\n\t}\n\n\t// Truncate database file to size after LTX file.\n\t// If this is zero-length database then delete all the database files.\n\tif dec.Header().Commit > 0 {\n\t\tif err := db.truncateDatabase(dbFile, dec.Header().Commit); err != nil {\n\t\t\treturn fmt.Errorf(\"truncate database file: %w\", err)\n\t\t}\n\t} else {\n\t\tdbMode = DBModeRollback\n\n\t\t// If the database has been deleted, ensure the local files are removed.\n\t\tif err := db.os.Remove(\"APPLYLTX:DROP:DB\", db.DatabasePath()); err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"delete database file: %w\", err)\n\t\t}\n\t\tif err := db.os.Remove(\"APPLYLTX:DROP:JOURNAL\", db.JournalPath()); err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"delete journal file: %w\", err)\n\t\t}\n\t\tif err := db.os.Remove(\"APPLYLTX:DROP:WAL\", db.WALPath()); err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"delete wal file: %w\", err)\n\t\t}\n\t\tif err := db.os.Remove(\"APPLYLTX:DROP:SHM\", db.SHMPath()); err != nil && !os.IsNotExist(err) {\n\t\t\treturn fmt.Errorf(\"delete shm file: %w\", err)\n\t\t}\n\n\t\tif invalidator := db.store.Invalidator; invalidator != nil {\n\t\t\t_ = invalidator.InvalidateEntry(db.Name())\n\t\t\t_ = invalidator.InvalidateEntry(db.Name() + \"-journal\")\n\t\t\t_ = invalidator.InvalidateEntry(db.Name() + \"-wal\")\n\t\t\t_ = invalidator.InvalidateEntry(db.Name() + \"-shm\")\n\t\t}\n\t}\n\n\ttrailer = dec.Trailer()\n\tdb.pageN.Store(dec.Header().Commit)\n\tdb.mode.Store(dbMode)\n\n\t// Ensure checksum matches the post-apply checksum.\n\tif chksum, err := db.checksum(dec.Header().Commit, nil); err != nil {\n\t\treturn fmt.Errorf(\"compute checksum: %w\", err)\n\t} else if chksum != dec.Trailer().PostApplyChecksum {\n\t\treturn fmt.Errorf(\"database checksum %s on TXID %s does not match LTX post-apply checksum %s\",\n\t\t\tchksum, dec.Header().MaxTXID.String(), dec.Trailer().PostApplyChecksum)\n\t}\n\n\t// Update transaction for database.\n\tpos := ltx.Pos{\n\t\tTXID:              dec.Header().MaxTXID,\n\t\tPostApplyChecksum: dec.Trailer().PostApplyChecksum,\n\t}\n\tif err := db.setPos(pos, dec.Header().Timestamp); err != nil {\n\t\treturn fmt.Errorf(\"set pos: %w\", err)\n\t}\n\n\t// Rewrite SHM so that the transaction is visible.\n\tif err := db.updateSHM(); err != nil {\n\t\treturn fmt.Errorf(\"update shm: %w\", err)\n\t}\n\n\t// Invalidate entire database if this was a snapshot.\n\tif invalidator := db.store.Invalidator; invalidator != nil && hdr.IsSnapshot() {\n\t\tif err := invalidator.InvalidateDB(db); err != nil {\n\t\t\treturn fmt.Errorf(\"invalidate db: %w\", err)\n\t\t}\n\t}\n\n\t// Notify store of database change.\n\tdb.store.MarkDirty(db.name)\n\n\t// Notify event stream subscribers of new transaction.\n\tdb.store.NotifyEvent(Event{\n\t\tType: EventTypeTx,\n\t\tDB:   db.name,\n\t\tData: TxEventData{\n\t\t\tTXID:              pos.TXID,\n\t\t\tPostApplyChecksum: pos.PostApplyChecksum,\n\t\t\tPageSize:          dec.Header().PageSize,\n\t\t\tCommit:            dec.Header().Commit,\n\t\t\tTimestamp:         time.UnixMilli(dec.Header().Timestamp).UTC(),\n\t\t},\n\t})\n\n\t// Calculate latency since LTX file was written.\n\tlatency := float64(time.Now().UnixMilli()-dec.Header().Timestamp) / 1000\n\tdbLatencySecondsMetricVec.WithLabelValues(db.name).Set(latency)\n\n\treturn nil\n}\n\n// updateSHM recomputes the SHM header for a replica node (with no WAL frames).\nfunc (db *DB) updateSHM() error {\n\t// This lock prevents an issue where triggering SHM invalidation in FUSE\n\t// causes a write to be issued through the mmap which overwrites our change.\n\t// This lock blocks that from occurring.\n\tdb.shmMu.Lock()\n\tdefer db.shmMu.Unlock()\n\n\tdb.updatingSHM.Store(true)\n\tdefer db.updatingSHM.Store(false)\n\n\tTraceLog.Printf(\"[UpdateSHM(%s)]\", db.name)\n\tdefer TraceLog.Printf(\"[UpdateSHMDone(%s)]\", db.name)\n\n\tf, err := db.os.OpenFile(\"UPDATESHM\", db.SHMPath(), os.O_RDWR|os.O_CREATE, 0o666)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\t// Read current header, if it exists.\n\tdata := make([]byte, WALIndexBlockSize)\n\tif _, err := internal.ReadFullAt(f, data, 0); err != nil && err != io.EOF && err != io.ErrUnexpectedEOF {\n\t\treturn fmt.Errorf(\"read shm header: %w\", err)\n\t}\n\n\t// Read previous header. SHM uses native endianness so use unsafe to map to the struct.\n\tprevHdr := *(*walIndexHdr)(unsafe.Pointer(&data[0]))\n\n\t// Write header.\n\thdr := walIndexHdr{\n\t\tversion:     3007000,\n\t\tchange:      prevHdr.change + 1,\n\t\tisInit:      1,\n\t\tbigEndCksum: prevHdr.bigEndCksum,\n\t\tpageSize:    encodePageSize(db.pageSize),\n\t\tpageN:       db.PageN(),\n\t\tframeCksum:  [2]uint32{db.wal.chksum1, db.wal.chksum2},\n\t\tsalt:        [2]uint32{db.wal.salt1, db.wal.salt2},\n\t}\n\thdrBytes := (*[40]byte)(unsafe.Pointer(&hdr))[:]\n\thdr.cksum[0], hdr.cksum[1] = WALChecksum(NativeEndian, 0, 0, hdrBytes)\n\n\t// Write two copies of header and then the checkpoint info.\n\tckptInfo := walCkptInfo{\n\t\treadMark: [5]uint32{0x00000000, 0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff},\n\t}\n\tcopy(data[0:48], (*[48]byte)(unsafe.Pointer(&hdr))[:])\n\tcopy(data[48:96], (*[48]byte)(unsafe.Pointer(&hdr))[:])\n\tcopy(data[96:], (*[40]byte)(unsafe.Pointer(&ckptInfo))[:])\n\n\t// Overwrite the SHM index block.\n\tif _, err := f.WriteAt(data, 0); err != nil {\n\t\treturn err\n\t} else if err := f.Truncate(int64(len(data))); err != nil {\n\t\treturn err\n\t}\n\n\t// Invalidate page cache.\n\tif invalidator := db.store.Invalidator; invalidator != nil {\n\t\tif err := invalidator.InvalidateSHM(db); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Export writes the contents of the database to dst.\n// Returns the current replication position.\nfunc (db *DB) Export(ctx context.Context, dst io.Writer) (ltx.Pos, error) {\n\tgs := db.newGuardSet(0) // TODO(fsm): Track internal owners?\n\tdefer gs.Unlock()\n\n\t// Acquire PENDING then SHARED. Release PENDING immediately afterward.\n\tif err := gs.pending.RLock(ctx); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"acquire PENDING read lock: %w\", err)\n\t}\n\tif err := gs.shared.RLock(ctx); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"acquire SHARED read lock: %w\", err)\n\t}\n\tgs.pending.Unlock()\n\n\t// If this is WAL mode then temporarily obtain a write lock so we can copy\n\t// out the current database size & wal frames before returning to a read lock.\n\tif db.Mode() == DBModeWAL {\n\t\tif err := gs.write.Lock(ctx); err != nil {\n\t\t\treturn ltx.Pos{}, fmt.Errorf(\"acquire temporary exclusive WAL_WRITE_LOCK: %w\", err)\n\t\t}\n\t}\n\n\t// Determine current position & snapshot overriding WAL frames.\n\tpos := db.Pos()\n\tpageSize, pageN := db.pageSize, db.PageN()\n\twalFrameOffsets := make(map[uint32]int64, len(db.wal.frameOffsets))\n\tfor k, v := range db.wal.frameOffsets {\n\t\twalFrameOffsets[k] = v\n\t}\n\n\t// Release write lock, if acquired.\n\tgs.write.Unlock()\n\n\t// Acquire the CKPT & READ locks to prevent checkpointing, in case this is in WAL mode.\n\tif err := gs.ckpt.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire CKPT read lock: %w\", err)\n\t}\n\tif err := gs.recover.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire RECOVER read lock: %w\", err)\n\t}\n\tif err := gs.read0.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire READ0 read lock: %w\", err)\n\t}\n\tif err := gs.read1.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire READ1 read lock: %w\", err)\n\t}\n\tif err := gs.read2.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire READ2 read lock: %w\", err)\n\t}\n\tif err := gs.read3.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire READ3 read lock: %w\", err)\n\t}\n\tif err := gs.read4.RLock(ctx); err != nil {\n\t\treturn pos, fmt.Errorf(\"acquire READ4 read lock: %w\", err)\n\t}\n\n\t// Open database file.\n\tdbFile, err := db.os.Open(\"EXPORT:DB\", db.DatabasePath())\n\tif err != nil {\n\t\treturn pos, fmt.Errorf(\"open database file: %w\", err)\n\t}\n\tdefer func() { _ = dbFile.Close() }()\n\n\t// Open WAL file if we have overriding WAL frames.\n\tvar walFile *os.File\n\tif len(walFrameOffsets) > 0 {\n\t\tif walFile, err = db.os.Open(\"EXPORT:WAL\", db.WALPath()); err != nil {\n\t\t\treturn pos, fmt.Errorf(\"open wal file: %w\", err)\n\t\t}\n\t\tdefer func() { _ = walFile.Close() }()\n\t}\n\n\t// Write page frames.\n\tpageData := make([]byte, pageSize)\n\tfor pgno := uint32(1); pgno <= pageN; pgno++ {\n\t\t// Read from WAL if page exists in offset map. Otherwise read from DB.\n\t\tif walFrameOffset, ok := walFrameOffsets[pgno]; ok {\n\t\t\tif _, err := walFile.Seek(walFrameOffset+WALFrameHeaderSize, io.SeekStart); err != nil {\n\t\t\t\treturn pos, fmt.Errorf(\"seek wal page: %w\", err)\n\t\t\t} else if _, err := io.ReadFull(walFile, pageData); err != nil {\n\t\t\t\treturn pos, fmt.Errorf(\"read wal page: %w\", err)\n\t\t\t}\n\t\t} else {\n\t\t\tif _, err := dbFile.Seek(int64(pgno-1)*int64(pageSize), io.SeekStart); err != nil {\n\t\t\t\treturn pos, fmt.Errorf(\"seek database page: %w\", err)\n\t\t\t} else if _, err := io.ReadFull(dbFile, pageData); err != nil {\n\t\t\t\treturn pos, fmt.Errorf(\"read database page: %w\", err)\n\t\t\t}\n\t\t}\n\n\t\tif _, err := dst.Write(pageData); err != nil {\n\t\t\treturn pos, fmt.Errorf(\"write page %d: %w\", pgno, err)\n\t\t}\n\t}\n\n\treturn pos, nil\n}\n\n// Import replaces the contents of the database with the contents from the r.\n// NOTE: LiteFS does not validate the integrity of the imported database!\nfunc (db *DB) Import(ctx context.Context, r io.Reader) error {\n\tif !db.store.IsPrimary() {\n\t\treturn ErrReadOnlyReplica\n\t}\n\n\t// Acquire write lock.\n\tguard, err := db.AcquireWriteLock(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer guard.Unlock()\n\n\t// Invalidate journal, if one exists.\n\tif err := db.invalidateJournal(JournalModePersist); err != nil {\n\t\treturn fmt.Errorf(\"invalidate journal: %w\", err)\n\t}\n\n\t// Truncate WAL, if it exists.\n\tif _, err := db.os.Stat(\"IMPORT:WAL\", db.WALPath()); err == nil {\n\t\tif err := db.TruncateWAL(ctx, 0); err != nil {\n\t\t\treturn fmt.Errorf(\"truncate wal: %w\", err)\n\t\t}\n\t}\n\n\tpos, err := db.importToLTX(ctx, r)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn db.ApplyLTXNoLock(db.LTXPath(pos.TXID, pos.TXID), true)\n}\n\n// importToLTX reads a SQLite database and writes it to the next LTX file.\nfunc (db *DB) importToLTX(ctx context.Context, r io.Reader) (ltx.Pos, error) {\n\t// Read header to determine DB mode, page size, & commit.\n\thdr, data, err := readSQLiteDatabaseHeader(r)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"read database header: %w\", err)\n\t}\n\n\t// Prepend header back onto original reader.\n\tr = io.MultiReader(bytes.NewReader(data), r)\n\n\t// Determine resulting position.\n\tpos := db.Pos()\n\tpos.TXID++\n\tpreApplyChecksum := pos.PostApplyChecksum\n\tpos.PostApplyChecksum = 0\n\n\t// Open file descriptors for the header & page blocks for new LTX file.\n\tltxPath := db.LTXPath(pos.TXID, pos.TXID)\n\ttmpPath := ltxPath + \".tmp\"\n\t_ = db.os.Remove(\"IMPORTTOLTX\", tmpPath)\n\n\tf, err := db.os.Create(\"IMPORTTOLTX\", tmpPath)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"cannot create LTX file: %w\", err)\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tenc := ltx.NewEncoder(f)\n\tif err := enc.EncodeHeader(ltx.Header{\n\t\tVersion:          1,\n\t\tFlags:            db.store.ltxHeaderFlags(),\n\t\tPageSize:         hdr.PageSize,\n\t\tCommit:           hdr.PageN,\n\t\tMinTXID:          pos.TXID,\n\t\tMaxTXID:          pos.TXID,\n\t\tTimestamp:        db.Now().UnixMilli(),\n\t\tPreApplyChecksum: preApplyChecksum,\n\t\tNodeID:           db.store.ID(),\n\t}); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"cannot encode ltx header: %s\", err)\n\t}\n\n\t// Generate LTX file from reader.\n\tbuf := make([]byte, hdr.PageSize)\n\tlockPgno := ltx.LockPgno(hdr.PageSize)\n\tfor pgno := uint32(1); pgno <= hdr.PageN; pgno++ {\n\t\tif _, err := io.ReadFull(r, buf); err != nil {\n\t\t\treturn ltx.Pos{}, fmt.Errorf(\"read page %d: %w\", pgno, err)\n\t\t}\n\n\t\t// Skip the lock page.\n\t\tif pgno == lockPgno {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Reset file change counter & schema cookie to ensure existing connections reload.\n\t\tif pgno == 1 {\n\t\t\tbinary.BigEndian.PutUint32(buf[24:], 0)\n\t\t\tbinary.BigEndian.PutUint32(buf[40:], 0)\n\t\t}\n\n\t\tif err := enc.EncodePage(ltx.PageHeader{Pgno: pgno}, buf); err != nil {\n\t\t\treturn ltx.Pos{}, fmt.Errorf(\"encode ltx page: pgno=%d err=%w\", pgno, err)\n\t\t}\n\n\t\tpageChksum := ltx.ChecksumPage(pgno, buf)\n\t\tpos.PostApplyChecksum = ltx.ChecksumFlag | (pos.PostApplyChecksum ^ pageChksum)\n\t}\n\n\t// Finish page block to compute checksum and then finish header block.\n\tenc.SetPostApplyChecksum(pos.PostApplyChecksum)\n\tif err := enc.Close(); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"close ltx encoder: %s\", err)\n\t} else if err := f.Sync(); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"sync ltx file: %s\", err)\n\t} else if err := f.Close(); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"close ltx file: %s\", err)\n\t}\n\n\t// Atomically rename the file\n\tif err := db.os.Rename(\"IMPORTTOLTX\", tmpPath, ltxPath); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(ltxPath)); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\n\treturn pos, nil\n}\n\n// AcquireWriteLock acquires the appropriate locks for a write depending on if\n// the database uses a rollback journal or WAL.\nfunc (db *DB) AcquireWriteLock(ctx context.Context, fn func() error) (_ *GuardSet, err error) {\n\tTraceLog.Printf(\"[AcquireWriteLock(%s)]: \", db.name)\n\tdefer TraceLog.Printf(\"[AcquireWriteLock.DONE(%s)]: %s\", db.name, errorKeyValue(err))\n\n\tconst interval = 1 * time.Millisecond\n\tconst maxInterval = 500 * time.Millisecond\n\n\tticker := time.NewTimer(interval)\n\tdefer ticker.Stop()\n\n\tfor i := 0; ; i++ {\n\t\t// Execute callback on each lock attempt to see if we should exit.\n\t\t// Used by HALT lock to check for racing locks with same ID.\n\t\tif fn != nil {\n\t\t\tif err := fn(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tif gs := db.TryAcquireWriteLock(); gs != nil {\n\t\t\treturn gs, nil\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil, context.Cause(ctx)\n\t\tcase <-ticker.C:\n\t\t\td := (2 ^ time.Duration(i)) * interval\n\t\t\tif d > maxInterval {\n\t\t\t\td = maxInterval\n\t\t\t}\n\t\t\tticker.Reset(d)\n\t\t}\n\t}\n}\n\n// TryAcquireWriteLock acquires the appropriate locks for a write.\n// If any locks fail then the action is aborted.\nfunc (db *DB) TryAcquireWriteLock() (ret *GuardSet) {\n\tvar blockedBy string\n\tdefer func() {\n\t\tif ret == nil {\n\t\t\tTraceLog.Printf(\"[TryAcquireWriteLock.Fail(%s)]: blockedBy=%s\", db.name, blockedBy)\n\t\t}\n\t}()\n\n\tgs := db.newGuardSet(0)\n\tdefer func() {\n\t\tif ret == nil {\n\t\t\tgs.Unlock()\n\t\t}\n\t}()\n\n\t// Acquire shared lock to check database mode.\n\tif !gs.pending.TryRLock() {\n\t\tblockedBy = \"rlock(PENDING)\"\n\t\treturn nil\n\t}\n\tif !gs.shared.TryRLock() {\n\t\tblockedBy = \"rlock(SHARED)\"\n\t\treturn nil\n\t}\n\tgs.pending.Unlock()\n\n\t// If this is a rollback journal, upgrade all database locks to exclusive.\n\tif db.Mode() == DBModeRollback {\n\t\tif !gs.reserved.TryLock() {\n\t\t\tblockedBy = \"lock(RESERVED)\"\n\t\t\treturn nil\n\t\t}\n\t\tif !gs.pending.TryLock() {\n\t\t\tblockedBy = \"lock(PENDING)\"\n\t\t\treturn nil\n\t\t}\n\t\tif !gs.shared.TryLock() {\n\t\t\tblockedBy = \"lock(SHARED)\"\n\t\t\treturn nil\n\t\t}\n\t\treturn gs\n\t}\n\n\tif !gs.dms.TryRLock() {\n\t\tblockedBy = \"rlock(DMS)\"\n\t\treturn nil\n\t}\n\tif !gs.write.TryLock() {\n\t\tblockedBy = \"lock(WRITE)\"\n\t\treturn nil\n\t}\n\tif !gs.ckpt.TryLock() {\n\t\tblockedBy = \"lock(CKPT)\"\n\t\treturn nil\n\t}\n\tif !gs.recover.TryLock() {\n\t\tblockedBy = \"lock(RECOVER)\"\n\t\treturn nil\n\t}\n\tif !gs.read0.TryLock() {\n\t\tblockedBy = \"lock(READ0)\"\n\t\treturn nil\n\t}\n\tif !gs.read1.TryLock() {\n\t\tblockedBy = \"lock(READ1)\"\n\t\treturn nil\n\t}\n\tif !gs.read2.TryLock() {\n\t\tblockedBy = \"lock(READ2)\"\n\t\treturn nil\n\t}\n\tif !gs.read3.TryLock() {\n\t\tblockedBy = \"lock(READ3)\"\n\t\treturn nil\n\t}\n\tif !gs.read4.TryLock() {\n\t\tblockedBy = \"lock(READ4)\"\n\t\treturn nil\n\t}\n\n\treturn gs\n}\n\n// GuardSet returns a guard set for the given owner, if it exists.\nfunc (db *DB) GuardSet(owner uint64) *GuardSet {\n\tdb.guardSets.mu.Lock()\n\tdefer db.guardSets.mu.Unlock()\n\treturn db.guardSets.m[owner]\n}\n\n// CreateGuardSetIfNotExists returns a guard set for the given owner.\n// Creates a new guard set if one is not associated with the owner.\nfunc (db *DB) CreateGuardSetIfNotExists(owner uint64) *GuardSet {\n\tdb.guardSets.mu.Lock()\n\tdefer db.guardSets.mu.Unlock()\n\n\tguardSet := db.guardSets.m[owner]\n\tif guardSet == nil {\n\t\tguardSet = db.newGuardSet(owner)\n\t\tdb.guardSets.m[owner] = guardSet\n\t}\n\treturn guardSet\n}\n\n// newGuardSet returns a set of guards that can control locking for the database file.\nfunc (db *DB) newGuardSet(owner uint64) *GuardSet {\n\treturn &GuardSet{\n\t\towner: owner,\n\n\t\tpending:  db.pendingLock.Guard(),\n\t\tshared:   db.sharedLock.Guard(),\n\t\treserved: db.reservedLock.Guard(),\n\n\t\twrite:   db.writeLock.Guard(),\n\t\tckpt:    db.ckptLock.Guard(),\n\t\trecover: db.recoverLock.Guard(),\n\t\tread0:   db.read0Lock.Guard(),\n\t\tread1:   db.read1Lock.Guard(),\n\t\tread2:   db.read2Lock.Guard(),\n\t\tread3:   db.read3Lock.Guard(),\n\t\tread4:   db.read4Lock.Guard(),\n\t\tdms:     db.dmsLock.Guard(),\n\t}\n}\n\n// TryLocks attempts to lock one or more locks on the database for a given owner.\n// Returns an error if no locks are supplied.\nfunc (db *DB) TryLocks(ctx context.Context, owner uint64, lockTypes []LockType) (bool, error) {\n\tguardSet := db.CreateGuardSetIfNotExists(owner)\n\tfor _, lockType := range lockTypes {\n\t\tguard := guardSet.Guard(lockType)\n\n\t\t// There is a race condition where a passive checkpoint can copy out data\n\t\t// from the WAL to the database before an LTX file is written. To prevent\n\t\t// that, we require that the owner has acquired the WRITE lock before\n\t\t// acquiring the CKPT lock or that the WRITE lock is not currently locked.\n\t\t// This ensures that no checkpoint can occur while a WAL write is in-progress.\n\t\t//\n\t\t// Another option would be to rebuild the last LTX file on recovery from the\n\t\t// WAL, however there could be a race in there between WAL fsync() and\n\t\t// checkpoint. That option needs more investigation.\n\t\tif lockType == LockTypeCkpt &&\n\t\t\tdb.writeLock.State() != RWMutexStateUnlocked && // is there a writer?\n\t\t\tguardSet.write.State() != RWMutexStateExclusive { // is this owner the writer?\n\t\t\tTraceLog.Printf(\"[TryLock(%s)]: type=%s owner=%d status=IMPLICIT-FAIL\", db.name, lockType, owner)\n\t\t\treturn false, nil\n\t\t}\n\n\t\tok := guard.TryLock()\n\n\t\tstatus := \"OK\"\n\t\tif !ok {\n\t\t\tstatus = \"FAIL\"\n\t\t}\n\t\tTraceLog.Printf(\"[TryLock(%s)]: type=%s owner=%d status=%s\", db.name, lockType, owner, status)\n\n\t\tif !ok {\n\t\t\treturn false, nil\n\t\t}\n\n\t\t// TODO(fwd): Move remote lock to lock byte on database.\n\n\t\t// Start a new transaction on the database. This may start a remote transaction.\n\t\t//if lockType == LockTypeWrite {\n\t\t//\tif err := db.store.Begin(context.Background(), db.name); err != nil {\n\t\t//\t\tguard.Unlock()\n\t\t//\t\treturn false, err\n\t\t//\t}\n\t\t//}\n\t}\n\treturn true, nil\n}\n\n// CanLock returns true if all locks can acquire a write lock.\n// If false, also returns the mutex state of the blocking lock.\nfunc (db *DB) CanLock(ctx context.Context, owner uint64, lockTypes []LockType) (bool, RWMutexState) {\n\tguardSet := db.CreateGuardSetIfNotExists(owner)\n\tfor _, lockType := range lockTypes {\n\t\tguard := guardSet.Guard(lockType)\n\t\tif canLock, mutexState := guard.CanLock(); !canLock {\n\t\t\treturn false, mutexState\n\t\t}\n\t}\n\treturn true, RWMutexStateUnlocked\n}\n\n// TryRLocks attempts to read lock one or more locks on the database for a given owner.\n// Returns an error if no locks are supplied.\nfunc (db *DB) TryRLocks(ctx context.Context, owner uint64, lockTypes []LockType) bool {\n\tguardSet := db.CreateGuardSetIfNotExists(owner)\n\tfor _, lockType := range lockTypes {\n\t\tok := guardSet.Guard(lockType).TryRLock()\n\n\t\tstatus := \"OK\"\n\t\tif !ok {\n\t\t\tstatus = \"FAIL\"\n\t\t}\n\t\tTraceLog.Printf(\"[TryRLock(%s)]: type=%s owner=%d status=%s\", db.name, lockType, owner, status)\n\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// CanRLock returns true if all locks can acquire a read lock.\nfunc (db *DB) CanRLock(ctx context.Context, owner uint64, lockTypes []LockType) bool {\n\tguardSet := db.CreateGuardSetIfNotExists(owner)\n\tfor _, lockType := range lockTypes {\n\t\tif !guardSet.Guard(lockType).CanRLock() {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Unlock unlocks one or more locks on the database for a given owner.\nfunc (db *DB) Unlock(ctx context.Context, owner uint64, lockTypes []LockType) error {\n\tguardSet := db.GuardSet(owner)\n\tif guardSet == nil {\n\t\treturn nil\n\t}\n\n\t// Process WAL if we have an exclusive lock on WAL_WRITE_LOCK.\n\tif ContainsLockType(lockTypes, LockTypeWrite) && guardSet.Write().State() == RWMutexStateExclusive {\n\t\tif err := db.CommitWAL(ctx); err != nil {\n\t\t\tlog.Printf(\"commit wal error(2): %s\", err)\n\t\t}\n\t}\n\n\tfor _, lockType := range lockTypes {\n\t\tTraceLog.Printf(\"[Unlock(%s)]: type=%s owner=%d\", db.name, lockType, owner)\n\t\tguardSet.Guard(lockType).Unlock()\n\t}\n\n\t// TODO: Release guard set if completely unlocked.\n\n\treturn nil\n}\n\n// InWriteTx returns true if the RESERVED lock has an exclusive lock.\nfunc (db *DB) InWriteTx() bool {\n\treturn db.reservedLock.State() == RWMutexStateExclusive\n}\n\n// blockChksum returns the aggregate checksum for a block of pages.\n// Must hold db.chksums.mu lock.\nfunc (db *DB) blockChksum(block uint32) ltx.Checksum {\n\tif block >= uint32(len(db.chksums.blocks)) || db.chksums.blocks[block] == 0 {\n\t\tdb.recomputeBlockChksum(block)\n\t}\n\treturn db.chksums.blocks[block]\n}\n\n// recomputeBlockChksum regenerates a cached block-level checksum.\n// Must hold db.chksums.mu when invoked.\nfunc (db *DB) recomputeBlockChksum(block uint32) {\n\t// Ensure there is enough space in the blocks list.\n\tif n := int(block) + 1; n > len(db.chksums.blocks) {\n\t\tdb.chksums.blocks = append(db.chksums.blocks, make([]ltx.Checksum, n-len(db.chksums.blocks))...)\n\t}\n\n\t// Aggregate all page checksums within the block.\n\tvar chksum ltx.Checksum\n\tfor i := uint32(0); i < ChecksumBlockSize; i++ {\n\t\tpgno := (block * ChecksumBlockSize) + i + 1\n\t\tpageChksum := db.databasePageChecksum(pgno)\n\t\tchksum = ltx.ChecksumFlag | (chksum ^ pageChksum)\n\t}\n\n\tdb.chksums.blocks[block] = chksum\n}\n\n// checksum returns the checksum of the database based on per-page checksums.\nfunc (db *DB) checksum(pageN uint32, newWALChecksums map[uint32]ltx.Checksum) (ltx.Checksum, error) {\n\tif pageN == 0 {\n\t\treturn ltx.ChecksumFlag, nil\n\t}\n\n\tdb.chksums.mu.Lock()\n\tdefer db.chksums.mu.Unlock()\n\n\t// Ignore blocks which have pages in the WAL.\n\tblockN := pageChksumBlock(pageN) + 1\n\tignoredBlocks := make([]bool, blockN)\n\tfor pgno := range db.wal.chksums {\n\t\tignoredBlocks[pageChksumBlock(pgno)] = true\n\t}\n\tfor pgno := range newWALChecksums {\n\t\tignoredBlocks[pageChksumBlock(pgno)] = true\n\t}\n\n\tvar chksum ltx.Checksum\n\tfor block := uint32(0); block < blockN; block++ {\n\t\t// Use cached block checksum if it is computed and it does not have a\n\t\t// page in the WAL.\n\t\tif !ignoredBlocks[block] {\n\t\t\tblockChksum := db.blockChksum(block)\n\t\t\tif blockChksum != 0 {\n\t\t\t\tchksum = ltx.ChecksumFlag | (chksum ^ blockChksum)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// All other pages need to be computed individually.\n\t\tfor i := uint32(0); i < ChecksumBlockSize; i++ {\n\t\t\tpgno := (block * ChecksumBlockSize) + i + 1\n\t\t\tif pgno > pageN {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tpageChksum, ok := db.pageChecksum(pgno, pageN, newWALChecksums)\n\t\t\tif !ok {\n\t\t\t\treturn 0, fmt.Errorf(\"missing checksum for page %d\", pgno)\n\t\t\t}\n\t\t\tchksum = ltx.ChecksumFlag | (chksum ^ pageChksum)\n\t\t}\n\t}\n\n\treturn chksum, nil\n}\n\n// pageChecksum returns the latest checksum for a given page. This first tries\n// to find the last valid version of the page on the WAL. If that doesn't exist,\n// it returns the checksum of the page on the database. Returns zero if no\n// checksum exists for the page.\n//\n// The lock page will always return a checksum of zero and a true.\n//\n// Database WRITE lock and db.chksums.mu should be held when invoked.\nfunc (db *DB) pageChecksum(pgno, pageN uint32, newWALChecksums map[uint32]ltx.Checksum) (chksum ltx.Checksum, ok bool) {\n\t// The lock page should never have a checksum.\n\tif pgno == ltx.LockPgno(db.pageSize) {\n\t\treturn 0, true\n\t}\n\n\t// Pages past the end of the database should always have no checksum.\n\t// This is generally handled by the caller but it's added here too.\n\tif pgno > pageN {\n\t\treturn 0, false\n\t}\n\n\t// If we're trying to calculate the checksum of an in-progress WAL transaction,\n\t// we'll check the new checksums to be added first.\n\tif len(newWALChecksums) > 0 {\n\t\tif chksum, ok = newWALChecksums[pgno]; ok {\n\t\t\treturn chksum, true\n\t\t}\n\t}\n\n\t// Next, find the last valid checksum within committed WAL pages.\n\tif chksums := db.wal.chksums[pgno]; len(chksums) > 0 {\n\t\treturn chksums[len(chksums)-1], true\n\t}\n\n\t// Finally, pull the checksum from the database.\n\tchksum = db.databasePageChecksum(pgno)\n\treturn chksum, chksum != 0\n}\n\n// databasePageChecksum returns the checksum for a page in the database file.\n// Returns zero if unset. Must hold db.chksums.mu.\nfunc (db *DB) databasePageChecksum(pgno uint32) ltx.Checksum {\n\tassert(pgno > 0, \"database pgno must be larger than zero\")\n\n\tif i := pgno - 1; i < uint32(len(db.chksums.pages)) {\n\t\treturn db.chksums.pages[i]\n\t}\n\treturn 0\n}\n\n// setDatabasePageChecksum sets the checksum for a page in the database file.\n// Must hold db.chksums.mu.\nfunc (db *DB) setDatabasePageChecksum(pgno uint32, chksum ltx.Checksum) {\n\tassert(pgno > 0, \"database pgno must be larger than zero\")\n\n\t// Always overwrite the lock page as a zero checksum.\n\tif pgno == ltx.LockPgno(db.pageSize) {\n\t\tchksum = 0\n\t}\n\n\t// Ensure we have at least enough space to set the checksum.\n\tif n := int(pgno); n > len(db.chksums.pages) {\n\t\tdb.chksums.pages = append(db.chksums.pages, make([]ltx.Checksum, n-len(db.chksums.pages))...)\n\t}\n\n\tdb.chksums.pages[pgno-1] = chksum\n\n\t// Clear cached block checksum, if available.\n\tif block := pageChksumBlock(pgno); block < uint32(len(db.chksums.blocks)) {\n\t\tdb.chksums.blocks[block] = 0\n\t}\n}\n\nfunc (db *DB) resetDatabasePageChecksumsAfter(commit uint32) {\n\tfor i := commit; i < uint32(len(db.chksums.pages)); i++ {\n\t\tpgno := i + 1\n\t\tdb.setDatabasePageChecksum(pgno, 0)\n\t}\n}\n\n// WriteSnapshotTo writes an LTX snapshot to dst.\nfunc (db *DB) WriteSnapshotTo(ctx context.Context, dst io.Writer) (header ltx.Header, trailer ltx.Trailer, err error) {\n\tgs := db.newGuardSet(0) // TODO(fsm): Track internal owners?\n\tdefer gs.Unlock()\n\n\t// Acquire PENDING then SHARED. Release PENDING immediately afterward.\n\tif err := gs.pending.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire PENDING read lock: %w\", err)\n\t}\n\tif err := gs.shared.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire SHARED read lock: %w\", err)\n\t}\n\tgs.pending.Unlock()\n\n\t// If this is WAL mode then temporarily obtain a write lock so we can copy\n\t// out the current database size & wal frames before returning to a read lock.\n\tif db.Mode() == DBModeWAL {\n\t\tif err := gs.write.Lock(ctx); err != nil {\n\t\t\treturn header, trailer, fmt.Errorf(\"acquire temporary exclusive WAL_WRITE_LOCK: %w\", err)\n\t\t}\n\t}\n\n\t// Determine current position & snapshot overriding WAL frames.\n\tpos := db.Pos()\n\tpageSize, pageN := db.pageSize, db.PageN()\n\twalFrameOffsets := make(map[uint32]int64, len(db.wal.frameOffsets))\n\tfor k, v := range db.wal.frameOffsets {\n\t\twalFrameOffsets[k] = v\n\t}\n\n\t// Release write lock, if acquired.\n\tgs.write.Unlock()\n\n\t// Acquire the CKPT/RECOVER locks while we check reads.\n\tif err := gs.ckpt.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire CKPT read lock: %w\", err)\n\t}\n\tif err := gs.recover.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire RECOVER read lock: %w\", err)\n\t}\n\n\t// Acquire READ locks to prevent checkpointing, in case this is in WAL mode.\n\tif err := gs.read0.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire READ0 read lock: %w\", err)\n\t}\n\tif err := gs.read1.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire READ1 read lock: %w\", err)\n\t}\n\tif err := gs.read2.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire READ2 read lock: %w\", err)\n\t}\n\tif err := gs.read3.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire READ3 read lock: %w\", err)\n\t}\n\tif err := gs.read4.RLock(ctx); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"acquire READ4 read lock: %w\", err)\n\t}\n\n\t// Release CKPT & RECOVER locks since we have the read locks.\n\tgs.ckpt.Unlock()\n\tgs.recover.Unlock()\n\n\t// Log transaction ID for the snapshot.\n\tlog.Printf(\"writing snapshot %q @ %s\", db.name, pos.TXID.String())\n\n\t// Open database file. File may not exist if the database has been deleted.\n\tvar dbFile *os.File\n\tif dbFile, err = db.os.Open(\"WRITESNAPSHOT:DB\", db.DatabasePath()); err != nil && !os.IsNotExist(err) {\n\t\treturn header, trailer, fmt.Errorf(\"open database file: %w\", err)\n\t} else if err == nil {\n\t\tdefer func() { _ = dbFile.Close() }()\n\t}\n\n\t// Open WAL file if we have overriding WAL frames.\n\tvar walFile *os.File\n\tif len(walFrameOffsets) > 0 {\n\t\tif walFile, err = db.os.Open(\"WRITESNAPSHOT:WAL\", db.WALPath()); err != nil {\n\t\t\treturn header, trailer, fmt.Errorf(\"open wal file: %w\", err)\n\t\t}\n\t\tdefer func() { _ = walFile.Close() }()\n\t}\n\n\t// Write current database state to an LTX writer.\n\tenc := ltx.NewEncoder(dst)\n\tif err := enc.EncodeHeader(ltx.Header{\n\t\tVersion:   ltx.Version,\n\t\tFlags:     db.store.ltxHeaderFlags(),\n\t\tPageSize:  pageSize,\n\t\tCommit:    pageN,\n\t\tMinTXID:   1,\n\t\tMaxTXID:   pos.TXID,\n\t\tTimestamp: db.Now().UnixMilli(),\n\t\tNodeID:    db.store.ID(),\n\t}); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"encode ltx header: %w\", err)\n\t}\n\n\t// Write page frames.\n\tpageData := make([]byte, pageSize)\n\tlockPgno := ltx.LockPgno(pageSize)\n\tvar chksum ltx.Checksum\n\tfor pgno := uint32(1); pgno <= pageN; pgno++ {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn header, trailer, context.Cause(ctx)\n\t\tdefault:\n\t\t}\n\n\t\t// Skip the lock page.\n\t\tif pgno == lockPgno {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Read from WAL if page exists in offset map. Otherwise read from DB.\n\t\tif walFrameOffset, ok := walFrameOffsets[pgno]; ok {\n\t\t\tif _, err := walFile.Seek(walFrameOffset+WALFrameHeaderSize, io.SeekStart); err != nil {\n\t\t\t\treturn header, trailer, fmt.Errorf(\"seek wal page: %w\", err)\n\t\t\t} else if _, err := io.ReadFull(walFile, pageData); err != nil {\n\t\t\t\treturn header, trailer, fmt.Errorf(\"read wal page: %w\", err)\n\t\t\t}\n\t\t} else {\n\t\t\tif _, err := dbFile.Seek(int64(pgno-1)*int64(pageSize), io.SeekStart); err != nil {\n\t\t\t\treturn header, trailer, fmt.Errorf(\"seek database page: %w\", err)\n\t\t\t} else if _, err := io.ReadFull(dbFile, pageData); err != nil {\n\t\t\t\treturn header, trailer, fmt.Errorf(\"read database page: %w\", err)\n\t\t\t}\n\t\t}\n\n\t\tif err := enc.EncodePage(ltx.PageHeader{Pgno: pgno}, pageData); err != nil {\n\t\t\treturn header, trailer, fmt.Errorf(\"encode page frame: %w\", err)\n\t\t}\n\n\t\tchksum ^= ltx.ChecksumPage(pgno, pageData)\n\t}\n\n\t// Set the database checksum before we write the trailer.\n\tpostApplyChecksum := ltx.ChecksumFlag | chksum\n\tif postApplyChecksum != pos.PostApplyChecksum {\n\t\treturn header, trailer, fmt.Errorf(\"snapshot checksum mismatch at tx %s: %x <> %x\", pos.TXID.String(), postApplyChecksum, pos.PostApplyChecksum)\n\t}\n\tenc.SetPostApplyChecksum(postApplyChecksum)\n\n\tif err := enc.Close(); err != nil {\n\t\treturn header, trailer, fmt.Errorf(\"close ltx encoder: %w\", err)\n\t}\n\n\treturn enc.Header(), enc.Trailer(), nil\n}\n\n// EnforceRetention removes all LTX files created before minTime.\nfunc (db *DB) EnforceRetention(ctx context.Context, minTime time.Time) error {\n\thwm := db.HWM()\n\n\t// Collect all LTX files.\n\tents, err := db.ReadLTXDir()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"read ltx dir: %w\", err)\n\t} else if len(ents) == 0 {\n\t\treturn nil // no LTX files, exit\n\t}\n\n\t// Delete all files that are before the minimum time.\n\tvar totalN int\n\tvar totalSize int64\n\tfor i, ent := range ents {\n\t\t// Check if file qualifies for deletion.\n\t\tfi, err := ent.Info()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"info: %w\", err)\n\t\t}\n\n\t\t// Parse TXID range from filename.\n\t\t_, maxTXID, err := ltx.ParseFilename(ent.Name())\n\t\tif err != nil {\n\t\t\tcontinue // unknown file, skip\n\t\t}\n\n\t\t// File should be marked for removal if it is older than the retention period.\n\t\tshouldRemove := fi.ModTime().Before(minTime)\n\n\t\t// If a backup service is enabled, ensure the LTX file has been persisted\n\t\t// to long-term storage. This is typically something like S3 which has\n\t\t// very high durability.\n\t\tif db.store.BackupClient != nil {\n\t\t\tshouldRemove = shouldRemove && maxTXID < hwm\n\t\t}\n\n\t\t// Ensure the latest LTX file is never deleted.\n\t\tif i == len(ents)-1 {\n\t\t\tshouldRemove = false\n\t\t}\n\n\t\t// If we aren't removing the file, just track its metrics and skip.\n\t\tif !shouldRemove {\n\t\t\ttotalN++\n\t\t\ttotalSize += fi.Size()\n\t\t\tcontinue\n\t\t}\n\n\t\t// Remove file if it passes all the checks.\n\t\tfilename := filepath.Join(db.LTXDir(), ent.Name())\n\t\tif err := db.os.Remove(\"ENFORCERETENTION\", filename); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Update metrics.\n\t\tdbLTXReapCountMetricVec.WithLabelValues(db.name).Inc()\n\t}\n\n\t// Reset metrics for LTX disk usage.\n\tdbLTXCountMetricVec.WithLabelValues(db.name).Set(float64(totalN))\n\tdbLTXBytesMetricVec.WithLabelValues(db.name).Set(float64(totalSize))\n\n\treturn nil\n}\n\ntype dbVarJSON struct {\n\tName     string `json:\"name\"`\n\tTXID     string `json:\"txid\"`\n\tChecksum string `json:\"checksum\"`\n\n\tLocks struct {\n\t\tPending  string `json:\"pending\"`\n\t\tShared   string `json:\"shared\"`\n\t\tReserved string `json:\"reserved\"`\n\n\t\tWrite   string `json:\"write\"`\n\t\tCkpt    string `json:\"ckpt\"`\n\t\tRecover string `json:\"recover\"`\n\t\tRead0   string `json:\"read0\"`\n\t\tRead1   string `json:\"read1\"`\n\t\tRead2   string `json:\"read2\"`\n\t\tRead3   string `json:\"read3\"`\n\t\tRead4   string `json:\"read4\"`\n\t\tDMS     string `json:\"dms\"`\n\t} `json:\"locks\"`\n}\n\n// JouralReader represents a reader of the SQLite journal file format.\ntype JournalReader struct {\n\tf      *os.File\n\tfi     os.FileInfo // cached file info\n\toffset int64       // read offset\n\tframe  []byte      // frame buffer\n\n\tisValid    bool   // true, if at least one valid header exists\n\tframeN     int32  // Number of pages in the segment\n\tnonce      uint32 // A random nonce for the checksum\n\tcommit     uint32 // Initial size of the database in pages\n\tsectorSize uint32 // Size of the disk sectors\n\tpageSize   uint32 // Size of each page, in bytes\n}\n\n// JournalReader returns a new instance of JournalReader.\nfunc NewJournalReader(f *os.File, pageSize uint32) *JournalReader {\n\treturn &JournalReader{\n\t\tf:        f,\n\t\tpageSize: pageSize,\n\t}\n}\n\n// DatabaseSize returns the size of the database before the journal transaction, in bytes.\nfunc (r *JournalReader) DatabaseSize() int64 {\n\treturn int64(r.commit) * int64(r.pageSize)\n}\n\n// IsValid returns true if at least one journal header was read.\nfunc (r *JournalReader) IsValid() bool { return r.isValid }\n\n// Next reads the next segment of the journal. Returns io.EOF if no more segments exist.\nfunc (r *JournalReader) Next() (err error) {\n\t// Determine journal size on initial call.\n\tif r.fi == nil {\n\t\tif r.fi, err = r.f.Stat(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Ensure offset is sector-aligned.\n\tr.offset = journalHeaderOffset(r.offset, int64(r.sectorSize))\n\n\t// Read full header.\n\thdr := make([]byte, SQLITE_JOURNAL_HEADER_SIZE)\n\tif _, err := internal.ReadFullAt(r.f, hdr, r.offset); err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn io.EOF // no header or partial header\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\t// Exit if PERSIST journal mode has cleared the header for finalization.\n\tif isByteSliceZero(hdr) {\n\t\treturn io.EOF\n\t}\n\n\t// After the first segment, we require the magic bytes.\n\tif r.offset > 0 && !bytes.Equal(hdr[:8], []byte(SQLITE_JOURNAL_HEADER_STRING)) {\n\t\treturn io.EOF\n\t}\n\n\t// Read number of frames in journal segment. Set to -1 if no-sync was set\n\t// and set to 0 if the journal was not sync'd. In these two cases we will\n\t// calculate the frame count based on the journal size.\n\tr.frameN = int32(binary.BigEndian.Uint32(hdr[8:]))\n\tif r.frameN == -1 {\n\t\tr.frameN = int32((r.fi.Size() - int64(r.sectorSize)) / int64(r.pageSize))\n\t} else if r.frameN == 0 {\n\t\tr.frameN = int32((r.fi.Size() - r.offset) / int64(r.pageSize))\n\t}\n\n\t// Read remaining fields from header.\n\tr.nonce = binary.BigEndian.Uint32(hdr[12:])  // cksumInit\n\tr.commit = binary.BigEndian.Uint32(hdr[16:]) // dbSize\n\n\t// Only read sector and page size from first journal header.\n\tif r.offset == 0 {\n\t\tr.sectorSize = binary.BigEndian.Uint32(hdr[20:])\n\n\t\t// Use page size from journal reader, if set to 0.\n\t\tpageSize := binary.BigEndian.Uint32(hdr[24:])\n\t\tif pageSize == 0 {\n\t\t\tpageSize = r.pageSize\n\t\t}\n\t\tif pageSize != r.pageSize {\n\t\t\treturn fmt.Errorf(\"journal header page size (%d) does not match database (%d)\", pageSize, r.pageSize)\n\t\t}\n\t}\n\n\t// Exit if file doesn't have more than the initial sector.\n\tif r.offset+int64(r.sectorSize) > r.fi.Size() {\n\t\treturn io.EOF\n\t}\n\n\t// Move journal offset to first sector.\n\tr.offset += int64(r.sectorSize)\n\n\t// Create a buffer to read the segment frames.\n\tr.frame = make([]byte, r.pageSize+4+4)\n\n\t// Mark journal as valid if we've read at least one header.\n\tr.isValid = true\n\n\treturn nil\n}\n\n// ReadFrame returns the page number and page data for the next frame.\n// Returns io.EOF after the last frame. Page data should not be retained.\nfunc (r *JournalReader) ReadFrame() (pgno uint32, data []byte, err error) {\n\t// No more frames, exit.\n\tif r.frameN == 0 {\n\t\treturn 0, nil, io.EOF\n\t}\n\n\t// Read the next frame from the journal.\n\tn, err := internal.ReadFullAt(r.f, r.frame, r.offset)\n\tif err == io.ErrUnexpectedEOF {\n\t\treturn 0, nil, io.EOF\n\t} else if err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\tpgno = binary.BigEndian.Uint32(r.frame[0:])\n\tdata = r.frame[4 : len(r.frame)-4]\n\tchksum := binary.BigEndian.Uint32(r.frame[len(r.frame)-4:])\n\n\tif chksum != JournalChecksum(data, r.nonce) {\n\t\treturn 0, nil, io.EOF\n\t}\n\n\tr.frameN--\n\tr.offset += int64(n)\n\n\treturn pgno, data, nil\n}\n\n// journalHeaderOffset returns a sector-aligned offset.\nfunc journalHeaderOffset(offset, sectorSize int64) int64 {\n\tif offset == 0 {\n\t\treturn 0\n\t}\n\treturn ((offset-1)/sectorSize + 1) * sectorSize\n}\n\n// TrimName removes \"-journal\", \"-shm\" or \"-wal\" from the given name.\nfunc TrimName(name string) string {\n\tif suffix := \"-journal\"; strings.HasSuffix(name, suffix) {\n\t\tname = strings.TrimSuffix(name, suffix)\n\t}\n\tif suffix := \"-wal\"; strings.HasSuffix(name, suffix) {\n\t\tname = strings.TrimSuffix(name, suffix)\n\t}\n\tif suffix := \"-shm\"; strings.HasSuffix(name, suffix) {\n\t\tname = strings.TrimSuffix(name, suffix)\n\t}\n\treturn name\n}\n\n// errorKeyValue returns a key/value pair of the error. Returns a blank string if err is empty.\nfunc errorKeyValue(err error) string {\n\tif err == nil {\n\t\treturn \"\"\n\t}\n\treturn \"err=\" + err.Error()\n}\n\n// HaltLock represents a lock remotely held on the primary. This allows the\n// local node to perform writes and send them to the primary while the lock is held.\ntype HaltLock struct {\n\t// Unique identifier for the lock.\n\tID int64 `json:\"id\"`\n\n\t// Position of the primary when this lock was acquired.\n\tPos ltx.Pos `json:\"pos\"`\n\n\t// Time that the halt lock expires at.\n\tExpires *time.Time `json:\"expires\"`\n}\n\n// haltLockAndGuard groups a halt lock and its associated guard set.\ntype haltLockAndGuard struct {\n\thaltLock *HaltLock\n\tguardSet *GuardSet\n}\n\n// ChecksumBlockSize is the number of pages that are grouped into a single checksum block.\nconst ChecksumBlockSize = 256\n\n// pageChksumBlock returns the checksum block that the page belongs to.\nfunc pageChksumBlock(pgno uint32) uint32 {\n\tassert(pgno > 0, \"pgno must be greater than zero\")\n\treturn (pgno - 1) / ChecksumBlockSize\n}\n\n// Database metrics.\nvar (\n\tdbTXIDMetricVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_txid\",\n\t\tHelp: \"Current transaction ID.\",\n\t}, []string{\"db\"})\n\n\tdbDatabaseWriteCountMetricVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litefs_db_database_write_count\",\n\t\tHelp: \"Number of writes to the database file.\",\n\t}, []string{\"db\"})\n\n\tdbJournalWriteCountMetricVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litefs_db_journal_write_count\",\n\t\tHelp: \"Number of writes to the journal file.\",\n\t}, []string{\"db\"})\n\n\tdbWALWriteCountMetricVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litefs_db_wal_write_count\",\n\t\tHelp: \"Number of writes to the WAL file.\",\n\t}, []string{\"db\"})\n\n\tdbSHMWriteCountMetricVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litefs_db_shm_write_count\",\n\t\tHelp: \"Number of writes to the shared memory file.\",\n\t}, []string{\"db\"})\n\n\tdbCommitCountMetricVec = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"litefs_db_commit_count\",\n\t\tHelp: \"Number of database commits.\",\n\t}, []string{\"db\"})\n\n\tdbLTXCountMetricVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_ltx_count\",\n\t\tHelp: \"Number of LTX files on disk.\",\n\t}, []string{\"db\"})\n\n\tdbLTXBytesMetricVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_ltx_bytes\",\n\t\tHelp: \"Number of bytes used by LTX files on disk.\",\n\t}, []string{\"db\"})\n\n\tdbLTXReapCountMetricVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_ltx_reap_count\",\n\t\tHelp: \"Number of LTX files removed by retention.\",\n\t}, []string{\"db\"})\n\n\tdbLatencySecondsMetricVec = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_lag_seconds\",\n\t\tHelp: \"Latency between generating an LTX file and consuming it.\",\n\t}, []string{\"db\"})\n)\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "fly",
          "type": "tree",
          "content": null
        },
        {
          "name": "fuse",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 2.263671875,
          "content": "module github.com/superfly/litefs\n\ngo 1.21\n\nrequire (\n\tbazil.org/fuse v0.0.0-20230120002735-62a210ff1fd5\n\tgithub.com/hashicorp/consul/api v1.11.0\n\tgithub.com/mattn/go-shellwords v1.0.12\n\tgithub.com/mattn/go-sqlite3 v1.14.16-0.20220918133448-90900be5db1a\n\tgithub.com/prometheus/client_golang v1.13.0\n\tgithub.com/superfly/litefs-go v0.0.0-20230227231337-34ea5dcf1e0b\n\tgithub.com/superfly/ltx v0.3.14\n\tgolang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc\n\tgolang.org/x/net v0.17.0\n\tgolang.org/x/sync v0.4.0\n\tgolang.org/x/sys v0.13.0\n\tgopkg.in/natefinch/lumberjack.v2 v2.0.0\n\tgopkg.in/yaml.v3 v3.0.1\n)\n\nrequire (\n\tgithub.com/armon/go-metrics v0.3.10 // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.1.2 // indirect\n\tgithub.com/fatih/color v1.9.0 // indirect\n\tgithub.com/golang/protobuf v1.5.2 // indirect\n\tgithub.com/hashicorp/go-cleanhttp v0.5.1 // indirect\n\tgithub.com/hashicorp/go-hclog v0.14.1 // indirect\n\tgithub.com/hashicorp/go-immutable-radix v1.3.0 // indirect\n\tgithub.com/hashicorp/go-msgpack v0.5.5 // indirect\n\tgithub.com/hashicorp/go-multierror v1.1.1 // indirect\n\tgithub.com/hashicorp/go-rootcerts v1.0.2 // indirect\n\tgithub.com/hashicorp/go-sockaddr v1.0.2 // indirect\n\tgithub.com/hashicorp/go-uuid v1.0.2 // indirect\n\tgithub.com/hashicorp/golang-lru v0.5.4 // indirect\n\tgithub.com/hashicorp/memberlist v0.3.1 // indirect\n\tgithub.com/hashicorp/serf v0.9.7 // indirect\n\tgithub.com/mattn/go-colorable v0.1.6 // indirect\n\tgithub.com/mattn/go-isatty v0.0.12 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.1 // indirect\n\tgithub.com/mitchellh/go-homedir v1.1.0 // indirect\n\tgithub.com/mitchellh/go-testing-interface v1.14.0 // indirect\n\tgithub.com/mitchellh/mapstructure v1.4.1 // indirect\n\tgithub.com/pierrec/lz4/v4 v4.1.17 // indirect\n\tgithub.com/prometheus/client_model v0.2.0 // indirect\n\tgithub.com/prometheus/common v0.37.0 // indirect\n\tgithub.com/prometheus/procfs v0.8.0 // indirect\n\tgithub.com/stretchr/testify v1.7.0 // indirect\n\tgolang.org/x/text v0.13.0 // indirect\n\tgoogle.golang.org/protobuf v1.28.1 // indirect\n)\n\n// replace github.com/superfly/litefs-go => ../litefs-go\n// replace github.com/mattn/go-sqlite3 => ../../mattn/go-sqlite3\n// replace github.com/pierrec/lz4/v4 => ../../pierrec/lz4\n// replace github.com/superfly/ltx => ../ltx\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 61.203125,
          "content": "bazil.org/fuse v0.0.0-20230120002735-62a210ff1fd5 h1:A0NsYy4lDBZAC6QiYeJ4N+XuHIKBpyhAVRMHRQZKTeQ=\nbazil.org/fuse v0.0.0-20230120002735-62a210ff1fd5/go.mod h1:gG3RZAMXCa/OTes6rr9EwusmR1OH1tDDy+cg9c5YliY=\ncloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\ncloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\ncloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\ncloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\ncloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\ncloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\ncloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=\ncloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\ncloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=\ncloud.google.com/go v0.56.0/go.mod h1:jr7tqZxxKOVYizybht9+26Z/gUq7tiRzu+ACVAMbKVk=\ncloud.google.com/go v0.57.0/go.mod h1:oXiQ6Rzq3RAkkY7N6t3TcE6jE+CIBBbA36lwQ1JyzZs=\ncloud.google.com/go v0.62.0/go.mod h1:jmCYTdRCQuc1PHIIJ/maLInMho30T/Y0M4hTdTShOYc=\ncloud.google.com/go v0.65.0/go.mod h1:O5N8zS7uWy9vkA9vayVHs65eM1ubvY4h553ofrNHObY=\ncloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\ncloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\ncloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=\ncloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\ncloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\ncloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\ncloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\ncloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=\ncloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\ncloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\ncloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=\ncloud.google.com/go/pubsub v1.3.1/go.mod h1:i+ucay31+CNRpDW4Lu78I4xXG+O1r/MAHgjpRVR+TSU=\ncloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\ncloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\ncloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=\ncloud.google.com/go/storage v1.8.0/go.mod h1:Wv1Oy7z6Yz3DshWRJFhqM/UCfaWIRTdp0RXyy7KQOVs=\ncloud.google.com/go/storage v1.10.0/go.mod h1:FLPqc6j+Ki4BU591ie1oL6qBQGu2Bl/tZ9ullr3+Kg0=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngithub.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/DataDog/datadog-go v3.2.0+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/armon/circbuf v0.0.0-20150827004946-bbbad097214e/go.mod h1:3U/XgcO3hCbHZ8TKRvWD2dDTCfh9M9ya+I9JpbB7O8o=\ngithub.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da/go.mod h1:Q73ZrmVTwzkszR9V5SSuryQ31EELlFMUz1kKyl939pY=\ngithub.com/armon/go-metrics v0.3.10 h1:FR+drcQStOe+32sYyJYyZ7FIdgoGGBnwLl+flodp8Uo=\ngithub.com/armon/go-metrics v0.3.10/go.mod h1:4O98XIr/9W0sxpJ8UaYkvjk10Iff7SnFrb4QAOwNTFc=\ngithub.com/armon/go-radix v0.0.0-20180808171621-7fddfc383310/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\ngithub.com/armon/go-radix v1.0.0/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/bgentry/speakeasy v0.1.0/go.mod h1:+zsyZBPWlz7T6j88CTgSN5bM796AkVf0kBD4zp0CCIs=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.1.2 h1:YRXhKfTDauu4ajMg1TPgFO5jnlC2HCbmLXMcTG5cbYE=\ngithub.com/cespare/xxhash/v2 v2.1.2/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\ngithub.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\ngithub.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\ngithub.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=\ngithub.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=\ngithub.com/fatih/color v1.9.0 h1:8xPHl4/q1VyqGIPif1F+1V3Y3lSmrq01EabUW3CoW5s=\ngithub.com/fatih/color v1.9.0/go.mod h1:eQcE1qtQxscV5RaZvpXrrb8Drkc3/DdQ+uUYCNjL+zU=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-kit/log v0.2.0/go.mod h1:NwTd00d/i8cPZ3xOwwiv2PO5MOcx78fFErGNcVmBjv0=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-logfmt/logfmt v0.5.1/go.mod h1:WYhtIu8zTZfxdn5+rREduYbwxfcBr/Vr6KEVveWlfTs=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\ngithub.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.3/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.4/go.mod h1:l3mdAwkq5BuhzHwde/uurv3sEJeZMXNpwsxVWU71h+4=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2 h1:ROPKBNFfQgOUMifHyP+KYbvpjbdoFNs+aK7DXlji0Tw=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.0 h1:0udJVsspx3VBr5FwtLhQQtuAsVc79tTq0ocGIPAU6qo=\ngithub.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.4.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.8 h1:e6P7q2lk1O+qJJb4BtCQXlK8vWEO8V1ZeuEdJNOqZyg=\ngithub.com/google/go-cmp v0.5.8/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\ngithub.com/google/martian/v3 v3.0.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\ngithub.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200430221834-fc25d7d30c6d/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200708004538-1a94d8640e99/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\ngithub.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\ngithub.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\ngithub.com/hashicorp/consul/api v1.11.0 h1:Hw/G8TtRvOElqxVIhBzXciiSTbapq8hZ2XKZsXk5ZCE=\ngithub.com/hashicorp/consul/api v1.11.0/go.mod h1:XjsvQN+RJGWI2TWy1/kqaE16HrR2J/FWgkYjdZQsX9M=\ngithub.com/hashicorp/consul/sdk v0.8.0 h1:OJtKBtEjboEZvG6AOUdh4Z1Zbyu0WcxQ0qatRrZHTVU=\ngithub.com/hashicorp/consul/sdk v0.8.0/go.mod h1:GBvyrGALthsZObzUGsfgHZQDXjg4lOjagTIwIR1vPms=\ngithub.com/hashicorp/errwrap v1.0.0 h1:hLrqtEDnRye3+sgx6z4qVLNuviH3MR5aQ0ykNJa/UYA=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\ngithub.com/hashicorp/go-cleanhttp v0.5.1 h1:dH3aiDG9Jvb5r5+bYHsikaOUIpcM0xvgMXVoDkXMzJM=\ngithub.com/hashicorp/go-cleanhttp v0.5.1/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\ngithub.com/hashicorp/go-hclog v0.12.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\ngithub.com/hashicorp/go-hclog v0.14.1 h1:nQcJDQwIAGnmoUWp8ubocEX40cCml/17YkF6csQLReU=\ngithub.com/hashicorp/go-hclog v0.14.1/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\ngithub.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-immutable-radix v1.3.0 h1:8exGP7ego3OmkfksihtSouGMZ+hQrhxx+FVELeXpVPE=\ngithub.com/hashicorp/go-immutable-radix v1.3.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-msgpack v0.5.3/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=\ngithub.com/hashicorp/go-msgpack v0.5.5 h1:i9R9JSrqIz0QVLz3sz+i3YJdT7TTSLcfLLzJi9aZTuI=\ngithub.com/hashicorp/go-msgpack v0.5.5/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=\ngithub.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\ngithub.com/hashicorp/go-multierror v1.1.0/go.mod h1:spPvp8C1qA32ftKqdAHm4hHTbPw+vmowP0z+KUhOZdA=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/go-retryablehttp v0.5.3/go.mod h1:9B5zBasrRhHXnJnui7y6sL7es7NDiJgTc6Er0maI1Xs=\ngithub.com/hashicorp/go-rootcerts v1.0.2 h1:jzhAVGtqPKbwpyCPELlgNWhE1znq+qwJtW5Oi2viEzc=\ngithub.com/hashicorp/go-rootcerts v1.0.2/go.mod h1:pqUvnprVnM5bf7AOirdbb01K4ccR319Vf4pU3K5EGc8=\ngithub.com/hashicorp/go-sockaddr v1.0.0/go.mod h1:7Xibr9yA9JjQq1JpNB2Vw7kxv8xerXegt+ozgdvDeDU=\ngithub.com/hashicorp/go-sockaddr v1.0.2 h1:ztczhD1jLxIRjVejw8gFomI1BQZOe2WoVOu0SyteCQc=\ngithub.com/hashicorp/go-sockaddr v1.0.2/go.mod h1:rB4wwRAUzs07qva3c5SdrY/NEtAUjGlgmH/UkBUC97A=\ngithub.com/hashicorp/go-syslog v1.0.0/go.mod h1:qPfqrKkXGihmCqbJM2mZgkZGvKG1dFdvsLplgctolz4=\ngithub.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-uuid v1.0.1/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-uuid v1.0.2 h1:cfejS+Tpcp13yd5nYHWDI6qVCny6wyX2Mt5SGur2IGE=\ngithub.com/hashicorp/go-uuid v1.0.2/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.4 h1:YDjusn29QI/Das2iO9M0BHnIbxPeyuCHsjMW+lJfyTc=\ngithub.com/hashicorp/golang-lru v0.5.4/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\ngithub.com/hashicorp/logutils v1.0.0/go.mod h1:QIAnNjmIWmVIIkWDTG1z5v++HQmx9WQRO+LraFDTW64=\ngithub.com/hashicorp/mdns v1.0.1/go.mod h1:4gW7WsVCke5TE7EPeYliwHlRUyBtfCwuFwuMg2DmyNY=\ngithub.com/hashicorp/mdns v1.0.4/go.mod h1:mtBihi+LeNXGtG8L9dX59gAEa12BDtBQSp4v/YAJqrc=\ngithub.com/hashicorp/memberlist v0.2.2/go.mod h1:MS2lj3INKhZjWNqd3N0m3J+Jxf3DAOnAH9VT3Sh9MUE=\ngithub.com/hashicorp/memberlist v0.3.0/go.mod h1:MS2lj3INKhZjWNqd3N0m3J+Jxf3DAOnAH9VT3Sh9MUE=\ngithub.com/hashicorp/memberlist v0.3.1 h1:MXgUXLqva1QvpVEDQW1IQLG0wivQAtmFlHRQ+1vWZfM=\ngithub.com/hashicorp/memberlist v0.3.1/go.mod h1:MS2lj3INKhZjWNqd3N0m3J+Jxf3DAOnAH9VT3Sh9MUE=\ngithub.com/hashicorp/serf v0.9.5/go.mod h1:UWDWwZeL5cuWDJdl0C6wrvrUwEqtQ4ZKBKKENpqIUyk=\ngithub.com/hashicorp/serf v0.9.7 h1:hkdgbqizGQHuU5IPqYM1JdSMV8nKfpuOnZYXssk9muY=\ngithub.com/hashicorp/serf v0.9.7/go.mod h1:TXZNMjZQijwlDvp+r0b63xZ45H7JmCmgg4gpTwn9UV4=\ngithub.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.9/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\ngithub.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.2.0 h1:s5hAObm+yFO5uHYt5dYjxi2rXrsnmRpJx4OYvIWUaQs=\ngithub.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\ngithub.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\ngithub.com/mattn/go-colorable v0.1.6 h1:6Su7aK7lXmJ/U79bYtBjLNaha4Fs1Rg9plHpcH+vvnE=\ngithub.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-isatty v0.0.3/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\ngithub.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.10/go.mod h1:qgIWMr58cqv1PHHyhnkY9lrL7etaEgOFcMEpPG5Rm84=\ngithub.com/mattn/go-isatty v0.0.11/go.mod h1:PhnuNfih5lzO57/f3n+odYbM4JtupLOxQOAqxQCu2WE=\ngithub.com/mattn/go-isatty v0.0.12 h1:wuysRhFDzyxgEmMf5xjvJ2M9dZoWAXNNr5LSBS7uHXY=\ngithub.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\ngithub.com/mattn/go-shellwords v1.0.12 h1:M2zGm7EW6UQJvDeQxo4T51eKPurbeFbe8WtebGE2xrk=\ngithub.com/mattn/go-shellwords v1.0.12/go.mod h1:EZzvwXDESEeg03EKmM+RmDnNOPKG4lLtQsUlTZDWQ8Y=\ngithub.com/mattn/go-sqlite3 v1.14.16-0.20220918133448-90900be5db1a h1:R51PEx1nBUbx9U8tizt++w4kw58bh+1wA030ClGhwjA=\ngithub.com/mattn/go-sqlite3 v1.14.16-0.20220918133448-90900be5db1a/go.mod h1:2eHXhiwb8IkHr+BDWZGa96P6+rkvnG63S2DGjv9HUNg=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1 h1:4hp9jkHxhMHkqkrB3Ix0jegS5sx/RkqARlsWZ6pIwiU=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/miekg/dns v1.0.14/go.mod h1:W1PPwlIAgtquWBMBEV9nkV9Cazfe8ScdGz/Lj7v3Nrg=\ngithub.com/miekg/dns v1.1.26/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=\ngithub.com/miekg/dns v1.1.41 h1:WMszZWJG0XmzbK9FEmzH2TVcqYzFesusSIB41b8KHxY=\ngithub.com/miekg/dns v1.1.41/go.mod h1:p6aan82bvRIyn+zDIv9xYNUpwa73JcSh9BKwknJysuI=\ngithub.com/mitchellh/cli v1.0.0/go.mod h1:hNIlj7HEI86fIcpObd7a0FcrxTWetlwJDGcceTlRvqc=\ngithub.com/mitchellh/cli v1.1.0/go.mod h1:xcISNoH86gajksDmfB23e/pu+B+GeFRMYmoHXxx3xhI=\ngithub.com/mitchellh/go-homedir v1.1.0 h1:lukF9ziXFxDFPkA1vsr5zpc1XuPDn/wFntq5mG+4E0Y=\ngithub.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\ngithub.com/mitchellh/go-testing-interface v1.0.0/go.mod h1:kRemZodwjscx+RGhAo8eIhFbs2+BFgRtFPeD/KE+zxI=\ngithub.com/mitchellh/go-testing-interface v1.14.0 h1:/x0XQ6h+3U3nAyk1yx+bHPURrKa9sVVvYbuqZ7pIAtI=\ngithub.com/mitchellh/go-testing-interface v1.14.0/go.mod h1:gfgS7OtZj6MA4U1UrDRp04twqAjfvlZyCfX3sDjEym8=\ngithub.com/mitchellh/go-wordwrap v1.0.0/go.mod h1:ZXFpozHsX6DPmq2I0TCekCxypsnAUbP2oI0UX1GXzOo=\ngithub.com/mitchellh/mapstructure v0.0.0-20160808181253-ca63d7c062ee/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\ngithub.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\ngithub.com/mitchellh/mapstructure v1.4.1 h1:CpVNEelQCZBooIPDn+AR3NpivK/TIKU8bDxdASFVQag=\ngithub.com/mitchellh/mapstructure v1.4.1/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/pascaldekloe/goe v0.0.0-20180627143212-57f6aae5913c/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/pascaldekloe/goe v0.1.0 h1:cBOtyMzM9HTpWjXfbbunk26uA6nG3a8n06Wieeh0MwY=\ngithub.com/pascaldekloe/goe v0.1.0/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/pierrec/lz4/v4 v4.1.17 h1:kV4Ip+/hUBC+8T6+2EgburRtkE9ef4nbY3f4dFhGjMc=\ngithub.com/pierrec/lz4/v4 v4.1.17/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/posener/complete v1.1.1/go.mod h1:em0nMJCgc9GFtwrmVmEMR/ZL6WyhyjMBndrE9hABlRI=\ngithub.com/posener/complete v1.2.3/go.mod h1:WZIdtGGp+qx0sLrYKtIRAruyNpv6hFCicSgv7Sy7s/s=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.4.0/go.mod h1:e9GMxYsXl05ICDXkRhurwBS4Q3OK1iX/F2sw+iXX5zU=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.0/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_golang v1.12.1/go.mod h1:3Z9XVyYiZYEO+YQWt3RD2R3jrbd179Rt297l4aS6nDY=\ngithub.com/prometheus/client_golang v1.13.0 h1:b71QUfeo5M8gq2+evJdTPfZhYMAU0uKPkyPJ7TPsloU=\ngithub.com/prometheus/client_golang v1.13.0/go.mod h1:vTeo+zgvILHsnnj/39Ou/1fPN5nJFOEMgftOUOmlvYQ=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0 h1:uq5h0d+GuxiXLJLNABMgp2qUWDPiLvgCzz2dUR+/W/M=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.9.1/go.mod h1:yhUN8i9wzaXS3w1O07YhxHEBxD+W35wd8bs7vj7HSQ4=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/common v0.32.1/go.mod h1:vu+V0TpY+O6vW9J44gczi3Ap/oXXR10b+M/gUGO4Hls=\ngithub.com/prometheus/common v0.37.0 h1:ccBbHCgIiT9uSoFY0vX8H3zsNR5eLt17/RQLUvn8pXE=\ngithub.com/prometheus/common v0.37.0/go.mod h1:phzohg0JFMnBEFGxTDbfu3QyL5GI8gTQJFhYO5B3mfA=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.0.8/go.mod h1:7Qr8sr6344vo1JqZ6HhLceV9o3AJ1Ff+GxbHq6oeK9A=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.7.3/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.8.0 h1:ODq8ZFEaYeCaZOJlZZdJA2AbQR98dSHSM1KW/You5mo=\ngithub.com/prometheus/procfs v0.8.0/go.mod h1:z7EfXMXOkbkqb9IINtpCn86r/to3BnA0uaxHdg830/4=\ngithub.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/ryanuber/columnize v0.0.0-20160712163229-9b3edd62028f/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\ngithub.com/ryanuber/columnize v2.1.0+incompatible/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529 h1:nn5Wsu0esKSJiIVhscUtVbo7ada43DJhG55ua/hjS5I=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529/go.mod h1:DxrIzT+xaE7yg65j358z/aeFdxmN0P9QXhEzd20vsDc=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1 h1:2vfRuCMp5sSVIDSqO8oNnWJq7mPa6KVP3iPIwFBuy8A=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.7.0 h1:nwc3DEeHmmLAfoZucVR881uASk0Mfjw8xYJ99tb5CcY=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/superfly/litefs-go v0.0.0-20230227231337-34ea5dcf1e0b h1:+WuhtZFB8fNdPeaMUtuB/U8aknXBXdDW/mBm/HTYJNg=\ngithub.com/superfly/litefs-go v0.0.0-20230227231337-34ea5dcf1e0b/go.mod h1:h+GUx1V2s0C5nY73ZN82760eWEJrpMaiDweF31VmJKk=\ngithub.com/superfly/ltx v0.3.14 h1:u/w2NJsyaAyh04+Oy2x5tQpWJzpzf9iQbY1wgfMZnW4=\ngithub.com/superfly/ltx v0.3.14/go.mod h1:ly+Dq7UVacQVEI5/b0r6j+PSNy9ibwx1yikcWAaSkhE=\ngithub.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=\ngithub.com/tv42/httpunix v0.0.0-20191220191345-2ba4b9c3382c h1:u6SKchux2yDvFQnDHS3lPnIRmfVJ5Sxy3ao2SIdysLQ=\ngithub.com/tv42/httpunix v0.0.0-20191220191345-2ba4b9c3382c/go.mod h1:hzIxponao9Kjc7aWznkXaL4U4TWaDSs8zcsY4Ka08nM=\ngithub.com/yuin/goldmark v1.1.25/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngo.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\ngo.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\ngo.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.4/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20181029021203-45a5f77698d3/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB+9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\ngolang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\ngolang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\ngolang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\ngolang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc h1:mCRnTeVUjcrhlRmO0VK8a6k6Rrf6TF9htwo2pJVSjIU=\ngolang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc/go.mod h1:V1LtkGg67GoY2N1AnLN78QLrzxkLyJw7RJb1gzOOz9w=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\ngolang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181023162649-9b4f9f5ad519/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190628185345-da137c7871d7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200501053045-e0ff5e5a1de5/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200506145744-7e3656a0809f/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200513185701-a91f0712d120/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210410081132-afb366fc7cd1/go.mod h1:9tjilg8BloeKEkVJvy7fQ90B1CfIiPueXVOjqfkSzI8=\ngolang.org/x/net v0.0.0-20210525063256-abc453219eb5/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220127200216-cd36cc0744dd/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\ngolang.org/x/net v0.0.0-20220225172249-27dd8689420f/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\ngolang.org/x/net v0.17.0 h1:pVaXccu2ozPjCXewfr1S7xza/zcXTity9cCdXQYSjIM=\ngolang.org/x/net v0.17.0/go.mod h1:NxSsAGuq816PNPmqtQdLE42eU2Fs7NoRIZrHJAlaCOE=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20210514164344-f6687ab2804c/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20220223155221-ee480838109b/go.mod h1:DAh4E804XQdzx2j+YRIaUnCqCV2RuMz24cGBJ5QYIrc=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200317015054-43a5402ce75a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200625203802-6e8e738ad208/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.4.0 h1:zxkM55ReGkDlKSM+Fu41A+zmbZuaPVbGMzvvdUPznYQ=\ngolang.org/x/sync v0.4.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sys v0.0.0-20180823144017-11551d06cbcc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181026203630-95b1ffbd15a5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191008105621-543471e840be/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200124204421-9fbb57f87de9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200331124033-c3d80250170d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200501052902-10377860bb8e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200511232937-7e40ca221e25/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200515095857-1151b9dac4a9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200523222454-059865788121/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200803210538-64077c9b5642/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210303074136-134d130e1a04/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211216021012-1d35b9e2eb4e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220114195835-da31bd327af9/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.13.0 h1:Af8nKPmuFypiUBjVoU9V20FiaFXOcuZI21p0ycVYYGE=\ngolang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.13.0 h1:ablQoSUd0tRdKxZewP80B+BaqeKJuVhuRxj/dkrun3k=\ngolang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\ngolang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200227222343-706bc42d1f0d/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200312045724-11d5b4c81c7d/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200331025713-a30bf2db82d4/go.mod h1:Sl4aGygMT6LrqrWclx+PTx3U+LnKx/seiNR+3G19Ar8=\ngolang.org/x/tools v0.0.0-20200501065659-ab2804fb9c9d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200512131952-2bc93b1c0c88/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200515010526-7d3b6ebf133d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200618134242-20370b0cb4b2/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200729194436-6467de6f59a7/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200804011535-6c149bb5ef0d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200825202427-b303f430e36d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\ngoogle.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\ngoogle.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.19.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.29.0/go.mod h1:Lcubydp8VUV7KeIHD9z2Bys/sm/vGKnG1UHuDBSrHWM=\ngoogle.golang.org/api v0.30.0/go.mod h1:QGmEvQ87FHZNiUVJkT14jQNYJ4ZJjdRF23ZXz5138Fc=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\ngoogle.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\ngoogle.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=\ngoogle.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200228133532-8c2c7df3a383/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200312145019-da6875a35672/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200331122359-1ee6d9798940/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200430143042-b979b6f78d84/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200511104702-f5ebc3bea380/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200515170657-fc4c6c6a6587/go.mod h1:YsZOwe1myG/8QRHRsmBRE1LrgQY60beZKjly0O1fX9U=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20200618031413-b414f8b61790/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\ngoogle.golang.org/genproto v0.0.0-20200729003335-053ba62fc06f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200804131852-c06518451d9c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200825200019-8632dd797987/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\ngoogle.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.28.0/go.mod h1:rpkK4SK4GF4Ach/+MFLZUBavHOvF2JJB5uozKKal+60=\ngoogle.golang.org/grpc v1.29.1/go.mod h1:itym6AZVZYACWQqET3MqgPpjcuV5QH3BxFS3IjizoKk=\ngoogle.golang.org/grpc v1.30.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.31.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=\ngoogle.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.28.1 h1:d0NfwRgPtno5B1Wa6L2DAG+KivqkdutMf1UhdNx175w=\ngoogle.golang.org/protobuf v1.28.1/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0 h1:1Lc07Kr7qY4U2YPouBjpCLxpiyxIVoxqXgkXLknAOE8=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0/go.mod h1:l0ndWWf7gzL7RNwBG7wST/UCcT4T24xpD6X8LsfU/+k=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\nhonnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nhonnef.co/go/tools v0.0.1-2020.1.4/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nrsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\nrsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\nrsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\n"
        },
        {
          "name": "http",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "lease.go",
          "type": "blob",
          "size": 4.9599609375,
          "content": "package litefs\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"time\"\n)\n\n// Leaser represents an API for obtaining a lease for leader election.\ntype Leaser interface {\n\tio.Closer\n\n\t// Type returns the name of the leaser.\n\tType() string\n\n\tHostname() string\n\tAdvertiseURL() string\n\n\t// Acquire attempts to acquire the lease to become the primary.\n\tAcquire(ctx context.Context) (Lease, error)\n\n\t// AcquireExisting returns a lease from an existing lease ID.\n\t// This occurs when the primary is handed off to a replica node.\n\tAcquireExisting(ctx context.Context, leaseID string) (Lease, error)\n\n\t// PrimaryInfo attempts to read the current primary data.\n\t// Returns ErrNoPrimary if no primary currently has the lease.\n\tPrimaryInfo(ctx context.Context) (PrimaryInfo, error)\n\n\t// ClusterID returns the cluster ID set on the leaser.\n\t// This is used to ensure two clusters do not accidentally overlap.\n\tClusterID(ctx context.Context) (string, error)\n\n\t// SetClusterID sets the cluster ID on the leaser.\n\tSetClusterID(ctx context.Context, clusterID string) error\n}\n\n// Lease represents an acquired lease from a Leaser.\ntype Lease interface {\n\tID() string\n\tRenewedAt() time.Time\n\tTTL() time.Duration\n\n\t// Renew attempts to reset the TTL on the lease.\n\t// Returns ErrLeaseExpired if the lease has expired or was deleted.\n\tRenew(ctx context.Context) error\n\n\t// Marks the lease as handed-off to another node.\n\t// This should send the nodeID to the channel returned by HandoffCh().\n\tHandoff(ctx context.Context, nodeID uint64) error\n\tHandoffCh() <-chan uint64\n\n\t// Close attempts to remove the lease from the server.\n\tClose() error\n}\n\n// PrimaryInfo is the JSON object stored in the Consul lease value.\ntype PrimaryInfo struct {\n\tHostname     string `json:\"hostname\"`\n\tAdvertiseURL string `json:\"advertise-url\"`\n}\n\n// Clone returns a copy of info.\nfunc (info *PrimaryInfo) Clone() *PrimaryInfo {\n\tif info == nil {\n\t\treturn nil\n\t}\n\tother := *info\n\treturn &other\n}\n\n// StaticLeaser always returns a lease to a static primary.\ntype StaticLeaser struct {\n\tisPrimary    bool\n\thostname     string\n\tadvertiseURL string\n}\n\n// NewStaticLeaser returns a new instance of StaticLeaser.\nfunc NewStaticLeaser(isPrimary bool, hostname, advertiseURL string) *StaticLeaser {\n\treturn &StaticLeaser{\n\t\tisPrimary:    isPrimary,\n\t\thostname:     hostname,\n\t\tadvertiseURL: advertiseURL,\n\t}\n}\n\n// Close is a no-op.\nfunc (l *StaticLeaser) Close() (err error) { return nil }\n\n// Type returns \"static\".\nfunc (l *StaticLeaser) Type() string { return \"static\" }\n\nfunc (l *StaticLeaser) Hostname() string {\n\treturn l.hostname\n}\n\n// AdvertiseURL returns the primary URL if this is the primary.\n// Otherwise returns blank.\nfunc (l *StaticLeaser) AdvertiseURL() string {\n\tif l.isPrimary {\n\t\treturn l.advertiseURL\n\t}\n\treturn \"\"\n}\n\n// Acquire returns a lease if this node is the static primary.\n// Otherwise returns ErrPrimaryExists.\nfunc (l *StaticLeaser) Acquire(ctx context.Context) (Lease, error) {\n\tif !l.isPrimary {\n\t\treturn nil, ErrPrimaryExists\n\t}\n\treturn &StaticLease{leaser: l}, nil\n}\n\n// AcquireExisting always returns an error. Static leasing does not support handoff.\nfunc (l *StaticLeaser) AcquireExisting(ctx context.Context, leaseID string) (Lease, error) {\n\treturn nil, fmt.Errorf(\"static lease handoff not supported\")\n}\n\n// PrimaryInfo returns the primary's info.\n// Returns ErrNoPrimary if the node is the primary.\nfunc (l *StaticLeaser) PrimaryInfo(ctx context.Context) (PrimaryInfo, error) {\n\tif l.isPrimary {\n\t\treturn PrimaryInfo{}, ErrNoPrimary\n\t}\n\treturn PrimaryInfo{\n\t\tHostname:     l.hostname,\n\t\tAdvertiseURL: l.advertiseURL,\n\t}, nil\n}\n\n// IsPrimary returns true if the current node is the primary.\nfunc (l *StaticLeaser) IsPrimary() bool {\n\treturn l.isPrimary\n}\n\n// ClusterID always returns a blank string for the static leaser.\nfunc (l *StaticLeaser) ClusterID(ctx context.Context) (string, error) {\n\treturn \"\", nil\n}\n\n// SetClusterID is always a no-op for the static leaser.\nfunc (l *StaticLeaser) SetClusterID(ctx context.Context, clusterID string) error {\n\treturn nil\n}\n\nvar _ Lease = (*StaticLease)(nil)\n\n// StaticLease represents a lease for a fixed primary.\ntype StaticLease struct {\n\tleaser *StaticLeaser\n}\n\n// ID always returns a blank string.\nfunc (l *StaticLease) ID() string { return \"\" }\n\n// RenewedAt returns the Unix epoch in UTC.\nfunc (l *StaticLease) RenewedAt() time.Time { return time.Unix(0, 0).UTC() }\n\n// TTL returns the duration until the lease expires which is a time well into the future.\nfunc (l *StaticLease) TTL() time.Duration { return staticLeaseExpiresAt.Sub(l.RenewedAt()) }\n\n// Renew is a no-op.\nfunc (l *StaticLease) Renew(ctx context.Context) error { return nil }\n\n// Handoff always returns an error.\nfunc (l *StaticLease) Handoff(ctx context.Context, nodeID uint64) error {\n\treturn fmt.Errorf(\"static lease does not support handoff\")\n}\n\n// HandoffCh always returns a nil channel.\nfunc (l *StaticLease) HandoffCh() <-chan uint64 { return nil }\n\nfunc (l *StaticLease) Close() error { return nil }\n\nvar staticLeaseExpiresAt = time.Date(3000, time.January, 1, 0, 0, 0, 0, time.UTC)\n"
        },
        {
          "name": "lease_test.go",
          "type": "blob",
          "size": 2.33984375,
          "content": "package litefs_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/superfly/litefs\"\n)\n\nfunc TestStaticLeaser(t *testing.T) {\n\tt.Run(\"Primary\", func(t *testing.T) {\n\t\tl := litefs.NewStaticLeaser(true, \"localhost\", \"http://localhost:20202\")\n\t\tif got, want := l.AdvertiseURL(), \"http://localhost:20202\"; got != want {\n\t\t\tt.Fatalf(\"got %q, want %q\", got, want)\n\t\t}\n\n\t\tif info, err := l.PrimaryInfo(context.Background()); err != litefs.ErrNoPrimary {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := info.Hostname, \"\"; got != want {\n\t\t\tt.Fatalf(\"Hostname=%q, want %q\", got, want)\n\t\t} else if got, want := info.AdvertiseURL, \"\"; got != want {\n\t\t\tt.Fatalf(\"AdvertiseURL=%q, want %q\", got, want)\n\t\t}\n\n\t\tif lease, err := l.Acquire(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if lease == nil {\n\t\t\tt.Fatal(\"expected lease\")\n\t\t}\n\n\t\tif err := l.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"Replica\", func(t *testing.T) {\n\t\tl := litefs.NewStaticLeaser(false, \"localhost\", \"http://localhost:20202\")\n\t\tif got, want := l.AdvertiseURL(), \"\"; got != want {\n\t\t\tt.Fatalf(\"got %q, want %q\", got, want)\n\t\t}\n\n\t\tif info, err := l.PrimaryInfo(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := info.Hostname, \"localhost\"; got != want {\n\t\t\tt.Fatalf(\"Hostname=%q, want %q\", got, want)\n\t\t} else if got, want := info.AdvertiseURL, \"http://localhost:20202\"; got != want {\n\t\t\tt.Fatalf(\"AdvertiseURL=%q, want %q\", got, want)\n\t\t}\n\n\t\tif lease, err := l.Acquire(context.Background()); err != litefs.ErrPrimaryExists {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t} else if lease != nil {\n\t\t\tt.Fatal(\"expected no lease\")\n\t\t}\n\t})\n}\n\nfunc TestStaticLease(t *testing.T) {\n\tleaser := litefs.NewStaticLeaser(true, \"localhost\", \"http://localhost:20202\")\n\tlease, err := leaser.Acquire(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif got, want := lease.RenewedAt().String(), `1970-01-01 00:00:00 +0000 UTC`; got != want {\n\t\tt.Fatalf(\"RenewedAt()=%q, want %q\", got, want)\n\t}\n\tif lease.TTL() <= 0 {\n\t\tt.Fatal(\"expected TTL\")\n\t}\n\n\tif err := lease.Renew(context.Background()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := lease.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// newPrimaryStaticLeaser returns a new instance of StaticLeaser for primary node testing.\nfunc newPrimaryStaticLeaser() *litefs.StaticLeaser {\n\treturn litefs.NewStaticLeaser(true, \"localhost\", \"http://localhost:20202\")\n}\n"
        },
        {
          "name": "lfsc",
          "type": "tree",
          "content": null
        },
        {
          "name": "litefs.go",
          "type": "blob",
          "size": 19.5146484375,
          "content": "package litefs\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/rand\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"golang.org/x/exp/slog\"\n)\n\nfunc init() {\n\tassert(unsafe.Sizeof(walIndexHdr{}) == 48, \"invalid walIndexHdr size\")\n\tassert(unsafe.Sizeof(walCkptInfo{}) == 40, \"invalid walCkptInfo size\")\n}\n\n// Global log level. Can be adjusted dynamically.\nvar LogLevel struct {\n\tsync.Mutex\n\tslog.LevelVar\n}\n\nconst (\n\t// ClusterIDLen is the length of a cluster ID.\n\tClusterIDLen = 20\n\n\t// ClusterIDPrefix is the prefix for every cluster ID.\n\tClusterIDPrefix = \"LFSC\"\n)\n\n// ErrInvalidClusterID is returned when a cluster ID is invalid.\nvar ErrInvalidClusterID = errors.New(\"invalid cluster id\")\n\n// GenerateClusterID returns a new, randomly-generated cluster ID.\nfunc GenerateClusterID() string {\n\tb := make([]byte, (ClusterIDLen-len(ClusterIDPrefix))/2)\n\tif _, err := io.ReadFull(rand.Reader, b); err != nil {\n\t\tpanic(err)\n\t}\n\treturn fmt.Sprintf(\"%s%X\", ClusterIDPrefix, b)\n}\n\n// ValidateClusterID returns nil if id is a valid cluster ID.\nfunc ValidateClusterID(id string) error {\n\tif len(id) != ClusterIDLen || !strings.HasPrefix(id, ClusterIDPrefix) {\n\t\treturn ErrInvalidClusterID\n\t}\n\treturn nil\n}\n\n// NodeInfo represents basic info about a node.\ntype NodeInfo struct {\n\tClusterID string `json:\"clusterID,omitempty\"` // cluster ID\n\tIsPrimary bool   `json:\"isPrimary\"`           // if true, node is currently primary\n\tCandidate bool   `json:\"candidate\"`           // if true, node is eligible to be primary\n\tPath      string `json:\"path\"`                // data directory\n\n\tPrimary struct {\n\t\tHostname string `json:\"hostname\"`\n\t} `json:\"primary\"`\n}\n\n// Environment represents an interface for interacting with the host environment.\ntype Environment interface {\n\t// Type returns the name of the environment type.\n\tType() string\n\n\t// SetPrimaryStatus sets marks the current node as the primary or not.\n\tSetPrimaryStatus(ctx context.Context, isPrimary bool)\n}\n\ntype nopEnvironment struct{}\n\nfunc (*nopEnvironment) Type() string { return \"\" }\n\nfunc (*nopEnvironment) SetPrimaryStatus(ctx context.Context, v bool) {}\n\n// NativeEndian is always set to little endian as that is the only endianness\n// used by supported platforms for LiteFS. This may be expanded in the future.\nvar NativeEndian = binary.LittleEndian\n\n// LiteFS errors\nvar (\n\tErrDatabaseNotFound = fmt.Errorf(\"database not found\")\n\tErrDatabaseExists   = fmt.Errorf(\"database already exists\")\n\n\tErrNoPrimary     = errors.New(\"no primary\")\n\tErrPrimaryExists = errors.New(\"primary exists\")\n\tErrNotEligible   = errors.New(\"not eligible to become primary\")\n\tErrLeaseExpired  = errors.New(\"lease expired\")\n\tErrNoHaltPrimary = errors.New(\"no remote halt needed on primary node\")\n\n\tErrReadOnlyReplica  = fmt.Errorf(\"read only replica\")\n\tErrDuplicateLTXFile = fmt.Errorf(\"duplicate ltx file\")\n)\n\n// SQLite constants\nconst (\n\tWALHeaderSize      = 32\n\tWALFrameHeaderSize = 24\n\tWALIndexHeaderSize = 136\n\tWALIndexBlockSize  = 32768\n)\n\n// SQLite rollback journal lock constants.\nconst (\n\tPENDING_BYTE  = 0x40000000\n\tRESERVED_BYTE = (PENDING_BYTE + 1)\n\tSHARED_FIRST  = (PENDING_BYTE + 2)\n\tSHARED_SIZE   = 510\n)\n\n// SQLite WAL lock constants.\nconst (\n\tWAL_WRITE_LOCK   = 120\n\tWAL_CKPT_LOCK    = 121\n\tWAL_RECOVER_LOCK = 122\n\tWAL_READ_LOCK0   = 123\n\tWAL_READ_LOCK1   = 124\n\tWAL_READ_LOCK2   = 125\n\tWAL_READ_LOCK3   = 126\n\tWAL_READ_LOCK4   = 127\n)\n\n// JournalMode represents a SQLite journal mode.\ntype JournalMode string\n\nconst (\n\tJournalModeDelete   = \"DELETE\"\n\tJournalModeTruncate = \"TRUNCATE\"\n\tJournalModePersist  = \"PERSIST\"\n\tJournalModeWAL      = \"WAL\"\n)\n\n// FileType represents a type of SQLite file.\ntype FileType int\n\n// Database file types.\nconst (\n\tFileTypeNone = FileType(iota)\n\tFileTypeDatabase\n\tFileTypeJournal\n\tFileTypeWAL\n\tFileTypeSHM\n\tFileTypePos\n\tFileTypeLock\n)\n\n// IsValid returns true if t is a valid file type.\nfunc (t FileType) IsValid() bool {\n\tswitch t {\n\tcase FileTypeDatabase, FileTypeJournal, FileTypeWAL, FileTypeSHM, FileTypePos, FileTypeLock:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// TraceLogFlags are the flags to be used with TraceLog.\nconst TraceLogFlags = log.LstdFlags | log.Lmicroseconds | log.LUTC\n\n// TraceLog is a log for low-level tracing.\nvar TraceLog = log.New(io.Discard, \"\", TraceLogFlags)\n\nvar ErrInvalidNodeID = errors.New(\"invalid node id\")\n\n// ParseNodeID parses a 16-character hex string into a node identifier.\nfunc ParseNodeID(s string) (uint64, error) {\n\tv, err := strconv.ParseUint(s, 16, 64)\n\tif err != nil {\n\t\treturn 0, ErrInvalidNodeID\n\t}\n\treturn v, nil\n}\n\n// FormatNodeID formats a node identifier as a 16-character uppercase hex string.\nfunc FormatNodeID(id uint64) string {\n\treturn fmt.Sprintf(\"%016X\", id)\n}\n\n// Invalidator is a callback for the store to use to invalidate the kernel page cache.\ntype Invalidator interface {\n\tInvalidateDB(db *DB) error\n\tInvalidateDBRange(db *DB, offset, size int64) error\n\tInvalidateSHM(db *DB) error\n\tInvalidatePos(db *DB) error\n\tInvalidateEntry(name string) error\n\tInvalidateLag() error\n}\n\nfunc assert(condition bool, msg string) {\n\tif !condition {\n\t\tpanic(\"assertion failed: \" + msg)\n\t}\n}\n\n// WALReader wraps an io.Reader and parses SQLite WAL frames.\n//\n// This reader verifies the salt & checksum integrity while it reads. It does\n// not enforce transaction boundaries (i.e. it may return uncommitted frames).\n// It is the responsibility of the caller to handle this.\ntype WALReader struct {\n\tr      io.Reader\n\tframeN int\n\n\tbo       binary.ByteOrder\n\tpageSize uint32\n\tseq      uint32\n\n\tsalt1, salt2     uint32\n\tchksum1, chksum2 uint32\n}\n\n// NewWALReader returns a new instance of WALReader.\nfunc NewWALReader(r io.Reader) *WALReader {\n\treturn &WALReader{r: r}\n}\n\n// PageSize returns the page size from the header. Must call ReadHeader() first.\nfunc (r *WALReader) PageSize() uint32 { return r.pageSize }\n\n// Offset returns the file offset of the last read frame.\n// Returns zero if no frames have been read.\nfunc (r *WALReader) Offset() int64 {\n\tif r.frameN == 0 {\n\t\treturn 0\n\t}\n\treturn WALHeaderSize + ((int64(r.frameN) - 1) * (WALFrameHeaderSize + int64(r.pageSize)))\n}\n\n// ReadHeader reads the WAL header into the reader. Returns io.EOF if WAL is invalid.\nfunc (r *WALReader) ReadHeader() error {\n\t// If we have a partial WAL, then mark WAL as done.\n\thdr := make([]byte, WALHeaderSize)\n\tif _, err := io.ReadFull(r.r, hdr); err == io.ErrUnexpectedEOF {\n\t\treturn io.EOF\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\t// Determine byte order of checksums.\n\tswitch magic := binary.BigEndian.Uint32(hdr[0:]); magic {\n\tcase 0x377f0682:\n\t\tr.bo = binary.LittleEndian\n\tcase 0x377f0683:\n\t\tr.bo = binary.BigEndian\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid wal header magic: %x\", magic)\n\t}\n\n\t// If the header checksum doesn't match then we may have failed with\n\t// a partial WAL header write during checkpointing.\n\tchksum1 := binary.BigEndian.Uint32(hdr[24:])\n\tchksum2 := binary.BigEndian.Uint32(hdr[28:])\n\tif v0, v1 := WALChecksum(r.bo, 0, 0, hdr[:24]); v0 != chksum1 || v1 != chksum2 {\n\t\treturn io.EOF\n\t}\n\n\t// Verify version is correct.\n\tif version := binary.BigEndian.Uint32(hdr[4:]); version != 3007000 {\n\t\treturn fmt.Errorf(\"unsupported wal version: %d\", version)\n\t}\n\n\tr.pageSize = binary.BigEndian.Uint32(hdr[8:])\n\tr.seq = binary.BigEndian.Uint32(hdr[12:])\n\tr.salt1 = binary.BigEndian.Uint32(hdr[16:])\n\tr.salt2 = binary.BigEndian.Uint32(hdr[20:])\n\tr.chksum1, r.chksum2 = chksum1, chksum2\n\n\treturn nil\n}\n\n// ReadFrame reads the next frame from the WAL and returns the page number.\n// Returns io.EOF at the end of the valid WAL.\nfunc (r *WALReader) ReadFrame(data []byte) (pgno, commit uint32, err error) {\n\tif len(data) != int(r.pageSize) {\n\t\treturn 0, 0, fmt.Errorf(\"WALReader.ReadFrame(): buffer size (%d) must match page size (%d)\", len(data), r.pageSize)\n\t}\n\n\t// Read WAL frame header.\n\thdr := make([]byte, WALFrameHeaderSize)\n\tif _, err := io.ReadFull(r.r, hdr); err == io.ErrUnexpectedEOF {\n\t\treturn 0, 0, io.EOF\n\t} else if err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Read WAL page data.\n\tif _, err := io.ReadFull(r.r, data); err == io.ErrUnexpectedEOF {\n\t\treturn 0, 0, io.EOF\n\t} else if err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Verify salt matches the salt in the header.\n\tsalt1 := binary.BigEndian.Uint32(hdr[8:])\n\tsalt2 := binary.BigEndian.Uint32(hdr[12:])\n\tif r.salt1 != salt1 || r.salt2 != salt2 {\n\t\treturn 0, 0, io.EOF\n\t}\n\n\t// Verify the checksum is valid.\n\tchksum1 := binary.BigEndian.Uint32(hdr[16:])\n\tchksum2 := binary.BigEndian.Uint32(hdr[20:])\n\tr.chksum1, r.chksum2 = WALChecksum(r.bo, r.chksum1, r.chksum2, hdr[:8]) // frame header\n\tr.chksum1, r.chksum2 = WALChecksum(r.bo, r.chksum1, r.chksum2, data)    // frame data\n\tif r.chksum1 != chksum1 || r.chksum2 != chksum2 {\n\t\treturn 0, 0, io.EOF\n\t}\n\n\tpgno = binary.BigEndian.Uint32(hdr[0:])\n\tcommit = binary.BigEndian.Uint32(hdr[4:])\n\n\tr.frameN++\n\n\treturn pgno, commit, nil\n}\n\n// WALChecksum computes a running SQLite WAL checksum over a byte slice.\nfunc WALChecksum(bo binary.ByteOrder, s0, s1 uint32, b []byte) (uint32, uint32) {\n\tassert(len(b)%8 == 0, \"misaligned checksum byte slice\")\n\n\t// Iterate over 8-byte units and compute checksum.\n\tfor i := 0; i < len(b); i += 8 {\n\t\ts0 += bo.Uint32(b[i:]) + s1\n\t\ts1 += bo.Uint32(b[i+4:]) + s0\n\t}\n\treturn s0, s1\n}\n\n// JournalChecksum returns the checksum used by the journal format.\nfunc JournalChecksum(data []byte, initial uint32) uint32 {\n\tchksum := initial\n\tfor i := len(data) - 200; i > 0; i -= 200 {\n\t\tchksum += uint32(data[i])\n\t}\n\treturn chksum\n}\n\nconst (\n\tSQLITE_DATABASE_HEADER_STRING = \"SQLite format 3\\x00\"\n\n\t// Location of the database size, in pages, in the main database file.\n\tSQLITE_DATABASE_SIZE_OFFSET = 28\n\n\t/// Magic header string that identifies a SQLite journal header.\n\t/// https://www.sqlite.org/fileformat.html#the_rollback_journal\n\tSQLITE_JOURNAL_HEADER_STRING = \"\\xd9\\xd5\\x05\\xf9\\x20\\xa1\\x63\\xd7\"\n\n\t// Size of the journal header, in bytes.\n\tSQLITE_JOURNAL_HEADER_SIZE = 28\n)\n\n// LockType represents a SQLite lock type.\ntype LockType int\n\n// String returns the name of the lock type.\nfunc (t LockType) String() string {\n\tswitch t {\n\tcase LockTypeHalt:\n\t\treturn \"HALT\"\n\tcase LockTypePending:\n\t\treturn \"PENDING\"\n\tcase LockTypeReserved:\n\t\treturn \"RESERVED\"\n\tcase LockTypeShared:\n\t\treturn \"SHARED\"\n\tcase LockTypeWrite:\n\t\treturn \"WRITE\"\n\tcase LockTypeCkpt:\n\t\treturn \"CKPT\"\n\tcase LockTypeRecover:\n\t\treturn \"RECOVER\"\n\tcase LockTypeRead0:\n\t\treturn \"READ0\"\n\tcase LockTypeRead1:\n\t\treturn \"READ1\"\n\tcase LockTypeRead2:\n\t\treturn \"READ2\"\n\tcase LockTypeRead3:\n\t\treturn \"READ3\"\n\tcase LockTypeRead4:\n\t\treturn \"READ4\"\n\tcase LockTypeDMS:\n\t\treturn \"DMS\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"UNKNOWN<%d>\", t)\n\t}\n}\n\nconst (\n\t// Database file locks\n\tLockTypeHalt     = LockType(72)         // LiteFS-specific lock byte\n\tLockTypePending  = LockType(0x40000000) // 1073741824\n\tLockTypeReserved = LockType(0x40000001) // 1073741825\n\tLockTypeShared   = LockType(0x40000002) // 1073741826\n\n\t// SHM file locks\n\tLockTypeWrite   = LockType(120)\n\tLockTypeCkpt    = LockType(121)\n\tLockTypeRecover = LockType(122)\n\tLockTypeRead0   = LockType(123)\n\tLockTypeRead1   = LockType(124)\n\tLockTypeRead2   = LockType(125)\n\tLockTypeRead3   = LockType(126)\n\tLockTypeRead4   = LockType(127)\n\tLockTypeDMS     = LockType(128)\n)\n\n// ContainsLockType returns true if a contains typ.\nfunc ContainsLockType(a []LockType, typ LockType) bool {\n\tfor _, v := range a {\n\t\tif v == typ {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// ParseDatabaseLockRange returns a list of SQLite database locks that are within a range.\n//\n// This does not include the HALT lock as that is specific to LiteFS and we don't\n// want to accidentally include it when locking/unlocking the whole file.\nfunc ParseDatabaseLockRange(start, end uint64) []LockType {\n\ta := make([]LockType, 0, 3)\n\tif start <= uint64(LockTypePending) && uint64(LockTypePending) <= end {\n\t\ta = append(a, LockTypePending)\n\t}\n\tif start <= uint64(LockTypeReserved) && uint64(LockTypeReserved) <= end {\n\t\ta = append(a, LockTypeReserved)\n\t}\n\tif start <= uint64(LockTypeShared) && uint64(LockTypeShared) <= end {\n\t\ta = append(a, LockTypeShared)\n\t}\n\treturn a\n}\n\n// ParseSHMLockRange returns a list of SQLite WAL locks that are within a range.\nfunc ParseSHMLockRange(start, end uint64) []LockType {\n\ta := make([]LockType, 0, 3)\n\tif start <= uint64(LockTypeWrite) && uint64(LockTypeWrite) <= end {\n\t\ta = append(a, LockTypeWrite)\n\t}\n\tif start <= uint64(LockTypeCkpt) && uint64(LockTypeCkpt) <= end {\n\t\ta = append(a, LockTypeCkpt)\n\t}\n\tif start <= uint64(LockTypeRecover) && uint64(LockTypeRecover) <= end {\n\t\ta = append(a, LockTypeRecover)\n\t}\n\n\tif start <= uint64(LockTypeRead0) && uint64(LockTypeRead0) <= end {\n\t\ta = append(a, LockTypeRead0)\n\t}\n\tif start <= uint64(LockTypeRead1) && uint64(LockTypeRead1) <= end {\n\t\ta = append(a, LockTypeRead1)\n\t}\n\tif start <= uint64(LockTypeRead2) && uint64(LockTypeRead2) <= end {\n\t\ta = append(a, LockTypeRead2)\n\t}\n\tif start <= uint64(LockTypeRead3) && uint64(LockTypeRead3) <= end {\n\t\ta = append(a, LockTypeRead3)\n\t}\n\tif start <= uint64(LockTypeRead4) && uint64(LockTypeRead4) <= end {\n\t\ta = append(a, LockTypeRead4)\n\t}\n\tif start <= uint64(LockTypeDMS) && uint64(LockTypeDMS) <= end {\n\t\ta = append(a, LockTypeDMS)\n\t}\n\n\treturn a\n}\n\n// GuardSet represents a set of mutex guards by a single owner.\ntype GuardSet struct {\n\towner uint64\n\n\t// Database file locks\n\tpending  RWMutexGuard\n\tshared   RWMutexGuard\n\treserved RWMutexGuard\n\n\t// SHM file locks\n\twrite   RWMutexGuard\n\tckpt    RWMutexGuard\n\trecover RWMutexGuard\n\tread0   RWMutexGuard\n\tread1   RWMutexGuard\n\tread2   RWMutexGuard\n\tread3   RWMutexGuard\n\tread4   RWMutexGuard\n\tdms     RWMutexGuard\n}\n\n// Pending returns a reference to the PENDING mutex guard.\nfunc (s *GuardSet) Pending() *RWMutexGuard { return &s.pending }\n\n// Shared returns a reference to the SHARED mutex guard.\nfunc (s *GuardSet) Shared() *RWMutexGuard { return &s.shared }\n\n// Reserved returns a reference to the RESERVED mutex guard.\nfunc (s *GuardSet) Reserved() *RWMutexGuard { return &s.reserved }\n\n// Write returns a reference to the WRITE mutex guard.\nfunc (s *GuardSet) Write() *RWMutexGuard { return &s.write }\n\n// Ckpt returns a reference to the CKPT mutex guard.\nfunc (s *GuardSet) Ckpt() *RWMutexGuard { return &s.ckpt }\n\n// Recover returns a reference to the RECOVER mutex guard.\nfunc (s *GuardSet) Recover() *RWMutexGuard { return &s.recover }\n\n// Read0 returns a reference to the READ0 mutex guard.\nfunc (s *GuardSet) Read0() *RWMutexGuard { return &s.read0 }\n\n// Read1 returns a reference to the READ1 mutex guard.\nfunc (s *GuardSet) Read1() *RWMutexGuard { return &s.read1 }\n\n// Read2 returns a reference to the READ2 mutex guard.\nfunc (s *GuardSet) Read2() *RWMutexGuard { return &s.read2 }\n\n// Read3 returns a reference to the READ3 mutex guard.\nfunc (s *GuardSet) Read3() *RWMutexGuard { return &s.read3 }\n\n// Read4 returns a reference to the READ4 mutex guard.\nfunc (s *GuardSet) Read4() *RWMutexGuard { return &s.read4 }\n\n// DMS returns a reference to the DMS mutex guard.\nfunc (s *GuardSet) DMS() *RWMutexGuard { return &s.dms }\n\n// Guard returns a guard by lock type. Panic on invalid lock type.\nfunc (s *GuardSet) Guard(lockType LockType) *RWMutexGuard {\n\tswitch lockType {\n\n\t// Database locks\n\tcase LockTypePending:\n\t\treturn &s.pending\n\tcase LockTypeShared:\n\t\treturn &s.shared\n\tcase LockTypeReserved:\n\t\treturn &s.reserved\n\n\t// SHM file locks\n\tcase LockTypeWrite:\n\t\treturn &s.write\n\tcase LockTypeCkpt:\n\t\treturn &s.ckpt\n\tcase LockTypeRecover:\n\t\treturn &s.recover\n\tcase LockTypeRead0:\n\t\treturn &s.read0\n\tcase LockTypeRead1:\n\t\treturn &s.read1\n\tcase LockTypeRead2:\n\t\treturn &s.read2\n\tcase LockTypeRead3:\n\t\treturn &s.read3\n\tcase LockTypeRead4:\n\t\treturn &s.read4\n\tcase LockTypeDMS:\n\t\treturn &s.dms\n\n\tdefault:\n\t\tpanic(\"GuardSet.Guard(): invalid database lock type\")\n\t}\n}\n\n// Unlock unlocks all the guards in reversed order that they are acquired by SQLite.\nfunc (s *GuardSet) Unlock() {\n\ts.UnlockDatabase()\n\ts.UnlockSHM()\n}\n\n// UnlockDatabase unlocks all the database file guards.\nfunc (s *GuardSet) UnlockDatabase() {\n\ts.pending.Unlock()\n\ts.shared.Unlock()\n\ts.reserved.Unlock()\n}\n\n// UnlockSHM unlocks all the SHM file guards.\nfunc (s *GuardSet) UnlockSHM() {\n\ts.write.Unlock()\n\ts.ckpt.Unlock()\n\ts.recover.Unlock()\n\ts.read0.Unlock()\n\ts.read1.Unlock()\n\ts.read2.Unlock()\n\ts.read3.Unlock()\n\ts.read4.Unlock()\n\ts.dms.Unlock()\n}\n\n// SQLite constants\nconst (\n\tdatabaseHeaderSize = 100\n)\n\ntype sqliteDatabaseHeader struct {\n\tWriteVersion int\n\tReadVersion  int\n\tPageSize     uint32\n\tPageN        uint32\n}\n\nvar errInvalidDatabaseHeader = errors.New(\"invalid database header\")\n\n// readSQLiteDatabaseHeader reads specific fields from the header of a SQLite database file.\n// Returns errInvalidDatabaseHeader if the database file is too short or if the\n// file has invalid magic.\nfunc readSQLiteDatabaseHeader(r io.Reader) (hdr sqliteDatabaseHeader, data []byte, err error) {\n\tb := make([]byte, databaseHeaderSize)\n\tif n, err := io.ReadFull(r, b); err == io.ErrUnexpectedEOF {\n\t\treturn hdr, b[:n], errInvalidDatabaseHeader // short file\n\t} else if err != nil {\n\t\treturn hdr, b[:n], err // empty file (EOF) or other errors\n\t} else if !bytes.Equal(b[:len(SQLITE_DATABASE_HEADER_STRING)], []byte(SQLITE_DATABASE_HEADER_STRING)) {\n\t\treturn hdr, b[:n], errInvalidDatabaseHeader // invalid magic\n\t}\n\n\thdr.WriteVersion = int(b[18])\n\thdr.ReadVersion = int(b[19])\n\thdr.PageSize = uint32(binary.BigEndian.Uint16(b[16:]))\n\thdr.PageN = binary.BigEndian.Uint32(b[28:])\n\n\t// SQLite page size has a special value for 64K pages.\n\tif hdr.PageSize == 1 {\n\t\thdr.PageSize = 65536\n\t}\n\n\treturn hdr, b, nil\n}\n\n// encodePageSize returns sz as a uint16. If sz is 64K, it returns 1.\nfunc encodePageSize(sz uint32) uint16 {\n\tif sz == 65536 {\n\t\treturn 1\n\t}\n\treturn uint16(sz)\n}\n\n// DBMode represents either a rollback journal or WAL mode.\ntype DBMode int\n\n// String returns the string representation of m.\nfunc (m DBMode) String() string {\n\tswitch m {\n\tcase DBModeRollback:\n\t\treturn \"ROLLBACK_MODE\"\n\tcase DBModeWAL:\n\t\treturn \"WAL_MODE\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"DBMode<%d>\", m)\n\t}\n}\n\n// Database journal modes.\nconst (\n\tDBModeRollback = DBMode(0)\n\tDBModeWAL      = DBMode(1)\n)\n\n// walIndexHdr is copied from wal.c\ntype walIndexHdr struct {\n\tversion     uint32    // Wal-index version\n\tunused      uint32    // Unused (padding) field\n\tchange      uint32    // Counter incremented each transaction\n\tisInit      uint8     // 1 when initialized\n\tbigEndCksum uint8     // True if checksums in WAL are big-endian\n\tpageSize    uint16    // Database page size in bytes. 1==64K\n\tmxFrame     uint32    // Index of last valid frame in the WAL\n\tpageN       uint32    // Size of database in pages\n\tframeCksum  [2]uint32 // Checksum of last frame in log\n\tsalt        [2]uint32 // Two salt values copied from WAL header\n\tcksum       [2]uint32 // Checksum over all prior fields\n}\n\n// walCkptInfo is copied from wal.c\ntype walCkptInfo struct {\n\tbackfill          uint32    // Number of WAL frames backfilled into DB\n\treadMark          [5]uint32 // Reader marks\n\tlock              [8]uint8  // Reserved space for locks\n\tbackfillAttempted uint32    // WAL frames perhaps written, or maybe not\n\tnotUsed0          uint32    // Available for future enhancements\n}\n\n// OS represents an interface for os package calls so they can be mocked for testing.\ntype OS interface {\n\tCreate(op, name string) (*os.File, error)\n\tMkdir(op, path string, perm os.FileMode) error\n\tMkdirAll(op, path string, perm os.FileMode) error\n\tOpen(op, name string) (*os.File, error)\n\tOpenFile(op, opname string, flag int, perm os.FileMode) (*os.File, error)\n\tReadDir(op, opname string) ([]os.DirEntry, error)\n\tReadFile(op, name string) ([]byte, error)\n\tRemove(op, name string) error\n\tRemoveAll(op, name string) error\n\tRename(op, oldpath, newpath string) error\n\tStat(op, name string) (os.FileInfo, error)\n\tTruncate(op, name string, size int64) error\n\tWriteFile(op, name string, data []byte, perm os.FileMode) error\n}\n"
        },
        {
          "name": "litefs_test.go",
          "type": "blob",
          "size": 8.1708984375,
          "content": "package litefs_test\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/superfly/litefs\"\n)\n\nfunc TestFileType_IsValid(t *testing.T) {\n\tt.Run(\"Valid\", func(t *testing.T) {\n\t\tfor _, typ := range []litefs.FileType{\n\t\t\tlitefs.FileTypeDatabase,\n\t\t\tlitefs.FileTypeJournal,\n\t\t\tlitefs.FileTypeWAL,\n\t\t\tlitefs.FileTypeSHM,\n\t\t} {\n\t\t\tif !typ.IsValid() {\n\t\t\t\tt.Fatalf(\"expected valid for %d\", typ)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"Invalid\", func(t *testing.T) {\n\t\tif litefs.FileType(100).IsValid() {\n\t\t\tt.Fatalf(\"expected invalid\")\n\t\t}\n\t})\n\tt.Run(\"None\", func(t *testing.T) {\n\t\tif litefs.FileTypeNone.IsValid() {\n\t\t\tt.Fatalf(\"expected invalid\")\n\t\t}\n\t})\n}\n\nfunc TestWALReader(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tbuf := make([]byte, 4096)\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/ok/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Initialize reader with header info.\n\t\tr := litefs.NewWALReader(bytes.NewReader(b))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := r.PageSize(), uint32(4096); got != want {\n\t\t\tt.Fatalf(\"PageSize()=%d, want %d\", got, want)\n\t\t} else if got, want := r.Offset(), int64(0); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\t// Read first frame.\n\t\tif pgno, commit, err := r.ReadFrame(buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pgno, uint32(1); got != want {\n\t\t\tt.Fatalf(\"pgno=%d, want %d\", got, want)\n\t\t} else if got, want := commit, uint32(0); got != want {\n\t\t\tt.Fatalf(\"commit=%d, want %d\", got, want)\n\t\t} else if !bytes.Equal(buf, b[56:4152]) {\n\t\t\tt.Fatal(\"page data mismatch\")\n\t\t} else if got, want := r.Offset(), int64(32); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\t// Read second frame. End of transaction.\n\t\tif pgno, commit, err := r.ReadFrame(buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pgno, uint32(2); got != want {\n\t\t\tt.Fatalf(\"pgno=%d, want %d\", got, want)\n\t\t} else if got, want := commit, uint32(2); got != want {\n\t\t\tt.Fatalf(\"commit=%d, want %d\", got, want)\n\t\t} else if !bytes.Equal(buf, b[4176:8272]) {\n\t\t\tt.Fatal(\"page data mismatch\")\n\t\t} else if got, want := r.Offset(), int64(4152); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\t// Read third frame.\n\t\tif pgno, commit, err := r.ReadFrame(buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pgno, uint32(2); got != want {\n\t\t\tt.Fatalf(\"pgno=%d, want %d\", got, want)\n\t\t} else if got, want := commit, uint32(2); got != want {\n\t\t\tt.Fatalf(\"commit=%d, want %d\", got, want)\n\t\t} else if !bytes.Equal(buf, b[8296:12392]) {\n\t\t\tt.Fatal(\"page data mismatch\")\n\t\t} else if got, want := r.Offset(), int64(8272); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\tif _, _, err := r.ReadFrame(buf); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"SaltMismatch\", func(t *testing.T) {\n\t\tbuf := make([]byte, 4096)\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/salt-mismatch/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Initialize reader with header info.\n\t\tr := litefs.NewWALReader(bytes.NewReader(b))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := r.PageSize(), uint32(4096); got != want {\n\t\t\tt.Fatalf(\"PageSize()=%d, want %d\", got, want)\n\t\t} else if got, want := r.Offset(), int64(0); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\t// Read first frame.\n\t\tif pgno, commit, err := r.ReadFrame(buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pgno, uint32(1); got != want {\n\t\t\tt.Fatalf(\"pgno=%d, want %d\", got, want)\n\t\t} else if got, want := commit, uint32(0); got != want {\n\t\t\tt.Fatalf(\"commit=%d, want %d\", got, want)\n\t\t} else if !bytes.Equal(buf, b[56:4152]) {\n\t\t\tt.Fatal(\"page data mismatch\")\n\t\t}\n\n\t\t// Read second frame. Salt has been altered so it doesn't match header.\n\t\tif _, _, err := r.ReadFrame(buf); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"FrameChecksumMismatch\", func(t *testing.T) {\n\t\tbuf := make([]byte, 4096)\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/frame-checksum-mismatch/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Initialize reader with header info.\n\t\tr := litefs.NewWALReader(bytes.NewReader(b))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := r.PageSize(), uint32(4096); got != want {\n\t\t\tt.Fatalf(\"PageSize()=%d, want %d\", got, want)\n\t\t} else if got, want := r.Offset(), int64(0); got != want {\n\t\t\tt.Fatalf(\"Offset()=%d, want %d\", got, want)\n\t\t}\n\n\t\t// Read first frame.\n\t\tif pgno, commit, err := r.ReadFrame(buf); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := pgno, uint32(1); got != want {\n\t\t\tt.Fatalf(\"pgno=%d, want %d\", got, want)\n\t\t} else if got, want := commit, uint32(0); got != want {\n\t\t\tt.Fatalf(\"commit=%d, want %d\", got, want)\n\t\t} else if !bytes.Equal(buf, b[56:4152]) {\n\t\t\tt.Fatal(\"page data mismatch\")\n\t\t}\n\n\t\t// Read second frame. Checksum has been altered so it doesn't match.\n\t\tif _, _, err := r.ReadFrame(buf); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ZeroLength\", func(t *testing.T) {\n\t\tr := litefs.NewWALReader(bytes.NewReader(nil))\n\t\tif err := r.ReadHeader(); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"PartialHeader\", func(t *testing.T) {\n\t\tr := litefs.NewWALReader(bytes.NewReader(make([]byte, 10)))\n\t\tif err := r.ReadHeader(); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"BadMagic\", func(t *testing.T) {\n\t\tr := litefs.NewWALReader(bytes.NewReader(make([]byte, 32)))\n\t\tif err := r.ReadHeader(); err == nil || err.Error() != `invalid wal header magic: 0` {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"BadHeaderChecksum\", func(t *testing.T) {\n\t\tdata := []byte{\n\t\t\t0x37, 0x7f, 0x06, 0x83, 0x00, 0x00, 0x00, 0x00,\n\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00}\n\t\tr := litefs.NewWALReader(bytes.NewReader(data))\n\t\tif err := r.ReadHeader(); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"BadHeaderVersion\", func(t *testing.T) {\n\t\tdata := []byte{\n\t\t\t0x37, 0x7f, 0x06, 0x83, 0x00, 0x00, 0x00, 0x01,\n\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n\t\t\t0x15, 0x7b, 0x20, 0x92, 0xbb, 0xf8, 0x34, 0x1d}\n\t\tr := litefs.NewWALReader(bytes.NewReader(data))\n\t\tif err := r.ReadHeader(); err == nil || err.Error() != `unsupported wal version: 1` {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrBufferSize\", func(t *testing.T) {\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/ok/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Initialize reader with header info.\n\t\tr := litefs.NewWALReader(bytes.NewReader(b))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif _, _, err := r.ReadFrame(make([]byte, 512)); err == nil || err.Error() != `WALReader.ReadFrame(): buffer size (512) must match page size (4096)` {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrPartialFrameHeader\", func(t *testing.T) {\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/ok/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tr := litefs.NewWALReader(bytes.NewReader(b[:40]))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, _, err := r.ReadFrame(make([]byte, 4096)); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrFrameHeaderOnly\", func(t *testing.T) {\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/ok/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tr := litefs.NewWALReader(bytes.NewReader(b[:56]))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, _, err := r.ReadFrame(make([]byte, 4096)); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrPartialFrameData\", func(t *testing.T) {\n\t\tb, err := os.ReadFile(\"testdata/wal-reader/ok/wal\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tr := litefs.NewWALReader(bytes.NewReader(b[:1000]))\n\t\tif err := r.ReadHeader(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, _, err := r.ReadFrame(make([]byte, 4096)); err != io.EOF {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n}\n\ntype errWriter struct{ afterN int }\n\nfunc (w *errWriter) Write(p []byte) (int, error) {\n\tif w.afterN -= len(p); w.afterN <= 0 {\n\t\treturn 0, fmt.Errorf(\"write error occurred\")\n\t}\n\treturn len(p), nil\n}\n"
        },
        {
          "name": "mock",
          "type": "tree",
          "content": null
        },
        {
          "name": "rwmutex.go",
          "type": "blob",
          "size": 6.669921875,
          "content": "package litefs\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\n// RWMutexInterval is the time between reattempting lock acquisition.\nconst RWMutexInterval = 10 * time.Microsecond\n\n// RWMutex is a reader/writer mutual exclusion lock. It wraps the sync package\n// to provide additional capabilities such as lock upgrades & downgrades. It\n// only supports TryLock() & TryRLock() as that is what's supported by our\n// FUSE file system.\ntype RWMutex struct {\n\tmu      sync.Mutex\n\tsharedN int           // number of readers\n\texcl    *RWMutexGuard // exclusive lock holder\n\n\t// If set, this function is called when the state transitions.\n\t// Must be set before use of the mutex or its guards.\n\tOnLockStateChange func(prevState, newState RWMutexState)\n}\n\n// Guard returns an unlocked guard for the mutex.\nfunc (rw *RWMutex) Guard() RWMutexGuard {\n\treturn RWMutexGuard{rw: rw, state: RWMutexStateUnlocked}\n}\n\n// State returns whether the mutex has a exclusive lock, one or more shared\n// locks, or if the mutex is unlocked.\nfunc (rw *RWMutex) State() RWMutexState {\n\trw.mu.Lock()\n\tdefer rw.mu.Unlock()\n\treturn rw.state()\n}\n\nfunc (rw *RWMutex) state() RWMutexState {\n\tif rw.excl != nil {\n\t\treturn RWMutexStateExclusive\n\t} else if rw.sharedN > 0 {\n\t\treturn RWMutexStateShared\n\t}\n\treturn RWMutexStateUnlocked\n}\n\n// RWMutexGuard is a reference to a mutex. Locking, unlocking, upgrading, &\n// downgrading operations are all performed via the guard instead of directly\n// on the RWMutex itself as this works similarly to how POSIX locks work.\ntype RWMutexGuard struct {\n\trw    *RWMutex\n\tstate RWMutexState\n}\n\n// State returns the current state of the guard.\nfunc (g *RWMutexGuard) State() RWMutexState {\n\tg.rw.mu.Lock()\n\tdefer g.rw.mu.Unlock()\n\treturn g.state\n}\n\n// Lock attempts to obtain a exclusive lock for the guard. Returns an error if ctx is done.\nfunc (g *RWMutexGuard) Lock(ctx context.Context) error {\n\tif g.TryLock() {\n\t\treturn nil\n\t}\n\n\tticker := time.NewTicker(RWMutexInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn context.Cause(ctx)\n\t\tcase <-ticker.C:\n\t\t\tif g.TryLock() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n}\n\n// TryLock upgrades the lock from a shared lock to an exclusive lock.\n// This is a no-op if the lock is already an exclusive lock. This function will\n// trigger OnLockStateChange on the mutex, if set, and if state changes.\nfunc (g *RWMutexGuard) TryLock() bool {\n\tg.rw.mu.Lock()\n\tprevState := g.rw.state()\n\tv := g.tryLock()\n\tfn, newState := g.rw.OnLockStateChange, g.rw.state()\n\tg.rw.mu.Unlock()\n\n\tif fn != nil && prevState != newState {\n\t\tfn(prevState, newState)\n\t}\n\treturn v\n}\n\nfunc (g *RWMutexGuard) tryLock() bool {\n\tswitch g.state {\n\tcase RWMutexStateUnlocked:\n\t\tif g.rw.sharedN != 0 || g.rw.excl != nil {\n\t\t\treturn false\n\t\t}\n\t\tg.rw.sharedN, g.rw.excl = 0, g\n\t\tg.state = RWMutexStateExclusive\n\t\treturn true\n\n\tcase RWMutexStateShared:\n\t\tassert(g.rw.excl == nil, \"exclusive lock already held while upgrading shared lock\")\n\t\tif g.rw.sharedN > 1 {\n\t\t\treturn false // another shared lock is being held\n\t\t}\n\n\t\tassert(g.rw.sharedN == 1, \"invalid shared lock count on guard upgrade\")\n\t\tg.rw.sharedN, g.rw.excl = 0, g\n\t\tg.state = RWMutexStateExclusive\n\t\treturn true\n\n\tcase RWMutexStateExclusive:\n\t\treturn true // no-op\n\n\tdefault:\n\t\tpanic(\"RWMutexGuard.TryLock(): unreachable\")\n\t}\n}\n\n// CanLock returns true if the guard can become an exclusive lock.\n// Also returns the current state of the underlying mutex to determine if the\n// lock is blocked by a shared or exclusive lock.\nfunc (g *RWMutexGuard) CanLock() (canLock bool, mutexState RWMutexState) {\n\tg.rw.mu.Lock()\n\tdefer g.rw.mu.Unlock()\n\n\tswitch g.state {\n\tcase RWMutexStateUnlocked:\n\t\treturn g.rw.sharedN == 0 && g.rw.excl == nil, g.rw.state()\n\tcase RWMutexStateShared:\n\t\treturn g.rw.sharedN == 1, g.rw.state()\n\tcase RWMutexStateExclusive:\n\t\treturn true, g.rw.state()\n\tdefault:\n\t\tpanic(\"RWMutexGuard.CanLock(): unreachable\")\n\t}\n}\n\n// RLock attempts to obtain a shared lock for the guard. Returns an error if ctx is done.\nfunc (g *RWMutexGuard) RLock(ctx context.Context) error {\n\tif g.TryRLock() {\n\t\treturn nil\n\t}\n\n\tticker := time.NewTicker(RWMutexInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn context.Cause(ctx)\n\t\tcase <-ticker.C:\n\t\t\tif g.TryRLock() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n}\n\n// TryRLock attempts to obtain a shared lock on the mutex for the guard. This will upgrade\n// an unlocked guard and downgrade an exclusive guard. Shared guards are a no-op.\nfunc (g *RWMutexGuard) TryRLock() bool {\n\tg.rw.mu.Lock()\n\tprevState := g.rw.state()\n\tv := g.tryRLock()\n\tfn, newState := g.rw.OnLockStateChange, g.rw.state()\n\tg.rw.mu.Unlock()\n\n\tif fn != nil && prevState != newState {\n\t\tfn(prevState, newState)\n\t}\n\treturn v\n}\n\nfunc (g *RWMutexGuard) tryRLock() bool {\n\tswitch g.state {\n\tcase RWMutexStateUnlocked:\n\t\tif g.rw.excl != nil {\n\t\t\treturn false\n\t\t}\n\t\tg.rw.sharedN++\n\t\tg.state = RWMutexStateShared\n\t\treturn true\n\n\tcase RWMutexStateShared:\n\t\treturn true // no-op\n\n\tcase RWMutexStateExclusive:\n\t\tassert(g.rw.excl == g, \"attempted downgrade of non-exclusive guard\")\n\t\tg.rw.sharedN, g.rw.excl = 1, nil\n\t\tg.state = RWMutexStateShared\n\t\treturn true\n\n\tdefault:\n\t\tpanic(\"RWMutexGuard.TryRLock(): unreachable\")\n\t}\n}\n\n// CanRLock returns true if the guard can become a shared lock.\nfunc (g *RWMutexGuard) CanRLock() bool {\n\tg.rw.mu.Lock()\n\tdefer g.rw.mu.Unlock()\n\n\tswitch g.state {\n\tcase RWMutexStateUnlocked:\n\t\treturn g.rw.excl == nil\n\tcase RWMutexStateShared, RWMutexStateExclusive:\n\t\treturn true\n\tdefault:\n\t\tpanic(\"RWMutexGuard.CanRLock(): unreachable\")\n\t}\n}\n\n// Unlock unlocks the underlying mutex.\nfunc (g *RWMutexGuard) Unlock() {\n\tg.rw.mu.Lock()\n\tprevState := g.rw.state()\n\tg.unlock()\n\tfn, newState := g.rw.OnLockStateChange, g.rw.state()\n\tg.rw.mu.Unlock()\n\n\tif fn != nil && prevState != newState {\n\t\tfn(prevState, newState)\n\t}\n}\n\nfunc (g *RWMutexGuard) unlock() {\n\tswitch g.state {\n\tcase RWMutexStateUnlocked:\n\t\treturn // already unlocked, skip\n\tcase RWMutexStateShared:\n\t\tassert(g.rw.sharedN > 0, \"invalid shared lock state on unlock\")\n\t\tg.rw.sharedN--\n\t\tg.state = RWMutexStateUnlocked\n\tcase RWMutexStateExclusive:\n\t\tassert(g.rw.excl == g, \"attempted unlock of non-exclusive guard\")\n\t\tg.rw.sharedN, g.rw.excl = 0, nil\n\t\tg.state = RWMutexStateUnlocked\n\tdefault:\n\t\tpanic(\"RWMutexGuard.Unlock(): unreachable\")\n\t}\n}\n\n// RWMutexState represents the lock state of an RWMutex or RWMutexGuard.\ntype RWMutexState int\n\n// String returns the string representation of the state.\nfunc (s RWMutexState) String() string {\n\tswitch s {\n\tcase RWMutexStateUnlocked:\n\t\treturn \"unlocked\"\n\tcase RWMutexStateShared:\n\t\treturn \"shared\"\n\tcase RWMutexStateExclusive:\n\t\treturn \"exclusive\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"<unknown(%d)>\", s)\n\t}\n}\n\nconst (\n\tRWMutexStateUnlocked = RWMutexState(iota)\n\tRWMutexStateShared\n\tRWMutexStateExclusive\n)\n"
        },
        {
          "name": "rwmutex_test.go",
          "type": "blob",
          "size": 7.392578125,
          "content": "package litefs_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/superfly/litefs\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestRWMutexGuard_TryLock(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0, g1 := mu.Guard(), mu.Guard()\n\t\tif !g0.TryLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t} else if g1.TryLock() {\n\t\t\tt.Fatal(\"expected lock failure\")\n\t\t}\n\t\tg0.Unlock()\n\t})\n\n\tt.Run(\"Relock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tg0.Unlock()\n\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryLock() {\n\t\t\tt.Fatal(\"expected lock after unlock\")\n\t\t}\n\t\tg1.Unlock()\n\t})\n\n\tt.Run(\"BlockedBySharedLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryRLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tg0.Unlock()\n\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryLock() {\n\t\t\tt.Fatal(\"expected lock after shared unlock\")\n\t\t}\n\t\tg1.Unlock()\n\t})\n}\n\nfunc TestRWMutexGuard_Lock(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif err := g0.Lock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tch := make(chan int)\n\t\tvar g errgroup.Group\n\t\tg.Go(func() error {\n\t\t\tg1 := mu.Guard()\n\t\t\tif err := g1.Lock(context.Background()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tclose(ch)\n\t\t\treturn nil\n\t\t})\n\n\t\tselect {\n\t\tcase <-ch:\n\t\t\tt.Fatal(\"lock obtained too soon\")\n\t\tcase <-time.After(100 * time.Millisecond):\n\t\t}\n\n\t\tg0.Unlock()\n\n\t\tselect {\n\t\tcase <-ch:\n\t\tcase <-time.After(100 * time.Millisecond):\n\t\t\tt.Fatal(\"timeout waiting for lock\")\n\t\t}\n\n\t\tif err := g.Wait(); err != nil {\n\t\t\tt.Fatalf(\"goroutine failed: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ContextCanceled\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif err := g0.Lock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer g0.Unlock()\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tvar g errgroup.Group\n\t\tg.Go(func() error {\n\t\t\tg1 := mu.Guard()\n\t\t\tif err := g1.Lock(ctx); err != context.Canceled {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tcancel()\n\t\ttime.Sleep(100 * time.Millisecond)\n\n\t\tif err := g.Wait(); err != nil {\n\t\t\tt.Fatalf(\"goroutine failed: %s\", err)\n\t\t}\n\t})\n}\n\nfunc TestRWMutexGuard_CanLock(t *testing.T) {\n\tt.Run(\"WithExclusiveLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tguard0 := mu.Guard()\n\t\tif canLock, _ := guard0.CanLock(); !canLock {\n\t\t\tt.Fatal(\"expected to be able to lock\")\n\t\t}\n\t\tg := mu.Guard()\n\t\tg.TryLock()\n\t\tguard1 := mu.Guard()\n\t\tif canLock, mutexState := guard1.CanLock(); canLock {\n\t\t\tt.Fatal(\"expected to not be able to lock\")\n\t\t} else if got, want := mutexState, litefs.RWMutexStateExclusive; got != want {\n\t\t\tt.Fatalf(\"mutex=%s, expected %s\", got, want)\n\t\t}\n\t\tg.Unlock()\n\n\t\tguard2 := mu.Guard()\n\t\tif canLock, _ := guard2.CanLock(); !canLock {\n\t\t\tt.Fatal(\"expected to be able to lock again\")\n\t\t}\n\t})\n\n\tt.Run(\"WithSharedLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tguard0 := mu.Guard()\n\t\tif canLock, _ := guard0.CanLock(); !canLock {\n\t\t\tt.Fatal(\"expected to be able to lock\")\n\t\t}\n\t\tg := mu.Guard()\n\t\tg.TryRLock()\n\t\tguard1 := mu.Guard()\n\t\tif canLock, mutexState := guard1.CanLock(); canLock {\n\t\t\tt.Fatal(\"expected to not be able to lock\")\n\t\t} else if got, want := mutexState, litefs.RWMutexStateShared; got != want {\n\t\t\tt.Fatalf(\"mutex=%s, want %s\", got, want)\n\t\t}\n\t\tg.Unlock()\n\n\t\tguard2 := mu.Guard()\n\t\tif canLock, _ := guard2.CanLock(); !canLock {\n\t\t\tt.Fatal(\"expected to be able to lock again\")\n\t\t}\n\t})\n}\n\nfunc TestRWMutexGuard_TryRLock(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryRLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryRLock() {\n\t\t\tt.Fatal(\"expected another lock\")\n\t\t}\n\t\tif guard := mu.Guard(); guard.TryLock() {\n\t\t\tt.Fatal(\"expected lock failure\")\n\t\t}\n\t\tg0.Unlock()\n\t\tg1.Unlock()\n\n\t\tg2 := mu.Guard()\n\t\tif !g2.TryLock() {\n\t\t\tt.Fatal(\"expected lock after unlock\")\n\t\t}\n\t\tg2.Unlock()\n\t})\n\n\tt.Run(\"BlockedByExclusiveLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tif guard := mu.Guard(); guard.TryRLock() {\n\t\t\tt.Fatalf(\"expected lock failure\")\n\t\t}\n\t\tg0.Unlock()\n\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryLock() {\n\t\t\tt.Fatal(\"expected lock after unlock\")\n\t\t}\n\t\tg1.Unlock()\n\t})\n\n\tt.Run(\"AfterDowngrade\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tif guard := mu.Guard(); guard.TryRLock() {\n\t\t\tt.Fatalf(\"expected lock failure\")\n\t\t}\n\t\tif !g0.TryRLock() {\n\t\t\tt.Fatal(\"expected downgrade\")\n\t\t}\n\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryRLock() {\n\t\t\tt.Fatal(\"expected lock after downgrade\")\n\t\t}\n\t\tg0.Unlock()\n\t\tg1.Unlock()\n\t})\n\n\tt.Run(\"AfterUpgrade\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif !g0.TryRLock() {\n\t\t\tt.Fatal(\"expected lock\")\n\t\t}\n\t\tif !g0.TryLock() {\n\t\t\tt.Fatal(\"expected upgrade\")\n\t\t}\n\t\tif guard := mu.Guard(); guard.TryRLock() {\n\t\t\tt.Fatalf(\"expected lock failure\")\n\t\t}\n\t\tif !g0.TryRLock() { // downgrade\n\t\t\tt.Fatal(\"expected downgrade\")\n\t\t}\n\n\t\tg1 := mu.Guard()\n\t\tif !g1.TryRLock() {\n\t\t\tt.Fatal(\"expected lock after downgrade\")\n\t\t}\n\t\tg0.Unlock()\n\t\tg1.Unlock()\n\t})\n}\n\nfunc TestRWMutexGuard_RLock(t *testing.T) {\n\tt.Run(\"MultipleSharedLocks\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif err := g0.RLock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tg1 := mu.Guard()\n\t\tif err := g1.RLock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tg0.Unlock()\n\t\tg1.Unlock()\n\t})\n\n\tt.Run(\"Blocked\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif err := g0.Lock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tvar g errgroup.Group\n\t\tg.Go(func() error {\n\t\t\tg1 := mu.Guard()\n\t\t\tif err := g1.RLock(context.Background()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tg1.Unlock()\n\t\t\treturn nil\n\t\t})\n\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tg0.Unlock()\n\t\ttime.Sleep(100 * time.Millisecond)\n\n\t\tif err := g.Wait(); err != nil {\n\t\t\tt.Fatalf(\"goroutine failed: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ContextCanceled\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tg0 := mu.Guard()\n\t\tif err := g0.Lock(context.Background()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer g0.Unlock()\n\n\t\tch := make(chan int)\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tvar g errgroup.Group\n\t\tg.Go(func() error {\n\t\t\tg1 := mu.Guard()\n\t\t\tif err := g1.RLock(ctx); err != context.Canceled {\n\t\t\t\treturn fmt.Errorf(\"unexpected error: %v\", err)\n\t\t\t}\n\t\t\tclose(ch)\n\t\t\treturn nil\n\t\t})\n\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tcancel()\n\t\t<-ch\n\n\t\tif err := g.Wait(); err != nil {\n\t\t\tt.Fatalf(\"goroutine failed: %s\", err)\n\t\t}\n\t})\n}\n\nfunc TestRWMutexGuard_CanRLock(t *testing.T) {\n\tt.Run(\"WithExclusiveLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tif guard := mu.Guard(); !guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to be able to lock\")\n\t\t}\n\t\tg := mu.Guard()\n\t\tg.TryLock()\n\t\tif guard := mu.Guard(); guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to not be able to lock\")\n\t\t}\n\t\tg.Unlock()\n\n\t\tif guard := mu.Guard(); !guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to be able to lock again\")\n\t\t}\n\t})\n\n\tt.Run(\"WithSharedLock\", func(t *testing.T) {\n\t\tvar mu litefs.RWMutex\n\t\tif guard := mu.Guard(); !guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to be able to lock\")\n\t\t}\n\t\tg := mu.Guard()\n\t\tg.TryRLock()\n\t\tif guard := mu.Guard(); !guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to be able to lock\")\n\t\t}\n\t\tg.Unlock()\n\n\t\tif guard := mu.Guard(); !guard.CanRLock() {\n\t\t\tt.Fatal(\"expected to be able to lock again\")\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "store.go",
          "type": "blob",
          "size": 54.9921875,
          "content": "package litefs\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\tcrand \"crypto/rand\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"expvar\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/superfly/litefs/internal\"\n\t\"github.com/superfly/litefs/internal/chunk\"\n\t\"github.com/superfly/ltx\"\n\t\"golang.org/x/exp/slog\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// Default store settings.\nconst (\n\tDefaultReconnectDelay = 1 * time.Second\n\tDefaultDemoteDelay    = 10 * time.Second\n\n\tDefaultRetention                = 10 * time.Minute\n\tDefaultRetentionMonitorInterval = 1 * time.Minute\n\n\tDefaultHaltAcquireTimeout      = 10 * time.Second\n\tDefaultHaltLockTTL             = 30 * time.Second\n\tDefaultHaltLockMonitorInterval = 5 * time.Second\n\n\tDefaultBackupDelay            = 1 * time.Second\n\tDefaultBackupFullSyncInterval = 10 * time.Second\n)\n\nconst (\n\t// MaxBackupLTXFileN is the number of LTX files that can be compacted\n\t// together at a time when sending data to the backup service.\n\tMaxBackupLTXFileN = 256\n\n\tMetricsMonitorInterval = 1 * time.Second\n)\n\nvar ErrStoreClosed = fmt.Errorf(\"store closed\")\n\n// GlobalStore represents a single store used for metrics collection.\nvar GlobalStore atomic.Value\n\n// Store represents a collection of databases.\ntype Store struct {\n\tmu   sync.Mutex\n\tpath string\n\n\tid                   uint64 // unique node id\n\tclusterID            atomic.Value\n\tdbs                  map[string]*DB\n\tchangeSetSubscribers map[*ChangeSetSubscriber]struct{}\n\teventSubscribers     map[*EventSubscriber]struct{}\n\tprimaryTimestamp     atomic.Int64 // ms since epoch of last update from primary. -1 if primary\n\n\tlease       Lease         // if not nil, store is current primary\n\tprimaryCh   chan struct{} // closed when primary loses leadership\n\tprimaryInfo *PrimaryInfo  // contains info about the current primary\n\tcandidate   bool          // if true, we are eligible to become the primary\n\treadyCh     chan struct{} // closed when primary found or acquired\n\tdemoteCh    chan struct{} // closed when Demote() is called\n\n\tctx    context.Context\n\tcancel context.CancelCauseFunc\n\tg      errgroup.Group\n\n\t// The operating system interface to use for system calls. Defaults to SystemOS.\n\tOS   OS\n\tExit func(int)\n\n\t// Client used to connect to other LiteFS instances.\n\tClient Client\n\n\t// Leaser manages the lease that controls leader election.\n\tLeaser Leaser\n\n\t// BackupClient is the client to connect to an external backup service.\n\tBackupClient BackupClient\n\n\t// If true, LTX files are compressed using LZ4.\n\tCompress bool\n\n\t// Time to wait after disconnecting from the primary to reconnect.\n\tReconnectDelay time.Duration\n\n\t// Time to wait after manually demoting trying to become primary again.\n\tDemoteDelay time.Duration\n\n\t// Length of time to retain LTX files.\n\tRetention                time.Duration\n\tRetentionMonitorInterval time.Duration\n\n\t// Max time to hold HALT lock and interval between expiration checks.\n\tHaltLockTTL             time.Duration\n\tHaltLockMonitorInterval time.Duration\n\n\t// Time to wait to acquire the HALT lock.\n\tHaltAcquireTimeout time.Duration\n\n\t// Time after a change is made before it is sent to the backup service.\n\t// This allows multiple changes in quick succession to be batched together.\n\tBackupDelay time.Duration\n\n\t// Interval between checks to re-fetch the position map. This ensures that\n\t// restores on the backup server are detected by the LiteFS primary.\n\tBackupFullSyncInterval time.Duration\n\n\t// Callback to notify kernel of file changes.\n\tInvalidator Invalidator\n\n\t// Interface to interact with the host environment.\n\tEnvironment Environment\n\n\t// Specifies a subset of databases to replicate from the primary.\n\tDatabaseFilter []string\n\n\t// If true, computes and verifies the checksum of the entire database\n\t// after every transaction. Should only be used during testing.\n\tStrictVerify bool\n}\n\n// NewStore returns a new instance of Store.\nfunc NewStore(path string, candidate bool) *Store {\n\tprimaryCh := make(chan struct{})\n\tclose(primaryCh)\n\n\t// Generate random node ID to prevent connecting to itself.\n\tb := make([]byte, 16)\n\tif _, err := io.ReadFull(crand.Reader, b); err != nil {\n\t\tpanic(fmt.Errorf(\"cannot generate node id: %w\", err))\n\t}\n\n\ts := &Store{\n\t\tid:   binary.BigEndian.Uint64(b),\n\t\tpath: path,\n\n\t\tdbs: make(map[string]*DB),\n\n\t\tchangeSetSubscribers: make(map[*ChangeSetSubscriber]struct{}),\n\t\teventSubscribers:     make(map[*EventSubscriber]struct{}),\n\n\t\tcandidate: candidate,\n\t\tprimaryCh: primaryCh,\n\t\treadyCh:   make(chan struct{}),\n\t\tdemoteCh:  make(chan struct{}),\n\n\t\tOS:   &internal.SystemOS{},\n\t\tExit: os.Exit,\n\n\t\tReconnectDelay: DefaultReconnectDelay,\n\t\tDemoteDelay:    DefaultDemoteDelay,\n\n\t\tRetention:                DefaultRetention,\n\t\tRetentionMonitorInterval: DefaultRetentionMonitorInterval,\n\n\t\tHaltAcquireTimeout:      DefaultHaltAcquireTimeout,\n\t\tHaltLockTTL:             DefaultHaltLockTTL,\n\t\tHaltLockMonitorInterval: DefaultHaltLockMonitorInterval,\n\n\t\tBackupDelay:            DefaultBackupDelay,\n\t\tBackupFullSyncInterval: DefaultBackupFullSyncInterval,\n\n\t\tEnvironment: &nopEnvironment{},\n\t}\n\ts.ctx, s.cancel = context.WithCancelCause(context.Background())\n\ts.clusterID.Store(\"\")\n\ts.primaryTimestamp.Store(-1)\n\n\treturn s\n}\n\n// Path returns underlying data directory.\nfunc (s *Store) Path() string { return s.path }\n\n// DBDir returns the folder that stores all databases.\nfunc (s *Store) DBDir() string {\n\treturn filepath.Join(s.path, \"dbs\")\n}\n\n// DBPath returns the folder that stores a single database.\nfunc (s *Store) DBPath(name string) string {\n\treturn filepath.Join(s.path, \"dbs\", name)\n}\n\n// ClusterIDPath returns the filename where the cluster ID is stored.\nfunc (s *Store) ClusterIDPath() string {\n\treturn filepath.Join(s.path, \"clusterid\")\n}\n\n// ID returns the unique identifier for this instance. Available after Open().\n// Persistent across restarts if underlying storage is persistent.\nfunc (s *Store) ID() uint64 {\n\treturn s.id\n}\n\n// ClusterID returns the cluster ID.\nfunc (s *Store) ClusterID() string {\n\treturn s.clusterID.Load().(string)\n}\n\n// setClusterID saves the cluster ID to disk.\nfunc (s *Store) setClusterID(id string) error {\n\tif s.ClusterID() == id {\n\t\treturn nil // no-op\n\t}\n\n\tif err := ValidateClusterID(id); err != nil {\n\t\treturn err\n\t}\n\n\tfilename := s.ClusterIDPath()\n\ttempFilename := filename + \".tmp\"\n\tdefer func() { _ = s.OS.Remove(\"SETCLUSTERID\", tempFilename) }()\n\n\tif err := s.OS.MkdirAll(\"SETCLUSTERID\", filepath.Dir(filename), 0o777); err != nil {\n\t\treturn err\n\t}\n\n\tf, err := s.OS.Create(\"SETCLUSTERID\", tempFilename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tif _, err := io.WriteString(f, id+\"\\n\"); err != nil {\n\t\treturn err\n\t}\n\n\tif err := f.Sync(); err != nil {\n\t\treturn err\n\t} else if err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s.OS.Rename(\"SETCLUSTERID\", tempFilename, filename); err != nil {\n\t\treturn err\n\t} else if err := internal.Sync(filepath.Dir(filename)); err != nil {\n\t\treturn err\n\t}\n\n\ts.clusterID.Store(id)\n\treturn nil\n}\n\n// Open initializes the store based on files in the data directory.\nfunc (s *Store) Open() error {\n\tif s.Leaser == nil {\n\t\treturn fmt.Errorf(\"leaser required\")\n\t}\n\n\tif err := s.OS.MkdirAll(\"OPEN\", s.path, 0o777); err != nil {\n\t\treturn err\n\t}\n\n\t// Attempt to remove persisted node ID from disk.\n\t// See: https://github.com/superfly/litefs/issues/361\n\t_ = s.OS.Remove(\"OPEN:ID\", filepath.Join(s.path, \"id\"))\n\n\t// Load cluster ID from disk, if available locally.\n\tif err := s.readClusterID(); err != nil {\n\t\treturn fmt.Errorf(\"load cluster id: %w\", err)\n\t}\n\n\tif err := s.openDatabases(); err != nil {\n\t\treturn fmt.Errorf(\"open databases: %w\", err)\n\t}\n\n\t// Begin background replication monitor.\n\ts.g.Go(func() error { return s.monitorLease(s.ctx) })\n\n\t// Begin lock monitor.\n\ts.g.Go(func() error { return s.monitorHaltLock(s.ctx) })\n\n\t// Begin retention monitor.\n\tif s.RetentionMonitorInterval > 0 {\n\t\ts.g.Go(func() error { return s.monitorRetention(s.ctx) })\n\t}\n\n\treturn nil\n}\n\n// readClusterID reads the cluster ID from the \"clusterid\" file.\n// Skipped if no cluster id file exists.\nfunc (s *Store) readClusterID() error {\n\tb, err := s.OS.ReadFile(\"READCLUSTERID\", s.ClusterIDPath())\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\tclusterID := strings.TrimSpace(string(b))\n\tif err := ValidateClusterID(clusterID); err != nil {\n\t\treturn err\n\t}\n\ts.clusterID.Store(clusterID)\n\n\treturn nil\n}\n\nfunc (s *Store) openDatabases() error {\n\tif err := s.OS.MkdirAll(\"OPENDATABASES\", s.DBDir(), 0o777); err != nil {\n\t\treturn err\n\t}\n\n\tfis, err := s.OS.ReadDir(\"OPENDATABASES\", s.DBDir())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"readdir: %w\", err)\n\t}\n\tfor _, fi := range fis {\n\t\tif err := s.openDatabase(fi.Name()); err != nil {\n\t\t\treturn fmt.Errorf(\"open database(%q): %w\", fi.Name(), err)\n\t\t}\n\t}\n\n\t// Update metrics.\n\tstoreDBCountMetric.Set(float64(len(s.dbs)))\n\n\treturn nil\n}\n\nfunc (s *Store) openDatabase(name string) error {\n\t// Instantiate and open database.\n\tdb := NewDB(s, name, s.DBPath(name))\n\tif err := db.Open(); err != nil {\n\t\treturn err\n\t}\n\n\t// Add to internal lookups.\n\ts.dbs[db.Name()] = db\n\n\treturn nil\n}\n\n// Close signals for the store to shut down.\nfunc (s *Store) Close() (retErr error) {\n\ts.cancel(ErrStoreClosed)\n\tretErr = s.g.Wait()\n\n\t// Release outstanding HALT locks.\n\tfor _, db := range s.DBs() {\n\t\thaltLock := db.RemoteHaltLock()\n\t\tif haltLock == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Printf(\"releasing halt lock on %q\", db.Name())\n\n\t\tif err := db.ReleaseRemoteHaltLock(context.Background(), haltLock.ID); err != nil {\n\t\t\tlog.Printf(\"cannot release halt lock on %q on shutdown\", db.Name())\n\t\t}\n\t}\n\n\treturn retErr\n}\n\n// ReadyCh returns a channel that is closed once the store has become primary\n// or once it has connected to the primary.\nfunc (s *Store) ReadyCh() chan struct{} {\n\treturn s.readyCh\n}\n\nfunc (s *Store) isReady() bool {\n\tselect {\n\tcase <-s.readyCh:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// markReady closes the ready channel if it hasn't already been closed.\nfunc (s *Store) markReady() {\n\tselect {\n\tcase <-s.readyCh:\n\t\treturn\n\tdefault:\n\t\tclose(s.readyCh)\n\t}\n}\n\n// Demote instructs store to destroy its primary lease, if any.\n// Store will wait momentarily before attempting to become primary again.\nfunc (s *Store) Demote() {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tclose(s.demoteCh)\n\ts.demoteCh = make(chan struct{})\n}\n\n// Handoff instructs store to send its lease to a connected replica.\nfunc (s *Store) Handoff(ctx context.Context, nodeID uint64) error {\n\tvar lease Lease\n\tif err := func() error {\n\t\ts.mu.Lock()\n\t\tdefer s.mu.Unlock()\n\n\t\t// Ensure this node is currently the primary and has a lease.\n\t\tlease = s.lease\n\t\tif lease == nil {\n\t\t\treturn fmt.Errorf(\"node is not currently primary\")\n\t\t}\n\n\t\t// Find connected subscriber by node ID.\n\t\tsub := s.changeSetSubscriberByNodeID(nodeID)\n\t\tif sub == nil {\n\t\t\treturn fmt.Errorf(\"target node is not currently connected\")\n\t\t}\n\t\treturn nil\n\t}(); err != nil {\n\t\treturn err\n\t}\n\n\t// Attempt to handoff the lease.\n\t// Not all lease systems support handoff so this may return an error.\n\treturn lease.Handoff(ctx, nodeID)\n}\n\n// IsPrimary returns true if store has a lease to be the primary.\nfunc (s *Store) IsPrimary() bool {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.isPrimary()\n}\n\nfunc (s *Store) isPrimary() bool { return s.lease != nil }\n\nfunc (s *Store) setLease(lease Lease) {\n\t// Create a new channel to notify about primary loss when becoming primary.\n\t// Or close existing channel if we are losing our primary status.\n\tif (s.lease != nil) != (lease != nil) {\n\t\tif lease != nil {\n\t\t\ts.primaryCh = make(chan struct{})\n\t\t\ts.setPrimaryTimestamp(0)\n\t\t} else {\n\t\t\tclose(s.primaryCh)\n\t\t\ts.setPrimaryTimestamp(-1)\n\t\t}\n\t}\n\n\t// Store current lease\n\ts.lease = lease\n\n\t// Update metrics.\n\tif s.isPrimary() {\n\t\tstoreIsPrimaryMetric.Set(1)\n\t} else {\n\t\tstoreIsPrimaryMetric.Set(0)\n\t}\n\n\ts.notifyPrimaryChange()\n}\n\n// PrimaryCtx wraps ctx with another context that will cancel when no longer primary.\nfunc (s *Store) PrimaryCtx(ctx context.Context) context.Context {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.primaryCtx(ctx)\n}\n\nfunc (s *Store) primaryCtx(ctx context.Context) context.Context {\n\treturn newPrimaryCtx(ctx, s.primaryCh)\n}\n\n// PrimaryInfo returns info about the current primary.\nfunc (s *Store) PrimaryInfo() (isPrimary bool, info *PrimaryInfo) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.isPrimary(), s.primaryInfo.Clone()\n}\n\n// PrimaryInfoWithContext continually attempts to fetch the primary info until available.\n// Returns when isPrimary is true, info is non-nil, or when ctx is done.\nfunc (s *Store) PrimaryInfoWithContext(ctx context.Context) (isPrimary bool, info *PrimaryInfo) {\n\tif isPrimary, info = s.PrimaryInfo(); isPrimary || info != nil {\n\t\treturn isPrimary, info\n\t}\n\n\tticker := time.NewTicker(100 * time.Microsecond)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn isPrimary, info\n\t\tcase <-ticker.C:\n\t\t\tif isPrimary, info = s.PrimaryInfo(); isPrimary || info != nil {\n\t\t\t\treturn isPrimary, info\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (s *Store) setPrimaryInfo(info *PrimaryInfo) {\n\ts.primaryInfo = info\n\ts.notifyPrimaryChange()\n}\n\n// Candidate returns true if store is eligible to be the primary.\nfunc (s *Store) Candidate() bool {\n\treturn s.candidate\n}\n\n// DBByName returns a database by name.\n// Returns nil if the database does not exist.\nfunc (s *Store) DB(name string) *DB {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.dbs[name]\n}\n\n// DBs returns a list of databases.\nfunc (s *Store) DBs() []*DB {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\ta := make([]*DB, 0, len(s.dbs))\n\tfor _, db := range s.dbs {\n\t\ta = append(a, db)\n\t}\n\treturn a\n}\n\n// CreateDB creates a new database with the given name. The returned file handle\n// must be closed by the caller. Returns an error if a database with the same\n// name already exists.\nfunc (s *Store) CreateDB(name string) (db *DB, f *os.File, err error) {\n\tdefer func() {\n\t\tTraceLog.Printf(\"[CreateDatabase(%s)]: %s\", name, errorKeyValue(err))\n\t}()\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Check if the database already exists. We could have a reference to a\n\t// zero-length database which means it was previously deleted.\n\trequireNewDB := true\n\tif db = s.dbs[name]; db != nil {\n\t\tif db.PageN() > 0 {\n\t\t\treturn nil, nil, ErrDatabaseExists\n\t\t}\n\t\trequireNewDB = false\n\t}\n\n\t// Generate database directory with name file & empty database file.\n\tdbPath := s.DBPath(name)\n\tif err := s.OS.MkdirAll(\"CREATDEDB\", dbPath, 0o777); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tf, err = s.OS.OpenFile(\"CREATDEDB\", filepath.Join(dbPath, \"database\"), os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_TRUNC, 0o666)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Create new database instance and add to maps.\n\tif requireNewDB {\n\t\tdb = NewDB(s, name, dbPath)\n\t\tif err := db.Open(); err != nil {\n\t\t\t_ = f.Close()\n\t\t\treturn nil, nil, err\n\t\t}\n\t\ts.dbs[name] = db\n\t}\n\n\t// Notify listeners of change.\n\ts.markDirty(name)\n\n\t// Update metrics\n\tstoreDBCountMetric.Set(float64(len(s.dbs)))\n\n\treturn db, f, nil\n}\n\n// CreateDBIfNotExists creates an empty database with the given name.\nfunc (s *Store) CreateDBIfNotExists(name string) (*DB, error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Exit if database with same name already exists.\n\tif db := s.dbs[name]; db != nil {\n\t\treturn db, nil\n\t}\n\n\t// Generate database directory with name file & empty database file.\n\tdbPath := s.DBPath(name)\n\tif err := s.OS.MkdirAll(\"CREATDEDBIFNOTEXISTS\", dbPath, 0o777); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := s.OS.WriteFile(\"CREATDEDBIFNOTEXISTS\", filepath.Join(dbPath, \"database\"), nil, 0o666); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create new database instance and add to maps.\n\tdb := NewDB(s, name, dbPath)\n\tif err := db.Open(); err != nil {\n\t\treturn nil, err\n\t}\n\ts.dbs[name] = db\n\n\t// Notify listeners of change.\n\ts.markDirty(name)\n\n\t// Update metrics\n\tstoreDBCountMetric.Set(float64(len(s.dbs)))\n\n\treturn db, nil\n}\n\n// PosMap returns a map of databases and their transactional position.\nfunc (s *Store) PosMap() map[string]ltx.Pos {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tm := make(map[string]ltx.Pos, len(s.dbs))\n\tfor _, db := range s.dbs {\n\t\tm[db.Name()] = db.Pos()\n\t}\n\treturn m\n}\n\n// SubscribeChangeSet creates a new subscriber for store changes.\nfunc (s *Store) SubscribeChangeSet(nodeID uint64) *ChangeSetSubscriber {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tsub := newChangeSetSubscriber(s, nodeID)\n\ts.changeSetSubscribers[sub] = struct{}{}\n\n\tstoreSubscriberCountMetric.Set(float64(len(s.changeSetSubscribers)))\n\treturn sub\n}\n\n// UnsubscribeChangeSet removes a subscriber from the store.\nfunc (s *Store) UnsubscribeChangeSet(sub *ChangeSetSubscriber) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tdelete(s.changeSetSubscribers, sub)\n\tstoreSubscriberCountMetric.Set(float64(len(s.changeSetSubscribers)))\n}\n\n// SubscriberByNodeID returns a subscriber by node ID.\n// Returns nil if the node is not currently subscribed to the store.\nfunc (s *Store) SubscriberByNodeID(nodeID uint64) *ChangeSetSubscriber {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.changeSetSubscriberByNodeID(nodeID)\n}\n\nfunc (s *Store) changeSetSubscriberByNodeID(nodeID uint64) *ChangeSetSubscriber {\n\tfor sub := range s.changeSetSubscribers {\n\t\tif sub.NodeID() == nodeID {\n\t\t\treturn sub\n\t\t}\n\t}\n\treturn nil\n}\n\n// MarkDirty marks a database dirty on all subscribers.\nfunc (s *Store) MarkDirty(name string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.markDirty(name)\n}\n\nfunc (s *Store) markDirty(name string) {\n\tfor sub := range s.changeSetSubscribers {\n\t\tsub.MarkDirty(name)\n\t}\n}\n\n// SubscribeEvents creates a new subscriber for store events.\nfunc (s *Store) SubscribeEvents() *EventSubscriber {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tvar hostname string\n\tif s.primaryInfo != nil {\n\t\thostname = s.primaryInfo.Hostname\n\t}\n\n\tsub := newEventSubscriber(s)\n\tsub.ch <- Event{\n\t\tType: EventTypeInit,\n\t\tData: InitEventData{\n\t\t\tIsPrimary: s.isPrimary(),\n\t\t\tHostname:  hostname,\n\t\t},\n\t}\n\n\ts.eventSubscribers[sub] = struct{}{}\n\n\treturn sub\n}\n\n// UnsubscribeEvents removes an event subscriber from the store.\nfunc (s *Store) UnsubscribeEvents(sub *EventSubscriber) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.unsubscribeEvents(sub)\n}\n\nfunc (s *Store) unsubscribeEvents(sub *EventSubscriber) {\n\tif _, ok := s.eventSubscribers[sub]; ok {\n\t\tdelete(s.eventSubscribers, sub)\n\t\tclose(sub.ch)\n\t}\n}\n\n// NotifyEvent sends event to all event subscribers.\n// If a subscriber has no additional buffer space available then it is closed.\nfunc (s *Store) NotifyEvent(event Event) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.notifyEvent(event)\n}\n\nfunc (s *Store) notifyEvent(event Event) {\n\tfor sub := range s.eventSubscribers {\n\t\tselect {\n\t\tcase sub.ch <- event:\n\t\tdefault:\n\t\t\ts.unsubscribeEvents(sub)\n\t\t}\n\t}\n}\n\nfunc (s *Store) notifyPrimaryChange() {\n\tvar hostname string\n\tif s.primaryInfo != nil {\n\t\thostname = s.primaryInfo.Hostname\n\t}\n\n\ts.notifyEvent(Event{\n\t\tType: EventTypePrimaryChange,\n\t\tData: PrimaryChangeEventData{\n\t\t\tIsPrimary: s.isPrimary(),\n\t\t\tHostname:  hostname,\n\t\t},\n\t})\n}\n\n// monitorLease continuously handles either the leader lease or replicates from the primary.\nfunc (s *Store) monitorLease(ctx context.Context) (err error) {\n\t// Initialize environment to indicate this node is not a primary.\n\ts.Environment.SetPrimaryStatus(ctx, false)\n\n\tvar handoffLeaseID string\n\tfor {\n\t\t// Exit if store is closed.\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn nil\n\t\t}\n\n\t\t// If a cluster ID exists on the server, ensure it matches what we have.\n\t\tvar info PrimaryInfo\n\t\tif leaserClusterID, err := s.Leaser.ClusterID(ctx); err != nil {\n\t\t\tlog.Printf(\"cannot fetch cluster ID from %q lease, retrying: %s\", s.Leaser.Type(), err)\n\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\tcontinue\n\n\t\t} else if leaserClusterID != \"\" && s.ClusterID() != \"\" && leaserClusterID != s.ClusterID() {\n\t\t\tlog.Printf(\"cannot connect, %q lease already initialized with different ID: %s\", s.Leaser.Type(), leaserClusterID)\n\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\tcontinue\n\n\t\t} else if leaserClusterID != \"\" && s.ClusterID() == \"\" {\n\t\t\tlog.Printf(\"cannot become primary, local node has no cluster ID and %q lease already initialized with cluster ID %s\", s.Leaser.Type(), leaserClusterID)\n\n\t\t\tif info, err = s.Leaser.PrimaryInfo(ctx); err != nil {\n\t\t\t\tlog.Printf(\"cannot find primary, retrying: %s\", err)\n\t\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t} else {\n\t\t\t// At this point, either the leaser has a cluster ID and ours matches,\n\t\t\t// or the leaser has no cluster ID. We'll update the leaser once we\n\t\t\t// become primary.\n\n\t\t\t// If we have been handed a lease ID from the current primary, use that\n\t\t\t// and act like we're the new primary.\n\t\t\tvar lease Lease\n\t\t\tif handoffLeaseID != \"\" {\n\t\t\t\t// Move lease to a local variable so we can clear the outer scope.\n\t\t\t\tleaseID := handoffLeaseID\n\t\t\t\thandoffLeaseID = \"\"\n\n\t\t\t\t// We'll only try to acquire the lease once. If it fails, then it\n\t\t\t\t// reverts back to the regular primary/replica flow.\n\t\t\t\tlog.Printf(\"%s: acquiring existing lease from handoff\", FormatNodeID(s.id))\n\t\t\t\tif lease, err = s.Leaser.AcquireExisting(ctx, leaseID); err != nil {\n\t\t\t\t\tlog.Printf(\"%s: cannot acquire existing lease from handoff, retrying: %s\", FormatNodeID(s.id), err)\n\t\t\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t} else {\n\t\t\t\t// Otherwise, attempt to either obtain a primary lock or read the current primary.\n\t\t\t\tlease, info, err = s.acquireLeaseOrPrimaryInfo(ctx)\n\t\t\t\tif err == ErrNoPrimary && !s.candidate {\n\t\t\t\t\tlog.Printf(\"%s: cannot find primary & ineligible to become primary, retrying: %s\", FormatNodeID(s.id), err)\n\t\t\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\t\t\tcontinue\n\t\t\t\t} else if err != nil {\n\t\t\t\t\tlog.Printf(\"%s: cannot acquire lease or find primary, retrying: %s\", FormatNodeID(s.id), err)\n\t\t\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Monitor as primary if we have obtained a lease.\n\t\t\tif lease != nil {\n\t\t\t\tlog.Printf(\"%s: primary lease acquired, advertising as %s\", FormatNodeID(s.id), s.Leaser.AdvertiseURL())\n\t\t\t\tif err := s.monitorLeaseAsPrimary(ctx, lease); err != nil {\n\t\t\t\t\tlog.Printf(\"%s: primary lease lost, retrying: %s\", FormatNodeID(s.id), err)\n\t\t\t\t}\n\t\t\t\tif err := s.Recover(ctx); err != nil {\n\t\t\t\t\tlog.Printf(\"%s: state change recovery error (primary): %s\", FormatNodeID(s.id), err)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Monitor as replica if another primary already exists.\n\t\tlog.Printf(\"%s: existing primary found (%s), connecting as replica to %q\", FormatNodeID(s.id), info.Hostname, info.AdvertiseURL)\n\t\tif handoffLeaseID, err = s.monitorLeaseAsReplica(ctx, info); err == nil {\n\t\t\tlog.Printf(\"%s: disconnected from primary, retrying\", FormatNodeID(s.id))\n\t\t} else {\n\t\t\tlog.Printf(\"%s: disconnected from primary with error, retrying: %s\", FormatNodeID(s.id), err)\n\t\t}\n\t\tif err := s.Recover(ctx); err != nil {\n\t\t\tlog.Printf(\"%s: state change recovery error (replica): %s\", FormatNodeID(s.id), err)\n\t\t}\n\n\t\t// Ignore the sleep if we are receiving a handed off lease.\n\t\tif handoffLeaseID == \"\" {\n\t\t\tsleepWithContext(ctx, s.ReconnectDelay)\n\t\t}\n\t}\n}\n\nfunc (s *Store) acquireLeaseOrPrimaryInfo(ctx context.Context) (Lease, PrimaryInfo, error) {\n\t// Attempt to find an existing primary first.\n\tinfo, err := s.Leaser.PrimaryInfo(ctx)\n\tif err == ErrNoPrimary && !s.candidate {\n\t\treturn nil, info, err // no primary, not eligible to become primary\n\t} else if err != nil && err != ErrNoPrimary {\n\t\treturn nil, info, fmt.Errorf(\"fetch primary url: %w\", err)\n\t} else if err == nil {\n\t\treturn nil, info, nil\n\t}\n\n\t// If no primary, attempt to become primary.\n\tlease, err := s.Leaser.Acquire(ctx)\n\tif err == ErrPrimaryExists {\n\t\t// passthrough and retry primary info fetch\n\t} else if err != nil {\n\t\treturn nil, info, fmt.Errorf(\"acquire lease: %w\", err)\n\t} else if lease != nil {\n\t\treturn lease, info, nil\n\t}\n\n\t// If we raced to become primary and another node beat us, retry the fetch.\n\tinfo, err = s.Leaser.PrimaryInfo(ctx)\n\tif err != nil {\n\t\treturn nil, info, err\n\t}\n\treturn nil, info, nil\n}\n\n// monitorLeaseAsPrimary monitors & renews the current lease.\n// NOTE: This code is borrowed from the consul/api's RenewPeriodic() implementation.\nfunc (s *Store) monitorLeaseAsPrimary(ctx context.Context, lease Lease) error {\n\tconst timeout = 1 * time.Second\n\n\t// Attempt to destroy lease when we exit this function.\n\tvar demoted bool\n\tcloseLeaseOnExit := true\n\tdefer func() {\n\t\tif closeLeaseOnExit {\n\t\t\tlog.Printf(\"%s: exiting primary, destroying lease\", FormatNodeID(s.id))\n\t\t\tif err := lease.Close(); err != nil {\n\t\t\t\tlog.Printf(\"%s: cannot remove lease: %s\", FormatNodeID(s.id), err)\n\t\t\t}\n\t\t} else {\n\t\t\tlog.Printf(\"%s: exiting primary, preserving lease for handoff\", FormatNodeID(s.id))\n\t\t}\n\n\t\t// Pause momentarily if this was a manual demotion.\n\t\tif demoted {\n\t\t\tlog.Printf(\"%s: waiting for %s after demotion\", FormatNodeID(s.id), s.DemoteDelay)\n\t\t\tsleepWithContext(ctx, s.DemoteDelay)\n\t\t}\n\t}()\n\n\t// If the leaser doesn't have a cluster ID yet, generate one or set it to ours.\n\tif v, err := s.Leaser.ClusterID(ctx); err != nil {\n\t\treturn fmt.Errorf(\"set cluster id: %w\", err)\n\t} else if v == \"\" {\n\t\t// Use existing ID or generate a new one.\n\t\tclusterID := s.ClusterID()\n\t\tif clusterID == \"\" {\n\t\t\tclusterID = GenerateClusterID()\n\t\t}\n\n\t\t// Update the cluster ID on the leaser.\n\t\tif err := s.Leaser.SetClusterID(ctx, clusterID); err != nil {\n\t\t\treturn fmt.Errorf(\"set leaser cluster id: %w\", err)\n\t\t}\n\n\t\t// Save the cluster ID to disk, in case we generated a new one above.\n\t\tif err := s.setClusterID(clusterID); err != nil {\n\t\t\treturn fmt.Errorf(\"set local cluster id: %w\", err)\n\t\t}\n\n\t\tlog.Printf(\"set cluster id on %q lease %q\", s.Leaser.Type(), clusterID)\n\t}\n\n\t// Mark as the primary node while we're in this function.\n\ts.mu.Lock()\n\ts.setLease(lease)\n\tprimaryCtx := s.primaryCtx(context.Background())\n\tdemoteCh := s.demoteCh\n\ts.mu.Unlock()\n\n\t// Mark store as ready if we've obtained primary status.\n\ts.markReady()\n\n\t// Run background goroutine to push data to long-term storage while we are primary.\n\t// This context is canceled when the lease is cleared on exit of the function.\n\tvar g sync.WaitGroup\n\tdefer g.Wait()\n\n\tif s.BackupClient != nil && s.BackupDelay > 0 {\n\t\tg.Add(1)\n\t\tgo func() { defer g.Done(); s.monitorPrimaryBackup(primaryCtx) }()\n\t}\n\n\t// Ensure that we are no longer marked as primary once we exit this function.\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tdefer s.mu.Unlock()\n\t\ts.setLease(nil)\n\t}()\n\n\t// Notify host environment that we are primary.\n\ts.Environment.SetPrimaryStatus(ctx, true)\n\tdefer func() { s.Environment.SetPrimaryStatus(ctx, false) }()\n\n\twaitDur := lease.TTL() / 2\n\n\tfor {\n\t\tselect {\n\t\tcase <-time.After(waitDur):\n\t\t\t// Attempt to renew the lease. If the lease is gone then we need to\n\t\t\t// just exit and we can start over or connect to the new primary.\n\t\t\t//\n\t\t\t// If we just have a connection error then we'll try to more\n\t\t\t// aggressively retry the renewal until we exceed TTL.\n\t\t\tif err := lease.Renew(ctx); err == ErrLeaseExpired {\n\t\t\t\treturn err\n\t\t\t} else if err != nil {\n\t\t\t\t// If our next renewal will exceed TTL, exit now.\n\t\t\t\tif time.Since(lease.RenewedAt())+timeout > lease.TTL() {\n\t\t\t\t\ttime.Sleep(timeout)\n\t\t\t\t\treturn ErrLeaseExpired\n\t\t\t\t}\n\n\t\t\t\t// Otherwise log error and try again after a shorter period.\n\t\t\t\tlog.Printf(\"%s: lease renewal error, retrying: %s\", FormatNodeID(s.id), err)\n\t\t\t\twaitDur = time.Second\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Renewal was successful, restart with low frequency.\n\t\t\twaitDur = lease.TTL() / 2\n\n\t\tcase <-demoteCh:\n\t\t\tdemoted = true\n\t\t\tlog.Printf(\"%s: node manually demoted\", FormatNodeID(s.id))\n\t\t\treturn nil\n\n\t\tcase nodeID := <-lease.HandoffCh():\n\t\t\tif err := s.processHandoff(ctx, nodeID, lease); err != nil {\n\t\t\t\tlog.Printf(\"%s: handoff unsuccessful, continuing as primary\", FormatNodeID(s.id))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcloseLeaseOnExit = false\n\t\t\treturn nil\n\n\t\tcase <-ctx.Done():\n\t\t\treturn nil // release lease when we shut down\n\t\t}\n\t}\n}\n\n// monitorPrimaryBackup executes in the background while the node is primary.\n// The context is canceled when the primary status is lost.\nfunc (s *Store) monitorPrimaryBackup(ctx context.Context) {\n\tlog.Printf(\"begin primary backup stream: url=%s\", s.BackupClient.URL())\n\tdefer log.Printf(\"primary backup stream exiting\")\n\n\tticker := time.NewTicker(1 * time.Second)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tif err := s.streamBackup(ctx, false); err != nil {\n\t\t\t\tlog.Printf(\"backup stream failed, retrying: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// SyncBackup connects to a backup server performs a one-time sync.\nfunc (s *Store) SyncBackup(ctx context.Context) error {\n\treturn s.streamBackup(ctx, true)\n}\n\n// streamBackup connects to a backup server and continuously streams LTX files.\nfunc (s *Store) streamBackup(ctx context.Context, oneTime bool) (err error) {\n\t// Start subscription immediately so we can collect any changes.\n\tsubscription := s.SubscribeChangeSet(0)\n\tdefer func() { _ = subscription.Close() }()\n\n\tslog.Info(\"begin streaming backup\", slog.Duration(\"full-sync-interval\", s.BackupFullSyncInterval))\n\tdefer func() { slog.Info(\"exiting streaming backup\") }()\n\n\ttimer := time.NewTimer(0)\n\tdefer timer.Stop()\n\n\tvar posMapTickerCh <-chan time.Time\n\tif s.BackupFullSyncInterval > 0 {\n\t\tticker := time.NewTicker(s.BackupFullSyncInterval)\n\t\tdefer ticker.Stop()\n\t\tposMapTickerCh = ticker.C\n\t}\n\n\tvar posMap map[string]ltx.Pos\n\tdirtySet := make(map[string]struct{})\n\nLOOP:\n\tfor {\n\t\t// If we don't have a position map yet or if it's been reset then fetch\n\t\t// a new one from the backup server and update our dirty set.\n\t\tif posMap == nil {\n\t\t\tslog.Debug(\"fetching position map from backup server\")\n\t\t\tif posMap, err = s.BackupClient.PosMap(ctx); err != nil {\n\t\t\t\treturn fmt.Errorf(\"fetch position map: %w\", err)\n\t\t\t}\n\t\t\tfor name := range posMap {\n\t\t\t\tdirtySet[name] = struct{}{}\n\t\t\t}\n\t\t\tfor _, db := range s.DBs() {\n\t\t\t\tdirtySet[db.Name()] = struct{}{}\n\t\t\t}\n\t\t}\n\n\t\tslog.Debug(\"syncing databases to backup\", slog.Int(\"dirty\", len(dirtySet)))\n\n\t\t// Send pending transactions for each database.\n\t\tfor name := range dirtySet {\n\t\t\t// Send all outstanding LTX files to the backup service. The backup\n\t\t\t// service is the data authority so if we cannot stream a contiguous\n\t\t\t// set of changes (e.g. position mismatch) then we need to revert to\n\t\t\t// the current snapshot state of the backup service.\n\t\t\tvar pmErr *ltx.PosMismatchError\n\t\t\tnewPos, err := s.streamBackupDB(ctx, name, posMap[name])\n\t\t\tif errors.As(err, &pmErr) {\n\t\t\t\tnewPos, err = s.restoreDBFromBackup(ctx, name)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"restore from backup error (%q): %s\", name, err)\n\t\t\t\t}\n\t\t\t} else if err != nil {\n\t\t\t\treturn fmt.Errorf(\"backup stream error (%q): %s\", name, err)\n\t\t\t}\n\n\t\t\t// If the position returned is empty then clear it from our map.\n\t\t\tif newPos.IsZero() {\n\t\t\t\tslog.Debug(\"no position, removing from backup sync\", slog.String(\"name\", name))\n\t\t\t\tdelete(posMap, name)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Update the latest position on the backup service for this database.\n\t\t\tslog.Debug(\"database synced to backup\",\n\t\t\t\tslog.String(\"name\", name),\n\t\t\t\tslog.String(\"pos\", newPos.String()))\n\t\t\tposMap[name] = newPos\n\t\t}\n\n\t\t// If not continuous, exit here. Used for deterministic testing through SyncBackup().\n\t\tif oneTime {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Wait for new changes.\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase <-posMapTickerCh: // periodically re-fetch position map from backup server\n\t\t\tposMap = nil\n\t\t\tcontinue LOOP\n\t\tcase <-subscription.NotifyCh():\n\t\t}\n\n\t\t// Wait for a a delay to batch changes together.\n\t\ttimer.Reset(s.BackupDelay)\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase <-timer.C:\n\t\t\tdirtySet = subscription.DirtySet()\n\t\t}\n\t}\n}\n\nfunc (s *Store) streamBackupDB(ctx context.Context, name string, remotePos ltx.Pos) (newPos ltx.Pos, err error) {\n\tslog.Debug(\"sync database to backup\", slog.String(\"name\", name))\n\n\tdb := s.DB(name)\n\tif db == nil {\n\t\t// TODO: Handle database deletion\n\t\tslog.Warn(\"restoring from backup\", slog.String(\"name\", name), slog.String(\"reason\", \"no-local\"))\n\t\treturn ltx.Pos{}, ltx.NewPosMismatchError(remotePos)\n\t}\n\n\t// Check local replication position.\n\t// If we haven't written anything yet then try to send data.\n\tlocalPos := db.Pos()\n\tif localPos.IsZero() {\n\t\treturn localPos, nil\n\t}\n\n\t// If the database doesn't exist remotely, perform a full snapshot.\n\tif remotePos.IsZero() {\n\t\treturn s.streamBackupDBSnapshot(ctx, db)\n\t}\n\n\t// If the position from the backup server is ahead of the primary then we\n\t// need to perform a recovery so that we snapshot from the backup server.\n\tif remotePos.TXID > localPos.TXID {\n\t\tslog.Warn(\"restoring from backup\",\n\t\t\tslog.String(\"name\", name),\n\t\t\tslog.Group(\"pos\",\n\t\t\t\tslog.String(\"local\", localPos.String()),\n\t\t\t\tslog.String(\"remote\", remotePos.String()),\n\t\t\t),\n\t\t\tslog.String(\"reason\", \"remote-ahead\"),\n\t\t)\n\t\tlog.Printf(\"backup of database %q is ahead of local copy, restoring from backup\", name)\n\t\treturn ltx.Pos{}, ltx.NewPosMismatchError(remotePos) // backup TXID ahead of primary, needs recovery\n\t}\n\n\t// If the TXID matches the backup server, we need to ensure the checksum\n\t// does as well. If it doesn't, we need to grab a snapshot from the backup\n\t// server. If it does, then we can exit as we're already in sync.\n\tif remotePos.TXID == localPos.TXID {\n\t\tif remotePos.PostApplyChecksum != localPos.PostApplyChecksum {\n\t\t\tslog.Warn(\"restoring from backup\",\n\t\t\t\tslog.String(\"name\", name),\n\t\t\t\tslog.Group(\"pos\",\n\t\t\t\t\tslog.String(\"local\", localPos.String()),\n\t\t\t\t\tslog.String(\"remote\", remotePos.String()),\n\t\t\t\t),\n\t\t\t\tslog.String(\"reason\", \"chksum-mismatch\"),\n\t\t\t)\n\t\t\treturn ltx.Pos{}, ltx.NewPosMismatchError(remotePos) // same TXID, different checksum\n\t\t}\n\n\t\tslog.Debug(\"database in sync with backup, skipping\", slog.String(\"name\", name))\n\t\treturn localPos, nil // already in sync\n\t}\n\n\tassert(remotePos.TXID < localPos.TXID, \"remote/local position must be ordered\")\n\n\t// OPTIMIZE: Check that remote postApplyChecksum equals next TXID's preApplyChecksum\n\n\t// Collect all transaction files to catch up from remote position to current position.\n\tvar rdrs []io.Reader\n\tdefer func() {\n\t\tfor _, r := range rdrs {\n\t\t\t_ = r.(io.Closer).Close()\n\t\t}\n\t}()\n\n\tfor txID, n := remotePos.TXID+1, 0; txID <= localPos.TXID && n < MaxBackupLTXFileN; txID, n = txID+1, n+1 {\n\t\tf, err := db.OpenLTXFile(txID)\n\t\tif os.IsNotExist(err) {\n\t\t\tslog.Warn(\"restoring from backup\",\n\t\t\t\tslog.String(\"name\", name),\n\t\t\t\tslog.Group(\"pos\",\n\t\t\t\t\tslog.String(\"local\", localPos.String()),\n\t\t\t\t\tslog.String(\"remote\", remotePos.String()),\n\t\t\t\t),\n\t\t\t\tslog.String(\"txid\", txID.String()),\n\t\t\t\tslog.String(\"reason\", \"ltx-not-found\"),\n\t\t\t)\n\t\t\treturn ltx.Pos{}, ltx.NewPosMismatchError(remotePos)\n\t\t} else if err != nil {\n\t\t\treturn ltx.Pos{}, fmt.Errorf(\"open ltx file: %w\", err)\n\t\t}\n\t\trdrs = append(rdrs, f)\n\t}\n\n\t// Compact LTX files through a pipe so we can pass it to the backup client.\n\tpr, pw := io.Pipe()\n\tvar pos ltx.Pos\n\tgo func() {\n\t\tcompactor := ltx.NewCompactor(pw, rdrs)\n\t\tcompactor.HeaderFlags = s.ltxHeaderFlags()\n\t\tif err := compactor.Compact(ctx); err != nil {\n\t\t\t_ = pw.CloseWithError(err)\n\t\t\treturn\n\t\t}\n\n\t\tpos = ltx.NewPos(compactor.Header().MaxTXID, compactor.Trailer().PostApplyChecksum)\n\t\t_ = pw.Close()\n\t}()\n\n\tvar pmErr *ltx.PosMismatchError\n\thwm, err := s.BackupClient.WriteTx(ctx, name, pr)\n\tif errors.As(err, &pmErr) {\n\t\tslog.Warn(\"restoring from backup\",\n\t\t\tslog.String(\"name\", name),\n\t\t\tslog.Group(\"pos\",\n\t\t\t\tslog.String(\"local\", localPos.String()),\n\t\t\t\tslog.String(\"remote\", pmErr.Pos.String()),\n\t\t\t),\n\t\t\tslog.String(\"reason\", \"out-of-sync\"),\n\t\t)\n\t\treturn ltx.Pos{}, pmErr\n\t} else if err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"write backup tx: %w\", err)\n\t}\n\tdb.SetHWM(hwm)\n\n\treturn pos, nil\n}\n\n// streamBackupDBSnapshot writes the entire snapshot to the backup client.\n// This is done when no data exists on the remote backup for the database.\nfunc (s *Store) streamBackupDBSnapshot(ctx context.Context, db *DB) (newPos ltx.Pos, err error) {\n\tvar v atomic.Value\n\tv.Store(ltx.Pos{})\n\n\t// Run snapshot through a goroutine so we can pipe it to the backup writer.\n\tpr, pw := io.Pipe()\n\tgo func() {\n\t\theader, trailer, err := db.WriteSnapshotTo(ctx, pw)\n\t\tv.Store(ltx.NewPos(header.MaxTXID, trailer.PostApplyChecksum))\n\t\t_ = pw.CloseWithError(err)\n\t}()\n\n\thwm, err := s.BackupClient.WriteTx(ctx, db.Name(), pr)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"write backup tx snapshot: %w\", err)\n\t}\n\tdb.SetHWM(hwm)\n\n\tpos := v.Load().(ltx.Pos)\n\treturn pos, nil\n}\n\n// restoreDBFromBackup pulls the current snapshot from the backup service and\n// restores it to the local database. The backup service acts as the data\n// authority so this occurs when we cannot provide a contiguous series of\n// transaction files to the backup service.\nfunc (s *Store) restoreDBFromBackup(ctx context.Context, name string) (newPos ltx.Pos, err error) {\n\t// Read the snapshot from the backup service.\n\trc, err := s.BackupClient.FetchSnapshot(ctx, name)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"fetch backup snapshot: %w\", err)\n\t}\n\tdefer func() { _ = rc.Close() }()\n\n\t// Create the database if it doesn't exist.\n\tdb, err := s.CreateDBIfNotExists(name)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"create database: %s\", err)\n\t}\n\n\tt := time.Now()\n\tslog.Debug(\"beginning database restore from backup\",\n\t\tslog.String(\"name\", name),\n\t\tslog.String(\"prev_pos\", db.Pos().String()),\n\t)\n\n\t// Acquire the write lock so we can apply the snapshot.\n\tguard, err := db.AcquireWriteLock(ctx, nil)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"acquire write lock: %s\", err)\n\t}\n\tdefer guard.Unlock()\n\n\tif err := db.recover(ctx); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"recover: %s\", err)\n\t}\n\n\t// Wrap request body in a chunked reader.\n\tltxPath, err := db.WriteLTXFileAt(ctx, rc)\n\tif err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"write ltx file: %s\", err)\n\t}\n\n\t// Apply transaction to database.\n\tif err := db.ApplyLTXNoLock(ltxPath, true); err != nil {\n\t\treturn ltx.Pos{}, fmt.Errorf(\"cannot apply ltx: %s\", err)\n\t}\n\tnewPos = db.Pos()\n\n\tslog.Warn(\"database restore complete\",\n\t\tslog.String(\"name\", name),\n\t\tslog.String(\"pos\", newPos.String()),\n\t\tslog.Duration(\"elapsed\", time.Since(t)),\n\t)\n\n\treturn newPos, nil\n}\n\nfunc (s *Store) processHandoff(ctx context.Context, nodeID uint64, lease Lease) error {\n\t// Find subscriber to ensure it is still connected.\n\tsub := s.SubscriberByNodeID(nodeID)\n\tif sub == nil {\n\t\treturn fmt.Errorf(\"node is no longer connected\")\n\t}\n\n\t// Renew the lease one last time before handing off.\n\tif err := lease.Renew(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tctx, cancel := context.WithTimeoutCause(ctx, 5*time.Second, fmt.Errorf(\"handoff processing timeout\"))\n\tdefer cancel()\n\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn context.Cause(ctx)\n\tcase sub.HandoffCh() <- lease.ID():\n\t\treturn nil\n\t}\n}\n\n// monitorLeaseAsReplica tries to connect to the primary node and stream down changes.\nfunc (s *Store) monitorLeaseAsReplica(ctx context.Context, info PrimaryInfo) (handoffLeaseID string, err error) {\n\tif s.Client == nil {\n\t\treturn \"\", fmt.Errorf(\"no client set, skipping replica monitor\")\n\t}\n\n\t// Store the URL of the primary while we're in this function.\n\ts.mu.Lock()\n\ts.setPrimaryInfo(&info)\n\ts.mu.Unlock()\n\n\t// Clear the primary URL once we leave this function since we can no longer connect.\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tdefer s.mu.Unlock()\n\t\ts.setPrimaryInfo(nil)\n\t}()\n\n\tposMap := s.PosMap()\n\tst, err := s.Client.Stream(ctx, info.AdvertiseURL, s.id, posMap, s.DatabaseFilter)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"connect to primary: %s ('%s')\", err, info.AdvertiseURL)\n\t}\n\tdefer func() { _ = st.Close() }()\n\n\t// Adopt cluster ID from primary node if we don't have a cluster ID yet.\n\tif s.ClusterID() == \"\" && st.ClusterID() != \"\" {\n\t\tif err := s.setClusterID(st.ClusterID()); err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"set local cluster id: %w\", err)\n\t\t}\n\t}\n\n\t// Verify that we are attaching onto a node in the same cluster.\n\tif s.ClusterID() != st.ClusterID() {\n\t\treturn \"\", fmt.Errorf(\"cannot stream from primary with a different cluster id: %s <> %s\", s.ClusterID(), st.ClusterID())\n\t}\n\n\tfor {\n\t\tframe, err := ReadStreamFrame(st)\n\t\tif err == io.EOF {\n\t\t\treturn \"\", nil // clean disconnect\n\t\t} else if err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"next frame: %w\", err)\n\t\t}\n\n\t\tswitch frame := frame.(type) {\n\t\tcase *LTXStreamFrame:\n\t\t\tif err := s.processLTXStreamFrame(ctx, frame, chunk.NewReader(st)); err != nil {\n\t\t\t\treturn \"\", fmt.Errorf(\"process ltx stream frame: %w\", err)\n\t\t\t}\n\t\tcase *ReadyStreamFrame:\n\t\t\t// Mark store as ready once we've received an initial replication set.\n\t\t\ts.markReady()\n\t\tcase *EndStreamFrame:\n\t\t\t// Server cleanly disconnected\n\t\t\treturn \"\", nil\n\t\tcase *DropDBStreamFrame:\n\t\t\tlog.Printf(\"deprecated drop db frame received, skipping\")\n\t\tcase *HandoffStreamFrame:\n\t\t\treturn frame.LeaseID, nil\n\t\tcase *HWMStreamFrame:\n\t\t\tif db := s.DB(frame.Name); db != nil {\n\t\t\t\tdb.SetHWM(frame.TXID)\n\t\t\t}\n\t\tcase *HeartbeatStreamFrame:\n\t\t\ts.setPrimaryTimestamp(frame.Timestamp)\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid stream frame type: 0x%02x\", frame.Type())\n\t\t}\n\t}\n}\n\n// monitorRetention periodically enforces retention of LTX files on the databases.\nfunc (s *Store) monitorRetention(ctx context.Context) error {\n\tticker := time.NewTicker(s.RetentionMonitorInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase <-ticker.C:\n\t\t\tif err := s.EnforceRetention(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\n// monitorHaltLock periodically check all halt locks for expiration.\nfunc (s *Store) monitorHaltLock(ctx context.Context) error {\n\tticker := time.NewTicker(s.HaltLockMonitorInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase <-ticker.C:\n\t\t\ts.EnforceHaltLockExpiration(ctx)\n\t\t}\n\t}\n}\n\n// EnforceHaltLockExpiration expires any overdue HALT locks.\nfunc (s *Store) EnforceHaltLockExpiration(ctx context.Context) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tfor _, db := range s.dbs {\n\t\tdb.EnforceHaltLockExpiration(ctx)\n\t}\n}\n\n// Recover forces a rollback (journal) or checkpoint (wal) on all open databases.\n// This is done when switching the primary/replica state.\nfunc (s *Store) Recover(ctx context.Context) (err error) {\n\tfor _, db := range s.DBs() {\n\t\tif err := db.Recover(ctx); err != nil {\n\t\t\treturn fmt.Errorf(\"db %q: %w\", db.Name(), err)\n\t\t}\n\t}\n\treturn nil\n}\n\n// EnforceRetention enforces retention of LTX files on all databases.\nfunc (s *Store) EnforceRetention(ctx context.Context) (err error) {\n\t// Skip enforcement if not set.\n\tif s.Retention <= 0 {\n\t\treturn nil\n\t}\n\n\tminTime := time.Now().Add(-s.Retention).UTC()\n\n\tfor _, db := range s.DBs() {\n\t\tif e := db.EnforceRetention(ctx, minTime); err == nil {\n\t\t\terr = fmt.Errorf(\"cannot enforce retention on db %q: %w\", db.Name(), e)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (s *Store) processLTXStreamFrame(ctx context.Context, frame *LTXStreamFrame, src io.Reader) (err error) {\n\tdb, err := s.CreateDBIfNotExists(frame.Name)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"create database: %w\", err)\n\t}\n\n\thdr, data, err := ltx.DecodeHeader(src)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"peek ltx header: %w\", err)\n\t}\n\tsrc = io.MultiReader(bytes.NewReader(data), src)\n\n\tTraceLog.Printf(\"[ProcessLTXStreamFrame.Begin(%s)]: txid=%s-%s, preApplyChecksum=%s\", db.Name(), hdr.MinTXID.String(), hdr.MaxTXID.String(), hdr.PreApplyChecksum)\n\tdefer func() {\n\t\tTraceLog.Printf(\"[ProcessLTXStreamFrame.End(%s)]: %s\", db.name, errorKeyValue(err))\n\t}()\n\n\t// Acquire lock unless we are waiting for a database position, in which case,\n\t// we already have the lock.\n\tguardSet, err := db.AcquireWriteLock(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer guardSet.Unlock()\n\n\t// Skip frame if it already occurred on this node. This can happen if the\n\t// replica node created the transaction and forwarded it to the primary.\n\tif hdr.NodeID == s.ID() {\n\t\tdec := ltx.NewDecoder(src)\n\t\tif err := dec.Verify(); err != nil {\n\t\t\treturn fmt.Errorf(\"verify duplicate ltx file: %w\", err)\n\t\t}\n\t\tif _, err := io.Copy(io.Discard, src); err != nil {\n\t\t\treturn fmt.Errorf(\"discard ltx body: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// If we receive an LTX file while holding the remote HALT lock then the\n\t// remote lock must have expired or been released so we can clear it locally.\n\t//\n\t// We also hold the local WRITE lock so a local write cannot be in-progress.\n\tif haltLock := db.RemoteHaltLock(); haltLock != nil {\n\t\tTraceLog.Printf(\"[ProcessLTXStreamFrame.Unhalt(%s)]: replica holds HALT lock but received LTX file, unsetting HALT lock\", db.Name())\n\t\tif err := db.UnsetRemoteHaltLock(ctx, haltLock.ID); err != nil {\n\t\t\treturn fmt.Errorf(\"release remote halt lock: %w\", err)\n\t\t}\n\t}\n\n\t// Verify LTX file pre-apply checksum matches the current database position\n\t// unless this is a snapshot, which will overwrite all data.\n\tif !hdr.IsSnapshot() {\n\t\texpectedPos := ltx.Pos{\n\t\t\tTXID:              hdr.MinTXID - 1,\n\t\t\tPostApplyChecksum: hdr.PreApplyChecksum,\n\t\t}\n\t\tif pos := db.Pos(); pos != expectedPos {\n\t\t\treturn fmt.Errorf(\"position mismatch on db %q: %s <> %s\", db.Name(), pos, expectedPos)\n\t\t}\n\t}\n\n\t// Write LTX file to a temporary file and we'll atomically rename later.\n\tpath := db.LTXPath(hdr.MinTXID, hdr.MaxTXID)\n\ttmpPath := fmt.Sprintf(\"%s.%d.tmp\", path, rand.Int())\n\tdefer func() { _ = s.OS.Remove(\"PROCESSLTX\", tmpPath) }()\n\n\tf, err := s.OS.Create(\"PROCESSLTX\", tmpPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot create temp ltx file: %w\", err)\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tn, err := io.Copy(f, src)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"write ltx file: %w\", err)\n\t} else if err := f.Sync(); err != nil {\n\t\treturn fmt.Errorf(\"fsync ltx file: %w\", err)\n\t}\n\n\t// Atomically rename file.\n\tif err := s.OS.Rename(\"PROCESSLTX\", tmpPath, path); err != nil {\n\t\treturn fmt.Errorf(\"rename ltx file: %w\", err)\n\t} else if err := internal.Sync(filepath.Dir(path)); err != nil {\n\t\treturn fmt.Errorf(\"sync ltx dir: %w\", err)\n\t}\n\n\t// Update metrics\n\tdbLTXCountMetricVec.WithLabelValues(db.Name()).Inc()\n\tdbLTXBytesMetricVec.WithLabelValues(db.Name()).Set(float64(n))\n\n\t// Remove other LTX files after a snapshot.\n\tif hdr.IsSnapshot() {\n\t\tdir, file := filepath.Split(path)\n\t\tlog.Printf(\"snapshot received for %q, removing other ltx files: %s\", db.Name(), file)\n\t\tif err := removeFilesExcept(s.OS, dir, file); err != nil {\n\t\t\treturn fmt.Errorf(\"remove ltx except snapshot: %w\", err)\n\t\t}\n\t}\n\n\t// Attempt to apply the LTX file to the database.\n\tif err := db.ApplyLTXNoLock(path, true); err != nil {\n\t\treturn fmt.Errorf(\"apply ltx: %w\", err)\n\t}\n\n\t// Don't consider ltx timestamps for PrimaryTimestamp until initial\n\t// replication is finished. Users might assume databases are up-to-date\n\t// when they're not.\n\tif s.isReady() {\n\t\ts.setPrimaryTimestamp(hdr.Timestamp)\n\t}\n\n\treturn nil\n}\n\n// ltxHeaderFlags returns flags used for the LTX header.\nfunc (s *Store) ltxHeaderFlags() uint32 {\n\tvar flags uint32\n\tif s.Compress {\n\t\tflags |= ltx.HeaderFlagCompressLZ4\n\t}\n\treturn flags\n}\n\n// PrimaryTimestamp returns the last timestamp (ms since epoch) received from\n// the primary. Returns -1 if we are the primary or if we haven't finished\n// initial replication yet.\nfunc (s *Store) PrimaryTimestamp() int64 {\n\treturn s.primaryTimestamp.Load()\n}\n\n// 0 means we're primary. -1 means we're a new replica\nfunc (s *Store) setPrimaryTimestamp(ts int64) {\n\ts.primaryTimestamp.Store(ts)\n\n\tif invalidator := s.Invalidator; invalidator != nil {\n\t\tif err := invalidator.InvalidateLag(); err != nil {\n\t\t\tlog.Printf(\"error invalidating .lag cache: %s\\n\", err)\n\t\t}\n\t}\n}\n\n// Lag returns the number of seconds that the local instance is lagging\n// behind the primary node. Returns 0 if the node is the primary or if the\n// node is not marked as ready yet.\nfunc (s *Store) Lag() time.Duration {\n\tswitch ts := s.PrimaryTimestamp(); ts {\n\tcase 0, -1:\n\t\treturn 0 // primary or not ready\n\tdefault:\n\t\treturn time.Duration(time.Now().UnixMilli()-ts) * time.Millisecond\n\t}\n}\n\n// Expvar returns a variable for debugging output.\nfunc (s *Store) Expvar() expvar.Var { return (*StoreVar)(s) }\n\nvar _ expvar.Var = (*StoreVar)(nil)\n\ntype StoreVar Store\n\nfunc (v *StoreVar) String() string {\n\ts := (*Store)(v)\n\tm := &storeVarJSON{\n\t\tIsPrimary: s.IsPrimary(),\n\t\tCandidate: s.candidate,\n\t\tDBs:       make(map[string]*dbVarJSON),\n\t}\n\n\tfor _, db := range s.DBs() {\n\t\tpos := db.Pos()\n\n\t\tdbJSON := &dbVarJSON{\n\t\t\tName:     db.Name(),\n\t\t\tTXID:     pos.TXID.String(),\n\t\t\tChecksum: pos.PostApplyChecksum.String(),\n\t\t}\n\n\t\tdbJSON.Locks.Pending = db.pendingLock.State().String()\n\t\tdbJSON.Locks.Shared = db.sharedLock.State().String()\n\t\tdbJSON.Locks.Reserved = db.reservedLock.State().String()\n\n\t\tdbJSON.Locks.Write = db.writeLock.State().String()\n\t\tdbJSON.Locks.Ckpt = db.ckptLock.State().String()\n\t\tdbJSON.Locks.Recover = db.recoverLock.State().String()\n\t\tdbJSON.Locks.Read0 = db.read0Lock.State().String()\n\t\tdbJSON.Locks.Read1 = db.read1Lock.State().String()\n\t\tdbJSON.Locks.Read2 = db.read2Lock.State().String()\n\t\tdbJSON.Locks.Read3 = db.read3Lock.State().String()\n\t\tdbJSON.Locks.Read4 = db.read4Lock.State().String()\n\t\tdbJSON.Locks.DMS = db.dmsLock.State().String()\n\n\t\tm.DBs[db.Name()] = dbJSON\n\t}\n\n\tb, err := json.Marshal(m)\n\tif err != nil {\n\t\treturn \"null\"\n\t}\n\treturn string(b)\n}\n\ntype storeVarJSON struct {\n\tIsPrimary bool                  `json:\"isPrimary\"`\n\tCandidate bool                  `json:\"candidate\"`\n\tDBs       map[string]*dbVarJSON `json:\"dbs\"`\n}\n\n// ChangeSetSubscriber subscribes to changes to databases in the store.\n//\n// It implements a set of \"dirty\" databases instead of a channel of all events\n// as clients can be slow and we don't want to cause channels to back up. It\n// is the responsibility of the caller to determine the state changes which is\n// usually just checking the position of the client versus the store's database.\ntype ChangeSetSubscriber struct {\n\tstore  *Store\n\tnodeID uint64\n\n\tmu        sync.Mutex\n\tnotifyCh  chan struct{}\n\tdirtySet  map[string]struct{}\n\thandoffCh chan string\n}\n\n// newChangeSetSubscriber returns a new instance of Subscriber associated with a store.\nfunc newChangeSetSubscriber(store *Store, nodeID uint64) *ChangeSetSubscriber {\n\ts := &ChangeSetSubscriber{\n\t\tstore:     store,\n\t\tnodeID:    nodeID,\n\t\tnotifyCh:  make(chan struct{}, 1),\n\t\tdirtySet:  make(map[string]struct{}),\n\t\thandoffCh: make(chan string),\n\t}\n\treturn s\n}\n\n// Close removes the subscriber from the store.\nfunc (s *ChangeSetSubscriber) Close() error {\n\ts.store.UnsubscribeChangeSet(s)\n\treturn nil\n}\n\n// NodeID returns the ID of the subscribed node.\nfunc (s *ChangeSetSubscriber) NodeID() uint64 { return s.nodeID }\n\n// NotifyCh returns a channel that receives a value when the dirty set has changed.\nfunc (s *ChangeSetSubscriber) NotifyCh() <-chan struct{} { return s.notifyCh }\n\n// HandoffCh returns a channel that returns a lease ID on handoff.\nfunc (s *ChangeSetSubscriber) HandoffCh() chan string { return s.handoffCh }\n\n// MarkDirty marks a database ID as dirty.\nfunc (s *ChangeSetSubscriber) MarkDirty(name string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.dirtySet[name] = struct{}{}\n\n\tselect {\n\tcase s.notifyCh <- struct{}{}:\n\tdefault:\n\t}\n}\n\n// DirtySet returns a set of database IDs that have changed since the last call\n// to DirtySet(). This call clears the set.\nfunc (s *ChangeSetSubscriber) DirtySet() map[string]struct{} {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tdirtySet := s.dirtySet\n\ts.dirtySet = make(map[string]struct{})\n\treturn dirtySet\n}\n\nconst EventChannelBufferSize = 1024\n\n// EventSubscriber subscribes to generic store events.\ntype EventSubscriber struct {\n\tstore *Store\n\tch    chan Event\n}\n\n// newEventSubscriber returns a new instance of Subscriber associated with a store.\nfunc newEventSubscriber(store *Store) *EventSubscriber {\n\treturn &EventSubscriber{\n\t\tstore: store,\n\t\tch:    make(chan Event, EventChannelBufferSize),\n\t}\n}\n\n// Stop closes the subscriber and removes it from the store.\nfunc (s *EventSubscriber) Stop() {\n\ts.store.UnsubscribeEvents(s)\n}\n\n// C returns a channel that receives event notifications.\n// If caller cannot read events fast enough then channel will be closed.\nfunc (s *EventSubscriber) C() <-chan Event { return s.ch }\n\nconst (\n\tEventTypeInit          = \"init\"\n\tEventTypeTx            = \"tx\"\n\tEventTypePrimaryChange = \"primaryChange\"\n)\n\n// Event represents a generic event.\ntype Event struct {\n\tType string `json:\"type\"`\n\tDB   string `json:\"db,omitempty\"`\n\tData any    `json:\"data,omitempty\"`\n}\n\nfunc (e *Event) UnmarshalJSON(data []byte) error {\n\tvar v eventJSON\n\tif err := json.Unmarshal(data, &v); err != nil {\n\t\treturn err\n\t}\n\n\te.Type = v.Type\n\te.DB = v.DB\n\n\tswitch v.Type {\n\tcase EventTypeInit:\n\t\te.Data = &InitEventData{}\n\tcase EventTypeTx:\n\t\te.Data = &TxEventData{}\n\tcase EventTypePrimaryChange:\n\t\te.Data = &PrimaryChangeEventData{}\n\tdefault:\n\t\te.Data = nil\n\t}\n\tif err := json.Unmarshal(v.Data, &e.Data); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\ntype eventJSON struct {\n\tType string          `json:\"type\"`\n\tDB   string          `json:\"db,omitempty\"`\n\tData json.RawMessage `json:\"data,omitempty\"`\n}\n\ntype InitEventData struct {\n\tIsPrimary bool   `json:\"isPrimary\"`\n\tHostname  string `json:\"hostname,omitempty\"`\n}\n\ntype TxEventData struct {\n\tTXID              ltx.TXID     `json:\"txID\"`\n\tPostApplyChecksum ltx.Checksum `json:\"postApplyChecksum\"`\n\tPageSize          uint32       `json:\"pageSize\"`\n\tCommit            uint32       `json:\"commit\"`\n\tTimestamp         time.Time    `json:\"timestamp\"`\n}\n\ntype PrimaryChangeEventData struct {\n\tIsPrimary bool   `json:\"isPrimary\"`\n\tHostname  string `json:\"hostname,omitempty\"`\n}\n\nvar _ context.Context = (*primaryCtx)(nil)\n\n// primaryCtx represents a context that is marked done when the node loses its primary status.\ntype primaryCtx struct {\n\tparent    context.Context\n\tprimaryCh chan struct{}\n\tdone      chan struct{}\n}\n\nfunc newPrimaryCtx(parent context.Context, primaryCh chan struct{}) *primaryCtx {\n\tctx := &primaryCtx{\n\t\tparent:    parent,\n\t\tprimaryCh: primaryCh,\n\t\tdone:      make(chan struct{}),\n\t}\n\n\tgo func() {\n\t\tselect {\n\t\tcase <-ctx.primaryCh:\n\t\t\tclose(ctx.done)\n\t\tcase <-ctx.parent.Done():\n\t\t\tclose(ctx.done)\n\t\t}\n\t}()\n\n\treturn ctx\n}\n\nfunc (ctx *primaryCtx) Deadline() (deadline time.Time, ok bool) {\n\treturn ctx.parent.Deadline()\n}\n\nfunc (ctx *primaryCtx) Done() <-chan struct{} {\n\treturn ctx.done\n}\n\nfunc (ctx *primaryCtx) Err() error {\n\tselect {\n\tcase <-ctx.primaryCh:\n\t\treturn ErrLeaseExpired\n\tdefault:\n\t\treturn ctx.parent.Err()\n\t}\n}\n\nfunc (ctx *primaryCtx) Value(key any) any {\n\treturn ctx.parent.Value(key)\n}\n\n// removeFilesExcept removes all files from a directory except a given filename.\n// Attempts to remove all files, even in the event of an error. Returns the\n// first error encountered.\nfunc removeFilesExcept(osys OS, dir, filename string) (retErr error) {\n\tents, err := osys.ReadDir(\"REMOVEFILESEXCEPT\", dir)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, ent := range ents {\n\t\t// Skip directories & exception file.\n\t\tif ent.IsDir() || ent.Name() == filename {\n\t\t\tcontinue\n\t\t}\n\t\tif err := osys.Remove(\"REMOVEFILESEXCEPT\", filepath.Join(dir, ent.Name())); retErr == nil {\n\t\t\tretErr = err\n\t\t}\n\t}\n\n\treturn retErr\n}\n\n// sleepWithContext sleeps for a given amount of time or until the context is canceled.\nfunc sleepWithContext(ctx context.Context, d time.Duration) {\n\t// Skip timer creation if context is already canceled.\n\tif ctx.Err() != nil {\n\t\treturn\n\t}\n\n\ttimer := time.NewTimer(d)\n\tdefer timer.Stop()\n\n\tselect {\n\tcase <-ctx.Done():\n\tcase <-timer.C:\n\t}\n}\n\n// Store metrics.\nvar (\n\tstoreDBCountMetric = promauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"litefs_db_count\",\n\t\tHelp: \"Number of managed databases.\",\n\t})\n\n\tstoreIsPrimaryMetric = promauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"litefs_is_primary\",\n\t\tHelp: \"Primary status of the node.\",\n\t})\n\n\tstoreSubscriberCountMetric = promauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"litefs_subscriber_count\",\n\t\tHelp: \"Number of connected subscribers\",\n\t})\n\n\t_ = promauto.NewGaugeFunc(prometheus.GaugeOpts{\n\t\tName: \"litefs_lag_seconds\",\n\t\tHelp: \"Lag behind the primary node, in seconds\",\n\t}, func() float64 {\n\t\tif s := GlobalStore.Load(); s != nil {\n\t\t\treturn s.(*Store).Lag().Seconds()\n\t\t}\n\t\treturn 0\n\t})\n)\n"
        },
        {
          "name": "store_test.go",
          "type": "blob",
          "size": 9.1220703125,
          "content": "package litefs_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/superfly/litefs\"\n\t\"github.com/superfly/litefs/internal/testingutil\"\n\t\"github.com/superfly/litefs/mock\"\n\t\"github.com/superfly/ltx\"\n)\n\n// Ensure store can create a new, empty database.\nfunc TestStore_CreateDB(t *testing.T) {\n\tstore := newOpenStore(t, newPrimaryStaticLeaser(), nil)\n\n\t// Database should be empty.\n\tdb, f, err := store.CreateDB(\"test1.db\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if err := f.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif got, want := db.Name(), \"test1.db\"; got != want {\n\t\tt.Fatalf(\"Name=%s, want %s\", got, want)\n\t}\n\tif got, want := db.Pos(), (ltx.Pos{}); !reflect.DeepEqual(got, want) {\n\t\tt.Fatalf(\"Pos=%#v, want %#v\", got, want)\n\t}\n\tif got, want := db.TXID(), ltx.TXID(0); !reflect.DeepEqual(got, want) {\n\t\tt.Fatalf(\"TXID=%#v, want %#v\", got, want)\n\t}\n\tif got, want := db.Path(), filepath.Join(store.Path(), \"dbs\", \"test1.db\"); got != want {\n\t\tt.Fatalf(\"Path=%s, want %s\", got, want)\n\t}\n\tif got, want := db.LTXDir(), filepath.Join(store.Path(), \"dbs\", \"test1.db\", \"ltx\"); got != want {\n\t\tt.Fatalf(\"LTXDir=%s, want %s\", got, want)\n\t}\n\tif got, want := db.LTXPath(1, 2), filepath.Join(store.Path(), \"dbs\", \"test1.db\", \"ltx\", \"0000000000000001-0000000000000002.ltx\"); got != want {\n\t\tt.Fatalf(\"LTXPath=%s, want %s\", got, want)\n\t}\n\n\t// Ensure we can create another database.\n\t_, f, err = store.CreateDB(\"test2.db\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if err := f.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestStore_Open(t *testing.T) {\n\tt.Run(\"ExistingEmptyDB\", func(t *testing.T) {\n\t\tstore := newStoreFromFixture(t, newPrimaryStaticLeaser(), nil, \"testdata/store/open-name-only\")\n\t\tif err := store.Open(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tdb := store.DB(\"test.db\")\n\t\tif db == nil {\n\t\t\tt.Fatal(\"expected database\")\n\t\t}\n\t\tif got, want := db.Name(), \"test.db\"; got != want {\n\t\t\tt.Fatalf(\"name=%v, want %v\", got, want)\n\t\t}\n\t\tif got, want := db.Pos(), (ltx.Pos{}); got != want {\n\t\t\tt.Fatalf(\"pos=%v, want %v\", got, want)\n\t\t}\n\n\t\t// Ensure next database uses the next highest identifier.\n\t\tdb, f, err := store.CreateDB(\"test2.db\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := f.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif got, want := db.Name(), \"test2.db\"; got != want {\n\t\t\tt.Fatalf(\"Name=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"InvalidDatabaseHeader\", func(t *testing.T) {\n\t\tstore := newStoreFromFixture(t, newPrimaryStaticLeaser(), nil, \"testdata/store/open-invalid-database-header\")\n\t\tif err := store.Open(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tdb := store.DB(\"test.db\")\n\t\tif db == nil {\n\t\t\tt.Fatal(\"expected database\")\n\t\t}\n\t\tif got, want := db.Name(), \"test.db\"; got != want {\n\t\t\tt.Fatalf(\"name=%v, want %v\", got, want)\n\t\t}\n\t\tif got, want := db.Pos(), (ltx.Pos{}); got != want {\n\t\t\tt.Fatalf(\"pos=%v, want %v\", got, want)\n\t\t}\n\t})\n\n\tt.Run(\"ShortDatabase\", func(t *testing.T) {\n\t\tstore := newStoreFromFixture(t, newPrimaryStaticLeaser(), nil, \"testdata/store/open-short-database\")\n\t\tif err := store.Open(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tdb := store.DB(\"test.db\")\n\t\tif db == nil {\n\t\t\tt.Fatal(\"expected database\")\n\t\t}\n\t\tif got, want := db.Name(), \"test.db\"; got != want {\n\t\t\tt.Fatalf(\"name=%v, want %v\", got, want)\n\t\t}\n\t\tif got, want := db.Pos(), (ltx.Pos{}); got != want {\n\t\t\tt.Fatalf(\"pos=%v, want %v\", got, want)\n\t\t}\n\t})\n}\n\n// Ensures that an existing database can write a snapshot after open.\n// See: https://github.com/superfly/litefs/issues/173\nfunc TestStore_OpenAndWriteSnapshot(t *testing.T) {\n\tstore := newStoreFromFixture(t, newPrimaryStaticLeaser(), nil, \"testdata/store/open-and-write-snapshot\")\n\tif err := store.Open(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar buf bytes.Buffer\n\tdb := store.DB(\"sqlite.db\")\n\tif _, _, err := db.WriteSnapshotTo(context.Background(), &buf); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdec := ltx.NewDecoder(&buf)\n\tif err := dec.Verify(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestPrimaryInfo_Clone(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tinfo := &litefs.PrimaryInfo{Hostname: \"foo\", AdvertiseURL: \"bar\"}\n\t\tif other := info.Clone(); *other != *info {\n\t\t\tt.Fatal(\"mismatch\")\n\t\t}\n\t})\n\tt.Run(\"Nil\", func(t *testing.T) {\n\t\tvar info *litefs.PrimaryInfo\n\t\tif other := info.Clone(); other != nil {\n\t\t\tt.Fatal(\"expected nil\")\n\t\t}\n\t})\n}\n\n// Ensure store generates a unique ID.\nfunc TestStore_ID(t *testing.T) {\n\tstore := newStore(t, newPrimaryStaticLeaser(), nil)\n\tif err := store.Open(); err != nil {\n\t\tt.Fatal(err)\n\t} else if err := store.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tid := store.ID()\n\tif id == 0 {\n\t\tt.Fatal(\"expected id\")\n\t}\n}\n\n// Ensure store returns a context that is done when node loses primary status.\nfunc TestStore_PrimaryCtx(t *testing.T) {\n\tt.Run(\"InitialPrimary\", func(t *testing.T) {\n\t\tstore := newOpenStore(t, newPrimaryStaticLeaser(), nil)\n\t\tif ctx := store.PrimaryCtx(context.Background()); ctx.Err() != nil {\n\t\t\tt.Fatal(\"expected no error\")\n\t\t}\n\t})\n\n\tt.Run(\"PrimaryLost\", func(t *testing.T) {\n\t\tvar isPrimary atomic.Bool\n\t\tisPrimary.Store(true)\n\n\t\tlease := mock.Lease{\n\t\t\tRenewedAtFunc: func() time.Time { return time.Time{} },\n\t\t\tTTLFunc:       func() time.Duration { return 10 * time.Millisecond },\n\t\t\tRenewFunc: func(ctx context.Context) error {\n\t\t\t\tif !isPrimary.Load() {\n\t\t\t\t\treturn litefs.ErrLeaseExpired\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t},\n\t\t\tHandoffChFunc: func() <-chan uint64 { return nil },\n\t\t\tCloseFunc:     func() error { return nil },\n\t\t}\n\n\t\tvar clusterID string\n\t\tleaser := mock.Leaser{\n\t\t\tCloseFunc:        func() error { return nil },\n\t\t\tAdvertiseURLFunc: func() string { return \"http://localhost:20202\" },\n\t\t\tAcquireFunc: func(ctx context.Context) (litefs.Lease, error) {\n\t\t\t\treturn &lease, nil\n\t\t\t},\n\t\t\tPrimaryInfoFunc: func(ctx context.Context) (litefs.PrimaryInfo, error) {\n\t\t\t\treturn litefs.PrimaryInfo{}, litefs.ErrNoPrimary\n\t\t\t},\n\t\t\tClusterIDFunc: func(ctx context.Context) (string, error) {\n\t\t\t\treturn clusterID, nil\n\t\t\t},\n\t\t\tSetClusterIDFunc: func(ctx context.Context, id string) error {\n\t\t\t\tclusterID = id\n\t\t\t\treturn nil\n\t\t\t},\n\t\t}\n\n\t\tclient := mock.Client{\n\t\t\tStreamFunc: func(ctx context.Context, rawurl string, nodeID uint64, posMap map[string]ltx.Pos, filter []string) (litefs.Stream, error) {\n\t\t\t\treturn &mock.Stream{\n\t\t\t\t\tReadCloser:    io.NopCloser(&bytes.Buffer{}),\n\t\t\t\t\tClusterIDFunc: func() string { return \"\" },\n\t\t\t\t}, nil\n\t\t\t},\n\t\t}\n\n\t\tstore := newOpenStore(t, &leaser, &client)\n\n\t\t// Ensure store starts in primary state.\n\t\tctx := store.PrimaryCtx(context.Background())\n\t\tif ctx.Err() != nil {\n\t\t\tt.Fatal(\"expected no error\")\n\t\t}\n\n\t\t// Mark lease as unrenewable so that store loses lease.\n\t\tisPrimary.Store(false)\n\n\t\t// Check context until it closes.\n\t\ttestingutil.RetryUntil(t, 1*time.Millisecond, 5*time.Second, func() error {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn nil // ok\n\t\t\tdefault:\n\t\t\t\treturn fmt.Errorf(\"expected closed context\")\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"InitialReplica\", func(t *testing.T) {\n\t\tleaser := litefs.NewStaticLeaser(false, \"localhost\", \"http://localhost:20202\")\n\t\tclient := mock.Client{\n\t\t\tStreamFunc: func(ctx context.Context, rawurl string, nodeID uint64, posMap map[string]ltx.Pos, filter []string) (litefs.Stream, error) {\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tif err := litefs.WriteStreamFrame(&buf, &litefs.ReadyStreamFrame{}); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn &mock.Stream{\n\t\t\t\t\tReadCloser:    io.NopCloser(&buf),\n\t\t\t\t\tClusterIDFunc: func() string { return \"\" },\n\t\t\t\t}, nil\n\t\t\t},\n\t\t}\n\n\t\tstore := newOpenStore(t, leaser, &client)\n\t\tctx := store.PrimaryCtx(context.Background())\n\t\tif err := ctx.Err(); err != litefs.ErrLeaseExpired {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ParentCancelation\", func(t *testing.T) {\n\t\tstore := newOpenStore(t, newPrimaryStaticLeaser(), nil)\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tif ctx := store.PrimaryCtx(ctx); ctx.Err() != nil {\n\t\t\tt.Fatal(\"expected no error\")\n\t\t}\n\n\t\t// Cancel & wait for propagation.\n\t\tcancel()\n\t\tselect {\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Fatal(\"timeout waiting for parent cancelation\")\n\t\tcase <-ctx.Done():\n\t\t\tif err := ctx.Err(); err != context.Canceled {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// newStore returns a new instance of a Store on a temporary directory.\n// This store will automatically close when the test ends.\nfunc newStore(tb testing.TB, leaser litefs.Leaser, client litefs.Client) *litefs.Store {\n\tstore := litefs.NewStore(tb.TempDir(), true)\n\tstore.Leaser = leaser\n\tstore.Client = client\n\ttb.Cleanup(func() {\n\t\tif err := store.Close(); err != nil {\n\t\t\ttb.Fatalf(\"cannot close store: %s\", err)\n\t\t}\n\t})\n\treturn store\n}\n\n// newOpenStore returns a new instance of an empty, opened store.\nfunc newOpenStore(tb testing.TB, leaser litefs.Leaser, client litefs.Client) *litefs.Store {\n\ttb.Helper()\n\tstore := newStore(tb, leaser, client)\n\tif err := store.Open(); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\n\tselect {\n\tcase <-time.After(5 * time.Second):\n\t\ttb.Fatal(\"timeout waiting for store ready\")\n\tcase <-store.ReadyCh(): // wait for lease\n\t}\n\treturn store\n}\n\nfunc newStoreFromFixture(tb testing.TB, leaser litefs.Leaser, client litefs.Client, path string) *litefs.Store {\n\ttb.Helper()\n\tstore := newStore(tb, leaser, client)\n\ttestingutil.MustCopyDir(tb, path, store.Path())\n\treturn store\n}\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}