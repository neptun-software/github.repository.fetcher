{
  "metadata": {
    "timestamp": 1736566576194,
    "page": 127,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "alecthomas/chroma",
      "stars": 4433,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.244140625,
          "content": "root = true\n\n[*]\nindent_style = tab\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.xml]\nindent_style = space\nindent_size = 2\ninsert_final_newline = false\n\n[*.yml]\nindent_style = space\nindent_size = 2\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.349609375,
          "content": "# Binaries for programs and plugins\n.git\n.idea\n.vscode\n.hermit\n*.exe\n*.dll\n*.so\n*.dylib\n/cmd/chroma/chroma\n\n# Test binary, build with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n*.out\n\n# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736\n.glide/\n\n_models/\n\n_examples/\n*.min.*\nbuild/\n"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 2.078125,
          "content": "run:\n  tests: true\n  skip-dirs:\n    - _examples\n\noutput:\n  print-issued-lines: false\n\nlinters:\n  enable-all: true\n  disable:\n    - maligned\n    - megacheck\n    - lll\n    - gocyclo\n    - dupl\n    - gochecknoglobals\n    - funlen\n    - godox\n    - wsl\n    - gomnd\n    - gocognit\n    - goerr113\n    - nolintlint\n    - testpackage\n    - godot\n    - nestif\n    - paralleltest\n    - nlreturn\n    - cyclop\n    - exhaustivestruct\n    - gci\n    - gofumpt\n    - errorlint\n    - exhaustive\n    - ifshort\n    - wrapcheck\n    - stylecheck\n    - thelper\n    - nonamedreturns\n    - revive\n    - dupword\n    - exhaustruct\n    - varnamelen\n    - forcetypeassert\n    - ireturn\n    - maintidx\n    - govet\n    - nosnakecase\n    - testableexamples\n    - musttag\n    - depguard\n    - goconst\n    - perfsprint\n    - mnd\n    - predeclared\n\nlinters-settings:\n  govet:\n    check-shadowing: true\n  gocyclo:\n    min-complexity: 10\n  dupl:\n    threshold: 100\n  goconst:\n    min-len: 8\n    min-occurrences: 3\n  forbidigo:\n    #forbid:\n    #  - (Must)?NewLexer$\n    exclude_godoc_examples: false\n\n\nissues:\n  max-per-linter: 0\n  max-same: 0\n  exclude-use-default: false\n  exclude:\n    # Captured by errcheck.\n    - '^(G104|G204):'\n    # Very commonly not checked.\n    - 'Error return value of .(.*\\.Help|.*\\.MarkFlagRequired|(os\\.)?std(out|err)\\..*|.*Close|.*Flush|os\\.Remove(All)?|.*printf?|os\\.(Un)?Setenv). is not checked'\n    - 'exported method (.*\\.MarshalJSON|.*\\.UnmarshalJSON|.*\\.EntityURN|.*\\.GoString|.*\\.Pos) should have comment or be unexported'\n    - 'composite literal uses unkeyed fields'\n    - 'declaration of \"err\" shadows declaration'\n    - 'should not use dot imports'\n    - 'Potential file inclusion via variable'\n    - 'should have comment or be unexported'\n    - 'comment on exported var .* should be of the form'\n    - 'at least one file in a package should have a package comment'\n    - 'string literal contains the Unicode'\n    - 'methods on the same type should have the same receiver name'\n    - '_TokenType_name should be _TokenTypeName'\n    - '`_TokenType_map` should be `_TokenTypeMap`'\n    - 'rewrite if-else to switch statement'\n"
        },
        {
          "name": ".goreleaser.yml",
          "type": "blob",
          "size": 0.705078125,
          "content": "project_name: chroma\nrelease:\n  github:\n    owner: alecthomas\n    name: chroma\nbrews:\n  -\n    install: bin.install \"chroma\"\nenv:\n  - CGO_ENABLED=0\nbuilds:\n- goos:\n    - linux\n    - darwin\n    - windows\n  goarch:\n    - arm64\n    - amd64\n    - \"386\"\n  goarm:\n    - \"6\"\n  dir: ./cmd/chroma\n  main: .\n  ldflags: -s -w -X main.version={{.Version}} -X main.commit={{.Commit}} -X main.date={{.Date}}\n  binary: chroma\narchives:\n  -\n    format: tar.gz\n    name_template: '{{ .Binary }}-{{ .Version }}-{{ .Os }}-{{ .Arch }}{{ if .Arm }}v{{\n    .Arm }}{{ end }}'\n    files:\n      - COPYING\n      - README*\nsnapshot:\n  name_template: SNAPSHOT-{{ .Commit }}\nchecksum:\n  name_template: '{{ .ProjectName }}-{{ .Version }}-checksums.txt'\n"
        },
        {
          "name": "Bitfile",
          "type": "blob",
          "size": 0.673828125,
          "content": "VERSION = %(git describe --tags --dirty  --always)%\nexport CGOENABLED = 0\n\ntokentype_enumer.go: types.go\n  build: go generate\n\n# Regenerate the list of lexers in the README\nREADME.md: lexers/*.go lexers/*/*.xml table.py\n  build: ./table.py\n  -clean\n\nimplicit %{1}%{2}.min.%{3}: **/*.{css,js}\n  build: esbuild --bundle %{IN} --minify --outfile=%{OUT}\n\nimplicit build/%{1}: cmd/*\n  cd cmd/%{1}\n  inputs: cmd/%{1}/**/* **/*.go\n  build: go build -ldflags=\"-X 'main.version=%{VERSION}'\" -o ../../build/%{1} .\n\n#upload: chromad\n#  build:\n#    scp chromad root@swapoff.org:\n#    ssh root@swapoff.org 'install -m755 ./chromad /srv/http/swapoff.org/bin && service chromad restart'\n#    touch upload\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 1.0302734375,
          "content": "Copyright (C) 2017 Alec Thomas\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.736328125,
          "content": ".PHONY: chromad upload all\n\nVERSION ?= $(shell git describe --tags --dirty  --always)\nexport GOOS ?= linux\nexport GOARCH ?= amd64\n\nall: README.md tokentype_string.go\n\nREADME.md: lexers/*/*.go\n\t./table.py\n\ntokentype_string.go: types.go\n\tgo generate\n\nchromad:\n\trm -rf build\n\tesbuild --bundle cmd/chromad/static/index.js --minify --outfile=cmd/chromad/static/index.min.js\n\tesbuild --bundle cmd/chromad/static/index.css --minify --outfile=cmd/chromad/static/index.min.css\n\t(export CGOENABLED=0 ; cd ./cmd/chromad && go build -ldflags=\"-X 'main.version=$(VERSION)'\" -o ../../build/chromad .)\n\nupload: build/chromad\n\tscp build/chromad root@swapoff.org: && \\\n\t\tssh root@swapoff.org 'install -m755 ./chromad /srv/http/swapoff.org/bin && service chromad restart'\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.1513671875,
          "content": "# Chroma â€” A general purpose syntax highlighter in pure Go\n\n[![Golang Documentation](https://godoc.org/github.com/alecthomas/chroma?status.svg)](https://godoc.org/github.com/alecthomas/chroma) [![CI](https://github.com/alecthomas/chroma/actions/workflows/ci.yml/badge.svg)](https://github.com/alecthomas/chroma/actions/workflows/ci.yml) [![Slack chat](https://img.shields.io/static/v1?logo=slack&style=flat&label=slack&color=green&message=gophers)](https://invite.slack.golangbridge.org/)\n\nChroma takes source code and other structured text and converts it into syntax\nhighlighted HTML, ANSI-coloured text, etc.\n\nChroma is based heavily on [Pygments](http://pygments.org/), and includes\ntranslators for Pygments lexers and styles.\n\n## Table of Contents\n\n<!-- TOC -->\n\n1. [Supported languages](#supported-languages)\n2. [Try it](#try-it)\n3. [Using the library](#using-the-library)\n   1. [Quick start](#quick-start)\n   2. [Identifying the language](#identifying-the-language)\n   3. [Formatting the output](#formatting-the-output)\n   4. [The HTML formatter](#the-html-formatter)\n4. [More detail](#more-detail)\n   1. [Lexers](#lexers)\n   2. [Formatters](#formatters)\n   3. [Styles](#styles)\n5. [Command-line interface](#command-line-interface)\n6. [Testing lexers](#testing-lexers)\n7. [What's missing compared to Pygments?](#whats-missing-compared-to-pygments)\n\n<!-- /TOC -->\n\n## Supported languages\n\n| Prefix | Language                                                                                                                                                                                                                                            |\n| :----: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|   A    | ABAP, ABNF, ActionScript, ActionScript 3, Ada, Agda, AL, Alloy, Angular2, ANTLR, ApacheConf, APL, AppleScript, ArangoDB AQL, Arduino, ArmAsm, AutoHotkey, AutoIt, Awk                                                                               |\n|   B    | Ballerina, Bash, Bash Session, Batchfile, BibTeX, Bicep, BlitzBasic, BNF, BQN, Brainfuck                                                                                                                                                            |\n|   C    | C, C#, C++, Caddyfile, Caddyfile Directives, Cap'n Proto, Cassandra CQL, Ceylon, CFEngine3, cfstatement, ChaiScript, Chapel, Cheetah, Clojure, CMake, COBOL, CoffeeScript, Common Lisp, Coq, Crystal, CSS, Cython                                   |\n|   D    | D, Dart, Dax, Desktop Entry, Diff, Django/Jinja, dns, Docker, DTD, Dylan                                                                                                                                                                            |\n|   E    | EBNF, Elixir, Elm, EmacsLisp, Erlang                                                                                                                                                                                                                |\n|   F    | Factor, Fennel, Fish, Forth, Fortran, FortranFixed, FSharp                                                                                                                                                                                          |\n|   G    | GAS, GDScript, Genshi, Genshi HTML, Genshi Text, Gherkin, Gleam, GLSL, Gnuplot, Go, Go HTML Template, Go Text Template, GraphQL, Groff, Groovy                                                                                                      |\n|   H    | Handlebars, Hare, Haskell, Haxe, HCL, Hexdump, HLB, HLSL, HolyC, HTML, HTTP, Hy                                                                                                                                                                     |\n|   I    | Idris, Igor, INI, Io, ISCdhcpd                                                                                                                                                                                                                      |\n|   J    | J, Java, JavaScript, JSON, Jsonnet, Julia, Jungle                                                                                                                                                                                                   |\n|   K    | Kotlin                                                                                                                                                                                                                                              |\n|   L    | Lighttpd configuration file, LLVM, Lua                                                                                                                                                                                                              |\n|   M    | Makefile, Mako, markdown, Mason, Materialize SQL dialect, Mathematica, Matlab, MCFunction, Meson, Metal, MiniZinc, MLIR, Modula-2, MonkeyC, MorrowindScript, Myghty, MySQL                                                                          |\n|   N    | NASM, Natural, Newspeak, Nginx configuration file, Nim, Nix, NSIS                                                                                                                                                                                   |\n|   O    | Objective-C, OCaml, Octave, Odin, OnesEnterprise, OpenEdge ABL, OpenSCAD, Org Mode                                                                                                                                                                  |\n|   P    | PacmanConf, Perl, PHP, PHTML, Pig, PkgConfig, PL/pgSQL, plaintext, Plutus Core, Pony, PostgreSQL SQL dialect, PostScript, POVRay, PowerQuery, PowerShell, Prolog, PromQL, Promela, properties, Protocol Buffer, PRQL, PSL, Puppet, Python, Python 2 |\n|   Q    | QBasic, QML                                                                                                                                                                                                                                         |\n|   R    | R, Racket, Ragel, Raku, react, ReasonML, reg, Rego, reStructuredText, Rexx, RPMSpec, Ruby, Rust                                                                                                                                                     |\n|   S    | SAS, Sass, Scala, Scheme, Scilab, SCSS, Sed, Sieve, Smali, Smalltalk, Smarty, SNBT, Snobol, Solidity, SourcePawn, SPARQL, SQL, SquidConf, Standard ML, stas, Stylus, Svelte, Swift, SYSTEMD, systemverilog                                                |\n|   T    | TableGen, Tal, TASM, Tcl, Tcsh, Termcap, Terminfo, Terraform, TeX, Thrift, TOML, TradingView, Transact-SQL, Turing, Turtle, Twig, TypeScript, TypoScript, TypoScriptCssData, TypoScriptHtmlData, Typst                                              |\n|   V    | V, V shell, Vala, VB.net, verilog, VHDL, VHS, VimL, vue                                                                                                                                                                                             |\n|   W    | WDTE, WebGPU Shading Language, Whiley                                                                                                                                                                                                               |\n|   X    | XML, Xorg                                                                                                                                                                                                                                           |\n|   Y    | YAML, YANG                                                                                                                                                                                                                                          |\n|   Z    | Z80 Assembly, Zed, Zig                                                                                                                                                                                                                              |\n\n_I will attempt to keep this section up to date, but an authoritative list can be\ndisplayed with `chroma --list`._\n\n## Try it\n\nTry out various languages and styles on the [Chroma Playground](https://swapoff.org/chroma/playground/).\n\n## Using the library\n\nThis is version 2 of Chroma, use the import path:\n\n```go\nimport \"github.com/alecthomas/chroma/v2\"\n```\n\nChroma, like Pygments, has the concepts of\n[lexers](https://github.com/alecthomas/chroma/tree/master/lexers),\n[formatters](https://github.com/alecthomas/chroma/tree/master/formatters) and\n[styles](https://github.com/alecthomas/chroma/tree/master/styles).\n\nLexers convert source text into a stream of tokens, styles specify how token\ntypes are mapped to colours, and formatters convert tokens and styles into\nformatted output.\n\nA package exists for each of these, containing a global `Registry` variable\nwith all of the registered implementations. There are also helper functions\nfor using the registry in each package, such as looking up lexers by name or\nmatching filenames, etc.\n\nIn all cases, if a lexer, formatter or style can not be determined, `nil` will\nbe returned. In this situation you may want to default to the `Fallback`\nvalue in each respective package, which provides sane defaults.\n\n### Quick start\n\nA convenience function exists that can be used to simply format some source\ntext, without any effort:\n\n```go\nerr := quick.Highlight(os.Stdout, someSourceCode, \"go\", \"html\", \"monokai\")\n```\n\n### Identifying the language\n\nTo highlight code, you'll first have to identify what language the code is\nwritten in. There are three primary ways to do that:\n\n1. Detect the language from its filename.\n\n   ```go\n   lexer := lexers.Match(\"foo.go\")\n   ```\n\n2. Explicitly specify the language by its Chroma syntax ID (a full list is available from `lexers.Names()`).\n\n   ```go\n   lexer := lexers.Get(\"go\")\n   ```\n\n3. Detect the language from its content.\n\n   ```go\n   lexer := lexers.Analyse(\"package main\\n\\nfunc main()\\n{\\n}\\n\")\n   ```\n\nIn all cases, `nil` will be returned if the language can not be identified.\n\n```go\nif lexer == nil {\n  lexer = lexers.Fallback\n}\n```\n\nAt this point, it should be noted that some lexers can be extremely chatty. To\nmitigate this, you can use the coalescing lexer to coalesce runs of identical\ntoken types into a single token:\n\n```go\nlexer = chroma.Coalesce(lexer)\n```\n\n### Formatting the output\n\nOnce a language is identified you will need to pick a formatter and a style (theme).\n\n```go\nstyle := styles.Get(\"swapoff\")\nif style == nil {\n  style = styles.Fallback\n}\nformatter := formatters.Get(\"html\")\nif formatter == nil {\n  formatter = formatters.Fallback\n}\n```\n\nThen obtain an iterator over the tokens:\n\n```go\ncontents, err := ioutil.ReadAll(r)\niterator, err := lexer.Tokenise(nil, string(contents))\n```\n\nAnd finally, format the tokens from the iterator:\n\n```go\nerr := formatter.Format(w, style, iterator)\n```\n\n### The HTML formatter\n\nBy default the `html` registered formatter generates standalone HTML with\nembedded CSS. More flexibility is available through the `formatters/html` package.\n\nFirstly, the output generated by the formatter can be customised with the\nfollowing constructor options:\n\n- `Standalone()` - generate standalone HTML with embedded CSS.\n- `WithClasses()` - use classes rather than inlined style attributes.\n- `ClassPrefix(prefix)` - prefix each generated CSS class.\n- `TabWidth(width)` - Set the rendered tab width, in characters.\n- `WithLineNumbers()` - Render line numbers (style with `LineNumbers`).\n- `WithLinkableLineNumbers()` - Make the line numbers linkable and be a link to themselves.\n- `HighlightLines(ranges)` - Highlight lines in these ranges (style with `LineHighlight`).\n- `LineNumbersInTable()` - Use a table for formatting line numbers and code, rather than spans.\n\nIf `WithClasses()` is used, the corresponding CSS can be obtained from the formatter with:\n\n```go\nformatter := html.New(html.WithClasses(true))\nerr := formatter.WriteCSS(w, style)\n```\n\n## More detail\n\n### Lexers\n\nSee the [Pygments documentation](http://pygments.org/docs/lexerdevelopment/)\nfor details on implementing lexers. Most concepts apply directly to Chroma,\nbut see existing lexer implementations for real examples.\n\nIn many cases lexers can be automatically converted directly from Pygments by\nusing the included Python 3 script `pygments2chroma_xml.py`. I use something like\nthe following:\n\n```sh\npython3 _tools/pygments2chroma_xml.py \\\n  pygments.lexers.jvm.KotlinLexer \\\n  > lexers/embedded/kotlin.xml\n```\n\nSee notes in [pygments-lexers.txt](https://github.com/alecthomas/chroma/blob/master/pygments-lexers.txt)\nfor a list of lexers, and notes on some of the issues importing them.\n\n### Formatters\n\nChroma supports HTML output, as well as terminal output in 8 colour, 256 colour, and true-colour.\n\nA `noop` formatter is included that outputs the token text only, and a `tokens`\nformatter outputs raw tokens. The latter is useful for debugging lexers.\n\n### Styles\n\nChroma styles are defined in XML. The style entries use the\n[same syntax](http://pygments.org/docs/styles/) as Pygments.\n\nAll Pygments styles have been converted to Chroma using the `_tools/style.py`\nscript.\n\nWhen you work with one of [Chroma's styles](https://github.com/alecthomas/chroma/tree/master/styles),\nknow that the `Background` token type provides the default style for tokens. It does so\nby defining a foreground color and background color.\n\nFor example, this gives each token name not defined in the style a default color\nof `#f8f8f8` and uses `#000000` for the highlighted code block's background:\n\n```xml\n<entry type=\"Background\" style=\"#f8f8f2 bg:#000000\"/>\n```\n\nAlso, token types in a style file are hierarchical. For instance, when `CommentSpecial` is not defined, Chroma uses the token style from `Comment`. So when several comment tokens use the same color, you'll only need to define `Comment` and override the one that has a different color.\n\nFor a quick overview of the available styles and how they look, check out the [Chroma Style Gallery](https://xyproto.github.io/splash/docs/).\n\n## Command-line interface\n\nA command-line interface to Chroma is included.\n\nBinaries are available to install from [the releases page](https://github.com/alecthomas/chroma/releases).\n\nThe CLI can be used as a preprocessor to colorise output of `less(1)`,\nsee documentation for the `LESSOPEN` environment variable.\n\nThe `--fail` flag can be used to suppress output and return with exit status\n1 to facilitate falling back to some other preprocessor in case chroma\ndoes not resolve a specific lexer to use for the given file. For example:\n\n```shell\nexport LESSOPEN='| p() { chroma --fail \"$1\" || cat \"$1\"; }; p \"%s\"'\n```\n\nReplace `cat` with your favourite fallback preprocessor.\n\nWhen invoked as `.lessfilter`, the `--fail` flag is automatically turned\non under the hood for easy integration with [lesspipe shipping with\nDebian and derivatives](https://manpages.debian.org/lesspipe#USER_DEFINED_FILTERS);\nfor that setup the `chroma` executable can be just symlinked to `~/.lessfilter`.\n\n## Testing lexers\n\nIf you edit some lexers and want to try it, open a shell in `cmd/chromad` and run:\n\n```shell\ngo run . --csrf-key=securekey\n```\n\nA Link will be printed. Open it in your Browser. Now you can test on the Playground with your local changes.\n\nIf you want to run the tests and the lexers, open a shell in the root directory and run:\n\n```shell\ngo test ./lexers\n```\n\nWhen updating or adding a lexer, please add tests. See [lexers/README.md](lexers/README.md) for more.\n\n## What's missing compared to Pygments?\n\n- Quite a few lexers, for various reasons (pull-requests welcome):\n  - Pygments lexers for complex languages often include custom code to\n    handle certain aspects, such as Raku's ability to nest code inside\n    regular expressions. These require time and effort to convert.\n  - I mostly only converted languages I had heard of, to reduce the porting cost.\n- Some more esoteric features of Pygments are omitted for simplicity.\n- Though the Chroma API supports content detection, very few languages support them.\n  I have plans to implement a statistical analyser at some point, but not enough time.\n"
        },
        {
          "name": "_tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "coalesce.go",
          "type": "blob",
          "size": 0.76171875,
          "content": "package chroma\n\n// Coalesce is a Lexer interceptor that collapses runs of common types into a single token.\nfunc Coalesce(lexer Lexer) Lexer { return &coalescer{lexer} }\n\ntype coalescer struct{ Lexer }\n\nfunc (d *coalescer) Tokenise(options *TokeniseOptions, text string) (Iterator, error) {\n\tvar prev Token\n\tit, err := d.Lexer.Tokenise(options, text)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn func() Token {\n\t\tfor token := it(); token != (EOF); token = it() {\n\t\t\tif len(token.Value) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif prev == EOF {\n\t\t\t\tprev = token\n\t\t\t} else {\n\t\t\t\tif prev.Type == token.Type && len(prev.Value) < 8192 {\n\t\t\t\t\tprev.Value += token.Value\n\t\t\t\t} else {\n\t\t\t\t\tout := prev\n\t\t\t\t\tprev = token\n\t\t\t\t\treturn out\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tout := prev\n\t\tprev = EOF\n\t\treturn out\n\t}, nil\n}\n"
        },
        {
          "name": "coalesce_test.go",
          "type": "blob",
          "size": 0.3916015625,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestCoalesce(t *testing.T) {\n\tlexer := Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": []Rule{\n\t\t\t{`[!@#$%^&*()]`, Punctuation, nil},\n\t\t},\n\t}))\n\tactual, err := Tokenise(lexer, nil, \"!@#$\")\n\tassert.NoError(t, err)\n\texpected := []Token{{Punctuation, \"!@#$\"}}\n\tassert.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "colour.go",
          "type": "blob",
          "size": 5.4169921875,
          "content": "package chroma\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// ANSI2RGB maps ANSI colour names, as supported by Chroma, to hex RGB values.\nvar ANSI2RGB = map[string]string{\n\t\"#ansiblack\":     \"000000\",\n\t\"#ansidarkred\":   \"7f0000\",\n\t\"#ansidarkgreen\": \"007f00\",\n\t\"#ansibrown\":     \"7f7fe0\",\n\t\"#ansidarkblue\":  \"00007f\",\n\t\"#ansipurple\":    \"7f007f\",\n\t\"#ansiteal\":      \"007f7f\",\n\t\"#ansilightgray\": \"e5e5e5\",\n\t// Normal\n\t\"#ansidarkgray\":  \"555555\",\n\t\"#ansired\":       \"ff0000\",\n\t\"#ansigreen\":     \"00ff00\",\n\t\"#ansiyellow\":    \"ffff00\",\n\t\"#ansiblue\":      \"0000ff\",\n\t\"#ansifuchsia\":   \"ff00ff\",\n\t\"#ansiturquoise\": \"00ffff\",\n\t\"#ansiwhite\":     \"ffffff\",\n\n\t// Aliases without the \"ansi\" prefix, because...why?\n\t\"#black\":     \"000000\",\n\t\"#darkred\":   \"7f0000\",\n\t\"#darkgreen\": \"007f00\",\n\t\"#brown\":     \"7f7fe0\",\n\t\"#darkblue\":  \"00007f\",\n\t\"#purple\":    \"7f007f\",\n\t\"#teal\":      \"007f7f\",\n\t\"#lightgray\": \"e5e5e5\",\n\t// Normal\n\t\"#darkgray\":  \"555555\",\n\t\"#red\":       \"ff0000\",\n\t\"#green\":     \"00ff00\",\n\t\"#yellow\":    \"ffff00\",\n\t\"#blue\":      \"0000ff\",\n\t\"#fuchsia\":   \"ff00ff\",\n\t\"#turquoise\": \"00ffff\",\n\t\"#white\":     \"ffffff\",\n}\n\n// Colour represents an RGB colour.\ntype Colour int32\n\n// NewColour creates a Colour directly from RGB values.\nfunc NewColour(r, g, b uint8) Colour {\n\treturn ParseColour(fmt.Sprintf(\"%02x%02x%02x\", r, g, b))\n}\n\n// Distance between this colour and another.\n//\n// This uses the approach described here (https://www.compuphase.com/cmetric.htm).\n// This is not as accurate as LAB, et. al. but is *vastly* simpler and sufficient for our needs.\nfunc (c Colour) Distance(e2 Colour) float64 {\n\tar, ag, ab := int64(c.Red()), int64(c.Green()), int64(c.Blue())\n\tbr, bg, bb := int64(e2.Red()), int64(e2.Green()), int64(e2.Blue())\n\trmean := (ar + br) / 2\n\tr := ar - br\n\tg := ag - bg\n\tb := ab - bb\n\treturn math.Sqrt(float64((((512 + rmean) * r * r) >> 8) + 4*g*g + (((767 - rmean) * b * b) >> 8)))\n}\n\n// Brighten returns a copy of this colour with its brightness adjusted.\n//\n// If factor is negative, the colour is darkened.\n//\n// Uses approach described here (http://www.pvladov.com/2012/09/make-color-lighter-or-darker.html).\nfunc (c Colour) Brighten(factor float64) Colour {\n\tr := float64(c.Red())\n\tg := float64(c.Green())\n\tb := float64(c.Blue())\n\n\tif factor < 0 {\n\t\tfactor++\n\t\tr *= factor\n\t\tg *= factor\n\t\tb *= factor\n\t} else {\n\t\tr = (255-r)*factor + r\n\t\tg = (255-g)*factor + g\n\t\tb = (255-b)*factor + b\n\t}\n\treturn NewColour(uint8(r), uint8(g), uint8(b))\n}\n\n// BrightenOrDarken brightens a colour if it is < 0.5 brightness or darkens if > 0.5 brightness.\nfunc (c Colour) BrightenOrDarken(factor float64) Colour {\n\tif c.Brightness() < 0.5 {\n\t\treturn c.Brighten(factor)\n\t}\n\treturn c.Brighten(-factor)\n}\n\n// ClampBrightness returns a copy of this colour with its brightness adjusted such that\n// it falls within the range [min, max] (or very close to it due to rounding errors).\n// The supplied values use the same [0.0, 1.0] range as Brightness.\nfunc (c Colour) ClampBrightness(min, max float64) Colour {\n\tif !c.IsSet() {\n\t\treturn c\n\t}\n\n\tmin = math.Max(min, 0)\n\tmax = math.Min(max, 1)\n\tcurrent := c.Brightness()\n\ttarget := math.Min(math.Max(current, min), max)\n\tif current == target {\n\t\treturn c\n\t}\n\n\tr := float64(c.Red())\n\tg := float64(c.Green())\n\tb := float64(c.Blue())\n\trgb := r + g + b\n\tif target > current {\n\t\t// Solve for x: target == ((255-r)*x + r + (255-g)*x + g + (255-b)*x + b) / 255 / 3\n\t\treturn c.Brighten((target*255*3 - rgb) / (255*3 - rgb))\n\t}\n\t// Solve for x: target == (r*(x+1) + g*(x+1) + b*(x+1)) / 255 / 3\n\treturn c.Brighten((target*255*3)/rgb - 1)\n}\n\n// Brightness of the colour (roughly) in the range 0.0 to 1.0.\nfunc (c Colour) Brightness() float64 {\n\treturn (float64(c.Red()) + float64(c.Green()) + float64(c.Blue())) / 255.0 / 3.0\n}\n\n// ParseColour in the forms #rgb, #rrggbb, #ansi<colour>, or #<colour>.\n// Will return an \"unset\" colour if invalid.\nfunc ParseColour(colour string) Colour {\n\tcolour = normaliseColour(colour)\n\tn, err := strconv.ParseUint(colour, 16, 32)\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn Colour(n + 1) //nolint:gosec\n}\n\n// MustParseColour is like ParseColour except it panics if the colour is invalid.\n//\n// Will panic if colour is in an invalid format.\nfunc MustParseColour(colour string) Colour {\n\tparsed := ParseColour(colour)\n\tif !parsed.IsSet() {\n\t\tpanic(fmt.Errorf(\"invalid colour %q\", colour))\n\t}\n\treturn parsed\n}\n\n// IsSet returns true if the colour is set.\nfunc (c Colour) IsSet() bool { return c != 0 }\n\nfunc (c Colour) String() string   { return fmt.Sprintf(\"#%06x\", int(c-1)) }\nfunc (c Colour) GoString() string { return fmt.Sprintf(\"Colour(0x%06x)\", int(c-1)) }\n\n// Red component of colour.\nfunc (c Colour) Red() uint8 { return uint8(((c - 1) >> 16) & 0xff) } //nolint:gosec\n\n// Green component of colour.\nfunc (c Colour) Green() uint8 { return uint8(((c - 1) >> 8) & 0xff) } //nolint:gosec\n\n// Blue component of colour.\nfunc (c Colour) Blue() uint8 { return uint8((c - 1) & 0xff) } //nolint:gosec\n\n// Colours is an orderable set of colours.\ntype Colours []Colour\n\nfunc (c Colours) Len() int           { return len(c) }\nfunc (c Colours) Swap(i, j int)      { c[i], c[j] = c[j], c[i] }\nfunc (c Colours) Less(i, j int) bool { return c[i] < c[j] }\n\n// Convert colours to #rrggbb.\nfunc normaliseColour(colour string) string {\n\tif ansi, ok := ANSI2RGB[colour]; ok {\n\t\treturn ansi\n\t}\n\tif strings.HasPrefix(colour, \"#\") {\n\t\tcolour = colour[1:]\n\t\tif len(colour) == 3 {\n\t\t\treturn colour[0:1] + colour[0:1] + colour[1:2] + colour[1:2] + colour[2:3] + colour[2:3]\n\t\t}\n\t}\n\treturn colour\n}\n"
        },
        {
          "name": "colour_test.go",
          "type": "blob",
          "size": 2.630859375,
          "content": "package chroma\n\nimport (\n\t\"math\"\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestColourRGB(t *testing.T) {\n\tcolour := ParseColour(\"#8913af\")\n\tassert.Equal(t, uint8(0x89), colour.Red())\n\tassert.Equal(t, uint8(0x13), colour.Green())\n\tassert.Equal(t, uint8(0xaf), colour.Blue())\n}\n\nfunc TestColourString(t *testing.T) {\n\tassert.Equal(t, \"#8913af\", ParseColour(\"#8913af\").String())\n}\n\nfunc distance(a, b uint8) uint8 {\n\tif a < b {\n\t\treturn b - a\n\t}\n\treturn a - b\n}\n\nfunc TestColourBrighten(t *testing.T) {\n\tactual := NewColour(128, 128, 128).Brighten(0.5)\n\t// Closeish to what we expect is fine.\n\tassert.True(t, distance(192, actual.Red()) <= 2)\n\tassert.True(t, distance(192, actual.Blue()) <= 2)\n\tassert.True(t, distance(192, actual.Green()) <= 2)\n\tactual = NewColour(128, 128, 128).Brighten(-0.5)\n\tassert.True(t, distance(65, actual.Red()) <= 2)\n\tassert.True(t, distance(65, actual.Blue()) <= 2)\n\tassert.True(t, distance(65, actual.Green()) <= 2)\n}\n\nfunc TestColourBrightess(t *testing.T) {\n\tactual := NewColour(128, 128, 128).Brightness()\n\tassert.True(t, distance(128, uint8(actual*255.0)) <= 2)\n}\n\n// hue returns c's hue. See https://stackoverflow.com/a/23094494.\nfunc hue(c Colour) float64 {\n\tr := float64(c.Red()) / 255\n\tg := float64(c.Green()) / 255\n\tb := float64(c.Blue()) / 255\n\n\tmin := math.Min(math.Min(r, g), b)\n\tmax := math.Max(math.Max(r, g), b)\n\n\tswitch {\n\tcase r == min:\n\t\treturn (g - b) / (max - min)\n\tcase g == min:\n\t\treturn 2 + (b-r)/(max-min)\n\tdefault:\n\t\treturn 4 + (r-g)/(max-min)\n\t}\n}\n\nfunc TestColourClampBrightness(t *testing.T) {\n\t// Start with a colour with a brightness close to 0.5.\n\tinitial := NewColour(0, 128, 255)\n\tbr := initial.Brightness()\n\tassertInDelta(t, 0.5, br)\n\n\t// Passing a range that includes the colour's brightness should be a no-op.\n\tassert.Equal(t, initial.String(), initial.ClampBrightness(br-0.01, br+0.01).String())\n\n\t// Clamping to [0, 0] or [1, 1] should produce black or white, respectively.\n\tassert.Equal(t, \"#000000\", initial.ClampBrightness(0, 0).String())\n\tassert.Equal(t, \"#ffffff\", initial.ClampBrightness(1, 1).String())\n\n\t// Clamping to a brighter or darker range should produce the requested\n\t// brightness while preserving the colour's hue.\n\tbrighter := initial.ClampBrightness(0.75, 1)\n\tassertInDelta(t, 0.75, brighter.Brightness())\n\tassertInDelta(t, hue(initial), hue(brighter))\n\n\tdarker := initial.ClampBrightness(0, 0.25)\n\tassertInDelta(t, 0.25, darker.Brightness())\n\tassertInDelta(t, hue(initial), hue(darker))\n}\n\nfunc assertInDelta(t *testing.T, expected, actual float64) {\n\tconst delta = 0.01 // used for brightness and hue comparisons\n\tassert.True(t, actual > (expected-delta) && actual < (expected+delta))\n}\n"
        },
        {
          "name": "delegate.go",
          "type": "blob",
          "size": 3.5498046875,
          "content": "package chroma\n\nimport (\n\t\"bytes\"\n)\n\ntype delegatingLexer struct {\n\troot     Lexer\n\tlanguage Lexer\n}\n\n// DelegatingLexer combines two lexers to handle the common case of a language embedded inside another, such as PHP\n// inside HTML or PHP inside plain text.\n//\n// It takes two lexer as arguments: a root lexer and a language lexer.  First everything is scanned using the language\n// lexer, which must return \"Other\" for unrecognised tokens. Then all \"Other\" tokens are lexed using the root lexer.\n// Finally, these two sets of tokens are merged.\n//\n// The lexers from the template lexer package use this base lexer.\nfunc DelegatingLexer(root Lexer, language Lexer) Lexer {\n\treturn &delegatingLexer{\n\t\troot:     root,\n\t\tlanguage: language,\n\t}\n}\n\nfunc (d *delegatingLexer) AnalyseText(text string) float32 {\n\treturn d.root.AnalyseText(text)\n}\n\nfunc (d *delegatingLexer) SetAnalyser(analyser func(text string) float32) Lexer {\n\td.root.SetAnalyser(analyser)\n\treturn d\n}\n\nfunc (d *delegatingLexer) SetRegistry(r *LexerRegistry) Lexer {\n\td.root.SetRegistry(r)\n\td.language.SetRegistry(r)\n\treturn d\n}\n\nfunc (d *delegatingLexer) Config() *Config {\n\treturn d.language.Config()\n}\n\n// An insertion is the character range where language tokens should be inserted.\ntype insertion struct {\n\tstart, end int\n\ttokens     []Token\n}\n\nfunc (d *delegatingLexer) Tokenise(options *TokeniseOptions, text string) (Iterator, error) { // nolint: gocognit\n\ttokens, err := Tokenise(Coalesce(d.language), options, text)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Compute insertions and gather \"Other\" tokens.\n\tothers := &bytes.Buffer{}\n\tinsertions := []*insertion{}\n\tvar insert *insertion\n\toffset := 0\n\tvar last Token\n\tfor _, t := range tokens {\n\t\tif t.Type == Other {\n\t\t\tif last != EOF && insert != nil && last.Type != Other {\n\t\t\t\tinsert.end = offset\n\t\t\t}\n\t\t\tothers.WriteString(t.Value)\n\t\t} else {\n\t\t\tif last == EOF || last.Type == Other {\n\t\t\t\tinsert = &insertion{start: offset}\n\t\t\t\tinsertions = append(insertions, insert)\n\t\t\t}\n\t\t\tinsert.tokens = append(insert.tokens, t)\n\t\t}\n\t\tlast = t\n\t\toffset += len(t.Value)\n\t}\n\n\tif len(insertions) == 0 {\n\t\treturn d.root.Tokenise(options, text)\n\t}\n\n\t// Lex the other tokens.\n\trootTokens, err := Tokenise(Coalesce(d.root), options, others.String())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Interleave the two sets of tokens.\n\tvar out []Token\n\toffset = 0 // Offset into text.\n\ttokenIndex := 0\n\tnextToken := func() Token {\n\t\tif tokenIndex >= len(rootTokens) {\n\t\t\treturn EOF\n\t\t}\n\t\tt := rootTokens[tokenIndex]\n\t\ttokenIndex++\n\t\treturn t\n\t}\n\tinsertionIndex := 0\n\tnextInsertion := func() *insertion {\n\t\tif insertionIndex >= len(insertions) {\n\t\t\treturn nil\n\t\t}\n\t\ti := insertions[insertionIndex]\n\t\tinsertionIndex++\n\t\treturn i\n\t}\n\tt := nextToken()\n\ti := nextInsertion()\n\tfor t != EOF || i != nil {\n\t\t// fmt.Printf(\"%d->%d:%q   %d->%d:%q\\n\", offset, offset+len(t.Value), t.Value, i.start, i.end, Stringify(i.tokens...))\n\t\tif t == EOF || (i != nil && i.start < offset+len(t.Value)) {\n\t\t\tvar l Token\n\t\t\tl, t = splitToken(t, i.start-offset)\n\t\t\tif l != EOF {\n\t\t\t\tout = append(out, l)\n\t\t\t\toffset += len(l.Value)\n\t\t\t}\n\t\t\tout = append(out, i.tokens...)\n\t\t\toffset += i.end - i.start\n\t\t\tif t == EOF {\n\t\t\t\tt = nextToken()\n\t\t\t}\n\t\t\ti = nextInsertion()\n\t\t} else {\n\t\t\tout = append(out, t)\n\t\t\toffset += len(t.Value)\n\t\t\tt = nextToken()\n\t\t}\n\t}\n\treturn Literator(out...), nil\n}\n\nfunc splitToken(t Token, offset int) (l Token, r Token) {\n\tif t == EOF {\n\t\treturn EOF, EOF\n\t}\n\tif offset == 0 {\n\t\treturn EOF, t\n\t}\n\tif offset == len(t.Value) {\n\t\treturn t, EOF\n\t}\n\tl = t.Clone()\n\tr = t.Clone()\n\tl.Value = l.Value[:offset]\n\tr.Value = r.Value[offset:]\n\treturn\n}\n"
        },
        {
          "name": "delegate_test.go",
          "type": "blob",
          "size": 2.5810546875,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc makeDelegationTestLexers(t *testing.T) (lang Lexer, root Lexer) {\n\treturn mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\t\"root\": {\n\t\t\t\t{`\\<\\?`, CommentPreproc, Push(\"inside\")},\n\t\t\t\t{`.`, Other, nil},\n\t\t\t},\n\t\t\t\"inside\": {\n\t\t\t\t{`\\?\\>`, CommentPreproc, Pop(1)},\n\t\t\t\t{`\\bwhat\\b`, Keyword, nil},\n\t\t\t\t{`\\s+`, Whitespace, nil},\n\t\t\t},\n\t\t}),\n\t\tmustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\t\"root\": {\n\t\t\t\t{`\\bhello\\b`, Keyword, nil},\n\t\t\t\t{`\\b(world|there)\\b`, Name, nil},\n\t\t\t\t{`\\s+`, Whitespace, nil},\n\t\t\t},\n\t\t})\n}\n\nfunc TestDelegate(t *testing.T) {\n\ttestdata := []struct {\n\t\tname     string\n\t\tsource   string\n\t\texpected []Token\n\t}{\n\t\t{\"SourceInMiddle\", `hello world <? what ?> there`, []Token{\n\t\t\t{Keyword, \"hello\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"world\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t// lang\n\t\t\t{CommentPreproc, \"<?\"},\n\t\t\t{Whitespace, \" \"},\n\t\t\t{Keyword, \"what\"},\n\t\t\t{Whitespace, \" \"},\n\t\t\t{CommentPreproc, \"?>\"},\n\t\t\t// /lang\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"there\"},\n\t\t}},\n\t\t{\"SourceBeginning\", `<? what ?> hello world there`, []Token{\n\t\t\t{CommentPreproc, \"<?\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"what\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{CommentPreproc, \"?>\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"hello\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"world\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"there\"},\n\t\t}},\n\t\t{\"SourceEnd\", `hello world <? what there`, []Token{\n\t\t\t{Keyword, \"hello\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"world\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t// lang\n\t\t\t{CommentPreproc, \"<?\"},\n\t\t\t{Whitespace, \" \"},\n\t\t\t{Keyword, \"what\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Error, \"there\"},\n\t\t}},\n\t\t{\"SourceMultiple\", \"hello world <? what ?> hello there <? what ?> hello\", []Token{\n\t\t\t{Keyword, \"hello\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"world\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{CommentPreproc, \"<?\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"what\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{CommentPreproc, \"?>\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"hello\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Name, \"there\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{CommentPreproc, \"<?\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"what\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{CommentPreproc, \"?>\"},\n\t\t\t{TextWhitespace, \" \"},\n\t\t\t{Keyword, \"hello\"},\n\t\t}},\n\t}\n\tlang, root := makeDelegationTestLexers(t)\n\tdelegate := DelegatingLexer(root, lang)\n\tfor _, test := range testdata {\n\t\t// nolint: scopelint\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tit, err := delegate.Tokenise(nil, test.source)\n\t\t\tassert.NoError(t, err)\n\t\t\tactual := it.Tokens()\n\t\t\tassert.Equal(t, test.expected, actual)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.318359375,
          "content": "// Package chroma takes source code and other structured text and converts it into syntax highlighted HTML, ANSI-\n// coloured text, etc.\n//\n// Chroma is based heavily on Pygments, and includes translators for Pygments lexers and styles.\n//\n// For more information, go here: https://github.com/alecthomas/chroma\npackage chroma\n"
        },
        {
          "name": "emitters.go",
          "type": "blob",
          "size": 6.349609375,
          "content": "package chroma\n\nimport (\n\t\"fmt\"\n)\n\n// An Emitter takes group matches and returns tokens.\ntype Emitter interface {\n\t// Emit tokens for the given regex groups.\n\tEmit(groups []string, state *LexerState) Iterator\n}\n\n// SerialisableEmitter is an Emitter that can be serialised and deserialised to/from JSON.\ntype SerialisableEmitter interface {\n\tEmitter\n\tEmitterKind() string\n}\n\n// EmitterFunc is a function that is an Emitter.\ntype EmitterFunc func(groups []string, state *LexerState) Iterator\n\n// Emit tokens for groups.\nfunc (e EmitterFunc) Emit(groups []string, state *LexerState) Iterator {\n\treturn e(groups, state)\n}\n\ntype Emitters []Emitter\n\ntype byGroupsEmitter struct {\n\tEmitters\n}\n\n// ByGroups emits a token for each matching group in the rule's regex.\nfunc ByGroups(emitters ...Emitter) Emitter {\n\treturn &byGroupsEmitter{Emitters: emitters}\n}\n\nfunc (b *byGroupsEmitter) EmitterKind() string { return \"bygroups\" }\n\nfunc (b *byGroupsEmitter) Emit(groups []string, state *LexerState) Iterator {\n\titerators := make([]Iterator, 0, len(groups)-1)\n\tif len(b.Emitters) != len(groups)-1 {\n\t\titerators = append(iterators, Error.Emit(groups, state))\n\t\t// panic(errors.Errorf(\"number of groups %q does not match number of emitters %v\", groups, emitters))\n\t} else {\n\t\tfor i, group := range groups[1:] {\n\t\t\tif b.Emitters[i] != nil {\n\t\t\t\titerators = append(iterators, b.Emitters[i].Emit([]string{group}, state))\n\t\t\t}\n\t\t}\n\t}\n\treturn Concaterator(iterators...)\n}\n\n// ByGroupNames emits a token for each named matching group in the rule's regex.\nfunc ByGroupNames(emitters map[string]Emitter) Emitter {\n\treturn EmitterFunc(func(groups []string, state *LexerState) Iterator {\n\t\titerators := make([]Iterator, 0, len(state.NamedGroups)-1)\n\t\tif len(state.NamedGroups)-1 == 0 {\n\t\t\tif emitter, ok := emitters[`0`]; ok {\n\t\t\t\titerators = append(iterators, emitter.Emit(groups, state))\n\t\t\t} else {\n\t\t\t\titerators = append(iterators, Error.Emit(groups, state))\n\t\t\t}\n\t\t} else {\n\t\t\truleRegex := state.Rules[state.State][state.Rule].Regexp\n\t\t\tfor i := 1; i < len(state.NamedGroups); i++ {\n\t\t\t\tgroupName := ruleRegex.GroupNameFromNumber(i)\n\t\t\t\tgroup := state.NamedGroups[groupName]\n\t\t\t\tif emitter, ok := emitters[groupName]; ok {\n\t\t\t\t\tif emitter != nil {\n\t\t\t\t\t\titerators = append(iterators, emitter.Emit([]string{group}, state))\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\titerators = append(iterators, Error.Emit([]string{group}, state))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn Concaterator(iterators...)\n\t})\n}\n\n// UsingByGroup emits tokens for the matched groups in the regex using a\n// sublexer. Used when lexing code blocks where the name of a sublexer is\n// contained within the block, for example on a Markdown text block or SQL\n// language block.\n//\n// An attempt to load the sublexer will be made using the captured value from\n// the text of the matched sublexerNameGroup. If a sublexer matching the\n// sublexerNameGroup is available, then tokens for the matched codeGroup will\n// be emitted using the sublexer. Otherwise, if no sublexer is available, then\n// tokens will be emitted from the passed emitter.\n//\n// Example:\n//\n//\tvar Markdown = internal.Register(MustNewLexer(\n//\t\t&Config{\n//\t\t\tName:      \"markdown\",\n//\t\t\tAliases:   []string{\"md\", \"mkd\"},\n//\t\t\tFilenames: []string{\"*.md\", \"*.mkd\", \"*.markdown\"},\n//\t\t\tMimeTypes: []string{\"text/x-markdown\"},\n//\t\t},\n//\t\tRules{\n//\t\t\t\"root\": {\n//\t\t\t\t{\"^(```)(\\\\w+)(\\\\n)([\\\\w\\\\W]*?)(^```$)\",\n//\t\t\t\t\tUsingByGroup(\n//\t\t\t\t\t\t2, 4,\n//\t\t\t\t\t\tString, String, String, Text, String,\n//\t\t\t\t\t),\n//\t\t\t\t\tnil,\n//\t\t\t\t},\n//\t\t\t},\n//\t\t},\n//\t))\n//\n// See the lexers/markdown.go for the complete example.\n//\n// Note: panic's if the number of emitters does not equal the number of matched\n// groups in the regex.\nfunc UsingByGroup(sublexerNameGroup, codeGroup int, emitters ...Emitter) Emitter {\n\treturn &usingByGroup{\n\t\tSublexerNameGroup: sublexerNameGroup,\n\t\tCodeGroup:         codeGroup,\n\t\tEmitters:          emitters,\n\t}\n}\n\ntype usingByGroup struct {\n\tSublexerNameGroup int      `xml:\"sublexer_name_group\"`\n\tCodeGroup         int      `xml:\"code_group\"`\n\tEmitters          Emitters `xml:\"emitters\"`\n}\n\nfunc (u *usingByGroup) EmitterKind() string { return \"usingbygroup\" }\nfunc (u *usingByGroup) Emit(groups []string, state *LexerState) Iterator {\n\t// bounds check\n\tif len(u.Emitters) != len(groups)-1 {\n\t\tpanic(\"UsingByGroup expects number of emitters to be the same as len(groups)-1\")\n\t}\n\n\t// grab sublexer\n\tsublexer := state.Registry.Get(groups[u.SublexerNameGroup])\n\n\t// build iterators\n\titerators := make([]Iterator, len(groups)-1)\n\tfor i, group := range groups[1:] {\n\t\tif i == u.CodeGroup-1 && sublexer != nil {\n\t\t\tvar err error\n\t\t\titerators[i], err = sublexer.Tokenise(nil, groups[u.CodeGroup])\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t} else if u.Emitters[i] != nil {\n\t\t\titerators[i] = u.Emitters[i].Emit([]string{group}, state)\n\t\t}\n\t}\n\treturn Concaterator(iterators...)\n}\n\n// UsingLexer returns an Emitter that uses a given Lexer for parsing and emitting.\n//\n// This Emitter is not serialisable.\nfunc UsingLexer(lexer Lexer) Emitter {\n\treturn EmitterFunc(func(groups []string, _ *LexerState) Iterator {\n\t\tit, err := lexer.Tokenise(&TokeniseOptions{State: \"root\", Nested: true}, groups[0])\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\treturn it\n\t})\n}\n\ntype usingEmitter struct {\n\tLexer string `xml:\"lexer,attr\"`\n}\n\nfunc (u *usingEmitter) EmitterKind() string { return \"using\" }\n\nfunc (u *usingEmitter) Emit(groups []string, state *LexerState) Iterator {\n\tif state.Registry == nil {\n\t\tpanic(fmt.Sprintf(\"no LexerRegistry available for Using(%q)\", u.Lexer))\n\t}\n\tlexer := state.Registry.Get(u.Lexer)\n\tif lexer == nil {\n\t\tpanic(fmt.Sprintf(\"no such lexer %q\", u.Lexer))\n\t}\n\tit, err := lexer.Tokenise(&TokeniseOptions{State: \"root\", Nested: true}, groups[0])\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn it\n}\n\n// Using returns an Emitter that uses a given Lexer reference for parsing and emitting.\n//\n// The referenced lexer must be stored in the same LexerRegistry.\nfunc Using(lexer string) Emitter {\n\treturn &usingEmitter{Lexer: lexer}\n}\n\ntype usingSelfEmitter struct {\n\tState string `xml:\"state,attr\"`\n}\n\nfunc (u *usingSelfEmitter) EmitterKind() string { return \"usingself\" }\n\nfunc (u *usingSelfEmitter) Emit(groups []string, state *LexerState) Iterator {\n\tit, err := state.Lexer.Tokenise(&TokeniseOptions{State: u.State, Nested: true}, groups[0])\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn it\n}\n\n// UsingSelf is like Using, but uses the current Lexer.\nfunc UsingSelf(stateName string) Emitter {\n\treturn &usingSelfEmitter{stateName}\n}\n"
        },
        {
          "name": "formatter.go",
          "type": "blob",
          "size": 1.041015625,
          "content": "package chroma\n\nimport (\n\t\"io\"\n)\n\n// A Formatter for Chroma lexers.\ntype Formatter interface {\n\t// Format returns a formatting function for tokens.\n\t//\n\t// If the iterator panics, the Formatter should recover.\n\tFormat(w io.Writer, style *Style, iterator Iterator) error\n}\n\n// A FormatterFunc is a Formatter implemented as a function.\n//\n// Guards against iterator panics.\ntype FormatterFunc func(w io.Writer, style *Style, iterator Iterator) error\n\nfunc (f FormatterFunc) Format(w io.Writer, s *Style, it Iterator) (err error) { // nolint\n\tdefer func() {\n\t\tif perr := recover(); perr != nil {\n\t\t\terr = perr.(error)\n\t\t}\n\t}()\n\treturn f(w, s, it)\n}\n\ntype recoveringFormatter struct {\n\tFormatter\n}\n\nfunc (r recoveringFormatter) Format(w io.Writer, s *Style, it Iterator) (err error) {\n\tdefer func() {\n\t\tif perr := recover(); perr != nil {\n\t\t\terr = perr.(error)\n\t\t}\n\t}()\n\treturn r.Formatter.Format(w, s, it)\n}\n\n// RecoveringFormatter wraps a formatter with panic recovery.\nfunc RecoveringFormatter(formatter Formatter) Formatter { return recoveringFormatter{formatter} }\n"
        },
        {
          "name": "formatters",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.224609375,
          "content": "module github.com/alecthomas/chroma/v2\n\ngo 1.22\n\nrequire (\n\tgithub.com/alecthomas/assert/v2 v2.11.0\n\tgithub.com/alecthomas/repr v0.4.0\n\tgithub.com/dlclark/regexp2 v1.11.4\n)\n\nrequire github.com/hexops/gotextdiff v1.0.3 // indirect\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 0.685546875,
          "content": "github.com/alecthomas/assert/v2 v2.11.0 h1:2Q9r3ki8+JYXvGsDyBXwH3LcJ+WK5D0gc5E8vS6K3D0=\ngithub.com/alecthomas/assert/v2 v2.11.0/go.mod h1:Bze95FyfUr7x34QZrjL+XP+0qgp/zg8yS+TtBj1WA3k=\ngithub.com/alecthomas/repr v0.4.0 h1:GhI2A8MACjfegCPVq9f1FLvIBS+DrQ2KQBFZP1iFzXc=\ngithub.com/alecthomas/repr v0.4.0/go.mod h1:Fr0507jx4eOXV7AlPV6AVZLYrLIuIeSOWtW57eE/O/4=\ngithub.com/dlclark/regexp2 v1.11.4 h1:rPYF9/LECdNymJufQKmri9gV604RvvABwgOA8un7yAo=\ngithub.com/dlclark/regexp2 v1.11.4/go.mod h1:DHkYz0B9wPfa6wondMfaivmHpzrQ3v9q8cnmRbL6yW8=\ngithub.com/hexops/gotextdiff v1.0.3 h1:gitA9+qJrrTCsiCl7+kh75nPqQt1cx4ZkudSTLoUqJM=\ngithub.com/hexops/gotextdiff v1.0.3/go.mod h1:pSWU5MAI3yDq+fZBTazCSJysOMbxWL1BSow5/V2vxeg=\n"
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 1.96484375,
          "content": "package chroma\n\nimport \"strings\"\n\n// An Iterator across tokens.\n//\n// EOF will be returned at the end of the Token stream.\n//\n// If an error occurs within an Iterator, it may propagate this in a panic. Formatters should recover.\ntype Iterator func() Token\n\n// Tokens consumes all tokens from the iterator and returns them as a slice.\nfunc (i Iterator) Tokens() []Token {\n\tvar out []Token\n\tfor t := i(); t != EOF; t = i() {\n\t\tout = append(out, t)\n\t}\n\treturn out\n}\n\n// Stdlib converts a Chroma iterator to a Go 1.23-compatible iterator.\nfunc (i Iterator) Stdlib() func(yield func(Token) bool) {\n\treturn func(yield func(Token) bool) {\n\t\tfor t := i(); t != EOF; t = i() {\n\t\t\tif !yield(t) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Concaterator concatenates tokens from a series of iterators.\nfunc Concaterator(iterators ...Iterator) Iterator {\n\treturn func() Token {\n\t\tfor len(iterators) > 0 {\n\t\t\tt := iterators[0]()\n\t\t\tif t != EOF {\n\t\t\t\treturn t\n\t\t\t}\n\t\t\titerators = iterators[1:]\n\t\t}\n\t\treturn EOF\n\t}\n}\n\n// Literator converts a sequence of literal Tokens into an Iterator.\nfunc Literator(tokens ...Token) Iterator {\n\treturn func() Token {\n\t\tif len(tokens) == 0 {\n\t\t\treturn EOF\n\t\t}\n\t\ttoken := tokens[0]\n\t\ttokens = tokens[1:]\n\t\treturn token\n\t}\n}\n\n// SplitTokensIntoLines splits tokens containing newlines in two.\nfunc SplitTokensIntoLines(tokens []Token) (out [][]Token) {\n\tvar line []Token // nolint: prealloc\n\tfor _, token := range tokens {\n\t\tfor strings.Contains(token.Value, \"\\n\") {\n\t\t\tparts := strings.SplitAfterN(token.Value, \"\\n\", 2)\n\t\t\t// Token becomes the tail.\n\t\t\ttoken.Value = parts[1]\n\n\t\t\t// Append the head to the line and flush the line.\n\t\t\tclone := token.Clone()\n\t\t\tclone.Value = parts[0]\n\t\t\tline = append(line, clone)\n\t\t\tout = append(out, line)\n\t\t\tline = nil\n\t\t}\n\t\tline = append(line, token)\n\t}\n\tif len(line) > 0 {\n\t\tout = append(out, line)\n\t}\n\t// Strip empty trailing token line.\n\tif len(out) > 0 {\n\t\tlast := out[len(out)-1]\n\t\tif len(last) == 1 && last[0].Value == \"\" {\n\t\t\tout = out[:len(out)-1]\n\t\t}\n\t}\n\treturn\n}\n"
        },
        {
          "name": "lexer.go",
          "type": "blob",
          "size": 4.431640625,
          "content": "package chroma\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\nvar (\n\tdefaultOptions = &TokeniseOptions{\n\t\tState:    \"root\",\n\t\tEnsureLF: true,\n\t}\n)\n\n// Config for a lexer.\ntype Config struct {\n\t// Name of the lexer.\n\tName string `xml:\"name,omitempty\"`\n\n\t// Shortcuts for the lexer\n\tAliases []string `xml:\"alias,omitempty\"`\n\n\t// File name globs\n\tFilenames []string `xml:\"filename,omitempty\"`\n\n\t// Secondary file name globs\n\tAliasFilenames []string `xml:\"alias_filename,omitempty\"`\n\n\t// MIME types\n\tMimeTypes []string `xml:\"mime_type,omitempty\"`\n\n\t// Regex matching is case-insensitive.\n\tCaseInsensitive bool `xml:\"case_insensitive,omitempty\"`\n\n\t// Regex matches all characters.\n\tDotAll bool `xml:\"dot_all,omitempty\"`\n\n\t// Regex does not match across lines ($ matches EOL).\n\t//\n\t// Defaults to multiline.\n\tNotMultiline bool `xml:\"not_multiline,omitempty\"`\n\n\t// Don't strip leading and trailing newlines from the input.\n\t// DontStripNL bool\n\n\t// Strip all leading and trailing whitespace from the input\n\t// StripAll bool\n\n\t// Make sure that the input ends with a newline. This\n\t// is required for some lexers that consume input linewise.\n\tEnsureNL bool `xml:\"ensure_nl,omitempty\"`\n\n\t// If given and greater than 0, expand tabs in the input.\n\t// TabSize int\n\n\t// Priority of lexer.\n\t//\n\t// If this is 0 it will be treated as a default of 1.\n\tPriority float32 `xml:\"priority,omitempty\"`\n\n\t// Analyse is a list of regexes to match against the input.\n\t//\n\t// If a match is found, the score is returned if single attribute is set to true,\n\t// otherwise the sum of all the score of matching patterns will be\n\t// used as the final score.\n\tAnalyse *AnalyseConfig `xml:\"analyse,omitempty\"`\n}\n\n// AnalyseConfig defines the list of regexes analysers.\ntype AnalyseConfig struct {\n\tRegexes []RegexConfig `xml:\"regex,omitempty\"`\n\t// If true, the first matching score is returned.\n\tFirst bool `xml:\"first,attr\"`\n}\n\n// RegexConfig defines a single regex pattern and its score in case of match.\ntype RegexConfig struct {\n\tPattern string  `xml:\"pattern,attr\"`\n\tScore   float32 `xml:\"score,attr\"`\n}\n\n// Token output to formatter.\ntype Token struct {\n\tType  TokenType `json:\"type\"`\n\tValue string    `json:\"value\"`\n}\n\nfunc (t *Token) String() string   { return t.Value }\nfunc (t *Token) GoString() string { return fmt.Sprintf(\"&Token{%s, %q}\", t.Type, t.Value) }\n\n// Clone returns a clone of the Token.\nfunc (t *Token) Clone() Token {\n\treturn *t\n}\n\n// EOF is returned by lexers at the end of input.\nvar EOF Token\n\n// TokeniseOptions contains options for tokenisers.\ntype TokeniseOptions struct {\n\t// State to start tokenisation in. Defaults to \"root\".\n\tState string\n\t// Nested tokenisation.\n\tNested bool\n\n\t// If true, all EOLs are converted into LF\n\t// by replacing CRLF and CR\n\tEnsureLF bool\n}\n\n// A Lexer for tokenising source code.\ntype Lexer interface {\n\t// Config describing the features of the Lexer.\n\tConfig() *Config\n\t// Tokenise returns an Iterator over tokens in text.\n\tTokenise(options *TokeniseOptions, text string) (Iterator, error)\n\t// SetRegistry sets the registry this Lexer is associated with.\n\t//\n\t// The registry should be used by the Lexer if it needs to look up other\n\t// lexers.\n\tSetRegistry(registry *LexerRegistry) Lexer\n\t// SetAnalyser sets a function the Lexer should use for scoring how\n\t// likely a fragment of text is to match this lexer, between 0.0 and 1.0.\n\t// A value of 1 indicates high confidence.\n\t//\n\t// Lexers may ignore this if they implement their own analysers.\n\tSetAnalyser(analyser func(text string) float32) Lexer\n\t// AnalyseText scores how likely a fragment of text is to match\n\t// this lexer, between 0.0 and 1.0. A value of 1 indicates high confidence.\n\tAnalyseText(text string) float32\n}\n\n// Lexers is a slice of lexers sortable by name.\ntype Lexers []Lexer\n\nfunc (l Lexers) Len() int      { return len(l) }\nfunc (l Lexers) Swap(i, j int) { l[i], l[j] = l[j], l[i] }\nfunc (l Lexers) Less(i, j int) bool {\n\treturn strings.ToLower(l[i].Config().Name) < strings.ToLower(l[j].Config().Name)\n}\n\n// PrioritisedLexers is a slice of lexers sortable by priority.\ntype PrioritisedLexers []Lexer\n\nfunc (l PrioritisedLexers) Len() int      { return len(l) }\nfunc (l PrioritisedLexers) Swap(i, j int) { l[i], l[j] = l[j], l[i] }\nfunc (l PrioritisedLexers) Less(i, j int) bool {\n\tip := l[i].Config().Priority\n\tif ip == 0 {\n\t\tip = 1\n\t}\n\tjp := l[j].Config().Priority\n\tif jp == 0 {\n\t\tjp = 1\n\t}\n\treturn ip > jp\n}\n\n// Analyser determines how appropriate this lexer is for the given text.\ntype Analyser interface {\n\tAnalyseText(text string) float32\n}\n"
        },
        {
          "name": "lexer_test.go",
          "type": "blob",
          "size": 1.1142578125,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestTokenTypeClassifiers(t *testing.T) {\n\tassert.True(t, GenericDeleted.InCategory(Generic))\n\tassert.True(t, LiteralStringBacktick.InSubCategory(String))\n\tassert.Equal(t, LiteralStringBacktick.String(), \"LiteralStringBacktick\")\n}\n\nfunc TestSimpleLexer(t *testing.T) {\n\tlexer := mustNewLexer(t, &Config{\n\t\tName:      \"INI\",\n\t\tAliases:   []string{\"ini\", \"cfg\"},\n\t\tFilenames: []string{\"*.ini\", \"*.cfg\"},\n\t}, map[string][]Rule{\n\t\t\"root\": {\n\t\t\t{`\\s+`, Whitespace, nil},\n\t\t\t{`;.*?$`, Comment, nil},\n\t\t\t{`\\[.*?\\]$`, Keyword, nil},\n\t\t\t{`(.*?)(\\s*)(=)(\\s*)(.*?)$`, ByGroups(Name, Whitespace, Operator, Whitespace, String), nil},\n\t\t},\n\t})\n\tactual, err := Tokenise(lexer, nil, `\n\t; this is a comment\n\t[section]\n\ta = 10\n`)\n\tassert.NoError(t, err)\n\texpected := []Token{\n\t\t{Whitespace, \"\\n\\t\"},\n\t\t{Comment, \"; this is a comment\"},\n\t\t{Whitespace, \"\\n\\t\"},\n\t\t{Keyword, \"[section]\"},\n\t\t{Whitespace, \"\\n\\t\"},\n\t\t{Name, \"a\"},\n\t\t{Whitespace, \" \"},\n\t\t{Operator, \"=\"},\n\t\t{Whitespace, \" \"},\n\t\t{LiteralString, \"10\"},\n\t\t{Whitespace, \"\\n\"},\n\t}\n\tassert.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "lexers",
          "type": "tree",
          "content": null
        },
        {
          "name": "mutators.go",
          "type": "blob",
          "size": 4.9267578125,
          "content": "package chroma\n\nimport (\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// A Mutator modifies the behaviour of the lexer.\ntype Mutator interface {\n\t// Mutate the lexer state machine as it is processing.\n\tMutate(state *LexerState) error\n}\n\n// SerialisableMutator is a Mutator that can be serialised and deserialised.\ntype SerialisableMutator interface {\n\tMutator\n\tMutatorKind() string\n}\n\n// A LexerMutator is an additional interface that a Mutator can implement\n// to modify the lexer when it is compiled.\ntype LexerMutator interface {\n\t// MutateLexer can be implemented to mutate the lexer itself.\n\t//\n\t// Rules are the lexer rules, state is the state key for the rule the mutator is associated with.\n\tMutateLexer(rules CompiledRules, state string, rule int) error\n}\n\n// A MutatorFunc is a Mutator that mutates the lexer state machine as it is processing.\ntype MutatorFunc func(state *LexerState) error\n\nfunc (m MutatorFunc) Mutate(state *LexerState) error { return m(state) } // nolint\n\ntype multiMutator struct {\n\tMutators []Mutator `xml:\"mutator\"`\n}\n\nfunc (m *multiMutator) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\tfor {\n\t\ttoken, err := d.Token()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch token := token.(type) {\n\t\tcase xml.StartElement:\n\t\t\tmutator, err := unmarshalMutator(d, token)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tm.Mutators = append(m.Mutators, mutator)\n\n\t\tcase xml.EndElement:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (m *multiMutator) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tname := xml.Name{Local: \"mutators\"}\n\tif err := e.EncodeToken(xml.StartElement{Name: name}); err != nil {\n\t\treturn err\n\t}\n\tfor _, m := range m.Mutators {\n\t\tif err := marshalMutator(e, m); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn e.EncodeToken(xml.EndElement{Name: name})\n}\n\nfunc (m *multiMutator) MutatorKind() string { return \"mutators\" }\n\nfunc (m *multiMutator) Mutate(state *LexerState) error {\n\tfor _, modifier := range m.Mutators {\n\t\tif err := modifier.Mutate(state); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Mutators applies a set of Mutators in order.\nfunc Mutators(modifiers ...Mutator) Mutator {\n\treturn &multiMutator{modifiers}\n}\n\ntype includeMutator struct {\n\tState string `xml:\"state,attr\"`\n}\n\n// Include the given state.\nfunc Include(state string) Rule {\n\treturn Rule{Mutator: &includeMutator{state}}\n}\n\nfunc (i *includeMutator) MutatorKind() string { return \"include\" }\n\nfunc (i *includeMutator) Mutate(s *LexerState) error {\n\treturn fmt.Errorf(\"should never reach here Include(%q)\", i.State)\n}\n\nfunc (i *includeMutator) MutateLexer(rules CompiledRules, state string, rule int) error {\n\tincludedRules, ok := rules[i.State]\n\tif !ok {\n\t\treturn fmt.Errorf(\"invalid include state %q\", i.State)\n\t}\n\trules[state] = append(rules[state][:rule], append(includedRules, rules[state][rule+1:]...)...)\n\treturn nil\n}\n\ntype combinedMutator struct {\n\tStates []string `xml:\"state,attr\"`\n}\n\nfunc (c *combinedMutator) MutatorKind() string { return \"combined\" }\n\n// Combined creates a new anonymous state from the given states, and pushes that state.\nfunc Combined(states ...string) Mutator {\n\treturn &combinedMutator{states}\n}\n\nfunc (c *combinedMutator) Mutate(s *LexerState) error {\n\treturn fmt.Errorf(\"should never reach here Combined(%v)\", c.States)\n}\n\nfunc (c *combinedMutator) MutateLexer(rules CompiledRules, state string, rule int) error {\n\tname := \"__combined_\" + strings.Join(c.States, \"__\")\n\tif _, ok := rules[name]; !ok {\n\t\tcombined := []*CompiledRule{}\n\t\tfor _, state := range c.States {\n\t\t\trules, ok := rules[state]\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"invalid combine state %q\", state)\n\t\t\t}\n\t\t\tcombined = append(combined, rules...)\n\t\t}\n\t\trules[name] = combined\n\t}\n\trules[state][rule].Mutator = Push(name)\n\treturn nil\n}\n\ntype pushMutator struct {\n\tStates []string `xml:\"state,attr\"`\n}\n\nfunc (p *pushMutator) MutatorKind() string { return \"push\" }\n\nfunc (p *pushMutator) Mutate(s *LexerState) error {\n\tif len(p.States) == 0 {\n\t\ts.Stack = append(s.Stack, s.State)\n\t} else {\n\t\tfor _, state := range p.States {\n\t\t\tif state == \"#pop\" {\n\t\t\t\ts.Stack = s.Stack[:len(s.Stack)-1]\n\t\t\t} else {\n\t\t\t\ts.Stack = append(s.Stack, state)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Push states onto the stack.\nfunc Push(states ...string) Mutator {\n\treturn &pushMutator{states}\n}\n\ntype popMutator struct {\n\tDepth int `xml:\"depth,attr\"`\n}\n\nfunc (p *popMutator) MutatorKind() string { return \"pop\" }\n\nfunc (p *popMutator) Mutate(state *LexerState) error {\n\tif len(state.Stack) == 0 {\n\t\treturn fmt.Errorf(\"nothing to pop\")\n\t}\n\tstate.Stack = state.Stack[:len(state.Stack)-p.Depth]\n\treturn nil\n}\n\n// Pop state from the stack when rule matches.\nfunc Pop(n int) Mutator {\n\treturn &popMutator{n}\n}\n\n// Default returns a Rule that applies a set of Mutators.\nfunc Default(mutators ...Mutator) Rule {\n\treturn Rule{Mutator: Mutators(mutators...)}\n}\n\n// Stringify returns the raw string for a set of tokens.\nfunc Stringify(tokens ...Token) string {\n\tout := []string{}\n\tfor _, t := range tokens {\n\t\tout = append(out, t.Value)\n\t}\n\treturn strings.Join(out, \"\")\n}\n"
        },
        {
          "name": "mutators_test.go",
          "type": "blob",
          "size": 1.2490234375,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestInclude(t *testing.T) {\n\tinclude := Include(\"other\")\n\tactual := CompiledRules{\n\t\t\"root\": {{Rule: include}},\n\t\t\"other\": {\n\t\t\t{Rule: Rule{Pattern: \"//.+\", Type: Comment}},\n\t\t\t{Rule: Rule{Pattern: `\"[^\"]*\"`, Type: String}},\n\t\t},\n\t}\n\tlexer := &RegexLexer{rules: actual}\n\terr := include.Mutator.(LexerMutator).MutateLexer(lexer.rules, \"root\", 0)\n\tassert.NoError(t, err)\n\texpected := CompiledRules{\n\t\t\"root\": {\n\t\t\t{Rule: Rule{\n\t\t\t\tPattern: \"//.+\",\n\t\t\t\tType:    Comment,\n\t\t\t}},\n\t\t\t{Rule: Rule{\n\t\t\t\tPattern: `\"[^\"]*\"`,\n\t\t\t\tType:    String,\n\t\t\t}},\n\t\t},\n\t\t\"other\": {\n\t\t\t{Rule: Rule{\n\t\t\t\tPattern: \"//.+\",\n\t\t\t\tType:    Comment,\n\t\t\t}},\n\t\t\t{Rule: Rule{\n\t\t\t\tPattern: `\"[^\"]*\"`,\n\t\t\t\tType:    String,\n\t\t\t}},\n\t\t},\n\t}\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestCombine(t *testing.T) {\n\tl := mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\":  {{`hello`, String, Combined(\"world\", \"bye\", \"space\")}},\n\t\t\"world\": {{`world`, Name, nil}},\n\t\t\"bye\":   {{`bye`, Name, nil}},\n\t\t\"space\": {{`\\s+`, Whitespace, nil}},\n\t})\n\tit, err := l.Tokenise(nil, \"hello world\")\n\tassert.NoError(t, err)\n\texpected := []Token{{String, `hello`}, {Whitespace, ` `}, {Name, `world`}}\n\tassert.Equal(t, expected, it.Tokens())\n}\n"
        },
        {
          "name": "pygments-lexers.txt",
          "type": "blob",
          "size": 11.1923828125,
          "content": "Generated with:\n\n  g 'class.*RegexLexer' | pawk --strict -F: '\"pygments.lexers.%s.%s\" % (f[0].split(\".\")[0], f[2].split()[1].split(\"(\")[0])' > lexers.txt\n\nkotlin:\n  invalid unicode escape sequences\n  FIXED: Have to disable wide Unicode characters in unistring.py\n\npygments.lexers.ambient.AmbientTalkLexer\npygments.lexers.ampl.AmplLexer\npygments.lexers.actionscript.ActionScriptLexer\npygments.lexers.actionscript.ActionScript3Lexer\npygments.lexers.actionscript.MxmlLexer\npygments.lexers.algebra.GAPLexer\npygments.lexers.algebra.MathematicaLexer\npygments.lexers.algebra.MuPADLexer\npygments.lexers.algebra.BCLexer\npygments.lexers.apl.APLLexer\npygments.lexers.bibtex.BibTeXLexer\npygments.lexers.bibtex.BSTLexer\npygments.lexers.basic.BlitzMaxLexer\npygments.lexers.basic.BlitzBasicLexer\npygments.lexers.basic.MonkeyLexer\npygments.lexers.basic.CbmBasicV2Lexer\npygments.lexers.basic.QBasicLexer\npygments.lexers.automation.AutohotkeyLexer\npygments.lexers.automation.AutoItLexer\npygments.lexers.archetype.AtomsLexer\npygments.lexers.c_like.ClayLexer\npygments.lexers.c_like.ValaLexer\npygments.lexers.asm.GasLexer\npygments.lexers.asm.ObjdumpLexer\npygments.lexers.asm.HsailLexer\npygments.lexers.asm.LlvmLexer\npygments.lexers.asm.NasmLexer\npygments.lexers.asm.TasmLexer\npygments.lexers.asm.Ca65Lexer\npygments.lexers.business.CobolLexer\npygments.lexers.business.ABAPLexer\npygments.lexers.business.OpenEdgeLexer\npygments.lexers.business.GoodDataCLLexer\npygments.lexers.business.MaqlLexer\npygments.lexers.capnproto.CapnProtoLexer\npygments.lexers.chapel.ChapelLexer\npygments.lexers.clean.CleanLexer\npygments.lexers.c_cpp.CFamilyLexer\npygments.lexers.console.VCTreeStatusLexer\npygments.lexers.console.PyPyLogLexer\npygments.lexers.csound.CsoundLexer\npygments.lexers.csound.CsoundDocumentLexer\npygments.lexers.csound.CsoundDocumentLexer\npygments.lexers.crystal.CrystalLexer\npygments.lexers.dalvik.SmaliLexer\npygments.lexers.css.CssLexer\npygments.lexers.css.SassLexer\npygments.lexers.css.ScssLexer\npygments.lexers.configs.IniLexer\npygments.lexers.configs.RegeditLexer\npygments.lexers.configs.PropertiesLexer\npygments.lexers.configs.KconfigLexer\npygments.lexers.configs.Cfengine3Lexer\npygments.lexers.configs.ApacheConfLexer\npygments.lexers.configs.SquidConfLexer\npygments.lexers.configs.NginxConfLexer\npygments.lexers.configs.LighttpdConfLexer\npygments.lexers.configs.DockerLexer\npygments.lexers.configs.TerraformLexer\npygments.lexers.configs.TermcapLexer\npygments.lexers.configs.TerminfoLexer\npygments.lexers.configs.PkgConfigLexer\npygments.lexers.configs.PacmanConfLexer\npygments.lexers.data.YamlLexer\npygments.lexers.data.JsonLexer\npygments.lexers.diff.DiffLexer\npygments.lexers.diff.DarcsPatchLexer\npygments.lexers.diff.WDiffLexer\npygments.lexers.dotnet.CSharpLexer\npygments.lexers.dotnet.NemerleLexer\npygments.lexers.dotnet.BooLexer\npygments.lexers.dotnet.VbNetLexer\npygments.lexers.dotnet.GenericAspxLexer\npygments.lexers.dotnet.FSharpLexer\npygments.lexers.dylan.DylanLexer\npygments.lexers.dylan.DylanLidLexer\npygments.lexers.ecl.ECLLexer\npygments.lexers.eiffel.EiffelLexer\npygments.lexers.dsls.ProtoBufLexer\npygments.lexers.dsls.ThriftLexer\npygments.lexers.dsls.BroLexer\npygments.lexers.dsls.PuppetLexer\npygments.lexers.dsls.RslLexer\npygments.lexers.dsls.MscgenLexer\npygments.lexers.dsls.VGLLexer\npygments.lexers.dsls.AlloyLexer\npygments.lexers.dsls.PanLexer\npygments.lexers.dsls.CrmshLexer\npygments.lexers.dsls.FlatlineLexer\npygments.lexers.dsls.SnowballLexer\npygments.lexers.elm.ElmLexer\npygments.lexers.erlang.ErlangLexer\npygments.lexers.erlang.ElixirLexer\npygments.lexers.ezhil.EzhilLexer\npygments.lexers.esoteric.BrainfuckLexer\npygments.lexers.esoteric.BefungeLexer\npygments.lexers.esoteric.CAmkESLexer\npygments.lexers.esoteric.CapDLLexer\npygments.lexers.esoteric.RedcodeLexer\npygments.lexers.esoteric.AheuiLexer\npygments.lexers.factor.FactorLexer\npygments.lexers.fantom.FantomLexer\npygments.lexers.felix.FelixLexer\npygments.lexers.forth.ForthLexer\npygments.lexers.fortran.FortranLexer\npygments.lexers.fortran.FortranFixedLexer\npygments.lexers.go.GoLexer\npygments.lexers.foxpro.FoxProLexer\npygments.lexers.graph.CypherLexer\npygments.lexers.grammar_notation.BnfLexer\npygments.lexers.grammar_notation.AbnfLexer\npygments.lexers.grammar_notation.JsgfLexer\npygments.lexers.graphics.GLShaderLexer\npygments.lexers.graphics.PostScriptLexer\npygments.lexers.graphics.AsymptoteLexer\npygments.lexers.graphics.GnuplotLexer\npygments.lexers.graphics.PovrayLexer\npygments.lexers.hexdump.HexdumpLexer\npygments.lexers.haskell.HaskellLexer\npygments.lexers.haskell.IdrisLexer\npygments.lexers.haskell.AgdaLexer\npygments.lexers.haskell.CryptolLexer\npygments.lexers.haskell.KokaLexer\npygments.lexers.haxe.HaxeLexer\npygments.lexers.haxe.HxmlLexer\npygments.lexers.hdl.VerilogLexer\npygments.lexers.hdl.SystemVerilogLexer\npygments.lexers.hdl.VhdlLexer\npygments.lexers.idl.IDLLexer\npygments.lexers.inferno.LimboLexer\npygments.lexers.igor.IgorLexer\npygments.lexers.html.HtmlLexer\npygments.lexers.html.DtdLexer\npygments.lexers.html.XmlLexer\npygments.lexers.html.HamlLexer\npygments.lexers.html.ScamlLexer\npygments.lexers.html.PugLexer\npygments.lexers.installers.NSISLexer\npygments.lexers.installers.RPMSpecLexer\npygments.lexers.installers.SourcesListLexer\npygments.lexers.installers.DebianControlLexer\npygments.lexers.iolang.IoLexer\npygments.lexers.julia.JuliaLexer\npygments.lexers.int_fiction.Inform6Lexer\npygments.lexers.int_fiction.Inform7Lexer\npygments.lexers.int_fiction.Tads3Lexer\npygments.lexers.make.BaseMakefileLexer\npygments.lexers.make.CMakeLexer\npygments.lexers.javascript.JavascriptLexer\npygments.lexers.javascript.KalLexer\npygments.lexers.javascript.LiveScriptLexer\npygments.lexers.javascript.DartLexer\npygments.lexers.javascript.TypeScriptLexer\npygments.lexers.javascript.LassoLexer\npygments.lexers.javascript.ObjectiveJLexer\npygments.lexers.javascript.CoffeeScriptLexer\npygments.lexers.javascript.MaskLexer\npygments.lexers.javascript.EarlGreyLexer\npygments.lexers.javascript.JuttleLexer\npygments.lexers.jvm.JavaLexer\npygments.lexers.jvm.ScalaLexer\npygments.lexers.jvm.GosuLexer\npygments.lexers.jvm.GroovyLexer\npygments.lexers.jvm.IokeLexer\npygments.lexers.jvm.ClojureLexer\npygments.lexers.jvm.TeaLangLexer\npygments.lexers.jvm.CeylonLexer\npygments.lexers.jvm.KotlinLexer\npygments.lexers.jvm.XtendLexer\npygments.lexers.jvm.PigLexer\npygments.lexers.jvm.GoloLexer\npygments.lexers.jvm.JasminLexer\npygments.lexers.markup.BBCodeLexer\npygments.lexers.markup.MoinWikiLexer\npygments.lexers.markup.RstLexer\npygments.lexers.markup.TexLexer\npygments.lexers.markup.GroffLexer\npygments.lexers.markup.MozPreprocHashLexer\npygments.lexers.markup.MarkdownLexer\npygments.lexers.ml.SMLLexer\npygments.lexers.ml.OcamlLexer\npygments.lexers.ml.OpaLexer\npygments.lexers.modeling.ModelicaLexer\npygments.lexers.modeling.BugsLexer\npygments.lexers.modeling.JagsLexer\npygments.lexers.modeling.StanLexer\npygments.lexers.matlab.MatlabLexer\npygments.lexers.matlab.OctaveLexer\npygments.lexers.matlab.ScilabLexer\npygments.lexers.monte.MonteLexer\npygments.lexers.lisp.SchemeLexer\npygments.lexers.lisp.CommonLispLexer\npygments.lexers.lisp.HyLexer\npygments.lexers.lisp.RacketLexer\npygments.lexers.lisp.NewLispLexer\npygments.lexers.lisp.EmacsLispLexer\npygments.lexers.lisp.ShenLexer\npygments.lexers.lisp.XtlangLexer\npygments.lexers.modula2.Modula2Lexer\npygments.lexers.ncl.NCLLexer\npygments.lexers.nim.NimLexer\npygments.lexers.nit.NitLexer\npygments.lexers.nix.NixLexer\npygments.lexers.oberon.ComponentPascalLexer\npygments.lexers.ooc.OocLexer\npygments.lexers.objective.SwiftLexer\npygments.lexers.parasail.ParaSailLexer\npygments.lexers.pawn.SourcePawnLexer\npygments.lexers.pawn.PawnLexer\npygments.lexers.pascal.AdaLexer\npygments.lexers.parsers.RagelLexer\npygments.lexers.parsers.RagelEmbeddedLexer\npygments.lexers.parsers.AntlrLexer\npygments.lexers.parsers.TreetopBaseLexer\npygments.lexers.parsers.EbnfLexer\npygments.lexers.php.ZephirLexer\npygments.lexers.php.PhpLexer\npygments.lexers.perl.PerlLexer\npygments.lexers.perl.Perl6Lexer\npygments.lexers.praat.PraatLexer\npygments.lexers.prolog.PrologLexer\npygments.lexers.prolog.LogtalkLexer\npygments.lexers.qvt.QVToLexer\npygments.lexers.rdf.SparqlLexer\npygments.lexers.rdf.TurtleLexer\npygments.lexers.python.PythonLexer\npygments.lexers.python.Python3Lexer\npygments.lexers.python.PythonTracebackLexer\npygments.lexers.python.Python3TracebackLexer\npygments.lexers.python.CythonLexer\npygments.lexers.python.DgLexer\npygments.lexers.rebol.RebolLexer\npygments.lexers.rebol.RedLexer\npygments.lexers.resource.ResourceLexer\npygments.lexers.rnc.RNCCompactLexer\npygments.lexers.roboconf.RoboconfGraphLexer\npygments.lexers.roboconf.RoboconfInstancesLexer\npygments.lexers.rust.RustLexer\npygments.lexers.ruby.RubyLexer\npygments.lexers.ruby.FancyLexer\npygments.lexers.sas.SASLexer\npygments.lexers.smalltalk.SmalltalkLexer\npygments.lexers.smalltalk.NewspeakLexer\npygments.lexers.smv.NuSMVLexer\npygments.lexers.shell.BashLexer\npygments.lexers.shell.BatchLexer\npygments.lexers.shell.TcshLexer\npygments.lexers.shell.PowerShellLexer\npygments.lexers.shell.FishShellLexer\npygments.lexers.snobol.SnobolLexer\npygments.lexers.scripting.LuaLexer\npygments.lexers.scripting.ChaiscriptLexer\npygments.lexers.scripting.LSLLexer\npygments.lexers.scripting.AppleScriptLexer\npygments.lexers.scripting.RexxLexer\npygments.lexers.scripting.MOOCodeLexer\npygments.lexers.scripting.HybrisLexer\npygments.lexers.scripting.EasytrieveLexer\npygments.lexers.scripting.JclLexer\npygments.lexers.supercollider.SuperColliderLexer\npygments.lexers.stata.StataLexer\npygments.lexers.tcl.TclLexer\npygments.lexers.sql.PostgresLexer\npygments.lexers.sql.PlPgsqlLexer\npygments.lexers.sql.PsqlRegexLexer\npygments.lexers.sql.SqlLexer\npygments.lexers.sql.TransactSqlLexer\npygments.lexers.sql.MySqlLexer\npygments.lexers.sql.RqlLexer\npygments.lexers.testing.GherkinLexer\npygments.lexers.testing.TAPLexer\npygments.lexers.textedit.AwkLexer\npygments.lexers.textedit.VimLexer\npygments.lexers.textfmts.IrcLogsLexer\npygments.lexers.textfmts.GettextLexer\npygments.lexers.textfmts.HttpLexer\npygments.lexers.textfmts.TodotxtLexer\npygments.lexers.trafficscript.RtsLexer\npygments.lexers.theorem.CoqLexer\npygments.lexers.theorem.IsabelleLexer\npygments.lexers.theorem.LeanLexer\npygments.lexers.templates.SmartyLexer\npygments.lexers.templates.VelocityLexer\npygments.lexers.templates.DjangoLexer\npygments.lexers.templates.MyghtyLexer\npygments.lexers.templates.MasonLexer\npygments.lexers.templates.MakoLexer\npygments.lexers.templates.CheetahLexer\npygments.lexers.templates.GenshiTextLexer\npygments.lexers.templates.GenshiMarkupLexer\npygments.lexers.templates.JspRootLexer\npygments.lexers.templates.EvoqueLexer\npygments.lexers.templates.ColdfusionLexer\npygments.lexers.templates.ColdfusionMarkupLexer\npygments.lexers.templates.TeaTemplateRootLexer\npygments.lexers.templates.HandlebarsLexer\npygments.lexers.templates.LiquidLexer\npygments.lexers.templates.TwigLexer\npygments.lexers.templates.Angular2Lexer\npygments.lexers.urbi.UrbiscriptLexer\npygments.lexers.typoscript.TypoScriptCssDataLexer\npygments.lexers.typoscript.TypoScriptHtmlDataLexer\npygments.lexers.typoscript.TypoScriptLexer\npygments.lexers.varnish.VCLLexer\npygments.lexers.verification.BoogieLexer\npygments.lexers.verification.SilverLexer\npygments.lexers.x10.X10Lexer\npygments.lexers.whiley.WhileyLexer\npygments.lexers.xorg.XorgLexer\npygments.lexers.webmisc.DuelLexer\npygments.lexers.webmisc.XQueryLexer\npygments.lexers.webmisc.QmlLexer\npygments.lexers.webmisc.CirruLexer\npygments.lexers.webmisc.SlimLexer\n"
        },
        {
          "name": "quick",
          "type": "tree",
          "content": null
        },
        {
          "name": "regexp.go",
          "type": "blob",
          "size": 12.0341796875,
          "content": "package chroma\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/dlclark/regexp2\"\n)\n\n// A Rule is the fundamental matching unit of the Regex lexer state machine.\ntype Rule struct {\n\tPattern string\n\tType    Emitter\n\tMutator Mutator\n}\n\n// Words creates a regex that matches any of the given literal words.\nfunc Words(prefix, suffix string, words ...string) string {\n\tsort.Slice(words, func(i, j int) bool {\n\t\treturn len(words[j]) < len(words[i])\n\t})\n\tfor i, word := range words {\n\t\twords[i] = regexp.QuoteMeta(word)\n\t}\n\treturn prefix + `(` + strings.Join(words, `|`) + `)` + suffix\n}\n\n// Tokenise text using lexer, returning tokens as a slice.\nfunc Tokenise(lexer Lexer, options *TokeniseOptions, text string) ([]Token, error) {\n\tvar out []Token\n\tit, err := lexer.Tokenise(options, text)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor t := it(); t != EOF; t = it() {\n\t\tout = append(out, t)\n\t}\n\treturn out, nil\n}\n\n// Rules maps from state to a sequence of Rules.\ntype Rules map[string][]Rule\n\n// Rename clones rules then a rule.\nfunc (r Rules) Rename(oldRule, newRule string) Rules {\n\tr = r.Clone()\n\tr[newRule] = r[oldRule]\n\tdelete(r, oldRule)\n\treturn r\n}\n\n// Clone returns a clone of the Rules.\nfunc (r Rules) Clone() Rules {\n\tout := map[string][]Rule{}\n\tfor key, rules := range r {\n\t\tout[key] = make([]Rule, len(rules))\n\t\tcopy(out[key], rules)\n\t}\n\treturn out\n}\n\n// Merge creates a clone of \"r\" then merges \"rules\" into the clone.\nfunc (r Rules) Merge(rules Rules) Rules {\n\tout := r.Clone()\n\tfor k, v := range rules.Clone() {\n\t\tout[k] = v\n\t}\n\treturn out\n}\n\n// MustNewLexer creates a new Lexer with deferred rules generation or panics.\nfunc MustNewLexer(config *Config, rules func() Rules) *RegexLexer {\n\tlexer, err := NewLexer(config, rules)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn lexer\n}\n\n// NewLexer creates a new regex-based Lexer.\n//\n// \"rules\" is a state machine transition map. Each key is a state. Values are sets of rules\n// that match input, optionally modify lexer state, and output tokens.\nfunc NewLexer(config *Config, rulesFunc func() Rules) (*RegexLexer, error) {\n\tif config == nil {\n\t\tconfig = &Config{}\n\t}\n\tfor _, glob := range append(config.Filenames, config.AliasFilenames...) {\n\t\t_, err := filepath.Match(glob, \"\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %q is not a valid glob: %w\", config.Name, glob, err)\n\t\t}\n\t}\n\tr := &RegexLexer{\n\t\tconfig:         config,\n\t\tfetchRulesFunc: func() (Rules, error) { return rulesFunc(), nil },\n\t}\n\t// One-off code to generate XML lexers in the Chroma source tree.\n\t// var nameCleanRe = regexp.MustCompile(`[^-+A-Za-z0-9_]`)\n\t// name := strings.ToLower(nameCleanRe.ReplaceAllString(config.Name, \"_\"))\n\t// data, err := Marshal(r)\n\t// if err != nil {\n\t// \tif errors.Is(err, ErrNotSerialisable) {\n\t// \t\tfmt.Fprintf(os.Stderr, \"warning: %q: %s\\n\", name, err)\n\t// \t\treturn r, nil\n\t// \t}\n\t// \treturn nil, err\n\t// }\n\t// _, file, _, ok := runtime.Caller(2)\n\t// if !ok {\n\t// \tpanic(\"??\")\n\t// }\n\t// fmt.Println(file)\n\t// if strings.Contains(file, \"/lexers/\") {\n\t// \tdir := filepath.Join(filepath.Dir(file), \"embedded\")\n\t// \terr = os.MkdirAll(dir, 0700)\n\t// \tif err != nil {\n\t// \t\treturn nil, err\n\t// \t}\n\t// \tfilename := filepath.Join(dir, name) + \".xml\"\n\t// \tfmt.Println(filename)\n\t// \terr = ioutil.WriteFile(filename, data, 0600)\n\t// \tif err != nil {\n\t// \t\treturn nil, err\n\t// \t}\n\t// }\n\treturn r, nil\n}\n\n// Trace enables debug tracing.\nfunc (r *RegexLexer) Trace(trace bool) *RegexLexer {\n\tr.trace = trace\n\treturn r\n}\n\n// A CompiledRule is a Rule with a pre-compiled regex.\n//\n// Note that regular expressions are lazily compiled on first use of the lexer.\ntype CompiledRule struct {\n\tRule\n\tRegexp *regexp2.Regexp\n\tflags  string\n}\n\n// CompiledRules is a map of rule name to sequence of compiled rules in that rule.\ntype CompiledRules map[string][]*CompiledRule\n\n// LexerState contains the state for a single lex.\ntype LexerState struct {\n\tLexer    *RegexLexer\n\tRegistry *LexerRegistry\n\tText     []rune\n\tPos      int\n\tRules    CompiledRules\n\tStack    []string\n\tState    string\n\tRule     int\n\t// Group matches.\n\tGroups []string\n\t// Named Group matches.\n\tNamedGroups map[string]string\n\t// Custum context for mutators.\n\tMutatorContext map[interface{}]interface{}\n\titeratorStack  []Iterator\n\toptions        *TokeniseOptions\n\tnewlineAdded   bool\n}\n\n// Set mutator context.\nfunc (l *LexerState) Set(key interface{}, value interface{}) {\n\tl.MutatorContext[key] = value\n}\n\n// Get mutator context.\nfunc (l *LexerState) Get(key interface{}) interface{} {\n\treturn l.MutatorContext[key]\n}\n\n// Iterator returns the next Token from the lexer.\nfunc (l *LexerState) Iterator() Token { // nolint: gocognit\n\tend := len(l.Text)\n\tif l.newlineAdded {\n\t\tend--\n\t}\n\tfor l.Pos < end && len(l.Stack) > 0 {\n\t\t// Exhaust the iterator stack, if any.\n\t\tfor len(l.iteratorStack) > 0 {\n\t\t\tn := len(l.iteratorStack) - 1\n\t\t\tt := l.iteratorStack[n]()\n\t\t\tif t.Type == Ignore {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif t == EOF {\n\t\t\t\tl.iteratorStack = l.iteratorStack[:n]\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn t\n\t\t}\n\n\t\tl.State = l.Stack[len(l.Stack)-1]\n\t\tif l.Lexer.trace {\n\t\t\tfmt.Fprintf(os.Stderr, \"%s: pos=%d, text=%q\\n\", l.State, l.Pos, string(l.Text[l.Pos:]))\n\t\t}\n\t\tselectedRule, ok := l.Rules[l.State]\n\t\tif !ok {\n\t\t\tpanic(\"unknown state \" + l.State)\n\t\t}\n\t\truleIndex, rule, groups, namedGroups := matchRules(l.Text, l.Pos, selectedRule)\n\t\t// No match.\n\t\tif groups == nil {\n\t\t\t// From Pygments :\\\n\t\t\t//\n\t\t\t// If the RegexLexer encounters a newline that is flagged as an error token, the stack is\n\t\t\t// emptied and the lexer continues scanning in the 'root' state. This can help producing\n\t\t\t// error-tolerant highlighting for erroneous input, e.g. when a single-line string is not\n\t\t\t// closed.\n\t\t\tif l.Text[l.Pos] == '\\n' && l.State != l.options.State {\n\t\t\t\tl.Stack = []string{l.options.State}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tl.Pos++\n\t\t\treturn Token{Error, string(l.Text[l.Pos-1 : l.Pos])}\n\t\t}\n\t\tl.Rule = ruleIndex\n\t\tl.Groups = groups\n\t\tl.NamedGroups = namedGroups\n\t\tl.Pos += utf8.RuneCountInString(groups[0])\n\t\tif rule.Mutator != nil {\n\t\t\tif err := rule.Mutator.Mutate(l); err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t\tif rule.Type != nil {\n\t\t\tl.iteratorStack = append(l.iteratorStack, rule.Type.Emit(l.Groups, l))\n\t\t}\n\t}\n\t// Exhaust the IteratorStack, if any.\n\t// Duplicate code, but eh.\n\tfor len(l.iteratorStack) > 0 {\n\t\tn := len(l.iteratorStack) - 1\n\t\tt := l.iteratorStack[n]()\n\t\tif t.Type == Ignore {\n\t\t\tcontinue\n\t\t}\n\t\tif t == EOF {\n\t\t\tl.iteratorStack = l.iteratorStack[:n]\n\t\t\tcontinue\n\t\t}\n\t\treturn t\n\t}\n\n\t// If we get to here and we still have text, return it as an error.\n\tif l.Pos != len(l.Text) && len(l.Stack) == 0 {\n\t\tvalue := string(l.Text[l.Pos:])\n\t\tl.Pos = len(l.Text)\n\t\treturn Token{Type: Error, Value: value}\n\t}\n\treturn EOF\n}\n\n// RegexLexer is the default lexer implementation used in Chroma.\ntype RegexLexer struct {\n\tregistry *LexerRegistry // The LexerRegistry this Lexer is associated with, if any.\n\tconfig   *Config\n\tanalyser func(text string) float32\n\ttrace    bool\n\n\tmu             sync.Mutex\n\tcompiled       bool\n\trawRules       Rules\n\trules          map[string][]*CompiledRule\n\tfetchRulesFunc func() (Rules, error)\n\tcompileOnce    sync.Once\n}\n\nfunc (r *RegexLexer) String() string {\n\treturn r.config.Name\n}\n\n// Rules in the Lexer.\nfunc (r *RegexLexer) Rules() (Rules, error) {\n\tif err := r.needRules(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn r.rawRules, nil\n}\n\n// SetRegistry the lexer will use to lookup other lexers if necessary.\nfunc (r *RegexLexer) SetRegistry(registry *LexerRegistry) Lexer {\n\tr.registry = registry\n\treturn r\n}\n\n// SetAnalyser sets the analyser function used to perform content inspection.\nfunc (r *RegexLexer) SetAnalyser(analyser func(text string) float32) Lexer {\n\tr.analyser = analyser\n\treturn r\n}\n\n// AnalyseText scores how likely a fragment of text is to match this lexer, between 0.0 and 1.0.\nfunc (r *RegexLexer) AnalyseText(text string) float32 {\n\tif r.analyser != nil {\n\t\treturn r.analyser(text)\n\t}\n\treturn 0\n}\n\n// SetConfig replaces the Config for this Lexer.\nfunc (r *RegexLexer) SetConfig(config *Config) *RegexLexer {\n\tr.config = config\n\treturn r\n}\n\n// Config returns the Config for this Lexer.\nfunc (r *RegexLexer) Config() *Config {\n\treturn r.config\n}\n\n// Regex compilation is deferred until the lexer is used. This is to avoid significant init() time costs.\nfunc (r *RegexLexer) maybeCompile() (err error) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\tif r.compiled {\n\t\treturn nil\n\t}\n\tfor state, rules := range r.rules {\n\t\tfor i, rule := range rules {\n\t\t\tif rule.Regexp == nil {\n\t\t\t\tpattern := \"(?:\" + rule.Pattern + \")\"\n\t\t\t\tif rule.flags != \"\" {\n\t\t\t\t\tpattern = \"(?\" + rule.flags + \")\" + pattern\n\t\t\t\t}\n\t\t\t\tpattern = `\\G` + pattern\n\t\t\t\trule.Regexp, err = regexp2.Compile(pattern, 0)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to compile rule %s.%d: %s\", state, i, err)\n\t\t\t\t}\n\t\t\t\trule.Regexp.MatchTimeout = time.Millisecond * 250\n\t\t\t}\n\t\t}\n\t}\nrestart:\n\tseen := map[LexerMutator]bool{}\n\tfor state := range r.rules {\n\t\tfor i := range len(r.rules[state]) {\n\t\t\trule := r.rules[state][i]\n\t\t\tif compile, ok := rule.Mutator.(LexerMutator); ok {\n\t\t\t\tif seen[compile] {\n\t\t\t\t\treturn fmt.Errorf(\"saw mutator %T twice; this should not happen\", compile)\n\t\t\t\t}\n\t\t\t\tseen[compile] = true\n\t\t\t\tif err := compile.MutateLexer(r.rules, state, i); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\t// Process the rules again in case the mutator added/removed rules.\n\t\t\t\t//\n\t\t\t\t// This sounds bad, but shouldn't be significant in practice.\n\t\t\t\tgoto restart\n\t\t\t}\n\t\t}\n\t}\n\tr.compiled = true\n\treturn nil\n}\n\nfunc (r *RegexLexer) fetchRules() error {\n\trules, err := r.fetchRulesFunc()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s: failed to compile rules: %w\", r.config.Name, err)\n\t}\n\tif _, ok := rules[\"root\"]; !ok {\n\t\treturn fmt.Errorf(\"no \\\"root\\\" state\")\n\t}\n\tcompiledRules := map[string][]*CompiledRule{}\n\tfor state, rules := range rules {\n\t\tcompiledRules[state] = nil\n\t\tfor _, rule := range rules {\n\t\t\tflags := \"\"\n\t\t\tif !r.config.NotMultiline {\n\t\t\t\tflags += \"m\"\n\t\t\t}\n\t\t\tif r.config.CaseInsensitive {\n\t\t\t\tflags += \"i\"\n\t\t\t}\n\t\t\tif r.config.DotAll {\n\t\t\t\tflags += \"s\"\n\t\t\t}\n\t\t\tcompiledRules[state] = append(compiledRules[state], &CompiledRule{Rule: rule, flags: flags})\n\t\t}\n\t}\n\n\tr.rawRules = rules\n\tr.rules = compiledRules\n\treturn nil\n}\n\nfunc (r *RegexLexer) needRules() error {\n\tvar err error\n\tif r.fetchRulesFunc != nil {\n\t\tr.compileOnce.Do(func() {\n\t\t\terr = r.fetchRules()\n\t\t})\n\t}\n\tif err := r.maybeCompile(); err != nil {\n\t\treturn err\n\t}\n\treturn err\n}\n\n// Tokenise text using lexer, returning an iterator.\nfunc (r *RegexLexer) Tokenise(options *TokeniseOptions, text string) (Iterator, error) {\n\terr := r.needRules()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif options == nil {\n\t\toptions = defaultOptions\n\t}\n\tif options.EnsureLF {\n\t\ttext = ensureLF(text)\n\t}\n\tnewlineAdded := false\n\tif !options.Nested && r.config.EnsureNL && !strings.HasSuffix(text, \"\\n\") {\n\t\ttext += \"\\n\"\n\t\tnewlineAdded = true\n\t}\n\tstate := &LexerState{\n\t\tRegistry:       r.registry,\n\t\tnewlineAdded:   newlineAdded,\n\t\toptions:        options,\n\t\tLexer:          r,\n\t\tText:           []rune(text),\n\t\tStack:          []string{options.State},\n\t\tRules:          r.rules,\n\t\tMutatorContext: map[interface{}]interface{}{},\n\t}\n\treturn state.Iterator, nil\n}\n\n// MustRules is like Rules() but will panic on error.\nfunc (r *RegexLexer) MustRules() Rules {\n\trules, err := r.Rules()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn rules\n}\n\nfunc matchRules(text []rune, pos int, rules []*CompiledRule) (int, *CompiledRule, []string, map[string]string) {\n\tfor i, rule := range rules {\n\t\tmatch, err := rule.Regexp.FindRunesMatchStartingAt(text, pos)\n\t\tif match != nil && err == nil && match.Index == pos {\n\t\t\tgroups := []string{}\n\t\t\tnamedGroups := make(map[string]string)\n\t\t\tfor _, g := range match.Groups() {\n\t\t\t\tnamedGroups[g.Name] = g.String()\n\t\t\t\tgroups = append(groups, g.String())\n\t\t\t}\n\t\t\treturn i, rule, groups, namedGroups\n\t\t}\n\t}\n\treturn 0, &CompiledRule{}, nil, nil\n}\n\n// replace \\r and \\r\\n with \\n\n// same as strings.ReplaceAll but more efficient\nfunc ensureLF(text string) string {\n\tbuf := make([]byte, len(text))\n\tvar j int\n\tfor i := range len(text) {\n\t\tc := text[i]\n\t\tif c == '\\r' {\n\t\t\tif i < len(text)-1 && text[i+1] == '\\n' {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc = '\\n'\n\t\t}\n\t\tbuf[j] = c\n\t\tj++\n\t}\n\treturn string(buf[:j])\n}\n"
        },
        {
          "name": "regexp_test.go",
          "type": "blob",
          "size": 4.9501953125,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc mustNewLexer(t *testing.T, config *Config, rules Rules) *RegexLexer { // nolint: forbidigo\n\tlexer, err := NewLexer(config, func() Rules {\n\t\treturn rules\n\t})\n\tassert.NoError(t, err)\n\treturn lexer\n}\n\nfunc TestNewlineAtEndOfFile(t *testing.T) {\n\tl := Coalesce(mustNewLexer(t, &Config{EnsureNL: true}, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`(\\w+)(\\n)`, ByGroups(Keyword, Whitespace), nil},\n\t\t},\n\t}))\n\tit, err := l.Tokenise(nil, `hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{Keyword, \"hello\"}, {Whitespace, \"\\n\"}}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`(\\w+)(\\n)`, ByGroups(Keyword, Whitespace), nil},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(nil, `hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{Error, \"hello\"}}, it.Tokens())\n}\n\nfunc TestMatchingAtStart(t *testing.T) {\n\tl := Coalesce(mustNewLexer(t, &Config{}, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`\\s+`, Whitespace, nil},\n\t\t\t{`^-`, Punctuation, Push(\"directive\")},\n\t\t\t{`->`, Operator, nil},\n\t\t},\n\t\t\"directive\": {\n\t\t\t{\"module\", NameEntity, Pop(1)},\n\t\t},\n\t}))\n\tit, err := l.Tokenise(nil, `-module ->`)\n\tassert.NoError(t, err)\n\tassert.Equal(t,\n\t\t[]Token{{Punctuation, \"-\"}, {NameEntity, \"module\"}, {Whitespace, \" \"}, {Operator, \"->\"}},\n\t\tit.Tokens())\n}\n\nfunc TestEnsureLFOption(t *testing.T) {\n\tl := Coalesce(mustNewLexer(t, &Config{}, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`(\\w+)(\\r?\\n|\\r)`, ByGroups(Keyword, Whitespace), nil},\n\t\t},\n\t}))\n\tit, err := l.Tokenise(&TokeniseOptions{\n\t\tState:    \"root\",\n\t\tEnsureLF: true,\n\t}, \"hello\\r\\nworld\\r\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{\n\t\t{Keyword, \"hello\"},\n\t\t{Whitespace, \"\\n\"},\n\t\t{Keyword, \"world\"},\n\t\t{Whitespace, \"\\n\"},\n\t}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`(\\w+)(\\r?\\n|\\r)`, ByGroups(Keyword, Whitespace), nil},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(&TokeniseOptions{\n\t\tState:    \"root\",\n\t\tEnsureLF: false,\n\t}, \"hello\\r\\nworld\\r\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{\n\t\t{Keyword, \"hello\"},\n\t\t{Whitespace, \"\\r\\n\"},\n\t\t{Keyword, \"world\"},\n\t\t{Whitespace, \"\\r\"},\n\t}, it.Tokens())\n}\n\nfunc TestEnsureLFFunc(t *testing.T) {\n\ttests := []struct{ in, out string }{\n\t\t{in: \"\", out: \"\"},\n\t\t{in: \"abc\", out: \"abc\"},\n\t\t{in: \"\\r\", out: \"\\n\"},\n\t\t{in: \"a\\r\", out: \"a\\n\"},\n\t\t{in: \"\\rb\", out: \"\\nb\"},\n\t\t{in: \"a\\rb\", out: \"a\\nb\"},\n\t\t{in: \"\\r\\n\", out: \"\\n\"},\n\t\t{in: \"a\\r\\n\", out: \"a\\n\"},\n\t\t{in: \"\\r\\nb\", out: \"\\nb\"},\n\t\t{in: \"a\\r\\nb\", out: \"a\\nb\"},\n\t\t{in: \"\\r\\r\\r\\n\\r\", out: \"\\n\\n\\n\\n\"},\n\t}\n\tfor _, test := range tests {\n\t\tout := ensureLF(test.in)\n\t\tassert.Equal(t, out, test.out)\n\t}\n}\n\nfunc TestByGroupNames(t *testing.T) {\n\tl := Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{\n\t\t\t\t`(?<key>\\w+)(?<operator>=)(?<value>\\w+)`,\n\t\t\t\tByGroupNames(map[string]Emitter{\n\t\t\t\t\t`key`:      String,\n\t\t\t\t\t`operator`: Operator,\n\t\t\t\t\t`value`:    String,\n\t\t\t\t}),\n\t\t\t\tnil,\n\t\t\t},\n\t\t},\n\t}))\n\tit, err := l.Tokenise(nil, `abc=123`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{String, `abc`}, {Operator, `=`}, {String, `123`}}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{\n\t\t\t\t`(?<key>\\w+)(?<operator>=)(?<value>\\w+)`,\n\t\t\t\tByGroupNames(map[string]Emitter{\n\t\t\t\t\t`key`:   String,\n\t\t\t\t\t`value`: String,\n\t\t\t\t}),\n\t\t\t\tnil,\n\t\t\t},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(nil, `abc=123`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{String, `abc`}, {Error, `=`}, {String, `123`}}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{\n\t\t\t\t`(?<key>\\w+)=(?<value>\\w+)`,\n\t\t\t\tByGroupNames(map[string]Emitter{\n\t\t\t\t\t`key`:   String,\n\t\t\t\t\t`value`: String,\n\t\t\t\t}),\n\t\t\t\tnil,\n\t\t\t},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(nil, `abc=123`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{String, `abc123`}}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{\n\t\t\t\t`(?<key>\\w+)(?<op>=)(?<value>\\w+)`,\n\t\t\t\tByGroupNames(map[string]Emitter{\n\t\t\t\t\t`key`:      String,\n\t\t\t\t\t`operator`: Operator,\n\t\t\t\t\t`value`:    String,\n\t\t\t\t}),\n\t\t\t\tnil,\n\t\t\t},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(nil, `abc=123`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{String, `abc`}, {Error, `=`}, {String, `123`}}, it.Tokens())\n\n\tl = Coalesce(mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{\n\t\t\t\t`\\w+=\\w+`,\n\t\t\t\tByGroupNames(map[string]Emitter{\n\t\t\t\t\t`key`:      String,\n\t\t\t\t\t`operator`: Operator,\n\t\t\t\t\t`value`:    String,\n\t\t\t\t}),\n\t\t\t\tnil,\n\t\t\t},\n\t\t},\n\t}))\n\tit, err = l.Tokenise(nil, `abc=123`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{Error, `abc=123`}}, it.Tokens())\n}\n\nfunc TestIgnoreToken(t *testing.T) {\n\tl := Coalesce(mustNewLexer(t, &Config{EnsureNL: true}, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`(\\s*)(\\w+)(?:\\1)(\\n)`, ByGroups(Ignore, Keyword, Whitespace), nil},\n\t\t},\n\t}))\n\tit, err := l.Tokenise(nil, `  hello  `)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []Token{{Keyword, \"hello\"}, {TextWhitespace, \"\\n\"}}, it.Tokens())\n}\n"
        },
        {
          "name": "registry.go",
          "type": "blob",
          "size": 4.70703125,
          "content": "package chroma\n\nimport (\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n)\n\nvar (\n\tignoredSuffixes = [...]string{\n\t\t// Editor backups\n\t\t\"~\", \".bak\", \".old\", \".orig\",\n\t\t// Debian and derivatives apt/dpkg/ucf backups\n\t\t\".dpkg-dist\", \".dpkg-old\", \".ucf-dist\", \".ucf-new\", \".ucf-old\",\n\t\t// Red Hat and derivatives rpm backups\n\t\t\".rpmnew\", \".rpmorig\", \".rpmsave\",\n\t\t// Build system input/template files\n\t\t\".in\",\n\t}\n)\n\n// LexerRegistry is a registry of Lexers.\ntype LexerRegistry struct {\n\tLexers  Lexers\n\tbyName  map[string]Lexer\n\tbyAlias map[string]Lexer\n}\n\n// NewLexerRegistry creates a new LexerRegistry of Lexers.\nfunc NewLexerRegistry() *LexerRegistry {\n\treturn &LexerRegistry{\n\t\tbyName:  map[string]Lexer{},\n\t\tbyAlias: map[string]Lexer{},\n\t}\n}\n\n// Names of all lexers, optionally including aliases.\nfunc (l *LexerRegistry) Names(withAliases bool) []string {\n\tout := []string{}\n\tfor _, lexer := range l.Lexers {\n\t\tconfig := lexer.Config()\n\t\tout = append(out, config.Name)\n\t\tif withAliases {\n\t\t\tout = append(out, config.Aliases...)\n\t\t}\n\t}\n\tsort.Strings(out)\n\treturn out\n}\n\n// Get a Lexer by name, alias or file extension.\nfunc (l *LexerRegistry) Get(name string) Lexer {\n\tif lexer := l.byName[name]; lexer != nil {\n\t\treturn lexer\n\t}\n\tif lexer := l.byAlias[name]; lexer != nil {\n\t\treturn lexer\n\t}\n\tif lexer := l.byName[strings.ToLower(name)]; lexer != nil {\n\t\treturn lexer\n\t}\n\tif lexer := l.byAlias[strings.ToLower(name)]; lexer != nil {\n\t\treturn lexer\n\t}\n\n\tcandidates := PrioritisedLexers{}\n\t// Try file extension.\n\tif lexer := l.Match(\"filename.\" + name); lexer != nil {\n\t\tcandidates = append(candidates, lexer)\n\t}\n\t// Try exact filename.\n\tif lexer := l.Match(name); lexer != nil {\n\t\tcandidates = append(candidates, lexer)\n\t}\n\tif len(candidates) == 0 {\n\t\treturn nil\n\t}\n\tsort.Sort(candidates)\n\treturn candidates[0]\n}\n\n// MatchMimeType attempts to find a lexer for the given MIME type.\nfunc (l *LexerRegistry) MatchMimeType(mimeType string) Lexer {\n\tmatched := PrioritisedLexers{}\n\tfor _, l := range l.Lexers {\n\t\tfor _, lmt := range l.Config().MimeTypes {\n\t\t\tif mimeType == lmt {\n\t\t\t\tmatched = append(matched, l)\n\t\t\t}\n\t\t}\n\t}\n\tif len(matched) != 0 {\n\t\tsort.Sort(matched)\n\t\treturn matched[0]\n\t}\n\treturn nil\n}\n\n// Match returns the first lexer matching filename.\n//\n// Note that this iterates over all file patterns in all lexers, so is not fast.\nfunc (l *LexerRegistry) Match(filename string) Lexer {\n\tfilename = filepath.Base(filename)\n\tmatched := PrioritisedLexers{}\n\t// First, try primary filename matches.\n\tfor _, lexer := range l.Lexers {\n\t\tconfig := lexer.Config()\n\t\tfor _, glob := range config.Filenames {\n\t\t\tok, err := filepath.Match(glob, filename)\n\t\t\tif err != nil { // nolint\n\t\t\t\tpanic(err)\n\t\t\t} else if ok {\n\t\t\t\tmatched = append(matched, lexer)\n\t\t\t} else {\n\t\t\t\tfor _, suf := range &ignoredSuffixes {\n\t\t\t\t\tok, err := filepath.Match(glob+suf, filename)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpanic(err)\n\t\t\t\t\t} else if ok {\n\t\t\t\t\t\tmatched = append(matched, lexer)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(matched) > 0 {\n\t\tsort.Sort(matched)\n\t\treturn matched[0]\n\t}\n\tmatched = nil\n\t// Next, try filename aliases.\n\tfor _, lexer := range l.Lexers {\n\t\tconfig := lexer.Config()\n\t\tfor _, glob := range config.AliasFilenames {\n\t\t\tok, err := filepath.Match(glob, filename)\n\t\t\tif err != nil { // nolint\n\t\t\t\tpanic(err)\n\t\t\t} else if ok {\n\t\t\t\tmatched = append(matched, lexer)\n\t\t\t} else {\n\t\t\t\tfor _, suf := range &ignoredSuffixes {\n\t\t\t\t\tok, err := filepath.Match(glob+suf, filename)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpanic(err)\n\t\t\t\t\t} else if ok {\n\t\t\t\t\t\tmatched = append(matched, lexer)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(matched) > 0 {\n\t\tsort.Sort(matched)\n\t\treturn matched[0]\n\t}\n\treturn nil\n}\n\n// Analyse text content and return the \"best\" lexer..\nfunc (l *LexerRegistry) Analyse(text string) Lexer {\n\tvar picked Lexer\n\thighest := float32(0.0)\n\tfor _, lexer := range l.Lexers {\n\t\tif analyser, ok := lexer.(Analyser); ok {\n\t\t\tweight := analyser.AnalyseText(text)\n\t\t\tif weight > highest {\n\t\t\t\tpicked = lexer\n\t\t\t\thighest = weight\n\t\t\t}\n\t\t}\n\t}\n\treturn picked\n}\n\n// Register a Lexer with the LexerRegistry. If the lexer is already registered\n// it will be replaced.\nfunc (l *LexerRegistry) Register(lexer Lexer) Lexer {\n\tlexer.SetRegistry(l)\n\tconfig := lexer.Config()\n\n\tl.byName[config.Name] = lexer\n\tl.byName[strings.ToLower(config.Name)] = lexer\n\n\tfor _, alias := range config.Aliases {\n\t\tl.byAlias[alias] = lexer\n\t\tl.byAlias[strings.ToLower(alias)] = lexer\n\t}\n\n\tl.Lexers = add(l.Lexers, lexer)\n\n\treturn lexer\n}\n\n// add adds a lexer to a slice of lexers if it doesn't already exist, or if found will replace it.\nfunc add(lexers Lexers, lexer Lexer) Lexers {\n\tfor i, val := range lexers {\n\t\tif val == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tif val.Config().Name == lexer.Config().Name {\n\t\t\tlexers[i] = lexer\n\t\t\treturn lexers\n\t\t}\n\t}\n\n\treturn append(lexers, lexer)\n}\n"
        },
        {
          "name": "remap.go",
          "type": "blob",
          "size": 2.0546875,
          "content": "package chroma\n\ntype remappingLexer struct {\n\tlexer  Lexer\n\tmapper func(Token) []Token\n}\n\n// RemappingLexer remaps a token to a set of, potentially empty, tokens.\nfunc RemappingLexer(lexer Lexer, mapper func(Token) []Token) Lexer {\n\treturn &remappingLexer{lexer, mapper}\n}\n\nfunc (r *remappingLexer) AnalyseText(text string) float32 {\n\treturn r.lexer.AnalyseText(text)\n}\n\nfunc (r *remappingLexer) SetAnalyser(analyser func(text string) float32) Lexer {\n\tr.lexer.SetAnalyser(analyser)\n\treturn r\n}\n\nfunc (r *remappingLexer) SetRegistry(registry *LexerRegistry) Lexer {\n\tr.lexer.SetRegistry(registry)\n\treturn r\n}\n\nfunc (r *remappingLexer) Config() *Config {\n\treturn r.lexer.Config()\n}\n\nfunc (r *remappingLexer) Tokenise(options *TokeniseOptions, text string) (Iterator, error) {\n\tit, err := r.lexer.Tokenise(options, text)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar buffer []Token\n\treturn func() Token {\n\t\tfor {\n\t\t\tif len(buffer) > 0 {\n\t\t\t\tt := buffer[0]\n\t\t\t\tbuffer = buffer[1:]\n\t\t\t\treturn t\n\t\t\t}\n\t\t\tt := it()\n\t\t\tif t == EOF {\n\t\t\t\treturn t\n\t\t\t}\n\t\t\tbuffer = r.mapper(t)\n\t\t}\n\t}, nil\n}\n\n// TypeMapping defines type maps for the TypeRemappingLexer.\ntype TypeMapping []struct {\n\tFrom, To TokenType\n\tWords    []string\n}\n\n// TypeRemappingLexer remaps types of tokens coming from a parent Lexer.\n//\n// eg. Map \"defvaralias\" tokens of type NameVariable to NameFunction:\n//\n//\tmapping := TypeMapping{\n//\t\t{NameVariable, NameFunction, []string{\"defvaralias\"},\n//\t}\n//\tlexer = TypeRemappingLexer(lexer, mapping)\nfunc TypeRemappingLexer(lexer Lexer, mapping TypeMapping) Lexer {\n\t// Lookup table for fast remapping.\n\tlut := map[TokenType]map[string]TokenType{}\n\tfor _, rt := range mapping {\n\t\tkm, ok := lut[rt.From]\n\t\tif !ok {\n\t\t\tkm = map[string]TokenType{}\n\t\t\tlut[rt.From] = km\n\t\t}\n\t\tif len(rt.Words) == 0 {\n\t\t\tkm[\"\"] = rt.To\n\t\t} else {\n\t\t\tfor _, k := range rt.Words {\n\t\t\t\tkm[k] = rt.To\n\t\t\t}\n\t\t}\n\t}\n\treturn RemappingLexer(lexer, func(t Token) []Token {\n\t\tif k, ok := lut[t.Type]; ok {\n\t\t\tif tt, ok := k[t.Value]; ok {\n\t\t\t\tt.Type = tt\n\t\t\t} else if tt, ok := k[\"\"]; ok {\n\t\t\t\tt.Type = tt\n\t\t\t}\n\t\t}\n\t\treturn []Token{t}\n\t})\n}\n"
        },
        {
          "name": "remap_test.go",
          "type": "blob",
          "size": 0.734375,
          "content": "package chroma\n\nimport (\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestRemappingLexer(t *testing.T) {\n\tvar lexer Lexer = mustNewLexer(t, nil, Rules{ // nolint: forbidigo\n\t\t\"root\": {\n\t\t\t{`\\s+`, Whitespace, nil},\n\t\t\t{`\\w+`, Name, nil},\n\t\t},\n\t})\n\tlexer = TypeRemappingLexer(lexer, TypeMapping{\n\t\t{Name, Keyword, []string{\"if\", \"else\"}},\n\t})\n\n\tit, err := lexer.Tokenise(nil, `if true then print else end`)\n\tassert.NoError(t, err)\n\texpected := []Token{\n\t\t{Keyword, \"if\"}, {TextWhitespace, \" \"}, {Name, \"true\"}, {TextWhitespace, \" \"}, {Name, \"then\"},\n\t\t{TextWhitespace, \" \"}, {Name, \"print\"}, {TextWhitespace, \" \"}, {Keyword, \"else\"},\n\t\t{TextWhitespace, \" \"}, {Name, \"end\"},\n\t}\n\tactual := it.Tokens()\n\tassert.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "renovate.json5",
          "type": "blob",
          "size": 0.376953125,
          "content": "{\n\t$schema: \"https://docs.renovatebot.com/renovate-schema.json\",\n\textends: [\n\t\t\"config:recommended\",\n\t\t\":semanticCommits\",\n\t\t\":semanticCommitTypeAll(chore)\",\n\t\t\":semanticCommitScope(deps)\",\n\t\t\"group:allNonMajor\",\n\t\t\"schedule:earlyMondays\", // Run once a week.\n\t],\n\tpackageRules: [\n\t\t{\n\t\t\tmatchPackageNames: [\"golangci-lint\"],\n\t\t\tmatchManagers: [\"hermit\"],\n\t\t\tenabled: false,\n\t\t},\n\t],\n}\n"
        },
        {
          "name": "serialise.go",
          "type": "blob",
          "size": 10.8603515625,
          "content": "package chroma\n\nimport (\n\t\"compress/gzip\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"math\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/dlclark/regexp2\"\n)\n\n// Serialisation of Chroma rules to XML. The format is:\n//\n//\t<rules>\n//\t  <state name=\"$STATE\">\n//\t    <rule [pattern=\"$PATTERN\"]>\n//\t      [<$EMITTER ...>]\n//\t      [<$MUTATOR ...>]\n//\t    </rule>\n//\t  </state>\n//\t</rules>\n//\n// eg. Include(\"String\") would become:\n//\n//\t<rule>\n//\t  <include state=\"String\" />\n//\t</rule>\n//\n//\t[null, null, {\"kind\": \"include\", \"state\": \"String\"}]\n//\n// eg. Rule{`\\d+`, Text, nil} would become:\n//\n//\t<rule pattern=\"\\\\d+\">\n//\t  <token type=\"Text\"/>\n//\t</rule>\n//\n// eg. Rule{`\"`, String, Push(\"String\")}\n//\n//\t<rule pattern=\"\\\"\">\n//\t  <token type=\"String\" />\n//\t  <push state=\"String\" />\n//\t</rule>\n//\n// eg. Rule{`(\\w+)(\\n)`, ByGroups(Keyword, Whitespace), nil},\n//\n//\t<rule pattern=\"(\\\\w+)(\\\\n)\">\n//\t  <bygroups token=\"Keyword\" token=\"Whitespace\" />\n//\t  <push state=\"String\" />\n//\t</rule>\nvar (\n\t// ErrNotSerialisable is returned if a lexer contains Rules that cannot be serialised.\n\tErrNotSerialisable = fmt.Errorf(\"not serialisable\")\n\temitterTemplates   = func() map[string]SerialisableEmitter {\n\t\tout := map[string]SerialisableEmitter{}\n\t\tfor _, emitter := range []SerialisableEmitter{\n\t\t\t&byGroupsEmitter{},\n\t\t\t&usingSelfEmitter{},\n\t\t\tTokenType(0),\n\t\t\t&usingEmitter{},\n\t\t\t&usingByGroup{},\n\t\t} {\n\t\t\tout[emitter.EmitterKind()] = emitter\n\t\t}\n\t\treturn out\n\t}()\n\tmutatorTemplates = func() map[string]SerialisableMutator {\n\t\tout := map[string]SerialisableMutator{}\n\t\tfor _, mutator := range []SerialisableMutator{\n\t\t\t&includeMutator{},\n\t\t\t&combinedMutator{},\n\t\t\t&multiMutator{},\n\t\t\t&pushMutator{},\n\t\t\t&popMutator{},\n\t\t} {\n\t\t\tout[mutator.MutatorKind()] = mutator\n\t\t}\n\t\treturn out\n\t}()\n)\n\n// fastUnmarshalConfig unmarshals only the Config from a serialised lexer.\nfunc fastUnmarshalConfig(from fs.FS, path string) (*Config, error) {\n\tr, err := from.Open(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer r.Close()\n\tdec := xml.NewDecoder(r)\n\tfor {\n\t\ttoken, err := dec.Token()\n\t\tif err != nil {\n\t\t\tif errors.Is(err, io.EOF) {\n\t\t\t\treturn nil, fmt.Errorf(\"could not find <config> element\")\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tswitch se := token.(type) {\n\t\tcase xml.StartElement:\n\t\t\tif se.Name.Local != \"config\" {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tvar config Config\n\t\t\terr = dec.DecodeElement(&config, &se)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%s: %w\", path, err)\n\t\t\t}\n\t\t\treturn &config, nil\n\t\t}\n\t}\n}\n\n// MustNewXMLLexer constructs a new RegexLexer from an XML file or panics.\nfunc MustNewXMLLexer(from fs.FS, path string) *RegexLexer {\n\tlex, err := NewXMLLexer(from, path)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn lex\n}\n\n// NewXMLLexer creates a new RegexLexer from a serialised RegexLexer.\nfunc NewXMLLexer(from fs.FS, path string) (*RegexLexer, error) {\n\tconfig, err := fastUnmarshalConfig(from, path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, glob := range append(config.Filenames, config.AliasFilenames...) {\n\t\t_, err := filepath.Match(glob, \"\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %q is not a valid glob: %w\", config.Name, glob, err)\n\t\t}\n\t}\n\n\tvar analyserFn func(string) float32\n\n\tif config.Analyse != nil {\n\t\ttype regexAnalyse struct {\n\t\t\tre    *regexp2.Regexp\n\t\t\tscore float32\n\t\t}\n\n\t\tregexAnalysers := make([]regexAnalyse, 0, len(config.Analyse.Regexes))\n\n\t\tfor _, ra := range config.Analyse.Regexes {\n\t\t\tre, err := regexp2.Compile(ra.Pattern, regexp2.None)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%s: %q is not a valid analyser regex: %w\", config.Name, ra.Pattern, err)\n\t\t\t}\n\n\t\t\tregexAnalysers = append(regexAnalysers, regexAnalyse{re, ra.Score})\n\t\t}\n\n\t\tanalyserFn = func(text string) float32 {\n\t\t\tvar score float32\n\n\t\t\tfor _, ra := range regexAnalysers {\n\t\t\t\tok, err := ra.re.MatchString(text)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0\n\t\t\t\t}\n\n\t\t\t\tif ok && config.Analyse.First {\n\t\t\t\t\treturn float32(math.Min(float64(ra.score), 1.0))\n\t\t\t\t}\n\n\t\t\t\tif ok {\n\t\t\t\t\tscore += ra.score\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn float32(math.Min(float64(score), 1.0))\n\t\t}\n\t}\n\n\treturn &RegexLexer{\n\t\tconfig:   config,\n\t\tanalyser: analyserFn,\n\t\tfetchRulesFunc: func() (Rules, error) {\n\t\t\tvar lexer struct {\n\t\t\t\tConfig\n\t\t\t\tRules Rules `xml:\"rules\"`\n\t\t\t}\n\t\t\t// Try to open .xml fallback to .xml.gz\n\t\t\tfr, err := from.Open(path)\n\t\t\tif err != nil {\n\t\t\t\tif errors.Is(err, fs.ErrNotExist) {\n\t\t\t\t\tpath += \".gz\"\n\t\t\t\t\tfr, err = from.Open(path)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t\tdefer fr.Close()\n\t\t\tvar r io.Reader = fr\n\t\t\tif strings.HasSuffix(path, \".gz\") {\n\t\t\t\tr, err = gzip.NewReader(r)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"%s: %w\", path, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\terr = xml.NewDecoder(r).Decode(&lexer)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%s: %w\", path, err)\n\t\t\t}\n\t\t\treturn lexer.Rules, nil\n\t\t},\n\t}, nil\n}\n\n// Marshal a RegexLexer to XML.\nfunc Marshal(l *RegexLexer) ([]byte, error) {\n\ttype lexer struct {\n\t\tConfig Config `xml:\"config\"`\n\t\tRules  Rules  `xml:\"rules\"`\n\t}\n\n\trules, err := l.Rules()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\troot := &lexer{\n\t\tConfig: *l.Config(),\n\t\tRules:  rules,\n\t}\n\tdata, err := xml.MarshalIndent(root, \"\", \"  \")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tre := regexp.MustCompile(`></[a-zA-Z]+>`)\n\tdata = re.ReplaceAll(data, []byte(`/>`))\n\treturn data, nil\n}\n\n// Unmarshal a RegexLexer from XML.\nfunc Unmarshal(data []byte) (*RegexLexer, error) {\n\ttype lexer struct {\n\t\tConfig Config `xml:\"config\"`\n\t\tRules  Rules  `xml:\"rules\"`\n\t}\n\troot := &lexer{}\n\terr := xml.Unmarshal(data, root)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid Lexer XML: %w\", err)\n\t}\n\tlex, err := NewLexer(&root.Config, func() Rules { return root.Rules })\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn lex, nil\n}\n\nfunc marshalMutator(e *xml.Encoder, mutator Mutator) error {\n\tif mutator == nil {\n\t\treturn nil\n\t}\n\tsmutator, ok := mutator.(SerialisableMutator)\n\tif !ok {\n\t\treturn fmt.Errorf(\"unsupported mutator: %w\", ErrNotSerialisable)\n\t}\n\treturn e.EncodeElement(mutator, xml.StartElement{Name: xml.Name{Local: smutator.MutatorKind()}})\n}\n\nfunc unmarshalMutator(d *xml.Decoder, start xml.StartElement) (Mutator, error) {\n\tkind := start.Name.Local\n\tmutator, ok := mutatorTemplates[kind]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unknown mutator %q: %w\", kind, ErrNotSerialisable)\n\t}\n\tvalue, target := newFromTemplate(mutator)\n\tif err := d.DecodeElement(target, &start); err != nil {\n\t\treturn nil, err\n\t}\n\treturn value().(SerialisableMutator), nil\n}\n\nfunc marshalEmitter(e *xml.Encoder, emitter Emitter) error {\n\tif emitter == nil {\n\t\treturn nil\n\t}\n\tsemitter, ok := emitter.(SerialisableEmitter)\n\tif !ok {\n\t\treturn fmt.Errorf(\"unsupported emitter %T: %w\", emitter, ErrNotSerialisable)\n\t}\n\treturn e.EncodeElement(emitter, xml.StartElement{\n\t\tName: xml.Name{Local: semitter.EmitterKind()},\n\t})\n}\n\nfunc unmarshalEmitter(d *xml.Decoder, start xml.StartElement) (Emitter, error) {\n\tkind := start.Name.Local\n\tmutator, ok := emitterTemplates[kind]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unknown emitter %q: %w\", kind, ErrNotSerialisable)\n\t}\n\tvalue, target := newFromTemplate(mutator)\n\tif err := d.DecodeElement(target, &start); err != nil {\n\t\treturn nil, err\n\t}\n\treturn value().(SerialisableEmitter), nil\n}\n\nfunc (r Rule) MarshalXML(e *xml.Encoder, _ xml.StartElement) error {\n\tstart := xml.StartElement{\n\t\tName: xml.Name{Local: \"rule\"},\n\t}\n\tif r.Pattern != \"\" {\n\t\tstart.Attr = append(start.Attr, xml.Attr{\n\t\t\tName:  xml.Name{Local: \"pattern\"},\n\t\t\tValue: r.Pattern,\n\t\t})\n\t}\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\tif err := marshalEmitter(e, r.Type); err != nil {\n\t\treturn err\n\t}\n\tif err := marshalMutator(e, r.Mutator); err != nil {\n\t\treturn err\n\t}\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\nfunc (r *Rule) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\tfor _, attr := range start.Attr {\n\t\tif attr.Name.Local == \"pattern\" {\n\t\t\tr.Pattern = attr.Value\n\t\t\tbreak\n\t\t}\n\t}\n\tfor {\n\t\ttoken, err := d.Token()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch token := token.(type) {\n\t\tcase xml.StartElement:\n\t\t\tmutator, err := unmarshalMutator(d, token)\n\t\t\tif err != nil && !errors.Is(err, ErrNotSerialisable) {\n\t\t\t\treturn err\n\t\t\t} else if err == nil {\n\t\t\t\tif r.Mutator != nil {\n\t\t\t\t\treturn fmt.Errorf(\"duplicate mutator\")\n\t\t\t\t}\n\t\t\t\tr.Mutator = mutator\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\temitter, err := unmarshalEmitter(d, token)\n\t\t\tif err != nil && !errors.Is(err, ErrNotSerialisable) { // nolint: gocritic\n\t\t\t\treturn err\n\t\t\t} else if err == nil {\n\t\t\t\tif r.Type != nil {\n\t\t\t\t\treturn fmt.Errorf(\"duplicate emitter\")\n\t\t\t\t}\n\t\t\t\tr.Type = emitter\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase xml.EndElement:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\ntype xmlRuleState struct {\n\tName  string `xml:\"name,attr\"`\n\tRules []Rule `xml:\"rule\"`\n}\n\ntype xmlRules struct {\n\tStates []xmlRuleState `xml:\"state\"`\n}\n\nfunc (r Rules) MarshalXML(e *xml.Encoder, _ xml.StartElement) error {\n\txr := xmlRules{}\n\tfor state, rules := range r {\n\t\txr.States = append(xr.States, xmlRuleState{\n\t\t\tName:  state,\n\t\t\tRules: rules,\n\t\t})\n\t}\n\treturn e.EncodeElement(xr, xml.StartElement{Name: xml.Name{Local: \"rules\"}})\n}\n\nfunc (r *Rules) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\txr := xmlRules{}\n\tif err := d.DecodeElement(&xr, &start); err != nil {\n\t\treturn err\n\t}\n\tif *r == nil {\n\t\t*r = Rules{}\n\t}\n\tfor _, state := range xr.States {\n\t\t(*r)[state.Name] = state.Rules\n\t}\n\treturn nil\n}\n\ntype xmlTokenType struct {\n\tType string `xml:\"type,attr\"`\n}\n\nfunc (t *TokenType) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\tel := xmlTokenType{}\n\tif err := d.DecodeElement(&el, &start); err != nil {\n\t\treturn err\n\t}\n\ttt, err := TokenTypeString(el.Type)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*t = tt\n\treturn nil\n}\n\nfunc (t TokenType) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tstart.Attr = append(start.Attr, xml.Attr{Name: xml.Name{Local: \"type\"}, Value: t.String()})\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\n// This hijinks is a bit unfortunate but without it we can't deserialise into TokenType.\nfunc newFromTemplate(template interface{}) (value func() interface{}, target interface{}) {\n\tt := reflect.TypeOf(template)\n\tif t.Kind() == reflect.Ptr {\n\t\tv := reflect.New(t.Elem())\n\t\treturn v.Interface, v.Interface()\n\t}\n\tv := reflect.New(t)\n\treturn func() interface{} { return v.Elem().Interface() }, v.Interface()\n}\n\nfunc (b *Emitters) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\tfor {\n\t\ttoken, err := d.Token()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch token := token.(type) {\n\t\tcase xml.StartElement:\n\t\t\temitter, err := unmarshalEmitter(d, token)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t*b = append(*b, emitter)\n\n\t\tcase xml.EndElement:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (b Emitters) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\tfor _, m := range b {\n\t\tif err := marshalEmitter(e, m); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n"
        },
        {
          "name": "serialise_test.go",
          "type": "blob",
          "size": 3.966796875,
          "content": "package chroma\n\nimport (\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestEmitterSerialisationRoundTrip(t *testing.T) {\n\ttests := []struct {\n\t\tname    string\n\t\temitter Emitter\n\t}{\n\t\t{\"ByGroups\", ByGroups(Name, Using(\"Go\"))},\n\t\t{\"UsingSelf\", UsingSelf(\"root\")},\n\t\t{\"Using\", Using(\"Go\")},\n\t\t{\"UsingByGroup\", UsingByGroup(1, 2, Name)},\n\t\t{\"TokenType\", Name},\n\t}\n\tfor _, test := range tests {\n\t\t// nolint: scopelint\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tdata, err := xml.Marshal(test.emitter)\n\t\t\tassert.NoError(t, err)\n\t\t\tt.Logf(\"%s\", data)\n\t\t\tvalue, target := newFromTemplate(test.emitter)\n\t\t\terr = xml.Unmarshal(data, target)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, test.emitter, value().(Emitter))\n\t\t})\n\t}\n}\n\nfunc TestMutatorSerialisationRoundTrip(t *testing.T) {\n\ttests := []struct {\n\t\tname    string\n\t\tmutator Mutator\n\t}{\n\t\t{\"Include\", Include(\"string\").Mutator},\n\t\t{\"Combined\", Combined(\"a\", \"b\", \"c\")},\n\t\t{\"Multi\", Mutators(Include(\"string\").Mutator, Push(\"quote\"))},\n\t\t{\"Push\", Push(\"include\")},\n\t\t{\"Pop\", Pop(1)},\n\t}\n\tfor _, test := range tests {\n\t\t// nolint: scopelint\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tdata, err := xml.Marshal(test.mutator)\n\t\t\tassert.NoError(t, err)\n\t\t\tt.Logf(\"%s\", data)\n\t\t\tvalue, target := newFromTemplate(test.mutator)\n\t\t\terr = xml.Unmarshal(data, target)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, test.mutator, value().(Mutator))\n\t\t})\n\t}\n}\n\nfunc TestMarshal(t *testing.T) {\n\tactual := MustNewLexer(&Config{\n\t\tName:      \"PkgConfig\",\n\t\tAliases:   []string{\"pkgconfig\"},\n\t\tFilenames: []string{\"*.pc\"},\n\t}, func() Rules {\n\t\treturn Rules{\n\t\t\t\"root\": {\n\t\t\t\t{`#.*$`, CommentSingle, nil},\n\t\t\t\t{`^(\\w+)(=)`, ByGroups(NameAttribute, Operator), nil},\n\t\t\t\t{`^([\\w.]+)(:)`, ByGroups(NameTag, Punctuation), Push(\"spvalue\")},\n\t\t\t\tInclude(\"interp\"),\n\t\t\t\t{`[^${}#=:\\n.]+`, Text, nil},\n\t\t\t\t{`.`, Text, nil},\n\t\t\t},\n\t\t\t\"interp\": {\n\t\t\t\t{`\\$\\$`, Text, nil},\n\t\t\t\t{`\\$\\{`, LiteralStringInterpol, Push(\"curly\")},\n\t\t\t},\n\t\t\t\"curly\": {\n\t\t\t\t{`\\}`, LiteralStringInterpol, Pop(1)},\n\t\t\t\t{`\\w+`, NameAttribute, nil},\n\t\t\t},\n\t\t\t\"spvalue\": {\n\t\t\t\tInclude(\"interp\"),\n\t\t\t\t{`#.*$`, CommentSingle, Pop(1)},\n\t\t\t\t{`\\n`, Text, Pop(1)},\n\t\t\t\t{`[^${}#\\n]+`, Text, nil},\n\t\t\t\t{`.`, Text, nil},\n\t\t\t},\n\t\t}\n\t})\n\tdata, err := Marshal(actual)\n\tassert.NoError(t, err)\n\texpected, err := Unmarshal(data)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected.Config(), actual.Config())\n\tassert.Equal(t, mustRules(t, expected), mustRules(t, actual))\n}\n\nfunc mustRules(t testing.TB, r *RegexLexer) Rules {\n\tt.Helper()\n\trules, err := r.Rules()\n\tassert.NoError(t, err)\n\treturn rules\n}\n\nfunc TestRuleSerialisation(t *testing.T) {\n\ttests := []Rule{\n\t\tInclude(\"String\"),\n\t\t{`\\d+`, Text, nil},\n\t\t{`\"`, String, Push(\"String\")},\n\t}\n\tfor _, test := range tests {\n\t\tdata, err := xml.Marshal(test)\n\t\tassert.NoError(t, err)\n\t\tt.Log(string(data))\n\t\tactual := Rule{}\n\t\terr = xml.Unmarshal(data, &actual)\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, test, actual)\n\t}\n}\n\nfunc TestRulesSerialisation(t *testing.T) {\n\texpected := Rules{\n\t\t\"root\": {\n\t\t\t{`#.*$`, CommentSingle, nil},\n\t\t\t{`^(\\w+)(=)`, ByGroups(NameAttribute, Operator), nil},\n\t\t\t{`^([\\w.]+)(:)`, ByGroups(NameTag, Punctuation), Push(\"spvalue\")},\n\t\t\tInclude(\"interp\"),\n\t\t\t{`[^${}#=:\\n.]+`, Text, nil},\n\t\t\t{`.`, Text, nil},\n\t\t},\n\t\t\"interp\": {\n\t\t\t{`\\$\\$`, Text, nil},\n\t\t\t{`\\$\\{`, LiteralStringInterpol, Push(\"curly\")},\n\t\t},\n\t\t\"curly\": {\n\t\t\t{`\\}`, LiteralStringInterpol, Pop(1)},\n\t\t\t{`\\w+`, NameAttribute, nil},\n\t\t},\n\t\t\"spvalue\": {\n\t\t\tInclude(\"interp\"),\n\t\t\t{`#.*$`, CommentSingle, Pop(1)},\n\t\t\t{`\\n`, Text, Pop(1)},\n\t\t\t{`[^${}#\\n]+`, Text, nil},\n\t\t\t{`.`, Text, nil},\n\t\t},\n\t}\n\tdata, err := xml.MarshalIndent(expected, \"  \", \"  \")\n\tassert.NoError(t, err)\n\tre := regexp.MustCompile(`></[a-zA-Z]+>`)\n\tdata = re.ReplaceAll(data, []byte(`/>`))\n\tb := &bytes.Buffer{}\n\tw := gzip.NewWriter(b)\n\tfmt.Fprintln(w, string(data)) //nolint:errcheck\n\tw.Close()\n\tactual := Rules{}\n\terr = xml.Unmarshal(data, &actual)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "style.go",
          "type": "blob",
          "size": 11.291015625,
          "content": "package chroma\n\nimport (\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"io\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// Trilean value for StyleEntry value inheritance.\ntype Trilean uint8\n\n// Trilean states.\nconst (\n\tPass Trilean = iota\n\tYes\n\tNo\n)\n\nfunc (t Trilean) String() string {\n\tswitch t {\n\tcase Yes:\n\t\treturn \"Yes\"\n\tcase No:\n\t\treturn \"No\"\n\tdefault:\n\t\treturn \"Pass\"\n\t}\n}\n\n// Prefix returns s with \"no\" as a prefix if Trilean is no.\nfunc (t Trilean) Prefix(s string) string {\n\tif t == Yes {\n\t\treturn s\n\t} else if t == No {\n\t\treturn \"no\" + s\n\t}\n\treturn \"\"\n}\n\n// A StyleEntry in the Style map.\ntype StyleEntry struct {\n\t// Hex colours.\n\tColour     Colour\n\tBackground Colour\n\tBorder     Colour\n\n\tBold      Trilean\n\tItalic    Trilean\n\tUnderline Trilean\n\tNoInherit bool\n}\n\nfunc (s StyleEntry) MarshalText() ([]byte, error) {\n\treturn []byte(s.String()), nil\n}\n\nfunc (s StyleEntry) String() string {\n\tout := []string{}\n\tif s.Bold != Pass {\n\t\tout = append(out, s.Bold.Prefix(\"bold\"))\n\t}\n\tif s.Italic != Pass {\n\t\tout = append(out, s.Italic.Prefix(\"italic\"))\n\t}\n\tif s.Underline != Pass {\n\t\tout = append(out, s.Underline.Prefix(\"underline\"))\n\t}\n\tif s.NoInherit {\n\t\tout = append(out, \"noinherit\")\n\t}\n\tif s.Colour.IsSet() {\n\t\tout = append(out, s.Colour.String())\n\t}\n\tif s.Background.IsSet() {\n\t\tout = append(out, \"bg:\"+s.Background.String())\n\t}\n\tif s.Border.IsSet() {\n\t\tout = append(out, \"border:\"+s.Border.String())\n\t}\n\treturn strings.Join(out, \" \")\n}\n\n// Sub subtracts e from s where elements match.\nfunc (s StyleEntry) Sub(e StyleEntry) StyleEntry {\n\tout := StyleEntry{}\n\tif e.Colour != s.Colour {\n\t\tout.Colour = s.Colour\n\t}\n\tif e.Background != s.Background {\n\t\tout.Background = s.Background\n\t}\n\tif e.Bold != s.Bold {\n\t\tout.Bold = s.Bold\n\t}\n\tif e.Italic != s.Italic {\n\t\tout.Italic = s.Italic\n\t}\n\tif e.Underline != s.Underline {\n\t\tout.Underline = s.Underline\n\t}\n\tif e.Border != s.Border {\n\t\tout.Border = s.Border\n\t}\n\treturn out\n}\n\n// Inherit styles from ancestors.\n//\n// Ancestors should be provided from oldest to newest.\nfunc (s StyleEntry) Inherit(ancestors ...StyleEntry) StyleEntry {\n\tout := s\n\tfor i := len(ancestors) - 1; i >= 0; i-- {\n\t\tif out.NoInherit {\n\t\t\treturn out\n\t\t}\n\t\tancestor := ancestors[i]\n\t\tif !out.Colour.IsSet() {\n\t\t\tout.Colour = ancestor.Colour\n\t\t}\n\t\tif !out.Background.IsSet() {\n\t\t\tout.Background = ancestor.Background\n\t\t}\n\t\tif !out.Border.IsSet() {\n\t\t\tout.Border = ancestor.Border\n\t\t}\n\t\tif out.Bold == Pass {\n\t\t\tout.Bold = ancestor.Bold\n\t\t}\n\t\tif out.Italic == Pass {\n\t\t\tout.Italic = ancestor.Italic\n\t\t}\n\t\tif out.Underline == Pass {\n\t\t\tout.Underline = ancestor.Underline\n\t\t}\n\t}\n\treturn out\n}\n\nfunc (s StyleEntry) IsZero() bool {\n\treturn s.Colour == 0 && s.Background == 0 && s.Border == 0 && s.Bold == Pass && s.Italic == Pass &&\n\t\ts.Underline == Pass && !s.NoInherit\n}\n\n// A StyleBuilder is a mutable structure for building styles.\n//\n// Once built, a Style is immutable.\ntype StyleBuilder struct {\n\tentries map[TokenType]string\n\tname    string\n\tparent  *Style\n}\n\nfunc NewStyleBuilder(name string) *StyleBuilder {\n\treturn &StyleBuilder{name: name, entries: map[TokenType]string{}}\n}\n\nfunc (s *StyleBuilder) AddAll(entries StyleEntries) *StyleBuilder {\n\tfor ttype, entry := range entries {\n\t\ts.entries[ttype] = entry\n\t}\n\treturn s\n}\n\nfunc (s *StyleBuilder) Get(ttype TokenType) StyleEntry {\n\t// This is less than ideal, but it's the price for not having to check errors on each Add().\n\tentry, _ := ParseStyleEntry(s.entries[ttype])\n\tif s.parent != nil {\n\t\tentry = entry.Inherit(s.parent.Get(ttype))\n\t}\n\treturn entry\n}\n\n// Add an entry to the Style map.\n//\n// See http://pygments.org/docs/styles/#style-rules for details.\nfunc (s *StyleBuilder) Add(ttype TokenType, entry string) *StyleBuilder { // nolint: gocyclo\n\ts.entries[ttype] = entry\n\treturn s\n}\n\nfunc (s *StyleBuilder) AddEntry(ttype TokenType, entry StyleEntry) *StyleBuilder {\n\ts.entries[ttype] = entry.String()\n\treturn s\n}\n\n// Transform passes each style entry currently defined in the builder to the supplied\n// function and saves the returned value. This can be used to adjust a style's colours;\n// see Colour's ClampBrightness function, for example.\nfunc (s *StyleBuilder) Transform(transform func(StyleEntry) StyleEntry) *StyleBuilder {\n\ttypes := make(map[TokenType]struct{})\n\tfor tt := range s.entries {\n\t\ttypes[tt] = struct{}{}\n\t}\n\tif s.parent != nil {\n\t\tfor _, tt := range s.parent.Types() {\n\t\t\ttypes[tt] = struct{}{}\n\t\t}\n\t}\n\tfor tt := range types {\n\t\ts.AddEntry(tt, transform(s.Get(tt)))\n\t}\n\treturn s\n}\n\nfunc (s *StyleBuilder) Build() (*Style, error) {\n\tstyle := &Style{\n\t\tName:    s.name,\n\t\tentries: map[TokenType]StyleEntry{},\n\t\tparent:  s.parent,\n\t}\n\tfor ttype, descriptor := range s.entries {\n\t\tentry, err := ParseStyleEntry(descriptor)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid entry for %s: %s\", ttype, err)\n\t\t}\n\t\tstyle.entries[ttype] = entry\n\t}\n\treturn style, nil\n}\n\n// StyleEntries mapping TokenType to colour definition.\ntype StyleEntries map[TokenType]string\n\n// NewXMLStyle parses an XML style definition.\nfunc NewXMLStyle(r io.Reader) (*Style, error) {\n\tdec := xml.NewDecoder(r)\n\tstyle := &Style{}\n\treturn style, dec.Decode(style)\n}\n\n// MustNewXMLStyle is like NewXMLStyle but panics on error.\nfunc MustNewXMLStyle(r io.Reader) *Style {\n\tstyle, err := NewXMLStyle(r)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn style\n}\n\n// NewStyle creates a new style definition.\nfunc NewStyle(name string, entries StyleEntries) (*Style, error) {\n\treturn NewStyleBuilder(name).AddAll(entries).Build()\n}\n\n// MustNewStyle creates a new style or panics.\nfunc MustNewStyle(name string, entries StyleEntries) *Style {\n\tstyle, err := NewStyle(name, entries)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn style\n}\n\n// A Style definition.\n//\n// See http://pygments.org/docs/styles/ for details. Semantics are intended to be identical.\ntype Style struct {\n\tName    string\n\tentries map[TokenType]StyleEntry\n\tparent  *Style\n}\n\nfunc (s *Style) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif s.parent != nil {\n\t\treturn fmt.Errorf(\"cannot marshal style with parent\")\n\t}\n\tstart.Name = xml.Name{Local: \"style\"}\n\tstart.Attr = []xml.Attr{{Name: xml.Name{Local: \"name\"}, Value: s.Name}}\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\tsorted := make([]TokenType, 0, len(s.entries))\n\tfor ttype := range s.entries {\n\t\tsorted = append(sorted, ttype)\n\t}\n\tsort.Slice(sorted, func(i, j int) bool { return sorted[i] < sorted[j] })\n\tfor _, ttype := range sorted {\n\t\tentry := s.entries[ttype]\n\t\tel := xml.StartElement{Name: xml.Name{Local: \"entry\"}}\n\t\tel.Attr = []xml.Attr{\n\t\t\t{Name: xml.Name{Local: \"type\"}, Value: ttype.String()},\n\t\t\t{Name: xml.Name{Local: \"style\"}, Value: entry.String()},\n\t\t}\n\t\tif err := e.EncodeToken(el); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := e.EncodeToken(xml.EndElement{Name: el.Name}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\nfunc (s *Style) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {\n\tfor _, attr := range start.Attr {\n\t\tif attr.Name.Local == \"name\" {\n\t\t\ts.Name = attr.Value\n\t\t} else {\n\t\t\treturn fmt.Errorf(\"unexpected attribute %s\", attr.Name.Local)\n\t\t}\n\t}\n\tif s.Name == \"\" {\n\t\treturn fmt.Errorf(\"missing style name attribute\")\n\t}\n\ts.entries = map[TokenType]StyleEntry{}\n\tfor {\n\t\ttok, err := d.Token()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch el := tok.(type) {\n\t\tcase xml.StartElement:\n\t\t\tif el.Name.Local != \"entry\" {\n\t\t\t\treturn fmt.Errorf(\"unexpected element %s\", el.Name.Local)\n\t\t\t}\n\t\t\tvar ttype TokenType\n\t\t\tvar entry StyleEntry\n\t\t\tfor _, attr := range el.Attr {\n\t\t\t\tswitch attr.Name.Local {\n\t\t\t\tcase \"type\":\n\t\t\t\t\tttype, err = TokenTypeString(attr.Value)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\tcase \"style\":\n\t\t\t\t\tentry, err = ParseStyleEntry(attr.Value)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Errorf(\"unexpected attribute %s\", attr.Name.Local)\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.entries[ttype] = entry\n\n\t\tcase xml.EndElement:\n\t\t\tif el.Name.Local == start.Name.Local {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Types that are styled.\nfunc (s *Style) Types() []TokenType {\n\tdedupe := map[TokenType]bool{}\n\tfor tt := range s.entries {\n\t\tdedupe[tt] = true\n\t}\n\tif s.parent != nil {\n\t\tfor _, tt := range s.parent.Types() {\n\t\t\tdedupe[tt] = true\n\t\t}\n\t}\n\tout := make([]TokenType, 0, len(dedupe))\n\tfor tt := range dedupe {\n\t\tout = append(out, tt)\n\t}\n\treturn out\n}\n\n// Builder creates a mutable builder from this Style.\n//\n// The builder can then be safely modified. This is a cheap operation.\nfunc (s *Style) Builder() *StyleBuilder {\n\treturn &StyleBuilder{\n\t\tname:    s.Name,\n\t\tentries: map[TokenType]string{},\n\t\tparent:  s,\n\t}\n}\n\n// Has checks if an exact style entry match exists for a token type.\n//\n// This is distinct from Get() which will merge parent tokens.\nfunc (s *Style) Has(ttype TokenType) bool {\n\treturn !s.get(ttype).IsZero() || s.synthesisable(ttype)\n}\n\n// Get a style entry. Will try sub-category or category if an exact match is not found, and\n// finally return the Background.\nfunc (s *Style) Get(ttype TokenType) StyleEntry {\n\treturn s.get(ttype).Inherit(\n\t\ts.get(Background),\n\t\ts.get(Text),\n\t\ts.get(ttype.Category()),\n\t\ts.get(ttype.SubCategory()))\n}\n\nfunc (s *Style) get(ttype TokenType) StyleEntry {\n\tout := s.entries[ttype]\n\tif out.IsZero() && s.parent != nil {\n\t\treturn s.parent.get(ttype)\n\t}\n\tif out.IsZero() && s.synthesisable(ttype) {\n\t\tout = s.synthesise(ttype)\n\t}\n\treturn out\n}\n\nfunc (s *Style) synthesise(ttype TokenType) StyleEntry {\n\tbg := s.get(Background)\n\ttext := StyleEntry{Colour: bg.Colour}\n\ttext.Colour = text.Colour.BrightenOrDarken(0.5)\n\n\tswitch ttype {\n\t// If we don't have a line highlight colour, make one that is 10% brighter/darker than the background.\n\tcase LineHighlight:\n\t\treturn StyleEntry{Background: bg.Background.BrightenOrDarken(0.1)}\n\n\t// If we don't have line numbers, use the text colour but 20% brighter/darker\n\tcase LineNumbers, LineNumbersTable:\n\t\treturn text\n\n\tdefault:\n\t\treturn StyleEntry{}\n\t}\n}\n\nfunc (s *Style) synthesisable(ttype TokenType) bool {\n\treturn ttype == LineHighlight || ttype == LineNumbers || ttype == LineNumbersTable\n}\n\n// MustParseStyleEntry parses a Pygments style entry or panics.\nfunc MustParseStyleEntry(entry string) StyleEntry {\n\tout, err := ParseStyleEntry(entry)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn out\n}\n\n// ParseStyleEntry parses a Pygments style entry.\nfunc ParseStyleEntry(entry string) (StyleEntry, error) { // nolint: gocyclo\n\tout := StyleEntry{}\n\tparts := strings.Fields(entry)\n\tfor _, part := range parts {\n\t\tswitch {\n\t\tcase part == \"italic\":\n\t\t\tout.Italic = Yes\n\t\tcase part == \"noitalic\":\n\t\t\tout.Italic = No\n\t\tcase part == \"bold\":\n\t\t\tout.Bold = Yes\n\t\tcase part == \"nobold\":\n\t\t\tout.Bold = No\n\t\tcase part == \"underline\":\n\t\t\tout.Underline = Yes\n\t\tcase part == \"nounderline\":\n\t\t\tout.Underline = No\n\t\tcase part == \"inherit\":\n\t\t\tout.NoInherit = false\n\t\tcase part == \"noinherit\":\n\t\t\tout.NoInherit = true\n\t\tcase part == \"bg:\":\n\t\t\tout.Background = 0\n\t\tcase strings.HasPrefix(part, \"bg:#\"):\n\t\t\tout.Background = ParseColour(part[3:])\n\t\t\tif !out.Background.IsSet() {\n\t\t\t\treturn StyleEntry{}, fmt.Errorf(\"invalid background colour %q\", part)\n\t\t\t}\n\t\tcase strings.HasPrefix(part, \"border:#\"):\n\t\t\tout.Border = ParseColour(part[7:])\n\t\t\tif !out.Border.IsSet() {\n\t\t\t\treturn StyleEntry{}, fmt.Errorf(\"invalid border colour %q\", part)\n\t\t\t}\n\t\tcase strings.HasPrefix(part, \"#\"):\n\t\t\tout.Colour = ParseColour(part)\n\t\t\tif !out.Colour.IsSet() {\n\t\t\t\treturn StyleEntry{}, fmt.Errorf(\"invalid colour %q\", part)\n\t\t\t}\n\t\tdefault:\n\t\t\treturn StyleEntry{}, fmt.Errorf(\"unknown style element %q\", part)\n\t\t}\n\t}\n\treturn out, nil\n}\n"
        },
        {
          "name": "style_test.go",
          "type": "blob",
          "size": 3.9453125,
          "content": "package chroma\n\nimport (\n\t\"encoding/xml\"\n\t\"testing\"\n\n\tassert \"github.com/alecthomas/assert/v2\"\n)\n\nfunc TestStyleInherit(t *testing.T) {\n\ts, err := NewStyle(\"test\", StyleEntries{\n\t\tName:         \"bold #f00\",\n\t\tNameVariable: \"#fff\",\n\t})\n\tassert.NoError(t, err)\n\tassert.Equal(t, StyleEntry{Colour: 0x1000000, Bold: Yes}, s.Get(NameVariable))\n}\n\nfunc TestStyleColours(t *testing.T) {\n\ts, err := NewStyle(\"test\", StyleEntries{\n\t\tName: \"#f00 bg:#001 border:#ansiblue\",\n\t})\n\tassert.NoError(t, err)\n\tassert.Equal(t, StyleEntry{Colour: 0xff0001, Background: 0x000012, Border: 0x000100}, s.Get(Name))\n}\n\nfunc TestStyleClone(t *testing.T) {\n\tparent, err := NewStyle(\"test\", StyleEntries{\n\t\tBackground: \"bg:#ffffff\",\n\t})\n\tassert.NoError(t, err)\n\tclone, err := parent.Builder().Add(Comment, \"#0f0\").Build()\n\tassert.NoError(t, err)\n\n\tassert.Equal(t, \"bg:#ffffff\", clone.Get(Background).String())\n\tassert.Equal(t, \"#00ff00 bg:#ffffff\", clone.Get(Comment).String())\n\tassert.Equal(t, \"bg:#ffffff\", parent.Get(Comment).String())\n}\n\nfunc TestSynthesisedStyleEntries(t *testing.T) {\n\tstyle, err := NewStyle(\"test\", StyleEntries{\n\t\tBackground: \"bg:#ffffff\",\n\t})\n\tassert.NoError(t, err)\n\tassert.True(t, style.Has(LineHighlight))\n\tassert.True(t, style.Has(LineNumbersTable))\n\tassert.True(t, style.Has(LineNumbers))\n\tassert.Equal(t, \"bg:#e5e5e5\", style.Get(LineHighlight).String())\n\tassert.Equal(t, \"#7f7f7f bg:#ffffff\", style.Get(LineNumbers).String())\n\tassert.Equal(t, \"#7f7f7f bg:#ffffff\", style.Get(LineNumbersTable).String())\n}\n\nfunc TestSynthesisedStyleClone(t *testing.T) {\n\tstyle, err := NewStyle(\"test\", StyleEntries{\n\t\tBackground:    \"bg:#ffffff\",\n\t\tLineHighlight: \"bg:#ffffff\",\n\t\tLineNumbers:   \"bg:#fffff1\",\n\t})\n\tassert.NoError(t, err)\n\tstyle, err = style.Builder().Build()\n\tassert.NoError(t, err)\n\tassert.True(t, style.Has(LineHighlight))\n\tassert.True(t, style.Has(LineNumbers))\n\tassert.Equal(t, \"bg:#ffffff\", style.Get(LineHighlight).String())\n\tassert.Equal(t, \"bg:#fffff1\", style.Get(LineNumbers).String())\n}\n\nfunc TestStyleBuilderTransform(t *testing.T) {\n\torig, err := NewStyle(\"test\", StyleEntries{\n\t\tName:         \"#000\",\n\t\tNameVariable: \"bold #f00\",\n\t})\n\tassert.NoError(t, err)\n\n\t// Derive a style that inherits entries from orig.\n\tbuilder := orig.Builder()\n\tbuilder.Add(NameVariableGlobal, \"#f30\")\n\tderiv, err := builder.Build()\n\tassert.NoError(t, err)\n\n\t// Use Transform to brighten or darken all of the colours in the derived style.\n\tlight, err := deriv.Builder().Transform(func(se StyleEntry) StyleEntry {\n\t\tse.Colour = se.Colour.ClampBrightness(0.9, 1)\n\t\treturn se\n\t}).Build()\n\tassert.NoError(t, err, \"Transform failed: %v\", err)\n\tassert.True(t, light.Get(Name).Colour.Brightness() >= 0.89)\n\tassert.True(t, light.Get(NameVariable).Colour.Brightness() >= 0.89)\n\tassert.True(t, light.Get(NameVariableGlobal).Colour.Brightness() >= 0.89)\n\n\tdark, err := deriv.Builder().Transform(func(se StyleEntry) StyleEntry {\n\t\tse.Colour = se.Colour.ClampBrightness(0, 0.1)\n\t\treturn se\n\t}).Build()\n\tassert.NoError(t, err, \"Transform failed: %v\", err)\n\tassert.True(t, dark.Get(Name).Colour.Brightness() <= 0.11)\n\tassert.True(t, dark.Get(NameVariable).Colour.Brightness() <= 0.11)\n\tassert.True(t, dark.Get(NameVariableGlobal).Colour.Brightness() <= 0.11)\n\n\t// The original styles should be unchanged.\n\tassert.Equal(t, \"#000000\", orig.Get(Name).Colour.String())\n\tassert.Equal(t, \"#ff0000\", orig.Get(NameVariable).Colour.String())\n\tassert.Equal(t, \"#ff3300\", deriv.Get(NameVariableGlobal).Colour.String())\n}\n\nfunc TestStyleMarshaller(t *testing.T) {\n\texpected, err := NewStyle(\"test\", StyleEntries{\n\t\tWhitespace: \"bg:#ffffff\",\n\t\tText:       \"#000000 underline\",\n\t})\n\tassert.NoError(t, err)\n\tdata, err := xml.MarshalIndent(expected, \"\", \"  \")\n\tassert.NoError(t, err)\n\tassert.Equal(t, `<style name=\"test\">\n  <entry type=\"Text\" style=\"underline #000000\"></entry>\n  <entry type=\"TextWhitespace\" style=\"bg:#ffffff\"></entry>\n</style>`, string(data))\n\tactual := &Style{}\n\terr = xml.Unmarshal(data, actual)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "styles",
          "type": "tree",
          "content": null
        },
        {
          "name": "table.py",
          "type": "blob",
          "size": 0.8671875,
          "content": "#!/usr/bin/env python3\nimport re\nfrom collections import defaultdict\nfrom subprocess import check_output\n\nREADME_FILE = \"README.md\"\n\nlines = check_output([\"chroma\", \"--list\"]).decode(\"utf-8\").splitlines()\nlines = [line.strip() for line in lines if line.startswith(\"  \") and not line.startswith(\"   \")]\nlines = sorted(lines, key=lambda l: l.lower())\n\ntable = defaultdict(list)\n\nfor line in lines:\n    table[line[0].upper()].append(line)\n\nrows = []\nfor key, value in table.items():\n    rows.append(\"{} | {}\".format(key, \", \".join(value)))\ntbody = \"\\n\".join(rows)\n\nwith open(README_FILE, \"r\") as f:\n    content = f.read()\n\nwith open(README_FILE, \"w\") as f:\n    marker = re.compile(r\"(?P<start>:----: \\\\| --------\\n).*?(?P<end>\\n\\n)\", re.DOTALL)\n    replacement = r\"\\g<start>%s\\g<end>\" % tbody\n    updated_content = marker.sub(replacement, content)\n    f.write(updated_content)\n\nprint(tbody)\n"
        },
        {
          "name": "tokentype_enumer.go",
          "type": "blob",
          "size": 24.28125,
          "content": "// Code generated by \"enumer -text -type TokenType\"; DO NOT EDIT.\n\npackage chroma\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\nconst _TokenTypeName = \"IgnoreNoneOtherErrorCodeLineLineLinkLineTableTDLineTableLineHighlightLineNumbersTableLineNumbersLinePreWrapperBackgroundEOFTypeKeywordKeywordConstantKeywordDeclarationKeywordNamespaceKeywordPseudoKeywordReservedKeywordTypeNameNameAttributeNameBuiltinNameBuiltinPseudoNameClassNameConstantNameDecoratorNameEntityNameExceptionNameFunctionNameFunctionMagicNameKeywordNameLabelNameNamespaceNameOperatorNameOtherNamePseudoNamePropertyNameTagNameVariableNameVariableAnonymousNameVariableClassNameVariableGlobalNameVariableInstanceNameVariableMagicLiteralLiteralDateLiteralOtherLiteralStringLiteralStringAffixLiteralStringAtomLiteralStringBacktickLiteralStringBooleanLiteralStringCharLiteralStringDelimiterLiteralStringDocLiteralStringDoubleLiteralStringEscapeLiteralStringHeredocLiteralStringInterpolLiteralStringNameLiteralStringOtherLiteralStringRegexLiteralStringSingleLiteralStringSymbolLiteralNumberLiteralNumberBinLiteralNumberFloatLiteralNumberHexLiteralNumberIntegerLiteralNumberIntegerLongLiteralNumberOctLiteralNumberByteOperatorOperatorWordPunctuationCommentCommentHashbangCommentMultilineCommentSingleCommentSpecialCommentPreprocCommentPreprocFileGenericGenericDeletedGenericEmphGenericErrorGenericHeadingGenericInsertedGenericOutputGenericPromptGenericStrongGenericSubheadingGenericTracebackGenericUnderlineTextTextWhitespaceTextSymbolTextPunctuation\"\nconst _TokenTypeLowerName = \"ignorenoneothererrorcodelinelinelinklinetabletdlinetablelinehighlightlinenumberstablelinenumberslineprewrapperbackgroundeoftypekeywordkeywordconstantkeyworddeclarationkeywordnamespacekeywordpseudokeywordreservedkeywordtypenamenameattributenamebuiltinnamebuiltinpseudonameclassnameconstantnamedecoratornameentitynameexceptionnamefunctionnamefunctionmagicnamekeywordnamelabelnamenamespacenameoperatornameothernamepseudonamepropertynametagnamevariablenamevariableanonymousnamevariableclassnamevariableglobalnamevariableinstancenamevariablemagicliteralliteraldateliteralotherliteralstringliteralstringaffixliteralstringatomliteralstringbacktickliteralstringbooleanliteralstringcharliteralstringdelimiterliteralstringdocliteralstringdoubleliteralstringescapeliteralstringheredocliteralstringinterpolliteralstringnameliteralstringotherliteralstringregexliteralstringsingleliteralstringsymbolliteralnumberliteralnumberbinliteralnumberfloatliteralnumberhexliteralnumberintegerliteralnumberintegerlongliteralnumberoctliteralnumberbyteoperatoroperatorwordpunctuationcommentcommenthashbangcommentmultilinecommentsinglecommentspecialcommentpreproccommentpreprocfilegenericgenericdeletedgenericemphgenericerrorgenericheadinggenericinsertedgenericoutputgenericpromptgenericstronggenericsubheadinggenerictracebackgenericunderlinetexttextwhitespacetextsymboltextpunctuation\"\n\nvar _TokenTypeMap = map[TokenType]string{\n\t-14:  _TokenTypeName[0:6],\n\t-13:  _TokenTypeName[6:10],\n\t-12:  _TokenTypeName[10:15],\n\t-11:  _TokenTypeName[15:20],\n\t-10:  _TokenTypeName[20:28],\n\t-9:   _TokenTypeName[28:36],\n\t-8:   _TokenTypeName[36:47],\n\t-7:   _TokenTypeName[47:56],\n\t-6:   _TokenTypeName[56:69],\n\t-5:   _TokenTypeName[69:85],\n\t-4:   _TokenTypeName[85:96],\n\t-3:   _TokenTypeName[96:100],\n\t-2:   _TokenTypeName[100:110],\n\t-1:   _TokenTypeName[110:120],\n\t0:    _TokenTypeName[120:127],\n\t1000: _TokenTypeName[127:134],\n\t1001: _TokenTypeName[134:149],\n\t1002: _TokenTypeName[149:167],\n\t1003: _TokenTypeName[167:183],\n\t1004: _TokenTypeName[183:196],\n\t1005: _TokenTypeName[196:211],\n\t1006: _TokenTypeName[211:222],\n\t2000: _TokenTypeName[222:226],\n\t2001: _TokenTypeName[226:239],\n\t2002: _TokenTypeName[239:250],\n\t2003: _TokenTypeName[250:267],\n\t2004: _TokenTypeName[267:276],\n\t2005: _TokenTypeName[276:288],\n\t2006: _TokenTypeName[288:301],\n\t2007: _TokenTypeName[301:311],\n\t2008: _TokenTypeName[311:324],\n\t2009: _TokenTypeName[324:336],\n\t2010: _TokenTypeName[336:353],\n\t2011: _TokenTypeName[353:364],\n\t2012: _TokenTypeName[364:373],\n\t2013: _TokenTypeName[373:386],\n\t2014: _TokenTypeName[386:398],\n\t2015: _TokenTypeName[398:407],\n\t2016: _TokenTypeName[407:417],\n\t2017: _TokenTypeName[417:429],\n\t2018: _TokenTypeName[429:436],\n\t2019: _TokenTypeName[436:448],\n\t2020: _TokenTypeName[448:469],\n\t2021: _TokenTypeName[469:486],\n\t2022: _TokenTypeName[486:504],\n\t2023: _TokenTypeName[504:524],\n\t2024: _TokenTypeName[524:541],\n\t3000: _TokenTypeName[541:548],\n\t3001: _TokenTypeName[548:559],\n\t3002: _TokenTypeName[559:571],\n\t3100: _TokenTypeName[571:584],\n\t3101: _TokenTypeName[584:602],\n\t3102: _TokenTypeName[602:619],\n\t3103: _TokenTypeName[619:640],\n\t3104: _TokenTypeName[640:660],\n\t3105: _TokenTypeName[660:677],\n\t3106: _TokenTypeName[677:699],\n\t3107: _TokenTypeName[699:715],\n\t3108: _TokenTypeName[715:734],\n\t3109: _TokenTypeName[734:753],\n\t3110: _TokenTypeName[753:773],\n\t3111: _TokenTypeName[773:794],\n\t3112: _TokenTypeName[794:811],\n\t3113: _TokenTypeName[811:829],\n\t3114: _TokenTypeName[829:847],\n\t3115: _TokenTypeName[847:866],\n\t3116: _TokenTypeName[866:885],\n\t3200: _TokenTypeName[885:898],\n\t3201: _TokenTypeName[898:914],\n\t3202: _TokenTypeName[914:932],\n\t3203: _TokenTypeName[932:948],\n\t3204: _TokenTypeName[948:968],\n\t3205: _TokenTypeName[968:992],\n\t3206: _TokenTypeName[992:1008],\n\t3207: _TokenTypeName[1008:1025],\n\t4000: _TokenTypeName[1025:1033],\n\t4001: _TokenTypeName[1033:1045],\n\t5000: _TokenTypeName[1045:1056],\n\t6000: _TokenTypeName[1056:1063],\n\t6001: _TokenTypeName[1063:1078],\n\t6002: _TokenTypeName[1078:1094],\n\t6003: _TokenTypeName[1094:1107],\n\t6004: _TokenTypeName[1107:1121],\n\t6100: _TokenTypeName[1121:1135],\n\t6101: _TokenTypeName[1135:1153],\n\t7000: _TokenTypeName[1153:1160],\n\t7001: _TokenTypeName[1160:1174],\n\t7002: _TokenTypeName[1174:1185],\n\t7003: _TokenTypeName[1185:1197],\n\t7004: _TokenTypeName[1197:1211],\n\t7005: _TokenTypeName[1211:1226],\n\t7006: _TokenTypeName[1226:1239],\n\t7007: _TokenTypeName[1239:1252],\n\t7008: _TokenTypeName[1252:1265],\n\t7009: _TokenTypeName[1265:1282],\n\t7010: _TokenTypeName[1282:1298],\n\t7011: _TokenTypeName[1298:1314],\n\t8000: _TokenTypeName[1314:1318],\n\t8001: _TokenTypeName[1318:1332],\n\t8002: _TokenTypeName[1332:1342],\n\t8003: _TokenTypeName[1342:1357],\n}\n\nfunc (i TokenType) String() string {\n\tif str, ok := _TokenTypeMap[i]; ok {\n\t\treturn str\n\t}\n\treturn fmt.Sprintf(\"TokenType(%d)\", i)\n}\n\n// An \"invalid array index\" compiler error signifies that the constant values have changed.\n// Re-run the stringer command to generate them again.\nfunc _TokenTypeNoOp() {\n\tvar x [1]struct{}\n\t_ = x[Ignore-(-14)]\n\t_ = x[None-(-13)]\n\t_ = x[Other-(-12)]\n\t_ = x[Error-(-11)]\n\t_ = x[CodeLine-(-10)]\n\t_ = x[LineLink-(-9)]\n\t_ = x[LineTableTD-(-8)]\n\t_ = x[LineTable-(-7)]\n\t_ = x[LineHighlight-(-6)]\n\t_ = x[LineNumbersTable-(-5)]\n\t_ = x[LineNumbers-(-4)]\n\t_ = x[Line-(-3)]\n\t_ = x[PreWrapper-(-2)]\n\t_ = x[Background-(-1)]\n\t_ = x[EOFType-(0)]\n\t_ = x[Keyword-(1000)]\n\t_ = x[KeywordConstant-(1001)]\n\t_ = x[KeywordDeclaration-(1002)]\n\t_ = x[KeywordNamespace-(1003)]\n\t_ = x[KeywordPseudo-(1004)]\n\t_ = x[KeywordReserved-(1005)]\n\t_ = x[KeywordType-(1006)]\n\t_ = x[Name-(2000)]\n\t_ = x[NameAttribute-(2001)]\n\t_ = x[NameBuiltin-(2002)]\n\t_ = x[NameBuiltinPseudo-(2003)]\n\t_ = x[NameClass-(2004)]\n\t_ = x[NameConstant-(2005)]\n\t_ = x[NameDecorator-(2006)]\n\t_ = x[NameEntity-(2007)]\n\t_ = x[NameException-(2008)]\n\t_ = x[NameFunction-(2009)]\n\t_ = x[NameFunctionMagic-(2010)]\n\t_ = x[NameKeyword-(2011)]\n\t_ = x[NameLabel-(2012)]\n\t_ = x[NameNamespace-(2013)]\n\t_ = x[NameOperator-(2014)]\n\t_ = x[NameOther-(2015)]\n\t_ = x[NamePseudo-(2016)]\n\t_ = x[NameProperty-(2017)]\n\t_ = x[NameTag-(2018)]\n\t_ = x[NameVariable-(2019)]\n\t_ = x[NameVariableAnonymous-(2020)]\n\t_ = x[NameVariableClass-(2021)]\n\t_ = x[NameVariableGlobal-(2022)]\n\t_ = x[NameVariableInstance-(2023)]\n\t_ = x[NameVariableMagic-(2024)]\n\t_ = x[Literal-(3000)]\n\t_ = x[LiteralDate-(3001)]\n\t_ = x[LiteralOther-(3002)]\n\t_ = x[LiteralString-(3100)]\n\t_ = x[LiteralStringAffix-(3101)]\n\t_ = x[LiteralStringAtom-(3102)]\n\t_ = x[LiteralStringBacktick-(3103)]\n\t_ = x[LiteralStringBoolean-(3104)]\n\t_ = x[LiteralStringChar-(3105)]\n\t_ = x[LiteralStringDelimiter-(3106)]\n\t_ = x[LiteralStringDoc-(3107)]\n\t_ = x[LiteralStringDouble-(3108)]\n\t_ = x[LiteralStringEscape-(3109)]\n\t_ = x[LiteralStringHeredoc-(3110)]\n\t_ = x[LiteralStringInterpol-(3111)]\n\t_ = x[LiteralStringName-(3112)]\n\t_ = x[LiteralStringOther-(3113)]\n\t_ = x[LiteralStringRegex-(3114)]\n\t_ = x[LiteralStringSingle-(3115)]\n\t_ = x[LiteralStringSymbol-(3116)]\n\t_ = x[LiteralNumber-(3200)]\n\t_ = x[LiteralNumberBin-(3201)]\n\t_ = x[LiteralNumberFloat-(3202)]\n\t_ = x[LiteralNumberHex-(3203)]\n\t_ = x[LiteralNumberInteger-(3204)]\n\t_ = x[LiteralNumberIntegerLong-(3205)]\n\t_ = x[LiteralNumberOct-(3206)]\n\t_ = x[LiteralNumberByte-(3207)]\n\t_ = x[Operator-(4000)]\n\t_ = x[OperatorWord-(4001)]\n\t_ = x[Punctuation-(5000)]\n\t_ = x[Comment-(6000)]\n\t_ = x[CommentHashbang-(6001)]\n\t_ = x[CommentMultiline-(6002)]\n\t_ = x[CommentSingle-(6003)]\n\t_ = x[CommentSpecial-(6004)]\n\t_ = x[CommentPreproc-(6100)]\n\t_ = x[CommentPreprocFile-(6101)]\n\t_ = x[Generic-(7000)]\n\t_ = x[GenericDeleted-(7001)]\n\t_ = x[GenericEmph-(7002)]\n\t_ = x[GenericError-(7003)]\n\t_ = x[GenericHeading-(7004)]\n\t_ = x[GenericInserted-(7005)]\n\t_ = x[GenericOutput-(7006)]\n\t_ = x[GenericPrompt-(7007)]\n\t_ = x[GenericStrong-(7008)]\n\t_ = x[GenericSubheading-(7009)]\n\t_ = x[GenericTraceback-(7010)]\n\t_ = x[GenericUnderline-(7011)]\n\t_ = x[Text-(8000)]\n\t_ = x[TextWhitespace-(8001)]\n\t_ = x[TextSymbol-(8002)]\n\t_ = x[TextPunctuation-(8003)]\n}\n\nvar _TokenTypeValues = []TokenType{Ignore, None, Other, Error, CodeLine, LineLink, LineTableTD, LineTable, LineHighlight, LineNumbersTable, LineNumbers, Line, PreWrapper, Background, EOFType, Keyword, KeywordConstant, KeywordDeclaration, KeywordNamespace, KeywordPseudo, KeywordReserved, KeywordType, Name, NameAttribute, NameBuiltin, NameBuiltinPseudo, NameClass, NameConstant, NameDecorator, NameEntity, NameException, NameFunction, NameFunctionMagic, NameKeyword, NameLabel, NameNamespace, NameOperator, NameOther, NamePseudo, NameProperty, NameTag, NameVariable, NameVariableAnonymous, NameVariableClass, NameVariableGlobal, NameVariableInstance, NameVariableMagic, Literal, LiteralDate, LiteralOther, LiteralString, LiteralStringAffix, LiteralStringAtom, LiteralStringBacktick, LiteralStringBoolean, LiteralStringChar, LiteralStringDelimiter, LiteralStringDoc, LiteralStringDouble, LiteralStringEscape, LiteralStringHeredoc, LiteralStringInterpol, LiteralStringName, LiteralStringOther, LiteralStringRegex, LiteralStringSingle, LiteralStringSymbol, LiteralNumber, LiteralNumberBin, LiteralNumberFloat, LiteralNumberHex, LiteralNumberInteger, LiteralNumberIntegerLong, LiteralNumberOct, LiteralNumberByte, Operator, OperatorWord, Punctuation, Comment, CommentHashbang, CommentMultiline, CommentSingle, CommentSpecial, CommentPreproc, CommentPreprocFile, Generic, GenericDeleted, GenericEmph, GenericError, GenericHeading, GenericInserted, GenericOutput, GenericPrompt, GenericStrong, GenericSubheading, GenericTraceback, GenericUnderline, Text, TextWhitespace, TextSymbol, TextPunctuation}\n\nvar _TokenTypeNameToValueMap = map[string]TokenType{\n\t_TokenTypeName[0:6]:            Ignore,\n\t_TokenTypeLowerName[0:6]:       Ignore,\n\t_TokenTypeName[6:10]:           None,\n\t_TokenTypeLowerName[6:10]:      None,\n\t_TokenTypeName[10:15]:          Other,\n\t_TokenTypeLowerName[10:15]:     Other,\n\t_TokenTypeName[15:20]:          Error,\n\t_TokenTypeLowerName[15:20]:     Error,\n\t_TokenTypeName[20:28]:          CodeLine,\n\t_TokenTypeLowerName[20:28]:     CodeLine,\n\t_TokenTypeName[28:36]:          LineLink,\n\t_TokenTypeLowerName[28:36]:     LineLink,\n\t_TokenTypeName[36:47]:          LineTableTD,\n\t_TokenTypeLowerName[36:47]:     LineTableTD,\n\t_TokenTypeName[47:56]:          LineTable,\n\t_TokenTypeLowerName[47:56]:     LineTable,\n\t_TokenTypeName[56:69]:          LineHighlight,\n\t_TokenTypeLowerName[56:69]:     LineHighlight,\n\t_TokenTypeName[69:85]:          LineNumbersTable,\n\t_TokenTypeLowerName[69:85]:     LineNumbersTable,\n\t_TokenTypeName[85:96]:          LineNumbers,\n\t_TokenTypeLowerName[85:96]:     LineNumbers,\n\t_TokenTypeName[96:100]:         Line,\n\t_TokenTypeLowerName[96:100]:    Line,\n\t_TokenTypeName[100:110]:        PreWrapper,\n\t_TokenTypeLowerName[100:110]:   PreWrapper,\n\t_TokenTypeName[110:120]:        Background,\n\t_TokenTypeLowerName[110:120]:   Background,\n\t_TokenTypeName[120:127]:        EOFType,\n\t_TokenTypeLowerName[120:127]:   EOFType,\n\t_TokenTypeName[127:134]:        Keyword,\n\t_TokenTypeLowerName[127:134]:   Keyword,\n\t_TokenTypeName[134:149]:        KeywordConstant,\n\t_TokenTypeLowerName[134:149]:   KeywordConstant,\n\t_TokenTypeName[149:167]:        KeywordDeclaration,\n\t_TokenTypeLowerName[149:167]:   KeywordDeclaration,\n\t_TokenTypeName[167:183]:        KeywordNamespace,\n\t_TokenTypeLowerName[167:183]:   KeywordNamespace,\n\t_TokenTypeName[183:196]:        KeywordPseudo,\n\t_TokenTypeLowerName[183:196]:   KeywordPseudo,\n\t_TokenTypeName[196:211]:        KeywordReserved,\n\t_TokenTypeLowerName[196:211]:   KeywordReserved,\n\t_TokenTypeName[211:222]:        KeywordType,\n\t_TokenTypeLowerName[211:222]:   KeywordType,\n\t_TokenTypeName[222:226]:        Name,\n\t_TokenTypeLowerName[222:226]:   Name,\n\t_TokenTypeName[226:239]:        NameAttribute,\n\t_TokenTypeLowerName[226:239]:   NameAttribute,\n\t_TokenTypeName[239:250]:        NameBuiltin,\n\t_TokenTypeLowerName[239:250]:   NameBuiltin,\n\t_TokenTypeName[250:267]:        NameBuiltinPseudo,\n\t_TokenTypeLowerName[250:267]:   NameBuiltinPseudo,\n\t_TokenTypeName[267:276]:        NameClass,\n\t_TokenTypeLowerName[267:276]:   NameClass,\n\t_TokenTypeName[276:288]:        NameConstant,\n\t_TokenTypeLowerName[276:288]:   NameConstant,\n\t_TokenTypeName[288:301]:        NameDecorator,\n\t_TokenTypeLowerName[288:301]:   NameDecorator,\n\t_TokenTypeName[301:311]:        NameEntity,\n\t_TokenTypeLowerName[301:311]:   NameEntity,\n\t_TokenTypeName[311:324]:        NameException,\n\t_TokenTypeLowerName[311:324]:   NameException,\n\t_TokenTypeName[324:336]:        NameFunction,\n\t_TokenTypeLowerName[324:336]:   NameFunction,\n\t_TokenTypeName[336:353]:        NameFunctionMagic,\n\t_TokenTypeLowerName[336:353]:   NameFunctionMagic,\n\t_TokenTypeName[353:364]:        NameKeyword,\n\t_TokenTypeLowerName[353:364]:   NameKeyword,\n\t_TokenTypeName[364:373]:        NameLabel,\n\t_TokenTypeLowerName[364:373]:   NameLabel,\n\t_TokenTypeName[373:386]:        NameNamespace,\n\t_TokenTypeLowerName[373:386]:   NameNamespace,\n\t_TokenTypeName[386:398]:        NameOperator,\n\t_TokenTypeLowerName[386:398]:   NameOperator,\n\t_TokenTypeName[398:407]:        NameOther,\n\t_TokenTypeLowerName[398:407]:   NameOther,\n\t_TokenTypeName[407:417]:        NamePseudo,\n\t_TokenTypeLowerName[407:417]:   NamePseudo,\n\t_TokenTypeName[417:429]:        NameProperty,\n\t_TokenTypeLowerName[417:429]:   NameProperty,\n\t_TokenTypeName[429:436]:        NameTag,\n\t_TokenTypeLowerName[429:436]:   NameTag,\n\t_TokenTypeName[436:448]:        NameVariable,\n\t_TokenTypeLowerName[436:448]:   NameVariable,\n\t_TokenTypeName[448:469]:        NameVariableAnonymous,\n\t_TokenTypeLowerName[448:469]:   NameVariableAnonymous,\n\t_TokenTypeName[469:486]:        NameVariableClass,\n\t_TokenTypeLowerName[469:486]:   NameVariableClass,\n\t_TokenTypeName[486:504]:        NameVariableGlobal,\n\t_TokenTypeLowerName[486:504]:   NameVariableGlobal,\n\t_TokenTypeName[504:524]:        NameVariableInstance,\n\t_TokenTypeLowerName[504:524]:   NameVariableInstance,\n\t_TokenTypeName[524:541]:        NameVariableMagic,\n\t_TokenTypeLowerName[524:541]:   NameVariableMagic,\n\t_TokenTypeName[541:548]:        Literal,\n\t_TokenTypeLowerName[541:548]:   Literal,\n\t_TokenTypeName[548:559]:        LiteralDate,\n\t_TokenTypeLowerName[548:559]:   LiteralDate,\n\t_TokenTypeName[559:571]:        LiteralOther,\n\t_TokenTypeLowerName[559:571]:   LiteralOther,\n\t_TokenTypeName[571:584]:        LiteralString,\n\t_TokenTypeLowerName[571:584]:   LiteralString,\n\t_TokenTypeName[584:602]:        LiteralStringAffix,\n\t_TokenTypeLowerName[584:602]:   LiteralStringAffix,\n\t_TokenTypeName[602:619]:        LiteralStringAtom,\n\t_TokenTypeLowerName[602:619]:   LiteralStringAtom,\n\t_TokenTypeName[619:640]:        LiteralStringBacktick,\n\t_TokenTypeLowerName[619:640]:   LiteralStringBacktick,\n\t_TokenTypeName[640:660]:        LiteralStringBoolean,\n\t_TokenTypeLowerName[640:660]:   LiteralStringBoolean,\n\t_TokenTypeName[660:677]:        LiteralStringChar,\n\t_TokenTypeLowerName[660:677]:   LiteralStringChar,\n\t_TokenTypeName[677:699]:        LiteralStringDelimiter,\n\t_TokenTypeLowerName[677:699]:   LiteralStringDelimiter,\n\t_TokenTypeName[699:715]:        LiteralStringDoc,\n\t_TokenTypeLowerName[699:715]:   LiteralStringDoc,\n\t_TokenTypeName[715:734]:        LiteralStringDouble,\n\t_TokenTypeLowerName[715:734]:   LiteralStringDouble,\n\t_TokenTypeName[734:753]:        LiteralStringEscape,\n\t_TokenTypeLowerName[734:753]:   LiteralStringEscape,\n\t_TokenTypeName[753:773]:        LiteralStringHeredoc,\n\t_TokenTypeLowerName[753:773]:   LiteralStringHeredoc,\n\t_TokenTypeName[773:794]:        LiteralStringInterpol,\n\t_TokenTypeLowerName[773:794]:   LiteralStringInterpol,\n\t_TokenTypeName[794:811]:        LiteralStringName,\n\t_TokenTypeLowerName[794:811]:   LiteralStringName,\n\t_TokenTypeName[811:829]:        LiteralStringOther,\n\t_TokenTypeLowerName[811:829]:   LiteralStringOther,\n\t_TokenTypeName[829:847]:        LiteralStringRegex,\n\t_TokenTypeLowerName[829:847]:   LiteralStringRegex,\n\t_TokenTypeName[847:866]:        LiteralStringSingle,\n\t_TokenTypeLowerName[847:866]:   LiteralStringSingle,\n\t_TokenTypeName[866:885]:        LiteralStringSymbol,\n\t_TokenTypeLowerName[866:885]:   LiteralStringSymbol,\n\t_TokenTypeName[885:898]:        LiteralNumber,\n\t_TokenTypeLowerName[885:898]:   LiteralNumber,\n\t_TokenTypeName[898:914]:        LiteralNumberBin,\n\t_TokenTypeLowerName[898:914]:   LiteralNumberBin,\n\t_TokenTypeName[914:932]:        LiteralNumberFloat,\n\t_TokenTypeLowerName[914:932]:   LiteralNumberFloat,\n\t_TokenTypeName[932:948]:        LiteralNumberHex,\n\t_TokenTypeLowerName[932:948]:   LiteralNumberHex,\n\t_TokenTypeName[948:968]:        LiteralNumberInteger,\n\t_TokenTypeLowerName[948:968]:   LiteralNumberInteger,\n\t_TokenTypeName[968:992]:        LiteralNumberIntegerLong,\n\t_TokenTypeLowerName[968:992]:   LiteralNumberIntegerLong,\n\t_TokenTypeName[992:1008]:       LiteralNumberOct,\n\t_TokenTypeLowerName[992:1008]:  LiteralNumberOct,\n\t_TokenTypeName[1008:1025]:      LiteralNumberByte,\n\t_TokenTypeLowerName[1008:1025]: LiteralNumberByte,\n\t_TokenTypeName[1025:1033]:      Operator,\n\t_TokenTypeLowerName[1025:1033]: Operator,\n\t_TokenTypeName[1033:1045]:      OperatorWord,\n\t_TokenTypeLowerName[1033:1045]: OperatorWord,\n\t_TokenTypeName[1045:1056]:      Punctuation,\n\t_TokenTypeLowerName[1045:1056]: Punctuation,\n\t_TokenTypeName[1056:1063]:      Comment,\n\t_TokenTypeLowerName[1056:1063]: Comment,\n\t_TokenTypeName[1063:1078]:      CommentHashbang,\n\t_TokenTypeLowerName[1063:1078]: CommentHashbang,\n\t_TokenTypeName[1078:1094]:      CommentMultiline,\n\t_TokenTypeLowerName[1078:1094]: CommentMultiline,\n\t_TokenTypeName[1094:1107]:      CommentSingle,\n\t_TokenTypeLowerName[1094:1107]: CommentSingle,\n\t_TokenTypeName[1107:1121]:      CommentSpecial,\n\t_TokenTypeLowerName[1107:1121]: CommentSpecial,\n\t_TokenTypeName[1121:1135]:      CommentPreproc,\n\t_TokenTypeLowerName[1121:1135]: CommentPreproc,\n\t_TokenTypeName[1135:1153]:      CommentPreprocFile,\n\t_TokenTypeLowerName[1135:1153]: CommentPreprocFile,\n\t_TokenTypeName[1153:1160]:      Generic,\n\t_TokenTypeLowerName[1153:1160]: Generic,\n\t_TokenTypeName[1160:1174]:      GenericDeleted,\n\t_TokenTypeLowerName[1160:1174]: GenericDeleted,\n\t_TokenTypeName[1174:1185]:      GenericEmph,\n\t_TokenTypeLowerName[1174:1185]: GenericEmph,\n\t_TokenTypeName[1185:1197]:      GenericError,\n\t_TokenTypeLowerName[1185:1197]: GenericError,\n\t_TokenTypeName[1197:1211]:      GenericHeading,\n\t_TokenTypeLowerName[1197:1211]: GenericHeading,\n\t_TokenTypeName[1211:1226]:      GenericInserted,\n\t_TokenTypeLowerName[1211:1226]: GenericInserted,\n\t_TokenTypeName[1226:1239]:      GenericOutput,\n\t_TokenTypeLowerName[1226:1239]: GenericOutput,\n\t_TokenTypeName[1239:1252]:      GenericPrompt,\n\t_TokenTypeLowerName[1239:1252]: GenericPrompt,\n\t_TokenTypeName[1252:1265]:      GenericStrong,\n\t_TokenTypeLowerName[1252:1265]: GenericStrong,\n\t_TokenTypeName[1265:1282]:      GenericSubheading,\n\t_TokenTypeLowerName[1265:1282]: GenericSubheading,\n\t_TokenTypeName[1282:1298]:      GenericTraceback,\n\t_TokenTypeLowerName[1282:1298]: GenericTraceback,\n\t_TokenTypeName[1298:1314]:      GenericUnderline,\n\t_TokenTypeLowerName[1298:1314]: GenericUnderline,\n\t_TokenTypeName[1314:1318]:      Text,\n\t_TokenTypeLowerName[1314:1318]: Text,\n\t_TokenTypeName[1318:1332]:      TextWhitespace,\n\t_TokenTypeLowerName[1318:1332]: TextWhitespace,\n\t_TokenTypeName[1332:1342]:      TextSymbol,\n\t_TokenTypeLowerName[1332:1342]: TextSymbol,\n\t_TokenTypeName[1342:1357]:      TextPunctuation,\n\t_TokenTypeLowerName[1342:1357]: TextPunctuation,\n}\n\nvar _TokenTypeNames = []string{\n\t_TokenTypeName[0:6],\n\t_TokenTypeName[6:10],\n\t_TokenTypeName[10:15],\n\t_TokenTypeName[15:20],\n\t_TokenTypeName[20:28],\n\t_TokenTypeName[28:36],\n\t_TokenTypeName[36:47],\n\t_TokenTypeName[47:56],\n\t_TokenTypeName[56:69],\n\t_TokenTypeName[69:85],\n\t_TokenTypeName[85:96],\n\t_TokenTypeName[96:100],\n\t_TokenTypeName[100:110],\n\t_TokenTypeName[110:120],\n\t_TokenTypeName[120:127],\n\t_TokenTypeName[127:134],\n\t_TokenTypeName[134:149],\n\t_TokenTypeName[149:167],\n\t_TokenTypeName[167:183],\n\t_TokenTypeName[183:196],\n\t_TokenTypeName[196:211],\n\t_TokenTypeName[211:222],\n\t_TokenTypeName[222:226],\n\t_TokenTypeName[226:239],\n\t_TokenTypeName[239:250],\n\t_TokenTypeName[250:267],\n\t_TokenTypeName[267:276],\n\t_TokenTypeName[276:288],\n\t_TokenTypeName[288:301],\n\t_TokenTypeName[301:311],\n\t_TokenTypeName[311:324],\n\t_TokenTypeName[324:336],\n\t_TokenTypeName[336:353],\n\t_TokenTypeName[353:364],\n\t_TokenTypeName[364:373],\n\t_TokenTypeName[373:386],\n\t_TokenTypeName[386:398],\n\t_TokenTypeName[398:407],\n\t_TokenTypeName[407:417],\n\t_TokenTypeName[417:429],\n\t_TokenTypeName[429:436],\n\t_TokenTypeName[436:448],\n\t_TokenTypeName[448:469],\n\t_TokenTypeName[469:486],\n\t_TokenTypeName[486:504],\n\t_TokenTypeName[504:524],\n\t_TokenTypeName[524:541],\n\t_TokenTypeName[541:548],\n\t_TokenTypeName[548:559],\n\t_TokenTypeName[559:571],\n\t_TokenTypeName[571:584],\n\t_TokenTypeName[584:602],\n\t_TokenTypeName[602:619],\n\t_TokenTypeName[619:640],\n\t_TokenTypeName[640:660],\n\t_TokenTypeName[660:677],\n\t_TokenTypeName[677:699],\n\t_TokenTypeName[699:715],\n\t_TokenTypeName[715:734],\n\t_TokenTypeName[734:753],\n\t_TokenTypeName[753:773],\n\t_TokenTypeName[773:794],\n\t_TokenTypeName[794:811],\n\t_TokenTypeName[811:829],\n\t_TokenTypeName[829:847],\n\t_TokenTypeName[847:866],\n\t_TokenTypeName[866:885],\n\t_TokenTypeName[885:898],\n\t_TokenTypeName[898:914],\n\t_TokenTypeName[914:932],\n\t_TokenTypeName[932:948],\n\t_TokenTypeName[948:968],\n\t_TokenTypeName[968:992],\n\t_TokenTypeName[992:1008],\n\t_TokenTypeName[1008:1025],\n\t_TokenTypeName[1025:1033],\n\t_TokenTypeName[1033:1045],\n\t_TokenTypeName[1045:1056],\n\t_TokenTypeName[1056:1063],\n\t_TokenTypeName[1063:1078],\n\t_TokenTypeName[1078:1094],\n\t_TokenTypeName[1094:1107],\n\t_TokenTypeName[1107:1121],\n\t_TokenTypeName[1121:1135],\n\t_TokenTypeName[1135:1153],\n\t_TokenTypeName[1153:1160],\n\t_TokenTypeName[1160:1174],\n\t_TokenTypeName[1174:1185],\n\t_TokenTypeName[1185:1197],\n\t_TokenTypeName[1197:1211],\n\t_TokenTypeName[1211:1226],\n\t_TokenTypeName[1226:1239],\n\t_TokenTypeName[1239:1252],\n\t_TokenTypeName[1252:1265],\n\t_TokenTypeName[1265:1282],\n\t_TokenTypeName[1282:1298],\n\t_TokenTypeName[1298:1314],\n\t_TokenTypeName[1314:1318],\n\t_TokenTypeName[1318:1332],\n\t_TokenTypeName[1332:1342],\n\t_TokenTypeName[1342:1357],\n}\n\n// TokenTypeString retrieves an enum value from the enum constants string name.\n// Throws an error if the param is not part of the enum.\nfunc TokenTypeString(s string) (TokenType, error) {\n\tif val, ok := _TokenTypeNameToValueMap[s]; ok {\n\t\treturn val, nil\n\t}\n\n\tif val, ok := _TokenTypeNameToValueMap[strings.ToLower(s)]; ok {\n\t\treturn val, nil\n\t}\n\treturn 0, fmt.Errorf(\"%s does not belong to TokenType values\", s)\n}\n\n// TokenTypeValues returns all values of the enum\nfunc TokenTypeValues() []TokenType {\n\treturn _TokenTypeValues\n}\n\n// TokenTypeStrings returns a slice of all String values of the enum\nfunc TokenTypeStrings() []string {\n\tstrs := make([]string, len(_TokenTypeNames))\n\tcopy(strs, _TokenTypeNames)\n\treturn strs\n}\n\n// IsATokenType returns \"true\" if the value is listed in the enum definition. \"false\" otherwise\nfunc (i TokenType) IsATokenType() bool {\n\t_, ok := _TokenTypeMap[i]\n\treturn ok\n}\n\n// MarshalText implements the encoding.TextMarshaler interface for TokenType\nfunc (i TokenType) MarshalText() ([]byte, error) {\n\treturn []byte(i.String()), nil\n}\n\n// UnmarshalText implements the encoding.TextUnmarshaler interface for TokenType\nfunc (i *TokenType) UnmarshalText(text []byte) error {\n\tvar err error\n\t*i, err = TokenTypeString(string(text))\n\treturn err\n}\n"
        },
        {
          "name": "types.go",
          "type": "blob",
          "size": 7.001953125,
          "content": "package chroma\n\n//go:generate enumer -text -type TokenType\n\n// TokenType is the type of token to highlight.\n//\n// It is also an Emitter, emitting a single token of itself\ntype TokenType int\n\n// Set of TokenTypes.\n//\n// Categories of types are grouped in ranges of 1000, while sub-categories are in ranges of 100. For\n// example, the literal category is in the range 3000-3999. The sub-category for literal strings is\n// in the range 3100-3199.\n\n// Meta token types.\nconst (\n\t// Default background style.\n\tBackground TokenType = -1 - iota\n\t// PreWrapper style.\n\tPreWrapper\n\t// Line style.\n\tLine\n\t// Line numbers in output.\n\tLineNumbers\n\t// Line numbers in output when in table.\n\tLineNumbersTable\n\t// Line higlight style.\n\tLineHighlight\n\t// Line numbers table wrapper style.\n\tLineTable\n\t// Line numbers table TD wrapper style.\n\tLineTableTD\n\t// Line number links.\n\tLineLink\n\t// Code line wrapper style.\n\tCodeLine\n\t// Input that could not be tokenised.\n\tError\n\t// Other is used by the Delegate lexer to indicate which tokens should be handled by the delegate.\n\tOther\n\t// No highlighting.\n\tNone\n\t// Don't emit this token to the output.\n\tIgnore\n\t// Used as an EOF marker / nil token\n\tEOFType TokenType = 0\n)\n\n// Keywords.\nconst (\n\tKeyword TokenType = 1000 + iota\n\tKeywordConstant\n\tKeywordDeclaration\n\tKeywordNamespace\n\tKeywordPseudo\n\tKeywordReserved\n\tKeywordType\n)\n\n// Names.\nconst (\n\tName TokenType = 2000 + iota\n\tNameAttribute\n\tNameBuiltin\n\tNameBuiltinPseudo\n\tNameClass\n\tNameConstant\n\tNameDecorator\n\tNameEntity\n\tNameException\n\tNameFunction\n\tNameFunctionMagic\n\tNameKeyword\n\tNameLabel\n\tNameNamespace\n\tNameOperator\n\tNameOther\n\tNamePseudo\n\tNameProperty\n\tNameTag\n\tNameVariable\n\tNameVariableAnonymous\n\tNameVariableClass\n\tNameVariableGlobal\n\tNameVariableInstance\n\tNameVariableMagic\n)\n\n// Literals.\nconst (\n\tLiteral TokenType = 3000 + iota\n\tLiteralDate\n\tLiteralOther\n)\n\n// Strings.\nconst (\n\tLiteralString TokenType = 3100 + iota\n\tLiteralStringAffix\n\tLiteralStringAtom\n\tLiteralStringBacktick\n\tLiteralStringBoolean\n\tLiteralStringChar\n\tLiteralStringDelimiter\n\tLiteralStringDoc\n\tLiteralStringDouble\n\tLiteralStringEscape\n\tLiteralStringHeredoc\n\tLiteralStringInterpol\n\tLiteralStringName\n\tLiteralStringOther\n\tLiteralStringRegex\n\tLiteralStringSingle\n\tLiteralStringSymbol\n)\n\n// Literals.\nconst (\n\tLiteralNumber TokenType = 3200 + iota\n\tLiteralNumberBin\n\tLiteralNumberFloat\n\tLiteralNumberHex\n\tLiteralNumberInteger\n\tLiteralNumberIntegerLong\n\tLiteralNumberOct\n\tLiteralNumberByte\n)\n\n// Operators.\nconst (\n\tOperator TokenType = 4000 + iota\n\tOperatorWord\n)\n\n// Punctuation.\nconst (\n\tPunctuation TokenType = 5000 + iota\n)\n\n// Comments.\nconst (\n\tComment TokenType = 6000 + iota\n\tCommentHashbang\n\tCommentMultiline\n\tCommentSingle\n\tCommentSpecial\n)\n\n// Preprocessor \"comments\".\nconst (\n\tCommentPreproc TokenType = 6100 + iota\n\tCommentPreprocFile\n)\n\n// Generic tokens.\nconst (\n\tGeneric TokenType = 7000 + iota\n\tGenericDeleted\n\tGenericEmph\n\tGenericError\n\tGenericHeading\n\tGenericInserted\n\tGenericOutput\n\tGenericPrompt\n\tGenericStrong\n\tGenericSubheading\n\tGenericTraceback\n\tGenericUnderline\n)\n\n// Text.\nconst (\n\tText TokenType = 8000 + iota\n\tTextWhitespace\n\tTextSymbol\n\tTextPunctuation\n)\n\n// Aliases.\nconst (\n\tWhitespace = TextWhitespace\n\n\tDate = LiteralDate\n\n\tString          = LiteralString\n\tStringAffix     = LiteralStringAffix\n\tStringBacktick  = LiteralStringBacktick\n\tStringChar      = LiteralStringChar\n\tStringDelimiter = LiteralStringDelimiter\n\tStringDoc       = LiteralStringDoc\n\tStringDouble    = LiteralStringDouble\n\tStringEscape    = LiteralStringEscape\n\tStringHeredoc   = LiteralStringHeredoc\n\tStringInterpol  = LiteralStringInterpol\n\tStringOther     = LiteralStringOther\n\tStringRegex     = LiteralStringRegex\n\tStringSingle    = LiteralStringSingle\n\tStringSymbol    = LiteralStringSymbol\n\n\tNumber            = LiteralNumber\n\tNumberBin         = LiteralNumberBin\n\tNumberFloat       = LiteralNumberFloat\n\tNumberHex         = LiteralNumberHex\n\tNumberInteger     = LiteralNumberInteger\n\tNumberIntegerLong = LiteralNumberIntegerLong\n\tNumberOct         = LiteralNumberOct\n)\n\nvar (\n\tStandardTypes = map[TokenType]string{\n\t\tBackground:       \"bg\",\n\t\tPreWrapper:       \"chroma\",\n\t\tLine:             \"line\",\n\t\tLineNumbers:      \"ln\",\n\t\tLineNumbersTable: \"lnt\",\n\t\tLineHighlight:    \"hl\",\n\t\tLineTable:        \"lntable\",\n\t\tLineTableTD:      \"lntd\",\n\t\tLineLink:         \"lnlinks\",\n\t\tCodeLine:         \"cl\",\n\t\tText:             \"\",\n\t\tWhitespace:       \"w\",\n\t\tError:            \"err\",\n\t\tOther:            \"x\",\n\t\t// I have no idea what this is used for...\n\t\t// Escape:     \"esc\",\n\n\t\tKeyword:            \"k\",\n\t\tKeywordConstant:    \"kc\",\n\t\tKeywordDeclaration: \"kd\",\n\t\tKeywordNamespace:   \"kn\",\n\t\tKeywordPseudo:      \"kp\",\n\t\tKeywordReserved:    \"kr\",\n\t\tKeywordType:        \"kt\",\n\n\t\tName:                 \"n\",\n\t\tNameAttribute:        \"na\",\n\t\tNameBuiltin:          \"nb\",\n\t\tNameBuiltinPseudo:    \"bp\",\n\t\tNameClass:            \"nc\",\n\t\tNameConstant:         \"no\",\n\t\tNameDecorator:        \"nd\",\n\t\tNameEntity:           \"ni\",\n\t\tNameException:        \"ne\",\n\t\tNameFunction:         \"nf\",\n\t\tNameFunctionMagic:    \"fm\",\n\t\tNameProperty:         \"py\",\n\t\tNameLabel:            \"nl\",\n\t\tNameNamespace:        \"nn\",\n\t\tNameOther:            \"nx\",\n\t\tNameTag:              \"nt\",\n\t\tNameVariable:         \"nv\",\n\t\tNameVariableClass:    \"vc\",\n\t\tNameVariableGlobal:   \"vg\",\n\t\tNameVariableInstance: \"vi\",\n\t\tNameVariableMagic:    \"vm\",\n\n\t\tLiteral:     \"l\",\n\t\tLiteralDate: \"ld\",\n\n\t\tString:          \"s\",\n\t\tStringAffix:     \"sa\",\n\t\tStringBacktick:  \"sb\",\n\t\tStringChar:      \"sc\",\n\t\tStringDelimiter: \"dl\",\n\t\tStringDoc:       \"sd\",\n\t\tStringDouble:    \"s2\",\n\t\tStringEscape:    \"se\",\n\t\tStringHeredoc:   \"sh\",\n\t\tStringInterpol:  \"si\",\n\t\tStringOther:     \"sx\",\n\t\tStringRegex:     \"sr\",\n\t\tStringSingle:    \"s1\",\n\t\tStringSymbol:    \"ss\",\n\n\t\tNumber:            \"m\",\n\t\tNumberBin:         \"mb\",\n\t\tNumberFloat:       \"mf\",\n\t\tNumberHex:         \"mh\",\n\t\tNumberInteger:     \"mi\",\n\t\tNumberIntegerLong: \"il\",\n\t\tNumberOct:         \"mo\",\n\n\t\tOperator:     \"o\",\n\t\tOperatorWord: \"ow\",\n\n\t\tPunctuation: \"p\",\n\n\t\tComment:            \"c\",\n\t\tCommentHashbang:    \"ch\",\n\t\tCommentMultiline:   \"cm\",\n\t\tCommentPreproc:     \"cp\",\n\t\tCommentPreprocFile: \"cpf\",\n\t\tCommentSingle:      \"c1\",\n\t\tCommentSpecial:     \"cs\",\n\n\t\tGeneric:           \"g\",\n\t\tGenericDeleted:    \"gd\",\n\t\tGenericEmph:       \"ge\",\n\t\tGenericError:      \"gr\",\n\t\tGenericHeading:    \"gh\",\n\t\tGenericInserted:   \"gi\",\n\t\tGenericOutput:     \"go\",\n\t\tGenericPrompt:     \"gp\",\n\t\tGenericStrong:     \"gs\",\n\t\tGenericSubheading: \"gu\",\n\t\tGenericTraceback:  \"gt\",\n\t\tGenericUnderline:  \"gl\",\n\t}\n)\n\nfunc (t TokenType) Parent() TokenType {\n\tif t%100 != 0 {\n\t\treturn t / 100 * 100\n\t}\n\tif t%1000 != 0 {\n\t\treturn t / 1000 * 1000\n\t}\n\treturn 0\n}\n\nfunc (t TokenType) Category() TokenType {\n\treturn t / 1000 * 1000\n}\n\nfunc (t TokenType) SubCategory() TokenType {\n\treturn t / 100 * 100\n}\n\nfunc (t TokenType) InCategory(other TokenType) bool {\n\treturn t/1000 == other/1000\n}\n\nfunc (t TokenType) InSubCategory(other TokenType) bool {\n\treturn t/100 == other/100\n}\n\nfunc (t TokenType) Emit(groups []string, _ *LexerState) Iterator {\n\treturn Literator(Token{Type: t, Value: groups[0]})\n}\n\nfunc (t TokenType) EmitterKind() string { return \"token\" }\n"
        }
      ]
    }
  ]
}