{
  "metadata": {
    "timestamp": 1736566808056,
    "page": 372,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "go-redsync/redsync",
      "stars": 3495,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0244140625,
          "content": "/test.out\ndump.rdb\n.idea\n"
        },
        {
          "name": ".gitlab-ci.yml",
          "type": "blob",
          "size": 0.7373046875,
          "content": "variables:\n  GOCACHE: $CI_PROJECT_DIR/.go-build\n\ncache:\n  key: \"$CI_PROJECT_PATH $CI_BUILD_REF_NAME\"\n  paths:\n    - .go-build\n\nimage: registry.furqansoftware.net/cardboard/golang:1.21.5-bullseye-0\n\nstages:\n  - lint\n  - build\n  - test\n\nlint:\n  stage: lint\n  script:\n    - go install honnef.co/go/tools/cmd/staticcheck@2023.1.5\n    - staticcheck ./...\n\nbuild:\n  stage: build\n  script:\n    - go build ./...\n\ntest:build:\n  stage: test\n  needs:\n    - build\n  script:\n    - make test.build\n  artifacts:\n    expire_in: 3 hours\n    paths:\n      - test.out/*\n\ntest:run:\n  image: redis:7.2.3\n  stage: test\n  needs:\n    - test:build\n  dependencies:\n    - test:build\n  script:\n    - |\n      for f in test.out/*; do\n        chmod +x $f\n        ./$f -test.v\n      done\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.1064453125,
          "content": "---\nlanguage: go\n\ngo:\n  - \"1.15\"\n  - \"1.16\"\n\nenv:\n  global:\n    - GO111MODULE=on\n\nservices:\n  - redis-server\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.447265625,
          "content": "Copyright (c) 2023, Mahmud Ridwan\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the Redsync nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.43359375,
          "content": ".PHONY: test\ntest:\n\tgo test -race ./...\n\n.PHONY: test.build\ntest.build:\n\tmkdir -p test.out/\n\tfor f in $(shell go list ./...); do \\\n\t\tgo test -race -c -o test.out/`echo \"$$f\" | sha256sum | cut -c1-5`-`basename \"$$f\"` $$f ; \\\n\tdone\n\n.PHONY: test.run\ntest.run:\n\tfor f in test.out/*; do \\\n\t\t$$f ; \\\n\tdone\n\n.PHONY: lint\nlint:\n\tstaticcheck ./...\n\n.PHONY: lint.tools.install\nlint.tools.install:\n\tgo install honnef.co/go/tools/cmd/staticcheck@2023.1.5\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.35546875,
          "content": "# Redsync\n\n[![Go Reference](https://pkg.go.dev/badge/github.com/go-redsync/redsync/v4.svg)](https://pkg.go.dev/github.com/go-redsync/redsync/v4) [![Build Status](https://travis-ci.org/go-redsync/redsync.svg?branch=master)](https://travis-ci.org/go-redsync/redsync) \n\nRedsync provides a Redis-based distributed mutual exclusion lock implementation for Go as described in [this post](http://redis.io/topics/distlock). A reference library (by [antirez](https://github.com/antirez)) for Ruby is available at [github.com/antirez/redlock-rb](https://github.com/antirez/redlock-rb).\n\n## Installation\n\nInstall Redsync using the go get command:\n\n    $ go get github.com/go-redsync/redsync/v4\n\nTwo driver implementations will be installed; however, only the one used will be included in your project.\n\n * [Redigo](https://github.com/gomodule/redigo)\n * [Go-redis](https://github.com/go-redis/redis)\n\nSee the [examples](examples) folder for usage of each driver.\n\n## Documentation\n\n- [Reference](https://godoc.org/github.com/go-redsync/redsync)\n\n## Usage\n\nError handling is simplified to `panic` for shorter example.\n\n```go\npackage main\n\nimport (\n\tgoredislib \"github.com/redis/go-redis/v9\"\n\t\"github.com/go-redsync/redsync/v4\"\n\t\"github.com/go-redsync/redsync/v4/redis/goredis/v9\"\n)\n\nfunc main() {\n\t// Create a pool with go-redis (or redigo) which is the pool redisync will\n\t// use while communicating with Redis. This can also be any pool that\n\t// implements the `redis.Pool` interface.\n\tclient := goredislib.NewClient(&goredislib.Options{\n\t\tAddr: \"localhost:6379\",\n\t})\n\tpool := goredis.NewPool(client) // or, pool := redigo.NewPool(...)\n\n\t// Create an instance of redisync to be used to obtain a mutual exclusion\n\t// lock.\n\trs := redsync.New(pool)\n\n\t// Obtain a new mutex by using the same name for all instances wanting the\n\t// same lock.\n\tmutexname := \"my-global-mutex\"\n\tmutex := rs.NewMutex(mutexname)\n\n\t// Obtain a lock for our given mutex. After this is successful, no one else\n\t// can obtain the same lock (the same mutex name) until we unlock it.\n\tif err := mutex.Lock(); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Do your work that requires the lock.\n\n\t// Release the lock so other processes or threads can obtain a lock.\n\tif ok, err := mutex.Unlock(); !ok || err != nil {\n\t\tpanic(\"unlock failed\")\n\t}\n}\n```\n\n## Contributing\n\nContributions are welcome.\n\n## License\n\nRedsync is available under the [BSD (3-Clause) License](https://opensource.org/licenses/BSD-3-Clause).\n\n## Disclaimer\n\nThis code implements an algorithm which is currently a proposal, it was not formally analyzed. Make sure to understand how it works before using it in production environments.\n\n## Real World Uses\n\nBelow is a list of public, open source projects that use Redsync:\n\n- [Sourcegraph](https://github.com/sourcegraph/sourcegraph): Universal code search and intelligence platform. Uses Redsync in an internal cache implementation.\n- [Open Match](https://github.com/googleforgames/open-match) by Google: Flexible, extensible, and scalable video game matchmaking. Uses Redsync with its state store implementation.\n- [Gocron](https://github.com/go-co-op/gocron) by go-co-op: gocron is a job distributed scheduling package which lets you run Go functions at pre-determined intervals using a simple, human-friendly syntax. Uses Redsync with its distributed job scheduler implementation.\n\nIf you are using Redsync in a project please send a pull request to add it to the list.\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.2392578125,
          "content": "// Package redsync provides a Redis-based distributed mutual exclusion lock implementation as described in the post http://redis.io/topics/distlock.\n//\n// Values containing the types defined in this package should not be copied.\npackage redsync\n"
        },
        {
          "name": "error.go",
          "type": "blob",
          "size": 1.2646484375,
          "content": "package redsync\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n)\n\n// ErrFailed is the error resulting if Redsync fails to acquire the lock after\n// exhausting all retries.\nvar ErrFailed = errors.New(\"redsync: failed to acquire lock\")\n\n// ErrExtendFailed is the error resulting if Redsync fails to extend the\n// lock.\nvar ErrExtendFailed = errors.New(\"redsync: failed to extend lock\")\n\n// ErrLockAlreadyExpired is the error resulting if trying to unlock the lock which already expired.\nvar ErrLockAlreadyExpired = errors.New(\"redsync: failed to unlock, lock was already expired\")\n\n// ErrTaken happens when the lock is already taken in a quorum on nodes.\ntype ErrTaken struct {\n\tNodes []int\n}\n\nfunc (err ErrTaken) Error() string {\n\treturn fmt.Sprintf(\"lock already taken, locked nodes: %v\", err.Nodes)\n}\n\n// ErrNodeTaken is the error resulting if the lock is already taken in one of\n// the cluster's nodes\ntype ErrNodeTaken struct {\n\tNode int\n}\n\nfunc (err ErrNodeTaken) Error() string {\n\treturn fmt.Sprintf(\"node #%d: lock already taken\", err.Node)\n}\n\n// A RedisError is an error communicating with one of the Redis nodes.\ntype RedisError struct {\n\tNode int\n\tErr  error\n}\n\nfunc (e RedisError) Error() string {\n\treturn fmt.Sprintf(\"node #%d: %s\", e.Node, e.Err)\n}\n\nfunc (e RedisError) Unwrap() error {\n\treturn e.Err\n}\n"
        },
        {
          "name": "error_test.go",
          "type": "blob",
          "size": 1.0078125,
          "content": "package redsync\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestRedisErrorIs(t *testing.T) {\n\tcases := map[string]error{\n\t\t\"defined_error\":  ErrFailed,\n\t\t\"other_error\":    errors.New(\"other error\"),\n\t\t\"wrapping_error\": fmt.Errorf(\"wrapping: %w\", ErrFailed),\n\t}\n\n\tfor k, v := range cases {\n\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\terr := RedisError{\n\t\t\t\tNode: 0,\n\t\t\t\tErr:  v,\n\t\t\t}\n\n\t\t\tif !errors.Is(err, v) {\n\t\t\t\tt.Errorf(\"errors.Is must be true\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestRedisErrorAs(t *testing.T) {\n\tderr := dummyError{}\n\n\tcases := map[string]error{\n\t\t\"simple_error\":   derr,\n\t\t\"wrapping_error\": fmt.Errorf(\"wrapping: %w\", derr),\n\t}\n\n\tfor k, v := range cases {\n\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\terr := RedisError{\n\t\t\t\tNode: 0,\n\t\t\t\tErr:  v,\n\t\t\t}\n\n\t\t\tvar target dummyError\n\t\t\tif !errors.As(err, &target) {\n\t\t\t\tt.Errorf(\"errors.As must be true\")\n\t\t\t}\n\n\t\t\tif target != derr {\n\t\t\t\tt.Errorf(\"Expected target == %q, got %q\", derr, target)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype dummyError struct{}\n\nfunc (err dummyError) Error() string {\n\treturn \"dummy\"\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.6982421875,
          "content": "module github.com/go-redsync/redsync/v4\n\ngo 1.22\n\nrequire (\n\tgithub.com/go-redis/redis v6.15.9+incompatible\n\tgithub.com/go-redis/redis/v7 v7.4.1\n\tgithub.com/go-redis/redis/v8 v8.11.5\n\tgithub.com/gomodule/redigo v1.8.9\n\tgithub.com/hashicorp/go-multierror v1.1.1\n\tgithub.com/redis/go-redis/v9 v9.5.1\n\tgithub.com/redis/rueidis v1.0.19\n\tgithub.com/stvp/tempredis v0.0.0-20181119212430-b82af8480203\n\tgolang.org/x/sync v0.3.0\n)\n\nrequire (\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n\tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n)\n\nreplace github.com/stvp/tempredis => github.com/hjr265/tempredis v0.0.0-20231015061547-ad8aa5a343a2\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 8.1005859375,
          "content": "github.com/bsm/ginkgo/v2 v2.12.0 h1:Ny8MWAHyOepLGlLKYmXG4IEkioBysk6GpaRTLC8zwWs=\ngithub.com/bsm/ginkgo/v2 v2.12.0/go.mod h1:SwYbGRRDovPVboqFv0tPTcG1sN61LM1Z4ARdbAV9g4c=\ngithub.com/bsm/gomega v1.27.10 h1:yeMWxP2pV2fG3FgAODIY8EiRE3dy0aeFYt4l7wh6yKA=\ngithub.com/bsm/gomega v1.27.10/go.mod h1:JyEr/xRbxbtgWNi8tIEVPUYZ5Dzef52k01W3YH0H+O0=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/fsnotify/fsnotify v1.4.9 h1:hsms1Qyu0jgnwNXIxa+/V/PDsU6CfLf6CNO8H7IWoS4=\ngithub.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=\ngithub.com/go-logr/logr v1.2.4 h1:g01GSCwiDw2xSZfjJ2/T9M+S6pFdcNtFYsp+Y43HYDQ=\ngithub.com/go-logr/logr v1.2.4/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=\ngithub.com/go-redis/redis v6.15.9+incompatible h1:K0pv1D7EQUjfyoMql+r/jZqCLizCGKFlFgcHWWmHQjg=\ngithub.com/go-redis/redis v6.15.9+incompatible/go.mod h1:NAIEuMOZ/fxfXJIrKDQDz8wamY7mA7PouImQ2Jvg6kA=\ngithub.com/go-redis/redis/v7 v7.4.1 h1:PASvf36gyUpr2zdOUS/9Zqc80GbM+9BDyiJSJDDOrTI=\ngithub.com/go-redis/redis/v7 v7.4.1/go.mod h1:JDNMw23GTyLNC4GZu9njt15ctBQVn7xjRfnwdHj/Dcg=\ngithub.com/go-redis/redis/v8 v8.11.5 h1:AcZZR7igkdvfVmQTPnu9WE37LRrO/YrBH5zWyjDC0oI=\ngithub.com/go-redis/redis/v8 v8.11.5/go.mod h1:gREzHqY1hg6oD9ngVRbLStwAWKhA0FEgq8Jd4h5lpwo=\ngithub.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 h1:tfuBGBXKqDEevZMzYi5KSi8KkcZtzBcTgAUUtapy0OI=\ngithub.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572/go.mod h1:9Pwr4B2jHnOSGXyyzV8ROjYa2ojvAY6HCGYYfMoC3Ls=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/gomodule/redigo v1.8.9 h1:Sl3u+2BI/kk+VEatbj0scLdrFhjPmbxOc1myhDP41ws=\ngithub.com/gomodule/redigo v1.8.9/go.mod h1:7ArFNvsTjH8GMMzB4uy1snslv2BwmginuMs06a1uzZE=\ngithub.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=\ngithub.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/pprof v0.0.0-20230207041349-798e818bf904 h1:4/hN5RUoecvl+RmJRE2YxKWtnnQls6rQjjW5oV7qg2U=\ngithub.com/google/pprof v0.0.0-20230207041349-798e818bf904/go.mod h1:uglQLonpP8qtYCYyzA+8c/9qtqgA3qsXGYqCPKARAFg=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hjr265/tempredis v0.0.0-20231015061547-ad8aa5a343a2 h1:3IPtBIkMrXTxPqNlTYKaeLQT8FteBIL4ZwuWMf6bcUg=\ngithub.com/hjr265/tempredis v0.0.0-20231015061547-ad8aa5a343a2/go.mod h1:XI9XLfrJNCQs1vEL3HtLol9YiQkGRjIoa065wm0DW90=\ngithub.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/nxadm/tail v1.4.8 h1:nPr65rt6Y5JFSKQO7qToXr7pePgD6Gwiw05lkbyAQTE=\ngithub.com/nxadm/tail v1.4.8/go.mod h1:+ncqLTQzXmGhMZNUePPaPqPvBxHAIsmXswZKocGu+AU=\ngithub.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.10.1/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.16.5 h1:8xi0RTUf59SOSfEtZMvwTvXYMzG4gV23XVHOZiXNtnE=\ngithub.com/onsi/ginkgo v1.16.5/go.mod h1:+E8gABHa3K6zRBolWtd+ROzc/U5bkGt0FwiG042wbpU=\ngithub.com/onsi/ginkgo/v2 v2.11.0 h1:WgqUCUt/lT6yXoQ8Wef0fsNn5cAuMK7+KT9UFRz2tcU=\ngithub.com/onsi/ginkgo/v2 v2.11.0/go.mod h1:ZhrRA5XmEE3x3rhlzamx/JJvujdZoJ2uvgI7kR0iZvM=\ngithub.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/onsi/gomega v1.27.10 h1:naR28SdDFlqrG6kScpT8VWpu1xWY5nJRCF3XaYyBjhI=\ngithub.com/onsi/gomega v1.27.10/go.mod h1:RsS8tutOdbdgzbPtzzATp12yT7kM5I5aElG3evPbQ0M=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/redis/go-redis/v9 v9.5.1 h1:H1X4D3yHPaYrkL5X06Wh6xNVM/pX0Ft4RV0vMGvLBh8=\ngithub.com/redis/go-redis/v9 v9.5.1/go.mod h1:hdY0cQFCN4fnSYT6TkisLufl/4W5UIXyv0b/CLO2V2M=\ngithub.com/redis/rueidis v1.0.19 h1:s65oWtotzlIFN8eMPhyYwxlwLR1lUdhza2KtWprKYSo=\ngithub.com/redis/rueidis v1.0.19/go.mod h1:8B+r5wdnjwK3lTFml5VtxjzGOQAC+5UmujoD12pDrEo=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.7.0 h1:nwc3DEeHmmLAfoZucVR881uASk0Mfjw8xYJ99tb5CcY=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.12.0 h1:cfawfvKITfUsFCeJIHJrbSxpeu/E81khclypR0GVT50=\ngolang.org/x/net v0.12.0/go.mod h1:zEVYFnQC7m/vmpQFELhcD1EWkZlX69l4oqgmer6hfKA=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.3.0 h1:ftCYgMx6zT/asHUrPw8BLLscYtGznsLAnjq5RH9P66E=\ngolang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20191010194322-b09406accb47/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.10.0 h1:SqMFp9UcQJZa+pmYuAKjd9xq1f0j5rLcDIk0mj4qAsA=\ngolang.org/x/sys v0.10.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.11.0 h1:LAntKIrcmeSKERyiOh0XMV39LXS8IE9UL2yP7+f5ij4=\ngolang.org/x/text v0.11.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.9.3 h1:Gn1I8+64MsuTb/HpH+LmQtNas23LhUVr3rYZ0eKuaMM=\ngolang.org/x/tools v0.9.3/go.mod h1:owI94Op576fPu3cIGQeHs3joujW/2Oc6MtlxbF5dfNc=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "mutex.go",
          "type": "blob",
          "size": 8.626953125,
          "content": "package redsync\n\nimport (\n\t\"context\"\n\t\"crypto/rand\"\n\t\"encoding/base64\"\n\t\"time\"\n\n\t\"github.com/go-redsync/redsync/v4/redis\"\n\t\"github.com/hashicorp/go-multierror\"\n)\n\n// A DelayFunc is used to decide the amount of time to wait between retries.\ntype DelayFunc func(tries int) time.Duration\n\n// A Mutex is a distributed mutual exclusion lock.\ntype Mutex struct {\n\tname   string\n\texpiry time.Duration\n\n\ttries     int\n\tdelayFunc DelayFunc\n\n\tdriftFactor   float64\n\ttimeoutFactor float64\n\n\tquorum int\n\n\tgenValueFunc  func() (string, error)\n\tvalue         string\n\tuntil         time.Time\n\tshuffle       bool\n\tfailFast      bool\n\tsetNXOnExtend bool\n\n\tpools []redis.Pool\n}\n\n// Name returns mutex name (i.e. the Redis key).\nfunc (m *Mutex) Name() string {\n\treturn m.name\n}\n\n// Value returns the current random value. The value will be empty until a lock is acquired (or WithValue option is used).\nfunc (m *Mutex) Value() string {\n\treturn m.value\n}\n\n// Until returns the time of validity of acquired lock. The value will be zero value until a lock is acquired.\nfunc (m *Mutex) Until() time.Time {\n\treturn m.until\n}\n\n// TryLock only attempts to lock m once and returns immediately regardless of success or failure without retrying.\nfunc (m *Mutex) TryLock() error {\n\treturn m.TryLockContext(context.Background())\n}\n\n// TryLockContext only attempts to lock m once and returns immediately regardless of success or failure without retrying.\nfunc (m *Mutex) TryLockContext(ctx context.Context) error {\n\treturn m.lockContext(ctx, 1)\n}\n\n// Lock locks m. In case it returns an error on failure, you may retry to acquire the lock by calling this method again.\nfunc (m *Mutex) Lock() error {\n\treturn m.LockContext(context.Background())\n}\n\n// LockContext locks m. In case it returns an error on failure, you may retry to acquire the lock by calling this method again.\nfunc (m *Mutex) LockContext(ctx context.Context) error {\n\treturn m.lockContext(ctx, m.tries)\n}\n\n// lockContext locks m. In case it returns an error on failure, you may retry to acquire the lock by calling this method again.\nfunc (m *Mutex) lockContext(ctx context.Context, tries int) error {\n\tif ctx == nil {\n\t\tctx = context.Background()\n\t}\n\n\tvalue, err := m.genValueFunc()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar timer *time.Timer\n\tfor i := 0; i < tries; i++ {\n\t\tif i != 0 {\n\t\t\tif timer == nil {\n\t\t\t\ttimer = time.NewTimer(m.delayFunc(i))\n\t\t\t} else {\n\t\t\t\ttimer.Reset(m.delayFunc(i))\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\ttimer.Stop()\n\t\t\t\t// Exit early if the context is done.\n\t\t\t\treturn ErrFailed\n\t\t\tcase <-timer.C:\n\t\t\t\t// Fall-through when the delay timer completes.\n\t\t\t}\n\t\t}\n\n\t\tstart := time.Now()\n\n\t\tn, err := func() (int, error) {\n\t\t\tctx, cancel := context.WithTimeout(ctx, time.Duration(int64(float64(m.expiry)*m.timeoutFactor)))\n\t\t\tdefer cancel()\n\t\t\treturn m.actOnPoolsAsync(func(pool redis.Pool) (bool, error) {\n\t\t\t\treturn m.acquire(ctx, pool, value)\n\t\t\t})\n\t\t}()\n\n\t\tnow := time.Now()\n\t\tuntil := now.Add(m.expiry - now.Sub(start) - time.Duration(int64(float64(m.expiry)*m.driftFactor)))\n\t\tif n >= m.quorum && now.Before(until) {\n\t\t\tm.value = value\n\t\t\tm.until = until\n\t\t\treturn nil\n\t\t}\n\t\t_, _ = func() (int, error) {\n\t\t\tctx, cancel := context.WithTimeout(ctx, time.Duration(int64(float64(m.expiry)*m.timeoutFactor)))\n\t\t\tdefer cancel()\n\t\t\treturn m.actOnPoolsAsync(func(pool redis.Pool) (bool, error) {\n\t\t\t\treturn m.release(ctx, pool, value)\n\t\t\t})\n\t\t}()\n\t\tif i == tries-1 && err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn ErrFailed\n}\n\n// Unlock unlocks m and returns the status of unlock.\nfunc (m *Mutex) Unlock() (bool, error) {\n\treturn m.UnlockContext(context.Background())\n}\n\n// UnlockContext unlocks m and returns the status of unlock.\nfunc (m *Mutex) UnlockContext(ctx context.Context) (bool, error) {\n\tn, err := m.actOnPoolsAsync(func(pool redis.Pool) (bool, error) {\n\t\treturn m.release(ctx, pool, m.value)\n\t})\n\tif n < m.quorum {\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\n// Extend resets the mutex's expiry and returns the status of expiry extension.\nfunc (m *Mutex) Extend() (bool, error) {\n\treturn m.ExtendContext(context.Background())\n}\n\n// ExtendContext resets the mutex's expiry and returns the status of expiry extension.\nfunc (m *Mutex) ExtendContext(ctx context.Context) (bool, error) {\n\tstart := time.Now()\n\tn, err := m.actOnPoolsAsync(func(pool redis.Pool) (bool, error) {\n\t\treturn m.touch(ctx, pool, m.value, int(m.expiry/time.Millisecond))\n\t})\n\tif n < m.quorum {\n\t\treturn false, err\n\t}\n\tnow := time.Now()\n\tuntil := now.Add(m.expiry - now.Sub(start) - time.Duration(int64(float64(m.expiry)*m.driftFactor)))\n\tif now.Before(until) {\n\t\tm.until = until\n\t\treturn true, nil\n\t}\n\treturn false, ErrExtendFailed\n}\n\n// Valid returns true if the lock acquired through m is still valid. It may\n// also return true erroneously if quorum is achieved during the call and at\n// least one node then takes long enough to respond for the lock to expire.\n//\n// Deprecated: Use Until instead. See https://github.com/go-redsync/redsync/issues/72.\nfunc (m *Mutex) Valid() (bool, error) {\n\treturn m.ValidContext(context.Background())\n}\n\n// ValidContext returns true if the lock acquired through m is still valid. It may\n// also return true erroneously if quorum is achieved during the call and at\n// least one node then takes long enough to respond for the lock to expire.\n//\n// Deprecated: Use Until instead. See https://github.com/go-redsync/redsync/issues/72.\nfunc (m *Mutex) ValidContext(ctx context.Context) (bool, error) {\n\tn, err := m.actOnPoolsAsync(func(pool redis.Pool) (bool, error) {\n\t\treturn m.valid(ctx, pool)\n\t})\n\treturn n >= m.quorum, err\n}\n\nfunc (m *Mutex) valid(ctx context.Context, pool redis.Pool) (bool, error) {\n\tif m.value == \"\" {\n\t\treturn false, nil\n\t}\n\tconn, err := pool.Get(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer conn.Close()\n\treply, err := conn.Get(m.name)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn m.value == reply, nil\n}\n\nfunc genValue() (string, error) {\n\tb := make([]byte, 16)\n\t_, err := rand.Read(b)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (m *Mutex) acquire(ctx context.Context, pool redis.Pool, value string) (bool, error) {\n\tconn, err := pool.Get(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer conn.Close()\n\treply, err := conn.SetNX(m.name, value, m.expiry)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn reply, nil\n}\n\nvar deleteScript = redis.NewScript(1, `\n\tlocal val = redis.call(\"GET\", KEYS[1])\n\tif val == ARGV[1] then\n\t\treturn redis.call(\"DEL\", KEYS[1])\n\telseif val == false then\n\t\treturn -1\n\telse\n\t\treturn 0\n\tend\n`)\n\nfunc (m *Mutex) release(ctx context.Context, pool redis.Pool, value string) (bool, error) {\n\tconn, err := pool.Get(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer conn.Close()\n\tstatus, err := conn.Eval(deleteScript, m.name, value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif status == int64(-1) {\n\t\treturn false, ErrLockAlreadyExpired\n\t}\n\treturn status != int64(0), nil\n}\n\nvar touchWithSetNXScript = redis.NewScript(1, `\n\tif redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n\t\treturn redis.call(\"PEXPIRE\", KEYS[1], ARGV[2])\n\telseif redis.call(\"SET\", KEYS[1], ARGV[1], \"PX\", ARGV[2], \"NX\") then\n\t\treturn 1\n\telse\n\t\treturn 0\n\tend\n`)\n\nvar touchScript = redis.NewScript(1, `\n\tif redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n\t\treturn redis.call(\"PEXPIRE\", KEYS[1], ARGV[2])\n\telse\n\t\treturn 0\n\tend\n`)\n\nfunc (m *Mutex) touch(ctx context.Context, pool redis.Pool, value string, expiry int) (bool, error) {\n\tconn, err := pool.Get(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer conn.Close()\n\n\ttouchScript := touchScript\n\tif m.setNXOnExtend {\n\t\ttouchScript = touchWithSetNXScript\n\t}\n\n\tstatus, err := conn.Eval(touchScript, m.name, value, expiry)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn status != int64(0), nil\n}\n\nfunc (m *Mutex) actOnPoolsAsync(actFn func(redis.Pool) (bool, error)) (int, error) {\n\ttype result struct {\n\t\tnode     int\n\t\tstatusOK bool\n\t\terr      error\n\t}\n\n\tch := make(chan result, len(m.pools))\n\tfor node, pool := range m.pools {\n\t\tgo func(node int, pool redis.Pool) {\n\t\t\tr := result{node: node}\n\t\t\tr.statusOK, r.err = actFn(pool)\n\t\t\tch <- r\n\t\t}(node, pool)\n\t}\n\n\tvar (\n\t\tn     = 0\n\t\ttaken []int\n\t\terr   error\n\t)\n\n\tfor range m.pools {\n\t\tr := <-ch\n\t\tif r.statusOK {\n\t\t\tn++\n\t\t} else if r.err == ErrLockAlreadyExpired {\n\t\t\terr = multierror.Append(err, ErrLockAlreadyExpired)\n\t\t} else if r.err != nil {\n\t\t\terr = multierror.Append(err, &RedisError{Node: r.node, Err: r.err})\n\t\t} else {\n\t\t\ttaken = append(taken, r.node)\n\t\t\terr = multierror.Append(err, &ErrNodeTaken{Node: r.node})\n\t\t}\n\n\t\tif m.failFast {\n\t\t\t// fast return\n\t\t\tif n >= m.quorum {\n\t\t\t\treturn n, err\n\t\t\t}\n\n\t\t\t// fail fast\n\t\t\tif len(taken) >= m.quorum {\n\t\t\t\treturn n, &ErrTaken{Nodes: taken}\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(taken) >= m.quorum {\n\t\treturn n, &ErrTaken{Nodes: taken}\n\t}\n\treturn n, err\n}\n"
        },
        {
          "name": "mutex_test.go",
          "type": "blob",
          "size": 9.5791015625,
          "content": "package redsync\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/go-redsync/redsync/v4/redis\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestMutex(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex\", v.poolCount)\n\t\t\teg := errgroup.Group{}\n\t\t\tfor i, mutex := range mutexes {\n\t\t\t\tfunc(i int, mutex *Mutex) {\n\t\t\t\t\teg.Go(func() error {\n\t\t\t\t\t\terr := mutex.Lock()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\t\t\t\t\t\tdefer mutex.Unlock()\n\n\t\t\t\t\t\tif !isAcquired(ctx, v.pools, mutex) {\n\t\t\t\t\t\t\treturn fmt.Errorf(\"Expected n >= %d, got %d\", mutex.quorum, countAcquiredPools(ctx, v.pools, mutex))\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t})\n\t\t\t\t}(i, mutex)\n\t\t\t}\n\t\t\terr := eg.Wait()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTryLock(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-trylock\", v.poolCount)\n\t\t\teg := errgroup.Group{}\n\t\t\tfor i, mutex := range mutexes {\n\t\t\t\tfunc(i int, mutex *Mutex) {\n\t\t\t\t\teg.Go(func() error {\n\t\t\t\t\t\terr := mutex.TryLockContext(context.Background())\n\t\t\t\t\t\tfor {\n\t\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttime.Sleep(100 * time.Millisecond) // 1ms\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tok, err := mutex.Extend()\n\t\t\t\t\t\tif err != nil || !ok {\n\t\t\t\t\t\t\tt.Fatalf(\"extend failed: %v\", err)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tdefer mutex.Unlock()\n\n\t\t\t\t\t\tif !isAcquired(ctx, v.pools, mutex) {\n\t\t\t\t\t\t\treturn fmt.Errorf(\"Expected n >= %d, got %d\", mutex.quorum, countAcquiredPools(ctx, v.pools, mutex))\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t})\n\t\t\t\t}(i, mutex)\n\t\t\t}\n\t\t\terr := eg.Wait()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexAlreadyLocked(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(4) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\trs := New(v.pools...)\n\t\t\tkey := \"test-lock\"\n\n\t\t\tmutex1 := rs.NewMutex(key)\n\t\t\terr := mutex1.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tassertAcquired(ctx, t, v.pools, mutex1)\n\n\t\t\tmutex2 := rs.NewMutex(key)\n\t\t\terr = mutex2.TryLock()\n\t\t\tvar errTaken *ErrTaken\n\t\t\tif !errors.As(err, &errTaken) {\n\t\t\t\tt.Fatalf(\"mutex was not already locked: %s\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexExtend(t *testing.T) {\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)\n\t\t\tmutex := mutexes[0]\n\n\t\t\terr := mutex.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tdefer mutex.Unlock()\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\texpiries := getPoolExpiries(v.pools, mutex.name)\n\t\t\tok, err := mutex.Extend()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex extend failed: %s\", err)\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"Expected a valid mutex\")\n\t\t\t}\n\t\t\texpiries2 := getPoolExpiries(v.pools, mutex.name)\n\n\t\t\tfor i, expiry := range expiries {\n\t\t\t\tif expiry >= expiries2[i] {\n\t\t\t\t\tt.Fatalf(\"Expected expiries[%d] > %d, got %d\", i, expiry, expiries2[i])\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexExtendExpired(t *testing.T) {\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)\n\t\t\tmutex := mutexes[0]\n\t\t\tmutex.expiry = 500 * time.Millisecond\n\n\t\t\terr := mutex.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tdefer mutex.Unlock()\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t_, err = mutex.Extend()\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"mutex extend didn't fail\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestSetNXOnExtendAcquiresLockWhenKeyIsExpired(t *testing.T) {\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)\n\t\t\tmutex := mutexes[0]\n\t\t\tmutex.setNXOnExtend = true\n\t\t\tmutex.expiry = 500 * time.Millisecond\n\n\t\t\terr := mutex.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tdefer mutex.Unlock()\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t_, err = mutex.Extend()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex didn't extend\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestSetNXOnExtendFailsToAcquireLockWhenKeyIsTaken(t *testing.T) {\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tfirstMutex := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)[0]\n\t\t\tfirstMutex.expiry = 500 * time.Millisecond\n\n\t\t\terr := firstMutex.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tdefer firstMutex.Unlock()\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\tsecondMutex := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)[0]\n\t\t\tfirstMutex.expiry = 500 * time.Millisecond\n\t\t\terr = secondMutex.Lock()\n\t\t\tdefer secondMutex.Unlock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"second mutex couldn't lock\")\n\t\t\t}\n\n\t\t\tok, err := firstMutex.Extend()\n\t\t\tfirstMutex.setNXOnExtend = true\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"mutex extend didn't fail\")\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tt.Fatalf(\"Expected ok == false, got %v\", ok)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexUnlockExpired(t *testing.T) {\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex-extend\", 1)\n\t\t\tmutex := mutexes[0]\n\t\t\tmutex.expiry = 500 * time.Millisecond\n\n\t\t\terr := mutex.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tdefer mutex.Unlock()\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\tok, err := mutex.Unlock()\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"mutex unlock didn't fail\")\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tt.Fatalf(\"Expected ok == false, got %v\", ok)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexQuorum(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(4) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tfor mask := 0; mask < 1<<uint(len(v.pools)); mask++ {\n\t\t\t\tmutexes := newTestMutexes(v.pools, k+\"-test-mutex-partial-\"+strconv.Itoa(mask), 1)\n\t\t\t\tmutex := mutexes[0]\n\t\t\t\tmutex.tries = 1\n\n\t\t\t\tn := clogPools(v.pools, mask, mutex)\n\n\t\t\t\tif n >= len(v.pools)/2+1 {\n\t\t\t\t\terr := mutex.Lock()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t\t\t}\n\t\t\t\t\tassertAcquired(ctx, t, v.pools, mutex)\n\t\t\t\t} else {\n\t\t\t\t\terr := mutex.Lock()\n\t\t\t\t\tif errors.Is(err, &ErrNodeTaken{}) {\n\t\t\t\t\t\tt.Fatalf(\"Expected err == %q, got %q\", ErrNodeTaken{}, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestValid(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(4) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\trs := New(v.pools...)\n\t\t\tkey := k + \"-test-shared-lock\"\n\n\t\t\tmutex1 := rs.NewMutex(key, WithExpiry(time.Hour))\n\t\t\terr := mutex1.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tassertAcquired(ctx, t, v.pools, mutex1)\n\n\t\t\tok, err := mutex1.Valid()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex valid failed: %s\", err)\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"Expected a valid mutex\")\n\t\t\t}\n\n\t\t\tmutex2 := rs.NewMutex(key)\n\t\t\terr = mutex2.Lock()\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMutexLockUnlockSplit(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(4) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\trs := New(v.pools...)\n\t\t\tkey := \"test-split-lock\"\n\n\t\t\tmutex1 := rs.NewMutex(key, WithExpiry(time.Hour))\n\t\t\terr := mutex1.Lock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex lock failed: %s\", err)\n\t\t\t}\n\t\t\tassertAcquired(ctx, t, v.pools, mutex1)\n\n\t\t\tmutex2 := rs.NewMutex(key, WithExpiry(time.Hour), WithValue(mutex1.Value()))\n\t\t\tok, err := mutex2.Unlock()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"mutex unlock failed: %s\", err)\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"Expected a valid mutex\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc getPoolValues(ctx context.Context, pools []redis.Pool, name string) []string {\n\tvalues := make([]string, len(pools))\n\tfor i, pool := range pools {\n\t\tconn, err := pool.Get(ctx)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tvalue, err := conn.Get(name)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t_ = conn.Close()\n\t\tvalues[i] = value\n\t}\n\treturn values\n}\n\nfunc getPoolExpiries(pools []redis.Pool, name string) []int {\n\texpiries := make([]int, len(pools))\n\tfor i, pool := range pools {\n\t\tconn, err := pool.Get(context.TODO())\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\texpiry, err := conn.PTTL(name)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t_ = conn.Close()\n\t\texpiries[i] = int(expiry)\n\t}\n\treturn expiries\n}\n\nfunc clogPools(pools []redis.Pool, mask int, mutex *Mutex) int {\n\tn := 0\n\tfor i, pool := range pools {\n\t\tif mask&(1<<uint(i)) == 0 {\n\t\t\tn++\n\t\t\tcontinue\n\t\t}\n\t\tconn, err := pool.Get(context.Background())\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t_, err = conn.Set(mutex.name, \"foobar\")\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t_ = conn.Close()\n\t}\n\treturn n\n}\n\nfunc newTestMutexes(pools []redis.Pool, name string, n int) []*Mutex {\n\tmutexes := make([]*Mutex, n)\n\tfor i := 0; i < n; i++ {\n\t\tmutexes[i] = &Mutex{\n\t\t\tname:          name + \"-\" + strconv.Itoa(i),\n\t\t\texpiry:        8 * time.Second,\n\t\t\ttries:         32,\n\t\t\tdelayFunc:     func(tries int) time.Duration { return 500 * time.Millisecond },\n\t\t\tgenValueFunc:  genValue,\n\t\t\tdriftFactor:   0.01,\n\t\t\ttimeoutFactor: 0.05,\n\t\t\tquorum:        len(pools)/2 + 1,\n\t\t\tpools:         pools,\n\t\t}\n\t}\n\treturn mutexes\n}\n\nfunc isAcquired(ctx context.Context, pools []redis.Pool, mutex *Mutex) bool {\n\tn := countAcquiredPools(ctx, pools, mutex)\n\treturn n >= mutex.quorum\n}\n\nfunc countAcquiredPools(ctx context.Context, pools []redis.Pool, mutex *Mutex) int {\n\tn := 0\n\tvalues := getPoolValues(ctx, pools, mutex.name)\n\tfor _, value := range values {\n\t\tif value == mutex.value {\n\t\t\tn++\n\t\t}\n\t}\n\treturn n\n}\n\nfunc assertAcquired(ctx context.Context, t *testing.T, pools []redis.Pool, mutex *Mutex) {\n\tn := 0\n\tvalues := getPoolValues(ctx, pools, mutex.name)\n\tfor _, value := range values {\n\t\tif value == mutex.value {\n\t\t\tn++\n\t\t}\n\t}\n\tif !isAcquired(ctx, pools, mutex) {\n\t\tt.Fatalf(\"Expected n >= %d, got %d\", mutex.quorum, n)\n\t}\n}\n"
        },
        {
          "name": "redis",
          "type": "tree",
          "content": null
        },
        {
          "name": "redsync.go",
          "type": "blob",
          "size": 4.265625,
          "content": "package redsync\n\nimport (\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/go-redsync/redsync/v4/redis\"\n)\n\nconst (\n\tminRetryDelayMilliSec = 50\n\tmaxRetryDelayMilliSec = 250\n)\n\n// Redsync provides a simple method for creating distributed mutexes using multiple Redis connection pools.\ntype Redsync struct {\n\tpools []redis.Pool\n}\n\n// New creates and returns a new Redsync instance from given Redis connection pools.\nfunc New(pools ...redis.Pool) *Redsync {\n\treturn &Redsync{\n\t\tpools: pools,\n\t}\n}\n\n// NewMutex returns a new distributed mutex with given name.\nfunc (r *Redsync) NewMutex(name string, options ...Option) *Mutex {\n\tm := &Mutex{\n\t\tname:   name,\n\t\texpiry: 8 * time.Second,\n\t\ttries:  32,\n\t\tdelayFunc: func(tries int) time.Duration {\n\t\t\treturn time.Duration(rand.Intn(maxRetryDelayMilliSec-minRetryDelayMilliSec)+minRetryDelayMilliSec) * time.Millisecond\n\t\t},\n\t\tgenValueFunc:  genValue,\n\t\tdriftFactor:   0.01,\n\t\ttimeoutFactor: 0.05,\n\t\tquorum:        len(r.pools)/2 + 1,\n\t\tpools:         r.pools,\n\t}\n\tfor _, o := range options {\n\t\to.Apply(m)\n\t}\n\tif m.shuffle {\n\t\trandomPools(m.pools)\n\t}\n\treturn m\n}\n\n// An Option configures a mutex.\ntype Option interface {\n\tApply(*Mutex)\n}\n\n// OptionFunc is a function that configures a mutex.\ntype OptionFunc func(*Mutex)\n\n// Apply calls f(mutex)\nfunc (f OptionFunc) Apply(mutex *Mutex) {\n\tf(mutex)\n}\n\n// WithExpiry can be used to set the expiry of a mutex to the given value.\n// The default is 8s.\nfunc WithExpiry(expiry time.Duration) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.expiry = expiry\n\t})\n}\n\n// WithTries can be used to set the number of times lock acquire is attempted.\n// The default value is 32.\nfunc WithTries(tries int) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.tries = tries\n\t})\n}\n\n// WithRetryDelay can be used to set the amount of time to wait between retries.\n// The default value is rand(50ms, 250ms).\nfunc WithRetryDelay(delay time.Duration) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.delayFunc = func(tries int) time.Duration {\n\t\t\treturn delay\n\t\t}\n\t})\n}\n\n// WithSetNXOnExtend improves extending logic to extend the key if exist\n// and if not, tries to set a new key in redis\n// Useful if your redises restart often and you want to reduce the chances of losing the lock\n// Read this MR for more info: https://github.com/go-redsync/redsync/pull/149\nfunc WithSetNXOnExtend() Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.setNXOnExtend = true\n\t})\n}\n\n// WithRetryDelayFunc can be used to override default delay behavior.\nfunc WithRetryDelayFunc(delayFunc DelayFunc) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.delayFunc = delayFunc\n\t})\n}\n\n// WithDriftFactor can be used to set the clock drift factor.\n// The default value is 0.01.\nfunc WithDriftFactor(factor float64) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.driftFactor = factor\n\t})\n}\n\n// WithTimeoutFactor can be used to set the timeout factor.\n// The default value is 0.05.\nfunc WithTimeoutFactor(factor float64) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.timeoutFactor = factor\n\t})\n}\n\n// WithGenValueFunc can be used to set the custom value generator.\nfunc WithGenValueFunc(genValueFunc func() (string, error)) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.genValueFunc = genValueFunc\n\t})\n}\n\n// WithValue can be used to assign the random value without having to call lock.\n// This allows the ownership of a lock to be \"transferred\" and allows the lock to be unlocked from elsewhere.\nfunc WithValue(v string) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.value = v\n\t})\n}\n\n// WithFailFast can be used to quickly acquire and release the lock.\n// When some Redis servers are blocking, we do not need to wait for responses from all the Redis servers response.\n// As long as the quorum is met, we can assume the lock is acquired. The effect of this parameter is to achieve low\n// latency, avoid Redis blocking causing Lock/Unlock to not return for a long time.\nfunc WithFailFast(b bool) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.failFast = b\n\t})\n}\n\n// WithShufflePools can be used to shuffle Redis pools to reduce centralized access in concurrent scenarios.\nfunc WithShufflePools(b bool) Option {\n\treturn OptionFunc(func(m *Mutex) {\n\t\tm.shuffle = b\n\t})\n}\n\n// randomPools shuffles Redis pools.\nfunc randomPools(pools []redis.Pool) {\n\trand.Shuffle(len(pools), func(i, j int) {\n\t\tpools[i], pools[j] = pools[j], pools[i]\n\t})\n}\n"
        },
        {
          "name": "redsync_test.go",
          "type": "blob",
          "size": 4.404296875,
          "content": "package redsync\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\tgoredislib \"github.com/go-redis/redis\"\n\tgoredislib_v7 \"github.com/go-redis/redis/v7\"\n\tgoredislib_v8 \"github.com/go-redis/redis/v8\"\n\t\"github.com/go-redsync/redsync/v4/redis\"\n\t\"github.com/go-redsync/redsync/v4/redis/goredis\"\n\tgoredis_v7 \"github.com/go-redsync/redsync/v4/redis/goredis/v7\"\n\tgoredis_v8 \"github.com/go-redsync/redsync/v4/redis/goredis/v8\"\n\tgoredis_v9 \"github.com/go-redsync/redsync/v4/redis/goredis/v9\"\n\t\"github.com/go-redsync/redsync/v4/redis/redigo\"\n\trueidis \"github.com/go-redsync/redsync/v4/redis/rueidis\"\n\tredigolib \"github.com/gomodule/redigo/redis\"\n\tgoredislib_v9 \"github.com/redis/go-redis/v9\"\n\trueidislib \"github.com/redis/rueidis\"\n\t\"github.com/redis/rueidis/rueidiscompat\"\n\t\"github.com/stvp/tempredis\"\n)\n\nvar servers []*tempredis.Server\n\ntype testCase struct {\n\tpoolCount int\n\tpools     []redis.Pool\n}\n\nfunc makeCases(poolCount int) map[string]*testCase {\n\treturn map[string]*testCase{\n\t\t\"redigo\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsRedigo(poolCount),\n\t\t},\n\t\t\"goredis\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsGoredis(poolCount),\n\t\t},\n\t\t\"goredis_v7\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsGoredisV7(poolCount),\n\t\t},\n\t\t\"goredis_v8\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsGoredisV8(poolCount),\n\t\t},\n\t\t\"goredis_v9\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsGoredisV9(poolCount),\n\t\t},\n\t\t\"rueidis\": {\n\t\t\tpoolCount,\n\t\t\tnewMockPoolsRueidis(poolCount),\n\t\t},\n\t}\n}\n\n// Maintain separate blocks of servers for each type of driver\nconst (\n\tServerPools    = 6\n\tServerPoolSize = 8\n\tRedigoBlock    = 0\n\tGoredisBlock   = 1\n\tGoredisV7Block = 2\n\tGoredisV8Block = 3\n\tGoredisV9Block = 4\n\tRueidisBlock   = 5\n)\n\nfunc TestMain(m *testing.M) {\n\tfor i := 0; i < ServerPoolSize*ServerPools; i++ {\n\t\tserver, err := tempredis.Start(tempredis.Config{\n\t\t\t\"port\": strconv.Itoa(51200 + i),\n\t\t})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tservers = append(servers, server)\n\t}\n\tresult := m.Run()\n\tfor _, server := range servers {\n\t\t_ = server.Term()\n\t}\n\tos.Exit(result)\n}\n\nfunc TestRedsync(t *testing.T) {\n\tctx := context.Background()\n\tfor k, v := range makeCases(8) {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\trs := New(v.pools...)\n\n\t\t\tmutex := rs.NewMutex(\"test-redsync\")\n\t\t\t_ = mutex.Lock()\n\n\t\t\tassertAcquired(ctx, t, v.pools, mutex)\n\t\t})\n\t}\n}\n\nfunc newMockPoolsRedigo(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := RedigoBlock * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tserver := servers[i+offset]\n\t\tpools[i] = redigo.NewPool(&redigolib.Pool{\n\t\t\tMaxIdle:     3,\n\t\t\tIdleTimeout: 240 * time.Second,\n\t\t\tDial: func() (redigolib.Conn, error) {\n\t\t\t\treturn redigolib.Dial(\"unix\", server.Socket())\n\t\t\t},\n\t\t\tTestOnBorrow: func(c redigolib.Conn, t time.Time) error {\n\t\t\t\t_, err := c.Do(\"PING\")\n\t\t\t\treturn err\n\t\t\t},\n\t\t})\n\t}\n\treturn pools\n}\n\nfunc newMockPoolsGoredis(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := GoredisBlock * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tclient := goredislib.NewClient(&goredislib.Options{\n\t\t\tNetwork: \"unix\",\n\t\t\tAddr:    servers[i+offset].Socket(),\n\t\t})\n\t\tpools[i] = goredis.NewPool(client)\n\t}\n\treturn pools\n}\n\nfunc newMockPoolsGoredisV7(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := GoredisV7Block * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tclient := goredislib_v7.NewClient(&goredislib_v7.Options{\n\t\t\tNetwork: \"unix\",\n\t\t\tAddr:    servers[i+offset].Socket(),\n\t\t})\n\t\tpools[i] = goredis_v7.NewPool(client)\n\t}\n\treturn pools\n}\n\nfunc newMockPoolsGoredisV8(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := GoredisV8Block * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tclient := goredislib_v8.NewClient(&goredislib_v8.Options{\n\t\t\tNetwork: \"unix\",\n\t\t\tAddr:    servers[i+offset].Socket(),\n\t\t})\n\t\tpools[i] = goredis_v8.NewPool(client)\n\t}\n\treturn pools\n}\n\nfunc newMockPoolsGoredisV9(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := GoredisV9Block * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tclient := goredislib_v9.NewClient(&goredislib_v9.Options{\n\t\t\tNetwork: \"unix\",\n\t\t\tAddr:    servers[i+offset].Socket(),\n\t\t})\n\t\tpools[i] = goredis_v9.NewPool(client)\n\t}\n\treturn pools\n}\n\nfunc newMockPoolsRueidis(n int) []redis.Pool {\n\tpools := make([]redis.Pool, n)\n\n\toffset := RueidisBlock * ServerPoolSize\n\n\tfor i := 0; i < n; i++ {\n\t\tclient, err := rueidislib.NewClient(rueidislib.ClientOption{\n\t\t\tInitAddress: []string{\"127.0.0.1:\" + strconv.Itoa(51200+i+offset)},\n\t\t})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tpools[i] = rueidis.NewPool(rueidiscompat.NewAdapter(client))\n\t}\n\treturn pools\n}\n"
        }
      ]
    }
  ]
}