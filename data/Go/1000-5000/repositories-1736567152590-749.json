{
  "metadata": {
    "timestamp": 1736567152590,
    "page": 749,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "minio/minio-go",
      "stars": 2541,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0546875,
          "content": "*~\n*.test\nvalidator\ngolangci-lint\nfunctional_tests\n.idea"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 0.5087890625,
          "content": "linters-settings:\n  misspell:\n    locale: US\n\nlinters:\n  disable-all: true\n  enable:\n    - typecheck\n    - goimports\n    - misspell\n    - revive\n    - govet\n    - ineffassign\n    - gosimple\n    - unused\n    - gocritic\n\nissues:\n  exclude-use-default: false\n  exclude:\n      # todo fix these when we get enough time.\n      - \"singleCaseSwitch: should rewrite switch statement to if statement\"\n      - \"unlambda: replace\"\n      - \"captLocal:\"\n      - \"ifElseChain:\"\n      - \"elseif:\"\n      - \"should have a package comment\"\n"
        },
        {
          "name": "CNAME",
          "type": "blob",
          "size": 0.0146484375,
          "content": "minio-go.min.io"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.30859375,
          "content": "###  Developer Guidelines\n\n``minio-go`` welcomes your contribution. To make the process as seamless as possible, we ask for the following:\n\n* Go ahead and fork the project and make your changes. We encourage pull requests to discuss code changes.\n    - Fork it\n    - Create your feature branch (git checkout -b my-new-feature)\n    - Commit your changes (git commit -am 'Add some feature')\n    - Push to the branch (git push origin my-new-feature)\n    - Create new Pull Request\n\n* When you're ready to create a pull request, be sure to:\n    - Have test cases for the new code. If you have questions about how to do it, please ask in your pull request.\n    - Run `go fmt`\n    - Squash your commits into a single commit. `git rebase -i`. It's okay to force update your pull request.\n    - Make sure `go test -race ./...` and `go build` completes.\n      NOTE: go test runs functional tests and requires you to have a AWS S3 account. Set them as environment variables\n      ``ACCESS_KEY`` and ``SECRET_KEY``. To run shorter version of the tests please use ``go test -short -race ./...``\n\n* Read [Effective Go](https://github.com/golang/go/wiki/CodeReviewComments) article from Golang project\n    - `minio-go` project is strictly conformant with Golang style\n    - if you happen to observe offending code, please feel free to send a pull request\n"
        },
        {
          "name": "CREDITS",
          "type": "blob",
          "size": 56.515625,
          "content": "Go (the standard library)\nhttps://golang.org/\n----------------------------------------------------------------\nCopyright (c) 2009 The Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngithub.com/davecgh/go-spew\nhttps://github.com/davecgh/go-spew\n----------------------------------------------------------------\nISC License\n\nCopyright (c) 2012-2016 Dave Collins <dave@davec.name>\n\nPermission to use, copy, modify, and/or distribute this software for any\npurpose with or without fee is hereby granted, provided that the above\ncopyright notice and this permission notice appear in all copies.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\nWITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\nANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\nWHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\nACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\nOR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n================================================================\n\ngithub.com/dustin/go-humanize\nhttps://github.com/dustin/go-humanize\n----------------------------------------------------------------\nCopyright (c) 2005-2008  Dustin Sallings <dustin@spy.net>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n<http://www.opensource.org/licenses/mit-license.php>\n\n================================================================\n\ngithub.com/goccy/go-json\nhttps://github.com/goccy/go-json\n----------------------------------------------------------------\nMIT License\n\nCopyright (c) 2020 Masaaki Goshima\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n================================================================\n\ngithub.com/google/uuid\nhttps://github.com/google/uuid\n----------------------------------------------------------------\nCopyright (c) 2009,2014 Google Inc. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngithub.com/klauspost/compress\nhttps://github.com/klauspost/compress\n----------------------------------------------------------------\nCopyright (c) 2012 The Go Authors. All rights reserved.\nCopyright (c) 2019 Klaus Post. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n------------------\n\nFiles: gzhttp/*\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2016-2017 The New York Times Company\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------------\n\nFiles: s2/cmd/internal/readahead/*\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 Klaus Post\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---------------------\nFiles: snappy/*\nFiles: internal/snapref/*\n\nCopyright (c) 2011 The Snappy-Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n-----------------\n\nFiles: s2/cmd/internal/filepathx/*\n\nCopyright 2016 The filepathx Authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n================================================================\n\ngithub.com/klauspost/cpuid/v2\nhttps://github.com/klauspost/cpuid/v2\n----------------------------------------------------------------\nThe MIT License (MIT)\n\nCopyright (c) 2015 Klaus Post\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n================================================================\n\ngithub.com/minio/md5-simd\nhttps://github.com/minio/md5-simd\n----------------------------------------------------------------\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n================================================================\n\ngithub.com/pmezard/go-difflib\nhttps://github.com/pmezard/go-difflib\n----------------------------------------------------------------\nCopyright (c) 2013, Patrick Mezard\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n    Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution.\n    The names of its contributors may not be used to endorse or promote\nproducts derived from this software without specific prior written\npermission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\nTO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngithub.com/rs/xid\nhttps://github.com/rs/xid\n----------------------------------------------------------------\nCopyright (c) 2015 Olivier Poitrey <rs@dailymotion.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is furnished\nto do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n================================================================\n\ngithub.com/stretchr/testify\nhttps://github.com/stretchr/testify\n----------------------------------------------------------------\nMIT License\n\nCopyright (c) 2012-2018 Mat Ryer and Tyler Bunnell\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n================================================================\n\ngolang.org/x/crypto\nhttps://golang.org/x/crypto\n----------------------------------------------------------------\nCopyright (c) 2009 The Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngolang.org/x/net\nhttps://golang.org/x/net\n----------------------------------------------------------------\nCopyright (c) 2009 The Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngolang.org/x/sys\nhttps://golang.org/x/sys\n----------------------------------------------------------------\nCopyright (c) 2009 The Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngolang.org/x/text\nhttps://golang.org/x/text\n----------------------------------------------------------------\nCopyright (c) 2009 The Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n================================================================\n\ngopkg.in/ini.v1\nhttps://gopkg.in/ini.v1\n----------------------------------------------------------------\nApache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and\ndistribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright\nowner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities\nthat control, are controlled by, or are under common control with that entity.\nFor the purposes of this definition, \"control\" means (i) the power, direct or\nindirect, to cause the direction or management of such entity, whether by\ncontract or otherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising\npermissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including\nbut not limited to software source code, documentation source, and configuration\nfiles.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or\ntranslation of a Source form, including but not limited to compiled object code,\ngenerated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made\navailable under the License, as indicated by a copyright notice that is included\nin or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that\nis based on (or derived from) the Work and for which the editorial revisions,\nannotations, elaborations, or other modifications represent, as a whole, an\noriginal work of authorship. For the purposes of this License, Derivative Works\nshall not include works that remain separable from, or merely link (or bind by\nname) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version\nof the Work and any modifications or additions to that Work or Derivative Works\nthereof, that is intentionally submitted to Licensor for inclusion in the Work\nby the copyright owner or by an individual or Legal Entity authorized to submit\non behalf of the copyright owner. For the purposes of this definition,\n\"submitted\" means any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems, and\nissue tracking systems that are managed by, or on behalf of, the Licensor for\nthe purpose of discussing and improving the Work, but excluding communication\nthat is conspicuously marked or otherwise designated in writing by the copyright\nowner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf\nof whom a Contribution has been received by Licensor and subsequently\nincorporated within the Work.\n\n2. Grant of Copyright License.\n\nSubject to the terms and conditions of this License, each Contributor hereby\ngrants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\nirrevocable copyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the Work and such\nDerivative Works in Source or Object form.\n\n3. Grant of Patent License.\n\nSubject to the terms and conditions of this License, each Contributor hereby\ngrants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\nirrevocable (except as stated in this section) patent license to make, have\nmade, use, offer to sell, sell, import, and otherwise transfer the Work, where\nsuch license applies only to those patent claims licensable by such Contributor\nthat are necessarily infringed by their Contribution(s) alone or by combination\nof their Contribution(s) with the Work to which such Contribution(s) was\nsubmitted. If You institute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work or a\nContribution incorporated within the Work constitutes direct or contributory\npatent infringement, then any patent licenses granted to You under this License\nfor that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution.\n\nYou may reproduce and distribute copies of the Work or Derivative Works thereof\nin any medium, with or without modifications, and in Source or Object form,\nprovided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of\nthis License; and\nYou must cause any modified files to carry prominent notices stating that You\nchanged the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute,\nall copyright, patent, trademark, and attribution notices from the Source form\nof the Work, excluding those notices that do not pertain to any part of the\nDerivative Works; and\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any\nDerivative Works that You distribute must include a readable copy of the\nattribution notices contained within such NOTICE file, excluding those notices\nthat do not pertain to any part of the Derivative Works, in at least one of the\nfollowing places: within a NOTICE text file distributed as part of the\nDerivative Works; within the Source form or documentation, if provided along\nwith the Derivative Works; or, within a display generated by the Derivative\nWorks, if and wherever such third-party notices normally appear. The contents of\nthe NOTICE file are for informational purposes only and do not modify the\nLicense. You may add Your own attribution notices within Derivative Works that\nYou distribute, alongside or as an addendum to the NOTICE text from the Work,\nprovided that such additional attribution notices cannot be construed as\nmodifying the License.\nYou may add Your own copyright statement to Your modifications and may provide\nadditional or different license terms and conditions for use, reproduction, or\ndistribution of Your modifications, or for any such Derivative Works as a whole,\nprovided Your use, reproduction, and distribution of the Work otherwise complies\nwith the conditions stated in this License.\n\n5. Submission of Contributions.\n\nUnless You explicitly state otherwise, any Contribution intentionally submitted\nfor inclusion in the Work by You to the Licensor shall be under the terms and\nconditions of this License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify the terms of\nany separate license agreement you may have executed with Licensor regarding\nsuch Contributions.\n\n6. Trademarks.\n\nThis License does not grant permission to use the trade names, trademarks,\nservice marks, or product names of the Licensor, except as required for\nreasonable and customary use in describing the origin of the Work and\nreproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty.\n\nUnless required by applicable law or agreed to in writing, Licensor provides the\nWork (and each Contributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied,\nincluding, without limitation, any warranties or conditions of TITLE,\nNON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are\nsolely responsible for determining the appropriateness of using or\nredistributing the Work and assume any risks associated with Your exercise of\npermissions under this License.\n\n8. Limitation of Liability.\n\nIn no event and under no legal theory, whether in tort (including negligence),\ncontract, or otherwise, unless required by applicable law (such as deliberate\nand grossly negligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special, incidental,\nor consequential damages of any character arising as a result of this License or\nout of the use or inability to use the Work (including but not limited to\ndamages for loss of goodwill, work stoppage, computer failure or malfunction, or\nany and all other commercial damages or losses), even if such Contributor has\nbeen advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability.\n\nWhile redistributing the Work or Derivative Works thereof, You may choose to\noffer, and charge a fee for, acceptance of support, warranty, indemnity, or\nother liability obligations and/or rights consistent with this License. However,\nin accepting such obligations, You may act only on Your own behalf and on Your\nsole responsibility, not on behalf of any other Contributor, and only if You\nagree to indemnify, defend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason of your\naccepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work\n\nTo apply the Apache License to your work, attach the following boilerplate\nnotice, with the fields enclosed by brackets \"[]\" replaced with your own\nidentifying information. (Don't include the brackets!) The text should be\nenclosed in the appropriate comment syntax for the file format. We also\nrecommend that a file or class name and description of purpose be included on\nthe same \"printed page\" as the copyright notice for easier identification within\nthird-party archives.\n\n   Copyright 2014 Unknwon\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n================================================================\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 1.1669921875,
          "content": "# For maintainers only\n\n## Responsibilities\n\nPlease go through this link [Maintainer Responsibility](https://gist.github.com/abperiasamy/f4d9b31d3186bbd26522)\n\n### Making new releases\nTag and sign your release commit, additionally this step requires you to have access to MinIO's trusted private key.\n```sh\n$ export GNUPGHOME=/media/${USER}/minio/trusted\n$ git tag -s 4.0.0\n$ git push\n$ git push --tags\n```\n\n### Update version\nOnce release has been made update `libraryVersion` constant in `api.go` to next to be released version.\n\n```sh\n$ grep libraryVersion api.go\n      libraryVersion = \"4.0.1\"\n```\n\nCommit your changes\n```\n$ git commit -a -m \"Update version for next release\" --author \"MinIO Trusted <trusted@min.io>\"\n```\n\n### Announce\nAnnounce new release by adding release notes at https://github.com/minio/minio-go/releases from `trusted@min.io` account. Release notes requires two sections `highlights` and `changelog`. Highlights is a bulleted list of salient features in this release and Changelog contains list of all commits since the last release.\n\nTo generate `changelog`\n```sh\n$ git log --no-color --pretty=format:'-%d %s (%cr) <%an>' <last_release_tag>..<latest_release_tag>\n```\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.732421875,
          "content": "GOPATH := $(shell go env GOPATH)\nTMPDIR := $(shell mktemp -d)\n\nall: checks\n\n.PHONY: examples docs\n\nchecks: lint vet test examples functional-test\n\nlint:\n\t@mkdir -p ${GOPATH}/bin\n\t@echo \"Installing golangci-lint\" && curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(GOPATH)/bin\n\t@echo \"Running $@ check\"\n\t@GO111MODULE=on ${GOPATH}/bin/golangci-lint cache clean\n\t@GO111MODULE=on ${GOPATH}/bin/golangci-lint run --timeout=5m --config ./.golangci.yml\n\nvet:\n\t@GO111MODULE=on go vet ./...\n\t@echo \"Installing staticcheck\" && go install honnef.co/go/tools/cmd/staticcheck@latest\n\t${GOPATH}/bin/staticcheck -tests=false -checks=\"all,-ST1000,-ST1003,-ST1016,-ST1020,-ST1021,-ST1022,-ST1023,-ST1005\"\n\ntest:\n\t@GO111MODULE=on SERVER_ENDPOINT=localhost:9000 ACCESS_KEY=minioadmin SECRET_KEY=minioadmin ENABLE_HTTPS=1 MINT_MODE=full go test -race -v ./...\n\nexamples:\n\t@echo \"Building s3 examples\"\n\t@cd ./examples/s3 && $(foreach v,$(wildcard examples/s3/*.go),go build -mod=mod -o ${TMPDIR}/$(basename $(v)) $(notdir $(v)) || exit 1;)\n\t@echo \"Building minio examples\"\n\t@cd ./examples/minio && $(foreach v,$(wildcard examples/minio/*.go),go build -mod=mod -o ${TMPDIR}/$(basename $(v)) $(notdir $(v)) || exit 1;)\n\nfunctional-test:\n\t@GO111MODULE=on go build -race functional_tests.go\n\t@SERVER_ENDPOINT=localhost:9000 ACCESS_KEY=minioadmin SECRET_KEY=minioadmin ENABLE_HTTPS=1 MINT_MODE=full ./functional_tests\n\nfunctional-test-notls:\n\t@GO111MODULE=on go build -race functional_tests.go\n\t@SERVER_ENDPOINT=localhost:9000 ACCESS_KEY=minioadmin SECRET_KEY=minioadmin ENABLE_HTTPS=0 MINT_MODE=full ./functional_tests\n\nclean:\n\t@echo \"Cleaning up all the generated files\"\n\t@find . -name '*.test' | xargs rm -fv\n\t@find . -name '*~' | xargs rm -fv\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.3525390625,
          "content": "MinIO Cloud Storage, (C) 2014-2020 MinIO, Inc.\n\nThis product includes software developed at MinIO, Inc.\n(https://min.io/).\n\nThe MinIO project contains unmodified/modified subcomponents too with\nseparate copyright notices and license terms. Your use of the source\ncode for these subcomponents is subject to the terms and conditions\nof Apache License Version 2.0\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.6435546875,
          "content": "# MinIO Go Client SDK for Amazon S3 Compatible Cloud Storage [![Slack](https://slack.min.io/slack?type=svg)](https://slack.min.io) [![Sourcegraph](https://sourcegraph.com/github.com/minio/minio-go/-/badge.svg)](https://sourcegraph.com/github.com/minio/minio-go?badge) [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/minio/minio-go/blob/master/LICENSE)\n\nThe MinIO Go Client SDK provides straightforward APIs to access any Amazon S3 compatible object storage.\n\nThis Quickstart Guide covers how to install the MinIO client SDK, connect to MinIO, and create a sample file uploader.\nFor a complete list of APIs and examples, see the [godoc documentation](https://pkg.go.dev/github.com/minio/minio-go/v7) or [Go Client API Reference](https://min.io/docs/minio/linux/developers/go/API.html).\n\nThese examples presume a working [Go development environment](https://golang.org/doc/install) and the [MinIO `mc` command line tool](https://min.io/docs/minio/linux/reference/minio-mc.html).\n\n## Download from Github\n\nFrom your project directory:\n\n```sh\ngo get github.com/minio/minio-go/v7\n```\n\n## Initialize a MinIO Client Object\n\nThe MinIO client requires the following parameters to connect to an Amazon S3 compatible object storage:\n\n| Parameter         | Description                                                |\n| ----------------- | ---------------------------------------------------------- |\n| `endpoint`        | URL to object storage service.                             |\n| `_minio.Options_` | All the options such as credentials, custom transport etc. |\n\n```go\npackage main\n\nimport (\n\t\"log\"\n\n\t\"github.com/minio/minio-go/v7\"\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n)\n\nfunc main() {\n\tendpoint := \"play.min.io\"\n\taccessKeyID := \"Q3AM3UQ867SPQQA43P2F\"\n\tsecretAccessKey := \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\"\n\tuseSSL := true\n\n\t// Initialize minio client object.\n\tminioClient, err := minio.New(endpoint, &minio.Options{\n\t\tCreds:  credentials.NewStaticV4(accessKeyID, secretAccessKey, \"\"),\n\t\tSecure: useSSL,\n\t})\n\tif err != nil {\n\t\tlog.Fatalln(err)\n\t}\n\n\tlog.Printf(\"%#v\\n\", minioClient) // minioClient is now set up\n}\n```\n\n## Example - File Uploader\n\nThis sample code connects to an object storage server, creates a bucket, and uploads a file to the bucket.\nIt uses the MinIO `play` server, a public MinIO cluster located at [https://play.min.io](https://play.min.io).\n\nThe `play` server runs the latest stable version of MinIO and may be used for testing and development.\nThe access credentials shown in this example are open to the public and all data uploaded to `play` should be considered public and non-protected.\n\n### FileUploader.go\n\nThis example does the following:\n\n- Connects to the MinIO `play` server using the provided credentials.\n- Creates a bucket named `testbucket`.\n- Uploads a file named `testdata` from `/tmp`.\n- Verifies the file was created using `mc ls`.\n\n```go\n// FileUploader.go MinIO example\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\n\t\"github.com/minio/minio-go/v7\"\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n)\n\nfunc main() {\n\tctx := context.Background()\n\tendpoint := \"play.min.io\"\n\taccessKeyID := \"Q3AM3UQ867SPQQA43P2F\"\n\tsecretAccessKey := \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\"\n\tuseSSL := true\n\n\t// Initialize minio client object.\n\tminioClient, err := minio.New(endpoint, &minio.Options{\n\t\tCreds:  credentials.NewStaticV4(accessKeyID, secretAccessKey, \"\"),\n\t\tSecure: useSSL,\n\t})\n\tif err != nil {\n\t\tlog.Fatalln(err)\n\t}\n\n\t// Make a new bucket called testbucket.\n\tbucketName := \"testbucket\"\n\tlocation := \"us-east-1\"\n\n\terr = minioClient.MakeBucket(ctx, bucketName, minio.MakeBucketOptions{Region: location})\n\tif err != nil {\n\t\t// Check to see if we already own this bucket (which happens if you run this twice)\n\t\texists, errBucketExists := minioClient.BucketExists(ctx, bucketName)\n\t\tif errBucketExists == nil && exists {\n\t\t\tlog.Printf(\"We already own %s\\n\", bucketName)\n\t\t} else {\n\t\t\tlog.Fatalln(err)\n\t\t}\n\t} else {\n\t\tlog.Printf(\"Successfully created %s\\n\", bucketName)\n\t}\n\n\t// Upload the test file\n\t// Change the value of filePath if the file is in another location\n\tobjectName := \"testdata\"\n\tfilePath := \"/tmp/testdata\"\n\tcontentType := \"application/octet-stream\"\n\n\t// Upload the test file with FPutObject\n\tinfo, err := minioClient.FPutObject(ctx, bucketName, objectName, filePath, minio.PutObjectOptions{ContentType: contentType})\n\tif err != nil {\n\t\tlog.Fatalln(err)\n\t}\n\n\tlog.Printf(\"Successfully uploaded %s of size %d\\n\", objectName, info.Size)\n}\n```\n\n**1. Create a test file containing data:**\n\nYou can do this with `dd` on Linux or macOS systems:\n\n```sh\ndd if=/dev/urandom of=/tmp/testdata bs=2048 count=10\n```\n\nor `fsutil` on Windows:\n\n```sh\nfsutil file createnew \"C:\\Users\\<username>\\Desktop\\sample.txt\" 20480\n```\n\n**2. Run FileUploader with the following commands:**\n\n```sh\ngo mod init example/FileUploader\ngo get github.com/minio/minio-go/v7\ngo get github.com/minio/minio-go/v7/pkg/credentials\ngo run FileUploader.go\n```\n\nThe output resembles the following:\n\n```sh\n2023/11/01 14:27:55 Successfully created testbucket\n2023/11/01 14:27:55 Successfully uploaded testdata of size 20480\n```\n\n**3. Verify the Uploaded File With `mc ls`:**\n\n```sh\nmc ls play/testbucket\n[2023-11-01 14:27:55 UTC]  20KiB STANDARD TestDataFile\n```\n\n## API Reference\n\nThe full API Reference is available here.\n\n* [Complete API Reference](https://min.io/docs/minio/linux/developers/go/API.html)\n\n### API Reference : Bucket Operations\n\n* [`MakeBucket`](https://min.io/docs/minio/linux/developers/go/API.html#MakeBucket)\n* [`ListBuckets`](https://min.io/docs/minio/linux/developers/go/API.html#ListBuckets)\n* [`BucketExists`](https://min.io/docs/minio/linux/developers/go/API.html#BucketExists)\n* [`RemoveBucket`](https://min.io/docs/minio/linux/developers/go/API.html#RemoveBucket)\n* [`ListObjects`](https://min.io/docs/minio/linux/developers/go/API.html#ListObjects)\n* [`ListIncompleteUploads`](https://min.io/docs/minio/linux/developers/go/API.html#ListIncompleteUploads)\n\n### API Reference : Bucket policy Operations\n\n* [`SetBucketPolicy`](https://min.io/docs/minio/linux/developers/go/API.html#SetBucketPolicy)\n* [`GetBucketPolicy`](https://min.io/docs/minio/linux/developers/go/API.html#GetBucketPolicy)\n\n### API Reference : Bucket notification Operations\n\n* [`SetBucketNotification`](https://min.io/docs/minio/linux/developers/go/API.html#SetBucketNotification)\n* [`GetBucketNotification`](https://min.io/docs/minio/linux/developers/go/API.html#GetBucketNotification)\n* [`RemoveAllBucketNotification`](https://min.io/docs/minio/linux/developers/go/API.html#RemoveAllBucketNotification)\n* [`ListenBucketNotification`](https://min.io/docs/minio/linux/developers/go/API.html#ListenBucketNotification) (MinIO Extension)\n* [`ListenNotification`](https://min.io/docs/minio/linux/developers/go/API.html#ListenNotification) (MinIO Extension)\n\n### API Reference : File Object Operations\n\n* [`FPutObject`](https://min.io/docs/minio/linux/developers/go/API.html#FPutObject)\n* [`FGetObject`](https://min.io/docs/minio/linux/developers/go/API.html#FGetObject)\n\n### API Reference : Object Operations\n\n* [`GetObject`](https://min.io/docs/minio/linux/developers/go/API.html#GetObject)\n* [`PutObject`](https://min.io/docs/minio/linux/developers/go/API.html#PutObject)\n* [`PutObjectStreaming`](https://min.io/docs/minio/linux/developers/go/API.html#PutObjectStreaming)\n* [`StatObject`](https://min.io/docs/minio/linux/developers/go/API.html#StatObject)\n* [`CopyObject`](https://min.io/docs/minio/linux/developers/go/API.html#CopyObject)\n* [`RemoveObject`](https://min.io/docs/minio/linux/developers/go/API.html#RemoveObject)\n* [`RemoveObjects`](https://min.io/docs/minio/linux/developers/go/API.html#RemoveObjects)\n* [`RemoveIncompleteUpload`](https://min.io/docs/minio/linux/developers/go/API.html#RemoveIncompleteUpload)\n* [`SelectObjectContent`](https://min.io/docs/minio/linux/developers/go/API.html#SelectObjectContent)\n\n### API Reference : Presigned Operations\n\n* [`PresignedGetObject`](https://min.io/docs/minio/linux/developers/go/API.html#PresignedGetObject)\n* [`PresignedPutObject`](https://min.io/docs/minio/linux/developers/go/API.html#PresignedPutObject)\n* [`PresignedHeadObject`](https://min.io/docs/minio/linux/developers/go/API.html#PresignedHeadObject)\n* [`PresignedPostPolicy`](https://min.io/docs/minio/linux/developers/go/API.html#PresignedPostPolicy)\n\n### API Reference : Client custom settings\n\n* [`SetAppInfo`](https://min.io/docs/minio/linux/developers/go/API.html#SetAppInfo)\n* [`TraceOn`](https://min.io/docs/minio/linux/developers/go/API.html#TraceOn)\n* [`TraceOff`](https://min.io/docs/minio/linux/developers/go/API.html#TraceOff)\n\n## Full Examples\n\n### Full Examples : Bucket Operations\n\n* [makebucket.go](https://github.com/minio/minio-go/blob/master/examples/s3/makebucket.go)\n* [listbuckets.go](https://github.com/minio/minio-go/blob/master/examples/s3/listbuckets.go)\n* [bucketexists.go](https://github.com/minio/minio-go/blob/master/examples/s3/bucketexists.go)\n* [removebucket.go](https://github.com/minio/minio-go/blob/master/examples/s3/removebucket.go)\n* [listobjects.go](https://github.com/minio/minio-go/blob/master/examples/s3/listobjects.go)\n* [listobjectsV2.go](https://github.com/minio/minio-go/blob/master/examples/s3/listobjectsV2.go)\n* [listincompleteuploads.go](https://github.com/minio/minio-go/blob/master/examples/s3/listincompleteuploads.go)\n\n### Full Examples : Bucket policy Operations\n\n* [setbucketpolicy.go](https://github.com/minio/minio-go/blob/master/examples/s3/setbucketpolicy.go)\n* [getbucketpolicy.go](https://github.com/minio/minio-go/blob/master/examples/s3/getbucketpolicy.go)\n* [listbucketpolicies.go](https://github.com/minio/minio-go/blob/master/examples/s3/listbucketpolicies.go)\n\n### Full Examples : Bucket lifecycle Operations\n\n* [setbucketlifecycle.go](https://github.com/minio/minio-go/blob/master/examples/s3/setbucketlifecycle.go)\n* [getbucketlifecycle.go](https://github.com/minio/minio-go/blob/master/examples/s3/getbucketlifecycle.go)\n\n### Full Examples : Bucket encryption Operations\n\n* [setbucketencryption.go](https://github.com/minio/minio-go/blob/master/examples/s3/setbucketencryption.go)\n* [getbucketencryption.go](https://github.com/minio/minio-go/blob/master/examples/s3/getbucketencryption.go)\n* [removebucketencryption.go](https://github.com/minio/minio-go/blob/master/examples/s3/removebucketencryption.go)\n\n### Full Examples : Bucket replication Operations\n\n* [setbucketreplication.go](https://github.com/minio/minio-go/blob/master/examples/s3/setbucketreplication.go)\n* [getbucketreplication.go](https://github.com/minio/minio-go/blob/master/examples/s3/getbucketreplication.go)\n* [removebucketreplication.go](https://github.com/minio/minio-go/blob/master/examples/s3/removebucketreplication.go)\n\n### Full Examples : Bucket notification Operations\n\n* [setbucketnotification.go](https://github.com/minio/minio-go/blob/master/examples/s3/setbucketnotification.go)\n* [getbucketnotification.go](https://github.com/minio/minio-go/blob/master/examples/s3/getbucketnotification.go)\n* [removeallbucketnotification.go](https://github.com/minio/minio-go/blob/master/examples/s3/removeallbucketnotification.go)\n* [listenbucketnotification.go](https://github.com/minio/minio-go/blob/master/examples/minio/listenbucketnotification.go) (MinIO Extension)\n* [listennotification.go](https://github.com/minio/minio-go/blob/master/examples/minio/listen-notification.go) (MinIO Extension)\n\n### Full Examples : File Object Operations\n\n* [fputobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/fputobject.go)\n* [fgetobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/fgetobject.go)\n\n### Full Examples : Object Operations\n\n* [putobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/putobject.go)\n* [getobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/getobject.go)\n* [statobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/statobject.go)\n* [copyobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/copyobject.go)\n* [removeobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/removeobject.go)\n* [removeincompleteupload.go](https://github.com/minio/minio-go/blob/master/examples/s3/removeincompleteupload.go)\n* [removeobjects.go](https://github.com/minio/minio-go/blob/master/examples/s3/removeobjects.go)\n\n### Full Examples : Encrypted Object Operations\n\n* [put-encrypted-object.go](https://github.com/minio/minio-go/blob/master/examples/s3/put-encrypted-object.go)\n* [get-encrypted-object.go](https://github.com/minio/minio-go/blob/master/examples/s3/get-encrypted-object.go)\n* [fput-encrypted-object.go](https://github.com/minio/minio-go/blob/master/examples/s3/fputencrypted-object.go)\n\n### Full Examples : Presigned Operations\n\n* [presignedgetobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/presignedgetobject.go)\n* [presignedputobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/presignedputobject.go)\n* [presignedheadobject.go](https://github.com/minio/minio-go/blob/master/examples/s3/presignedheadobject.go)\n* [presignedpostpolicy.go](https://github.com/minio/minio-go/blob/master/examples/s3/presignedpostpolicy.go)\n\n## Explore Further\n\n* [Godoc Documentation](https://pkg.go.dev/github.com/minio/minio-go/v7)\n* [Complete Documentation](https://min.io/docs/minio/kubernetes/upstream/index.html)\n* [MinIO Go Client SDK API Reference](https://min.io/docs/minio/linux/developers/go/API.html)\n\n## Contribute\n\n[Contributors Guide](https://github.com/minio/minio-go/blob/master/CONTRIBUTING.md)\n\n## License\n\nThis SDK is distributed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0), see [LICENSE](https://github.com/minio/minio-go/blob/master/LICENSE) and [NOTICE](https://github.com/minio/minio-go/blob/master/NOTICE) for more information.\n"
        },
        {
          "name": "api-bucket-cors.go",
          "type": "blob",
          "size": 3.5390625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2024 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/cors\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// SetBucketCors sets the cors configuration for the bucket\nfunc (c *Client) SetBucketCors(ctx context.Context, bucketName string, corsConfig *cors.Config) error {\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tif corsConfig == nil {\n\t\treturn c.removeBucketCors(ctx, bucketName)\n\t}\n\n\treturn c.putBucketCors(ctx, bucketName, corsConfig)\n}\n\nfunc (c *Client) putBucketCors(ctx context.Context, bucketName string, corsConfig *cors.Config) error {\n\turlValues := make(url.Values)\n\turlValues.Set(\"cors\", \"\")\n\n\tcorsStr, err := corsConfig.ToXML()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(corsStr),\n\t\tcontentLength:    int64(len(corsStr)),\n\t\tcontentMD5Base64: sumMD5Base64([]byte(corsStr)),\n\t}\n\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (c *Client) removeBucketCors(ctx context.Context, bucketName string) error {\n\turlValues := make(url.Values)\n\turlValues.Set(\"cors\", \"\")\n\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp.StatusCode != http.StatusNoContent {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\treturn nil\n}\n\n// GetBucketCors returns the current cors\nfunc (c *Client) GetBucketCors(ctx context.Context, bucketName string) (*cors.Config, error) {\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\tbucketCors, err := c.getBucketCors(ctx, bucketName)\n\tif err != nil {\n\t\terrResponse := ToErrorResponse(err)\n\t\tif errResponse.Code == \"NoSuchCORSConfiguration\" {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\treturn bucketCors, nil\n}\n\nfunc (c *Client) getBucketCors(ctx context.Context, bucketName string) (*cors.Config, error) {\n\turlValues := make(url.Values)\n\turlValues.Set(\"cors\", \"\")\n\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex, // TODO: needed? copied over from other example, but not spec'd in API.\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\tcorsConfig, err := cors.ParseBucketCorsConfig(resp.Body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn corsConfig, nil\n}\n"
        },
        {
          "name": "api-bucket-encryption.go",
          "type": "blob",
          "size": 3.90625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/sse\"\n)\n\n// SetBucketEncryption sets the default encryption configuration on an existing bucket.\nfunc (c *Client) SetBucketEncryption(ctx context.Context, bucketName string, config *sse.Configuration) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tif config == nil {\n\t\treturn errInvalidArgument(\"configuration cannot be empty\")\n\t}\n\n\tbuf, err := xml.Marshal(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"encryption\", \"\")\n\n\t// Content-length is mandatory to set a default encryption configuration\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(buf),\n\t\tcontentLength:    int64(len(buf)),\n\t\tcontentMD5Base64: sumMD5Base64(buf),\n\t}\n\n\t// Execute PUT to upload a new bucket default encryption configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n\n// RemoveBucketEncryption removes the default encryption configuration on a bucket with a context to control cancellations and timeouts.\nfunc (c *Client) RemoveBucketEncryption(ctx context.Context, bucketName string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"encryption\", \"\")\n\n\t// DELETE default encryption configuration on a bucket.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n\n// GetBucketEncryption gets the default encryption configuration\n// on an existing bucket with a context to control cancellations and timeouts.\nfunc (c *Client) GetBucketEncryption(ctx context.Context, bucketName string) (*sse.Configuration, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"encryption\", \"\")\n\n\t// Execute GET on bucket to get the default encryption configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tencryptionConfig := &sse.Configuration{}\n\tif err = xmlDecoder(resp.Body, encryptionConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn encryptionConfig, nil\n}\n"
        },
        {
          "name": "api-bucket-lifecycle.go",
          "type": "blob",
          "size": 4.7607421875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/lifecycle\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// SetBucketLifecycle set the lifecycle on an existing bucket.\nfunc (c *Client) SetBucketLifecycle(ctx context.Context, bucketName string, config *lifecycle.Configuration) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// If lifecycle is empty then delete it.\n\tif config.Empty() {\n\t\treturn c.removeBucketLifecycle(ctx, bucketName)\n\t}\n\n\tbuf, err := xml.Marshal(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Save the updated lifecycle.\n\treturn c.putBucketLifecycle(ctx, bucketName, buf)\n}\n\n// Saves a new bucket lifecycle.\nfunc (c *Client) putBucketLifecycle(ctx context.Context, bucketName string, buf []byte) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"lifecycle\", \"\")\n\n\t// Content-length is mandatory for put lifecycle request\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(buf),\n\t\tcontentLength:    int64(len(buf)),\n\t\tcontentMD5Base64: sumMD5Base64(buf),\n\t}\n\n\t// Execute PUT to upload a new bucket lifecycle.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Remove lifecycle from a bucket.\nfunc (c *Client) removeBucketLifecycle(ctx context.Context, bucketName string) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"lifecycle\", \"\")\n\n\t// Execute DELETE on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// GetBucketLifecycle fetch bucket lifecycle configuration\nfunc (c *Client) GetBucketLifecycle(ctx context.Context, bucketName string) (*lifecycle.Configuration, error) {\n\tlc, _, err := c.GetBucketLifecycleWithInfo(ctx, bucketName)\n\treturn lc, err\n}\n\n// GetBucketLifecycleWithInfo fetch bucket lifecycle configuration along with when it was last updated\nfunc (c *Client) GetBucketLifecycleWithInfo(ctx context.Context, bucketName string) (*lifecycle.Configuration, time.Time, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tbucketLifecycle, updatedAt, err := c.getBucketLifecycle(ctx, bucketName)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tconfig := lifecycle.NewConfiguration()\n\tif err = xml.Unmarshal(bucketLifecycle, config); err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\treturn config, updatedAt, nil\n}\n\n// Request server for current bucket lifecycle.\nfunc (c *Client) getBucketLifecycle(ctx context.Context, bucketName string) ([]byte, time.Time, error) {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"lifecycle\", \"\")\n\turlValues.Set(\"withUpdatedAt\", \"true\")\n\n\t// Execute GET on bucket to get lifecycle.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, time.Time{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\tlcBytes, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tconst minIOLifecycleCfgUpdatedAt = \"X-Minio-LifecycleConfig-UpdatedAt\"\n\tvar updatedAt time.Time\n\tif timeStr := resp.Header.Get(minIOLifecycleCfgUpdatedAt); timeStr != \"\" {\n\t\tupdatedAt, err = time.Parse(iso8601DateFormat, timeStr)\n\t\tif err != nil {\n\t\t\treturn nil, time.Time{}, err\n\t\t}\n\t}\n\n\treturn lcBytes, updatedAt, nil\n}\n"
        },
        {
          "name": "api-bucket-notification.go",
          "type": "blob",
          "size": 7.8212890625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/goccy/go-json\"\n\t\"github.com/minio/minio-go/v7/pkg/notification\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// SetBucketNotification saves a new bucket notification with a context to control cancellations and timeouts.\nfunc (c *Client) SetBucketNotification(ctx context.Context, bucketName string, config notification.Configuration) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"notification\", \"\")\n\n\tnotifBytes, err := xml.Marshal(&config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tnotifBuffer := bytes.NewReader(notifBytes)\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      notifBuffer,\n\t\tcontentLength:    int64(len(notifBytes)),\n\t\tcontentMD5Base64: sumMD5Base64(notifBytes),\n\t\tcontentSHA256Hex: sum256Hex(notifBytes),\n\t}\n\n\t// Execute PUT to upload a new bucket notification.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// RemoveAllBucketNotification - Remove bucket notification clears all previously specified config\nfunc (c *Client) RemoveAllBucketNotification(ctx context.Context, bucketName string) error {\n\treturn c.SetBucketNotification(ctx, bucketName, notification.Configuration{})\n}\n\n// GetBucketNotification returns current bucket notification configuration\nfunc (c *Client) GetBucketNotification(ctx context.Context, bucketName string) (bucketNotification notification.Configuration, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn notification.Configuration{}, err\n\t}\n\treturn c.getBucketNotification(ctx, bucketName)\n}\n\n// Request server for notification rules.\nfunc (c *Client) getBucketNotification(ctx context.Context, bucketName string) (notification.Configuration, error) {\n\turlValues := make(url.Values)\n\turlValues.Set(\"notification\", \"\")\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn notification.Configuration{}, err\n\t}\n\treturn processBucketNotificationResponse(bucketName, resp)\n}\n\n// processes the GetNotification http response from the server.\nfunc processBucketNotificationResponse(bucketName string, resp *http.Response) (notification.Configuration, error) {\n\tif resp.StatusCode != http.StatusOK {\n\t\terrResponse := httpRespToErrorResponse(resp, bucketName, \"\")\n\t\treturn notification.Configuration{}, errResponse\n\t}\n\tvar bucketNotification notification.Configuration\n\terr := xmlDecoder(resp.Body, &bucketNotification)\n\tif err != nil {\n\t\treturn notification.Configuration{}, err\n\t}\n\treturn bucketNotification, nil\n}\n\n// ListenNotification listen for all events, this is a MinIO specific API\nfunc (c *Client) ListenNotification(ctx context.Context, prefix, suffix string, events []string) <-chan notification.Info {\n\treturn c.ListenBucketNotification(ctx, \"\", prefix, suffix, events)\n}\n\n// ListenBucketNotification listen for bucket events, this is a MinIO specific API\nfunc (c *Client) ListenBucketNotification(ctx context.Context, bucketName, prefix, suffix string, events []string) <-chan notification.Info {\n\tnotificationInfoCh := make(chan notification.Info, 1)\n\tconst notificationCapacity = 4 * 1024 * 1024\n\tnotificationEventBuffer := make([]byte, notificationCapacity)\n\t// Only success, start a routine to start reading line by line.\n\tgo func(notificationInfoCh chan<- notification.Info) {\n\t\tdefer close(notificationInfoCh)\n\n\t\t// Validate the bucket name.\n\t\tif bucketName != \"\" {\n\t\t\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\t\tErr: err,\n\t\t\t\t}:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Check ARN partition to verify if listening bucket is supported\n\t\tif s3utils.IsAmazonEndpoint(*c.endpointURL) || s3utils.IsGoogleEndpoint(*c.endpointURL) {\n\t\t\tselect {\n\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\tErr: errAPINotSupported(\"Listening for bucket notification is specific only to `minio` server endpoints\"),\n\t\t\t}:\n\t\t\tcase <-ctx.Done():\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Continuously run and listen on bucket notification.\n\t\t// Create a done channel to control 'ListObjects' go routine.\n\t\tretryDoneCh := make(chan struct{}, 1)\n\n\t\t// Indicate to our routine to exit cleanly upon return.\n\t\tdefer close(retryDoneCh)\n\n\t\t// Prepare urlValues to pass into the request on every loop\n\t\turlValues := make(url.Values)\n\t\turlValues.Set(\"ping\", \"10\")\n\t\turlValues.Set(\"prefix\", prefix)\n\t\turlValues.Set(\"suffix\", suffix)\n\t\turlValues[\"events\"] = events\n\n\t\t// Wait on the jitter retry loop.\n\t\tfor range c.newRetryTimerContinous(time.Second, time.Second*30, MaxJitter, retryDoneCh) {\n\t\t\t// Execute GET on bucket to list objects.\n\t\t\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\t\t\tbucketName:       bucketName,\n\t\t\t\tqueryValues:      urlValues,\n\t\t\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\t\tErr: err,\n\t\t\t\t}:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Validate http response, upon error return quickly.\n\t\t\tif resp.StatusCode != http.StatusOK {\n\t\t\t\terrResponse := httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t\t\tselect {\n\t\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\t\tErr: errResponse,\n\t\t\t\t}:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Initialize a new bufio scanner, to read line by line.\n\t\t\tbio := bufio.NewScanner(resp.Body)\n\n\t\t\t// Use a higher buffer to support unexpected\n\t\t\t// caching done by proxies\n\t\t\tbio.Buffer(notificationEventBuffer, notificationCapacity)\n\n\t\t\t// Unmarshal each line, returns marshaled values.\n\t\t\tfor bio.Scan() {\n\t\t\t\tvar notificationInfo notification.Info\n\t\t\t\tif err = json.Unmarshal(bio.Bytes(), &notificationInfo); err != nil {\n\t\t\t\t\t// Unexpected error during json unmarshal, send\n\t\t\t\t\t// the error to caller for actionable as needed.\n\t\t\t\t\tselect {\n\t\t\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\t\t\tErr: err,\n\t\t\t\t\t}:\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tcloseResponse(resp)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Empty events pinged from the server\n\t\t\t\tif len(notificationInfo.Records) == 0 && notificationInfo.Err == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Send notificationInfo\n\t\t\t\tselect {\n\t\t\t\tcase notificationInfoCh <- notificationInfo:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcloseResponse(resp)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif err = bio.Err(); err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase notificationInfoCh <- notification.Info{\n\t\t\t\t\tErr: err,\n\t\t\t\t}:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Close current connection before looping further.\n\t\t\tcloseResponse(resp)\n\n\t\t}\n\t}(notificationInfoCh)\n\n\t// Returns the notification info channel, for caller to start reading from.\n\treturn notificationInfoCh\n}\n"
        },
        {
          "name": "api-bucket-policy.go",
          "type": "blob",
          "size": 3.912109375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strings\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// SetBucketPolicy sets the access permissions on an existing bucket.\nfunc (c *Client) SetBucketPolicy(ctx context.Context, bucketName, policy string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// If policy is empty then delete the bucket policy.\n\tif policy == \"\" {\n\t\treturn c.removeBucketPolicy(ctx, bucketName)\n\t}\n\n\t// Save the updated policies.\n\treturn c.putBucketPolicy(ctx, bucketName, policy)\n}\n\n// Saves a new bucket policy.\nfunc (c *Client) putBucketPolicy(ctx context.Context, bucketName, policy string) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"policy\", \"\")\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:    bucketName,\n\t\tqueryValues:   urlValues,\n\t\tcontentBody:   strings.NewReader(policy),\n\t\tcontentLength: int64(len(policy)),\n\t}\n\n\t// Execute PUT to upload a new bucket policy.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusNoContent && resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Removes all policies on a bucket.\nfunc (c *Client) removeBucketPolicy(ctx context.Context, bucketName string) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"policy\", \"\")\n\n\t// Execute DELETE on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp.StatusCode != http.StatusNoContent {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\treturn nil\n}\n\n// GetBucketPolicy returns the current policy\nfunc (c *Client) GetBucketPolicy(ctx context.Context, bucketName string) (string, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn \"\", err\n\t}\n\tbucketPolicy, err := c.getBucketPolicy(ctx, bucketName)\n\tif err != nil {\n\t\terrResponse := ToErrorResponse(err)\n\t\tif errResponse.Code == \"NoSuchBucketPolicy\" {\n\t\t\treturn \"\", nil\n\t\t}\n\t\treturn \"\", err\n\t}\n\treturn bucketPolicy, nil\n}\n\n// Request server for current bucket policy.\nfunc (c *Client) getBucketPolicy(ctx context.Context, bucketName string) (string, error) {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"policy\", \"\")\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn \"\", httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\tbucketPolicyBuf, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tpolicy := string(bucketPolicyBuf)\n\treturn policy, err\n}\n"
        },
        {
          "name": "api-bucket-replication.go",
          "type": "blob",
          "size": 10.3076171875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/minio/minio-go/v7/pkg/replication\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// RemoveBucketReplication removes a replication config on an existing bucket.\nfunc (c *Client) RemoveBucketReplication(ctx context.Context, bucketName string) error {\n\treturn c.removeBucketReplication(ctx, bucketName)\n}\n\n// SetBucketReplication sets a replication config on an existing bucket.\nfunc (c *Client) SetBucketReplication(ctx context.Context, bucketName string, cfg replication.Config) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// If replication is empty then delete it.\n\tif cfg.Empty() {\n\t\treturn c.removeBucketReplication(ctx, bucketName)\n\t}\n\t// Save the updated replication.\n\treturn c.putBucketReplication(ctx, bucketName, cfg)\n}\n\n// Saves a new bucket replication.\nfunc (c *Client) putBucketReplication(ctx context.Context, bucketName string, cfg replication.Config) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication\", \"\")\n\treplication, err := xml.Marshal(cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(replication),\n\t\tcontentLength:    int64(len(replication)),\n\t\tcontentMD5Base64: sumMD5Base64(replication),\n\t}\n\n\t// Execute PUT to upload a new bucket replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\treturn nil\n}\n\n// Remove replication from a bucket.\nfunc (c *Client) removeBucketReplication(ctx context.Context, bucketName string) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication\", \"\")\n\n\t// Execute DELETE on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n\n// GetBucketReplication fetches bucket replication configuration.If config is not\n// found, returns empty config with nil error.\nfunc (c *Client) GetBucketReplication(ctx context.Context, bucketName string) (cfg replication.Config, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn cfg, err\n\t}\n\tbucketReplicationCfg, err := c.getBucketReplication(ctx, bucketName)\n\tif err != nil {\n\t\terrResponse := ToErrorResponse(err)\n\t\tif errResponse.Code == \"ReplicationConfigurationNotFoundError\" {\n\t\t\treturn cfg, nil\n\t\t}\n\t\treturn cfg, err\n\t}\n\treturn bucketReplicationCfg, nil\n}\n\n// Request server for current bucket replication config.\nfunc (c *Client) getBucketReplication(ctx context.Context, bucketName string) (cfg replication.Config, err error) {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication\", \"\")\n\n\t// Execute GET on bucket to get replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn cfg, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn cfg, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tif err = xmlDecoder(resp.Body, &cfg); err != nil {\n\t\treturn cfg, err\n\t}\n\n\treturn cfg, nil\n}\n\n// GetBucketReplicationMetrics fetches bucket replication status metrics\nfunc (c *Client) GetBucketReplicationMetrics(ctx context.Context, bucketName string) (s replication.Metrics, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn s, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication-metrics\", \"\")\n\n\t// Execute GET on bucket to get replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn s, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn s, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\trespBytes, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn s, err\n\t}\n\n\tif err := json.Unmarshal(respBytes, &s); err != nil {\n\t\treturn s, err\n\t}\n\treturn s, nil\n}\n\n// mustGetUUID - get a random UUID.\nfunc mustGetUUID() string {\n\tu, err := uuid.NewRandom()\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\treturn u.String()\n}\n\n// ResetBucketReplication kicks off replication of previously replicated objects if ExistingObjectReplication\n// is enabled in the replication config\nfunc (c *Client) ResetBucketReplication(ctx context.Context, bucketName string, olderThan time.Duration) (rID string, err error) {\n\trID = mustGetUUID()\n\t_, err = c.resetBucketReplicationOnTarget(ctx, bucketName, olderThan, \"\", rID)\n\tif err != nil {\n\t\treturn rID, err\n\t}\n\treturn rID, nil\n}\n\n// ResetBucketReplicationOnTarget kicks off replication of previously replicated objects if\n// ExistingObjectReplication is enabled in the replication config\nfunc (c *Client) ResetBucketReplicationOnTarget(ctx context.Context, bucketName string, olderThan time.Duration, tgtArn string) (replication.ResyncTargetsInfo, error) {\n\treturn c.resetBucketReplicationOnTarget(ctx, bucketName, olderThan, tgtArn, mustGetUUID())\n}\n\n// ResetBucketReplication kicks off replication of previously replicated objects if ExistingObjectReplication\n// is enabled in the replication config\nfunc (c *Client) resetBucketReplicationOnTarget(ctx context.Context, bucketName string, olderThan time.Duration, tgtArn, resetID string) (rinfo replication.ResyncTargetsInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication-reset\", \"\")\n\tif olderThan > 0 {\n\t\turlValues.Set(\"older-than\", olderThan.String())\n\t}\n\tif tgtArn != \"\" {\n\t\turlValues.Set(\"arn\", tgtArn)\n\t}\n\turlValues.Set(\"reset-id\", resetID)\n\t// Execute GET on bucket to get replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn rinfo, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn rinfo, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tif err = json.NewDecoder(resp.Body).Decode(&rinfo); err != nil {\n\t\treturn rinfo, err\n\t}\n\treturn rinfo, nil\n}\n\n// GetBucketReplicationResyncStatus gets the status of replication resync\nfunc (c *Client) GetBucketReplicationResyncStatus(ctx context.Context, bucketName, arn string) (rinfo replication.ResyncTargetsInfo, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn rinfo, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication-reset-status\", \"\")\n\tif arn != \"\" {\n\t\turlValues.Set(\"arn\", arn)\n\t}\n\t// Execute GET on bucket to get replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn rinfo, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn rinfo, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tif err = json.NewDecoder(resp.Body).Decode(&rinfo); err != nil {\n\t\treturn rinfo, err\n\t}\n\treturn rinfo, nil\n}\n\n// GetBucketReplicationMetricsV2 fetches bucket replication status metrics\nfunc (c *Client) GetBucketReplicationMetricsV2(ctx context.Context, bucketName string) (s replication.MetricsV2, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn s, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication-metrics\", \"2\")\n\n\t// Execute GET on bucket to get replication metrics.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn s, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn s, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\trespBytes, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn s, err\n\t}\n\n\tif err := json.Unmarshal(respBytes, &s); err != nil {\n\t\treturn s, err\n\t}\n\treturn s, nil\n}\n\n// CheckBucketReplication validates if replication is set up properly for a bucket\nfunc (c *Client) CheckBucketReplication(ctx context.Context, bucketName string) (err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"replication-check\", \"\")\n\n\t// Execute GET on bucket to get replication config.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api-bucket-tagging.go",
          "type": "blob",
          "size": 3.7880859375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/tags\"\n)\n\n// GetBucketTagging fetch tagging configuration for a bucket with a\n// context to control cancellations and timeouts.\nfunc (c *Client) GetBucketTagging(ctx context.Context, bucketName string) (*tags.Tags, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\t// Execute GET on bucket to get tagging configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tdefer io.Copy(io.Discard, resp.Body)\n\treturn tags.ParseBucketXML(resp.Body)\n}\n\n// SetBucketTagging sets tagging configuration for a bucket\n// with a context to control cancellations and timeouts.\nfunc (c *Client) SetBucketTagging(ctx context.Context, bucketName string, tags *tags.Tags) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tif tags == nil {\n\t\treturn errors.New(\"nil tags passed\")\n\t}\n\n\tbuf, err := xml.Marshal(tags)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\t// Content-length is mandatory to set a default encryption configuration\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(buf),\n\t\tcontentLength:    int64(len(buf)),\n\t\tcontentMD5Base64: sumMD5Base64(buf),\n\t}\n\n\t// Execute PUT on bucket to put tagging configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n\n// RemoveBucketTagging removes tagging configuration for a\n// bucket with a context to control cancellations and timeouts.\nfunc (c *Client) RemoveBucketTagging(ctx context.Context, bucketName string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\t// Execute DELETE on bucket to remove tagging configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api-bucket-versioning.go",
          "type": "blob",
          "size": 4.4404296875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// SetBucketVersioning sets a bucket versioning configuration\nfunc (c *Client) SetBucketVersioning(ctx context.Context, bucketName string, config BucketVersioningConfiguration) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tbuf, err := xml.Marshal(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"versioning\", \"\")\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(buf),\n\t\tcontentLength:    int64(len(buf)),\n\t\tcontentMD5Base64: sumMD5Base64(buf),\n\t\tcontentSHA256Hex: sum256Hex(buf),\n\t}\n\n\t// Execute PUT to set a bucket versioning.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// EnableVersioning - enable object versioning in given bucket.\nfunc (c *Client) EnableVersioning(ctx context.Context, bucketName string) error {\n\treturn c.SetBucketVersioning(ctx, bucketName, BucketVersioningConfiguration{Status: \"Enabled\"})\n}\n\n// SuspendVersioning - suspend object versioning in given bucket.\nfunc (c *Client) SuspendVersioning(ctx context.Context, bucketName string) error {\n\treturn c.SetBucketVersioning(ctx, bucketName, BucketVersioningConfiguration{Status: \"Suspended\"})\n}\n\n// ExcludedPrefix - holds individual prefixes excluded from being versioned.\ntype ExcludedPrefix struct {\n\tPrefix string\n}\n\n// BucketVersioningConfiguration is the versioning configuration structure\ntype BucketVersioningConfiguration struct {\n\tXMLName   xml.Name `xml:\"VersioningConfiguration\"`\n\tStatus    string   `xml:\"Status\"`\n\tMFADelete string   `xml:\"MfaDelete,omitempty\"`\n\t// MinIO extension - allows selective, prefix-level versioning exclusion.\n\t// Requires versioning to be enabled\n\tExcludedPrefixes []ExcludedPrefix `xml:\",omitempty\"`\n\tExcludeFolders   bool             `xml:\",omitempty\"`\n}\n\n// Various supported states\nconst (\n\tEnabled = \"Enabled\"\n\t// Disabled  State = \"Disabled\" only used by MFA Delete not supported yet.\n\tSuspended = \"Suspended\"\n)\n\n// Enabled returns true if bucket versioning is enabled\nfunc (b BucketVersioningConfiguration) Enabled() bool {\n\treturn b.Status == Enabled\n}\n\n// Suspended returns true if bucket versioning is suspended\nfunc (b BucketVersioningConfiguration) Suspended() bool {\n\treturn b.Status == Suspended\n}\n\n// GetBucketVersioning gets the versioning configuration on\n// an existing bucket with a context to control cancellations and timeouts.\nfunc (c *Client) GetBucketVersioning(ctx context.Context, bucketName string) (BucketVersioningConfiguration, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn BucketVersioningConfiguration{}, err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"versioning\", \"\")\n\n\t// Execute GET on bucket to get the versioning configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:  bucketName,\n\t\tqueryValues: urlValues,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn BucketVersioningConfiguration{}, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn BucketVersioningConfiguration{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tversioningConfig := BucketVersioningConfiguration{}\n\tif err = xmlDecoder(resp.Body, &versioningConfig); err != nil {\n\t\treturn versioningConfig, err\n\t}\n\n\treturn versioningConfig, nil\n}\n"
        },
        {
          "name": "api-compose-object.go",
          "type": "blob",
          "size": 18.0830078125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017, 2018 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// CopyDestOptions represents options specified by user for CopyObject/ComposeObject APIs\ntype CopyDestOptions struct {\n\tBucket string // points to destination bucket\n\tObject string // points to destination object\n\n\t// `Encryption` is the key info for server-side-encryption with customer\n\t// provided key. If it is nil, no encryption is performed.\n\tEncryption encrypt.ServerSide\n\n\t// `userMeta` is the user-metadata key-value pairs to be set on the\n\t// destination. The keys are automatically prefixed with `x-amz-meta-`\n\t// if needed. If nil is passed, and if only a single source (of any\n\t// size) is provided in the ComposeObject call, then metadata from the\n\t// source is copied to the destination.\n\t// if no user-metadata is provided, it is copied from source\n\t// (when there is only once source object in the compose\n\t// request)\n\tUserMetadata map[string]string\n\t// UserMetadata is only set to destination if ReplaceMetadata is true\n\t// other value is UserMetadata is ignored and we preserve src.UserMetadata\n\t// NOTE: if you set this value to true and now metadata is present\n\t// in UserMetadata your destination object will not have any metadata\n\t// set.\n\tReplaceMetadata bool\n\n\t// `userTags` is the user defined object tags to be set on destination.\n\t// This will be set only if the `replaceTags` field is set to true.\n\t// Otherwise this field is ignored\n\tUserTags    map[string]string\n\tReplaceTags bool\n\n\t// Specifies whether you want to apply a Legal Hold to the copied object.\n\tLegalHold LegalHoldStatus\n\n\t// Object Retention related fields\n\tMode            RetentionMode\n\tRetainUntilDate time.Time\n\n\tSize int64 // Needs to be specified if progress bar is specified.\n\t// Progress of the entire copy operation will be sent here.\n\tProgress io.Reader\n}\n\n// Process custom-metadata to remove a `x-amz-meta-` prefix if\n// present and validate that keys are distinct (after this\n// prefix removal).\nfunc filterCustomMeta(userMeta map[string]string) map[string]string {\n\tm := make(map[string]string)\n\tfor k, v := range userMeta {\n\t\tif strings.HasPrefix(strings.ToLower(k), \"x-amz-meta-\") {\n\t\t\tk = k[len(\"x-amz-meta-\"):]\n\t\t}\n\t\tif _, ok := m[k]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tm[k] = v\n\t}\n\treturn m\n}\n\n// Marshal converts all the CopyDestOptions into their\n// equivalent HTTP header representation\nfunc (opts CopyDestOptions) Marshal(header http.Header) {\n\tconst replaceDirective = \"REPLACE\"\n\tif opts.ReplaceTags {\n\t\theader.Set(amzTaggingHeaderDirective, replaceDirective)\n\t\tif tags := s3utils.TagEncode(opts.UserTags); tags != \"\" {\n\t\t\theader.Set(amzTaggingHeader, tags)\n\t\t}\n\t}\n\n\tif opts.LegalHold != LegalHoldStatus(\"\") {\n\t\theader.Set(amzLegalHoldHeader, opts.LegalHold.String())\n\t}\n\n\tif opts.Mode != RetentionMode(\"\") && !opts.RetainUntilDate.IsZero() {\n\t\theader.Set(amzLockMode, opts.Mode.String())\n\t\theader.Set(amzLockRetainUntil, opts.RetainUntilDate.Format(time.RFC3339))\n\t}\n\n\tif opts.Encryption != nil {\n\t\topts.Encryption.Marshal(header)\n\t}\n\n\tif opts.ReplaceMetadata {\n\t\theader.Set(\"x-amz-metadata-directive\", replaceDirective)\n\t\tfor k, v := range filterCustomMeta(opts.UserMetadata) {\n\t\t\tif isAmzHeader(k) || isStandardHeader(k) || isStorageClassHeader(k) || isMinioHeader(k) {\n\t\t\t\theader.Set(k, v)\n\t\t\t} else {\n\t\t\t\theader.Set(\"x-amz-meta-\"+k, v)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// toDestinationInfo returns a validated copyOptions object.\nfunc (opts CopyDestOptions) validate() (err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(opts.Bucket); err != nil {\n\t\treturn err\n\t}\n\tif err = s3utils.CheckValidObjectName(opts.Object); err != nil {\n\t\treturn err\n\t}\n\tif opts.Progress != nil && opts.Size < 0 {\n\t\treturn errInvalidArgument(\"For progress bar effective size needs to be specified\")\n\t}\n\treturn nil\n}\n\n// CopySrcOptions represents a source object to be copied, using\n// server-side copying APIs.\ntype CopySrcOptions struct {\n\tBucket, Object       string\n\tVersionID            string\n\tMatchETag            string\n\tNoMatchETag          string\n\tMatchModifiedSince   time.Time\n\tMatchUnmodifiedSince time.Time\n\tMatchRange           bool\n\tStart, End           int64\n\tEncryption           encrypt.ServerSide\n}\n\n// Marshal converts all the CopySrcOptions into their\n// equivalent HTTP header representation\nfunc (opts CopySrcOptions) Marshal(header http.Header) {\n\t// Set the source header\n\theader.Set(\"x-amz-copy-source\", s3utils.EncodePath(opts.Bucket+\"/\"+opts.Object))\n\tif opts.VersionID != \"\" {\n\t\theader.Set(\"x-amz-copy-source\", s3utils.EncodePath(opts.Bucket+\"/\"+opts.Object)+\"?versionId=\"+opts.VersionID)\n\t}\n\n\tif opts.MatchETag != \"\" {\n\t\theader.Set(\"x-amz-copy-source-if-match\", opts.MatchETag)\n\t}\n\tif opts.NoMatchETag != \"\" {\n\t\theader.Set(\"x-amz-copy-source-if-none-match\", opts.NoMatchETag)\n\t}\n\n\tif !opts.MatchModifiedSince.IsZero() {\n\t\theader.Set(\"x-amz-copy-source-if-modified-since\", opts.MatchModifiedSince.Format(http.TimeFormat))\n\t}\n\tif !opts.MatchUnmodifiedSince.IsZero() {\n\t\theader.Set(\"x-amz-copy-source-if-unmodified-since\", opts.MatchUnmodifiedSince.Format(http.TimeFormat))\n\t}\n\n\tif opts.Encryption != nil {\n\t\tencrypt.SSECopy(opts.Encryption).Marshal(header)\n\t}\n}\n\nfunc (opts CopySrcOptions) validate() (err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(opts.Bucket); err != nil {\n\t\treturn err\n\t}\n\tif err = s3utils.CheckValidObjectName(opts.Object); err != nil {\n\t\treturn err\n\t}\n\tif opts.Start > opts.End || opts.Start < 0 {\n\t\treturn errInvalidArgument(\"start must be non-negative, and start must be at most end.\")\n\t}\n\treturn nil\n}\n\n// Low level implementation of CopyObject API, supports only upto 5GiB worth of copy.\nfunc (c *Client) copyObjectDo(ctx context.Context, srcBucket, srcObject, destBucket, destObject string,\n\tmetadata map[string]string, srcOpts CopySrcOptions, dstOpts PutObjectOptions,\n) (ObjectInfo, error) {\n\t// Build headers.\n\theaders := make(http.Header)\n\n\t// Set all the metadata headers.\n\tfor k, v := range metadata {\n\t\theaders.Set(k, v)\n\t}\n\tif !dstOpts.Internal.ReplicationStatus.Empty() {\n\t\theaders.Set(amzBucketReplicationStatus, string(dstOpts.Internal.ReplicationStatus))\n\t}\n\tif !dstOpts.Internal.SourceMTime.IsZero() {\n\t\theaders.Set(minIOBucketSourceMTime, dstOpts.Internal.SourceMTime.Format(time.RFC3339Nano))\n\t}\n\tif dstOpts.Internal.SourceETag != \"\" {\n\t\theaders.Set(minIOBucketSourceETag, dstOpts.Internal.SourceETag)\n\t}\n\tif dstOpts.Internal.ReplicationRequest {\n\t\theaders.Set(minIOBucketReplicationRequest, \"true\")\n\t}\n\tif dstOpts.Internal.ReplicationValidityCheck {\n\t\theaders.Set(minIOBucketReplicationCheck, \"true\")\n\t}\n\tif !dstOpts.Internal.LegalholdTimestamp.IsZero() {\n\t\theaders.Set(minIOBucketReplicationObjectLegalHoldTimestamp, dstOpts.Internal.LegalholdTimestamp.Format(time.RFC3339Nano))\n\t}\n\tif !dstOpts.Internal.RetentionTimestamp.IsZero() {\n\t\theaders.Set(minIOBucketReplicationObjectRetentionTimestamp, dstOpts.Internal.RetentionTimestamp.Format(time.RFC3339Nano))\n\t}\n\tif !dstOpts.Internal.TaggingTimestamp.IsZero() {\n\t\theaders.Set(minIOBucketReplicationTaggingTimestamp, dstOpts.Internal.TaggingTimestamp.Format(time.RFC3339Nano))\n\t}\n\n\tif len(dstOpts.UserTags) != 0 {\n\t\theaders.Set(amzTaggingHeader, s3utils.TagEncode(dstOpts.UserTags))\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:   destBucket,\n\t\tobjectName:   destObject,\n\t\tcustomHeader: headers,\n\t}\n\tif dstOpts.Internal.SourceVersionID != \"\" {\n\t\tif dstOpts.Internal.SourceVersionID != nullVersionID {\n\t\t\tif _, err := uuid.Parse(dstOpts.Internal.SourceVersionID); err != nil {\n\t\t\t\treturn ObjectInfo{}, errInvalidArgument(err.Error())\n\t\t\t}\n\t\t}\n\t\turlValues := make(url.Values)\n\t\turlValues.Set(\"versionId\", dstOpts.Internal.SourceVersionID)\n\t\treqMetadata.queryValues = urlValues\n\t}\n\n\t// Set the source header\n\theaders.Set(\"x-amz-copy-source\", s3utils.EncodePath(srcBucket+\"/\"+srcObject))\n\tif srcOpts.VersionID != \"\" {\n\t\theaders.Set(\"x-amz-copy-source\", s3utils.EncodePath(srcBucket+\"/\"+srcObject)+\"?versionId=\"+srcOpts.VersionID)\n\t}\n\t// Send upload-part-copy request\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ObjectInfo{}, err\n\t}\n\n\t// Check if we got an error response.\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn ObjectInfo{}, httpRespToErrorResponse(resp, srcBucket, srcObject)\n\t}\n\n\tcpObjRes := copyObjectResult{}\n\terr = xmlDecoder(resp.Body, &cpObjRes)\n\tif err != nil {\n\t\treturn ObjectInfo{}, err\n\t}\n\n\tobjInfo := ObjectInfo{\n\t\tKey:          destObject,\n\t\tETag:         strings.Trim(cpObjRes.ETag, \"\\\"\"),\n\t\tLastModified: cpObjRes.LastModified,\n\t}\n\treturn objInfo, nil\n}\n\nfunc (c *Client) copyObjectPartDo(ctx context.Context, srcBucket, srcObject, destBucket, destObject, uploadID string,\n\tpartID int, startOffset, length int64, metadata map[string]string,\n) (p CompletePart, err error) {\n\theaders := make(http.Header)\n\n\t// Set source\n\theaders.Set(\"x-amz-copy-source\", s3utils.EncodePath(srcBucket+\"/\"+srcObject))\n\n\tif startOffset < 0 {\n\t\treturn p, errInvalidArgument(\"startOffset must be non-negative\")\n\t}\n\n\tif length >= 0 {\n\t\theaders.Set(\"x-amz-copy-source-range\", fmt.Sprintf(\"bytes=%d-%d\", startOffset, startOffset+length-1))\n\t}\n\n\tfor k, v := range metadata {\n\t\theaders.Set(k, v)\n\t}\n\n\tqueryValues := make(url.Values)\n\tqueryValues.Set(\"partNumber\", strconv.Itoa(partID))\n\tqueryValues.Set(\"uploadId\", uploadID)\n\n\tresp, err := c.executeMethod(ctx, http.MethodPut, requestMetadata{\n\t\tbucketName:   destBucket,\n\t\tobjectName:   destObject,\n\t\tcustomHeader: headers,\n\t\tqueryValues:  queryValues,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// Check if we got an error response.\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn p, httpRespToErrorResponse(resp, destBucket, destObject)\n\t}\n\n\t// Decode copy-part response on success.\n\tcpObjRes := copyObjectResult{}\n\terr = xmlDecoder(resp.Body, &cpObjRes)\n\tif err != nil {\n\t\treturn p, err\n\t}\n\tp.PartNumber, p.ETag = partID, cpObjRes.ETag\n\treturn p, nil\n}\n\n// uploadPartCopy - helper function to create a part in a multipart\n// upload via an upload-part-copy request\n// https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html\nfunc (c *Client) uploadPartCopy(ctx context.Context, bucket, object, uploadID string, partNumber int,\n\theaders http.Header,\n) (p CompletePart, err error) {\n\t// Build query parameters\n\turlValues := make(url.Values)\n\turlValues.Set(\"partNumber\", strconv.Itoa(partNumber))\n\turlValues.Set(\"uploadId\", uploadID)\n\n\t// Send upload-part-copy request\n\tresp, err := c.executeMethod(ctx, http.MethodPut, requestMetadata{\n\t\tbucketName:   bucket,\n\t\tobjectName:   object,\n\t\tcustomHeader: headers,\n\t\tqueryValues:  urlValues,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn p, err\n\t}\n\n\t// Check if we got an error response.\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn p, httpRespToErrorResponse(resp, bucket, object)\n\t}\n\n\t// Decode copy-part response on success.\n\tcpObjRes := copyObjectResult{}\n\terr = xmlDecoder(resp.Body, &cpObjRes)\n\tif err != nil {\n\t\treturn p, err\n\t}\n\tp.PartNumber, p.ETag = partNumber, cpObjRes.ETag\n\treturn p, nil\n}\n\n// ComposeObject - creates an object using server-side copying\n// of existing objects. It takes a list of source objects (with optional offsets)\n// and concatenates them into a new object using only server-side copying\n// operations. Optionally takes progress reader hook for applications to\n// look at current progress.\nfunc (c *Client) ComposeObject(ctx context.Context, dst CopyDestOptions, srcs ...CopySrcOptions) (UploadInfo, error) {\n\tif len(srcs) < 1 || len(srcs) > maxPartsCount {\n\t\treturn UploadInfo{}, errInvalidArgument(\"There must be as least one and up to 10000 source objects.\")\n\t}\n\n\tfor _, src := range srcs {\n\t\tif err := src.validate(); err != nil {\n\t\t\treturn UploadInfo{}, err\n\t\t}\n\t}\n\n\tif err := dst.validate(); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tsrcObjectInfos := make([]ObjectInfo, len(srcs))\n\tsrcObjectSizes := make([]int64, len(srcs))\n\tvar totalSize, totalParts int64\n\tvar err error\n\tfor i, src := range srcs {\n\t\topts := StatObjectOptions{ServerSideEncryption: encrypt.SSE(src.Encryption), VersionID: src.VersionID}\n\t\tsrcObjectInfos[i], err = c.StatObject(context.Background(), src.Bucket, src.Object, opts)\n\t\tif err != nil {\n\t\t\treturn UploadInfo{}, err\n\t\t}\n\n\t\tsrcCopySize := srcObjectInfos[i].Size\n\t\t// Check if a segment is specified, and if so, is the\n\t\t// segment within object bounds?\n\t\tif src.MatchRange {\n\t\t\t// Since range is specified,\n\t\t\t//    0 <= src.start <= src.end\n\t\t\t// so only invalid case to check is:\n\t\t\tif src.End >= srcCopySize || src.Start < 0 {\n\t\t\t\treturn UploadInfo{}, errInvalidArgument(\n\t\t\t\t\tfmt.Sprintf(\"CopySrcOptions %d has invalid segment-to-copy [%d, %d] (size is %d)\",\n\t\t\t\t\t\ti, src.Start, src.End, srcCopySize))\n\t\t\t}\n\t\t\tsrcCopySize = src.End - src.Start + 1\n\t\t}\n\n\t\t// Only the last source may be less than `absMinPartSize`\n\t\tif srcCopySize < absMinPartSize && i < len(srcs)-1 {\n\t\t\treturn UploadInfo{}, errInvalidArgument(\n\t\t\t\tfmt.Sprintf(\"CopySrcOptions %d is too small (%d) and it is not the last part\", i, srcCopySize))\n\t\t}\n\n\t\t// Is data to copy too large?\n\t\ttotalSize += srcCopySize\n\t\tif totalSize > maxMultipartPutObjectSize {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\"Cannot compose an object of size %d (> 5TiB)\", totalSize))\n\t\t}\n\n\t\t// record source size\n\t\tsrcObjectSizes[i] = srcCopySize\n\n\t\t// calculate parts needed for current source\n\t\ttotalParts += partsRequired(srcCopySize)\n\t\t// Do we need more parts than we are allowed?\n\t\tif totalParts > maxPartsCount {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\n\t\t\t\t\"Your proposed compose object requires more than %d parts\", maxPartsCount))\n\t\t}\n\t}\n\n\t// Single source object case (i.e. when only one source is\n\t// involved, it is being copied wholly and at most 5GiB in\n\t// size, emptyfiles are also supported).\n\tif (totalParts == 1 && srcs[0].Start == -1 && totalSize <= maxPartSize) || (totalSize == 0) {\n\t\treturn c.CopyObject(ctx, dst, srcs[0])\n\t}\n\n\t// Now, handle multipart-copy cases.\n\n\t// 1. Ensure that the object has not been changed while\n\t//    we are copying data.\n\tfor i, src := range srcs {\n\t\tsrc.MatchETag = srcObjectInfos[i].ETag\n\t}\n\n\t// 2. Initiate a new multipart upload.\n\n\t// Set user-metadata on the destination object. If no\n\t// user-metadata is specified, and there is only one source,\n\t// (only) then metadata from source is copied.\n\tvar userMeta map[string]string\n\tif dst.ReplaceMetadata {\n\t\tuserMeta = dst.UserMetadata\n\t} else {\n\t\tuserMeta = srcObjectInfos[0].UserMetadata\n\t}\n\n\tvar userTags map[string]string\n\tif dst.ReplaceTags {\n\t\tuserTags = dst.UserTags\n\t} else {\n\t\tuserTags = srcObjectInfos[0].UserTags\n\t}\n\n\tuploadID, err := c.newUploadID(ctx, dst.Bucket, dst.Object, PutObjectOptions{\n\t\tServerSideEncryption: dst.Encryption,\n\t\tUserMetadata:         userMeta,\n\t\tUserTags:             userTags,\n\t\tMode:                 dst.Mode,\n\t\tRetainUntilDate:      dst.RetainUntilDate,\n\t\tLegalHold:            dst.LegalHold,\n\t})\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// 3. Perform copy part uploads\n\tobjParts := []CompletePart{}\n\tpartIndex := 1\n\tfor i, src := range srcs {\n\t\th := make(http.Header)\n\t\tsrc.Marshal(h)\n\t\tif dst.Encryption != nil && dst.Encryption.Type() == encrypt.SSEC {\n\t\t\tdst.Encryption.Marshal(h)\n\t\t}\n\n\t\t// calculate start/end indices of parts after\n\t\t// splitting.\n\t\tstartIdx, endIdx := calculateEvenSplits(srcObjectSizes[i], src)\n\t\tfor j, start := range startIdx {\n\t\t\tend := endIdx[j]\n\n\t\t\t// Add (or reset) source range header for\n\t\t\t// upload part copy request.\n\t\t\th.Set(\"x-amz-copy-source-range\",\n\t\t\t\tfmt.Sprintf(\"bytes=%d-%d\", start, end))\n\n\t\t\t// make upload-part-copy request\n\t\t\tcomplPart, err := c.uploadPartCopy(ctx, dst.Bucket,\n\t\t\t\tdst.Object, uploadID, partIndex, h)\n\t\t\tif err != nil {\n\t\t\t\treturn UploadInfo{}, err\n\t\t\t}\n\t\t\tif dst.Progress != nil {\n\t\t\t\tio.CopyN(io.Discard, dst.Progress, end-start+1)\n\t\t\t}\n\t\t\tobjParts = append(objParts, complPart)\n\t\t\tpartIndex++\n\t\t}\n\t}\n\n\t// 4. Make final complete-multipart request.\n\tuploadInfo, err := c.completeMultipartUpload(ctx, dst.Bucket, dst.Object, uploadID,\n\t\tcompleteMultipartUpload{Parts: objParts}, PutObjectOptions{ServerSideEncryption: dst.Encryption})\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalSize\n\treturn uploadInfo, nil\n}\n\n// partsRequired is maximum parts possible with\n// max part size of ceiling(maxMultipartPutObjectSize / (maxPartsCount - 1))\nfunc partsRequired(size int64) int64 {\n\tmaxPartSize := maxMultipartPutObjectSize / (maxPartsCount - 1)\n\tr := size / int64(maxPartSize)\n\tif size%int64(maxPartSize) > 0 {\n\t\tr++\n\t}\n\treturn r\n}\n\n// calculateEvenSplits - computes splits for a source and returns\n// start and end index slices. Splits happen evenly to be sure that no\n// part is less than 5MiB, as that could fail the multipart request if\n// it is not the last part.\nfunc calculateEvenSplits(size int64, src CopySrcOptions) (startIndex, endIndex []int64) {\n\tif size == 0 {\n\t\treturn\n\t}\n\n\treqParts := partsRequired(size)\n\tstartIndex = make([]int64, reqParts)\n\tendIndex = make([]int64, reqParts)\n\t// Compute number of required parts `k`, as:\n\t//\n\t// k = ceiling(size / copyPartSize)\n\t//\n\t// Now, distribute the `size` bytes in the source into\n\t// k parts as evenly as possible:\n\t//\n\t// r parts sized (q+1) bytes, and\n\t// (k - r) parts sized q bytes, where\n\t//\n\t// size = q * k + r (by simple division of size by k,\n\t// so that 0 <= r < k)\n\t//\n\tstart := src.Start\n\tif start == -1 {\n\t\tstart = 0\n\t}\n\tquot, rem := size/reqParts, size%reqParts\n\tnextStart := start\n\tfor j := int64(0); j < reqParts; j++ {\n\t\tcurPartSize := quot\n\t\tif j < rem {\n\t\t\tcurPartSize++\n\t\t}\n\n\t\tcStart := nextStart\n\t\tcEnd := cStart + curPartSize - 1\n\t\tnextStart = cEnd + 1\n\n\t\tstartIndex[j], endIndex[j] = cStart, cEnd\n\t}\n\treturn\n}\n"
        },
        {
          "name": "api-compose-object_test.go",
          "type": "blob",
          "size": 5.1552734375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage minio\n\nimport (\n\t\"net/http\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n)\n\nconst (\n\tgb1    = 1024 * 1024 * 1024\n\tgb5    = 5 * gb1\n\tgb5p1  = gb5 + 1\n\tgb10p1 = 2*gb5 + 1\n\tgb10p2 = 2*gb5 + 2\n)\n\nfunc TestPartsRequired(t *testing.T) {\n\ttestCases := []struct {\n\t\tsize, ref int64\n\t}{\n\t\t{0, 0},\n\t\t{1, 1},\n\t\t{gb5, 10},\n\t\t{gb5p1, 10},\n\t\t{2 * gb5, 20},\n\t\t{gb10p1, 20},\n\t\t{gb10p2, 20},\n\t\t{gb10p1 + gb10p2, 40},\n\t\t{maxMultipartPutObjectSize, 10000},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tres := partsRequired(testCase.size)\n\t\tif res != testCase.ref {\n\t\t\tt.Errorf(\"Test %d - output did not match with reference results, Expected %d, got %d\", i+1, testCase.ref, res)\n\t\t}\n\t}\n}\n\nfunc TestCalculateEvenSplits(t *testing.T) {\n\ttestCases := []struct {\n\t\t// input size and source object\n\t\tsize int64\n\t\tsrc  CopySrcOptions\n\n\t\t// output part-indexes\n\t\tstarts, ends []int64\n\t}{\n\t\t{0, CopySrcOptions{Start: -1}, nil, nil},\n\t\t{1, CopySrcOptions{Start: -1}, []int64{0}, []int64{0}},\n\t\t{1, CopySrcOptions{Start: 0}, []int64{0}, []int64{0}},\n\n\t\t{gb1, CopySrcOptions{Start: -1}, []int64{0, 536870912}, []int64{536870911, 1073741823}},\n\t\t{\n\t\t\tgb5,\n\t\t\tCopySrcOptions{Start: -1},\n\t\t\t[]int64{\n\t\t\t\t0, 536870912, 1073741824, 1610612736, 2147483648, 2684354560,\n\t\t\t\t3221225472, 3758096384, 4294967296, 4831838208,\n\t\t\t},\n\t\t\t[]int64{\n\t\t\t\t536870911, 1073741823, 1610612735, 2147483647, 2684354559, 3221225471,\n\t\t\t\t3758096383, 4294967295, 4831838207, 5368709119,\n\t\t\t},\n\t\t},\n\n\t\t// 2 part splits\n\t\t{\n\t\t\tgb5p1,\n\t\t\tCopySrcOptions{Start: -1},\n\t\t\t[]int64{\n\t\t\t\t0, 536870913, 1073741825, 1610612737, 2147483649, 2684354561,\n\t\t\t\t3221225473, 3758096385, 4294967297, 4831838209,\n\t\t\t},\n\t\t\t[]int64{\n\t\t\t\t536870912, 1073741824, 1610612736, 2147483648, 2684354560, 3221225472,\n\t\t\t\t3758096384, 4294967296, 4831838208, 5368709120,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tgb5p1,\n\t\t\tCopySrcOptions{Start: -1},\n\t\t\t[]int64{\n\t\t\t\t0, 536870913, 1073741825, 1610612737, 2147483649, 2684354561,\n\t\t\t\t3221225473, 3758096385, 4294967297, 4831838209,\n\t\t\t},\n\t\t\t[]int64{\n\t\t\t\t536870912, 1073741824, 1610612736, 2147483648, 2684354560, 3221225472,\n\t\t\t\t3758096384, 4294967296, 4831838208, 5368709120,\n\t\t\t},\n\t\t},\n\n\t\t// 3 part splits\n\t\t{\n\t\t\tgb10p1,\n\t\t\tCopySrcOptions{Start: -1},\n\t\t\t[]int64{\n\t\t\t\t0, 536870913, 1073741825, 1610612737, 2147483649, 2684354561,\n\t\t\t\t3221225473, 3758096385, 4294967297, 4831838209, 5368709121,\n\t\t\t\t5905580033, 6442450945, 6979321857, 7516192769, 8053063681,\n\t\t\t\t8589934593, 9126805505, 9663676417, 10200547329,\n\t\t\t},\n\t\t\t[]int64{\n\t\t\t\t536870912, 1073741824, 1610612736, 2147483648, 2684354560,\n\t\t\t\t3221225472, 3758096384, 4294967296, 4831838208, 5368709120,\n\t\t\t\t5905580032, 6442450944, 6979321856, 7516192768, 8053063680,\n\t\t\t\t8589934592, 9126805504, 9663676416, 10200547328, 10737418240,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tgb10p2,\n\t\t\tCopySrcOptions{Start: -1},\n\t\t\t[]int64{\n\t\t\t\t0, 536870913, 1073741826, 1610612738, 2147483650, 2684354562,\n\t\t\t\t3221225474, 3758096386, 4294967298, 4831838210, 5368709122,\n\t\t\t\t5905580034, 6442450946, 6979321858, 7516192770, 8053063682,\n\t\t\t\t8589934594, 9126805506, 9663676418, 10200547330,\n\t\t\t},\n\t\t\t[]int64{\n\t\t\t\t536870912, 1073741825, 1610612737, 2147483649, 2684354561,\n\t\t\t\t3221225473, 3758096385, 4294967297, 4831838209, 5368709121,\n\t\t\t\t5905580033, 6442450945, 6979321857, 7516192769, 8053063681,\n\t\t\t\t8589934593, 9126805505, 9663676417, 10200547329, 10737418241,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tresStart, resEnd := calculateEvenSplits(testCase.size, testCase.src)\n\t\tif !reflect.DeepEqual(testCase.starts, resStart) || !reflect.DeepEqual(testCase.ends, resEnd) {\n\t\t\tt.Errorf(\"Test %d - output did not match with reference results, Expected %d/%d, got %d/%d\", i+1, testCase.starts, testCase.ends, resStart, resEnd)\n\t\t}\n\t}\n}\n\nfunc TestDestOptions(t *testing.T) {\n\tuserMetadata := map[string]string{\n\t\t\"test\":                \"test\",\n\t\t\"x-amz-acl\":           \"public-read-write\",\n\t\t\"content-type\":        \"application/binary\",\n\t\t\"X-Amz-Storage-Class\": \"rrs\",\n\t\t\"x-amz-grant-write\":   \"test@exo.ch\",\n\t}\n\n\tr := make(http.Header)\n\n\tdst := CopyDestOptions{\n\t\tBucket:          \"bucket\",\n\t\tObject:          \"object\",\n\t\tReplaceMetadata: true,\n\t\tUserMetadata:    userMetadata,\n\t}\n\tdst.Marshal(r)\n\n\tif v := r.Get(\"x-amz-metadata-directive\"); v != \"REPLACE\" {\n\t\tt.Errorf(\"Test - metadata directive was expected but is missing\")\n\t}\n\n\tfor k := range r {\n\t\tif strings.HasSuffix(k, \"test\") && !strings.HasPrefix(k, \"x-amz-meta-\") {\n\t\t\tt.Errorf(\"Test meta %q was expected as an x amz meta\", k)\n\t\t}\n\n\t\tif !strings.HasSuffix(k, \"test\") && strings.HasPrefix(k, \"x-amz-meta-\") {\n\t\t\tt.Errorf(\"Test an amz/standard/storageClass Header was expected but got an x amz meta data\")\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "api-copy-object.go",
          "type": "blob",
          "size": 2.09375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017, 2018 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"net/http\"\n)\n\n// CopyObject - copy a source object into a new object\nfunc (c *Client) CopyObject(ctx context.Context, dst CopyDestOptions, src CopySrcOptions) (UploadInfo, error) {\n\tif err := src.validate(); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tif err := dst.validate(); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\theader := make(http.Header)\n\tdst.Marshal(header)\n\tsrc.Marshal(header)\n\n\tresp, err := c.executeMethod(ctx, http.MethodPut, requestMetadata{\n\t\tbucketName:   dst.Bucket,\n\t\tobjectName:   dst.Object,\n\t\tcustomHeader: header,\n\t})\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdefer closeResponse(resp)\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn UploadInfo{}, httpRespToErrorResponse(resp, dst.Bucket, dst.Object)\n\t}\n\n\t// Update the progress properly after successful copy.\n\tif dst.Progress != nil {\n\t\tio.Copy(io.Discard, io.LimitReader(dst.Progress, dst.Size))\n\t}\n\n\tcpObjRes := copyObjectResult{}\n\tif err = xmlDecoder(resp.Body, &cpObjRes); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// extract lifecycle expiry date and rule ID\n\texpTime, ruleID := amzExpirationToExpiryDateRuleID(resp.Header.Get(amzExpiration))\n\n\treturn UploadInfo{\n\t\tBucket:           dst.Bucket,\n\t\tKey:              dst.Object,\n\t\tLastModified:     cpObjRes.LastModified,\n\t\tETag:             trimEtag(resp.Header.Get(\"ETag\")),\n\t\tVersionID:        resp.Header.Get(amzVersionID),\n\t\tExpiration:       expTime,\n\t\tExpirationRuleID: ruleID,\n\t}, nil\n}\n"
        },
        {
          "name": "api-datatypes.go",
          "type": "blob",
          "size": 7.2041015625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strings\"\n\t\"time\"\n)\n\n// BucketInfo container for bucket metadata.\ntype BucketInfo struct {\n\t// The name of the bucket.\n\tName string `json:\"name\"`\n\t// Date the bucket was created.\n\tCreationDate time.Time `json:\"creationDate\"`\n}\n\n// StringMap represents map with custom UnmarshalXML\ntype StringMap map[string]string\n\n// UnmarshalXML unmarshals the XML into a map of string to strings,\n// creating a key in the map for each tag and setting it's value to the\n// tags contents.\n//\n// The fact this function is on the pointer of Map is important, so that\n// if m is nil it can be initialized, which is often the case if m is\n// nested in another xml structural. This is also why the first thing done\n// on the first line is initialize it.\nfunc (m *StringMap) UnmarshalXML(d *xml.Decoder, _ xml.StartElement) error {\n\t*m = StringMap{}\n\tfor {\n\t\t// Format is <key>value</key>\n\t\tvar e struct {\n\t\t\tXMLName xml.Name\n\t\t\tValue   string `xml:\",chardata\"`\n\t\t}\n\t\terr := d.Decode(&e)\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t(*m)[e.XMLName.Local] = e.Value\n\t}\n\treturn nil\n}\n\n// URLMap represents map with custom UnmarshalXML\ntype URLMap map[string]string\n\n// UnmarshalXML unmarshals the XML into a map of string to strings,\n// creating a key in the map for each tag and setting it's value to the\n// tags contents.\n//\n// The fact this function is on the pointer of Map is important, so that\n// if m is nil it can be initialized, which is often the case if m is\n// nested in another xml structural. This is also why the first thing done\n// on the first line is initialize it.\nfunc (m *URLMap) UnmarshalXML(d *xml.Decoder, se xml.StartElement) error {\n\t*m = URLMap{}\n\tvar tgs string\n\tif err := d.DecodeElement(&tgs, &se); err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tfor tgs != \"\" {\n\t\tvar key string\n\t\tkey, tgs, _ = stringsCut(tgs, \"&\")\n\t\tif key == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tkey, value, _ := stringsCut(key, \"=\")\n\t\tkey, err := url.QueryUnescape(key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvalue, err = url.QueryUnescape(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t(*m)[key] = value\n\t}\n\treturn nil\n}\n\n// stringsCut slices s around the first instance of sep,\n// returning the text before and after sep.\n// The found result reports whether sep appears in s.\n// If sep does not appear in s, cut returns s, \"\", false.\nfunc stringsCut(s, sep string) (before, after string, found bool) {\n\tif i := strings.Index(s, sep); i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// Owner name.\ntype Owner struct {\n\tXMLName     xml.Name `xml:\"Owner\" json:\"owner\"`\n\tDisplayName string   `xml:\"ID\" json:\"name\"`\n\tID          string   `xml:\"DisplayName\" json:\"id\"`\n}\n\n// UploadInfo contains information about the\n// newly uploaded or copied object.\ntype UploadInfo struct {\n\tBucket       string\n\tKey          string\n\tETag         string\n\tSize         int64\n\tLastModified time.Time\n\tLocation     string\n\tVersionID    string\n\n\t// Lifecycle expiry-date and ruleID associated with the expiry\n\t// not to be confused with `Expires` HTTP header.\n\tExpiration       time.Time\n\tExpirationRuleID string\n\n\t// Verified checksum values, if any.\n\t// Values are base64 (standard) encoded.\n\t// For multipart objects this is a checksum of the checksum of each part.\n\tChecksumCRC32     string\n\tChecksumCRC32C    string\n\tChecksumSHA1      string\n\tChecksumSHA256    string\n\tChecksumCRC64NVME string\n}\n\n// RestoreInfo contains information of the restore operation of an archived object\ntype RestoreInfo struct {\n\t// Is the restoring operation is still ongoing\n\tOngoingRestore bool\n\t// When the restored copy of the archived object will be removed\n\tExpiryTime time.Time\n}\n\n// ObjectInfo container for object metadata.\ntype ObjectInfo struct {\n\t// An ETag is optionally set to md5sum of an object.  In case of multipart objects,\n\t// ETag is of the form MD5SUM-N where MD5SUM is md5sum of all individual md5sums of\n\t// each parts concatenated into one string.\n\tETag string `json:\"etag\"`\n\n\tKey          string    `json:\"name\"`         // Name of the object\n\tLastModified time.Time `json:\"lastModified\"` // Date and time the object was last modified.\n\tSize         int64     `json:\"size\"`         // Size in bytes of the object.\n\tContentType  string    `json:\"contentType\"`  // A standard MIME type describing the format of the object data.\n\tExpires      time.Time `json:\"expires\"`      // The date and time at which the object is no longer able to be cached.\n\n\t// Collection of additional metadata on the object.\n\t// eg: x-amz-meta-*, content-encoding etc.\n\tMetadata http.Header `json:\"metadata\" xml:\"-\"`\n\n\t// x-amz-meta-* headers stripped \"x-amz-meta-\" prefix containing the first value.\n\t// Only returned by MinIO servers.\n\tUserMetadata StringMap `json:\"userMetadata,omitempty\"`\n\n\t// x-amz-tagging values in their k/v values.\n\t// Only returned by MinIO servers.\n\tUserTags URLMap `json:\"userTags,omitempty\" xml:\"UserTags\"`\n\n\t// x-amz-tagging-count value\n\tUserTagCount int\n\n\t// Owner name.\n\tOwner Owner\n\n\t// ACL grant.\n\tGrant []Grant\n\n\t// The class of storage used to store the object.\n\tStorageClass string `json:\"storageClass\"`\n\n\t// Versioning related information\n\tIsLatest       bool\n\tIsDeleteMarker bool\n\tVersionID      string `xml:\"VersionId\"`\n\n\t// x-amz-replication-status value is either in one of the following states\n\t// - COMPLETED\n\t// - PENDING\n\t// - FAILED\n\t// - REPLICA (on the destination)\n\tReplicationStatus string `xml:\"ReplicationStatus\"`\n\t// set to true if delete marker has backing object version on target, and eligible to replicate\n\tReplicationReady bool\n\t// Lifecycle expiry-date and ruleID associated with the expiry\n\t// not to be confused with `Expires` HTTP header.\n\tExpiration       time.Time\n\tExpirationRuleID string\n\n\tRestore *RestoreInfo\n\n\t// Checksum values\n\tChecksumCRC32     string\n\tChecksumCRC32C    string\n\tChecksumSHA1      string\n\tChecksumSHA256    string\n\tChecksumCRC64NVME string\n\n\tInternal *struct {\n\t\tK int // Data blocks\n\t\tM int // Parity blocks\n\t} `xml:\"Internal\"`\n\n\t// Error\n\tErr error `json:\"-\"`\n}\n\n// ObjectMultipartInfo container for multipart object metadata.\ntype ObjectMultipartInfo struct {\n\t// Date and time at which the multipart upload was initiated.\n\tInitiated time.Time `type:\"timestamp\" timestampFormat:\"iso8601\"`\n\n\tInitiator initiator\n\tOwner     owner\n\n\t// The type of storage to use for the object. Defaults to 'STANDARD'.\n\tStorageClass string\n\n\t// Key of the object for which the multipart upload was initiated.\n\tKey string\n\n\t// Size in bytes of the object.\n\tSize int64\n\n\t// Upload ID that identifies the multipart upload.\n\tUploadID string `xml:\"UploadId\"`\n\n\t// Error\n\tErr error\n}\n"
        },
        {
          "name": "api-error-response.go",
          "type": "blob",
          "size": 8.0439453125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n)\n\n/* **** SAMPLE ERROR RESPONSE ****\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n   <Code>AccessDenied</Code>\n   <Message>Access Denied</Message>\n   <BucketName>bucketName</BucketName>\n   <Key>objectName</Key>\n   <RequestId>F19772218238A85A</RequestId>\n   <HostId>GuWkjyviSiGHizehqpmsD1ndz5NClSP19DOT+s2mv7gXGQ8/X1lhbDGiIJEXpGFD</HostId>\n</Error>\n*/\n\n// ErrorResponse - Is the typed error returned by all API operations.\n// ErrorResponse struct should be comparable since it is compared inside\n// golang http API (https://github.com/golang/go/issues/29768)\ntype ErrorResponse struct {\n\tXMLName    xml.Name `xml:\"Error\" json:\"-\"`\n\tCode       string\n\tMessage    string\n\tBucketName string\n\tKey        string\n\tResource   string\n\tRequestID  string `xml:\"RequestId\"`\n\tHostID     string `xml:\"HostId\"`\n\n\t// Region where the bucket is located. This header is returned\n\t// only in HEAD bucket and ListObjects response.\n\tRegion string\n\n\t// Captures the server string returned in response header.\n\tServer string\n\n\t// Underlying HTTP status code for the returned error\n\tStatusCode int `xml:\"-\" json:\"-\"`\n}\n\n// ToErrorResponse - Returns parsed ErrorResponse struct from body and\n// http headers.\n//\n// For example:\n//\n//\timport s3 \"github.com/minio/minio-go/v7\"\n//\t...\n//\t...\n//\treader, stat, err := s3.GetObject(...)\n//\tif err != nil {\n//\t   resp := s3.ToErrorResponse(err)\n//\t}\n//\t...\nfunc ToErrorResponse(err error) ErrorResponse {\n\tswitch err := err.(type) {\n\tcase ErrorResponse:\n\t\treturn err\n\tdefault:\n\t\treturn ErrorResponse{}\n\t}\n}\n\n// Error - Returns S3 error string.\nfunc (e ErrorResponse) Error() string {\n\tif e.Message == \"\" {\n\t\tmsg, ok := s3ErrorResponseMap[e.Code]\n\t\tif !ok {\n\t\t\tmsg = fmt.Sprintf(\"Error response code %s.\", e.Code)\n\t\t}\n\t\treturn msg\n\t}\n\treturn e.Message\n}\n\n// Common string for errors to report issue location in unexpected\n// cases.\nconst (\n\treportIssue = \"Please report this issue at https://github.com/minio/minio-go/issues.\"\n)\n\n// xmlDecodeAndBody reads the whole body up to 1MB and\n// tries to XML decode it into v.\n// The body that was read and any error from reading or decoding is returned.\nfunc xmlDecodeAndBody(bodyReader io.Reader, v interface{}) ([]byte, error) {\n\t// read the whole body (up to 1MB)\n\tconst maxBodyLength = 1 << 20\n\tbody, err := io.ReadAll(io.LimitReader(bodyReader, maxBodyLength))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn bytes.TrimSpace(body), xmlDecoder(bytes.NewReader(body), v)\n}\n\n// httpRespToErrorResponse returns a new encoded ErrorResponse\n// structure as error.\nfunc httpRespToErrorResponse(resp *http.Response, bucketName, objectName string) error {\n\tif resp == nil {\n\t\tmsg := \"Empty http response. \" + reportIssue\n\t\treturn errInvalidArgument(msg)\n\t}\n\n\terrResp := ErrorResponse{\n\t\tStatusCode: resp.StatusCode,\n\t\tServer:     resp.Header.Get(\"Server\"),\n\t}\n\n\terrBody, err := xmlDecodeAndBody(resp.Body, &errResp)\n\t// Xml decoding failed with no body, fall back to HTTP headers.\n\tif err != nil {\n\t\tswitch resp.StatusCode {\n\t\tcase http.StatusNotFound:\n\t\t\tif objectName == \"\" {\n\t\t\t\terrResp = ErrorResponse{\n\t\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\t\tCode:       \"NoSuchBucket\",\n\t\t\t\t\tMessage:    \"The specified bucket does not exist.\",\n\t\t\t\t\tBucketName: bucketName,\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terrResp = ErrorResponse{\n\t\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\t\tCode:       \"NoSuchKey\",\n\t\t\t\t\tMessage:    \"The specified key does not exist.\",\n\t\t\t\t\tBucketName: bucketName,\n\t\t\t\t\tKey:        objectName,\n\t\t\t\t}\n\t\t\t}\n\t\tcase http.StatusForbidden:\n\t\t\terrResp = ErrorResponse{\n\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\tCode:       \"AccessDenied\",\n\t\t\t\tMessage:    \"Access Denied.\",\n\t\t\t\tBucketName: bucketName,\n\t\t\t\tKey:        objectName,\n\t\t\t}\n\t\tcase http.StatusConflict:\n\t\t\terrResp = ErrorResponse{\n\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\tCode:       \"Conflict\",\n\t\t\t\tMessage:    \"Bucket not empty.\",\n\t\t\t\tBucketName: bucketName,\n\t\t\t}\n\t\tcase http.StatusPreconditionFailed:\n\t\t\terrResp = ErrorResponse{\n\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\tCode:       \"PreconditionFailed\",\n\t\t\t\tMessage:    s3ErrorResponseMap[\"PreconditionFailed\"],\n\t\t\t\tBucketName: bucketName,\n\t\t\t\tKey:        objectName,\n\t\t\t}\n\t\tdefault:\n\t\t\tmsg := resp.Status\n\t\t\tif len(errBody) > 0 {\n\t\t\t\tmsg = string(errBody)\n\t\t\t\tif len(msg) > 1024 {\n\t\t\t\t\tmsg = msg[:1024] + \"...\"\n\t\t\t\t}\n\t\t\t}\n\t\t\terrResp = ErrorResponse{\n\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\tCode:       resp.Status,\n\t\t\t\tMessage:    msg,\n\t\t\t\tBucketName: bucketName,\n\t\t\t}\n\t\t}\n\t}\n\n\tcode := resp.Header.Get(\"x-minio-error-code\")\n\tif code != \"\" {\n\t\terrResp.Code = code\n\t}\n\tdesc := resp.Header.Get(\"x-minio-error-desc\")\n\tif desc != \"\" {\n\t\terrResp.Message = strings.Trim(desc, `\"`)\n\t}\n\n\t// Save hostID, requestID and region information\n\t// from headers if not available through error XML.\n\tif errResp.RequestID == \"\" {\n\t\terrResp.RequestID = resp.Header.Get(\"x-amz-request-id\")\n\t}\n\tif errResp.HostID == \"\" {\n\t\terrResp.HostID = resp.Header.Get(\"x-amz-id-2\")\n\t}\n\tif errResp.Region == \"\" {\n\t\terrResp.Region = resp.Header.Get(\"x-amz-bucket-region\")\n\t}\n\tif errResp.Code == \"InvalidRegion\" && errResp.Region != \"\" {\n\t\terrResp.Message = fmt.Sprintf(\"Region does not match, expecting region ‘%s’.\", errResp.Region)\n\t}\n\n\treturn errResp\n}\n\n// errTransferAccelerationBucket - bucket name is invalid to be used with transfer acceleration.\nfunc errTransferAccelerationBucket(bucketName string) error {\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"InvalidArgument\",\n\t\tMessage:    \"The name of the bucket used for Transfer Acceleration must be DNS-compliant and must not contain periods ‘.’.\",\n\t\tBucketName: bucketName,\n\t}\n}\n\n// errEntityTooLarge - Input size is larger than supported maximum.\nfunc errEntityTooLarge(totalSize, maxObjectSize int64, bucketName, objectName string) error {\n\tmsg := fmt.Sprintf(\"Your proposed upload size ‘%d’ exceeds the maximum allowed object size ‘%d’ for single PUT operation.\", totalSize, maxObjectSize)\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"EntityTooLarge\",\n\t\tMessage:    msg,\n\t\tBucketName: bucketName,\n\t\tKey:        objectName,\n\t}\n}\n\n// errEntityTooSmall - Input size is smaller than supported minimum.\nfunc errEntityTooSmall(totalSize int64, bucketName, objectName string) error {\n\tmsg := fmt.Sprintf(\"Your proposed upload size ‘%d’ is below the minimum allowed object size ‘0B’ for single PUT operation.\", totalSize)\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"EntityTooSmall\",\n\t\tMessage:    msg,\n\t\tBucketName: bucketName,\n\t\tKey:        objectName,\n\t}\n}\n\n// errUnexpectedEOF - Unexpected end of file reached.\nfunc errUnexpectedEOF(totalRead, totalSize int64, bucketName, objectName string) error {\n\tmsg := fmt.Sprintf(\"Data read ‘%d’ is not equal to the size ‘%d’ of the input Reader.\", totalRead, totalSize)\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"UnexpectedEOF\",\n\t\tMessage:    msg,\n\t\tBucketName: bucketName,\n\t\tKey:        objectName,\n\t}\n}\n\n// errInvalidArgument - Invalid argument response.\nfunc errInvalidArgument(message string) error {\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"InvalidArgument\",\n\t\tMessage:    message,\n\t\tRequestID:  \"minio\",\n\t}\n}\n\n// errAPINotSupported - API not supported response\n// The specified API call is not supported\nfunc errAPINotSupported(message string) error {\n\treturn ErrorResponse{\n\t\tStatusCode: http.StatusNotImplemented,\n\t\tCode:       \"APINotSupported\",\n\t\tMessage:    message,\n\t\tRequestID:  \"minio\",\n\t}\n}\n"
        },
        {
          "name": "api-error-response_test.go",
          "type": "blob",
          "size": 10.57421875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"testing\"\n)\n\n// Tests validate the Error generator function for http response with error.\nfunc TestHttpRespToErrorResponse(t *testing.T) {\n\t// 'genAPIErrorResponse' generates ErrorResponse for given APIError.\n\t// provides a encodable populated response values.\n\tgenAPIErrorResponse := func(err APIError, bucketName string) ErrorResponse {\n\t\treturn ErrorResponse{\n\t\t\tCode:       err.Code,\n\t\t\tMessage:    err.Description,\n\t\t\tBucketName: bucketName,\n\t\t}\n\t}\n\n\t// Encodes the response headers into XML format.\n\tencodeErr := func(response ErrorResponse) []byte {\n\t\tbuf := &bytes.Buffer{}\n\t\tbuf.WriteString(xml.Header)\n\t\tencoder := xml.NewEncoder(buf)\n\t\terr := encoder.Encode(response)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error encoding response: %v\", err)\n\t\t}\n\t\treturn buf.Bytes()\n\t}\n\n\t// `createErrorResponse` Mocks a generic error response from the server.\n\tcreateErrorResponse := func(statusCode int, body []byte) *http.Response {\n\t\tresp := &http.Response{}\n\t\tresp.StatusCode = statusCode\n\t\tresp.Status = http.StatusText(statusCode)\n\t\tresp.Body = io.NopCloser(bytes.NewBuffer(body))\n\t\treturn resp\n\t}\n\n\t// `createAPIErrorResponse` Mocks XML error response from the server.\n\tcreateAPIErrorResponse := func(APIErr APIError, bucketName string) *http.Response {\n\t\t// generate error response.\n\t\t// response body contains the XML error message.\n\t\terrorResponse := genAPIErrorResponse(APIErr, bucketName)\n\t\tencodedErrorResponse := encodeErr(errorResponse)\n\t\treturn createErrorResponse(APIErr.HTTPStatusCode, encodedErrorResponse)\n\t}\n\n\t// 'genErrResponse' contructs error response based http Status Code\n\tgenErrResponse := func(resp *http.Response, code, message, bucketName, objectName string) ErrorResponse {\n\t\terrResp := ErrorResponse{\n\t\t\tStatusCode: resp.StatusCode,\n\t\t\tCode:       code,\n\t\t\tMessage:    message,\n\t\t\tBucketName: bucketName,\n\t\t\tKey:        objectName,\n\t\t\tRequestID:  resp.Header.Get(\"x-amz-request-id\"),\n\t\t\tHostID:     resp.Header.Get(\"x-amz-id-2\"),\n\t\t\tRegion:     resp.Header.Get(\"x-amz-bucket-region\"),\n\t\t}\n\t\treturn errResp\n\t}\n\n\t// Generate invalid argument error.\n\tgenInvalidError := func(message string) error {\n\t\terrResp := ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"InvalidArgument\",\n\t\t\tMessage:    message,\n\t\t\tRequestID:  \"minio\",\n\t\t}\n\t\treturn errResp\n\t}\n\n\t// Set common http response headers.\n\tsetCommonHeaders := func(resp *http.Response) *http.Response {\n\t\t// set headers.\n\t\tresp.Header = make(http.Header)\n\t\tresp.Header.Set(\"x-amz-request-id\", \"xyz\")\n\t\tresp.Header.Set(\"x-amz-id-2\", \"abc\")\n\t\tresp.Header.Set(\"x-amz-bucket-region\", \"us-east-1\")\n\t\treturn resp\n\t}\n\n\t// Generate http response with empty body.\n\t// Set the StatusCode to the argument supplied.\n\t// Sets common headers.\n\tgenEmptyBodyResponse := func(statusCode int) *http.Response {\n\t\tresp := &http.Response{\n\t\t\tStatusCode: statusCode,\n\t\t\tStatus:     http.StatusText(statusCode),\n\t\t\tBody:       io.NopCloser(bytes.NewReader(nil)),\n\t\t}\n\t\tsetCommonHeaders(resp)\n\t\treturn resp\n\t}\n\n\t// Decode XML error message from the http response body.\n\tdecodeXMLError := func(resp *http.Response) error {\n\t\terrResp := ErrorResponse{\n\t\t\tStatusCode: resp.StatusCode,\n\t\t}\n\t\terr := xmlDecoder(resp.Body, &errResp)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"XML decoding of response body failed: %v\", err)\n\t\t}\n\t\treturn errResp\n\t}\n\n\t// List of APIErrors used to generate/mock server side XML error response.\n\tAPIErrors := []APIError{\n\t\t{\n\t\t\tCode:           \"NoSuchBucketPolicy\",\n\t\t\tDescription:    \"The specified bucket does not have a bucket policy.\",\n\t\t\tHTTPStatusCode: http.StatusNotFound,\n\t\t},\n\t}\n\n\t// List of expected response.\n\t// Used for asserting the actual response.\n\texpectedErrResponse := []error{\n\t\tgenInvalidError(\"Empty http response. \" + \"Please report this issue at https://github.com/minio/minio-go/issues.\"),\n\t\tdecodeXMLError(createAPIErrorResponse(APIErrors[0], \"minio-bucket\")),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusNotFound}), \"NoSuchBucket\", \"The specified bucket does not exist.\", \"minio-bucket\", \"\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusNotFound}), \"NoSuchKey\", \"The specified key does not exist.\", \"minio-bucket\", \"Asia/\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusForbidden}), \"AccessDenied\", \"Access Denied.\", \"minio-bucket\", \"\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusConflict}), \"Conflict\", \"Bucket not empty.\", \"minio-bucket\", \"\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusBadRequest}), \"Bad Request\", \"Bad Request\", \"minio-bucket\", \"\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusInternalServerError}), \"Internal Server Error\", \"my custom object store error\", \"minio-bucket\", \"\"),\n\t\tgenErrResponse(setCommonHeaders(&http.Response{StatusCode: http.StatusInternalServerError}), \"Internal Server Error\", \"my custom object store error, with way too long body\", \"minio-bucket\", \"\"),\n\t}\n\n\t// List of http response to be used as input.\n\tinputResponses := []*http.Response{\n\t\tnil,\n\t\tcreateAPIErrorResponse(APIErrors[0], \"minio-bucket\"),\n\t\tgenEmptyBodyResponse(http.StatusNotFound),\n\t\tgenEmptyBodyResponse(http.StatusNotFound),\n\t\tgenEmptyBodyResponse(http.StatusForbidden),\n\t\tgenEmptyBodyResponse(http.StatusConflict),\n\t\tgenEmptyBodyResponse(http.StatusBadRequest),\n\t\tsetCommonHeaders(createErrorResponse(http.StatusInternalServerError, []byte(\"my custom object store error\\n\"))),\n\t\tsetCommonHeaders(createErrorResponse(http.StatusInternalServerError, append([]byte(\"my custom object store error, with way too long body\\n\"), bytes.Repeat([]byte(\"\\n\"), 2*1024*1024)...))),\n\t}\n\n\ttestCases := []struct {\n\t\tbucketName    string\n\t\tobjectName    string\n\t\tinputHTTPResp *http.Response\n\t\t// expected results.\n\t\texpectedResult error\n\t\t// flag indicating whether tests should pass.\n\t}{\n\t\t{\"minio-bucket\", \"\", inputResponses[0], expectedErrResponse[0]},\n\t\t{\"minio-bucket\", \"\", inputResponses[1], expectedErrResponse[1]},\n\t\t{\"minio-bucket\", \"\", inputResponses[2], expectedErrResponse[2]},\n\t\t{\"minio-bucket\", \"Asia/\", inputResponses[3], expectedErrResponse[3]},\n\t\t{\"minio-bucket\", \"\", inputResponses[4], expectedErrResponse[4]},\n\t\t{\"minio-bucket\", \"\", inputResponses[5], expectedErrResponse[5]},\n\t\t{\"minio-bucket\", \"\", inputResponses[6], expectedErrResponse[6]},\n\t\t{\"minio-bucket\", \"\", inputResponses[7], expectedErrResponse[7]},\n\t\t{\"minio-bucket\", \"\", inputResponses[8], expectedErrResponse[8]},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactualResult := httpRespToErrorResponse(testCase.inputHTTPResp, testCase.bucketName, testCase.objectName)\n\t\tif !reflect.DeepEqual(testCase.expectedResult, actualResult) {\n\t\t\tt.Errorf(\"Test %d: Expected result to be '%#v', but instead got '%#v'\", i+1, testCase.expectedResult, actualResult)\n\t\t}\n\t}\n}\n\n// Test validates 'ErrEntityTooLarge' error response.\nfunc TestErrEntityTooLarge(t *testing.T) {\n\tmsg := fmt.Sprintf(\"Your proposed upload size ‘%d’ exceeds the maximum allowed object size ‘%d’ for single PUT operation.\", 1000000, 99999)\n\texpectedResult := ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"EntityTooLarge\",\n\t\tMessage:    msg,\n\t\tBucketName: \"minio-bucket\",\n\t\tKey:        \"Asia/\",\n\t}\n\tactualResult := errEntityTooLarge(1000000, 99999, \"minio-bucket\", \"Asia/\")\n\tif !reflect.DeepEqual(expectedResult, actualResult) {\n\t\tt.Errorf(\"Expected result to be '%#v', but instead got '%#v'\", expectedResult, actualResult)\n\t}\n}\n\n// Test validates 'ErrEntityTooSmall' error response.\nfunc TestErrEntityTooSmall(t *testing.T) {\n\tmsg := fmt.Sprintf(\"Your proposed upload size ‘%d’ is below the minimum allowed object size ‘0B’ for single PUT operation.\", -1)\n\texpectedResult := ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"EntityTooSmall\",\n\t\tMessage:    msg,\n\t\tBucketName: \"minio-bucket\",\n\t\tKey:        \"Asia/\",\n\t}\n\tactualResult := errEntityTooSmall(-1, \"minio-bucket\", \"Asia/\")\n\tif !reflect.DeepEqual(expectedResult, actualResult) {\n\t\tt.Errorf(\"Expected result to be '%#v', but instead got '%#v'\", expectedResult, actualResult)\n\t}\n}\n\n// Test validates 'ErrUnexpectedEOF' error response.\nfunc TestErrUnexpectedEOF(t *testing.T) {\n\tmsg := fmt.Sprintf(\"Data read ‘%s’ is not equal to the size ‘%s’ of the input Reader.\",\n\t\tstrconv.FormatInt(100, 10), strconv.FormatInt(101, 10))\n\texpectedResult := ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"UnexpectedEOF\",\n\t\tMessage:    msg,\n\t\tBucketName: \"minio-bucket\",\n\t\tKey:        \"Asia/\",\n\t}\n\tactualResult := errUnexpectedEOF(100, 101, \"minio-bucket\", \"Asia/\")\n\tif !reflect.DeepEqual(expectedResult, actualResult) {\n\t\tt.Errorf(\"Expected result to be '%#v', but instead got '%#v'\", expectedResult, actualResult)\n\t}\n}\n\n// Test validates 'errInvalidArgument' response.\nfunc TestErrInvalidArgument(t *testing.T) {\n\texpectedResult := ErrorResponse{\n\t\tStatusCode: http.StatusBadRequest,\n\t\tCode:       \"InvalidArgument\",\n\t\tMessage:    \"Invalid Argument\",\n\t\tRequestID:  \"minio\",\n\t}\n\tactualResult := errInvalidArgument(\"Invalid Argument\")\n\tif !reflect.DeepEqual(expectedResult, actualResult) {\n\t\tt.Errorf(\"Expected result to be '%#v', but instead got '%#v'\", expectedResult, actualResult)\n\t}\n}\n\n// Tests if the Message field is missing.\nfunc TestErrWithoutMessage(t *testing.T) {\n\terrResp := ErrorResponse{\n\t\tCode:      \"AccessDenied\",\n\t\tRequestID: \"minio\",\n\t}\n\n\tif errResp.Error() != \"Access Denied.\" {\n\t\tt.Errorf(\"Expected \\\"Access Denied.\\\", got %s\", errResp)\n\t}\n\n\terrResp = ErrorResponse{\n\t\tCode:      \"InvalidArgument\",\n\t\tRequestID: \"minio\",\n\t}\n\tif errResp.Error() != fmt.Sprintf(\"Error response code %s.\", errResp.Code) {\n\t\tt.Errorf(\"Expected \\\"Error response code InvalidArgument.\\\", got \\\"%s\\\"\", errResp)\n\t}\n}\n\n// Tests if ErrorResponse is comparable since it is compared\n// inside golang http code (https://github.com/golang/go/issues/29768)\nfunc TestErrorResponseComparable(t *testing.T) {\n\tvar e1 interface{} = ErrorResponse{}\n\tvar e2 interface{} = ErrorResponse{}\n\tif e1 != e2 {\n\t\tt.Fatalf(\"ErrorResponse should be comparable\")\n\t}\n}\n"
        },
        {
          "name": "api-get-object-acl.go",
          "type": "blob",
          "size": 4.236328125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2018 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n)\n\n// Grantee represents the person being granted permissions.\ntype Grantee struct {\n\tXMLName     xml.Name `xml:\"Grantee\"`\n\tID          string   `xml:\"ID\"`\n\tDisplayName string   `xml:\"DisplayName\"`\n\tURI         string   `xml:\"URI\"`\n}\n\n// Grant holds grant information\ntype Grant struct {\n\tXMLName    xml.Name `xml:\"Grant\"`\n\tGrantee    Grantee\n\tPermission string `xml:\"Permission\"`\n}\n\n// AccessControlList contains the set of grantees and the permissions assigned to each grantee.\ntype AccessControlList struct {\n\tXMLName    xml.Name `xml:\"AccessControlList\"`\n\tGrant      []Grant\n\tPermission string `xml:\"Permission\"`\n}\n\ntype accessControlPolicy struct {\n\tXMLName           xml.Name `xml:\"AccessControlPolicy\"`\n\tOwner             Owner\n\tAccessControlList AccessControlList\n}\n\n// GetObjectACL get object ACLs\nfunc (c *Client) GetObjectACL(ctx context.Context, bucketName, objectName string) (*ObjectInfo, error) {\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName: bucketName,\n\t\tobjectName: objectName,\n\t\tqueryValues: url.Values{\n\t\t\t\"acl\": []string{\"\"},\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer closeResponse(resp)\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t}\n\n\tres := &accessControlPolicy{}\n\n\tif err := xmlDecoder(resp.Body, res); err != nil {\n\t\treturn nil, err\n\t}\n\n\tobjInfo, err := c.StatObject(ctx, bucketName, objectName, StatObjectOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tobjInfo.Owner.DisplayName = res.Owner.DisplayName\n\tobjInfo.Owner.ID = res.Owner.ID\n\n\tobjInfo.Grant = append(objInfo.Grant, res.AccessControlList.Grant...)\n\n\tcannedACL := getCannedACL(res)\n\tif cannedACL != \"\" {\n\t\tobjInfo.Metadata.Add(\"X-Amz-Acl\", cannedACL)\n\t\treturn &objInfo, nil\n\t}\n\n\tgrantACL := getAmzGrantACL(res)\n\tfor k, v := range grantACL {\n\t\tobjInfo.Metadata[k] = v\n\t}\n\n\treturn &objInfo, nil\n}\n\nfunc getCannedACL(aCPolicy *accessControlPolicy) string {\n\tgrants := aCPolicy.AccessControlList.Grant\n\n\tswitch {\n\tcase len(grants) == 1:\n\t\tif grants[0].Grantee.URI == \"\" && grants[0].Permission == \"FULL_CONTROL\" {\n\t\t\treturn \"private\"\n\t\t}\n\tcase len(grants) == 2:\n\t\tfor _, g := range grants {\n\t\t\tif g.Grantee.URI == \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\" && g.Permission == \"READ\" {\n\t\t\t\treturn \"authenticated-read\"\n\t\t\t}\n\t\t\tif g.Grantee.URI == \"http://acs.amazonaws.com/groups/global/AllUsers\" && g.Permission == \"READ\" {\n\t\t\t\treturn \"public-read\"\n\t\t\t}\n\t\t\tif g.Permission == \"READ\" && g.Grantee.ID == aCPolicy.Owner.ID {\n\t\t\t\treturn \"bucket-owner-read\"\n\t\t\t}\n\t\t}\n\tcase len(grants) == 3:\n\t\tfor _, g := range grants {\n\t\t\tif g.Grantee.URI == \"http://acs.amazonaws.com/groups/global/AllUsers\" && g.Permission == \"WRITE\" {\n\t\t\t\treturn \"public-read-write\"\n\t\t\t}\n\t\t}\n\t}\n\treturn \"\"\n}\n\nfunc getAmzGrantACL(aCPolicy *accessControlPolicy) map[string][]string {\n\tgrants := aCPolicy.AccessControlList.Grant\n\tres := map[string][]string{}\n\n\tfor _, g := range grants {\n\t\tswitch {\n\t\tcase g.Permission == \"READ\":\n\t\t\tres[\"X-Amz-Grant-Read\"] = append(res[\"X-Amz-Grant-Read\"], \"id=\"+g.Grantee.ID)\n\t\tcase g.Permission == \"WRITE\":\n\t\t\tres[\"X-Amz-Grant-Write\"] = append(res[\"X-Amz-Grant-Write\"], \"id=\"+g.Grantee.ID)\n\t\tcase g.Permission == \"READ_ACP\":\n\t\t\tres[\"X-Amz-Grant-Read-Acp\"] = append(res[\"X-Amz-Grant-Read-Acp\"], \"id=\"+g.Grantee.ID)\n\t\tcase g.Permission == \"WRITE_ACP\":\n\t\t\tres[\"X-Amz-Grant-Write-Acp\"] = append(res[\"X-Amz-Grant-Write-Acp\"], \"id=\"+g.Grantee.ID)\n\t\tcase g.Permission == \"FULL_CONTROL\":\n\t\t\tres[\"X-Amz-Grant-Full-Control\"] = append(res[\"X-Amz-Grant-Full-Control\"], \"id=\"+g.Grantee.ID)\n\t\t}\n\t}\n\treturn res\n}\n"
        },
        {
          "name": "api-get-object-attributes.go",
          "type": "blob",
          "size": 5.4931640625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// ObjectAttributesOptions are options used for the GetObjectAttributes API\n//\n// - MaxParts\n// How many parts the caller wants to be returned (default: 1000)\n//\n// - VersionID\n// The object version you want to attributes for\n//\n// - PartNumberMarker\n// the listing will start AFTER the part matching PartNumberMarker\n//\n// - ServerSideEncryption\n// The server-side encryption algorithm used when storing this object in Minio\ntype ObjectAttributesOptions struct {\n\tMaxParts             int\n\tVersionID            string\n\tPartNumberMarker     int\n\tServerSideEncryption encrypt.ServerSide\n}\n\n// ObjectAttributes is the response object returned by the GetObjectAttributes API\n//\n// - VersionID\n// The object version\n//\n// - LastModified\n// The last time the object was modified\n//\n// - ObjectAttributesResponse\n// Contains more information about the object\ntype ObjectAttributes struct {\n\tVersionID    string\n\tLastModified time.Time\n\tObjectAttributesResponse\n}\n\n// ObjectAttributesResponse contains details returned by the GetObjectAttributes API\n//\n// Noteworthy fields:\n//\n// - ObjectParts.PartsCount\n// Contains the total part count for the object (not the current response)\n//\n// - ObjectParts.PartNumberMarker\n// Pagination of parts will begin at (but not include) PartNumberMarker\n//\n// - ObjectParts.NextPartNumberMarket\n// The next PartNumberMarker to be used in order to continue pagination\n//\n// - ObjectParts.IsTruncated\n// Indicates if the last part is included in the request (does not check if parts are missing from the start of the list, ONLY the end)\n//\n// - ObjectParts.MaxParts\n// Reflects the MaxParts used by the caller or the default MaxParts value of the API\ntype ObjectAttributesResponse struct {\n\tETag         string `xml:\",omitempty\"`\n\tStorageClass string\n\tObjectSize   int\n\tChecksum     struct {\n\t\tChecksumCRC32  string `xml:\",omitempty\"`\n\t\tChecksumCRC32C string `xml:\",omitempty\"`\n\t\tChecksumSHA1   string `xml:\",omitempty\"`\n\t\tChecksumSHA256 string `xml:\",omitempty\"`\n\t}\n\tObjectParts struct {\n\t\tPartsCount           int\n\t\tPartNumberMarker     int\n\t\tNextPartNumberMarker int\n\t\tMaxParts             int\n\t\tIsTruncated          bool\n\t\tParts                []*ObjectAttributePart `xml:\"Part\"`\n\t}\n}\n\n// ObjectAttributePart is used by ObjectAttributesResponse to describe an object part\ntype ObjectAttributePart struct {\n\tChecksumCRC32  string `xml:\",omitempty\"`\n\tChecksumCRC32C string `xml:\",omitempty\"`\n\tChecksumSHA1   string `xml:\",omitempty\"`\n\tChecksumSHA256 string `xml:\",omitempty\"`\n\tPartNumber     int\n\tSize           int\n}\n\nfunc (o *ObjectAttributes) parseResponse(resp *http.Response) (err error) {\n\tmod, err := parseRFC7231Time(resp.Header.Get(\"Last-Modified\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\to.LastModified = mod\n\to.VersionID = resp.Header.Get(amzVersionID)\n\n\tresponse := new(ObjectAttributesResponse)\n\tif err := xml.NewDecoder(resp.Body).Decode(response); err != nil {\n\t\treturn err\n\t}\n\to.ObjectAttributesResponse = *response\n\n\treturn\n}\n\n// GetObjectAttributes API combines HeadObject and ListParts.\n// More details on usage can be found in the documentation for ObjectAttributesOptions{}\nfunc (c *Client) GetObjectAttributes(ctx context.Context, bucketName, objectName string, opts ObjectAttributesOptions) (*ObjectAttributes, error) {\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\n\turlValues := make(url.Values)\n\turlValues.Add(\"attributes\", \"\")\n\tif opts.VersionID != \"\" {\n\t\turlValues.Add(\"versionId\", opts.VersionID)\n\t}\n\n\theaders := make(http.Header)\n\theaders.Set(amzObjectAttributes, GetObjectAttributesTags)\n\n\tif opts.PartNumberMarker > 0 {\n\t\theaders.Set(amzPartNumberMarker, strconv.Itoa(opts.PartNumberMarker))\n\t}\n\n\tif opts.MaxParts > 0 {\n\t\theaders.Set(amzMaxParts, strconv.Itoa(opts.MaxParts))\n\t} else {\n\t\theaders.Set(amzMaxParts, strconv.Itoa(GetObjectAttributesMaxParts))\n\t}\n\n\tif opts.ServerSideEncryption != nil {\n\t\topts.ServerSideEncryption.Marshal(headers)\n\t}\n\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     headers,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdefer closeResponse(resp)\n\n\thasEtag := resp.Header.Get(ETag)\n\tif hasEtag != \"\" {\n\t\treturn nil, errors.New(\"getObjectAttributes is not supported by the current endpoint version\")\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tER := new(ErrorResponse)\n\t\tif err := xml.NewDecoder(resp.Body).Decode(ER); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn nil, *ER\n\t}\n\n\tOA := new(ObjectAttributes)\n\terr = OA.parseResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn OA, nil\n}\n"
        },
        {
          "name": "api-get-object-file.go",
          "type": "blob",
          "size": 3.376953125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// FGetObject - download contents of an object to a local file.\n// The options can be used to specify the GET request further.\nfunc (c *Client) FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts GetObjectOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\t// Verify if destination already exists.\n\tst, err := os.Stat(filePath)\n\tif err == nil {\n\t\t// If the destination exists and is a directory.\n\t\tif st.IsDir() {\n\t\t\treturn errInvalidArgument(\"fileName is a directory.\")\n\t\t}\n\t}\n\n\t// Proceed if file does not exist. return for all other errors.\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Extract top level directory.\n\tobjectDir, _ := filepath.Split(filePath)\n\tif objectDir != \"\" {\n\t\t// Create any missing top level directories.\n\t\tif err := os.MkdirAll(objectDir, 0o700); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Gather md5sum.\n\tobjectStat, err := c.StatObject(ctx, bucketName, objectName, StatObjectOptions(opts))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Write to a temporary file \"fileName.part.minio\" before saving.\n\tfilePartPath := filePath + sum256Hex([]byte(objectStat.ETag)) + \".part.minio\"\n\n\t// If exists, open in append mode. If not create it as a part file.\n\tfilePart, err := os.OpenFile(filePartPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o600)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If we return early with an error, be sure to close and delete\n\t// filePart.  If we have an error along the way there is a chance\n\t// that filePart is somehow damaged, and we should discard it.\n\tcloseAndRemove := true\n\tdefer func() {\n\t\tif closeAndRemove {\n\t\t\t_ = filePart.Close()\n\t\t\t_ = os.Remove(filePartPath)\n\t\t}\n\t}()\n\n\t// Issue Stat to get the current offset.\n\tst, err = filePart.Stat()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Initialize get object request headers to set the\n\t// appropriate range offsets to read from.\n\tif st.Size() > 0 {\n\t\topts.SetRange(st.Size(), 0)\n\t}\n\n\t// Seek to current position for incoming reader.\n\tobjectReader, objectStat, _, err := c.getObject(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Write to the part file.\n\tif _, err = io.CopyN(filePart, objectReader, objectStat.Size); err != nil {\n\t\treturn err\n\t}\n\n\t// Close the file before rename, this is specifically needed for Windows users.\n\tcloseAndRemove = false\n\tif err = filePart.Close(); err != nil {\n\t\treturn err\n\t}\n\n\t// Safely completed. Now commit by renaming to actual filename.\n\tif err = os.Rename(filePartPath, filePath); err != nil {\n\t\treturn err\n\t}\n\n\t// Return.\n\treturn nil\n}\n"
        },
        {
          "name": "api-get-object.go",
          "type": "blob",
          "size": 20.3525390625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"sync\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// GetObject wrapper function that accepts a request context\nfunc (c *Client) GetObject(ctx context.Context, bucketName, objectName string, opts GetObjectOptions) (*Object, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"InvalidBucketName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"XMinioInvalidObjectName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\n\tgctx, cancel := context.WithCancel(ctx)\n\n\t// Detect if snowball is server location we are talking to.\n\tvar snowball bool\n\tif location, ok := c.bucketLocCache.Get(bucketName); ok {\n\t\tsnowball = location == \"snowball\"\n\t}\n\n\tvar (\n\t\terr        error\n\t\thttpReader io.ReadCloser\n\t\tobjectInfo ObjectInfo\n\t\ttotalRead  int\n\t)\n\n\t// Create request channel.\n\treqCh := make(chan getRequest)\n\t// Create response channel.\n\tresCh := make(chan getResponse)\n\n\t// This routine feeds partial object data as and when the caller reads.\n\tgo func() {\n\t\tdefer close(resCh)\n\t\tdefer func() {\n\t\t\t// Close the http response body before returning.\n\t\t\t// This ends the connection with the server.\n\t\t\tif httpReader != nil {\n\t\t\t\thttpReader.Close()\n\t\t\t}\n\t\t}()\n\t\tdefer cancel()\n\n\t\t// Used to verify if etag of object has changed since last read.\n\t\tvar etag string\n\n\t\tfor req := range reqCh {\n\t\t\t// If this is the first request we may not need to do a getObject request yet.\n\t\t\tif req.isFirstReq {\n\t\t\t\t// First request is a Read/ReadAt.\n\t\t\t\tif req.isReadOp {\n\t\t\t\t\t// Differentiate between wanting the whole object and just a range.\n\t\t\t\t\tif req.isReadAt {\n\t\t\t\t\t\t// If this is a ReadAt request only get the specified range.\n\t\t\t\t\t\t// Range is set with respect to the offset and length of the buffer requested.\n\t\t\t\t\t\t// Do not set objectInfo from the first readAt request because it will not get\n\t\t\t\t\t\t// the whole object.\n\t\t\t\t\t\topts.SetRange(req.Offset, req.Offset+int64(len(req.Buffer))-1)\n\t\t\t\t\t} else if req.Offset > 0 {\n\t\t\t\t\t\topts.SetRange(req.Offset, 0)\n\t\t\t\t\t}\n\t\t\t\t\thttpReader, objectInfo, _, err = c.getObject(gctx, bucketName, objectName, opts)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tresCh <- getResponse{Error: err}\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tetag = objectInfo.ETag\n\t\t\t\t\t// Read at least firstReq.Buffer bytes, if not we have\n\t\t\t\t\t// reached our EOF.\n\t\t\t\t\tsize, err := readFull(httpReader, req.Buffer)\n\t\t\t\t\ttotalRead += size\n\t\t\t\t\tif size > 0 && err == io.ErrUnexpectedEOF {\n\t\t\t\t\t\tif int64(size) < objectInfo.Size {\n\t\t\t\t\t\t\t// In situations when returned size\n\t\t\t\t\t\t\t// is less than the expected content\n\t\t\t\t\t\t\t// length set by the server, make sure\n\t\t\t\t\t\t\t// we return io.ErrUnexpectedEOF\n\t\t\t\t\t\t\terr = io.ErrUnexpectedEOF\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// If an EOF happens after reading some but not\n\t\t\t\t\t\t\t// all the bytes ReadFull returns ErrUnexpectedEOF\n\t\t\t\t\t\t\terr = io.EOF\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if size == 0 && err == io.EOF && objectInfo.Size > 0 {\n\t\t\t\t\t\t// Special cases when server writes more data\n\t\t\t\t\t\t// than the content-length, net/http response\n\t\t\t\t\t\t// body returns an error, instead of converting\n\t\t\t\t\t\t// it to io.EOF - return unexpected EOF.\n\t\t\t\t\t\terr = io.ErrUnexpectedEOF\n\t\t\t\t\t}\n\t\t\t\t\t// Send back the first response.\n\t\t\t\t\tresCh <- getResponse{\n\t\t\t\t\t\tobjectInfo: objectInfo,\n\t\t\t\t\t\tSize:       size,\n\t\t\t\t\t\tError:      err,\n\t\t\t\t\t\tdidRead:    true,\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// First request is a Stat or Seek call.\n\t\t\t\t\t// Only need to run a StatObject until an actual Read or ReadAt request comes through.\n\n\t\t\t\t\t// Remove range header if already set, for stat Operations to get original file size.\n\t\t\t\t\tdelete(opts.headers, \"Range\")\n\t\t\t\t\tobjectInfo, err = c.StatObject(gctx, bucketName, objectName, StatObjectOptions(opts))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tresCh <- getResponse{\n\t\t\t\t\t\t\tError: err,\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Exit the go-routine.\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tetag = objectInfo.ETag\n\t\t\t\t\t// Send back the first response.\n\t\t\t\t\tresCh <- getResponse{\n\t\t\t\t\t\tobjectInfo: objectInfo,\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if req.settingObjectInfo { // Request is just to get objectInfo.\n\t\t\t\t// Remove range header if already set, for stat Operations to get original file size.\n\t\t\t\tdelete(opts.headers, \"Range\")\n\t\t\t\t// Check whether this is snowball\n\t\t\t\t// if yes do not use If-Match feature\n\t\t\t\t// it doesn't work.\n\t\t\t\tif etag != \"\" && !snowball {\n\t\t\t\t\topts.SetMatchETag(etag)\n\t\t\t\t}\n\t\t\t\tobjectInfo, err := c.StatObject(gctx, bucketName, objectName, StatObjectOptions(opts))\n\t\t\t\tif err != nil {\n\t\t\t\t\tresCh <- getResponse{\n\t\t\t\t\t\tError: err,\n\t\t\t\t\t}\n\t\t\t\t\t// Exit the goroutine.\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t// Send back the objectInfo.\n\t\t\t\tresCh <- getResponse{\n\t\t\t\t\tobjectInfo: objectInfo,\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Offset changes fetch the new object at an Offset.\n\t\t\t\t// Because the httpReader may not be set by the first\n\t\t\t\t// request if it was a stat or seek it must be checked\n\t\t\t\t// if the object has been read or not to only initialize\n\t\t\t\t// new ones when they haven't been already.\n\t\t\t\t// All readAt requests are new requests.\n\t\t\t\tif req.DidOffsetChange || !req.beenRead {\n\t\t\t\t\t// Check whether this is snowball\n\t\t\t\t\t// if yes do not use If-Match feature\n\t\t\t\t\t// it doesn't work.\n\t\t\t\t\tif etag != \"\" && !snowball {\n\t\t\t\t\t\topts.SetMatchETag(etag)\n\t\t\t\t\t}\n\t\t\t\t\tif httpReader != nil {\n\t\t\t\t\t\t// Close previously opened http reader.\n\t\t\t\t\t\thttpReader.Close()\n\t\t\t\t\t}\n\t\t\t\t\t// If this request is a readAt only get the specified range.\n\t\t\t\t\tif req.isReadAt {\n\t\t\t\t\t\t// Range is set with respect to the offset and length of the buffer requested.\n\t\t\t\t\t\topts.SetRange(req.Offset, req.Offset+int64(len(req.Buffer))-1)\n\t\t\t\t\t} else if req.Offset > 0 { // Range is set with respect to the offset.\n\t\t\t\t\t\topts.SetRange(req.Offset, 0)\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Remove range header if already set\n\t\t\t\t\t\tdelete(opts.headers, \"Range\")\n\t\t\t\t\t}\n\t\t\t\t\thttpReader, objectInfo, _, err = c.getObject(gctx, bucketName, objectName, opts)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tresCh <- getResponse{\n\t\t\t\t\t\t\tError: err,\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\ttotalRead = 0\n\t\t\t\t}\n\n\t\t\t\t// Read at least req.Buffer bytes, if not we have\n\t\t\t\t// reached our EOF.\n\t\t\t\tsize, err := readFull(httpReader, req.Buffer)\n\t\t\t\ttotalRead += size\n\t\t\t\tif size > 0 && err == io.ErrUnexpectedEOF {\n\t\t\t\t\tif int64(totalRead) < objectInfo.Size {\n\t\t\t\t\t\t// In situations when returned size\n\t\t\t\t\t\t// is less than the expected content\n\t\t\t\t\t\t// length set by the server, make sure\n\t\t\t\t\t\t// we return io.ErrUnexpectedEOF\n\t\t\t\t\t\terr = io.ErrUnexpectedEOF\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// If an EOF happens after reading some but not\n\t\t\t\t\t\t// all the bytes ReadFull returns ErrUnexpectedEOF\n\t\t\t\t\t\terr = io.EOF\n\t\t\t\t\t}\n\t\t\t\t} else if size == 0 && err == io.EOF && objectInfo.Size > 0 {\n\t\t\t\t\t// Special cases when server writes more data\n\t\t\t\t\t// than the content-length, net/http response\n\t\t\t\t\t// body returns an error, instead of converting\n\t\t\t\t\t// it to io.EOF - return unexpected EOF.\n\t\t\t\t\terr = io.ErrUnexpectedEOF\n\t\t\t\t}\n\n\t\t\t\t// Reply back how much was read.\n\t\t\t\tresCh <- getResponse{\n\t\t\t\t\tSize:       size,\n\t\t\t\t\tError:      err,\n\t\t\t\t\tdidRead:    true,\n\t\t\t\t\tobjectInfo: objectInfo,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Create a newObject through the information sent back by reqCh.\n\treturn newObject(gctx, cancel, reqCh, resCh), nil\n}\n\n// get request message container to communicate with internal\n// go-routine.\ntype getRequest struct {\n\tBuffer            []byte\n\tOffset            int64 // readAt offset.\n\tDidOffsetChange   bool  // Tracks the offset changes for Seek requests.\n\tbeenRead          bool  // Determines if this is the first time an object is being read.\n\tisReadAt          bool  // Determines if this request is a request to a specific range\n\tisReadOp          bool  // Determines if this request is a Read or Read/At request.\n\tisFirstReq        bool  // Determines if this request is the first time an object is being accessed.\n\tsettingObjectInfo bool  // Determines if this request is to set the objectInfo of an object.\n}\n\n// get response message container to reply back for the request.\ntype getResponse struct {\n\tSize       int\n\tError      error\n\tdidRead    bool       // Lets subsequent calls know whether or not httpReader has been initiated.\n\tobjectInfo ObjectInfo // Used for the first request.\n}\n\n// Object represents an open object. It implements\n// Reader, ReaderAt, Seeker, Closer for a HTTP stream.\ntype Object struct {\n\t// Mutex.\n\tmutex *sync.Mutex\n\n\t// User allocated and defined.\n\treqCh      chan<- getRequest\n\tresCh      <-chan getResponse\n\tctx        context.Context\n\tcancel     context.CancelFunc\n\tcurrOffset int64\n\tobjectInfo ObjectInfo\n\n\t// Ask lower level to initiate data fetching based on currOffset\n\tseekData bool\n\n\t// Keeps track of closed call.\n\tisClosed bool\n\n\t// Keeps track of if this is the first call.\n\tisStarted bool\n\n\t// Previous error saved for future calls.\n\tprevErr error\n\n\t// Keeps track of if this object has been read yet.\n\tbeenRead bool\n\n\t// Keeps track of if objectInfo has been set yet.\n\tobjectInfoSet bool\n}\n\n// doGetRequest - sends and blocks on the firstReqCh and reqCh of an object.\n// Returns back the size of the buffer read, if anything was read, as well\n// as any error encountered. For all first requests sent on the object\n// it is also responsible for sending back the objectInfo.\nfunc (o *Object) doGetRequest(request getRequest) (getResponse, error) {\n\tselect {\n\tcase <-o.ctx.Done():\n\t\treturn getResponse{}, o.ctx.Err()\n\tcase o.reqCh <- request:\n\t}\n\n\tresponse := <-o.resCh\n\n\t// Return any error to the top level.\n\tif response.Error != nil && response.Error != io.EOF {\n\t\treturn response, response.Error\n\t}\n\n\t// This was the first request.\n\tif !o.isStarted {\n\t\t// The object has been operated on.\n\t\to.isStarted = true\n\t}\n\t// Set the objectInfo if the request was not readAt\n\t// and it hasn't been set before.\n\tif !o.objectInfoSet && !request.isReadAt {\n\t\to.objectInfo = response.objectInfo\n\t\to.objectInfoSet = true\n\t}\n\t// Set beenRead only if it has not been set before.\n\tif !o.beenRead {\n\t\to.beenRead = response.didRead\n\t}\n\t// Data are ready on the wire, no need to reinitiate connection in lower level\n\to.seekData = false\n\n\treturn response, response.Error\n}\n\n// setOffset - handles the setting of offsets for\n// Read/ReadAt/Seek requests.\nfunc (o *Object) setOffset(bytesRead int64) error {\n\t// Update the currentOffset.\n\to.currOffset += bytesRead\n\n\tif o.objectInfo.Size > -1 && o.currOffset >= o.objectInfo.Size {\n\t\treturn io.EOF\n\t}\n\treturn nil\n}\n\n// Read reads up to len(b) bytes into b. It returns the number of\n// bytes read (0 <= n <= len(b)) and any error encountered. Returns\n// io.EOF upon end of file.\nfunc (o *Object) Read(b []byte) (n int, err error) {\n\tif o == nil {\n\t\treturn 0, errInvalidArgument(\"Object is nil\")\n\t}\n\n\t// Locking.\n\to.mutex.Lock()\n\tdefer o.mutex.Unlock()\n\n\t// prevErr is previous error saved from previous operation.\n\tif o.prevErr != nil || o.isClosed {\n\t\treturn 0, o.prevErr\n\t}\n\n\t// Create a new request.\n\treadReq := getRequest{\n\t\tisReadOp: true,\n\t\tbeenRead: o.beenRead,\n\t\tBuffer:   b,\n\t}\n\n\t// Alert that this is the first request.\n\tif !o.isStarted {\n\t\treadReq.isFirstReq = true\n\t}\n\n\t// Ask to establish a new data fetch routine based on seekData flag\n\treadReq.DidOffsetChange = o.seekData\n\treadReq.Offset = o.currOffset\n\n\t// Send and receive from the first request.\n\tresponse, err := o.doGetRequest(readReq)\n\tif err != nil && err != io.EOF {\n\t\t// Save the error for future calls.\n\t\to.prevErr = err\n\t\treturn response.Size, err\n\t}\n\n\t// Bytes read.\n\tbytesRead := int64(response.Size)\n\n\t// Set the new offset.\n\toerr := o.setOffset(bytesRead)\n\tif oerr != nil {\n\t\t// Save the error for future calls.\n\t\to.prevErr = oerr\n\t\treturn response.Size, oerr\n\t}\n\n\t// Return the response.\n\treturn response.Size, err\n}\n\n// Stat returns the ObjectInfo structure describing Object.\nfunc (o *Object) Stat() (ObjectInfo, error) {\n\tif o == nil {\n\t\treturn ObjectInfo{}, errInvalidArgument(\"Object is nil\")\n\t}\n\t// Locking.\n\to.mutex.Lock()\n\tdefer o.mutex.Unlock()\n\n\tif o.prevErr != nil && o.prevErr != io.EOF || o.isClosed {\n\t\treturn ObjectInfo{}, o.prevErr\n\t}\n\n\t// This is the first request.\n\tif !o.isStarted || !o.objectInfoSet {\n\t\t// Send the request and get the response.\n\t\t_, err := o.doGetRequest(getRequest{\n\t\t\tisFirstReq:        !o.isStarted,\n\t\t\tsettingObjectInfo: !o.objectInfoSet,\n\t\t})\n\t\tif err != nil {\n\t\t\to.prevErr = err\n\t\t\treturn ObjectInfo{}, err\n\t\t}\n\t}\n\n\treturn o.objectInfo, nil\n}\n\n// ReadAt reads len(b) bytes from the File starting at byte offset\n// off. It returns the number of bytes read and the error, if any.\n// ReadAt always returns a non-nil error when n < len(b). At end of\n// file, that error is io.EOF.\nfunc (o *Object) ReadAt(b []byte, offset int64) (n int, err error) {\n\tif o == nil {\n\t\treturn 0, errInvalidArgument(\"Object is nil\")\n\t}\n\n\t// Locking.\n\to.mutex.Lock()\n\tdefer o.mutex.Unlock()\n\n\t// prevErr is error which was saved in previous operation.\n\tif o.prevErr != nil && o.prevErr != io.EOF || o.isClosed {\n\t\treturn 0, o.prevErr\n\t}\n\n\t// Set the current offset to ReadAt offset, because the current offset will be shifted at the end of this method.\n\to.currOffset = offset\n\n\t// Can only compare offsets to size when size has been set.\n\tif o.objectInfoSet {\n\t\t// If offset is negative than we return io.EOF.\n\t\t// If offset is greater than or equal to object size we return io.EOF.\n\t\tif (o.objectInfo.Size > -1 && offset >= o.objectInfo.Size) || offset < 0 {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t}\n\n\t// Create the new readAt request.\n\treadAtReq := getRequest{\n\t\tisReadOp:        true,\n\t\tisReadAt:        true,\n\t\tDidOffsetChange: true,       // Offset always changes.\n\t\tbeenRead:        o.beenRead, // Set if this is the first request to try and read.\n\t\tOffset:          offset,     // Set the offset.\n\t\tBuffer:          b,\n\t}\n\n\t// Alert that this is the first request.\n\tif !o.isStarted {\n\t\treadAtReq.isFirstReq = true\n\t}\n\n\t// Send and receive from the first request.\n\tresponse, err := o.doGetRequest(readAtReq)\n\tif err != nil && err != io.EOF {\n\t\t// Save the error.\n\t\to.prevErr = err\n\t\treturn response.Size, err\n\t}\n\t// Bytes read.\n\tbytesRead := int64(response.Size)\n\t// There is no valid objectInfo yet\n\t// \tto compare against for EOF.\n\tif !o.objectInfoSet {\n\t\t// Update the currentOffset.\n\t\to.currOffset += bytesRead\n\t} else {\n\t\t// If this was not the first request update\n\t\t// the offsets and compare against objectInfo\n\t\t// for EOF.\n\t\toerr := o.setOffset(bytesRead)\n\t\tif oerr != nil {\n\t\t\to.prevErr = oerr\n\t\t\treturn response.Size, oerr\n\t\t}\n\t}\n\treturn response.Size, err\n}\n\n// Seek sets the offset for the next Read or Write to offset,\n// interpreted according to whence: 0 means relative to the\n// origin of the file, 1 means relative to the current offset,\n// and 2 means relative to the end.\n// Seek returns the new offset and an error, if any.\n//\n// Seeking to a negative offset is an error. Seeking to any positive\n// offset is legal, subsequent io operations succeed until the\n// underlying object is not closed.\nfunc (o *Object) Seek(offset int64, whence int) (n int64, err error) {\n\tif o == nil {\n\t\treturn 0, errInvalidArgument(\"Object is nil\")\n\t}\n\n\t// Locking.\n\to.mutex.Lock()\n\tdefer o.mutex.Unlock()\n\n\t// At EOF seeking is legal allow only io.EOF, for any other errors we return.\n\tif o.prevErr != nil && o.prevErr != io.EOF {\n\t\treturn 0, o.prevErr\n\t}\n\n\t// Negative offset is valid for whence of '2'.\n\tif offset < 0 && whence != 2 {\n\t\treturn 0, errInvalidArgument(fmt.Sprintf(\"Negative position not allowed for %d\", whence))\n\t}\n\n\t// This is the first request. So before anything else\n\t// get the ObjectInfo.\n\tif !o.isStarted || !o.objectInfoSet {\n\t\t// Create the new Seek request.\n\t\tseekReq := getRequest{\n\t\t\tisReadOp:   false,\n\t\t\tOffset:     offset,\n\t\t\tisFirstReq: true,\n\t\t}\n\t\t// Send and receive from the seek request.\n\t\t_, err := o.doGetRequest(seekReq)\n\t\tif err != nil {\n\t\t\t// Save the error.\n\t\t\to.prevErr = err\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\tnewOffset := o.currOffset\n\n\t// Switch through whence.\n\tswitch whence {\n\tdefault:\n\t\treturn 0, errInvalidArgument(fmt.Sprintf(\"Invalid whence %d\", whence))\n\tcase 0:\n\t\tif o.objectInfo.Size > -1 && offset > o.objectInfo.Size {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t\tnewOffset = offset\n\tcase 1:\n\t\tif o.objectInfo.Size > -1 && o.currOffset+offset > o.objectInfo.Size {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t\tnewOffset += offset\n\tcase 2:\n\t\t// If we don't know the object size return an error for io.SeekEnd\n\t\tif o.objectInfo.Size < 0 {\n\t\t\treturn 0, errInvalidArgument(\"Whence END is not supported when the object size is unknown\")\n\t\t}\n\t\t// Seeking to positive offset is valid for whence '2', but\n\t\t// since we are backing a Reader we have reached 'EOF' if\n\t\t// offset is positive.\n\t\tif offset > 0 {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t\t// Seeking to negative position not allowed for whence.\n\t\tif o.objectInfo.Size+offset < 0 {\n\t\t\treturn 0, errInvalidArgument(fmt.Sprintf(\"Seeking at negative offset not allowed for %d\", whence))\n\t\t}\n\t\tnewOffset = o.objectInfo.Size + offset\n\t}\n\t// Reset the saved error since we successfully seeked, let the Read\n\t// and ReadAt decide.\n\tif o.prevErr == io.EOF {\n\t\to.prevErr = nil\n\t}\n\n\t// Ask lower level to fetch again from source when necessary\n\to.seekData = (newOffset != o.currOffset) || o.seekData\n\to.currOffset = newOffset\n\n\t// Return the effective offset.\n\treturn o.currOffset, nil\n}\n\n// Close - The behavior of Close after the first call returns error\n// for subsequent Close() calls.\nfunc (o *Object) Close() (err error) {\n\tif o == nil {\n\t\treturn errInvalidArgument(\"Object is nil\")\n\t}\n\n\t// Locking.\n\to.mutex.Lock()\n\tdefer o.mutex.Unlock()\n\n\t// if already closed return an error.\n\tif o.isClosed {\n\t\treturn o.prevErr\n\t}\n\n\t// Close successfully.\n\to.cancel()\n\n\t// Close the request channel to indicate the internal go-routine to exit.\n\tclose(o.reqCh)\n\n\t// Save for future operations.\n\terrMsg := \"Object is already closed. Bad file descriptor.\"\n\to.prevErr = errors.New(errMsg)\n\t// Save here that we closed done channel successfully.\n\to.isClosed = true\n\treturn nil\n}\n\n// newObject instantiates a new *minio.Object*\n// ObjectInfo will be set by setObjectInfo\nfunc newObject(ctx context.Context, cancel context.CancelFunc, reqCh chan<- getRequest, resCh <-chan getResponse) *Object {\n\treturn &Object{\n\t\tctx:    ctx,\n\t\tcancel: cancel,\n\t\tmutex:  &sync.Mutex{},\n\t\treqCh:  reqCh,\n\t\tresCh:  resCh,\n\t}\n}\n\n// getObject - retrieve object from Object Storage.\n//\n// Additionally this function also takes range arguments to download the specified\n// range bytes of an object. Setting offset and length = 0 will download the full object.\n//\n// For more information about the HTTP Range header.\n// go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35.\nfunc (c *Client) getObject(ctx context.Context, bucketName, objectName string, opts GetObjectOptions) (io.ReadCloser, ObjectInfo, http.Header, error) {\n\t// Validate input arguments.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, ObjectInfo{}, nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"InvalidBucketName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, ObjectInfo{}, nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"XMinioInvalidObjectName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\n\t// Execute GET on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      opts.toQueryValues(),\n\t\tcustomHeader:     opts.Header(),\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tif err != nil {\n\t\treturn nil, ObjectInfo{}, nil, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n\t\t\treturn nil, ObjectInfo{}, nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\n\tobjectStat, err := ToObjectInfo(bucketName, objectName, resp.Header)\n\tif err != nil {\n\t\tcloseResponse(resp)\n\t\treturn nil, ObjectInfo{}, nil, err\n\t}\n\n\t// do not close body here, caller will close\n\treturn resp.Body, objectStat, resp.Header, nil\n}\n"
        },
        {
          "name": "api-get-object_test.go",
          "type": "blob",
          "size": 4.0654296875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2021 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"crypto/rand\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"testing\"\n)\n\nfunc TestGetObjectReturnSuccess(t *testing.T) {\n\tsrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.Header().Set(\"Last-Modified\", \"Wed, 21 Oct 2015 07:28:00 GMT\")\n\t\tw.Header().Set(\"Content-Length\", \"5\")\n\n\t\t// Write less bytes than the content length.\n\t\tw.Write([]byte(\"12345\"))\n\t}))\n\tdefer srv.Close()\n\n\t// New - instantiate minio client with options\n\tclnt, err := New(srv.Listener.Addr().String(), &Options{\n\t\tRegion: \"us-east-1\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := clnt.GetObject(context.Background(), \"bucketName\", \"objectName\", GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// We expect an error when reading back.\n\tbuf, err := io.ReadAll(obj)\n\tif err != nil {\n\t\tt.Fatalf(\"Expected 'nil', got %v\", err)\n\t}\n\n\tif len(buf) != 5 {\n\t\tt.Fatalf(\"Expected read bytes '5', got %v\", len(buf))\n\t}\n}\n\nfunc TestGetObjectReturnErrorIfServerTruncatesResponse(t *testing.T) {\n\tsrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.Header().Set(\"Last-Modified\", \"Wed, 21 Oct 2015 07:28:00 GMT\")\n\t\tw.Header().Set(\"Content-Length\", \"100\")\n\n\t\t// Write less bytes than the content length.\n\t\tw.Write([]byte(\"12345\"))\n\t}))\n\tdefer srv.Close()\n\n\t// New - instantiate minio client with options\n\tclnt, err := New(srv.Listener.Addr().String(), &Options{\n\t\tRegion: \"us-east-1\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := clnt.GetObject(context.Background(), \"bucketName\", \"objectName\", GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// We expect an error when reading back.\n\tif _, err = io.ReadAll(obj); err != io.ErrUnexpectedEOF {\n\t\tt.Fatalf(\"Expected %v, got %v\", io.ErrUnexpectedEOF, err)\n\t}\n}\n\nfunc TestGetObjectReturnErrorIfServerTruncatesResponseDouble(t *testing.T) {\n\tsrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.Header().Set(\"Last-Modified\", \"Wed, 21 Oct 2015 07:28:00 GMT\")\n\t\tw.Header().Set(\"Content-Length\", \"1024\")\n\n\t\t// Write less bytes than the content length.\n\t\tio.Copy(w, io.LimitReader(rand.Reader, 1023))\n\t}))\n\tdefer srv.Close()\n\n\t// New - instantiate minio client with options\n\tclnt, err := New(srv.Listener.Addr().String(), &Options{\n\t\tRegion: \"us-east-1\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := clnt.GetObject(context.Background(), \"bucketName\", \"objectName\", GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// We expect an error when reading back.\n\tif _, err = io.ReadAll(obj); err != io.ErrUnexpectedEOF {\n\t\tt.Fatalf(\"Expected %v, got %v\", io.ErrUnexpectedEOF, err)\n\t}\n}\n\nfunc TestGetObjectReturnErrorIfServerSendsMore(t *testing.T) {\n\tsrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.Header().Set(\"Last-Modified\", \"Wed, 21 Oct 2015 07:28:00 GMT\")\n\t\tw.Header().Set(\"Content-Length\", \"1\")\n\n\t\t// Write less bytes than the content length.\n\t\tw.Write([]byte(\"12345\"))\n\t}))\n\tdefer srv.Close()\n\n\t// New - instantiate minio client with options\n\tclnt, err := New(srv.Listener.Addr().String(), &Options{\n\t\tRegion: \"us-east-1\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tobj, err := clnt.GetObject(context.Background(), \"bucketName\", \"objectName\", GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// We expect an error when reading back.\n\tif _, err = io.ReadAll(obj); err != io.ErrUnexpectedEOF {\n\t\tt.Fatalf(\"Expected %v, got %v\", io.ErrUnexpectedEOF, err)\n\t}\n}\n"
        },
        {
          "name": "api-get-options.go",
          "type": "blob",
          "size": 5.9697265625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\n// AdvancedGetOptions for internal use by MinIO server - not intended for client use.\ntype AdvancedGetOptions struct {\n\tReplicationDeleteMarker           bool\n\tIsReplicationReadyForDeleteMarker bool\n\tReplicationProxyRequest           string\n}\n\n// GetObjectOptions are used to specify additional headers or options\n// during GET requests.\ntype GetObjectOptions struct {\n\theaders              map[string]string\n\treqParams            url.Values\n\tServerSideEncryption encrypt.ServerSide\n\tVersionID            string\n\tPartNumber           int\n\n\t// Include any checksums, if object was uploaded with checksum.\n\t// For multipart objects this is a checksum of part checksums.\n\t// https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\n\tChecksum bool\n\n\t// To be not used by external applications\n\tInternal AdvancedGetOptions\n}\n\n// StatObjectOptions are used to specify additional headers or options\n// during GET info/stat requests.\ntype StatObjectOptions = GetObjectOptions\n\n// Header returns the http.Header representation of the GET options.\nfunc (o GetObjectOptions) Header() http.Header {\n\theaders := make(http.Header, len(o.headers))\n\tfor k, v := range o.headers {\n\t\theaders.Set(k, v)\n\t}\n\tif o.ServerSideEncryption != nil && o.ServerSideEncryption.Type() == encrypt.SSEC {\n\t\to.ServerSideEncryption.Marshal(headers)\n\t}\n\t// this header is set for active-active replication scenario where GET/HEAD\n\t// to site A is proxy'd to site B if object/version missing on site A.\n\tif o.Internal.ReplicationProxyRequest != \"\" {\n\t\theaders.Set(minIOBucketReplicationProxyRequest, o.Internal.ReplicationProxyRequest)\n\t}\n\tif o.Checksum {\n\t\theaders.Set(\"x-amz-checksum-mode\", \"ENABLED\")\n\t}\n\treturn headers\n}\n\n// Set adds a key value pair to the options. The\n// key-value pair will be part of the HTTP GET request\n// headers.\nfunc (o *GetObjectOptions) Set(key, value string) {\n\tif o.headers == nil {\n\t\to.headers = make(map[string]string)\n\t}\n\to.headers[http.CanonicalHeaderKey(key)] = value\n}\n\n// SetReqParam - set request query string parameter\n// supported key: see supportedQueryValues and allowedCustomQueryPrefix.\n// If an unsupported key is passed in, it will be ignored and nothing will be done.\nfunc (o *GetObjectOptions) SetReqParam(key, value string) {\n\tif !isCustomQueryValue(key) && !isStandardQueryValue(key) {\n\t\t// do nothing\n\t\treturn\n\t}\n\tif o.reqParams == nil {\n\t\to.reqParams = make(url.Values)\n\t}\n\to.reqParams.Set(key, value)\n}\n\n// AddReqParam - add request query string parameter\n// supported key: see supportedQueryValues and allowedCustomQueryPrefix.\n// If an unsupported key is passed in, it will be ignored and nothing will be done.\nfunc (o *GetObjectOptions) AddReqParam(key, value string) {\n\tif !isCustomQueryValue(key) && !isStandardQueryValue(key) {\n\t\t// do nothing\n\t\treturn\n\t}\n\tif o.reqParams == nil {\n\t\to.reqParams = make(url.Values)\n\t}\n\to.reqParams.Add(key, value)\n}\n\n// SetMatchETag - set match etag.\nfunc (o *GetObjectOptions) SetMatchETag(etag string) error {\n\tif etag == \"\" {\n\t\treturn errInvalidArgument(\"ETag cannot be empty.\")\n\t}\n\to.Set(\"If-Match\", \"\\\"\"+etag+\"\\\"\")\n\treturn nil\n}\n\n// SetMatchETagExcept - set match etag except.\nfunc (o *GetObjectOptions) SetMatchETagExcept(etag string) error {\n\tif etag == \"\" {\n\t\treturn errInvalidArgument(\"ETag cannot be empty.\")\n\t}\n\to.Set(\"If-None-Match\", \"\\\"\"+etag+\"\\\"\")\n\treturn nil\n}\n\n// SetUnmodified - set unmodified time since.\nfunc (o *GetObjectOptions) SetUnmodified(modTime time.Time) error {\n\tif modTime.IsZero() {\n\t\treturn errInvalidArgument(\"Modified since cannot be empty.\")\n\t}\n\to.Set(\"If-Unmodified-Since\", modTime.Format(http.TimeFormat))\n\treturn nil\n}\n\n// SetModified - set modified time since.\nfunc (o *GetObjectOptions) SetModified(modTime time.Time) error {\n\tif modTime.IsZero() {\n\t\treturn errInvalidArgument(\"Modified since cannot be empty.\")\n\t}\n\to.Set(\"If-Modified-Since\", modTime.Format(http.TimeFormat))\n\treturn nil\n}\n\n// SetRange - set the start and end offset of the object to be read.\n// See https://tools.ietf.org/html/rfc7233#section-3.1 for reference.\nfunc (o *GetObjectOptions) SetRange(start, end int64) error {\n\tswitch {\n\tcase start == 0 && end < 0:\n\t\t// Read last '-end' bytes. `bytes=-N`.\n\t\to.Set(\"Range\", fmt.Sprintf(\"bytes=%d\", end))\n\tcase 0 < start && end == 0:\n\t\t// Read everything starting from offset\n\t\t// 'start'. `bytes=N-`.\n\t\to.Set(\"Range\", fmt.Sprintf(\"bytes=%d-\", start))\n\tcase 0 <= start && start <= end:\n\t\t// Read everything starting at 'start' till the\n\t\t// 'end'. `bytes=N-M`\n\t\to.Set(\"Range\", fmt.Sprintf(\"bytes=%d-%d\", start, end))\n\tdefault:\n\t\t// All other cases such as\n\t\t// bytes=-3-\n\t\t// bytes=5-3\n\t\t// bytes=-2-4\n\t\t// bytes=-3-0\n\t\t// bytes=-3--2\n\t\t// are invalid.\n\t\treturn errInvalidArgument(\n\t\t\tfmt.Sprintf(\n\t\t\t\t\"Invalid range specified: start=%d end=%d\",\n\t\t\t\tstart, end))\n\t}\n\treturn nil\n}\n\n// toQueryValues - Convert the versionId, partNumber, and reqParams in Options to query string parameters.\nfunc (o *GetObjectOptions) toQueryValues() url.Values {\n\turlValues := make(url.Values)\n\tif o.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", o.VersionID)\n\t}\n\tif o.PartNumber > 0 {\n\t\turlValues.Set(\"partNumber\", strconv.Itoa(o.PartNumber))\n\t}\n\n\tif o.reqParams != nil {\n\t\tfor key, values := range o.reqParams {\n\t\t\tfor _, value := range values {\n\t\t\t\turlValues.Add(key, value)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn urlValues\n}\n"
        },
        {
          "name": "api-list.go",
          "type": "blob",
          "size": 31.9326171875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// ListBuckets list all buckets owned by this authenticated user.\n//\n// This call requires explicit authentication, no anonymous requests are\n// allowed for listing buckets.\n//\n//\tapi := client.New(....)\n//\tfor message := range api.ListBuckets(context.Background()) {\n//\t    fmt.Println(message)\n//\t}\nfunc (c *Client) ListBuckets(ctx context.Context) ([]BucketInfo, error) {\n\t// Execute GET on service.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{contentSHA256Hex: emptySHA256Hex})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, httpRespToErrorResponse(resp, \"\", \"\")\n\t\t}\n\t}\n\tlistAllMyBucketsResult := listAllMyBucketsResult{}\n\terr = xmlDecoder(resp.Body, &listAllMyBucketsResult)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn listAllMyBucketsResult.Buckets.Bucket, nil\n}\n\n// Bucket List Operations.\nfunc (c *Client) listObjectsV2(ctx context.Context, bucketName string, opts ListObjectsOptions) <-chan ObjectInfo {\n\t// Allocate new list objects channel.\n\tobjectStatCh := make(chan ObjectInfo, 1)\n\t// Default listing is delimited at \"/\"\n\tdelimiter := \"/\"\n\tif opts.Recursive {\n\t\t// If recursive we do not delimit.\n\t\tdelimiter = \"\"\n\t}\n\n\t// Return object owner information by default\n\tfetchOwner := true\n\n\tsendObjectInfo := func(info ObjectInfo) {\n\t\tselect {\n\t\tcase objectStatCh <- info:\n\t\tcase <-ctx.Done():\n\t\t}\n\t}\n\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(objectStatCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn objectStatCh\n\t}\n\n\t// Validate incoming object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(opts.Prefix); err != nil {\n\t\tdefer close(objectStatCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn objectStatCh\n\t}\n\n\t// Initiate list objects goroutine here.\n\tgo func(objectStatCh chan<- ObjectInfo) {\n\t\tdefer func() {\n\t\t\tif contextCanceled(ctx) {\n\t\t\t\tobjectStatCh <- ObjectInfo{\n\t\t\t\t\tErr: ctx.Err(),\n\t\t\t\t}\n\t\t\t}\n\t\t\tclose(objectStatCh)\n\t\t}()\n\n\t\t// Save continuationToken for next request.\n\t\tvar continuationToken string\n\t\tfor {\n\t\t\t// Get list of objects a maximum of 1000 per request.\n\t\t\tresult, err := c.listObjectsV2Query(ctx, bucketName, opts.Prefix, continuationToken,\n\t\t\t\tfetchOwner, opts.WithMetadata, delimiter, opts.StartAfter, opts.MaxKeys, opts.headers)\n\t\t\tif err != nil {\n\t\t\t\tsendObjectInfo(ObjectInfo{\n\t\t\t\t\tErr: err,\n\t\t\t\t})\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// If contents are available loop through and send over channel.\n\t\t\tfor _, object := range result.Contents {\n\t\t\t\tobject.ETag = trimEtag(object.ETag)\n\t\t\t\tselect {\n\t\t\t\t// Send object content.\n\t\t\t\tcase objectStatCh <- object:\n\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Send all common prefixes if any.\n\t\t\t// NOTE: prefixes are only present if the request is delimited.\n\t\t\tfor _, obj := range result.CommonPrefixes {\n\t\t\t\tselect {\n\t\t\t\t// Send object prefixes.\n\t\t\t\tcase objectStatCh <- ObjectInfo{Key: obj.Prefix}:\n\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If continuation token present, save it for next request.\n\t\t\tif result.NextContinuationToken != \"\" {\n\t\t\t\tcontinuationToken = result.NextContinuationToken\n\t\t\t}\n\n\t\t\t// Listing ends result is not truncated, return right here.\n\t\t\tif !result.IsTruncated {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Add this to catch broken S3 API implementations.\n\t\t\tif continuationToken == \"\" {\n\t\t\t\tsendObjectInfo(ObjectInfo{\n\t\t\t\t\tErr: fmt.Errorf(\"listObjectsV2 is truncated without continuationToken, %s S3 server is incompatible with S3 API\", c.endpointURL),\n\t\t\t\t})\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}(objectStatCh)\n\treturn objectStatCh\n}\n\n// listObjectsV2Query - (List Objects V2) - List some or all (up to 1000) of the objects in a bucket.\n//\n// You can use the request parameters as selection criteria to return a subset of the objects in a bucket.\n// request parameters :-\n// ---------\n// ?prefix - Limits the response to keys that begin with the specified prefix.\n// ?continuation-token - Used to continue iterating over a set of objects\n// ?metadata - Specifies if we want metadata for the objects as part of list operation.\n// ?delimiter - A delimiter is a character you use to group keys.\n// ?start-after - Sets a marker to start listing lexically at this key onwards.\n// ?max-keys - Sets the maximum number of keys returned in the response body.\nfunc (c *Client) listObjectsV2Query(ctx context.Context, bucketName, objectPrefix, continuationToken string, fetchOwner, metadata bool, delimiter, startAfter string, maxkeys int, headers http.Header) (ListBucketV2Result, error) {\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn ListBucketV2Result{}, err\n\t}\n\t// Validate object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(objectPrefix); err != nil {\n\t\treturn ListBucketV2Result{}, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\n\t// Always set list-type in ListObjects V2\n\turlValues.Set(\"list-type\", \"2\")\n\n\tif metadata {\n\t\turlValues.Set(\"metadata\", \"true\")\n\t}\n\n\t// Set this conditionally if asked\n\tif startAfter != \"\" {\n\t\turlValues.Set(\"start-after\", startAfter)\n\t}\n\n\t// Always set encoding-type in ListObjects V2\n\turlValues.Set(\"encoding-type\", \"url\")\n\n\t// Set object prefix, prefix value to be set to empty is okay.\n\turlValues.Set(\"prefix\", objectPrefix)\n\n\t// Set delimiter, delimiter value to be set to empty is okay.\n\turlValues.Set(\"delimiter\", delimiter)\n\n\t// Set continuation token\n\tif continuationToken != \"\" {\n\t\turlValues.Set(\"continuation-token\", continuationToken)\n\t}\n\n\t// Fetch owner when listing\n\tif fetchOwner {\n\t\turlValues.Set(\"fetch-owner\", \"true\")\n\t}\n\n\t// Set max keys.\n\tif maxkeys > 0 {\n\t\turlValues.Set(\"max-keys\", fmt.Sprintf(\"%d\", maxkeys))\n\t}\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ListBucketV2Result{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ListBucketV2Result{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\t// Decode listBuckets XML.\n\tlistBucketResult := ListBucketV2Result{}\n\tif err = xmlDecoder(resp.Body, &listBucketResult); err != nil {\n\t\treturn listBucketResult, err\n\t}\n\n\t// This is an additional verification check to make\n\t// sure proper responses are received.\n\tif listBucketResult.IsTruncated && listBucketResult.NextContinuationToken == \"\" {\n\t\treturn listBucketResult, ErrorResponse{\n\t\t\tCode:    \"NotImplemented\",\n\t\t\tMessage: \"Truncated response should have continuation token set\",\n\t\t}\n\t}\n\n\tfor i, obj := range listBucketResult.Contents {\n\t\tlistBucketResult.Contents[i].Key, err = decodeS3Name(obj.Key, listBucketResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listBucketResult, err\n\t\t}\n\t\tlistBucketResult.Contents[i].LastModified = listBucketResult.Contents[i].LastModified.Truncate(time.Millisecond)\n\t}\n\n\tfor i, obj := range listBucketResult.CommonPrefixes {\n\t\tlistBucketResult.CommonPrefixes[i].Prefix, err = decodeS3Name(obj.Prefix, listBucketResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listBucketResult, err\n\t\t}\n\t}\n\n\t// Success.\n\treturn listBucketResult, nil\n}\n\nfunc (c *Client) listObjects(ctx context.Context, bucketName string, opts ListObjectsOptions) <-chan ObjectInfo {\n\t// Allocate new list objects channel.\n\tobjectStatCh := make(chan ObjectInfo, 1)\n\t// Default listing is delimited at \"/\"\n\tdelimiter := \"/\"\n\tif opts.Recursive {\n\t\t// If recursive we do not delimit.\n\t\tdelimiter = \"\"\n\t}\n\n\tsendObjectInfo := func(info ObjectInfo) {\n\t\tselect {\n\t\tcase objectStatCh <- info:\n\t\tcase <-ctx.Done():\n\t\t}\n\t}\n\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(objectStatCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn objectStatCh\n\t}\n\t// Validate incoming object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(opts.Prefix); err != nil {\n\t\tdefer close(objectStatCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn objectStatCh\n\t}\n\n\t// Initiate list objects goroutine here.\n\tgo func(objectStatCh chan<- ObjectInfo) {\n\t\tdefer func() {\n\t\t\tif contextCanceled(ctx) {\n\t\t\t\tobjectStatCh <- ObjectInfo{\n\t\t\t\t\tErr: ctx.Err(),\n\t\t\t\t}\n\t\t\t}\n\t\t\tclose(objectStatCh)\n\t\t}()\n\n\t\tmarker := opts.StartAfter\n\t\tfor {\n\t\t\t// Get list of objects a maximum of 1000 per request.\n\t\t\tresult, err := c.listObjectsQuery(ctx, bucketName, opts.Prefix, marker, delimiter, opts.MaxKeys, opts.headers)\n\t\t\tif err != nil {\n\t\t\t\tsendObjectInfo(ObjectInfo{\n\t\t\t\t\tErr: err,\n\t\t\t\t})\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// If contents are available loop through and send over channel.\n\t\t\tfor _, object := range result.Contents {\n\t\t\t\t// Save the marker.\n\t\t\t\tmarker = object.Key\n\t\t\t\tobject.ETag = trimEtag(object.ETag)\n\t\t\t\tselect {\n\t\t\t\t// Send object content.\n\t\t\t\tcase objectStatCh <- object:\n\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Send all common prefixes if any.\n\t\t\t// NOTE: prefixes are only present if the request is delimited.\n\t\t\tfor _, obj := range result.CommonPrefixes {\n\t\t\t\tselect {\n\t\t\t\t// Send object prefixes.\n\t\t\t\tcase objectStatCh <- ObjectInfo{Key: obj.Prefix}:\n\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If next marker present, save it for next request.\n\t\t\tif result.NextMarker != \"\" {\n\t\t\t\tmarker = result.NextMarker\n\t\t\t}\n\n\t\t\t// Listing ends result is not truncated, return right here.\n\t\t\tif !result.IsTruncated {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}(objectStatCh)\n\treturn objectStatCh\n}\n\nfunc (c *Client) listObjectVersions(ctx context.Context, bucketName string, opts ListObjectsOptions) <-chan ObjectInfo {\n\t// Allocate new list objects channel.\n\tresultCh := make(chan ObjectInfo, 1)\n\t// Default listing is delimited at \"/\"\n\tdelimiter := \"/\"\n\tif opts.Recursive {\n\t\t// If recursive we do not delimit.\n\t\tdelimiter = \"\"\n\t}\n\n\tsendObjectInfo := func(info ObjectInfo) {\n\t\tselect {\n\t\tcase resultCh <- info:\n\t\tcase <-ctx.Done():\n\t\t}\n\t}\n\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(resultCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn resultCh\n\t}\n\n\t// Validate incoming object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(opts.Prefix); err != nil {\n\t\tdefer close(resultCh)\n\t\tsendObjectInfo(ObjectInfo{\n\t\t\tErr: err,\n\t\t})\n\t\treturn resultCh\n\t}\n\n\t// Initiate list objects goroutine here.\n\tgo func(resultCh chan<- ObjectInfo) {\n\t\tdefer func() {\n\t\t\tif contextCanceled(ctx) {\n\t\t\t\tresultCh <- ObjectInfo{\n\t\t\t\t\tErr: ctx.Err(),\n\t\t\t\t}\n\t\t\t}\n\t\t\tclose(resultCh)\n\t\t}()\n\n\t\tvar (\n\t\t\tkeyMarker       = \"\"\n\t\t\tversionIDMarker = \"\"\n\t\t)\n\n\t\tfor {\n\t\t\t// Get list of objects a maximum of 1000 per request.\n\t\t\tresult, err := c.listObjectVersionsQuery(ctx, bucketName, opts, keyMarker, versionIDMarker, delimiter)\n\t\t\tif err != nil {\n\t\t\t\tsendObjectInfo(ObjectInfo{\n\t\t\t\t\tErr: err,\n\t\t\t\t})\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// If contents are available loop through and send over channel.\n\t\t\tfor _, version := range result.Versions {\n\t\t\t\tinfo := ObjectInfo{\n\t\t\t\t\tETag:           trimEtag(version.ETag),\n\t\t\t\t\tKey:            version.Key,\n\t\t\t\t\tLastModified:   version.LastModified.Truncate(time.Millisecond),\n\t\t\t\t\tSize:           version.Size,\n\t\t\t\t\tOwner:          version.Owner,\n\t\t\t\t\tStorageClass:   version.StorageClass,\n\t\t\t\t\tIsLatest:       version.IsLatest,\n\t\t\t\t\tVersionID:      version.VersionID,\n\t\t\t\t\tIsDeleteMarker: version.isDeleteMarker,\n\t\t\t\t\tUserTags:       version.UserTags,\n\t\t\t\t\tUserMetadata:   version.UserMetadata,\n\t\t\t\t\tInternal:       version.Internal,\n\t\t\t\t}\n\t\t\t\tselect {\n\t\t\t\t// Send object version info.\n\t\t\t\tcase resultCh <- info:\n\t\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Send all common prefixes if any.\n\t\t\t// NOTE: prefixes are only present if the request is delimited.\n\t\t\tfor _, obj := range result.CommonPrefixes {\n\t\t\t\tselect {\n\t\t\t\t// Send object prefixes.\n\t\t\t\tcase resultCh <- ObjectInfo{Key: obj.Prefix}:\n\t\t\t\t// If receives done from the caller, return here.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If next key marker is present, save it for next request.\n\t\t\tif result.NextKeyMarker != \"\" {\n\t\t\t\tkeyMarker = result.NextKeyMarker\n\t\t\t}\n\n\t\t\t// If next version id marker is present, save it for next request.\n\t\t\tif result.NextVersionIDMarker != \"\" {\n\t\t\t\tversionIDMarker = result.NextVersionIDMarker\n\t\t\t}\n\n\t\t\t// Listing ends result is not truncated, return right here.\n\t\t\tif !result.IsTruncated {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}(resultCh)\n\treturn resultCh\n}\n\n// listObjectVersions - (List Object Versions) - List some or all (up to 1000) of the existing objects\n// and their versions in a bucket.\n//\n// You can use the request parameters as selection criteria to return a subset of the objects in a bucket.\n// request parameters :-\n// ---------\n// ?key-marker - Specifies the key to start with when listing objects in a bucket.\n// ?version-id-marker - Specifies the version id marker to start with when listing objects with versions in a bucket.\n// ?delimiter - A delimiter is a character you use to group keys.\n// ?prefix - Limits the response to keys that begin with the specified prefix.\n// ?max-keys - Sets the maximum number of keys returned in the response body.\nfunc (c *Client) listObjectVersionsQuery(ctx context.Context, bucketName string, opts ListObjectsOptions, keyMarker, versionIDMarker, delimiter string) (ListVersionsResult, error) {\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn ListVersionsResult{}, err\n\t}\n\t// Validate object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(opts.Prefix); err != nil {\n\t\treturn ListVersionsResult{}, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\n\t// Set versions to trigger versioning API\n\turlValues.Set(\"versions\", \"\")\n\n\t// Set object prefix, prefix value to be set to empty is okay.\n\turlValues.Set(\"prefix\", opts.Prefix)\n\n\t// Set delimiter, delimiter value to be set to empty is okay.\n\turlValues.Set(\"delimiter\", delimiter)\n\n\t// Set object marker.\n\tif keyMarker != \"\" {\n\t\turlValues.Set(\"key-marker\", keyMarker)\n\t}\n\n\t// Set max keys.\n\tif opts.MaxKeys > 0 {\n\t\turlValues.Set(\"max-keys\", fmt.Sprintf(\"%d\", opts.MaxKeys))\n\t}\n\n\t// Set version ID marker\n\tif versionIDMarker != \"\" {\n\t\turlValues.Set(\"version-id-marker\", versionIDMarker)\n\t}\n\n\tif opts.WithMetadata {\n\t\turlValues.Set(\"metadata\", \"true\")\n\t}\n\n\t// Always set encoding-type\n\turlValues.Set(\"encoding-type\", \"url\")\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     opts.headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ListVersionsResult{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ListVersionsResult{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\t// Decode ListVersionsResult XML.\n\tlistObjectVersionsOutput := ListVersionsResult{}\n\terr = xmlDecoder(resp.Body, &listObjectVersionsOutput)\n\tif err != nil {\n\t\treturn ListVersionsResult{}, err\n\t}\n\n\tfor i, obj := range listObjectVersionsOutput.Versions {\n\t\tlistObjectVersionsOutput.Versions[i].Key, err = decodeS3Name(obj.Key, listObjectVersionsOutput.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listObjectVersionsOutput, err\n\t\t}\n\t}\n\n\tfor i, obj := range listObjectVersionsOutput.CommonPrefixes {\n\t\tlistObjectVersionsOutput.CommonPrefixes[i].Prefix, err = decodeS3Name(obj.Prefix, listObjectVersionsOutput.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listObjectVersionsOutput, err\n\t\t}\n\t}\n\n\tif listObjectVersionsOutput.NextKeyMarker != \"\" {\n\t\tlistObjectVersionsOutput.NextKeyMarker, err = decodeS3Name(listObjectVersionsOutput.NextKeyMarker, listObjectVersionsOutput.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listObjectVersionsOutput, err\n\t\t}\n\t}\n\n\treturn listObjectVersionsOutput, nil\n}\n\n// listObjects - (List Objects) - List some or all (up to 1000) of the objects in a bucket.\n//\n// You can use the request parameters as selection criteria to return a subset of the objects in a bucket.\n// request parameters :-\n// ---------\n// ?marker - Specifies the key to start with when listing objects in a bucket.\n// ?delimiter - A delimiter is a character you use to group keys.\n// ?prefix - Limits the response to keys that begin with the specified prefix.\n// ?max-keys - Sets the maximum number of keys returned in the response body.\nfunc (c *Client) listObjectsQuery(ctx context.Context, bucketName, objectPrefix, objectMarker, delimiter string, maxkeys int, headers http.Header) (ListBucketResult, error) {\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn ListBucketResult{}, err\n\t}\n\t// Validate object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(objectPrefix); err != nil {\n\t\treturn ListBucketResult{}, err\n\t}\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\n\t// Set object prefix, prefix value to be set to empty is okay.\n\turlValues.Set(\"prefix\", objectPrefix)\n\n\t// Set delimiter, delimiter value to be set to empty is okay.\n\turlValues.Set(\"delimiter\", delimiter)\n\n\t// Set object marker.\n\tif objectMarker != \"\" {\n\t\turlValues.Set(\"marker\", objectMarker)\n\t}\n\n\t// Set max keys.\n\tif maxkeys > 0 {\n\t\turlValues.Set(\"max-keys\", fmt.Sprintf(\"%d\", maxkeys))\n\t}\n\n\t// Always set encoding-type\n\turlValues.Set(\"encoding-type\", \"url\")\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ListBucketResult{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ListBucketResult{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\t// Decode listBuckets XML.\n\tlistBucketResult := ListBucketResult{}\n\terr = xmlDecoder(resp.Body, &listBucketResult)\n\tif err != nil {\n\t\treturn listBucketResult, err\n\t}\n\n\tfor i, obj := range listBucketResult.Contents {\n\t\tlistBucketResult.Contents[i].Key, err = decodeS3Name(obj.Key, listBucketResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listBucketResult, err\n\t\t}\n\t\tlistBucketResult.Contents[i].LastModified = listBucketResult.Contents[i].LastModified.Truncate(time.Millisecond)\n\t}\n\n\tfor i, obj := range listBucketResult.CommonPrefixes {\n\t\tlistBucketResult.CommonPrefixes[i].Prefix, err = decodeS3Name(obj.Prefix, listBucketResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listBucketResult, err\n\t\t}\n\t}\n\n\tif listBucketResult.NextMarker != \"\" {\n\t\tlistBucketResult.NextMarker, err = decodeS3Name(listBucketResult.NextMarker, listBucketResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listBucketResult, err\n\t\t}\n\t}\n\n\treturn listBucketResult, nil\n}\n\n// ListObjectsOptions holds all options of a list object request\ntype ListObjectsOptions struct {\n\t// Include objects versions in the listing\n\tWithVersions bool\n\t// Include objects metadata in the listing\n\tWithMetadata bool\n\t// Only list objects with the prefix\n\tPrefix string\n\t// Ignore '/' delimiter\n\tRecursive bool\n\t// The maximum number of objects requested per\n\t// batch, advanced use-case not useful for most\n\t// applications\n\tMaxKeys int\n\t// StartAfter start listing lexically at this\n\t// object onwards, this value can also be set\n\t// for Marker when `UseV1` is set to true.\n\tStartAfter string\n\n\t// Use the deprecated list objects V1 API\n\tUseV1 bool\n\n\theaders http.Header\n}\n\n// Set adds a key value pair to the options. The\n// key-value pair will be part of the HTTP GET request\n// headers.\nfunc (o *ListObjectsOptions) Set(key, value string) {\n\tif o.headers == nil {\n\t\to.headers = make(http.Header)\n\t}\n\to.headers.Set(key, value)\n}\n\n// ListObjects returns objects list after evaluating the passed options.\n//\n//\tapi := client.New(....)\n//\tfor object := range api.ListObjects(ctx, \"mytestbucket\", minio.ListObjectsOptions{Prefix: \"starthere\", Recursive:true}) {\n//\t    fmt.Println(object)\n//\t}\n//\n// If caller cancels the context, then the last entry on the 'chan ObjectInfo' will be the context.Error()\n// caller must drain the channel entirely and wait until channel is closed before proceeding, without\n// waiting on the channel to be closed completely you might leak goroutines.\nfunc (c *Client) ListObjects(ctx context.Context, bucketName string, opts ListObjectsOptions) <-chan ObjectInfo {\n\tif opts.WithVersions {\n\t\treturn c.listObjectVersions(ctx, bucketName, opts)\n\t}\n\n\t// Use legacy list objects v1 API\n\tif opts.UseV1 {\n\t\treturn c.listObjects(ctx, bucketName, opts)\n\t}\n\n\t// Check whether this is snowball region, if yes ListObjectsV2 doesn't work, fallback to listObjectsV1.\n\tif location, ok := c.bucketLocCache.Get(bucketName); ok {\n\t\tif location == \"snowball\" {\n\t\t\treturn c.listObjects(ctx, bucketName, opts)\n\t\t}\n\t}\n\n\treturn c.listObjectsV2(ctx, bucketName, opts)\n}\n\n// ListIncompleteUploads - List incompletely uploaded multipart objects.\n//\n// ListIncompleteUploads lists all incompleted objects matching the\n// objectPrefix from the specified bucket. If recursion is enabled\n// it would list all subdirectories and all its contents.\n//\n// Your input parameters are just bucketName, objectPrefix, recursive.\n// If you enable recursive as 'true' this function will return back all\n// the multipart objects in a given bucket name.\n//\n//\tapi := client.New(....)\n//\t// Recurively list all objects in 'mytestbucket'\n//\trecursive := true\n//\tfor message := range api.ListIncompleteUploads(context.Background(), \"mytestbucket\", \"starthere\", recursive) {\n//\t    fmt.Println(message)\n//\t}\nfunc (c *Client) ListIncompleteUploads(ctx context.Context, bucketName, objectPrefix string, recursive bool) <-chan ObjectMultipartInfo {\n\treturn c.listIncompleteUploads(ctx, bucketName, objectPrefix, recursive)\n}\n\n// contextCanceled returns whether a context is canceled.\nfunc contextCanceled(ctx context.Context) bool {\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// listIncompleteUploads lists all incomplete uploads.\nfunc (c *Client) listIncompleteUploads(ctx context.Context, bucketName, objectPrefix string, recursive bool) <-chan ObjectMultipartInfo {\n\t// Allocate channel for multipart uploads.\n\tobjectMultipartStatCh := make(chan ObjectMultipartInfo, 1)\n\t// Delimiter is set to \"/\" by default.\n\tdelimiter := \"/\"\n\tif recursive {\n\t\t// If recursive do not delimit.\n\t\tdelimiter = \"\"\n\t}\n\t// Validate bucket name.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(objectMultipartStatCh)\n\t\tobjectMultipartStatCh <- ObjectMultipartInfo{\n\t\t\tErr: err,\n\t\t}\n\t\treturn objectMultipartStatCh\n\t}\n\t// Validate incoming object prefix.\n\tif err := s3utils.CheckValidObjectNamePrefix(objectPrefix); err != nil {\n\t\tdefer close(objectMultipartStatCh)\n\t\tobjectMultipartStatCh <- ObjectMultipartInfo{\n\t\t\tErr: err,\n\t\t}\n\t\treturn objectMultipartStatCh\n\t}\n\tgo func(objectMultipartStatCh chan<- ObjectMultipartInfo) {\n\t\tdefer func() {\n\t\t\tif contextCanceled(ctx) {\n\t\t\t\tobjectMultipartStatCh <- ObjectMultipartInfo{\n\t\t\t\t\tErr: ctx.Err(),\n\t\t\t\t}\n\t\t\t}\n\t\t\tclose(objectMultipartStatCh)\n\t\t}()\n\n\t\t// object and upload ID marker for future requests.\n\t\tvar objectMarker string\n\t\tvar uploadIDMarker string\n\t\tfor {\n\t\t\t// list all multipart uploads.\n\t\t\tresult, err := c.listMultipartUploadsQuery(ctx, bucketName, objectMarker, uploadIDMarker, objectPrefix, delimiter, 0)\n\t\t\tif err != nil {\n\t\t\t\tobjectMultipartStatCh <- ObjectMultipartInfo{\n\t\t\t\t\tErr: err,\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tobjectMarker = result.NextKeyMarker\n\t\t\tuploadIDMarker = result.NextUploadIDMarker\n\n\t\t\t// Send all multipart uploads.\n\t\t\tfor _, obj := range result.Uploads {\n\t\t\t\t// Calculate total size of the uploaded parts if 'aggregateSize' is enabled.\n\t\t\t\tselect {\n\t\t\t\t// Send individual uploads here.\n\t\t\t\tcase objectMultipartStatCh <- obj:\n\t\t\t\t// If the context is canceled\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Send all common prefixes if any.\n\t\t\t// NOTE: prefixes are only present if the request is delimited.\n\t\t\tfor _, obj := range result.CommonPrefixes {\n\t\t\t\tselect {\n\t\t\t\t// Send delimited prefixes here.\n\t\t\t\tcase objectMultipartStatCh <- ObjectMultipartInfo{Key: obj.Prefix, Size: 0}:\n\t\t\t\t// If context is canceled.\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Listing ends if result not truncated, return right here.\n\t\t\tif !result.IsTruncated {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}(objectMultipartStatCh)\n\t// return.\n\treturn objectMultipartStatCh\n}\n\n// listMultipartUploadsQuery - (List Multipart Uploads).\n//   - Lists some or all (up to 1000) in-progress multipart uploads in a bucket.\n//\n// You can use the request parameters as selection criteria to return a subset of the uploads in a bucket.\n// request parameters. :-\n// ---------\n// ?key-marker - Specifies the multipart upload after which listing should begin.\n// ?upload-id-marker - Together with key-marker specifies the multipart upload after which listing should begin.\n// ?delimiter - A delimiter is a character you use to group keys.\n// ?prefix - Limits the response to keys that begin with the specified prefix.\n// ?max-uploads - Sets the maximum number of multipart uploads returned in the response body.\nfunc (c *Client) listMultipartUploadsQuery(ctx context.Context, bucketName, keyMarker, uploadIDMarker, prefix, delimiter string, maxUploads int) (ListMultipartUploadsResult, error) {\n\t// Get resources properly escaped and lined up before using them in http request.\n\turlValues := make(url.Values)\n\t// Set uploads.\n\turlValues.Set(\"uploads\", \"\")\n\t// Set object key marker.\n\tif keyMarker != \"\" {\n\t\turlValues.Set(\"key-marker\", keyMarker)\n\t}\n\t// Set upload id marker.\n\tif uploadIDMarker != \"\" {\n\t\turlValues.Set(\"upload-id-marker\", uploadIDMarker)\n\t}\n\n\t// Set object prefix, prefix value to be set to empty is okay.\n\turlValues.Set(\"prefix\", prefix)\n\n\t// Set delimiter, delimiter value to be set to empty is okay.\n\turlValues.Set(\"delimiter\", delimiter)\n\n\t// Always set encoding-type\n\turlValues.Set(\"encoding-type\", \"url\")\n\n\t// maxUploads should be 1000 or less.\n\tif maxUploads > 0 {\n\t\t// Set max-uploads.\n\t\turlValues.Set(\"max-uploads\", fmt.Sprintf(\"%d\", maxUploads))\n\t}\n\n\t// Execute GET on bucketName to list multipart uploads.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ListMultipartUploadsResult{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ListMultipartUploadsResult{}, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\t// Decode response body.\n\tlistMultipartUploadsResult := ListMultipartUploadsResult{}\n\terr = xmlDecoder(resp.Body, &listMultipartUploadsResult)\n\tif err != nil {\n\t\treturn listMultipartUploadsResult, err\n\t}\n\n\tlistMultipartUploadsResult.NextKeyMarker, err = decodeS3Name(listMultipartUploadsResult.NextKeyMarker, listMultipartUploadsResult.EncodingType)\n\tif err != nil {\n\t\treturn listMultipartUploadsResult, err\n\t}\n\n\tlistMultipartUploadsResult.NextUploadIDMarker, err = decodeS3Name(listMultipartUploadsResult.NextUploadIDMarker, listMultipartUploadsResult.EncodingType)\n\tif err != nil {\n\t\treturn listMultipartUploadsResult, err\n\t}\n\n\tfor i, obj := range listMultipartUploadsResult.Uploads {\n\t\tlistMultipartUploadsResult.Uploads[i].Key, err = decodeS3Name(obj.Key, listMultipartUploadsResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listMultipartUploadsResult, err\n\t\t}\n\t}\n\n\tfor i, obj := range listMultipartUploadsResult.CommonPrefixes {\n\t\tlistMultipartUploadsResult.CommonPrefixes[i].Prefix, err = decodeS3Name(obj.Prefix, listMultipartUploadsResult.EncodingType)\n\t\tif err != nil {\n\t\t\treturn listMultipartUploadsResult, err\n\t\t}\n\t}\n\n\treturn listMultipartUploadsResult, nil\n}\n\n// listObjectParts list all object parts recursively.\n//\n//lint:ignore U1000 Keep this around\nfunc (c *Client) listObjectParts(ctx context.Context, bucketName, objectName, uploadID string) (partsInfo map[int]ObjectPart, err error) {\n\t// Part number marker for the next batch of request.\n\tvar nextPartNumberMarker int\n\tpartsInfo = make(map[int]ObjectPart)\n\tfor {\n\t\t// Get list of uploaded parts a maximum of 1000 per request.\n\t\tlistObjPartsResult, err := c.listObjectPartsQuery(ctx, bucketName, objectName, uploadID, nextPartNumberMarker, 1000)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Append to parts info.\n\t\tfor _, part := range listObjPartsResult.ObjectParts {\n\t\t\t// Trim off the odd double quotes from ETag in the beginning and end.\n\t\t\tpart.ETag = trimEtag(part.ETag)\n\t\t\tpartsInfo[part.PartNumber] = part\n\t\t}\n\t\t// Keep part number marker, for the next iteration.\n\t\tnextPartNumberMarker = listObjPartsResult.NextPartNumberMarker\n\t\t// Listing ends result is not truncated, return right here.\n\t\tif !listObjPartsResult.IsTruncated {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Return all the parts.\n\treturn partsInfo, nil\n}\n\n// findUploadIDs lists all incomplete uploads and find the uploadIDs of the matching object name.\nfunc (c *Client) findUploadIDs(ctx context.Context, bucketName, objectName string) ([]string, error) {\n\tvar uploadIDs []string\n\t// Make list incomplete uploads recursive.\n\tisRecursive := true\n\t// List all incomplete uploads.\n\tfor mpUpload := range c.listIncompleteUploads(ctx, bucketName, objectName, isRecursive) {\n\t\tif mpUpload.Err != nil {\n\t\t\treturn nil, mpUpload.Err\n\t\t}\n\t\tif objectName == mpUpload.Key {\n\t\t\tuploadIDs = append(uploadIDs, mpUpload.UploadID)\n\t\t}\n\t}\n\t// Return the latest upload id.\n\treturn uploadIDs, nil\n}\n\n// listObjectPartsQuery (List Parts query)\n//   - lists some or all (up to 1000) parts that have been uploaded\n//     for a specific multipart upload\n//\n// You can use the request parameters as selection criteria to return\n// a subset of the uploads in a bucket, request parameters :-\n// ---------\n// ?part-number-marker - Specifies the part after which listing should\n// begin.\n// ?max-parts - Maximum parts to be listed per request.\nfunc (c *Client) listObjectPartsQuery(ctx context.Context, bucketName, objectName, uploadID string, partNumberMarker, maxParts int) (ListObjectPartsResult, error) {\n\t// Get resources properly escaped and lined up before using them in http request.\n\turlValues := make(url.Values)\n\t// Set part number marker.\n\turlValues.Set(\"part-number-marker\", fmt.Sprintf(\"%d\", partNumberMarker))\n\t// Set upload id.\n\turlValues.Set(\"uploadId\", uploadID)\n\n\t// maxParts should be 1000 or less.\n\tif maxParts > 0 {\n\t\t// Set max parts.\n\t\turlValues.Set(\"max-parts\", fmt.Sprintf(\"%d\", maxParts))\n\t}\n\n\t// Execute GET on objectName to get list of parts.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ListObjectPartsResult{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ListObjectPartsResult{}, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\t// Decode list object parts XML.\n\tlistObjectPartsResult := ListObjectPartsResult{}\n\terr = xmlDecoder(resp.Body, &listObjectPartsResult)\n\tif err != nil {\n\t\treturn listObjectPartsResult, err\n\t}\n\treturn listObjectPartsResult, nil\n}\n\n// Decode an S3 object name according to the encoding type\nfunc decodeS3Name(name, encodingType string) (string, error) {\n\tswitch encodingType {\n\tcase \"url\":\n\t\treturn url.QueryUnescape(name)\n\tdefault:\n\t\treturn name, nil\n\t}\n}\n"
        },
        {
          "name": "api-object-legal-hold.go",
          "type": "blob",
          "size": 4.7177734375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// objectLegalHold - object legal hold specified in\n// https://docs.aws.amazon.com/AmazonS3/latest/API/archive-RESTObjectPUTLegalHold.html\ntype objectLegalHold struct {\n\tXMLNS   string          `xml:\"xmlns,attr,omitempty\"`\n\tXMLName xml.Name        `xml:\"LegalHold\"`\n\tStatus  LegalHoldStatus `xml:\"Status,omitempty\"`\n}\n\n// PutObjectLegalHoldOptions represents options specified by user for PutObjectLegalHold call\ntype PutObjectLegalHoldOptions struct {\n\tVersionID string\n\tStatus    *LegalHoldStatus\n}\n\n// GetObjectLegalHoldOptions represents options specified by user for GetObjectLegalHold call\ntype GetObjectLegalHoldOptions struct {\n\tVersionID string\n}\n\n// LegalHoldStatus - object legal hold status.\ntype LegalHoldStatus string\n\nconst (\n\t// LegalHoldEnabled indicates legal hold is enabled\n\tLegalHoldEnabled LegalHoldStatus = \"ON\"\n\n\t// LegalHoldDisabled indicates legal hold is disabled\n\tLegalHoldDisabled LegalHoldStatus = \"OFF\"\n)\n\nfunc (r LegalHoldStatus) String() string {\n\treturn string(r)\n}\n\n// IsValid - check whether this legal hold status is valid or not.\nfunc (r LegalHoldStatus) IsValid() bool {\n\treturn r == LegalHoldEnabled || r == LegalHoldDisabled\n}\n\nfunc newObjectLegalHold(status *LegalHoldStatus) (*objectLegalHold, error) {\n\tif status == nil {\n\t\treturn nil, fmt.Errorf(\"Status not set\")\n\t}\n\tif !status.IsValid() {\n\t\treturn nil, fmt.Errorf(\"invalid legal hold status `%v`\", status)\n\t}\n\tlegalHold := &objectLegalHold{\n\t\tStatus: *status,\n\t}\n\treturn legalHold, nil\n}\n\n// PutObjectLegalHold : sets object legal hold for a given object and versionID.\nfunc (c *Client) PutObjectLegalHold(ctx context.Context, bucketName, objectName string, opts PutObjectLegalHoldOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"legal-hold\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\n\tlh, err := newObjectLegalHold(opts.Status)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlhData, err := xml.Marshal(lh)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(lhData),\n\t\tcontentLength:    int64(len(lhData)),\n\t\tcontentMD5Base64: sumMD5Base64(lhData),\n\t\tcontentSHA256Hex: sum256Hex(lhData),\n\t}\n\n\t// Execute PUT Object Legal Hold.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\treturn nil\n}\n\n// GetObjectLegalHold gets legal-hold status of given object.\nfunc (c *Client) GetObjectLegalHold(ctx context.Context, bucketName, objectName string, opts GetObjectLegalHoldOptions) (status *LegalHoldStatus, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\turlValues := make(url.Values)\n\turlValues.Set(\"legal-hold\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\tlh := &objectLegalHold{}\n\tif err = xml.NewDecoder(resp.Body).Decode(lh); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lh.Status, nil\n}\n"
        },
        {
          "name": "api-object-lock.go",
          "type": "blob",
          "size": 6.8212890625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2019 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// RetentionMode - object retention mode.\ntype RetentionMode string\n\nconst (\n\t// Governance - governance mode.\n\tGovernance RetentionMode = \"GOVERNANCE\"\n\n\t// Compliance - compliance mode.\n\tCompliance RetentionMode = \"COMPLIANCE\"\n)\n\nfunc (r RetentionMode) String() string {\n\treturn string(r)\n}\n\n// IsValid - check whether this retention mode is valid or not.\nfunc (r RetentionMode) IsValid() bool {\n\treturn r == Governance || r == Compliance\n}\n\n// ValidityUnit - retention validity unit.\ntype ValidityUnit string\n\nconst (\n\t// Days - denotes no. of days.\n\tDays ValidityUnit = \"DAYS\"\n\n\t// Years - denotes no. of years.\n\tYears ValidityUnit = \"YEARS\"\n)\n\nfunc (unit ValidityUnit) String() string {\n\treturn string(unit)\n}\n\n// IsValid - check whether this validity unit is valid or not.\nfunc (unit ValidityUnit) isValid() bool {\n\treturn unit == Days || unit == Years\n}\n\n// Retention - bucket level retention configuration.\ntype Retention struct {\n\tMode     RetentionMode\n\tValidity time.Duration\n}\n\nfunc (r Retention) String() string {\n\treturn fmt.Sprintf(\"{Mode:%v, Validity:%v}\", r.Mode, r.Validity)\n}\n\n// IsEmpty - returns whether retention is empty or not.\nfunc (r Retention) IsEmpty() bool {\n\treturn r.Mode == \"\" || r.Validity == 0\n}\n\n// objectLockConfig - object lock configuration specified in\n// https://docs.aws.amazon.com/AmazonS3/latest/API/Type_API_ObjectLockConfiguration.html\ntype objectLockConfig struct {\n\tXMLNS             string   `xml:\"xmlns,attr,omitempty\"`\n\tXMLName           xml.Name `xml:\"ObjectLockConfiguration\"`\n\tObjectLockEnabled string   `xml:\"ObjectLockEnabled\"`\n\tRule              *struct {\n\t\tDefaultRetention struct {\n\t\t\tMode  RetentionMode `xml:\"Mode\"`\n\t\t\tDays  *uint         `xml:\"Days\"`\n\t\t\tYears *uint         `xml:\"Years\"`\n\t\t} `xml:\"DefaultRetention\"`\n\t} `xml:\"Rule,omitempty\"`\n}\n\nfunc newObjectLockConfig(mode *RetentionMode, validity *uint, unit *ValidityUnit) (*objectLockConfig, error) {\n\tconfig := &objectLockConfig{\n\t\tObjectLockEnabled: \"Enabled\",\n\t}\n\n\tif mode != nil && validity != nil && unit != nil {\n\t\tif !mode.IsValid() {\n\t\t\treturn nil, fmt.Errorf(\"invalid retention mode `%v`\", mode)\n\t\t}\n\n\t\tif !unit.isValid() {\n\t\t\treturn nil, fmt.Errorf(\"invalid validity unit `%v`\", unit)\n\t\t}\n\n\t\tconfig.Rule = &struct {\n\t\t\tDefaultRetention struct {\n\t\t\t\tMode  RetentionMode `xml:\"Mode\"`\n\t\t\t\tDays  *uint         `xml:\"Days\"`\n\t\t\t\tYears *uint         `xml:\"Years\"`\n\t\t\t} `xml:\"DefaultRetention\"`\n\t\t}{}\n\n\t\tconfig.Rule.DefaultRetention.Mode = *mode\n\t\tif *unit == Days {\n\t\t\tconfig.Rule.DefaultRetention.Days = validity\n\t\t} else {\n\t\t\tconfig.Rule.DefaultRetention.Years = validity\n\t\t}\n\n\t\treturn config, nil\n\t}\n\n\tif mode == nil && validity == nil && unit == nil {\n\t\treturn config, nil\n\t}\n\n\treturn nil, fmt.Errorf(\"all of retention mode, validity and validity unit must be passed\")\n}\n\n// SetBucketObjectLockConfig sets object lock configuration in given bucket. mode, validity and unit are either all set or all nil.\nfunc (c *Client) SetBucketObjectLockConfig(ctx context.Context, bucketName string, mode *RetentionMode, validity *uint, unit *ValidityUnit) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"object-lock\", \"\")\n\n\tconfig, err := newObjectLockConfig(mode, validity, unit)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tconfigData, err := xml.Marshal(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(configData),\n\t\tcontentLength:    int64(len(configData)),\n\t\tcontentMD5Base64: sumMD5Base64(configData),\n\t\tcontentSHA256Hex: sum256Hex(configData),\n\t}\n\n\t// Execute PUT bucket object lock configuration.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// GetObjectLockConfig gets object lock configuration of given bucket.\nfunc (c *Client) GetObjectLockConfig(ctx context.Context, bucketName string) (objectLock string, mode *RetentionMode, validity *uint, unit *ValidityUnit, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn \"\", nil, nil, nil, err\n\t}\n\n\turlValues := make(url.Values)\n\turlValues.Set(\"object-lock\", \"\")\n\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn \"\", nil, nil, nil, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn \"\", nil, nil, nil, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\tconfig := &objectLockConfig{}\n\tif err = xml.NewDecoder(resp.Body).Decode(config); err != nil {\n\t\treturn \"\", nil, nil, nil, err\n\t}\n\n\tif config.Rule != nil {\n\t\tmode = &config.Rule.DefaultRetention.Mode\n\t\tif config.Rule.DefaultRetention.Days != nil {\n\t\t\tvalidity = config.Rule.DefaultRetention.Days\n\t\t\tdays := Days\n\t\t\tunit = &days\n\t\t} else {\n\t\t\tvalidity = config.Rule.DefaultRetention.Years\n\t\t\tyears := Years\n\t\t\tunit = &years\n\t\t}\n\t\treturn config.ObjectLockEnabled, mode, validity, unit, nil\n\t}\n\treturn config.ObjectLockEnabled, nil, nil, nil, nil\n}\n\n// GetBucketObjectLockConfig gets object lock configuration of given bucket.\nfunc (c *Client) GetBucketObjectLockConfig(ctx context.Context, bucketName string) (mode *RetentionMode, validity *uint, unit *ValidityUnit, err error) {\n\t_, mode, validity, unit, err = c.GetObjectLockConfig(ctx, bucketName)\n\treturn mode, validity, unit, err\n}\n\n// SetObjectLockConfig sets object lock configuration in given bucket. mode, validity and unit are either all set or all nil.\nfunc (c *Client) SetObjectLockConfig(ctx context.Context, bucketName string, mode *RetentionMode, validity *uint, unit *ValidityUnit) error {\n\treturn c.SetBucketObjectLockConfig(ctx, bucketName, mode, validity, unit)\n}\n"
        },
        {
          "name": "api-object-retention.go",
          "type": "blob",
          "size": 4.6337890625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2019-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// objectRetention - object retention specified in\n// https://docs.aws.amazon.com/AmazonS3/latest/API/Type_API_ObjectLockConfiguration.html\ntype objectRetention struct {\n\tXMLNS           string        `xml:\"xmlns,attr,omitempty\"`\n\tXMLName         xml.Name      `xml:\"Retention\"`\n\tMode            RetentionMode `xml:\"Mode,omitempty\"`\n\tRetainUntilDate *time.Time    `type:\"timestamp\" timestampFormat:\"iso8601\" xml:\"RetainUntilDate,omitempty\"`\n}\n\nfunc newObjectRetention(mode *RetentionMode, date *time.Time) (*objectRetention, error) {\n\tobjectRetention := &objectRetention{}\n\n\tif date != nil && !date.IsZero() {\n\t\tobjectRetention.RetainUntilDate = date\n\t}\n\tif mode != nil {\n\t\tif !mode.IsValid() {\n\t\t\treturn nil, fmt.Errorf(\"invalid retention mode `%v`\", mode)\n\t\t}\n\t\tobjectRetention.Mode = *mode\n\t}\n\n\treturn objectRetention, nil\n}\n\n// PutObjectRetentionOptions represents options specified by user for PutObject call\ntype PutObjectRetentionOptions struct {\n\tGovernanceBypass bool\n\tMode             *RetentionMode\n\tRetainUntilDate  *time.Time\n\tVersionID        string\n}\n\n// PutObjectRetention sets object retention for a given object and versionID.\nfunc (c *Client) PutObjectRetention(ctx context.Context, bucketName, objectName string, opts PutObjectRetentionOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"retention\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\n\tretention, err := newObjectRetention(opts.Mode, opts.RetainUntilDate)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tretentionData, err := xml.Marshal(retention)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Build headers.\n\theaders := make(http.Header)\n\n\tif opts.GovernanceBypass {\n\t\t// Set the bypass goverenance retention header\n\t\theaders.Set(amzBypassGovernance, \"true\")\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(retentionData),\n\t\tcontentLength:    int64(len(retentionData)),\n\t\tcontentMD5Base64: sumMD5Base64(retentionData),\n\t\tcontentSHA256Hex: sum256Hex(retentionData),\n\t\tcustomHeader:     headers,\n\t}\n\n\t// Execute PUT Object Retention.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\treturn nil\n}\n\n// GetObjectRetention gets retention of given object.\nfunc (c *Client) GetObjectRetention(ctx context.Context, bucketName, objectName, versionID string) (mode *RetentionMode, retainUntilDate *time.Time, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, nil, err\n\t}\n\turlValues := make(url.Values)\n\turlValues.Set(\"retention\", \"\")\n\tif versionID != \"\" {\n\t\turlValues.Set(\"versionId\", versionID)\n\t}\n\t// Execute GET on bucket to list objects.\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\tretention := &objectRetention{}\n\tif err = xml.NewDecoder(resp.Body).Decode(retention); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn &retention.Mode, retention.RetainUntilDate, nil\n}\n"
        },
        {
          "name": "api-object-tagging.go",
          "type": "blob",
          "size": 5.1728515625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/tags\"\n)\n\n// PutObjectTaggingOptions holds an object version id\n// to update tag(s) of a specific object version\ntype PutObjectTaggingOptions struct {\n\tVersionID string\n\tInternal  AdvancedObjectTaggingOptions\n}\n\n// AdvancedObjectTaggingOptions for internal use by MinIO server - not intended for client use.\ntype AdvancedObjectTaggingOptions struct {\n\tReplicationProxyRequest string\n}\n\n// PutObjectTagging replaces or creates object tag(s) and can target\n// a specific object version in a versioned bucket.\nfunc (c *Client) PutObjectTagging(ctx context.Context, bucketName, objectName string, otags *tags.Tags, opts PutObjectTaggingOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\theaders := make(http.Header, 0)\n\tif opts.Internal.ReplicationProxyRequest != \"\" {\n\t\theaders.Set(minIOBucketReplicationProxyRequest, opts.Internal.ReplicationProxyRequest)\n\t}\n\treqBytes, err := xml.Marshal(otags)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      bytes.NewReader(reqBytes),\n\t\tcontentLength:    int64(len(reqBytes)),\n\t\tcontentMD5Base64: sumMD5Base64(reqBytes),\n\t\tcustomHeader:     headers,\n\t}\n\n\t// Execute PUT to set a object tagging.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\treturn nil\n}\n\n// GetObjectTaggingOptions holds the object version ID\n// to fetch the tagging key/value pairs\ntype GetObjectTaggingOptions struct {\n\tVersionID string\n\tInternal  AdvancedObjectTaggingOptions\n}\n\n// GetObjectTagging fetches object tag(s) with options to target\n// a specific object version in a versioned bucket.\nfunc (c *Client) GetObjectTagging(ctx context.Context, bucketName, objectName string, opts GetObjectTaggingOptions) (*tags.Tags, error) {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\theaders := make(http.Header, 0)\n\tif opts.Internal.ReplicationProxyRequest != \"\" {\n\t\theaders.Set(minIOBucketReplicationProxyRequest, opts.Internal.ReplicationProxyRequest)\n\t}\n\t// Execute GET on object to get object tag(s)\n\tresp, err := c.executeMethod(ctx, http.MethodGet, requestMetadata{\n\t\tbucketName:   bucketName,\n\t\tobjectName:   objectName,\n\t\tqueryValues:  urlValues,\n\t\tcustomHeader: headers,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\n\treturn tags.ParseObjectXML(resp.Body)\n}\n\n// RemoveObjectTaggingOptions holds the version id of the object to remove\ntype RemoveObjectTaggingOptions struct {\n\tVersionID string\n\tInternal  AdvancedObjectTaggingOptions\n}\n\n// RemoveObjectTagging removes object tag(s) with options to control a specific object\n// version in a versioned bucket\nfunc (c *Client) RemoveObjectTagging(ctx context.Context, bucketName, objectName string, opts RemoveObjectTaggingOptions) error {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\turlValues.Set(\"tagging\", \"\")\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\theaders := make(http.Header, 0)\n\tif opts.Internal.ReplicationProxyRequest != \"\" {\n\t\theaders.Set(minIOBucketReplicationProxyRequest, opts.Internal.ReplicationProxyRequest)\n\t}\n\t// Execute DELETE on object to remove object tag(s)\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:   bucketName,\n\t\tobjectName:   objectName,\n\t\tqueryValues:  urlValues,\n\t\tcustomHeader: headers,\n\t})\n\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp != nil {\n\t\t// S3 returns \"204 No content\" after Object tag deletion.\n\t\tif resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\treturn err\n}\n"
        },
        {
          "name": "api-presigned.go",
          "type": "blob",
          "size": 7.9140625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/signer\"\n)\n\n// presignURL - Returns a presigned URL for an input 'method'.\n// Expires maximum is 7days - ie. 604800 and minimum is 1.\nfunc (c *Client) presignURL(ctx context.Context, method, bucketName, objectName string, expires time.Duration, reqParams url.Values, extraHeaders http.Header) (u *url.URL, err error) {\n\t// Input validation.\n\tif method == \"\" {\n\t\treturn nil, errInvalidArgument(\"method cannot be empty.\")\n\t}\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\tif err = isValidExpiry(expires); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert expires into seconds.\n\texpireSeconds := int64(expires / time.Second)\n\treqMetadata := requestMetadata{\n\t\tpresignURL:         true,\n\t\tbucketName:         bucketName,\n\t\tobjectName:         objectName,\n\t\texpires:            expireSeconds,\n\t\tqueryValues:        reqParams,\n\t\textraPresignHeader: extraHeaders,\n\t}\n\n\t// Instantiate a new request.\n\t// Since expires is set newRequest will presign the request.\n\tvar req *http.Request\n\tif req, err = c.newRequest(ctx, method, reqMetadata); err != nil {\n\t\treturn nil, err\n\t}\n\treturn req.URL, nil\n}\n\n// PresignedGetObject - Returns a presigned URL to access an object\n// data without credentials. URL can have a maximum expiry of\n// upto 7days or a minimum of 1sec. Additionally you can override\n// a set of response headers using the query parameters.\nfunc (c *Client) PresignedGetObject(ctx context.Context, bucketName, objectName string, expires time.Duration, reqParams url.Values) (u *url.URL, err error) {\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\treturn c.presignURL(ctx, http.MethodGet, bucketName, objectName, expires, reqParams, nil)\n}\n\n// PresignedHeadObject - Returns a presigned URL to access\n// object metadata without credentials. URL can have a maximum expiry\n// of upto 7days or a minimum of 1sec. Additionally you can override\n// a set of response headers using the query parameters.\nfunc (c *Client) PresignedHeadObject(ctx context.Context, bucketName, objectName string, expires time.Duration, reqParams url.Values) (u *url.URL, err error) {\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\treturn c.presignURL(ctx, http.MethodHead, bucketName, objectName, expires, reqParams, nil)\n}\n\n// PresignedPutObject - Returns a presigned URL to upload an object\n// without credentials. URL can have a maximum expiry of upto 7days\n// or a minimum of 1sec.\nfunc (c *Client) PresignedPutObject(ctx context.Context, bucketName, objectName string, expires time.Duration) (u *url.URL, err error) {\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\treturn c.presignURL(ctx, http.MethodPut, bucketName, objectName, expires, nil, nil)\n}\n\n// PresignHeader - similar to Presign() but allows including HTTP headers that\n// will be used to build the signature. The request using the resulting URL will\n// need to have the exact same headers to be added for signature validation to\n// pass.\n//\n// FIXME: The extra header parameter should be included in Presign() in the next\n// major version bump, and this function should then be deprecated.\nfunc (c *Client) PresignHeader(ctx context.Context, method, bucketName, objectName string, expires time.Duration, reqParams url.Values, extraHeaders http.Header) (u *url.URL, err error) {\n\treturn c.presignURL(ctx, method, bucketName, objectName, expires, reqParams, extraHeaders)\n}\n\n// Presign - returns a presigned URL for any http method of your choice along\n// with custom request params and extra signed headers. URL can have a maximum\n// expiry of upto 7days or a minimum of 1sec.\nfunc (c *Client) Presign(ctx context.Context, method, bucketName, objectName string, expires time.Duration, reqParams url.Values) (u *url.URL, err error) {\n\treturn c.presignURL(ctx, method, bucketName, objectName, expires, reqParams, nil)\n}\n\n// PresignedPostPolicy - Returns POST urlString, form data to upload an object.\nfunc (c *Client) PresignedPostPolicy(ctx context.Context, p *PostPolicy) (u *url.URL, formData map[string]string, err error) {\n\t// Validate input arguments.\n\tif p.expiration.IsZero() {\n\t\treturn nil, nil, errors.New(\"Expiration time must be specified\")\n\t}\n\tif _, ok := p.formData[\"key\"]; !ok {\n\t\treturn nil, nil, errors.New(\"object key must be specified\")\n\t}\n\tif _, ok := p.formData[\"bucket\"]; !ok {\n\t\treturn nil, nil, errors.New(\"bucket name must be specified\")\n\t}\n\n\tbucketName := p.formData[\"bucket\"]\n\t// Fetch the bucket location.\n\tlocation, err := c.getBucketLocation(ctx, bucketName)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tisVirtualHost := c.isVirtualHostStyleRequest(*c.endpointURL, bucketName)\n\n\tu, err = c.makeTargetURL(bucketName, \"\", location, isVirtualHost, nil)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Get credentials from the configured credentials provider.\n\tcredValues, err := c.credsProvider.GetWithContext(c.CredContext())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar (\n\t\tsignerType      = credValues.SignerType\n\t\tsessionToken    = credValues.SessionToken\n\t\taccessKeyID     = credValues.AccessKeyID\n\t\tsecretAccessKey = credValues.SecretAccessKey\n\t)\n\n\tif signerType.IsAnonymous() {\n\t\treturn nil, nil, errInvalidArgument(\"Presigned operations are not supported for anonymous credentials\")\n\t}\n\n\t// Keep time.\n\tt := time.Now().UTC()\n\t// For signature version '2' handle here.\n\tif signerType.IsV2() {\n\t\tpolicyBase64 := p.base64()\n\t\tp.formData[\"policy\"] = policyBase64\n\t\t// For Google endpoint set this value to be 'GoogleAccessId'.\n\t\tif s3utils.IsGoogleEndpoint(*c.endpointURL) {\n\t\t\tp.formData[\"GoogleAccessId\"] = accessKeyID\n\t\t} else {\n\t\t\t// For all other endpoints set this value to be 'AWSAccessKeyId'.\n\t\t\tp.formData[\"AWSAccessKeyId\"] = accessKeyID\n\t\t}\n\t\t// Sign the policy.\n\t\tp.formData[\"signature\"] = signer.PostPresignSignatureV2(policyBase64, secretAccessKey)\n\t\treturn u, p.formData, nil\n\t}\n\n\t// Add date policy.\n\tif err = p.addNewPolicy(policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$x-amz-date\",\n\t\tvalue:     t.Format(iso8601DateFormat),\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Add algorithm policy.\n\tif err = p.addNewPolicy(policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$x-amz-algorithm\",\n\t\tvalue:     signV4Algorithm,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Add a credential policy.\n\tcredential := signer.GetCredential(accessKeyID, location, t, signer.ServiceTypeS3)\n\tif err = p.addNewPolicy(policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$x-amz-credential\",\n\t\tvalue:     credential,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif sessionToken != \"\" {\n\t\tif err = p.addNewPolicy(policyCondition{\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: \"$x-amz-security-token\",\n\t\t\tvalue:     sessionToken,\n\t\t}); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\t// Get base64 encoded policy.\n\tpolicyBase64 := p.base64()\n\n\t// Fill in the form data.\n\tp.formData[\"policy\"] = policyBase64\n\tp.formData[\"x-amz-algorithm\"] = signV4Algorithm\n\tp.formData[\"x-amz-credential\"] = credential\n\tp.formData[\"x-amz-date\"] = t.Format(iso8601DateFormat)\n\tif sessionToken != \"\" {\n\t\tp.formData[\"x-amz-security-token\"] = sessionToken\n\t}\n\tp.formData[\"x-amz-signature\"] = signer.PostPresignSignatureV4(policyBase64, t, secretAccessKey, location)\n\treturn u, p.formData, nil\n}\n"
        },
        {
          "name": "api-prompt-object.go",
          "type": "blob",
          "size": 2.4052734375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2024 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"net/http\"\n\n\t\"github.com/goccy/go-json\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// PromptObject performs language model inference with the prompt and referenced object as context.\n// Inference is performed using a Lambda handler that can process the prompt and object.\n// Currently, this functionality is limited to certain MinIO servers.\nfunc (c *Client) PromptObject(ctx context.Context, bucketName, objectName, prompt string, opts PromptObjectOptions) (io.ReadCloser, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"InvalidBucketName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"XMinioInvalidObjectName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\n\topts.AddLambdaArnToReqParams(opts.LambdaArn)\n\topts.SetHeader(\"Content-Type\", \"application/json\")\n\topts.AddPromptArg(\"prompt\", prompt)\n\tpromptReqBytes, err := json.Marshal(opts.PromptArgs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Execute POST on bucket/object.\n\tresp, err := c.executeMethod(ctx, http.MethodPost, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      opts.toQueryValues(),\n\t\tcustomHeader:     opts.Header(),\n\t\tcontentSHA256Hex: sum256Hex(promptReqBytes),\n\t\tcontentBody:      bytes.NewReader(promptReqBytes),\n\t\tcontentLength:    int64(len(promptReqBytes)),\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tdefer closeResponse(resp)\n\t\treturn nil, httpRespToErrorResponse(resp, bucketName, objectName)\n\t}\n\n\treturn resp.Body, nil\n}\n"
        },
        {
          "name": "api-prompt-options.go",
          "type": "blob",
          "size": 2.517578125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2024 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"net/http\"\n\t\"net/url\"\n)\n\n// PromptObjectOptions provides options to PromptObject call.\n// LambdaArn is the ARN of the Prompt Lambda to be invoked.\n// PromptArgs is a map of key-value pairs to be passed to the inference action on the Prompt Lambda.\n// \"prompt\" is a reserved key and should not be used as a key in PromptArgs.\ntype PromptObjectOptions struct {\n\tLambdaArn  string\n\tPromptArgs map[string]any\n\theaders    map[string]string\n\treqParams  url.Values\n}\n\n// Header returns the http.Header representation of the POST options.\nfunc (o PromptObjectOptions) Header() http.Header {\n\theaders := make(http.Header, len(o.headers))\n\tfor k, v := range o.headers {\n\t\theaders.Set(k, v)\n\t}\n\treturn headers\n}\n\n// AddPromptArg Add a key value pair to the prompt arguments where the key is a string and\n// the value is a JSON serializable.\nfunc (o *PromptObjectOptions) AddPromptArg(key string, value any) {\n\tif o.PromptArgs == nil {\n\t\to.PromptArgs = make(map[string]any)\n\t}\n\to.PromptArgs[key] = value\n}\n\n// AddLambdaArnToReqParams adds the lambdaArn to the request query string parameters.\nfunc (o *PromptObjectOptions) AddLambdaArnToReqParams(lambdaArn string) {\n\tif o.reqParams == nil {\n\t\to.reqParams = make(url.Values)\n\t}\n\to.reqParams.Add(\"lambdaArn\", lambdaArn)\n}\n\n// SetHeader adds a key value pair to the options. The\n// key-value pair will be part of the HTTP POST request\n// headers.\nfunc (o *PromptObjectOptions) SetHeader(key, value string) {\n\tif o.headers == nil {\n\t\to.headers = make(map[string]string)\n\t}\n\to.headers[http.CanonicalHeaderKey(key)] = value\n}\n\n// toQueryValues - Convert the reqParams in Options to query string parameters.\nfunc (o *PromptObjectOptions) toQueryValues() url.Values {\n\turlValues := make(url.Values)\n\tif o.reqParams != nil {\n\t\tfor key, values := range o.reqParams {\n\t\t\tfor _, value := range values {\n\t\t\t\turlValues.Add(key, value)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn urlValues\n}\n"
        },
        {
          "name": "api-put-bucket.go",
          "type": "blob",
          "size": 3.7919921875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// Bucket operations\nfunc (c *Client) makeBucket(ctx context.Context, bucketName string, opts MakeBucketOptions) (err error) {\n\t// Validate the input arguments.\n\tif err := s3utils.CheckValidBucketNameStrict(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\terr = c.doMakeBucket(ctx, bucketName, opts.Region, opts.ObjectLocking)\n\tif err != nil && (opts.Region == \"\" || opts.Region == \"us-east-1\") {\n\t\tif resp, ok := err.(ErrorResponse); ok && resp.Code == \"AuthorizationHeaderMalformed\" && resp.Region != \"\" {\n\t\t\terr = c.doMakeBucket(ctx, bucketName, resp.Region, opts.ObjectLocking)\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (c *Client) doMakeBucket(ctx context.Context, bucketName, location string, objectLockEnabled bool) (err error) {\n\tdefer func() {\n\t\t// Save the location into cache on a successful makeBucket response.\n\t\tif err == nil {\n\t\t\tc.bucketLocCache.Set(bucketName, location)\n\t\t}\n\t}()\n\n\t// If location is empty, treat is a default region 'us-east-1'.\n\tif location == \"\" {\n\t\tlocation = \"us-east-1\"\n\t\t// For custom region clients, default\n\t\t// to custom region instead not 'us-east-1'.\n\t\tif c.region != \"\" {\n\t\t\tlocation = c.region\n\t\t}\n\t}\n\t// PUT bucket request metadata.\n\treqMetadata := requestMetadata{\n\t\tbucketName:     bucketName,\n\t\tbucketLocation: location,\n\t}\n\n\tif objectLockEnabled {\n\t\theaders := make(http.Header)\n\t\theaders.Add(\"x-amz-bucket-object-lock-enabled\", \"true\")\n\t\treqMetadata.customHeader = headers\n\t}\n\n\t// If location is not 'us-east-1' create bucket location config.\n\tif location != \"us-east-1\" && location != \"\" {\n\t\tcreateBucketConfig := createBucketConfiguration{}\n\t\tcreateBucketConfig.Location = location\n\t\tvar createBucketConfigBytes []byte\n\t\tcreateBucketConfigBytes, err = xml.Marshal(createBucketConfig)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treqMetadata.contentMD5Base64 = sumMD5Base64(createBucketConfigBytes)\n\t\treqMetadata.contentSHA256Hex = sum256Hex(createBucketConfigBytes)\n\t\treqMetadata.contentBody = bytes.NewReader(createBucketConfigBytes)\n\t\treqMetadata.contentLength = int64(len(createBucketConfigBytes))\n\t}\n\n\t// Execute PUT to create a new bucket.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\t// Success.\n\treturn nil\n}\n\n// MakeBucketOptions holds all options to tweak bucket creation\ntype MakeBucketOptions struct {\n\t// Bucket location\n\tRegion string\n\t// Enable object locking\n\tObjectLocking bool\n}\n\n// MakeBucket creates a new bucket with bucketName with a context to control cancellations and timeouts.\n//\n// Location is an optional argument, by default all buckets are\n// created in US Standard Region.\n//\n// For Amazon S3 for more supported regions - http://docs.aws.amazon.com/general/latest/gr/rande.html\n// For Google Cloud Storage for more supported regions - https://cloud.google.com/storage/docs/bucket-locations\nfunc (c *Client) MakeBucket(ctx context.Context, bucketName string, opts MakeBucketOptions) (err error) {\n\treturn c.makeBucket(ctx, bucketName, opts)\n}\n"
        },
        {
          "name": "api-put-object-common.go",
          "type": "blob",
          "size": 4.3095703125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"math\"\n\t\"os\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\nconst nullVersionID = \"null\"\n\n// Verify if reader is *minio.Object\nfunc isObject(reader io.Reader) (ok bool) {\n\t_, ok = reader.(*Object)\n\treturn\n}\n\n// Verify if reader is a generic ReaderAt\nfunc isReadAt(reader io.Reader) (ok bool) {\n\tvar v *os.File\n\tv, ok = reader.(*os.File)\n\tif ok {\n\t\t// Stdin, Stdout and Stderr all have *os.File type\n\t\t// which happen to also be io.ReaderAt compatible\n\t\t// we need to add special conditions for them to\n\t\t// be ignored by this function.\n\t\tfor _, f := range []string{\n\t\t\t\"/dev/stdin\",\n\t\t\t\"/dev/stdout\",\n\t\t\t\"/dev/stderr\",\n\t\t} {\n\t\t\tif f == v.Name() {\n\t\t\t\tok = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t} else {\n\t\t_, ok = reader.(io.ReaderAt)\n\t}\n\treturn\n}\n\n// OptimalPartInfo - calculate the optimal part info for a given\n// object size.\n//\n// NOTE: Assumption here is that for any object to be uploaded to any S3 compatible\n// object storage it will have the following parameters as constants.\n//\n//\tmaxPartsCount - 10000\n//\tminPartSize - 16MiB\n//\tmaxMultipartPutObjectSize - 5TiB\nfunc OptimalPartInfo(objectSize int64, configuredPartSize uint64) (totalPartsCount int, partSize, lastPartSize int64, err error) {\n\t// object size is '-1' set it to 5TiB.\n\tvar unknownSize bool\n\tif objectSize == -1 {\n\t\tunknownSize = true\n\t\tobjectSize = maxMultipartPutObjectSize\n\t}\n\n\t// object size is larger than supported maximum.\n\tif objectSize > maxMultipartPutObjectSize {\n\t\terr = errEntityTooLarge(objectSize, maxMultipartPutObjectSize, \"\", \"\")\n\t\treturn\n\t}\n\n\tvar partSizeFlt float64\n\tif configuredPartSize > 0 {\n\t\tif int64(configuredPartSize) > objectSize {\n\t\t\terr = errEntityTooLarge(int64(configuredPartSize), objectSize, \"\", \"\")\n\t\t\treturn\n\t\t}\n\n\t\tif !unknownSize {\n\t\t\tif objectSize > (int64(configuredPartSize) * maxPartsCount) {\n\t\t\t\terr = errInvalidArgument(\"Part size * max_parts(10000) is lesser than input objectSize.\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif configuredPartSize < absMinPartSize {\n\t\t\terr = errInvalidArgument(\"Input part size is smaller than allowed minimum of 5MiB.\")\n\t\t\treturn\n\t\t}\n\n\t\tif configuredPartSize > maxPartSize {\n\t\t\terr = errInvalidArgument(\"Input part size is bigger than allowed maximum of 5GiB.\")\n\t\t\treturn\n\t\t}\n\n\t\tpartSizeFlt = float64(configuredPartSize)\n\t\tif unknownSize {\n\t\t\t// If input has unknown size and part size is configured\n\t\t\t// keep it to maximum allowed as per 10000 parts.\n\t\t\tobjectSize = int64(configuredPartSize) * maxPartsCount\n\t\t}\n\t} else {\n\t\tconfiguredPartSize = minPartSize\n\t\t// Use floats for part size for all calculations to avoid\n\t\t// overflows during float64 to int64 conversions.\n\t\tpartSizeFlt = float64(objectSize / maxPartsCount)\n\t\tpartSizeFlt = math.Ceil(partSizeFlt/float64(configuredPartSize)) * float64(configuredPartSize)\n\t}\n\n\t// Total parts count.\n\ttotalPartsCount = int(math.Ceil(float64(objectSize) / partSizeFlt))\n\t// Part size.\n\tpartSize = int64(partSizeFlt)\n\t// Last part size.\n\tlastPartSize = objectSize - int64(totalPartsCount-1)*partSize\n\treturn totalPartsCount, partSize, lastPartSize, nil\n}\n\n// getUploadID - fetch upload id if already present for an object name\n// or initiate a new request to fetch a new upload id.\nfunc (c *Client) newUploadID(ctx context.Context, bucketName, objectName string, opts PutObjectOptions) (uploadID string, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn \"\", err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Initiate multipart upload for an object.\n\tinitMultipartUploadResult, err := c.initiateMultipartUpload(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn initMultipartUploadResult.UploadID, nil\n}\n"
        },
        {
          "name": "api-put-object-fan-out.go",
          "type": "blob",
          "size": 4.8349609375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2023 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"io\"\n\t\"mime/multipart\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\n// PutObjectFanOutEntry is per object entry fan-out metadata\ntype PutObjectFanOutEntry struct {\n\tKey                string            `json:\"key\"`\n\tUserMetadata       map[string]string `json:\"metadata,omitempty\"`\n\tUserTags           map[string]string `json:\"tags,omitempty\"`\n\tContentType        string            `json:\"contentType,omitempty\"`\n\tContentEncoding    string            `json:\"contentEncoding,omitempty\"`\n\tContentDisposition string            `json:\"contentDisposition,omitempty\"`\n\tContentLanguage    string            `json:\"contentLanguage,omitempty\"`\n\tCacheControl       string            `json:\"cacheControl,omitempty\"`\n\tRetention          RetentionMode     `json:\"retention,omitempty\"`\n\tRetainUntilDate    *time.Time        `json:\"retainUntil,omitempty\"`\n}\n\n// PutObjectFanOutRequest this is the request structure sent\n// to the server to fan-out the stream to multiple objects.\ntype PutObjectFanOutRequest struct {\n\tEntries  []PutObjectFanOutEntry\n\tChecksum Checksum\n\tSSE      encrypt.ServerSide\n}\n\n// PutObjectFanOutResponse this is the response structure sent\n// by the server upon success or failure for each object\n// fan-out keys. Additionally, this response carries ETag,\n// VersionID and LastModified for each object fan-out.\ntype PutObjectFanOutResponse struct {\n\tKey          string     `json:\"key\"`\n\tETag         string     `json:\"etag,omitempty\"`\n\tVersionID    string     `json:\"versionId,omitempty\"`\n\tLastModified *time.Time `json:\"lastModified,omitempty\"`\n\tError        string     `json:\"error,omitempty\"`\n}\n\n// PutObjectFanOut - is a variant of PutObject instead of writing a single object from a single\n// stream multiple objects are written, defined via a list of PutObjectFanOutRequests. Each entry\n// in PutObjectFanOutRequest carries an object keyname and its relevant metadata if any. `Key` is\n// mandatory, rest of the other options in PutObjectFanOutRequest are optional.\nfunc (c *Client) PutObjectFanOut(ctx context.Context, bucket string, fanOutData io.Reader, fanOutReq PutObjectFanOutRequest) ([]PutObjectFanOutResponse, error) {\n\tif len(fanOutReq.Entries) == 0 {\n\t\treturn nil, errInvalidArgument(\"fan out requests cannot be empty\")\n\t}\n\n\tpolicy := NewPostPolicy()\n\tpolicy.SetBucket(bucket)\n\tpolicy.SetKey(strconv.FormatInt(time.Now().UnixNano(), 16))\n\n\t// Expires in 15 minutes.\n\tpolicy.SetExpires(time.Now().UTC().Add(15 * time.Minute))\n\n\t// Set encryption headers if any.\n\tpolicy.SetEncryption(fanOutReq.SSE)\n\n\t// Set checksum headers if any.\n\terr := policy.SetChecksum(fanOutReq.Checksum)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\turl, formData, err := c.PresignedPostPolicy(ctx, policy)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tr, w := io.Pipe()\n\n\treq, err := http.NewRequest(http.MethodPost, url.String(), r)\n\tif err != nil {\n\t\tw.Close()\n\t\treturn nil, err\n\t}\n\n\tvar b strings.Builder\n\tenc := json.NewEncoder(&b)\n\tfor _, req := range fanOutReq.Entries {\n\t\tif req.Key == \"\" {\n\t\t\tw.Close()\n\t\t\treturn nil, errors.New(\"PutObjectFanOutRequest.Key is mandatory and cannot be empty\")\n\t\t}\n\t\tif err = enc.Encode(&req); err != nil {\n\t\t\tw.Close()\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tmwriter := multipart.NewWriter(w)\n\treq.Header.Add(\"Content-Type\", mwriter.FormDataContentType())\n\n\tgo func() {\n\t\tdefer w.Close()\n\t\tdefer mwriter.Close()\n\n\t\tfor k, v := range formData {\n\t\t\tif err := mwriter.WriteField(k, v); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif err := mwriter.WriteField(\"x-minio-fanout-list\", b.String()); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tmw, err := mwriter.CreateFormFile(\"file\", \"fanout-content\")\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif _, err = io.Copy(mw, fanOutData); err != nil {\n\t\t\treturn\n\t\t}\n\t}()\n\n\tresp, err := c.do(req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer closeResponse(resp)\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, httpRespToErrorResponse(resp, bucket, \"fanout-content\")\n\t}\n\n\tdec := json.NewDecoder(resp.Body)\n\tfanOutResp := make([]PutObjectFanOutResponse, 0, len(fanOutReq.Entries))\n\tfor dec.More() {\n\t\tvar m PutObjectFanOutResponse\n\t\tif err = dec.Decode(&m); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfanOutResp = append(fanOutResp, m)\n\t}\n\n\treturn fanOutResp, nil\n}\n"
        },
        {
          "name": "api-put-object-file-context.go",
          "type": "blob",
          "size": 1.9453125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"mime\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// FPutObject - Create an object in a bucket, with contents from file at filePath. Allows request cancellation.\nfunc (c *Client) FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts PutObjectOptions) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Open the referenced file.\n\tfileReader, err := os.Open(filePath)\n\t// If any error fail quickly here.\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdefer fileReader.Close()\n\n\t// Save the file stat.\n\tfileStat, err := fileReader.Stat()\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Save the file size.\n\tfileSize := fileStat.Size()\n\n\t// Set contentType based on filepath extension if not given or default\n\t// value of \"application/octet-stream\" if the extension has no associated type.\n\tif opts.ContentType == \"\" {\n\t\tif opts.ContentType = mime.TypeByExtension(filepath.Ext(filePath)); opts.ContentType == \"\" {\n\t\t\topts.ContentType = \"application/octet-stream\"\n\t\t}\n\t}\n\treturn c.PutObject(ctx, bucketName, objectName, fileReader, fileSize, opts)\n}\n"
        },
        {
          "name": "api-put-object-multipart.go",
          "type": "blob",
          "size": 14.9072265625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/hex\"\n\t\"encoding/xml\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\nfunc (c *Client) putObjectMultipart(ctx context.Context, bucketName, objectName string, reader io.Reader, size int64,\n\topts PutObjectOptions,\n) (info UploadInfo, err error) {\n\tinfo, err = c.putObjectMultipartNoStream(ctx, bucketName, objectName, reader, opts)\n\tif err != nil {\n\t\terrResp := ToErrorResponse(err)\n\t\t// Verify if multipart functionality is not available, if not\n\t\t// fall back to single PutObject operation.\n\t\tif errResp.Code == \"AccessDenied\" && strings.Contains(errResp.Message, \"Access Denied\") {\n\t\t\t// Verify if size of reader is greater than '5GiB'.\n\t\t\tif size > maxSinglePutObjectSize {\n\t\t\t\treturn UploadInfo{}, errEntityTooLarge(size, maxSinglePutObjectSize, bucketName, objectName)\n\t\t\t}\n\t\t\t// Fall back to uploading as single PutObject operation.\n\t\t\treturn c.putObject(ctx, bucketName, objectName, reader, size, opts)\n\t\t}\n\t}\n\treturn info, err\n}\n\nfunc (c *Client) putObjectMultipartNoStream(ctx context.Context, bucketName, objectName string, reader io.Reader, opts PutObjectOptions) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Total data read and written to server. should be equal to\n\t// 'size' at the end of the call.\n\tvar totalUploadedSize int64\n\n\t// Complete multipart upload.\n\tvar complMultipartUpload completeMultipartUpload\n\n\t// Calculate the optimal parts info for a given size.\n\ttotalPartsCount, partSize, _, err := OptimalPartInfo(-1, opts.PartSize)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Choose hash algorithms to be calculated by hashCopyN,\n\t// avoid sha256 with non-v4 signature request or\n\t// HTTPS connection.\n\thashAlgos, hashSums := c.hashMaterials(opts.SendContentMd5, !opts.DisableContentSha256)\n\tif len(hashSums) == 0 {\n\t\taddAutoChecksumHeaders(&opts)\n\t}\n\n\t// Initiate a new multipart upload.\n\tuploadID, err := c.newUploadID(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdelete(opts.UserMetadata, \"X-Amz-Checksum-Algorithm\")\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\t}\n\t}()\n\n\t// Part number always starts with '1'.\n\tpartNumber := 1\n\n\t// Initialize parts uploaded map.\n\tpartsInfo := make(map[int]ObjectPart)\n\n\t// Create a buffer.\n\tbuf := make([]byte, partSize)\n\n\t// Create checksums\n\t// CRC32C is ~50% faster on AMD64 @ 30GB/s\n\tcustomHeader := make(http.Header)\n\tcrc := opts.AutoChecksum.Hasher()\n\tfor partNumber <= totalPartsCount {\n\t\tlength, rErr := readFull(reader, buf)\n\t\tif rErr == io.EOF && partNumber > 1 {\n\t\t\tbreak\n\t\t}\n\n\t\tif rErr != nil && rErr != io.ErrUnexpectedEOF && rErr != io.EOF {\n\t\t\treturn UploadInfo{}, rErr\n\t\t}\n\n\t\t// Calculates hash sums while copying partSize bytes into cw.\n\t\tfor k, v := range hashAlgos {\n\t\t\tv.Write(buf[:length])\n\t\t\thashSums[k] = v.Sum(nil)\n\t\t\tv.Close()\n\t\t}\n\n\t\t// Update progress reader appropriately to the latest offset\n\t\t// as we read from the source.\n\t\trd := newHook(bytes.NewReader(buf[:length]), opts.Progress)\n\n\t\t// Checksums..\n\t\tvar (\n\t\t\tmd5Base64 string\n\t\t\tsha256Hex string\n\t\t)\n\n\t\tif hashSums[\"md5\"] != nil {\n\t\t\tmd5Base64 = base64.StdEncoding.EncodeToString(hashSums[\"md5\"])\n\t\t}\n\t\tif hashSums[\"sha256\"] != nil {\n\t\t\tsha256Hex = hex.EncodeToString(hashSums[\"sha256\"])\n\t\t}\n\t\tif len(hashSums) == 0 {\n\t\t\tcrc.Reset()\n\t\t\tcrc.Write(buf[:length])\n\t\t\tcSum := crc.Sum(nil)\n\t\t\tcustomHeader.Set(opts.AutoChecksum.Key(), base64.StdEncoding.EncodeToString(cSum))\n\t\t}\n\n\t\tp := uploadPartParams{bucketName: bucketName, objectName: objectName, uploadID: uploadID, reader: rd, partNumber: partNumber, md5Base64: md5Base64, sha256Hex: sha256Hex, size: int64(length), sse: opts.ServerSideEncryption, streamSha256: !opts.DisableContentSha256, customHeader: customHeader}\n\t\t// Proceed to upload the part.\n\t\tobjPart, uerr := c.uploadPart(ctx, p)\n\t\tif uerr != nil {\n\t\t\treturn UploadInfo{}, uerr\n\t\t}\n\n\t\t// Save successfully uploaded part metadata.\n\t\tpartsInfo[partNumber] = objPart\n\n\t\t// Save successfully uploaded size.\n\t\ttotalUploadedSize += int64(length)\n\n\t\t// Increment part number.\n\t\tpartNumber++\n\n\t\t// For unknown size, Read EOF we break away.\n\t\t// We do not have to upload till totalPartsCount.\n\t\tif rErr == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Loop over total uploaded parts to save them in\n\t// Parts array before completing the multipart request.\n\tallParts := make([]ObjectPart, 0, len(partsInfo))\n\tfor i := 1; i < partNumber; i++ {\n\t\tpart, ok := partsInfo[i]\n\t\tif !ok {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\"Missing part number %d\", i))\n\t\t}\n\t\tallParts = append(allParts, part)\n\t\tcomplMultipartUpload.Parts = append(complMultipartUpload.Parts, CompletePart{\n\t\t\tETag:              part.ETag,\n\t\t\tPartNumber:        part.PartNumber,\n\t\t\tChecksumCRC32:     part.ChecksumCRC32,\n\t\t\tChecksumCRC32C:    part.ChecksumCRC32C,\n\t\t\tChecksumSHA1:      part.ChecksumSHA1,\n\t\t\tChecksumSHA256:    part.ChecksumSHA256,\n\t\t\tChecksumCRC64NVME: part.ChecksumCRC64NVME,\n\t\t})\n\t}\n\n\t// Sort all completed parts.\n\tsort.Sort(completedParts(complMultipartUpload.Parts))\n\topts = PutObjectOptions{\n\t\tServerSideEncryption: opts.ServerSideEncryption,\n\t\tAutoChecksum:         opts.AutoChecksum,\n\t}\n\tapplyAutoChecksum(&opts, allParts)\n\n\tuploadInfo, err := c.completeMultipartUpload(ctx, bucketName, objectName, uploadID, complMultipartUpload, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalUploadedSize\n\treturn uploadInfo, nil\n}\n\n// initiateMultipartUpload - Initiates a multipart upload and returns an upload ID.\nfunc (c *Client) initiateMultipartUpload(ctx context.Context, bucketName, objectName string, opts PutObjectOptions) (initiateMultipartUploadResult, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn initiateMultipartUploadResult{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn initiateMultipartUploadResult{}, err\n\t}\n\n\t// Initialize url queries.\n\turlValues := make(url.Values)\n\turlValues.Set(\"uploads\", \"\")\n\n\tif opts.Internal.SourceVersionID != \"\" {\n\t\tif opts.Internal.SourceVersionID != nullVersionID {\n\t\t\tif _, err := uuid.Parse(opts.Internal.SourceVersionID); err != nil {\n\t\t\t\treturn initiateMultipartUploadResult{}, errInvalidArgument(err.Error())\n\t\t\t}\n\t\t}\n\t\turlValues.Set(\"versionId\", opts.Internal.SourceVersionID)\n\t}\n\n\t// Set ContentType header.\n\tcustomHeader := opts.Header()\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:   bucketName,\n\t\tobjectName:   objectName,\n\t\tqueryValues:  urlValues,\n\t\tcustomHeader: customHeader,\n\t}\n\n\t// Execute POST on an objectName to initiate multipart upload.\n\tresp, err := c.executeMethod(ctx, http.MethodPost, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn initiateMultipartUploadResult{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn initiateMultipartUploadResult{}, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\t// Decode xml for new multipart upload.\n\tinitiateMultipartUploadResult := initiateMultipartUploadResult{}\n\terr = xmlDecoder(resp.Body, &initiateMultipartUploadResult)\n\tif err != nil {\n\t\treturn initiateMultipartUploadResult, err\n\t}\n\treturn initiateMultipartUploadResult, nil\n}\n\ntype uploadPartParams struct {\n\tbucketName   string\n\tobjectName   string\n\tuploadID     string\n\treader       io.Reader\n\tpartNumber   int\n\tmd5Base64    string\n\tsha256Hex    string\n\tsize         int64\n\tsse          encrypt.ServerSide\n\tstreamSha256 bool\n\tcustomHeader http.Header\n\ttrailer      http.Header\n}\n\n// uploadPart - Uploads a part in a multipart upload.\nfunc (c *Client) uploadPart(ctx context.Context, p uploadPartParams) (ObjectPart, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(p.bucketName); err != nil {\n\t\treturn ObjectPart{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(p.objectName); err != nil {\n\t\treturn ObjectPart{}, err\n\t}\n\tif p.size > maxPartSize {\n\t\treturn ObjectPart{}, errEntityTooLarge(p.size, maxPartSize, p.bucketName, p.objectName)\n\t}\n\tif p.size <= -1 {\n\t\treturn ObjectPart{}, errEntityTooSmall(p.size, p.bucketName, p.objectName)\n\t}\n\tif p.partNumber <= 0 {\n\t\treturn ObjectPart{}, errInvalidArgument(\"Part number cannot be negative or equal to zero.\")\n\t}\n\tif p.uploadID == \"\" {\n\t\treturn ObjectPart{}, errInvalidArgument(\"UploadID cannot be empty.\")\n\t}\n\n\t// Get resources properly escaped and lined up before using them in http request.\n\turlValues := make(url.Values)\n\t// Set part number.\n\turlValues.Set(\"partNumber\", strconv.Itoa(p.partNumber))\n\t// Set upload id.\n\turlValues.Set(\"uploadId\", p.uploadID)\n\n\t// Set encryption headers, if any.\n\tif p.customHeader == nil {\n\t\tp.customHeader = make(http.Header)\n\t}\n\t// https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html\n\t// Server-side encryption is supported by the S3 Multipart Upload actions.\n\t// Unless you are using a customer-provided encryption key, you don't need\n\t// to specify the encryption parameters in each UploadPart request.\n\tif p.sse != nil && p.sse.Type() == encrypt.SSEC {\n\t\tp.sse.Marshal(p.customHeader)\n\t}\n\n\treqMetadata := requestMetadata{\n\t\tbucketName:       p.bucketName,\n\t\tobjectName:       p.objectName,\n\t\tqueryValues:      urlValues,\n\t\tcustomHeader:     p.customHeader,\n\t\tcontentBody:      p.reader,\n\t\tcontentLength:    p.size,\n\t\tcontentMD5Base64: p.md5Base64,\n\t\tcontentSHA256Hex: p.sha256Hex,\n\t\tstreamSha256:     p.streamSha256,\n\t\ttrailer:          p.trailer,\n\t}\n\n\t// Execute PUT on each part.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ObjectPart{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn ObjectPart{}, httpRespToErrorResponse(resp, p.bucketName, p.objectName)\n\t\t}\n\t}\n\t// Once successfully uploaded, return completed part.\n\th := resp.Header\n\tobjPart := ObjectPart{\n\t\tChecksumCRC32:     h.Get(ChecksumCRC32.Key()),\n\t\tChecksumCRC32C:    h.Get(ChecksumCRC32C.Key()),\n\t\tChecksumSHA1:      h.Get(ChecksumSHA1.Key()),\n\t\tChecksumSHA256:    h.Get(ChecksumSHA256.Key()),\n\t\tChecksumCRC64NVME: h.Get(ChecksumCRC64NVME.Key()),\n\t}\n\tobjPart.Size = p.size\n\tobjPart.PartNumber = p.partNumber\n\t// Trim off the odd double quotes from ETag in the beginning and end.\n\tobjPart.ETag = trimEtag(h.Get(\"ETag\"))\n\treturn objPart, nil\n}\n\n// completeMultipartUpload - Completes a multipart upload by assembling previously uploaded parts.\nfunc (c *Client) completeMultipartUpload(ctx context.Context, bucketName, objectName, uploadID string,\n\tcomplete completeMultipartUpload, opts PutObjectOptions,\n) (UploadInfo, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Initialize url queries.\n\turlValues := make(url.Values)\n\turlValues.Set(\"uploadId\", uploadID)\n\t// Marshal complete multipart body.\n\tcompleteMultipartUploadBytes, err := xml.Marshal(complete)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\theaders := opts.Header()\n\tif s3utils.IsAmazonEndpoint(*c.endpointURL) {\n\t\theaders.Del(encrypt.SseKmsKeyID)          // Remove X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id not supported in CompleteMultipartUpload\n\t\theaders.Del(encrypt.SseGenericHeader)     // Remove X-Amz-Server-Side-Encryption not supported in CompleteMultipartUpload\n\t\theaders.Del(encrypt.SseEncryptionContext) // Remove X-Amz-Server-Side-Encryption-Context not supported in CompleteMultipartUpload\n\t}\n\n\t// Instantiate all the complete multipart buffer.\n\tcompleteMultipartUploadBuffer := bytes.NewReader(completeMultipartUploadBytes)\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentBody:      completeMultipartUploadBuffer,\n\t\tcontentLength:    int64(len(completeMultipartUploadBytes)),\n\t\tcontentSHA256Hex: sum256Hex(completeMultipartUploadBytes),\n\t\tcustomHeader:     headers,\n\t}\n\n\t// Execute POST to complete multipart upload for an objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodPost, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn UploadInfo{}, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\n\t// Read resp.Body into a []bytes to parse for Error response inside the body\n\tvar b []byte\n\tb, err = io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\t// Decode completed multipart upload response on success.\n\tcompleteMultipartUploadResult := completeMultipartUploadResult{}\n\terr = xmlDecoder(bytes.NewReader(b), &completeMultipartUploadResult)\n\tif err != nil {\n\t\t// xml parsing failure due to presence an ill-formed xml fragment\n\t\treturn UploadInfo{}, err\n\t} else if completeMultipartUploadResult.Bucket == \"\" {\n\t\t// xml's Decode method ignores well-formed xml that don't apply to the type of value supplied.\n\t\t// In this case, it would leave completeMultipartUploadResult with the corresponding zero-values\n\t\t// of the members.\n\n\t\t// Decode completed multipart upload response on failure\n\t\tcompleteMultipartUploadErr := ErrorResponse{}\n\t\terr = xmlDecoder(bytes.NewReader(b), &completeMultipartUploadErr)\n\t\tif err != nil {\n\t\t\t// xml parsing failure due to presence an ill-formed xml fragment\n\t\t\treturn UploadInfo{}, err\n\t\t}\n\t\treturn UploadInfo{}, completeMultipartUploadErr\n\t}\n\n\t// extract lifecycle expiry date and rule ID\n\texpTime, ruleID := amzExpirationToExpiryDateRuleID(resp.Header.Get(amzExpiration))\n\n\treturn UploadInfo{\n\t\tBucket:           completeMultipartUploadResult.Bucket,\n\t\tKey:              completeMultipartUploadResult.Key,\n\t\tETag:             trimEtag(completeMultipartUploadResult.ETag),\n\t\tVersionID:        resp.Header.Get(amzVersionID),\n\t\tLocation:         completeMultipartUploadResult.Location,\n\t\tExpiration:       expTime,\n\t\tExpirationRuleID: ruleID,\n\n\t\tChecksumSHA256:    completeMultipartUploadResult.ChecksumSHA256,\n\t\tChecksumSHA1:      completeMultipartUploadResult.ChecksumSHA1,\n\t\tChecksumCRC32:     completeMultipartUploadResult.ChecksumCRC32,\n\t\tChecksumCRC32C:    completeMultipartUploadResult.ChecksumCRC32C,\n\t\tChecksumCRC64NVME: completeMultipartUploadResult.ChecksumCRC64NVME,\n\t}, nil\n}\n"
        },
        {
          "name": "api-put-object-streaming.go",
          "type": "blob",
          "size": 24.59765625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// putObjectMultipartStream - upload a large object using\n// multipart upload and streaming signature for signing payload.\n// Comprehensive put object operation involving multipart uploads.\n//\n// Following code handles these types of readers.\n//\n//   - *minio.Object\n//   - Any reader which has a method 'ReadAt()'\nfunc (c *Client) putObjectMultipartStream(ctx context.Context, bucketName, objectName string,\n\treader io.Reader, size int64, opts PutObjectOptions,\n) (info UploadInfo, err error) {\n\tif opts.ConcurrentStreamParts && opts.NumThreads > 1 {\n\t\tinfo, err = c.putObjectMultipartStreamParallel(ctx, bucketName, objectName, reader, opts)\n\t} else if !isObject(reader) && isReadAt(reader) && !opts.SendContentMd5 {\n\t\t// Verify if the reader implements ReadAt and it is not a *minio.Object then we will use parallel uploader.\n\t\tinfo, err = c.putObjectMultipartStreamFromReadAt(ctx, bucketName, objectName, reader.(io.ReaderAt), size, opts)\n\t} else {\n\t\tinfo, err = c.putObjectMultipartStreamOptionalChecksum(ctx, bucketName, objectName, reader, size, opts)\n\t}\n\tif err != nil && s3utils.IsGoogleEndpoint(*c.endpointURL) {\n\t\terrResp := ToErrorResponse(err)\n\t\t// Verify if multipart functionality is not available, if not\n\t\t// fall back to single PutObject operation.\n\t\tif errResp.Code == \"AccessDenied\" && strings.Contains(errResp.Message, \"Access Denied\") {\n\t\t\t// Verify if size of reader is greater than '5GiB'.\n\t\t\tif size > maxSinglePutObjectSize {\n\t\t\t\treturn UploadInfo{}, errEntityTooLarge(size, maxSinglePutObjectSize, bucketName, objectName)\n\t\t\t}\n\t\t\t// Fall back to uploading as single PutObject operation.\n\t\t\treturn c.putObject(ctx, bucketName, objectName, reader, size, opts)\n\t\t}\n\t}\n\treturn info, err\n}\n\n// uploadedPartRes - the response received from a part upload.\ntype uploadedPartRes struct {\n\tError   error // Any error encountered while uploading the part.\n\tPartNum int   // Number of the part uploaded.\n\tSize    int64 // Size of the part uploaded.\n\tPart    ObjectPart\n}\n\ntype uploadPartReq struct {\n\tPartNum int        // Number of the part uploaded.\n\tPart    ObjectPart // Size of the part uploaded.\n}\n\n// putObjectMultipartFromReadAt - Uploads files bigger than 128MiB.\n// Supports all readers which implements io.ReaderAt interface\n// (ReadAt method).\n//\n// NOTE: This function is meant to be used for all readers which\n// implement io.ReaderAt which allows us for resuming multipart\n// uploads but reading at an offset, which would avoid re-read the\n// data which was already uploaded. Internally this function uses\n// temporary files for staging all the data, these temporary files are\n// cleaned automatically when the caller i.e http client closes the\n// stream after uploading all the contents successfully.\nfunc (c *Client) putObjectMultipartStreamFromReadAt(ctx context.Context, bucketName, objectName string,\n\treader io.ReaderAt, size int64, opts PutObjectOptions,\n) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Calculate the optimal parts info for a given size.\n\ttotalPartsCount, partSize, lastPartSize, err := OptimalPartInfo(size, opts.PartSize)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif opts.Checksum.IsSet() {\n\t\topts.AutoChecksum = opts.Checksum\n\t}\n\twithChecksum := c.trailingHeaderSupport\n\tif withChecksum {\n\t\taddAutoChecksumHeaders(&opts)\n\t}\n\t// Initiate a new multipart upload.\n\tuploadID, err := c.newUploadID(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdelete(opts.UserMetadata, \"X-Amz-Checksum-Algorithm\")\n\n\t// Aborts the multipart upload in progress, if the\n\t// function returns any error, since we do not resume\n\t// we should purge the parts which have been uploaded\n\t// to relinquish storage space.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\t}\n\t}()\n\n\t// Total data read and written to server. should be equal to 'size' at the end of the call.\n\tvar totalUploadedSize int64\n\n\t// Complete multipart upload.\n\tvar complMultipartUpload completeMultipartUpload\n\n\t// Declare a channel that sends the next part number to be uploaded.\n\tuploadPartsCh := make(chan uploadPartReq)\n\n\t// Declare a channel that sends back the response of a part upload.\n\tuploadedPartsCh := make(chan uploadedPartRes)\n\n\t// Used for readability, lastPartNumber is always totalPartsCount.\n\tlastPartNumber := totalPartsCount\n\n\tpartitionCtx, partitionCancel := context.WithCancel(ctx)\n\tdefer partitionCancel()\n\t// Send each part number to the channel to be processed.\n\tgo func() {\n\t\tdefer close(uploadPartsCh)\n\n\t\tfor p := 1; p <= totalPartsCount; p++ {\n\t\t\tselect {\n\t\t\tcase <-partitionCtx.Done():\n\t\t\t\treturn\n\t\t\tcase uploadPartsCh <- uploadPartReq{PartNum: p}:\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Receive each part number from the channel allowing three parallel uploads.\n\tfor w := 1; w <= opts.getNumThreads(); w++ {\n\t\tgo func(partSize int64) {\n\t\t\tfor {\n\t\t\t\tvar uploadReq uploadPartReq\n\t\t\t\tvar ok bool\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tcase uploadReq, ok = <-uploadPartsCh:\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Each worker will draw from the part channel and upload in parallel.\n\t\t\t\t}\n\n\t\t\t\t// If partNumber was not uploaded we calculate the missing\n\t\t\t\t// part offset and size. For all other part numbers we\n\t\t\t\t// calculate offset based on multiples of partSize.\n\t\t\t\treadOffset := int64(uploadReq.PartNum-1) * partSize\n\n\t\t\t\t// As a special case if partNumber is lastPartNumber, we\n\t\t\t\t// calculate the offset based on the last part size.\n\t\t\t\tif uploadReq.PartNum == lastPartNumber {\n\t\t\t\t\treadOffset = size - lastPartSize\n\t\t\t\t\tpartSize = lastPartSize\n\t\t\t\t}\n\n\t\t\t\tsectionReader := newHook(io.NewSectionReader(reader, readOffset, partSize), opts.Progress)\n\t\t\t\ttrailer := make(http.Header, 1)\n\t\t\t\tif withChecksum {\n\t\t\t\t\tcrc := opts.AutoChecksum.Hasher()\n\t\t\t\t\ttrailer.Set(opts.AutoChecksum.Key(), base64.StdEncoding.EncodeToString(crc.Sum(nil)))\n\t\t\t\t\tsectionReader = newHashReaderWrapper(sectionReader, crc, func(hash []byte) {\n\t\t\t\t\t\ttrailer.Set(opts.AutoChecksum.Key(), base64.StdEncoding.EncodeToString(hash))\n\t\t\t\t\t})\n\t\t\t\t}\n\n\t\t\t\t// Proceed to upload the part.\n\t\t\t\tp := uploadPartParams{\n\t\t\t\t\tbucketName:   bucketName,\n\t\t\t\t\tobjectName:   objectName,\n\t\t\t\t\tuploadID:     uploadID,\n\t\t\t\t\treader:       sectionReader,\n\t\t\t\t\tpartNumber:   uploadReq.PartNum,\n\t\t\t\t\tsize:         partSize,\n\t\t\t\t\tsse:          opts.ServerSideEncryption,\n\t\t\t\t\tstreamSha256: !opts.DisableContentSha256,\n\t\t\t\t\tsha256Hex:    \"\",\n\t\t\t\t\ttrailer:      trailer,\n\t\t\t\t}\n\t\t\t\tobjPart, err := c.uploadPart(ctx, p)\n\t\t\t\tif err != nil {\n\t\t\t\t\tuploadedPartsCh <- uploadedPartRes{\n\t\t\t\t\t\tError: err,\n\t\t\t\t\t}\n\t\t\t\t\t// Exit the goroutine.\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// Save successfully uploaded part metadata.\n\t\t\t\tuploadReq.Part = objPart\n\n\t\t\t\t// Send successful part info through the channel.\n\t\t\t\tuploadedPartsCh <- uploadedPartRes{\n\t\t\t\t\tSize:    objPart.Size,\n\t\t\t\t\tPartNum: uploadReq.PartNum,\n\t\t\t\t\tPart:    uploadReq.Part,\n\t\t\t\t}\n\t\t\t}\n\t\t}(partSize)\n\t}\n\n\t// Gather the responses as they occur and update any\n\t// progress bar.\n\tallParts := make([]ObjectPart, 0, totalPartsCount)\n\tfor u := 1; u <= totalPartsCount; u++ {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn UploadInfo{}, ctx.Err()\n\t\tcase uploadRes := <-uploadedPartsCh:\n\t\t\tif uploadRes.Error != nil {\n\t\t\t\treturn UploadInfo{}, uploadRes.Error\n\t\t\t}\n\t\t\tallParts = append(allParts, uploadRes.Part)\n\t\t\t// Update the totalUploadedSize.\n\t\t\ttotalUploadedSize += uploadRes.Size\n\t\t\tcomplMultipartUpload.Parts = append(complMultipartUpload.Parts, CompletePart{\n\t\t\t\tETag:              uploadRes.Part.ETag,\n\t\t\t\tPartNumber:        uploadRes.Part.PartNumber,\n\t\t\t\tChecksumCRC32:     uploadRes.Part.ChecksumCRC32,\n\t\t\t\tChecksumCRC32C:    uploadRes.Part.ChecksumCRC32C,\n\t\t\t\tChecksumSHA1:      uploadRes.Part.ChecksumSHA1,\n\t\t\t\tChecksumSHA256:    uploadRes.Part.ChecksumSHA256,\n\t\t\t\tChecksumCRC64NVME: uploadRes.Part.ChecksumCRC64NVME,\n\t\t\t})\n\t\t}\n\t}\n\n\t// Verify if we uploaded all the data.\n\tif totalUploadedSize != size {\n\t\treturn UploadInfo{}, errUnexpectedEOF(totalUploadedSize, size, bucketName, objectName)\n\t}\n\n\t// Sort all completed parts.\n\tsort.Sort(completedParts(complMultipartUpload.Parts))\n\n\topts = PutObjectOptions{\n\t\tServerSideEncryption: opts.ServerSideEncryption,\n\t\tAutoChecksum:         opts.AutoChecksum,\n\t}\n\tif withChecksum {\n\t\tapplyAutoChecksum(&opts, allParts)\n\t}\n\n\tuploadInfo, err := c.completeMultipartUpload(ctx, bucketName, objectName, uploadID, complMultipartUpload, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalUploadedSize\n\treturn uploadInfo, nil\n}\n\nfunc (c *Client) putObjectMultipartStreamOptionalChecksum(ctx context.Context, bucketName, objectName string,\n\treader io.Reader, size int64, opts PutObjectOptions,\n) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tif opts.Checksum.IsSet() {\n\t\topts.AutoChecksum = opts.Checksum\n\t\topts.SendContentMd5 = false\n\t}\n\n\tif !opts.SendContentMd5 {\n\t\taddAutoChecksumHeaders(&opts)\n\t}\n\n\t// Calculate the optimal parts info for a given size.\n\ttotalPartsCount, partSize, lastPartSize, err := OptimalPartInfo(size, opts.PartSize)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\t// Initiates a new multipart request\n\tuploadID, err := c.newUploadID(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdelete(opts.UserMetadata, \"X-Amz-Checksum-Algorithm\")\n\n\t// Aborts the multipart upload if the function returns\n\t// any error, since we do not resume we should purge\n\t// the parts which have been uploaded to relinquish\n\t// storage space.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\t}\n\t}()\n\n\t// Create checksums\n\t// CRC32C is ~50% faster on AMD64 @ 30GB/s\n\tcustomHeader := make(http.Header)\n\tcrc := opts.AutoChecksum.Hasher()\n\tmd5Hash := c.md5Hasher()\n\tdefer md5Hash.Close()\n\n\t// Total data read and written to server. should be equal to 'size' at the end of the call.\n\tvar totalUploadedSize int64\n\n\t// Initialize parts uploaded map.\n\tpartsInfo := make(map[int]ObjectPart)\n\n\t// Create a buffer.\n\tbuf := make([]byte, partSize)\n\n\t// Avoid declaring variables in the for loop\n\tvar md5Base64 string\n\n\t// Part number always starts with '1'.\n\tvar partNumber int\n\tfor partNumber = 1; partNumber <= totalPartsCount; partNumber++ {\n\n\t\t// Proceed to upload the part.\n\t\tif partNumber == totalPartsCount {\n\t\t\tpartSize = lastPartSize\n\t\t}\n\n\t\tlength, rerr := readFull(reader, buf)\n\t\tif rerr == io.EOF && partNumber > 1 {\n\t\t\tbreak\n\t\t}\n\n\t\tif rerr != nil && rerr != io.ErrUnexpectedEOF && err != io.EOF {\n\t\t\treturn UploadInfo{}, rerr\n\t\t}\n\n\t\t// Calculate md5sum.\n\t\tif opts.SendContentMd5 {\n\t\t\tmd5Hash.Reset()\n\t\t\tmd5Hash.Write(buf[:length])\n\t\t\tmd5Base64 = base64.StdEncoding.EncodeToString(md5Hash.Sum(nil))\n\t\t} else {\n\t\t\t// Add CRC32C instead.\n\t\t\tcrc.Reset()\n\t\t\tcrc.Write(buf[:length])\n\t\t\tcSum := crc.Sum(nil)\n\t\t\tcustomHeader.Set(opts.AutoChecksum.KeyCapitalized(), base64.StdEncoding.EncodeToString(cSum))\n\t\t}\n\n\t\t// Update progress reader appropriately to the latest offset\n\t\t// as we read from the source.\n\t\thooked := newHook(bytes.NewReader(buf[:length]), opts.Progress)\n\t\tp := uploadPartParams{bucketName: bucketName, objectName: objectName, uploadID: uploadID, reader: hooked, partNumber: partNumber, md5Base64: md5Base64, size: partSize, sse: opts.ServerSideEncryption, streamSha256: !opts.DisableContentSha256, customHeader: customHeader}\n\t\tobjPart, uerr := c.uploadPart(ctx, p)\n\t\tif uerr != nil {\n\t\t\treturn UploadInfo{}, uerr\n\t\t}\n\n\t\t// Save successfully uploaded part metadata.\n\t\tpartsInfo[partNumber] = objPart\n\n\t\t// Save successfully uploaded size.\n\t\ttotalUploadedSize += partSize\n\t}\n\n\t// Verify if we uploaded all the data.\n\tif size > 0 {\n\t\tif totalUploadedSize != size {\n\t\t\treturn UploadInfo{}, errUnexpectedEOF(totalUploadedSize, size, bucketName, objectName)\n\t\t}\n\t}\n\n\t// Complete multipart upload.\n\tvar complMultipartUpload completeMultipartUpload\n\n\t// Loop over total uploaded parts to save them in\n\t// Parts array before completing the multipart request.\n\tallParts := make([]ObjectPart, 0, len(partsInfo))\n\tfor i := 1; i < partNumber; i++ {\n\t\tpart, ok := partsInfo[i]\n\t\tif !ok {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\"Missing part number %d\", i))\n\t\t}\n\t\tallParts = append(allParts, part)\n\t\tcomplMultipartUpload.Parts = append(complMultipartUpload.Parts, CompletePart{\n\t\t\tETag:              part.ETag,\n\t\t\tPartNumber:        part.PartNumber,\n\t\t\tChecksumCRC32:     part.ChecksumCRC32,\n\t\t\tChecksumCRC32C:    part.ChecksumCRC32C,\n\t\t\tChecksumSHA1:      part.ChecksumSHA1,\n\t\t\tChecksumSHA256:    part.ChecksumSHA256,\n\t\t\tChecksumCRC64NVME: part.ChecksumCRC64NVME,\n\t\t})\n\t}\n\n\t// Sort all completed parts.\n\tsort.Sort(completedParts(complMultipartUpload.Parts))\n\n\topts = PutObjectOptions{\n\t\tServerSideEncryption: opts.ServerSideEncryption,\n\t\tAutoChecksum:         opts.AutoChecksum,\n\t}\n\tapplyAutoChecksum(&opts, allParts)\n\tuploadInfo, err := c.completeMultipartUpload(ctx, bucketName, objectName, uploadID, complMultipartUpload, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalUploadedSize\n\treturn uploadInfo, nil\n}\n\n// putObjectMultipartStreamParallel uploads opts.NumThreads parts in parallel.\n// This is expected to take opts.PartSize * opts.NumThreads * (GOGC / 100) bytes of buffer.\nfunc (c *Client) putObjectMultipartStreamParallel(ctx context.Context, bucketName, objectName string,\n\treader io.Reader, opts PutObjectOptions,\n) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif opts.Checksum.IsSet() {\n\t\topts.SendContentMd5 = false\n\t\topts.AutoChecksum = opts.Checksum\n\t}\n\tif !opts.SendContentMd5 {\n\t\taddAutoChecksumHeaders(&opts)\n\t}\n\n\t// Cancel all when an error occurs.\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\t// Calculate the optimal parts info for a given size.\n\ttotalPartsCount, partSize, _, err := OptimalPartInfo(-1, opts.PartSize)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Initiates a new multipart request\n\tuploadID, err := c.newUploadID(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdelete(opts.UserMetadata, \"X-Amz-Checksum-Algorithm\")\n\n\t// Aborts the multipart upload if the function returns\n\t// any error, since we do not resume we should purge\n\t// the parts which have been uploaded to relinquish\n\t// storage space.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\t}\n\t}()\n\n\t// Create checksums\n\t// CRC32C is ~50% faster on AMD64 @ 30GB/s\n\tcrc := opts.AutoChecksum.Hasher()\n\n\t// Total data read and written to server. should be equal to 'size' at the end of the call.\n\tvar totalUploadedSize int64\n\n\t// Initialize parts uploaded map.\n\tpartsInfo := make(map[int]ObjectPart)\n\n\t// Create a buffer.\n\tnBuffers := int64(opts.NumThreads)\n\tbufs := make(chan []byte, nBuffers)\n\tall := make([]byte, nBuffers*partSize)\n\tfor i := int64(0); i < nBuffers; i++ {\n\t\tbufs <- all[i*partSize : i*partSize+partSize]\n\t}\n\n\tvar wg sync.WaitGroup\n\tvar mu sync.Mutex\n\terrCh := make(chan error, opts.NumThreads)\n\n\treader = newHook(reader, opts.Progress)\n\n\t// Part number always starts with '1'.\n\tvar partNumber int\n\tfor partNumber = 1; partNumber <= totalPartsCount; partNumber++ {\n\t\t// Proceed to upload the part.\n\t\tvar buf []byte\n\t\tselect {\n\t\tcase buf = <-bufs:\n\t\tcase err = <-errCh:\n\t\t\tcancel()\n\t\t\twg.Wait()\n\t\t\treturn UploadInfo{}, err\n\t\t}\n\n\t\tif int64(len(buf)) != partSize {\n\t\t\treturn UploadInfo{}, fmt.Errorf(\"read buffer < %d than expected partSize: %d\", len(buf), partSize)\n\t\t}\n\n\t\tlength, rerr := readFull(reader, buf)\n\t\tif rerr == io.EOF && partNumber > 1 {\n\t\t\t// Done\n\t\t\tbreak\n\t\t}\n\n\t\tif rerr != nil && rerr != io.ErrUnexpectedEOF && err != io.EOF {\n\t\t\tcancel()\n\t\t\twg.Wait()\n\t\t\treturn UploadInfo{}, rerr\n\t\t}\n\n\t\t// Calculate md5sum.\n\t\tcustomHeader := make(http.Header)\n\t\tif !opts.SendContentMd5 {\n\t\t\t// Add Checksum instead.\n\t\t\tcrc.Reset()\n\t\t\tcrc.Write(buf[:length])\n\t\t\tcSum := crc.Sum(nil)\n\t\t\tcustomHeader.Set(opts.AutoChecksum.Key(), base64.StdEncoding.EncodeToString(cSum))\n\t\t}\n\n\t\twg.Add(1)\n\t\tgo func(partNumber int) {\n\t\t\t// Avoid declaring variables in the for loop\n\t\t\tvar md5Base64 string\n\n\t\t\tif opts.SendContentMd5 {\n\t\t\t\tmd5Hash := c.md5Hasher()\n\t\t\t\tmd5Hash.Write(buf[:length])\n\t\t\t\tmd5Base64 = base64.StdEncoding.EncodeToString(md5Hash.Sum(nil))\n\t\t\t\tmd5Hash.Close()\n\t\t\t}\n\n\t\t\tdefer wg.Done()\n\t\t\tp := uploadPartParams{\n\t\t\t\tbucketName:   bucketName,\n\t\t\t\tobjectName:   objectName,\n\t\t\t\tuploadID:     uploadID,\n\t\t\t\treader:       bytes.NewReader(buf[:length]),\n\t\t\t\tpartNumber:   partNumber,\n\t\t\t\tmd5Base64:    md5Base64,\n\t\t\t\tsize:         int64(length),\n\t\t\t\tsse:          opts.ServerSideEncryption,\n\t\t\t\tstreamSha256: !opts.DisableContentSha256,\n\t\t\t\tcustomHeader: customHeader,\n\t\t\t}\n\t\t\tobjPart, uerr := c.uploadPart(ctx, p)\n\t\t\tif uerr != nil {\n\t\t\t\terrCh <- uerr\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Save successfully uploaded part metadata.\n\t\t\tmu.Lock()\n\t\t\tpartsInfo[partNumber] = objPart\n\t\t\tmu.Unlock()\n\n\t\t\t// Send buffer back so it can be reused.\n\t\t\tbufs <- buf\n\t\t}(partNumber)\n\n\t\t// Save successfully uploaded size.\n\t\ttotalUploadedSize += int64(length)\n\t}\n\twg.Wait()\n\n\t// Collect any error\n\tselect {\n\tcase err = <-errCh:\n\t\treturn UploadInfo{}, err\n\tdefault:\n\t}\n\n\t// Complete multipart upload.\n\tvar complMultipartUpload completeMultipartUpload\n\n\t// Loop over total uploaded parts to save them in\n\t// Parts array before completing the multipart request.\n\tallParts := make([]ObjectPart, 0, len(partsInfo))\n\tfor i := 1; i < partNumber; i++ {\n\t\tpart, ok := partsInfo[i]\n\t\tif !ok {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\"Missing part number %d\", i))\n\t\t}\n\t\tallParts = append(allParts, part)\n\t\tcomplMultipartUpload.Parts = append(complMultipartUpload.Parts, CompletePart{\n\t\t\tETag:              part.ETag,\n\t\t\tPartNumber:        part.PartNumber,\n\t\t\tChecksumCRC32:     part.ChecksumCRC32,\n\t\t\tChecksumCRC32C:    part.ChecksumCRC32C,\n\t\t\tChecksumSHA1:      part.ChecksumSHA1,\n\t\t\tChecksumSHA256:    part.ChecksumSHA256,\n\t\t\tChecksumCRC64NVME: part.ChecksumCRC64NVME,\n\t\t})\n\t}\n\n\t// Sort all completed parts.\n\tsort.Sort(completedParts(complMultipartUpload.Parts))\n\n\topts = PutObjectOptions{\n\t\tServerSideEncryption: opts.ServerSideEncryption,\n\t\tAutoChecksum:         opts.AutoChecksum,\n\t}\n\tapplyAutoChecksum(&opts, allParts)\n\n\tuploadInfo, err := c.completeMultipartUpload(ctx, bucketName, objectName, uploadID, complMultipartUpload, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalUploadedSize\n\treturn uploadInfo, nil\n}\n\n// putObject special function used Google Cloud Storage. This special function\n// is used for Google Cloud Storage since Google's multipart API is not S3 compatible.\nfunc (c *Client) putObject(ctx context.Context, bucketName, objectName string, reader io.Reader, size int64, opts PutObjectOptions) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Size -1 is only supported on Google Cloud Storage, we error\n\t// out in all other situations.\n\tif size < 0 && !s3utils.IsGoogleEndpoint(*c.endpointURL) {\n\t\treturn UploadInfo{}, errEntityTooSmall(size, bucketName, objectName)\n\t}\n\n\tif opts.SendContentMd5 && s3utils.IsGoogleEndpoint(*c.endpointURL) && size < 0 {\n\t\treturn UploadInfo{}, errInvalidArgument(\"MD5Sum cannot be calculated with size '-1'\")\n\t}\n\tif opts.Checksum.IsSet() {\n\t\topts.SendContentMd5 = false\n\t}\n\n\tvar readSeeker io.Seeker\n\tif size > 0 {\n\t\tif isReadAt(reader) && !isObject(reader) {\n\t\t\tseeker, ok := reader.(io.Seeker)\n\t\t\tif ok {\n\t\t\t\toffset, err := seeker.Seek(0, io.SeekCurrent)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn UploadInfo{}, errInvalidArgument(err.Error())\n\t\t\t\t}\n\t\t\t\treader = io.NewSectionReader(reader.(io.ReaderAt), offset, size)\n\t\t\t\treadSeeker = reader.(io.Seeker)\n\t\t\t}\n\t\t}\n\t}\n\n\tvar md5Base64 string\n\tif opts.SendContentMd5 {\n\t\t// Calculate md5sum.\n\t\thash := c.md5Hasher()\n\n\t\tif readSeeker != nil {\n\t\t\tif _, err := io.Copy(hash, reader); err != nil {\n\t\t\t\treturn UploadInfo{}, err\n\t\t\t}\n\t\t\t// Seek back to beginning of io.NewSectionReader's offset.\n\t\t\t_, err = readSeeker.Seek(0, io.SeekStart)\n\t\t\tif err != nil {\n\t\t\t\treturn UploadInfo{}, errInvalidArgument(err.Error())\n\t\t\t}\n\t\t} else {\n\t\t\t// Create a buffer.\n\t\t\tbuf := make([]byte, size)\n\n\t\t\tlength, err := readFull(reader, buf)\n\t\t\tif err != nil && err != io.ErrUnexpectedEOF && err != io.EOF {\n\t\t\t\treturn UploadInfo{}, err\n\t\t\t}\n\n\t\t\thash.Write(buf[:length])\n\t\t\treader = bytes.NewReader(buf[:length])\n\t\t}\n\n\t\tmd5Base64 = base64.StdEncoding.EncodeToString(hash.Sum(nil))\n\t\thash.Close()\n\t}\n\n\t// Update progress reader appropriately to the latest offset as we\n\t// read from the source.\n\tprogressReader := newHook(reader, opts.Progress)\n\n\t// This function does not calculate sha256 and md5sum for payload.\n\t// Execute put object.\n\treturn c.putObjectDo(ctx, bucketName, objectName, progressReader, md5Base64, \"\", size, opts)\n}\n\n// putObjectDo - executes the put object http operation.\n// NOTE: You must have WRITE permissions on a bucket to add an object to it.\nfunc (c *Client) putObjectDo(ctx context.Context, bucketName, objectName string, reader io.Reader, md5Base64, sha256Hex string, size int64, opts PutObjectOptions) (UploadInfo, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\t// Set headers.\n\tcustomHeader := opts.Header()\n\n\t// Populate request metadata.\n\treqMetadata := requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tcustomHeader:     customHeader,\n\t\tcontentBody:      reader,\n\t\tcontentLength:    size,\n\t\tcontentMD5Base64: md5Base64,\n\t\tcontentSHA256Hex: sha256Hex,\n\t\tstreamSha256:     !opts.DisableContentSha256,\n\t}\n\t// Add CRC when client supports it, MD5 is not set, not Google and we don't add SHA256 to chunks.\n\taddCrc := c.trailingHeaderSupport && md5Base64 == \"\" && !s3utils.IsGoogleEndpoint(*c.endpointURL) && (opts.DisableContentSha256 || c.secure)\n\tif opts.Checksum.IsSet() {\n\t\treqMetadata.addCrc = &opts.Checksum\n\t} else if addCrc {\n\t\t// If user has added checksums, don't add them ourselves.\n\t\tfor k := range opts.UserMetadata {\n\t\t\tif strings.HasPrefix(strings.ToLower(k), \"x-amz-checksum-\") {\n\t\t\t\taddCrc = false\n\t\t\t}\n\t\t}\n\t\tif addCrc {\n\t\t\topts.AutoChecksum.SetDefault(ChecksumCRC32C)\n\t\t\treqMetadata.addCrc = &opts.AutoChecksum\n\t\t}\n\t}\n\n\tif opts.Internal.SourceVersionID != \"\" {\n\t\tif opts.Internal.SourceVersionID != nullVersionID {\n\t\t\tif _, err := uuid.Parse(opts.Internal.SourceVersionID); err != nil {\n\t\t\t\treturn UploadInfo{}, errInvalidArgument(err.Error())\n\t\t\t}\n\t\t}\n\t\turlValues := make(url.Values)\n\t\turlValues.Set(\"versionId\", opts.Internal.SourceVersionID)\n\t\treqMetadata.queryValues = urlValues\n\t}\n\n\t// Execute PUT an objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodPut, reqMetadata)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn UploadInfo{}, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\n\t// extract lifecycle expiry date and rule ID\n\texpTime, ruleID := amzExpirationToExpiryDateRuleID(resp.Header.Get(amzExpiration))\n\th := resp.Header\n\treturn UploadInfo{\n\t\tBucket:           bucketName,\n\t\tKey:              objectName,\n\t\tETag:             trimEtag(h.Get(\"ETag\")),\n\t\tVersionID:        h.Get(amzVersionID),\n\t\tSize:             size,\n\t\tExpiration:       expTime,\n\t\tExpirationRuleID: ruleID,\n\n\t\t// Checksum values\n\t\tChecksumCRC32:     h.Get(ChecksumCRC32.Key()),\n\t\tChecksumCRC32C:    h.Get(ChecksumCRC32C.Key()),\n\t\tChecksumSHA1:      h.Get(ChecksumSHA1.Key()),\n\t\tChecksumSHA256:    h.Get(ChecksumSHA256.Key()),\n\t\tChecksumCRC64NVME: h.Get(ChecksumCRC64NVME.Key()),\n\t}, nil\n}\n"
        },
        {
          "name": "api-put-object.go",
          "type": "blob",
          "size": 16.15234375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"golang.org/x/net/http/httpguts\"\n)\n\n// ReplicationStatus represents replication status of object\ntype ReplicationStatus string\n\nconst (\n\t// ReplicationStatusPending indicates replication is pending\n\tReplicationStatusPending ReplicationStatus = \"PENDING\"\n\t// ReplicationStatusComplete indicates replication completed ok\n\tReplicationStatusComplete ReplicationStatus = \"COMPLETED\"\n\t// ReplicationStatusFailed indicates replication failed\n\tReplicationStatusFailed ReplicationStatus = \"FAILED\"\n\t// ReplicationStatusReplica indicates object is a replica of a source\n\tReplicationStatusReplica ReplicationStatus = \"REPLICA\"\n\t// ReplicationStatusReplicaEdge indicates object is a replica of a edge source\n\tReplicationStatusReplicaEdge ReplicationStatus = \"REPLICA-EDGE\"\n)\n\n// Empty returns true if no replication status set.\nfunc (r ReplicationStatus) Empty() bool {\n\treturn r == \"\"\n}\n\n// AdvancedPutOptions for internal use - to be utilized by replication, ILM transition\n// implementation on MinIO server\ntype AdvancedPutOptions struct {\n\tSourceVersionID          string\n\tSourceETag               string\n\tReplicationStatus        ReplicationStatus\n\tSourceMTime              time.Time\n\tReplicationRequest       bool\n\tRetentionTimestamp       time.Time\n\tTaggingTimestamp         time.Time\n\tLegalholdTimestamp       time.Time\n\tReplicationValidityCheck bool\n}\n\n// PutObjectOptions represents options specified by user for PutObject call\ntype PutObjectOptions struct {\n\tUserMetadata            map[string]string\n\tUserTags                map[string]string\n\tProgress                io.Reader\n\tContentType             string\n\tContentEncoding         string\n\tContentDisposition      string\n\tContentLanguage         string\n\tCacheControl            string\n\tExpires                 time.Time\n\tMode                    RetentionMode\n\tRetainUntilDate         time.Time\n\tServerSideEncryption    encrypt.ServerSide\n\tNumThreads              uint\n\tStorageClass            string\n\tWebsiteRedirectLocation string\n\tPartSize                uint64\n\tLegalHold               LegalHoldStatus\n\tSendContentMd5          bool\n\tDisableContentSha256    bool\n\tDisableMultipart        bool\n\n\t// AutoChecksum is the type of checksum that will be added if no other checksum is added,\n\t// like MD5 or SHA256 streaming checksum, and it is feasible for the upload type.\n\t// If none is specified CRC32C is used, since it is generally the fastest.\n\tAutoChecksum ChecksumType\n\n\t// Checksum will force a checksum of the specific type.\n\t// This requires that the client was created with \"TrailingHeaders:true\" option,\n\t// and that the destination server supports it.\n\t// Unavailable with V2 signatures & Google endpoints.\n\t// This will disable content MD5 checksums if set.\n\tChecksum ChecksumType\n\n\t// ConcurrentStreamParts will create NumThreads buffers of PartSize bytes,\n\t// fill them serially and upload them in parallel.\n\t// This can be used for faster uploads on non-seekable or slow-to-seek input.\n\tConcurrentStreamParts bool\n\tInternal              AdvancedPutOptions\n\n\tcustomHeaders http.Header\n}\n\n// SetMatchETag if etag matches while PUT MinIO returns an error\n// this is a MinIO specific extension to support optimistic locking\n// semantics.\nfunc (opts *PutObjectOptions) SetMatchETag(etag string) {\n\tif opts.customHeaders == nil {\n\t\topts.customHeaders = http.Header{}\n\t}\n\tif etag == \"*\" {\n\t\topts.customHeaders.Set(\"If-Match\", \"*\")\n\t} else {\n\t\topts.customHeaders.Set(\"If-Match\", \"\\\"\"+etag+\"\\\"\")\n\t}\n}\n\n// SetMatchETagExcept if etag does not match while PUT MinIO returns an\n// error this is a MinIO specific extension to support optimistic locking\n// semantics.\nfunc (opts *PutObjectOptions) SetMatchETagExcept(etag string) {\n\tif opts.customHeaders == nil {\n\t\topts.customHeaders = http.Header{}\n\t}\n\tif etag == \"*\" {\n\t\topts.customHeaders.Set(\"If-None-Match\", \"*\")\n\t} else {\n\t\topts.customHeaders.Set(\"If-None-Match\", \"\\\"\"+etag+\"\\\"\")\n\t}\n}\n\n// getNumThreads - gets the number of threads to be used in the multipart\n// put object operation\nfunc (opts PutObjectOptions) getNumThreads() (numThreads int) {\n\tif opts.NumThreads > 0 {\n\t\tnumThreads = int(opts.NumThreads)\n\t} else {\n\t\tnumThreads = totalWorkers\n\t}\n\treturn\n}\n\n// Header - constructs the headers from metadata entered by user in\n// PutObjectOptions struct\nfunc (opts PutObjectOptions) Header() (header http.Header) {\n\theader = make(http.Header)\n\n\tcontentType := opts.ContentType\n\tif contentType == \"\" {\n\t\tcontentType = \"application/octet-stream\"\n\t}\n\theader.Set(\"Content-Type\", contentType)\n\n\tif opts.ContentEncoding != \"\" {\n\t\theader.Set(\"Content-Encoding\", opts.ContentEncoding)\n\t}\n\tif opts.ContentDisposition != \"\" {\n\t\theader.Set(\"Content-Disposition\", opts.ContentDisposition)\n\t}\n\tif opts.ContentLanguage != \"\" {\n\t\theader.Set(\"Content-Language\", opts.ContentLanguage)\n\t}\n\tif opts.CacheControl != \"\" {\n\t\theader.Set(\"Cache-Control\", opts.CacheControl)\n\t}\n\n\tif !opts.Expires.IsZero() {\n\t\theader.Set(\"Expires\", opts.Expires.UTC().Format(http.TimeFormat))\n\t}\n\n\tif opts.Mode != \"\" {\n\t\theader.Set(amzLockMode, opts.Mode.String())\n\t}\n\n\tif !opts.RetainUntilDate.IsZero() {\n\t\theader.Set(\"X-Amz-Object-Lock-Retain-Until-Date\", opts.RetainUntilDate.Format(time.RFC3339))\n\t}\n\n\tif opts.LegalHold != \"\" {\n\t\theader.Set(amzLegalHoldHeader, opts.LegalHold.String())\n\t}\n\n\tif opts.ServerSideEncryption != nil {\n\t\topts.ServerSideEncryption.Marshal(header)\n\t}\n\n\tif opts.StorageClass != \"\" {\n\t\theader.Set(amzStorageClass, opts.StorageClass)\n\t}\n\n\tif opts.WebsiteRedirectLocation != \"\" {\n\t\theader.Set(amzWebsiteRedirectLocation, opts.WebsiteRedirectLocation)\n\t}\n\n\tif !opts.Internal.ReplicationStatus.Empty() {\n\t\theader.Set(amzBucketReplicationStatus, string(opts.Internal.ReplicationStatus))\n\t}\n\tif !opts.Internal.SourceMTime.IsZero() {\n\t\theader.Set(minIOBucketSourceMTime, opts.Internal.SourceMTime.Format(time.RFC3339Nano))\n\t}\n\tif opts.Internal.SourceETag != \"\" {\n\t\theader.Set(minIOBucketSourceETag, opts.Internal.SourceETag)\n\t}\n\tif opts.Internal.ReplicationRequest {\n\t\theader.Set(minIOBucketReplicationRequest, \"true\")\n\t}\n\tif opts.Internal.ReplicationValidityCheck {\n\t\theader.Set(minIOBucketReplicationCheck, \"true\")\n\t}\n\tif !opts.Internal.LegalholdTimestamp.IsZero() {\n\t\theader.Set(minIOBucketReplicationObjectLegalHoldTimestamp, opts.Internal.LegalholdTimestamp.Format(time.RFC3339Nano))\n\t}\n\tif !opts.Internal.RetentionTimestamp.IsZero() {\n\t\theader.Set(minIOBucketReplicationObjectRetentionTimestamp, opts.Internal.RetentionTimestamp.Format(time.RFC3339Nano))\n\t}\n\tif !opts.Internal.TaggingTimestamp.IsZero() {\n\t\theader.Set(minIOBucketReplicationTaggingTimestamp, opts.Internal.TaggingTimestamp.Format(time.RFC3339Nano))\n\t}\n\n\tif len(opts.UserTags) != 0 {\n\t\theader.Set(amzTaggingHeader, s3utils.TagEncode(opts.UserTags))\n\t}\n\n\tfor k, v := range opts.UserMetadata {\n\t\tif isAmzHeader(k) || isStandardHeader(k) || isStorageClassHeader(k) || isMinioHeader(k) {\n\t\t\theader.Set(k, v)\n\t\t} else {\n\t\t\theader.Set(\"x-amz-meta-\"+k, v)\n\t\t}\n\t}\n\n\t// set any other additional custom headers.\n\tfor k, v := range opts.customHeaders {\n\t\theader[k] = v\n\t}\n\n\treturn\n}\n\n// validate() checks if the UserMetadata map has standard headers or and raises an error if so.\nfunc (opts PutObjectOptions) validate(c *Client) (err error) {\n\tfor k, v := range opts.UserMetadata {\n\t\tif !httpguts.ValidHeaderFieldName(k) || isStandardHeader(k) || isSSEHeader(k) || isStorageClassHeader(k) || isMinioHeader(k) {\n\t\t\treturn errInvalidArgument(k + \" unsupported user defined metadata name\")\n\t\t}\n\t\tif !httpguts.ValidHeaderFieldValue(v) {\n\t\t\treturn errInvalidArgument(v + \" unsupported user defined metadata value\")\n\t\t}\n\t}\n\tif opts.Mode != \"\" && !opts.Mode.IsValid() {\n\t\treturn errInvalidArgument(opts.Mode.String() + \" unsupported retention mode\")\n\t}\n\tif opts.LegalHold != \"\" && !opts.LegalHold.IsValid() {\n\t\treturn errInvalidArgument(opts.LegalHold.String() + \" unsupported legal-hold status\")\n\t}\n\tif opts.Checksum.IsSet() {\n\t\tswitch {\n\t\tcase !c.trailingHeaderSupport:\n\t\t\treturn errInvalidArgument(\"Checksum requires Client with TrailingHeaders enabled\")\n\t\tcase c.overrideSignerType.IsV2():\n\t\t\treturn errInvalidArgument(\"Checksum cannot be used with v2 signatures\")\n\t\tcase s3utils.IsGoogleEndpoint(*c.endpointURL):\n\t\t\treturn errInvalidArgument(\"Checksum cannot be used with GCS endpoints\")\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// completedParts is a collection of parts sortable by their part numbers.\n// used for sorting the uploaded parts before completing the multipart request.\ntype completedParts []CompletePart\n\nfunc (a completedParts) Len() int           { return len(a) }\nfunc (a completedParts) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }\nfunc (a completedParts) Less(i, j int) bool { return a[i].PartNumber < a[j].PartNumber }\n\n// PutObject creates an object in a bucket.\n//\n// You must have WRITE permissions on a bucket to create an object.\n//\n//   - For size smaller than 16MiB PutObject automatically does a\n//     single atomic PUT operation.\n//\n//   - For size larger than 16MiB PutObject automatically does a\n//     multipart upload operation.\n//\n//   - For size input as -1 PutObject does a multipart Put operation\n//     until input stream reaches EOF. Maximum object size that can\n//     be uploaded through this operation will be 5TiB.\n//\n//     WARNING: Passing down '-1' will use memory and these cannot\n//     be reused for best outcomes for PutObject(), pass the size always.\n//\n// NOTE: Upon errors during upload multipart operation is entirely aborted.\nfunc (c *Client) PutObject(ctx context.Context, bucketName, objectName string, reader io.Reader, objectSize int64,\n\topts PutObjectOptions,\n) (info UploadInfo, err error) {\n\tif objectSize < 0 && opts.DisableMultipart {\n\t\treturn UploadInfo{}, errors.New(\"object size must be provided with disable multipart upload\")\n\t}\n\n\terr = opts.validate(c)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\treturn c.putObjectCommon(ctx, bucketName, objectName, reader, objectSize, opts)\n}\n\nfunc (c *Client) putObjectCommon(ctx context.Context, bucketName, objectName string, reader io.Reader, size int64, opts PutObjectOptions) (info UploadInfo, err error) {\n\t// Check for largest object size allowed.\n\tif size > int64(maxMultipartPutObjectSize) {\n\t\treturn UploadInfo{}, errEntityTooLarge(size, maxMultipartPutObjectSize, bucketName, objectName)\n\t}\n\topts.AutoChecksum.SetDefault(ChecksumCRC32C)\n\n\t// NOTE: Streaming signature is not supported by GCS.\n\tif s3utils.IsGoogleEndpoint(*c.endpointURL) {\n\t\treturn c.putObject(ctx, bucketName, objectName, reader, size, opts)\n\t}\n\n\tpartSize := opts.PartSize\n\tif opts.PartSize == 0 {\n\t\tpartSize = minPartSize\n\t}\n\n\tif c.overrideSignerType.IsV2() {\n\t\tif size >= 0 && size < int64(partSize) || opts.DisableMultipart {\n\t\t\treturn c.putObject(ctx, bucketName, objectName, reader, size, opts)\n\t\t}\n\t\treturn c.putObjectMultipart(ctx, bucketName, objectName, reader, size, opts)\n\t}\n\n\tif size < 0 {\n\t\tif opts.DisableMultipart {\n\t\t\treturn UploadInfo{}, errors.New(\"no length provided and multipart disabled\")\n\t\t}\n\t\tif opts.ConcurrentStreamParts && opts.NumThreads > 1 {\n\t\t\treturn c.putObjectMultipartStreamParallel(ctx, bucketName, objectName, reader, opts)\n\t\t}\n\t\treturn c.putObjectMultipartStreamNoLength(ctx, bucketName, objectName, reader, opts)\n\t}\n\n\tif size <= int64(partSize) || opts.DisableMultipart {\n\t\treturn c.putObject(ctx, bucketName, objectName, reader, size, opts)\n\t}\n\n\treturn c.putObjectMultipartStream(ctx, bucketName, objectName, reader, size, opts)\n}\n\nfunc (c *Client) putObjectMultipartStreamNoLength(ctx context.Context, bucketName, objectName string, reader io.Reader, opts PutObjectOptions) (info UploadInfo, err error) {\n\t// Input validation.\n\tif err = s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tif err = s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\t// Total data read and written to server. should be equal to\n\t// 'size' at the end of the call.\n\tvar totalUploadedSize int64\n\n\t// Complete multipart upload.\n\tvar complMultipartUpload completeMultipartUpload\n\n\t// Calculate the optimal parts info for a given size.\n\ttotalPartsCount, partSize, _, err := OptimalPartInfo(-1, opts.PartSize)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tif opts.Checksum.IsSet() {\n\t\topts.SendContentMd5 = false\n\t\topts.AutoChecksum = opts.Checksum\n\t}\n\tif !opts.SendContentMd5 {\n\t\taddAutoChecksumHeaders(&opts)\n\t}\n\n\t// Initiate a new multipart upload.\n\tuploadID, err := c.newUploadID(ctx, bucketName, objectName, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\tdelete(opts.UserMetadata, \"X-Amz-Checksum-Algorithm\")\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tc.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\t}\n\t}()\n\n\t// Part number always starts with '1'.\n\tpartNumber := 1\n\n\t// Initialize parts uploaded map.\n\tpartsInfo := make(map[int]ObjectPart)\n\n\t// Create a buffer.\n\tbuf := make([]byte, partSize)\n\n\t// Create checksums\n\t// CRC32C is ~50% faster on AMD64 @ 30GB/s\n\tcustomHeader := make(http.Header)\n\tcrc := opts.AutoChecksum.Hasher()\n\n\tfor partNumber <= totalPartsCount {\n\t\tlength, rerr := readFull(reader, buf)\n\t\tif rerr == io.EOF && partNumber > 1 {\n\t\t\tbreak\n\t\t}\n\n\t\tif rerr != nil && rerr != io.ErrUnexpectedEOF && rerr != io.EOF {\n\t\t\treturn UploadInfo{}, rerr\n\t\t}\n\n\t\tvar md5Base64 string\n\t\tif opts.SendContentMd5 {\n\t\t\t// Calculate md5sum.\n\t\t\thash := c.md5Hasher()\n\t\t\thash.Write(buf[:length])\n\t\t\tmd5Base64 = base64.StdEncoding.EncodeToString(hash.Sum(nil))\n\t\t\thash.Close()\n\t\t} else {\n\t\t\tcrc.Reset()\n\t\t\tcrc.Write(buf[:length])\n\t\t\tcSum := crc.Sum(nil)\n\t\t\tcustomHeader.Set(opts.AutoChecksum.Key(), base64.StdEncoding.EncodeToString(cSum))\n\t\t}\n\n\t\t// Update progress reader appropriately to the latest offset\n\t\t// as we read from the source.\n\t\trd := newHook(bytes.NewReader(buf[:length]), opts.Progress)\n\n\t\t// Proceed to upload the part.\n\t\tp := uploadPartParams{bucketName: bucketName, objectName: objectName, uploadID: uploadID, reader: rd, partNumber: partNumber, md5Base64: md5Base64, size: int64(length), sse: opts.ServerSideEncryption, streamSha256: !opts.DisableContentSha256, customHeader: customHeader}\n\t\tobjPart, uerr := c.uploadPart(ctx, p)\n\t\tif uerr != nil {\n\t\t\treturn UploadInfo{}, uerr\n\t\t}\n\n\t\t// Save successfully uploaded part metadata.\n\t\tpartsInfo[partNumber] = objPart\n\n\t\t// Save successfully uploaded size.\n\t\ttotalUploadedSize += int64(length)\n\n\t\t// Increment part number.\n\t\tpartNumber++\n\n\t\t// For unknown size, Read EOF we break away.\n\t\t// We do not have to upload till totalPartsCount.\n\t\tif rerr == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Loop over total uploaded parts to save them in\n\t// Parts array before completing the multipart request.\n\tallParts := make([]ObjectPart, 0, len(partsInfo))\n\tfor i := 1; i < partNumber; i++ {\n\t\tpart, ok := partsInfo[i]\n\t\tif !ok {\n\t\t\treturn UploadInfo{}, errInvalidArgument(fmt.Sprintf(\"Missing part number %d\", i))\n\t\t}\n\t\tallParts = append(allParts, part)\n\t\tcomplMultipartUpload.Parts = append(complMultipartUpload.Parts, CompletePart{\n\t\t\tETag:              part.ETag,\n\t\t\tPartNumber:        part.PartNumber,\n\t\t\tChecksumCRC32:     part.ChecksumCRC32,\n\t\t\tChecksumCRC32C:    part.ChecksumCRC32C,\n\t\t\tChecksumSHA1:      part.ChecksumSHA1,\n\t\t\tChecksumSHA256:    part.ChecksumSHA256,\n\t\t\tChecksumCRC64NVME: part.ChecksumCRC64NVME,\n\t\t})\n\t}\n\n\t// Sort all completed parts.\n\tsort.Sort(completedParts(complMultipartUpload.Parts))\n\n\topts = PutObjectOptions{\n\t\tServerSideEncryption: opts.ServerSideEncryption,\n\t\tAutoChecksum:         opts.AutoChecksum,\n\t}\n\tapplyAutoChecksum(&opts, allParts)\n\n\tuploadInfo, err := c.completeMultipartUpload(ctx, bucketName, objectName, uploadID, complMultipartUpload, opts)\n\tif err != nil {\n\t\treturn UploadInfo{}, err\n\t}\n\n\tuploadInfo.Size = totalUploadedSize\n\treturn uploadInfo, nil\n}\n"
        },
        {
          "name": "api-put-object_test.go",
          "type": "blob",
          "size": 4.97265625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage minio\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"net/http\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\nfunc TestPutObjectOptionsValidate(t *testing.T) {\n\ttestCases := []struct {\n\t\tname, value string\n\t\tshouldPass  bool\n\t}{\n\t\t// Invalid cases.\n\t\t{\"X-Amz-Matdesc\", \"blah\", false},\n\t\t{\"x-amz-meta-X-Amz-Iv\", \"blah\", false},\n\t\t{\"x-amz-meta-X-Amz-Key\", \"blah\", false},\n\t\t{\"x-amz-meta-X-Amz-Matdesc\", \"blah\", false},\n\t\t{\"It has spaces\", \"v\", false},\n\t\t{\"It,has@illegal=characters\", \"v\", false},\n\t\t{\"X-Amz-Iv\", \"blah\", false},\n\t\t{\"X-Amz-Key\", \"blah\", false},\n\t\t{\"X-Amz-Key-prefixed-header\", \"blah\", false},\n\t\t{\"Content-Type\", \"custom/content-type\", false},\n\t\t{\"content-type\", \"custom/content-type\", false},\n\t\t{\"Content-Encoding\", \"gzip\", false},\n\t\t{\"Cache-Control\", \"blah\", false},\n\t\t{\"Content-Disposition\", \"something\", false},\n\t\t{\"Content-Language\", \"somelanguage\", false},\n\n\t\t// Valid metadata names.\n\t\t{\"my-custom-header\", \"blah\", true},\n\t\t{\"custom-X-Amz-Key-middle\", \"blah\", true},\n\t\t{\"my-custom-header-X-Amz-Key\", \"blah\", true},\n\t\t{\"blah-X-Amz-Matdesc\", \"blah\", true},\n\t\t{\"X-Amz-MatDesc-suffix\", \"blah\", true},\n\t\t{\"It-Is-Fine\", \"v\", true},\n\t\t{\"Numbers-098987987-Should-Work\", \"v\", true},\n\t\t{\"Crazy-!#$%&'*+-.^_`|~-Should-193832-Be-Fine\", \"v\", true},\n\t}\n\tfor i, testCase := range testCases {\n\t\terr := PutObjectOptions{UserMetadata: map[string]string{\n\t\t\ttestCase.name: testCase.value,\n\t\t}}.validate(nil)\n\t\tif testCase.shouldPass && err != nil {\n\t\t\tt.Errorf(\"Test %d - output did not match with reference results, %s\", i+1, err)\n\t\t}\n\t}\n}\n\ntype InterceptRouteTripper struct {\n\trequest *http.Request\n}\n\nfunc (i *InterceptRouteTripper) RoundTrip(request *http.Request) (*http.Response, error) {\n\ti.request = request\n\treturn &http.Response{StatusCode: 200}, nil\n}\n\nfunc Test_SSEHeaders(t *testing.T) {\n\trt := &InterceptRouteTripper{}\n\tc, err := New(\"s3.amazonaws.com\", &Options{\n\t\tTransport: rt,\n\t})\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\ttestCases := map[string]struct {\n\t\tsse                            func() encrypt.ServerSide\n\t\tinitiateMultipartUploadHeaders http.Header\n\t\theaderNotAllowedAfterInit      []string\n\t}{\n\t\t\"noEncryption\": {\n\t\t\tsse:                            func() encrypt.ServerSide { return nil },\n\t\t\tinitiateMultipartUploadHeaders: http.Header{},\n\t\t},\n\t\t\"sse\": {\n\t\t\tsse: func() encrypt.ServerSide {\n\t\t\t\ts, err := encrypt.NewSSEKMS(\"keyId\", nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Error(err)\n\t\t\t\t}\n\t\t\t\treturn s\n\t\t\t},\n\t\t\tinitiateMultipartUploadHeaders: http.Header{\n\t\t\t\tencrypt.SseGenericHeader: []string{\"aws:kms\"},\n\t\t\t\tencrypt.SseKmsKeyID:      []string{\"keyId\"},\n\t\t\t},\n\t\t\theaderNotAllowedAfterInit: []string{encrypt.SseGenericHeader, encrypt.SseKmsKeyID, encrypt.SseEncryptionContext},\n\t\t},\n\t\t\"sse with context\": {\n\t\t\tsse: func() encrypt.ServerSide {\n\t\t\t\ts, err := encrypt.NewSSEKMS(\"keyId\", \"context\")\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Error(err)\n\t\t\t\t}\n\t\t\t\treturn s\n\t\t\t},\n\t\t\tinitiateMultipartUploadHeaders: http.Header{\n\t\t\t\tencrypt.SseGenericHeader:     []string{\"aws:kms\"},\n\t\t\t\tencrypt.SseKmsKeyID:          []string{\"keyId\"},\n\t\t\t\tencrypt.SseEncryptionContext: []string{base64.StdEncoding.EncodeToString([]byte(\"\\\"context\\\"\"))},\n\t\t\t},\n\t\t\theaderNotAllowedAfterInit: []string{encrypt.SseGenericHeader, encrypt.SseKmsKeyID, encrypt.SseEncryptionContext},\n\t\t},\n\t}\n\n\tfor name, tc := range testCases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\topts := PutObjectOptions{\n\t\t\t\tServerSideEncryption: tc.sse(),\n\t\t\t}\n\t\t\tc.bucketLocCache.Set(\"test\", \"region\")\n\t\t\tc.initiateMultipartUpload(context.Background(), \"test\", \"test\", opts)\n\t\t\tfor s, vls := range tc.initiateMultipartUploadHeaders {\n\t\t\t\tif !reflect.DeepEqual(rt.request.Header[s], vls) {\n\t\t\t\t\tt.Errorf(\"Header %v are not equal, want: %v got %v\", s, vls, rt.request.Header[s])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t_, err := c.uploadPart(context.Background(), uploadPartParams{\n\t\t\t\tbucketName: \"test\",\n\t\t\t\tobjectName: \"test\",\n\t\t\t\tpartNumber: 1,\n\t\t\t\tuploadID:   \"upId\",\n\t\t\t\tsse:        opts.ServerSideEncryption,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\n\t\t\tfor _, k := range tc.headerNotAllowedAfterInit {\n\t\t\t\tif rt.request.Header.Get(k) != \"\" {\n\t\t\t\t\tt.Errorf(\"header %v should not be set\", k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tc.completeMultipartUpload(context.Background(), \"test\", \"test\", \"upId\", completeMultipartUpload{}, opts)\n\n\t\t\tfor _, k := range tc.headerNotAllowedAfterInit {\n\t\t\t\tif rt.request.Header.Get(k) != \"\" {\n\t\t\t\t\tt.Errorf(\"header %v should not be set\", k)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "api-putobject-snowball.go",
          "type": "blob",
          "size": 6.0029296875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2021 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"archive/tar\"\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n)\n\n// SnowballOptions contains options for PutObjectsSnowball calls.\ntype SnowballOptions struct {\n\t// Opts is options applied to all objects.\n\tOpts PutObjectOptions\n\n\t// Processing options:\n\n\t// InMemory specifies that all objects should be collected in memory\n\t// before they are uploaded.\n\t// If false a temporary file will be created.\n\tInMemory bool\n\n\t// Compress enabled content compression before upload.\n\t// Compression will typically reduce memory and network usage,\n\t// Compression can safely be enabled with MinIO hosts.\n\tCompress bool\n\n\t// SkipErrs if enabled will skip any errors while reading the\n\t// object content while creating the snowball archive\n\tSkipErrs bool\n}\n\n// SnowballObject contains information about a single object to be added to the snowball.\ntype SnowballObject struct {\n\t// Key is the destination key, including prefix.\n\tKey string\n\n\t// Size is the content size of this object.\n\tSize int64\n\n\t// Modtime to apply to the object.\n\t// If Modtime is the zero value current time will be used.\n\tModTime time.Time\n\n\t// Content of the object.\n\t// Exactly 'Size' number of bytes must be provided.\n\tContent io.Reader\n\n\t// VersionID of the object; if empty, a new versionID will be generated\n\tVersionID string\n\n\t// Headers contains more options for this object upload, the same as you\n\t// would include in a regular PutObject operation, such as user metadata\n\t// and content-disposition, expires, ..\n\tHeaders http.Header\n\n\t// Close will be called when an object has finished processing.\n\t// Note that if PutObjectsSnowball returns because of an error,\n\t// objects not consumed from the input will NOT have been closed.\n\t// Leave as nil for no callback.\n\tClose func()\n}\n\ntype nopReadSeekCloser struct {\n\tio.ReadSeeker\n}\n\nfunc (n nopReadSeekCloser) Close() error {\n\treturn nil\n}\n\n// This is available as io.ReadSeekCloser from go1.16\ntype readSeekCloser interface {\n\tio.Reader\n\tio.Closer\n\tio.Seeker\n}\n\n// PutObjectsSnowball will put multiple objects with a single put call.\n// A (compressed) TAR file will be created which will contain multiple objects.\n// The key for each object will be used for the destination in the specified bucket.\n// Total size should be < 5TB.\n// This function blocks until 'objs' is closed and the content has been uploaded.\nfunc (c Client) PutObjectsSnowball(ctx context.Context, bucketName string, opts SnowballOptions, objs <-chan SnowballObject) (err error) {\n\terr = opts.Opts.validate(&c)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar tmpWriter io.Writer\n\tvar getTmpReader func() (rc readSeekCloser, sz int64, err error)\n\tif opts.InMemory {\n\t\tb := bytes.NewBuffer(nil)\n\t\ttmpWriter = b\n\t\tgetTmpReader = func() (readSeekCloser, int64, error) {\n\t\t\treturn nopReadSeekCloser{bytes.NewReader(b.Bytes())}, int64(b.Len()), nil\n\t\t}\n\t} else {\n\t\tf, err := os.CreateTemp(\"\", \"s3-putsnowballobjects-*\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tname := f.Name()\n\t\ttmpWriter = f\n\t\tvar once sync.Once\n\t\tdefer once.Do(func() {\n\t\t\tf.Close()\n\t\t})\n\t\tdefer os.Remove(name)\n\t\tgetTmpReader = func() (readSeekCloser, int64, error) {\n\t\t\tonce.Do(func() {\n\t\t\t\tf.Close()\n\t\t\t})\n\t\t\tf, err := os.Open(name)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\tst, err := f.Stat()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\treturn f, st.Size(), nil\n\t\t}\n\t}\n\tflush := func() error { return nil }\n\tif !opts.Compress {\n\t\tif !opts.InMemory {\n\t\t\t// Insert buffer for writes.\n\t\t\tbuf := bufio.NewWriterSize(tmpWriter, 1<<20)\n\t\t\tflush = buf.Flush\n\t\t\ttmpWriter = buf\n\t\t}\n\t} else {\n\t\ts2c := s2.NewWriter(tmpWriter, s2.WriterBetterCompression())\n\t\tflush = s2c.Close\n\t\tdefer s2c.Close()\n\t\ttmpWriter = s2c\n\t}\n\tt := tar.NewWriter(tmpWriter)\n\nobjectLoop:\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase obj, ok := <-objs:\n\t\t\tif !ok {\n\t\t\t\tbreak objectLoop\n\t\t\t}\n\n\t\t\tcloseObj := func() {}\n\t\t\tif obj.Close != nil {\n\t\t\t\tcloseObj = obj.Close\n\t\t\t}\n\n\t\t\t// Trim accidental slash prefix.\n\t\t\tobj.Key = strings.TrimPrefix(obj.Key, \"/\")\n\t\t\theader := tar.Header{\n\t\t\t\tTypeflag: tar.TypeReg,\n\t\t\t\tName:     obj.Key,\n\t\t\t\tSize:     obj.Size,\n\t\t\t\tModTime:  obj.ModTime,\n\t\t\t\tFormat:   tar.FormatPAX,\n\t\t\t}\n\t\t\tif header.ModTime.IsZero() {\n\t\t\t\theader.ModTime = time.Now().UTC()\n\t\t\t}\n\n\t\t\theader.PAXRecords = make(map[string]string)\n\t\t\tif obj.VersionID != \"\" {\n\t\t\t\theader.PAXRecords[\"minio.versionId\"] = obj.VersionID\n\t\t\t}\n\t\t\tfor k, vals := range obj.Headers {\n\t\t\t\theader.PAXRecords[\"minio.metadata.\"+k] = strings.Join(vals, \",\")\n\t\t\t}\n\n\t\t\tif err := t.WriteHeader(&header); err != nil {\n\t\t\t\tcloseObj()\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tn, err := io.Copy(t, obj.Content)\n\t\t\tif err != nil {\n\t\t\t\tcloseObj()\n\t\t\t\tif opts.SkipErrs {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif n != obj.Size {\n\t\t\t\tcloseObj()\n\t\t\t\tif opts.SkipErrs {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tcloseObj()\n\t\t}\n\t}\n\t// Flush tar\n\terr = t.Flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Flush compression\n\terr = flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif opts.Opts.UserMetadata == nil {\n\t\topts.Opts.UserMetadata = map[string]string{}\n\t}\n\topts.Opts.UserMetadata[\"X-Amz-Meta-Snowball-Auto-Extract\"] = \"true\"\n\topts.Opts.DisableMultipart = true\n\trc, sz, err := getTmpReader()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer rc.Close()\n\trand := c.random.Uint64()\n\t_, err = c.PutObject(ctx, bucketName, fmt.Sprintf(\"snowball-upload-%x.tar\", rand), rc, sz, opts.Opts)\n\treturn err\n}\n"
        },
        {
          "name": "api-remove.go",
          "type": "blob",
          "size": 15.708984375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n//revive:disable\n\n// Deprecated: BucketOptions will be renamed to RemoveBucketOptions in future versions.\ntype BucketOptions = RemoveBucketOptions\n\n//revive:enable\n\n// RemoveBucketOptions special headers to purge buckets, only\n// useful when endpoint is MinIO\ntype RemoveBucketOptions struct {\n\tForceDelete bool\n}\n\n// RemoveBucketWithOptions deletes the bucket name.\n//\n// All objects (including all object versions and delete markers)\n// in the bucket will be deleted forcibly if bucket options set\n// ForceDelete to 'true'.\nfunc (c *Client) RemoveBucketWithOptions(ctx context.Context, bucketName string, opts RemoveBucketOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\n\t// Build headers.\n\theaders := make(http.Header)\n\tif opts.ForceDelete {\n\t\theaders.Set(minIOForceDelete, \"true\")\n\t}\n\n\t// Execute DELETE on bucket.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\t// Remove the location from cache on a successful delete.\n\tc.bucketLocCache.Delete(bucketName)\n\treturn nil\n}\n\n// RemoveBucket deletes the bucket name.\n//\n//\tAll objects (including all object versions and delete markers).\n//\tin the bucket must be deleted before successfully attempting this request.\nfunc (c *Client) RemoveBucket(ctx context.Context, bucketName string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\t// Execute DELETE on bucket.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusNoContent {\n\t\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\n\t// Remove the location from cache on a successful delete.\n\tc.bucketLocCache.Delete(bucketName)\n\n\treturn nil\n}\n\n// AdvancedRemoveOptions intended for internal use by replication\ntype AdvancedRemoveOptions struct {\n\tReplicationDeleteMarker  bool\n\tReplicationStatus        ReplicationStatus\n\tReplicationMTime         time.Time\n\tReplicationRequest       bool\n\tReplicationValidityCheck bool // check permissions\n}\n\n// RemoveObjectOptions represents options specified by user for RemoveObject call\ntype RemoveObjectOptions struct {\n\tForceDelete      bool\n\tGovernanceBypass bool\n\tVersionID        string\n\tInternal         AdvancedRemoveOptions\n}\n\n// RemoveObject removes an object from a bucket.\nfunc (c *Client) RemoveObject(ctx context.Context, bucketName, objectName string, opts RemoveObjectOptions) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\tres := c.removeObject(ctx, bucketName, objectName, opts)\n\treturn res.Err\n}\n\nfunc (c *Client) removeObject(ctx context.Context, bucketName, objectName string, opts RemoveObjectOptions) RemoveObjectResult {\n\t// Get resources properly escaped and lined up before\n\t// using them in http request.\n\turlValues := make(url.Values)\n\n\tif opts.VersionID != \"\" {\n\t\turlValues.Set(\"versionId\", opts.VersionID)\n\t}\n\n\t// Build headers.\n\theaders := make(http.Header)\n\n\tif opts.GovernanceBypass {\n\t\t// Set the bypass goverenance retention header\n\t\theaders.Set(amzBypassGovernance, \"true\")\n\t}\n\tif opts.Internal.ReplicationDeleteMarker {\n\t\theaders.Set(minIOBucketReplicationDeleteMarker, \"true\")\n\t}\n\tif !opts.Internal.ReplicationMTime.IsZero() {\n\t\theaders.Set(minIOBucketSourceMTime, opts.Internal.ReplicationMTime.Format(time.RFC3339Nano))\n\t}\n\tif !opts.Internal.ReplicationStatus.Empty() {\n\t\theaders.Set(amzBucketReplicationStatus, string(opts.Internal.ReplicationStatus))\n\t}\n\tif opts.Internal.ReplicationRequest {\n\t\theaders.Set(minIOBucketReplicationRequest, \"true\")\n\t}\n\tif opts.Internal.ReplicationValidityCheck {\n\t\theaders.Set(minIOBucketReplicationCheck, \"true\")\n\t}\n\tif opts.ForceDelete {\n\t\theaders.Set(minIOForceDelete, \"true\")\n\t}\n\t// Execute DELETE on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tqueryValues:      urlValues,\n\t\tcustomHeader:     headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn RemoveObjectResult{Err: err}\n\t}\n\tif resp != nil {\n\t\t// if some unexpected error happened and max retry is reached, we want to let client know\n\t\tif resp.StatusCode != http.StatusNoContent {\n\t\t\terr := httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t\treturn RemoveObjectResult{Err: err}\n\t\t}\n\t}\n\n\t// DeleteObject always responds with http '204' even for\n\t// objects which do not exist. So no need to handle them\n\t// specifically.\n\treturn RemoveObjectResult{\n\t\tObjectName:            objectName,\n\t\tObjectVersionID:       opts.VersionID,\n\t\tDeleteMarker:          resp.Header.Get(\"x-amz-delete-marker\") == \"true\",\n\t\tDeleteMarkerVersionID: resp.Header.Get(\"x-amz-version-id\"),\n\t}\n}\n\n// RemoveObjectError - container of Multi Delete S3 API error\ntype RemoveObjectError struct {\n\tObjectName string\n\tVersionID  string\n\tErr        error\n}\n\n// RemoveObjectResult - container of Multi Delete S3 API result\ntype RemoveObjectResult struct {\n\tObjectName      string\n\tObjectVersionID string\n\n\tDeleteMarker          bool\n\tDeleteMarkerVersionID string\n\n\tErr error\n}\n\n// generateRemoveMultiObjects - generate the XML request for remove multi objects request\nfunc generateRemoveMultiObjectsRequest(objects []ObjectInfo) []byte {\n\tdelObjects := []deleteObject{}\n\tfor _, obj := range objects {\n\t\tdelObjects = append(delObjects, deleteObject{\n\t\t\tKey:       obj.Key,\n\t\t\tVersionID: obj.VersionID,\n\t\t})\n\t}\n\txmlBytes, _ := xml.Marshal(deleteMultiObjects{Objects: delObjects, Quiet: false})\n\treturn xmlBytes\n}\n\n// processRemoveMultiObjectsResponse - parse the remove multi objects web service\n// and return the success/failure result status for each object\nfunc processRemoveMultiObjectsResponse(body io.Reader, resultCh chan<- RemoveObjectResult) {\n\t// Parse multi delete XML response\n\trmResult := &deleteMultiObjectsResult{}\n\terr := xmlDecoder(body, rmResult)\n\tif err != nil {\n\t\tresultCh <- RemoveObjectResult{ObjectName: \"\", Err: err}\n\t\treturn\n\t}\n\n\t// Fill deletion that returned success\n\tfor _, obj := range rmResult.DeletedObjects {\n\t\tresultCh <- RemoveObjectResult{\n\t\t\tObjectName: obj.Key,\n\t\t\t// Only filled with versioned buckets\n\t\t\tObjectVersionID:       obj.VersionID,\n\t\t\tDeleteMarker:          obj.DeleteMarker,\n\t\t\tDeleteMarkerVersionID: obj.DeleteMarkerVersionID,\n\t\t}\n\t}\n\n\t// Fill deletion that returned an error.\n\tfor _, obj := range rmResult.UnDeletedObjects {\n\t\t// Version does not exist is not an error ignore and continue.\n\t\tswitch obj.Code {\n\t\tcase \"InvalidArgument\", \"NoSuchVersion\":\n\t\t\tcontinue\n\t\t}\n\t\tresultCh <- RemoveObjectResult{\n\t\t\tObjectName:      obj.Key,\n\t\t\tObjectVersionID: obj.VersionID,\n\t\t\tErr: ErrorResponse{\n\t\t\t\tCode:    obj.Code,\n\t\t\t\tMessage: obj.Message,\n\t\t\t},\n\t\t}\n\t}\n}\n\n// RemoveObjectsOptions represents options specified by user for RemoveObjects call\ntype RemoveObjectsOptions struct {\n\tGovernanceBypass bool\n}\n\n// RemoveObjects removes multiple objects from a bucket while\n// it is possible to specify objects versions which are received from\n// objectsCh. Remove failures are sent back via error channel.\nfunc (c *Client) RemoveObjects(ctx context.Context, bucketName string, objectsCh <-chan ObjectInfo, opts RemoveObjectsOptions) <-chan RemoveObjectError {\n\terrorCh := make(chan RemoveObjectError, 1)\n\n\t// Validate if bucket name is valid.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(errorCh)\n\t\terrorCh <- RemoveObjectError{\n\t\t\tErr: err,\n\t\t}\n\t\treturn errorCh\n\t}\n\t// Validate objects channel to be properly allocated.\n\tif objectsCh == nil {\n\t\tdefer close(errorCh)\n\t\terrorCh <- RemoveObjectError{\n\t\t\tErr: errInvalidArgument(\"Objects channel cannot be nil\"),\n\t\t}\n\t\treturn errorCh\n\t}\n\n\tresultCh := make(chan RemoveObjectResult, 1)\n\tgo c.removeObjects(ctx, bucketName, objectsCh, resultCh, opts)\n\tgo func() {\n\t\tdefer close(errorCh)\n\t\tfor res := range resultCh {\n\t\t\t// Send only errors to the error channel\n\t\t\tif res.Err == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\terrorCh <- RemoveObjectError{\n\t\t\t\tObjectName: res.ObjectName,\n\t\t\t\tVersionID:  res.ObjectVersionID,\n\t\t\t\tErr:        res.Err,\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn errorCh\n}\n\n// RemoveObjectsWithResult removes multiple objects from a bucket while\n// it is possible to specify objects versions which are received from\n// objectsCh. Remove results, successes and failures are sent back via\n// RemoveObjectResult channel\nfunc (c *Client) RemoveObjectsWithResult(ctx context.Context, bucketName string, objectsCh <-chan ObjectInfo, opts RemoveObjectsOptions) <-chan RemoveObjectResult {\n\tresultCh := make(chan RemoveObjectResult, 1)\n\n\t// Validate if bucket name is valid.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\tdefer close(resultCh)\n\t\tresultCh <- RemoveObjectResult{\n\t\t\tErr: err,\n\t\t}\n\t\treturn resultCh\n\t}\n\t// Validate objects channel to be properly allocated.\n\tif objectsCh == nil {\n\t\tdefer close(resultCh)\n\t\tresultCh <- RemoveObjectResult{\n\t\t\tErr: errInvalidArgument(\"Objects channel cannot be nil\"),\n\t\t}\n\t\treturn resultCh\n\t}\n\n\tgo c.removeObjects(ctx, bucketName, objectsCh, resultCh, opts)\n\treturn resultCh\n}\n\n// Return true if the character is within the allowed characters in an XML 1.0 document\n// The list of allowed characters can be found here: https://www.w3.org/TR/xml/#charsets\nfunc validXMLChar(r rune) (ok bool) {\n\treturn r == 0x09 ||\n\t\tr == 0x0A ||\n\t\tr == 0x0D ||\n\t\tr >= 0x20 && r <= 0xD7FF ||\n\t\tr >= 0xE000 && r <= 0xFFFD ||\n\t\tr >= 0x10000 && r <= 0x10FFFF\n}\n\nfunc hasInvalidXMLChar(str string) bool {\n\tfor _, s := range str {\n\t\tif !validXMLChar(s) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Generate and call MultiDelete S3 requests based on entries received from objectsCh\nfunc (c *Client) removeObjects(ctx context.Context, bucketName string, objectsCh <-chan ObjectInfo, resultCh chan<- RemoveObjectResult, opts RemoveObjectsOptions) {\n\tmaxEntries := 1000\n\tfinish := false\n\turlValues := make(url.Values)\n\turlValues.Set(\"delete\", \"\")\n\n\t// Close result channel when Multi delete finishes.\n\tdefer close(resultCh)\n\n\t// Loop over entries by 1000 and call MultiDelete requests\n\tfor {\n\t\tif finish {\n\t\t\tbreak\n\t\t}\n\t\tcount := 0\n\t\tvar batch []ObjectInfo\n\n\t\t// Try to gather 1000 entries\n\t\tfor object := range objectsCh {\n\t\t\tif hasInvalidXMLChar(object.Key) {\n\t\t\t\t// Use single DELETE so the object name will be in the request URL instead of the multi-delete XML document.\n\t\t\t\tremoveResult := c.removeObject(ctx, bucketName, object.Key, RemoveObjectOptions{\n\t\t\t\t\tVersionID:        object.VersionID,\n\t\t\t\t\tGovernanceBypass: opts.GovernanceBypass,\n\t\t\t\t})\n\t\t\t\tif err := removeResult.Err; err != nil {\n\t\t\t\t\t// Version does not exist is not an error ignore and continue.\n\t\t\t\t\tswitch ToErrorResponse(err).Code {\n\t\t\t\t\tcase \"InvalidArgument\", \"NoSuchVersion\":\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tresultCh <- removeResult\n\t\t\t\t}\n\n\t\t\t\tresultCh <- removeResult\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tbatch = append(batch, object)\n\t\t\tif count++; count >= maxEntries {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif count == 0 {\n\t\t\t// Multi Objects Delete API doesn't accept empty object list, quit immediately\n\t\t\tbreak\n\t\t}\n\t\tif count < maxEntries {\n\t\t\t// We didn't have 1000 entries, so this is the last batch\n\t\t\tfinish = true\n\t\t}\n\n\t\t// Build headers.\n\t\theaders := make(http.Header)\n\t\tif opts.GovernanceBypass {\n\t\t\t// Set the bypass goverenance retention header\n\t\t\theaders.Set(amzBypassGovernance, \"true\")\n\t\t}\n\n\t\t// Generate remove multi objects XML request\n\t\tremoveBytes := generateRemoveMultiObjectsRequest(batch)\n\t\t// Execute POST on bucket to remove objects.\n\t\tresp, err := c.executeMethod(ctx, http.MethodPost, requestMetadata{\n\t\t\tbucketName:       bucketName,\n\t\t\tqueryValues:      urlValues,\n\t\t\tcontentBody:      bytes.NewReader(removeBytes),\n\t\t\tcontentLength:    int64(len(removeBytes)),\n\t\t\tcontentMD5Base64: sumMD5Base64(removeBytes),\n\t\t\tcontentSHA256Hex: sum256Hex(removeBytes),\n\t\t\tcustomHeader:     headers,\n\t\t})\n\t\tif resp != nil {\n\t\t\tif resp.StatusCode != http.StatusOK {\n\t\t\t\te := httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t\t\tresultCh <- RemoveObjectResult{ObjectName: \"\", Err: e}\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tfor _, b := range batch {\n\t\t\t\tresultCh <- RemoveObjectResult{\n\t\t\t\t\tObjectName:      b.Key,\n\t\t\t\t\tObjectVersionID: b.VersionID,\n\t\t\t\t\tErr:             err,\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Process multiobjects remove xml response\n\t\tprocessRemoveMultiObjectsResponse(resp.Body, resultCh)\n\n\t\tcloseResponse(resp)\n\t}\n}\n\n// RemoveIncompleteUpload aborts an partially uploaded object.\nfunc (c *Client) RemoveIncompleteUpload(ctx context.Context, bucketName, objectName string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\t// Find multipart upload ids of the object to be aborted.\n\tuploadIDs, err := c.findUploadIDs(ctx, bucketName, objectName)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, uploadID := range uploadIDs {\n\t\t// abort incomplete multipart upload, based on the upload id passed.\n\t\terr := c.abortMultipartUpload(ctx, bucketName, objectName, uploadID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// abortMultipartUpload aborts a multipart upload for the given\n// uploadID, all previously uploaded parts are deleted.\nfunc (c *Client) abortMultipartUpload(ctx context.Context, bucketName, objectName, uploadID string) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\t// Initialize url queries.\n\turlValues := make(url.Values)\n\turlValues.Set(\"uploadId\", uploadID)\n\n\t// Execute DELETE on multipart upload.\n\tresp, err := c.executeMethod(ctx, http.MethodDelete, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusNoContent {\n\t\t\t// Abort has no response body, handle it for any errors.\n\t\t\tvar errorResponse ErrorResponse\n\t\t\tswitch resp.StatusCode {\n\t\t\tcase http.StatusNotFound:\n\t\t\t\t// This is needed specifically for abort and it cannot\n\t\t\t\t// be converged into default case.\n\t\t\t\terrorResponse = ErrorResponse{\n\t\t\t\t\tCode:       \"NoSuchUpload\",\n\t\t\t\t\tMessage:    \"The specified multipart upload does not exist.\",\n\t\t\t\t\tBucketName: bucketName,\n\t\t\t\t\tKey:        objectName,\n\t\t\t\t\tRequestID:  resp.Header.Get(\"x-amz-request-id\"),\n\t\t\t\t\tHostID:     resp.Header.Get(\"x-amz-id-2\"),\n\t\t\t\t\tRegion:     resp.Header.Get(\"x-amz-bucket-region\"),\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t\t}\n\t\t\treturn errorResponse\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api-restore.go",
          "type": "blob",
          "size": 5.65234375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * (C) 2018-2021 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/tags\"\n)\n\n// RestoreType represents the restore request type\ntype RestoreType string\n\nconst (\n\t// RestoreSelect represents the restore SELECT operation\n\tRestoreSelect = RestoreType(\"SELECT\")\n)\n\n// TierType represents a retrieval tier\ntype TierType string\n\nconst (\n\t// TierStandard is the standard retrieval tier\n\tTierStandard = TierType(\"Standard\")\n\t// TierBulk is the bulk retrieval tier\n\tTierBulk = TierType(\"Bulk\")\n\t// TierExpedited is the expedited retrieval tier\n\tTierExpedited = TierType(\"Expedited\")\n)\n\n// GlacierJobParameters represents the retrieval tier parameter\ntype GlacierJobParameters struct {\n\tTier TierType\n}\n\n// Encryption contains the type of server-side encryption used during object retrieval\ntype Encryption struct {\n\tEncryptionType string\n\tKMSContext     string\n\tKMSKeyID       string `xml:\"KMSKeyId\"`\n}\n\n// MetadataEntry represents a metadata information of the restored object.\ntype MetadataEntry struct {\n\tName  string\n\tValue string\n}\n\n// S3 holds properties of the copy of the archived object\ntype S3 struct {\n\tAccessControlList *AccessControlList `xml:\"AccessControlList,omitempty\"`\n\tBucketName        string\n\tPrefix            string\n\tCannedACL         *string        `xml:\"CannedACL,omitempty\"`\n\tEncryption        *Encryption    `xml:\"Encryption,omitempty\"`\n\tStorageClass      *string        `xml:\"StorageClass,omitempty\"`\n\tTagging           *tags.Tags     `xml:\"Tagging,omitempty\"`\n\tUserMetadata      *MetadataEntry `xml:\"UserMetadata,omitempty\"`\n}\n\n// SelectParameters holds the select request parameters\ntype SelectParameters struct {\n\tXMLName             xml.Name `xml:\"SelectParameters\"`\n\tExpressionType      QueryExpressionType\n\tExpression          string\n\tInputSerialization  SelectObjectInputSerialization\n\tOutputSerialization SelectObjectOutputSerialization\n}\n\n// OutputLocation holds properties of the copy of the archived object\ntype OutputLocation struct {\n\tXMLName xml.Name `xml:\"OutputLocation\"`\n\tS3      S3       `xml:\"S3\"`\n}\n\n// RestoreRequest holds properties of the restore object request\ntype RestoreRequest struct {\n\tXMLName              xml.Name              `xml:\"http://s3.amazonaws.com/doc/2006-03-01/ RestoreRequest\"`\n\tType                 *RestoreType          `xml:\"Type,omitempty\"`\n\tTier                 *TierType             `xml:\"Tier,omitempty\"`\n\tDays                 *int                  `xml:\"Days,omitempty\"`\n\tGlacierJobParameters *GlacierJobParameters `xml:\"GlacierJobParameters,omitempty\"`\n\tDescription          *string               `xml:\"Description,omitempty\"`\n\tSelectParameters     *SelectParameters     `xml:\"SelectParameters,omitempty\"`\n\tOutputLocation       *OutputLocation       `xml:\"OutputLocation,omitempty\"`\n}\n\n// SetDays sets the days parameter of the restore request\nfunc (r *RestoreRequest) SetDays(v int) {\n\tr.Days = &v\n}\n\n// SetGlacierJobParameters sets the GlacierJobParameters of the restore request\nfunc (r *RestoreRequest) SetGlacierJobParameters(v GlacierJobParameters) {\n\tr.GlacierJobParameters = &v\n}\n\n// SetType sets the type of the restore request\nfunc (r *RestoreRequest) SetType(v RestoreType) {\n\tr.Type = &v\n}\n\n// SetTier sets the retrieval tier of the restore request\nfunc (r *RestoreRequest) SetTier(v TierType) {\n\tr.Tier = &v\n}\n\n// SetDescription sets the description of the restore request\nfunc (r *RestoreRequest) SetDescription(v string) {\n\tr.Description = &v\n}\n\n// SetSelectParameters sets SelectParameters of the restore select request\nfunc (r *RestoreRequest) SetSelectParameters(v SelectParameters) {\n\tr.SelectParameters = &v\n}\n\n// SetOutputLocation sets the properties of the copy of the archived object\nfunc (r *RestoreRequest) SetOutputLocation(v OutputLocation) {\n\tr.OutputLocation = &v\n}\n\n// RestoreObject is a implementation of https://docs.aws.amazon.com/AmazonS3/latest/API/API_RestoreObject.html AWS S3 API\nfunc (c *Client) RestoreObject(ctx context.Context, bucketName, objectName, versionID string, req RestoreRequest) error {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn err\n\t}\n\n\trestoreRequestBytes, err := xml.Marshal(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\turlValues := make(url.Values)\n\turlValues.Set(\"restore\", \"\")\n\tif versionID != \"\" {\n\t\turlValues.Set(\"versionId\", versionID)\n\t}\n\n\t// Execute POST on bucket/object.\n\tresp, err := c.executeMethod(ctx, http.MethodPost, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcontentMD5Base64: sumMD5Base64(restoreRequestBytes),\n\t\tcontentSHA256Hex: sum256Hex(restoreRequestBytes),\n\t\tcontentBody:      bytes.NewReader(restoreRequestBytes),\n\t\tcontentLength:    int64(len(restoreRequestBytes)),\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif resp.StatusCode != http.StatusAccepted && resp.StatusCode != http.StatusOK {\n\t\treturn httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api-s3-datatypes.go",
          "type": "blob",
          "size": 12.443359375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"encoding/base64\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"io\"\n\t\"reflect\"\n\t\"time\"\n)\n\n// listAllMyBucketsResult container for listBuckets response.\ntype listAllMyBucketsResult struct {\n\t// Container for one or more buckets.\n\tBuckets struct {\n\t\tBucket []BucketInfo\n\t}\n\tOwner owner\n}\n\n// owner container for bucket owner information.\ntype owner struct {\n\tDisplayName string\n\tID          string\n}\n\n// CommonPrefix container for prefix response.\ntype CommonPrefix struct {\n\tPrefix string\n}\n\n// ListBucketV2Result container for listObjects response version 2.\ntype ListBucketV2Result struct {\n\t// A response can contain CommonPrefixes only if you have\n\t// specified a delimiter.\n\tCommonPrefixes []CommonPrefix\n\t// Metadata about each object returned.\n\tContents  []ObjectInfo\n\tDelimiter string\n\n\t// Encoding type used to encode object keys in the response.\n\tEncodingType string\n\n\t// A flag that indicates whether or not ListObjects returned all of the results\n\t// that satisfied the search criteria.\n\tIsTruncated bool\n\tMaxKeys     int64\n\tName        string\n\n\t// Hold the token that will be sent in the next request to fetch the next group of keys\n\tNextContinuationToken string\n\n\tContinuationToken string\n\tPrefix            string\n\n\t// FetchOwner and StartAfter are currently not used\n\tFetchOwner string\n\tStartAfter string\n}\n\n// Version is an element in the list object versions response\ntype Version struct {\n\tETag         string\n\tIsLatest     bool\n\tKey          string\n\tLastModified time.Time\n\tOwner        Owner\n\tSize         int64\n\tStorageClass string\n\tVersionID    string `xml:\"VersionId\"`\n\n\t// x-amz-meta-* headers stripped \"x-amz-meta-\" prefix containing the first value.\n\t// Only returned by MinIO servers.\n\tUserMetadata StringMap `json:\"userMetadata,omitempty\"`\n\n\t// x-amz-tagging values in their k/v values.\n\t// Only returned by MinIO servers.\n\tUserTags URLMap `json:\"userTags,omitempty\" xml:\"UserTags\"`\n\n\tInternal *struct {\n\t\tK int // Data blocks\n\t\tM int // Parity blocks\n\t} `xml:\"Internal\"`\n\n\tisDeleteMarker bool\n}\n\n// ListVersionsResult is an element in the list object versions response\n// and has a special Unmarshaler because we need to preserver the order\n// of <Version>  and <DeleteMarker> in ListVersionsResult.Versions slice\ntype ListVersionsResult struct {\n\tVersions []Version\n\n\tCommonPrefixes      []CommonPrefix\n\tName                string\n\tPrefix              string\n\tDelimiter           string\n\tMaxKeys             int64\n\tEncodingType        string\n\tIsTruncated         bool\n\tKeyMarker           string\n\tVersionIDMarker     string\n\tNextKeyMarker       string\n\tNextVersionIDMarker string\n}\n\n// UnmarshalXML is a custom unmarshal code for the response of ListObjectVersions, the custom\n// code will unmarshal <Version> and <DeleteMarker> tags and save them in Versions field to\n// preserve the lexical order of the listing.\nfunc (l *ListVersionsResult) UnmarshalXML(d *xml.Decoder, _ xml.StartElement) (err error) {\n\tfor {\n\t\t// Read tokens from the XML document in a stream.\n\t\tt, err := d.Token()\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\tse, ok := t.(xml.StartElement)\n\t\tif ok {\n\t\t\ttagName := se.Name.Local\n\t\t\tswitch tagName {\n\t\t\tcase \"Name\", \"Prefix\",\n\t\t\t\t\"Delimiter\", \"EncodingType\",\n\t\t\t\t\"KeyMarker\", \"NextKeyMarker\":\n\t\t\t\tvar s string\n\t\t\t\tif err = d.DecodeElement(&s, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tv := reflect.ValueOf(l).Elem().FieldByName(tagName)\n\t\t\t\tif v.IsValid() {\n\t\t\t\t\tv.SetString(s)\n\t\t\t\t}\n\t\t\tcase \"VersionIdMarker\":\n\t\t\t\t// VersionIdMarker is a special case because of 'Id' instead of 'ID' in field name\n\t\t\t\tvar s string\n\t\t\t\tif err = d.DecodeElement(&s, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tl.VersionIDMarker = s\n\t\t\tcase \"NextVersionIdMarker\":\n\t\t\t\t// NextVersionIdMarker is a special case because of 'Id' instead of 'ID' in field name\n\t\t\t\tvar s string\n\t\t\t\tif err = d.DecodeElement(&s, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tl.NextVersionIDMarker = s\n\t\t\tcase \"IsTruncated\": //        bool\n\t\t\t\tvar b bool\n\t\t\t\tif err = d.DecodeElement(&b, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tl.IsTruncated = b\n\t\t\tcase \"MaxKeys\": //       int64\n\t\t\t\tvar i int64\n\t\t\t\tif err = d.DecodeElement(&i, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tl.MaxKeys = i\n\t\t\tcase \"CommonPrefixes\":\n\t\t\t\tvar cp CommonPrefix\n\t\t\t\tif err = d.DecodeElement(&cp, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tl.CommonPrefixes = append(l.CommonPrefixes, cp)\n\t\t\tcase \"DeleteMarker\", \"Version\":\n\t\t\t\tvar v Version\n\t\t\t\tif err = d.DecodeElement(&v, &se); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif tagName == \"DeleteMarker\" {\n\t\t\t\t\tv.isDeleteMarker = true\n\t\t\t\t}\n\t\t\t\tl.Versions = append(l.Versions, v)\n\t\t\tdefault:\n\t\t\t\treturn errors.New(\"unrecognized option:\" + tagName)\n\t\t\t}\n\n\t\t}\n\t}\n\treturn nil\n}\n\n// ListBucketResult container for listObjects response.\ntype ListBucketResult struct {\n\t// A response can contain CommonPrefixes only if you have\n\t// specified a delimiter.\n\tCommonPrefixes []CommonPrefix\n\t// Metadata about each object returned.\n\tContents  []ObjectInfo\n\tDelimiter string\n\n\t// Encoding type used to encode object keys in the response.\n\tEncodingType string\n\n\t// A flag that indicates whether or not ListObjects returned all of the results\n\t// that satisfied the search criteria.\n\tIsTruncated bool\n\tMarker      string\n\tMaxKeys     int64\n\tName        string\n\n\t// When response is truncated (the IsTruncated element value in\n\t// the response is true), you can use the key name in this field\n\t// as marker in the subsequent request to get next set of objects.\n\t// Object storage lists objects in alphabetical order Note: This\n\t// element is returned only if you have delimiter request\n\t// parameter specified. If response does not include the NextMaker\n\t// and it is truncated, you can use the value of the last Key in\n\t// the response as the marker in the subsequent request to get the\n\t// next set of object keys.\n\tNextMarker string\n\tPrefix     string\n}\n\n// ListMultipartUploadsResult container for ListMultipartUploads response\ntype ListMultipartUploadsResult struct {\n\tBucket             string\n\tKeyMarker          string\n\tUploadIDMarker     string `xml:\"UploadIdMarker\"`\n\tNextKeyMarker      string\n\tNextUploadIDMarker string `xml:\"NextUploadIdMarker\"`\n\tEncodingType       string\n\tMaxUploads         int64\n\tIsTruncated        bool\n\tUploads            []ObjectMultipartInfo `xml:\"Upload\"`\n\tPrefix             string\n\tDelimiter          string\n\t// A response can contain CommonPrefixes only if you specify a delimiter.\n\tCommonPrefixes []CommonPrefix\n}\n\n// initiator container for who initiated multipart upload.\ntype initiator struct {\n\tID          string\n\tDisplayName string\n}\n\n// copyObjectResult container for copy object response.\ntype copyObjectResult struct {\n\tETag         string\n\tLastModified time.Time // time string format \"2006-01-02T15:04:05.000Z\"\n}\n\n// ObjectPart container for particular part of an object.\ntype ObjectPart struct {\n\t// Part number identifies the part.\n\tPartNumber int\n\n\t// Date and time the part was uploaded.\n\tLastModified time.Time\n\n\t// Entity tag returned when the part was uploaded, usually md5sum\n\t// of the part.\n\tETag string\n\n\t// Size of the uploaded part data.\n\tSize int64\n\n\t// Checksum values of each part.\n\tChecksumCRC32     string\n\tChecksumCRC32C    string\n\tChecksumSHA1      string\n\tChecksumSHA256    string\n\tChecksumCRC64NVME string\n}\n\n// Checksum will return the checksum for the given type.\n// Will return the empty string if not set.\nfunc (c ObjectPart) Checksum(t ChecksumType) string {\n\tswitch {\n\tcase t.Is(ChecksumCRC32C):\n\t\treturn c.ChecksumCRC32C\n\tcase t.Is(ChecksumCRC32):\n\t\treturn c.ChecksumCRC32\n\tcase t.Is(ChecksumSHA1):\n\t\treturn c.ChecksumSHA1\n\tcase t.Is(ChecksumSHA256):\n\t\treturn c.ChecksumSHA256\n\tcase t.Is(ChecksumCRC64NVME):\n\t\treturn c.ChecksumCRC64NVME\n\t}\n\treturn \"\"\n}\n\n// ChecksumRaw returns the decoded checksum from the part.\nfunc (c ObjectPart) ChecksumRaw(t ChecksumType) ([]byte, error) {\n\tb64 := c.Checksum(t)\n\tif b64 == \"\" {\n\t\treturn nil, errors.New(\"no checksum set\")\n\t}\n\tdecoded, err := base64.StdEncoding.DecodeString(b64)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(decoded) != t.RawByteLen() {\n\t\treturn nil, errors.New(\"checksum length mismatch\")\n\t}\n\treturn decoded, nil\n}\n\n// ListObjectPartsResult container for ListObjectParts response.\ntype ListObjectPartsResult struct {\n\tBucket   string\n\tKey      string\n\tUploadID string `xml:\"UploadId\"`\n\n\tInitiator initiator\n\tOwner     owner\n\n\tStorageClass         string\n\tPartNumberMarker     int\n\tNextPartNumberMarker int\n\tMaxParts             int\n\n\t// ChecksumAlgorithm will be CRC32, CRC32C, etc.\n\tChecksumAlgorithm string\n\n\t// ChecksumType is FULL_OBJECT or COMPOSITE (assume COMPOSITE when unset)\n\tChecksumType string\n\n\t// Indicates whether the returned list of parts is truncated.\n\tIsTruncated bool\n\tObjectParts []ObjectPart `xml:\"Part\"`\n\n\tEncodingType string\n}\n\n// initiateMultipartUploadResult container for InitiateMultiPartUpload\n// response.\ntype initiateMultipartUploadResult struct {\n\tBucket   string\n\tKey      string\n\tUploadID string `xml:\"UploadId\"`\n}\n\n// completeMultipartUploadResult container for completed multipart\n// upload response.\ntype completeMultipartUploadResult struct {\n\tLocation string\n\tBucket   string\n\tKey      string\n\tETag     string\n\n\t// Checksum values, hash of hashes of parts.\n\tChecksumCRC32     string\n\tChecksumCRC32C    string\n\tChecksumSHA1      string\n\tChecksumSHA256    string\n\tChecksumCRC64NVME string\n}\n\n// CompletePart sub container lists individual part numbers and their\n// md5sum, part of completeMultipartUpload.\ntype CompletePart struct {\n\t// Part number identifies the part.\n\tPartNumber int\n\tETag       string\n\n\t// Checksum values\n\tChecksumCRC32     string `xml:\"ChecksumCRC32,omitempty\"`\n\tChecksumCRC32C    string `xml:\"ChecksumCRC32C,omitempty\"`\n\tChecksumSHA1      string `xml:\"ChecksumSHA1,omitempty\"`\n\tChecksumSHA256    string `xml:\"ChecksumSHA256,omitempty\"`\n\tChecksumCRC64NVME string `xml:\",omitempty\"`\n}\n\n// Checksum will return the checksum for the given type.\n// Will return the empty string if not set.\nfunc (c CompletePart) Checksum(t ChecksumType) string {\n\tswitch {\n\tcase t.Is(ChecksumCRC32C):\n\t\treturn c.ChecksumCRC32C\n\tcase t.Is(ChecksumCRC32):\n\t\treturn c.ChecksumCRC32\n\tcase t.Is(ChecksumSHA1):\n\t\treturn c.ChecksumSHA1\n\tcase t.Is(ChecksumSHA256):\n\t\treturn c.ChecksumSHA256\n\tcase t.Is(ChecksumCRC64NVME):\n\t\treturn c.ChecksumCRC64NVME\n\t}\n\treturn \"\"\n}\n\n// completeMultipartUpload container for completing multipart upload.\ntype completeMultipartUpload struct {\n\tXMLName xml.Name       `xml:\"http://s3.amazonaws.com/doc/2006-03-01/ CompleteMultipartUpload\" json:\"-\"`\n\tParts   []CompletePart `xml:\"Part\"`\n}\n\n// createBucketConfiguration container for bucket configuration.\ntype createBucketConfiguration struct {\n\tXMLName  xml.Name `xml:\"http://s3.amazonaws.com/doc/2006-03-01/ CreateBucketConfiguration\" json:\"-\"`\n\tLocation string   `xml:\"LocationConstraint\"`\n}\n\n// deleteObject container for Delete element in MultiObjects Delete XML request\ntype deleteObject struct {\n\tKey       string\n\tVersionID string `xml:\"VersionId,omitempty\"`\n}\n\n// deletedObject container for Deleted element in MultiObjects Delete XML response\ntype deletedObject struct {\n\tKey       string\n\tVersionID string `xml:\"VersionId,omitempty\"`\n\t// These fields are ignored.\n\tDeleteMarker          bool\n\tDeleteMarkerVersionID string `xml:\"DeleteMarkerVersionId,omitempty\"`\n}\n\n// nonDeletedObject container for Error element (failed deletion) in MultiObjects Delete XML response\ntype nonDeletedObject struct {\n\tKey       string\n\tCode      string\n\tMessage   string\n\tVersionID string `xml:\"VersionId\"`\n}\n\n// deletedMultiObjects container for MultiObjects Delete XML request\ntype deleteMultiObjects struct {\n\tXMLName xml.Name `xml:\"Delete\"`\n\tQuiet   bool\n\tObjects []deleteObject `xml:\"Object\"`\n}\n\n// deletedMultiObjectsResult container for MultiObjects Delete XML response\ntype deleteMultiObjectsResult struct {\n\tXMLName          xml.Name           `xml:\"DeleteResult\"`\n\tDeletedObjects   []deletedObject    `xml:\"Deleted\"`\n\tUnDeletedObjects []nonDeletedObject `xml:\"Error\"`\n}\n"
        },
        {
          "name": "api-select.go",
          "type": "blob",
          "size": 21.1044921875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * (C) 2018-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strings\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// CSVFileHeaderInfo - is the parameter for whether to utilize headers.\ntype CSVFileHeaderInfo string\n\n// Constants for file header info.\nconst (\n\tCSVFileHeaderInfoNone   CSVFileHeaderInfo = \"NONE\"\n\tCSVFileHeaderInfoIgnore CSVFileHeaderInfo = \"IGNORE\"\n\tCSVFileHeaderInfoUse    CSVFileHeaderInfo = \"USE\"\n)\n\n// SelectCompressionType - is the parameter for what type of compression is\n// present\ntype SelectCompressionType string\n\n// Constants for compression types under select API.\nconst (\n\tSelectCompressionNONE SelectCompressionType = \"NONE\"\n\tSelectCompressionGZIP SelectCompressionType = \"GZIP\"\n\tSelectCompressionBZIP SelectCompressionType = \"BZIP2\"\n\n\t// Non-standard compression schemes, supported by MinIO hosts:\n\n\tSelectCompressionZSTD   SelectCompressionType = \"ZSTD\"   // Zstandard compression.\n\tSelectCompressionLZ4    SelectCompressionType = \"LZ4\"    // LZ4 Stream\n\tSelectCompressionS2     SelectCompressionType = \"S2\"     // S2 Stream\n\tSelectCompressionSNAPPY SelectCompressionType = \"SNAPPY\" // Snappy stream\n)\n\n// CSVQuoteFields - is the parameter for how CSV fields are quoted.\ntype CSVQuoteFields string\n\n// Constants for csv quote styles.\nconst (\n\tCSVQuoteFieldsAlways   CSVQuoteFields = \"Always\"\n\tCSVQuoteFieldsAsNeeded CSVQuoteFields = \"AsNeeded\"\n)\n\n// QueryExpressionType - is of what syntax the expression is, this should only\n// be SQL\ntype QueryExpressionType string\n\n// Constants for expression type.\nconst (\n\tQueryExpressionTypeSQL QueryExpressionType = \"SQL\"\n)\n\n// JSONType determines json input serialization type.\ntype JSONType string\n\n// Constants for JSONTypes.\nconst (\n\tJSONDocumentType JSONType = \"DOCUMENT\"\n\tJSONLinesType    JSONType = \"LINES\"\n)\n\n// ParquetInputOptions parquet input specific options\ntype ParquetInputOptions struct{}\n\n// CSVInputOptions csv input specific options\ntype CSVInputOptions struct {\n\tFileHeaderInfo    CSVFileHeaderInfo\n\tfileHeaderInfoSet bool\n\n\tRecordDelimiter    string\n\trecordDelimiterSet bool\n\n\tFieldDelimiter    string\n\tfieldDelimiterSet bool\n\n\tQuoteCharacter    string\n\tquoteCharacterSet bool\n\n\tQuoteEscapeCharacter    string\n\tquoteEscapeCharacterSet bool\n\n\tComments    string\n\tcommentsSet bool\n}\n\n// SetFileHeaderInfo sets the file header info in the CSV input options\nfunc (c *CSVInputOptions) SetFileHeaderInfo(val CSVFileHeaderInfo) {\n\tc.FileHeaderInfo = val\n\tc.fileHeaderInfoSet = true\n}\n\n// SetRecordDelimiter sets the record delimiter in the CSV input options\nfunc (c *CSVInputOptions) SetRecordDelimiter(val string) {\n\tc.RecordDelimiter = val\n\tc.recordDelimiterSet = true\n}\n\n// SetFieldDelimiter sets the field delimiter in the CSV input options\nfunc (c *CSVInputOptions) SetFieldDelimiter(val string) {\n\tc.FieldDelimiter = val\n\tc.fieldDelimiterSet = true\n}\n\n// SetQuoteCharacter sets the quote character in the CSV input options\nfunc (c *CSVInputOptions) SetQuoteCharacter(val string) {\n\tc.QuoteCharacter = val\n\tc.quoteCharacterSet = true\n}\n\n// SetQuoteEscapeCharacter sets the quote escape character in the CSV input options\nfunc (c *CSVInputOptions) SetQuoteEscapeCharacter(val string) {\n\tc.QuoteEscapeCharacter = val\n\tc.quoteEscapeCharacterSet = true\n}\n\n// SetComments sets the comments character in the CSV input options\nfunc (c *CSVInputOptions) SetComments(val string) {\n\tc.Comments = val\n\tc.commentsSet = true\n}\n\n// MarshalXML - produces the xml representation of the CSV input options struct\nfunc (c CSVInputOptions) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\tif c.FileHeaderInfo != \"\" || c.fileHeaderInfoSet {\n\t\tif err := e.EncodeElement(c.FileHeaderInfo, xml.StartElement{Name: xml.Name{Local: \"FileHeaderInfo\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.RecordDelimiter != \"\" || c.recordDelimiterSet {\n\t\tif err := e.EncodeElement(c.RecordDelimiter, xml.StartElement{Name: xml.Name{Local: \"RecordDelimiter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.FieldDelimiter != \"\" || c.fieldDelimiterSet {\n\t\tif err := e.EncodeElement(c.FieldDelimiter, xml.StartElement{Name: xml.Name{Local: \"FieldDelimiter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.QuoteCharacter != \"\" || c.quoteCharacterSet {\n\t\tif err := e.EncodeElement(c.QuoteCharacter, xml.StartElement{Name: xml.Name{Local: \"QuoteCharacter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.QuoteEscapeCharacter != \"\" || c.quoteEscapeCharacterSet {\n\t\tif err := e.EncodeElement(c.QuoteEscapeCharacter, xml.StartElement{Name: xml.Name{Local: \"QuoteEscapeCharacter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.Comments != \"\" || c.commentsSet {\n\t\tif err := e.EncodeElement(c.Comments, xml.StartElement{Name: xml.Name{Local: \"Comments\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\n// CSVOutputOptions csv output specific options\ntype CSVOutputOptions struct {\n\tQuoteFields    CSVQuoteFields\n\tquoteFieldsSet bool\n\n\tRecordDelimiter    string\n\trecordDelimiterSet bool\n\n\tFieldDelimiter    string\n\tfieldDelimiterSet bool\n\n\tQuoteCharacter    string\n\tquoteCharacterSet bool\n\n\tQuoteEscapeCharacter    string\n\tquoteEscapeCharacterSet bool\n}\n\n// SetQuoteFields sets the quote field parameter in the CSV output options\nfunc (c *CSVOutputOptions) SetQuoteFields(val CSVQuoteFields) {\n\tc.QuoteFields = val\n\tc.quoteFieldsSet = true\n}\n\n// SetRecordDelimiter sets the record delimiter character in the CSV output options\nfunc (c *CSVOutputOptions) SetRecordDelimiter(val string) {\n\tc.RecordDelimiter = val\n\tc.recordDelimiterSet = true\n}\n\n// SetFieldDelimiter sets the field delimiter character in the CSV output options\nfunc (c *CSVOutputOptions) SetFieldDelimiter(val string) {\n\tc.FieldDelimiter = val\n\tc.fieldDelimiterSet = true\n}\n\n// SetQuoteCharacter sets the quote character in the CSV output options\nfunc (c *CSVOutputOptions) SetQuoteCharacter(val string) {\n\tc.QuoteCharacter = val\n\tc.quoteCharacterSet = true\n}\n\n// SetQuoteEscapeCharacter sets the quote escape character in the CSV output options\nfunc (c *CSVOutputOptions) SetQuoteEscapeCharacter(val string) {\n\tc.QuoteEscapeCharacter = val\n\tc.quoteEscapeCharacterSet = true\n}\n\n// MarshalXML - produces the xml representation of the CSVOutputOptions struct\nfunc (c CSVOutputOptions) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\n\tif c.QuoteFields != \"\" || c.quoteFieldsSet {\n\t\tif err := e.EncodeElement(c.QuoteFields, xml.StartElement{Name: xml.Name{Local: \"QuoteFields\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.RecordDelimiter != \"\" || c.recordDelimiterSet {\n\t\tif err := e.EncodeElement(c.RecordDelimiter, xml.StartElement{Name: xml.Name{Local: \"RecordDelimiter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.FieldDelimiter != \"\" || c.fieldDelimiterSet {\n\t\tif err := e.EncodeElement(c.FieldDelimiter, xml.StartElement{Name: xml.Name{Local: \"FieldDelimiter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.QuoteCharacter != \"\" || c.quoteCharacterSet {\n\t\tif err := e.EncodeElement(c.QuoteCharacter, xml.StartElement{Name: xml.Name{Local: \"QuoteCharacter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif c.QuoteEscapeCharacter != \"\" || c.quoteEscapeCharacterSet {\n\t\tif err := e.EncodeElement(c.QuoteEscapeCharacter, xml.StartElement{Name: xml.Name{Local: \"QuoteEscapeCharacter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\n// JSONInputOptions json input specific options\ntype JSONInputOptions struct {\n\tType    JSONType\n\ttypeSet bool\n}\n\n// SetType sets the JSON type in the JSON input options\nfunc (j *JSONInputOptions) SetType(typ JSONType) {\n\tj.Type = typ\n\tj.typeSet = true\n}\n\n// MarshalXML - produces the xml representation of the JSONInputOptions struct\nfunc (j JSONInputOptions) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\n\tif j.Type != \"\" || j.typeSet {\n\t\tif err := e.EncodeElement(j.Type, xml.StartElement{Name: xml.Name{Local: \"Type\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\n// JSONOutputOptions - json output specific options\ntype JSONOutputOptions struct {\n\tRecordDelimiter    string\n\trecordDelimiterSet bool\n}\n\n// SetRecordDelimiter sets the record delimiter in the JSON output options\nfunc (j *JSONOutputOptions) SetRecordDelimiter(val string) {\n\tj.RecordDelimiter = val\n\tj.recordDelimiterSet = true\n}\n\n// MarshalXML - produces the xml representation of the JSONOutputOptions struct\nfunc (j JSONOutputOptions) MarshalXML(e *xml.Encoder, start xml.StartElement) error {\n\tif err := e.EncodeToken(start); err != nil {\n\t\treturn err\n\t}\n\n\tif j.RecordDelimiter != \"\" || j.recordDelimiterSet {\n\t\tif err := e.EncodeElement(j.RecordDelimiter, xml.StartElement{Name: xml.Name{Local: \"RecordDelimiter\"}}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn e.EncodeToken(xml.EndElement{Name: start.Name})\n}\n\n// SelectObjectInputSerialization - input serialization parameters\ntype SelectObjectInputSerialization struct {\n\tCompressionType SelectCompressionType `xml:\"CompressionType,omitempty\"`\n\tParquet         *ParquetInputOptions  `xml:\"Parquet,omitempty\"`\n\tCSV             *CSVInputOptions      `xml:\"CSV,omitempty\"`\n\tJSON            *JSONInputOptions     `xml:\"JSON,omitempty\"`\n}\n\n// SelectObjectOutputSerialization - output serialization parameters.\ntype SelectObjectOutputSerialization struct {\n\tCSV  *CSVOutputOptions  `xml:\"CSV,omitempty\"`\n\tJSON *JSONOutputOptions `xml:\"JSON,omitempty\"`\n}\n\n// SelectObjectOptions - represents the input select body\ntype SelectObjectOptions struct {\n\tXMLName              xml.Name           `xml:\"SelectObjectContentRequest\" json:\"-\"`\n\tServerSideEncryption encrypt.ServerSide `xml:\"-\"`\n\tExpression           string\n\tExpressionType       QueryExpressionType\n\tInputSerialization   SelectObjectInputSerialization\n\tOutputSerialization  SelectObjectOutputSerialization\n\tRequestProgress      struct {\n\t\tEnabled bool\n\t}\n}\n\n// Header returns the http.Header representation of the SelectObject options.\nfunc (o SelectObjectOptions) Header() http.Header {\n\theaders := make(http.Header)\n\tif o.ServerSideEncryption != nil && o.ServerSideEncryption.Type() == encrypt.SSEC {\n\t\to.ServerSideEncryption.Marshal(headers)\n\t}\n\treturn headers\n}\n\n// SelectObjectType - is the parameter which defines what type of object the\n// operation is being performed on.\ntype SelectObjectType string\n\n// Constants for input data types.\nconst (\n\tSelectObjectTypeCSV     SelectObjectType = \"CSV\"\n\tSelectObjectTypeJSON    SelectObjectType = \"JSON\"\n\tSelectObjectTypeParquet SelectObjectType = \"Parquet\"\n)\n\n// preludeInfo is used for keeping track of necessary information from the\n// prelude.\ntype preludeInfo struct {\n\ttotalLen  uint32\n\theaderLen uint32\n}\n\n// SelectResults is used for the streaming responses from the server.\ntype SelectResults struct {\n\tpipeReader *io.PipeReader\n\tresp       *http.Response\n\tstats      *StatsMessage\n\tprogress   *ProgressMessage\n}\n\n// ProgressMessage is a struct for progress xml message.\ntype ProgressMessage struct {\n\tXMLName xml.Name `xml:\"Progress\" json:\"-\"`\n\tStatsMessage\n}\n\n// StatsMessage is a struct for stat xml message.\ntype StatsMessage struct {\n\tXMLName        xml.Name `xml:\"Stats\" json:\"-\"`\n\tBytesScanned   int64\n\tBytesProcessed int64\n\tBytesReturned  int64\n}\n\n// messageType represents the type of message.\ntype messageType string\n\nconst (\n\terrorMsg  messageType = \"error\"\n\tcommonMsg messageType = \"event\"\n)\n\n// eventType represents the type of event.\ntype eventType string\n\n// list of event-types returned by Select API.\nconst (\n\tendEvent      eventType = \"End\"\n\trecordsEvent  eventType = \"Records\"\n\tprogressEvent eventType = \"Progress\"\n\tstatsEvent    eventType = \"Stats\"\n)\n\n// contentType represents content type of event.\ntype contentType string\n\nconst (\n\txmlContent contentType = \"text/xml\"\n)\n\n// SelectObjectContent is a implementation of http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html AWS S3 API.\nfunc (c *Client) SelectObjectContent(ctx context.Context, bucketName, objectName string, opts SelectObjectOptions) (*SelectResults, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn nil, err\n\t}\n\n\tselectReqBytes, err := xml.Marshal(opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\turlValues := make(url.Values)\n\turlValues.Set(\"select\", \"\")\n\turlValues.Set(\"select-type\", \"2\")\n\n\t// Execute POST on bucket/object.\n\tresp, err := c.executeMethod(ctx, http.MethodPost, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      urlValues,\n\t\tcustomHeader:     opts.Header(),\n\t\tcontentMD5Base64: sumMD5Base64(selectReqBytes),\n\t\tcontentSHA256Hex: sum256Hex(selectReqBytes),\n\t\tcontentBody:      bytes.NewReader(selectReqBytes),\n\t\tcontentLength:    int64(len(selectReqBytes)),\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn NewSelectResults(resp, bucketName)\n}\n\n// NewSelectResults creates a Select Result parser that parses the response\n// and returns a Reader that will return parsed and assembled select output.\nfunc NewSelectResults(resp *http.Response, bucketName string) (*SelectResults, error) {\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, httpRespToErrorResponse(resp, bucketName, \"\")\n\t}\n\n\tpipeReader, pipeWriter := io.Pipe()\n\tstreamer := &SelectResults{\n\t\tresp:       resp,\n\t\tstats:      &StatsMessage{},\n\t\tprogress:   &ProgressMessage{},\n\t\tpipeReader: pipeReader,\n\t}\n\tstreamer.start(pipeWriter)\n\treturn streamer, nil\n}\n\n// Close - closes the underlying response body and the stream reader.\nfunc (s *SelectResults) Close() error {\n\tdefer closeResponse(s.resp)\n\treturn s.pipeReader.Close()\n}\n\n// Read - is a reader compatible implementation for SelectObjectContent records.\nfunc (s *SelectResults) Read(b []byte) (n int, err error) {\n\treturn s.pipeReader.Read(b)\n}\n\n// Stats - information about a request's stats when processing is complete.\nfunc (s *SelectResults) Stats() *StatsMessage {\n\treturn s.stats\n}\n\n// Progress - information about the progress of a request.\nfunc (s *SelectResults) Progress() *ProgressMessage {\n\treturn s.progress\n}\n\n// start is the main function that decodes the large byte array into\n// several events that are sent through the eventstream.\nfunc (s *SelectResults) start(pipeWriter *io.PipeWriter) {\n\tgo func() {\n\t\tfor {\n\t\t\tvar prelude preludeInfo\n\t\t\theaders := make(http.Header)\n\t\t\tvar err error\n\n\t\t\t// Create CRC code\n\t\t\tcrc := crc32.New(crc32.IEEETable)\n\t\t\tcrcReader := io.TeeReader(s.resp.Body, crc)\n\n\t\t\t// Extract the prelude(12 bytes) into a struct to extract relevant information.\n\t\t\tprelude, err = processPrelude(crcReader, crc)\n\t\t\tif err != nil {\n\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\tcloseResponse(s.resp)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Extract the headers(variable bytes) into a struct to extract relevant information\n\t\t\tif prelude.headerLen > 0 {\n\t\t\t\tif err = extractHeader(io.LimitReader(crcReader, int64(prelude.headerLen)), headers); err != nil {\n\t\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Get the actual payload length so that the appropriate amount of\n\t\t\t// bytes can be read or parsed.\n\t\t\tpayloadLen := prelude.PayloadLen()\n\n\t\t\tm := messageType(headers.Get(\"message-type\"))\n\n\t\t\tswitch m {\n\t\t\tcase errorMsg:\n\t\t\t\tpipeWriter.CloseWithError(errors.New(headers.Get(\"error-code\") + \":\\\"\" + headers.Get(\"error-message\") + \"\\\"\"))\n\t\t\t\tcloseResponse(s.resp)\n\t\t\t\treturn\n\t\t\tcase commonMsg:\n\t\t\t\t// Get content-type of the payload.\n\t\t\t\tc := contentType(headers.Get(\"content-type\"))\n\n\t\t\t\t// Get event type of the payload.\n\t\t\t\te := eventType(headers.Get(\"event-type\"))\n\n\t\t\t\t// Handle all supported events.\n\t\t\t\tswitch e {\n\t\t\t\tcase endEvent:\n\t\t\t\t\tpipeWriter.Close()\n\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\treturn\n\t\t\t\tcase recordsEvent:\n\t\t\t\t\tif _, err = io.Copy(pipeWriter, io.LimitReader(crcReader, payloadLen)); err != nil {\n\t\t\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\tcase progressEvent:\n\t\t\t\t\tswitch c {\n\t\t\t\t\tcase xmlContent:\n\t\t\t\t\t\tif err = xmlDecoder(io.LimitReader(crcReader, payloadLen), s.progress); err != nil {\n\t\t\t\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tpipeWriter.CloseWithError(fmt.Errorf(\"Unexpected content-type %s sent for event-type %s\", c, progressEvent))\n\t\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\tcase statsEvent:\n\t\t\t\t\tswitch c {\n\t\t\t\t\tcase xmlContent:\n\t\t\t\t\t\tif err = xmlDecoder(io.LimitReader(crcReader, payloadLen), s.stats); err != nil {\n\t\t\t\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tpipeWriter.CloseWithError(fmt.Errorf(\"Unexpected content-type %s sent for event-type %s\", c, statsEvent))\n\t\t\t\t\t\tcloseResponse(s.resp)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Ensures that the full message's CRC is correct and\n\t\t\t// that the message is not corrupted\n\t\t\tif err := checkCRC(s.resp.Body, crc.Sum32()); err != nil {\n\t\t\t\tpipeWriter.CloseWithError(err)\n\t\t\t\tcloseResponse(s.resp)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t}\n\t}()\n}\n\n// PayloadLen is a function that calculates the length of the payload.\nfunc (p preludeInfo) PayloadLen() int64 {\n\treturn int64(p.totalLen - p.headerLen - 16)\n}\n\n// processPrelude is the function that reads the 12 bytes of the prelude and\n// ensures the CRC is correct while also extracting relevant information into\n// the struct,\nfunc processPrelude(prelude io.Reader, crc hash.Hash32) (preludeInfo, error) {\n\tvar err error\n\tpInfo := preludeInfo{}\n\n\t// reads total length of the message (first 4 bytes)\n\tpInfo.totalLen, err = extractUint32(prelude)\n\tif err != nil {\n\t\treturn pInfo, err\n\t}\n\n\t// reads total header length of the message (2nd 4 bytes)\n\tpInfo.headerLen, err = extractUint32(prelude)\n\tif err != nil {\n\t\treturn pInfo, err\n\t}\n\n\t// checks that the CRC is correct (3rd 4 bytes)\n\tpreCRC := crc.Sum32()\n\tif err := checkCRC(prelude, preCRC); err != nil {\n\t\treturn pInfo, err\n\t}\n\n\treturn pInfo, nil\n}\n\n// extracts the relevant information from the Headers.\nfunc extractHeader(body io.Reader, myHeaders http.Header) error {\n\tfor {\n\t\t// extracts the first part of the header,\n\t\theaderTypeName, err := extractHeaderType(body)\n\t\tif err != nil {\n\t\t\t// Since end of file, we have read all of our headers\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\t// reads the 7 present in the header and ignores it.\n\t\textractUint8(body)\n\n\t\theaderValueName, err := extractHeaderValue(body)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tmyHeaders.Set(headerTypeName, headerValueName)\n\n\t}\n\treturn nil\n}\n\n// extractHeaderType extracts the first half of the header message, the header type.\nfunc extractHeaderType(body io.Reader) (string, error) {\n\t// extracts 2 bit integer\n\theaderNameLen, err := extractUint8(body)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\t// extracts the string with the appropriate number of bytes\n\theaderName, err := extractString(body, int(headerNameLen))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn strings.TrimPrefix(headerName, \":\"), nil\n}\n\n// extractsHeaderValue extracts the second half of the header message, the\n// header value\nfunc extractHeaderValue(body io.Reader) (string, error) {\n\tbodyLen, err := extractUint16(body)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tbodyName, err := extractString(body, int(bodyLen))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn bodyName, nil\n}\n\n// extracts a string from byte array of a particular number of bytes.\nfunc extractString(source io.Reader, lenBytes int) (string, error) {\n\tmyVal := make([]byte, lenBytes)\n\t_, err := source.Read(myVal)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(myVal), nil\n}\n\n// extractUint32 extracts a 4 byte integer from the byte array.\nfunc extractUint32(r io.Reader) (uint32, error) {\n\tbuf := make([]byte, 4)\n\t_, err := readFull(r, buf)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn binary.BigEndian.Uint32(buf), nil\n}\n\n// extractUint16 extracts a 2 byte integer from the byte array.\nfunc extractUint16(r io.Reader) (uint16, error) {\n\tbuf := make([]byte, 2)\n\t_, err := readFull(r, buf)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn binary.BigEndian.Uint16(buf), nil\n}\n\n// extractUint8 extracts a 1 byte integer from the byte array.\nfunc extractUint8(r io.Reader) (uint8, error) {\n\tbuf := make([]byte, 1)\n\t_, err := readFull(r, buf)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn buf[0], nil\n}\n\n// checkCRC ensures that the CRC matches with the one from the reader.\nfunc checkCRC(r io.Reader, expect uint32) error {\n\tmsgCRC, err := extractUint32(r)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif msgCRC != expect {\n\t\treturn fmt.Errorf(\"Checksum Mismatch, MessageCRC of 0x%X does not equal expected CRC of 0x%X\", msgCRC, expect)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api-stat.go",
          "type": "blob",
          "size": 3.9267578125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"net/http\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\n// BucketExists verifies if bucket exists and you have permission to access it. Allows for a Context to\n// control cancellations and timeouts.\nfunc (c *Client) BucketExists(ctx context.Context, bucketName string) (bool, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn false, err\n\t}\n\n\t// Execute HEAD on bucketName.\n\tresp, err := c.executeMethod(ctx, http.MethodHead, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\tif ToErrorResponse(err).Code == \"NoSuchBucket\" {\n\t\t\treturn false, nil\n\t\t}\n\t\treturn false, err\n\t}\n\tif resp != nil {\n\t\tresperr := httpRespToErrorResponse(resp, bucketName, \"\")\n\t\tif ToErrorResponse(resperr).Code == \"NoSuchBucket\" {\n\t\t\treturn false, nil\n\t\t}\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn false, httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t}\n\t}\n\treturn true, nil\n}\n\n// StatObject verifies if object exists, you have permission to access it\n// and returns information about the object.\nfunc (c *Client) StatObject(ctx context.Context, bucketName, objectName string, opts StatObjectOptions) (ObjectInfo, error) {\n\t// Input validation.\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"InvalidBucketName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\tif err := s3utils.CheckValidObjectName(objectName); err != nil {\n\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\tStatusCode: http.StatusBadRequest,\n\t\t\tCode:       \"XMinioInvalidObjectName\",\n\t\t\tMessage:    err.Error(),\n\t\t}\n\t}\n\theaders := opts.Header()\n\tif opts.Internal.ReplicationDeleteMarker {\n\t\theaders.Set(minIOBucketReplicationDeleteMarker, \"true\")\n\t}\n\tif opts.Internal.IsReplicationReadyForDeleteMarker {\n\t\theaders.Set(isMinioTgtReplicationReady, \"true\")\n\t}\n\n\t// Execute HEAD on objectName.\n\tresp, err := c.executeMethod(ctx, http.MethodHead, requestMetadata{\n\t\tbucketName:       bucketName,\n\t\tobjectName:       objectName,\n\t\tqueryValues:      opts.toQueryValues(),\n\t\tcontentSHA256Hex: emptySHA256Hex,\n\t\tcustomHeader:     headers,\n\t})\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn ObjectInfo{}, err\n\t}\n\n\tif resp != nil {\n\t\tdeleteMarker := resp.Header.Get(amzDeleteMarker) == \"true\"\n\t\treplicationReady := resp.Header.Get(minioTgtReplicationReady) == \"true\"\n\t\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n\t\t\tif resp.StatusCode == http.StatusMethodNotAllowed && opts.VersionID != \"\" && deleteMarker {\n\t\t\t\terrResp := ErrorResponse{\n\t\t\t\t\tStatusCode: resp.StatusCode,\n\t\t\t\t\tCode:       \"MethodNotAllowed\",\n\t\t\t\t\tMessage:    \"The specified method is not allowed against this resource.\",\n\t\t\t\t\tBucketName: bucketName,\n\t\t\t\t\tKey:        objectName,\n\t\t\t\t}\n\t\t\t\treturn ObjectInfo{\n\t\t\t\t\tVersionID:      resp.Header.Get(amzVersionID),\n\t\t\t\t\tIsDeleteMarker: deleteMarker,\n\t\t\t\t}, errResp\n\t\t\t}\n\t\t\treturn ObjectInfo{\n\t\t\t\tVersionID:        resp.Header.Get(amzVersionID),\n\t\t\t\tIsDeleteMarker:   deleteMarker,\n\t\t\t\tReplicationReady: replicationReady, // whether delete marker can be replicated\n\t\t\t}, httpRespToErrorResponse(resp, bucketName, objectName)\n\t\t}\n\t}\n\n\treturn ToObjectInfo(bucketName, objectName, resp.Header)\n}\n"
        },
        {
          "name": "api.go",
          "type": "blob",
          "size": 29.9345703125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2024 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/cookiejar\"\n\t\"net/http/httptrace\"\n\t\"net/http/httputil\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\tmd5simd \"github.com/minio/md5-simd\"\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/signer\"\n\t\"golang.org/x/net/publicsuffix\"\n)\n\n// Client implements Amazon S3 compatible methods.\ntype Client struct {\n\t//  Standard options.\n\n\t// Parsed endpoint url provided by the user.\n\tendpointURL *url.URL\n\n\t// Holds various credential providers.\n\tcredsProvider *credentials.Credentials\n\n\t// Custom signerType value overrides all credentials.\n\toverrideSignerType credentials.SignatureType\n\n\t// User supplied.\n\tappInfo struct {\n\t\tappName    string\n\t\tappVersion string\n\t}\n\n\t// Indicate whether we are using https or not\n\tsecure bool\n\n\t// Needs allocation.\n\thttpClient     *http.Client\n\thttpTrace      *httptrace.ClientTrace\n\tbucketLocCache *bucketLocationCache\n\n\t// Advanced functionality.\n\tisTraceEnabled  bool\n\ttraceErrorsOnly bool\n\ttraceOutput     io.Writer\n\n\t// S3 specific accelerated endpoint.\n\ts3AccelerateEndpoint string\n\t// S3 dual-stack endpoints are enabled by default.\n\ts3DualstackEnabled bool\n\n\t// Region endpoint\n\tregion string\n\n\t// Random seed.\n\trandom *rand.Rand\n\n\t// lookup indicates type of url lookup supported by server. If not specified,\n\t// default to Auto.\n\tlookup BucketLookupType\n\n\t// Factory for MD5 hash functions.\n\tmd5Hasher    func() md5simd.Hasher\n\tsha256Hasher func() md5simd.Hasher\n\n\thealthStatus int32\n\n\ttrailingHeaderSupport bool\n\tmaxRetries            int\n}\n\n// Options for New method\ntype Options struct {\n\tCreds        *credentials.Credentials\n\tSecure       bool\n\tTransport    http.RoundTripper\n\tTrace        *httptrace.ClientTrace\n\tRegion       string\n\tBucketLookup BucketLookupType\n\n\t// Allows setting a custom region lookup based on URL pattern\n\t// not all URL patterns are covered by this library so if you\n\t// have a custom endpoints with many regions you can use this\n\t// function to perform region lookups appropriately.\n\tCustomRegionViaURL func(u url.URL) string\n\n\t// TrailingHeaders indicates server support of trailing headers.\n\t// Only supported for v4 signatures.\n\tTrailingHeaders bool\n\n\t// Custom hash routines. Leave nil to use standard.\n\tCustomMD5    func() md5simd.Hasher\n\tCustomSHA256 func() md5simd.Hasher\n\n\t// Number of times a request is retried. Defaults to 10 retries if this option is not configured.\n\t// Set to 1 to disable retries.\n\tMaxRetries int\n}\n\n// Global constants.\nconst (\n\tlibraryName    = \"minio-go\"\n\tlibraryVersion = \"v7.0.84\"\n)\n\n// User Agent should always following the below style.\n// Please open an issue to discuss any new changes here.\n//\n//\tMinIO (OS; ARCH) LIB/VER APP/VER\nconst (\n\tlibraryUserAgentPrefix = \"MinIO (\" + runtime.GOOS + \"; \" + runtime.GOARCH + \") \"\n\tlibraryUserAgent       = libraryUserAgentPrefix + libraryName + \"/\" + libraryVersion\n)\n\n// BucketLookupType is type of url lookup supported by server.\ntype BucketLookupType int\n\n// Different types of url lookup supported by the server.Initialized to BucketLookupAuto\nconst (\n\tBucketLookupAuto BucketLookupType = iota\n\tBucketLookupDNS\n\tBucketLookupPath\n)\n\n// New - instantiate minio client with options\nfunc New(endpoint string, opts *Options) (*Client, error) {\n\tif opts == nil {\n\t\treturn nil, errors.New(\"no options provided\")\n\t}\n\tclnt, err := privateNew(endpoint, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif s3utils.IsAmazonEndpoint(*clnt.endpointURL) {\n\t\t// If Amazon S3 set to signature v4.\n\t\tclnt.overrideSignerType = credentials.SignatureV4\n\t\t// Amazon S3 endpoints are resolved into dual-stack endpoints by default\n\t\t// for backwards compatibility.\n\t\tclnt.s3DualstackEnabled = true\n\t}\n\n\treturn clnt, nil\n}\n\n// EndpointURL returns the URL of the S3 endpoint.\nfunc (c *Client) EndpointURL() *url.URL {\n\tendpoint := *c.endpointURL // copy to prevent callers from modifying internal state\n\treturn &endpoint\n}\n\n// lockedRandSource provides protected rand source, implements rand.Source interface.\ntype lockedRandSource struct {\n\tlk  sync.Mutex\n\tsrc rand.Source\n}\n\n// Int63 returns a non-negative pseudo-random 63-bit integer as an int64.\nfunc (r *lockedRandSource) Int63() (n int64) {\n\tr.lk.Lock()\n\tn = r.src.Int63()\n\tr.lk.Unlock()\n\treturn\n}\n\n// Seed uses the provided seed value to initialize the generator to a\n// deterministic state.\nfunc (r *lockedRandSource) Seed(seed int64) {\n\tr.lk.Lock()\n\tr.src.Seed(seed)\n\tr.lk.Unlock()\n}\n\nfunc privateNew(endpoint string, opts *Options) (*Client, error) {\n\t// construct endpoint.\n\tendpointURL, err := getEndpointURL(endpoint, opts.Secure)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize cookies to preserve server sent cookies if any and replay\n\t// them upon each request.\n\tjar, err := cookiejar.New(&cookiejar.Options{PublicSuffixList: publicsuffix.List})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// instantiate new Client.\n\tclnt := new(Client)\n\n\t// Save the credentials.\n\tclnt.credsProvider = opts.Creds\n\n\t// Remember whether we are using https or not\n\tclnt.secure = opts.Secure\n\n\t// Save endpoint URL, user agent for future uses.\n\tclnt.endpointURL = endpointURL\n\n\ttransport := opts.Transport\n\tif transport == nil {\n\t\ttransport, err = DefaultTransport(opts.Secure)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tclnt.httpTrace = opts.Trace\n\n\t// Instantiate http client and bucket location cache.\n\tclnt.httpClient = &http.Client{\n\t\tJar:       jar,\n\t\tTransport: transport,\n\t\tCheckRedirect: func(_ *http.Request, _ []*http.Request) error {\n\t\t\treturn http.ErrUseLastResponse\n\t\t},\n\t}\n\n\t// Sets custom region, if region is empty bucket location cache is used automatically.\n\tif opts.Region == \"\" {\n\t\tif opts.CustomRegionViaURL != nil {\n\t\t\topts.Region = opts.CustomRegionViaURL(*clnt.endpointURL)\n\t\t} else {\n\t\t\topts.Region = s3utils.GetRegionFromURL(*clnt.endpointURL)\n\t\t}\n\t}\n\tclnt.region = opts.Region\n\n\t// Instantiate bucket location cache.\n\tclnt.bucketLocCache = newBucketLocationCache()\n\n\t// Introduce a new locked random seed.\n\tclnt.random = rand.New(&lockedRandSource{src: rand.NewSource(time.Now().UTC().UnixNano())})\n\n\t// Add default md5 hasher.\n\tclnt.md5Hasher = opts.CustomMD5\n\tclnt.sha256Hasher = opts.CustomSHA256\n\tif clnt.md5Hasher == nil {\n\t\tclnt.md5Hasher = newMd5Hasher\n\t}\n\tif clnt.sha256Hasher == nil {\n\t\tclnt.sha256Hasher = newSHA256Hasher\n\t}\n\n\tclnt.trailingHeaderSupport = opts.TrailingHeaders && clnt.overrideSignerType.IsV4()\n\n\t// Sets bucket lookup style, whether server accepts DNS or Path lookup. Default is Auto - determined\n\t// by the SDK. When Auto is specified, DNS lookup is used for Amazon/Google cloud endpoints and Path for all other endpoints.\n\tclnt.lookup = opts.BucketLookup\n\n\t// healthcheck is not initialized\n\tclnt.healthStatus = unknown\n\n\tclnt.maxRetries = MaxRetry\n\tif opts.MaxRetries > 0 {\n\t\tclnt.maxRetries = opts.MaxRetries\n\t}\n\n\t// Return.\n\treturn clnt, nil\n}\n\n// SetAppInfo - add application details to user agent.\nfunc (c *Client) SetAppInfo(appName, appVersion string) {\n\t// if app name and version not set, we do not set a new user agent.\n\tif appName != \"\" && appVersion != \"\" {\n\t\tc.appInfo.appName = appName\n\t\tc.appInfo.appVersion = appVersion\n\t}\n}\n\n// TraceOn - enable HTTP tracing.\nfunc (c *Client) TraceOn(outputStream io.Writer) {\n\t// if outputStream is nil then default to os.Stdout.\n\tif outputStream == nil {\n\t\toutputStream = os.Stdout\n\t}\n\t// Sets a new output stream.\n\tc.traceOutput = outputStream\n\n\t// Enable tracing.\n\tc.isTraceEnabled = true\n}\n\n// TraceErrorsOnlyOn - same as TraceOn, but only errors will be traced.\nfunc (c *Client) TraceErrorsOnlyOn(outputStream io.Writer) {\n\tc.TraceOn(outputStream)\n\tc.traceErrorsOnly = true\n}\n\n// TraceErrorsOnlyOff - Turns off the errors only tracing and everything will be traced after this call.\n// If all tracing needs to be turned off, call TraceOff().\nfunc (c *Client) TraceErrorsOnlyOff() {\n\tc.traceErrorsOnly = false\n}\n\n// TraceOff - disable HTTP tracing.\nfunc (c *Client) TraceOff() {\n\t// Disable tracing.\n\tc.isTraceEnabled = false\n\tc.traceErrorsOnly = false\n}\n\n// SetS3TransferAccelerate - turns s3 accelerated endpoint on or off for all your\n// requests. This feature is only specific to S3 for all other endpoints this\n// function does nothing. To read further details on s3 transfer acceleration\n// please vist -\n// http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nfunc (c *Client) SetS3TransferAccelerate(accelerateEndpoint string) {\n\tif s3utils.IsAmazonEndpoint(*c.endpointURL) {\n\t\tc.s3AccelerateEndpoint = accelerateEndpoint\n\t}\n}\n\n// SetS3EnableDualstack turns s3 dual-stack endpoints on or off for all requests.\n// The feature is only specific to S3 and is on by default. To read more about\n// Amazon S3 dual-stack endpoints visit -\n// https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html\nfunc (c *Client) SetS3EnableDualstack(enabled bool) {\n\tif s3utils.IsAmazonEndpoint(*c.endpointURL) {\n\t\tc.s3DualstackEnabled = enabled\n\t}\n}\n\n// Hash materials provides relevant initialized hash algo writers\n// based on the expected signature type.\n//\n//   - For signature v4 request if the connection is insecure compute only sha256.\n//   - For signature v4 request if the connection is secure compute only md5.\n//   - For anonymous request compute md5.\nfunc (c *Client) hashMaterials(isMd5Requested, isSha256Requested bool) (hashAlgos map[string]md5simd.Hasher, hashSums map[string][]byte) {\n\thashSums = make(map[string][]byte)\n\thashAlgos = make(map[string]md5simd.Hasher)\n\tif c.overrideSignerType.IsV4() {\n\t\tif c.secure {\n\t\t\thashAlgos[\"md5\"] = c.md5Hasher()\n\t\t} else {\n\t\t\tif isSha256Requested {\n\t\t\t\thashAlgos[\"sha256\"] = c.sha256Hasher()\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif c.overrideSignerType.IsAnonymous() {\n\t\t\thashAlgos[\"md5\"] = c.md5Hasher()\n\t\t}\n\t}\n\tif isMd5Requested {\n\t\thashAlgos[\"md5\"] = c.md5Hasher()\n\t}\n\treturn hashAlgos, hashSums\n}\n\nconst (\n\tunknown = -1\n\toffline = 0\n\tonline  = 1\n)\n\n// IsOnline returns true if healthcheck enabled and client is online.\n// If HealthCheck function has not been called this will always return true.\nfunc (c *Client) IsOnline() bool {\n\treturn !c.IsOffline()\n}\n\n// sets online healthStatus to offline\nfunc (c *Client) markOffline() {\n\tatomic.CompareAndSwapInt32(&c.healthStatus, online, offline)\n}\n\n// IsOffline returns true if healthcheck enabled and client is offline\n// If HealthCheck function has not been called this will always return false.\nfunc (c *Client) IsOffline() bool {\n\treturn atomic.LoadInt32(&c.healthStatus) == offline\n}\n\n// HealthCheck starts a healthcheck to see if endpoint is up.\n// Returns a context cancellation function, to stop the health check,\n// and an error if health check is already started.\nfunc (c *Client) HealthCheck(hcDuration time.Duration) (context.CancelFunc, error) {\n\tif atomic.LoadInt32(&c.healthStatus) != unknown {\n\t\treturn nil, fmt.Errorf(\"health check is running\")\n\t}\n\tif hcDuration < 1*time.Second {\n\t\treturn nil, fmt.Errorf(\"health check duration should be at least 1 second\")\n\t}\n\tprobeBucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"probe-health-\")\n\tctx, cancelFn := context.WithCancel(context.Background())\n\tatomic.StoreInt32(&c.healthStatus, offline)\n\t{\n\t\t// Change to online, if we can connect.\n\t\tgctx, gcancel := context.WithTimeout(ctx, 3*time.Second)\n\t\t_, err := c.getBucketLocation(gctx, probeBucketName)\n\t\tgcancel()\n\t\tif !IsNetworkOrHostDown(err, false) {\n\t\t\tswitch ToErrorResponse(err).Code {\n\t\t\tcase \"NoSuchBucket\", \"AccessDenied\", \"\":\n\t\t\t\tatomic.CompareAndSwapInt32(&c.healthStatus, offline, online)\n\t\t\t}\n\t\t}\n\t}\n\n\tgo func(duration time.Duration) {\n\t\ttimer := time.NewTimer(duration)\n\t\tdefer timer.Stop()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tatomic.StoreInt32(&c.healthStatus, unknown)\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\t// Do health check the first time and ONLY if the connection is marked offline\n\t\t\t\tif c.IsOffline() {\n\t\t\t\t\tgctx, gcancel := context.WithTimeout(context.Background(), 3*time.Second)\n\t\t\t\t\t_, err := c.getBucketLocation(gctx, probeBucketName)\n\t\t\t\t\tgcancel()\n\t\t\t\t\tif !IsNetworkOrHostDown(err, false) {\n\t\t\t\t\t\tswitch ToErrorResponse(err).Code {\n\t\t\t\t\t\tcase \"NoSuchBucket\", \"AccessDenied\", \"\":\n\t\t\t\t\t\t\tatomic.CompareAndSwapInt32(&c.healthStatus, offline, online)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttimer.Reset(duration)\n\t\t\t}\n\t\t}\n\t}(hcDuration)\n\treturn cancelFn, nil\n}\n\n// requestMetadata - is container for all the values to make a request.\ntype requestMetadata struct {\n\t// If set newRequest presigns the URL.\n\tpresignURL bool\n\n\t// User supplied.\n\tbucketName         string\n\tobjectName         string\n\tqueryValues        url.Values\n\tcustomHeader       http.Header\n\textraPresignHeader http.Header\n\texpires            int64\n\n\t// Generated by our internal code.\n\tbucketLocation   string\n\tcontentBody      io.Reader\n\tcontentLength    int64\n\tcontentMD5Base64 string // carries base64 encoded md5sum\n\tcontentSHA256Hex string // carries hex encoded sha256sum\n\tstreamSha256     bool\n\taddCrc           *ChecksumType\n\ttrailer          http.Header // (http.Request).Trailer. Requires v4 signature.\n}\n\n// dumpHTTP - dump HTTP request and response.\nfunc (c *Client) dumpHTTP(req *http.Request, resp *http.Response) error {\n\t// Starts http dump.\n\t_, err := fmt.Fprintln(c.traceOutput, \"---------START-HTTP---------\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Filter out Signature field from Authorization header.\n\torigAuth := req.Header.Get(\"Authorization\")\n\tif origAuth != \"\" {\n\t\treq.Header.Set(\"Authorization\", redactSignature(origAuth))\n\t}\n\n\t// Only display request header.\n\treqTrace, err := httputil.DumpRequestOut(req, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Write request to trace output.\n\t_, err = fmt.Fprint(c.traceOutput, string(reqTrace))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Only display response header.\n\tvar respTrace []byte\n\n\t// For errors we make sure to dump response body as well.\n\tif resp.StatusCode != http.StatusOK &&\n\t\tresp.StatusCode != http.StatusPartialContent &&\n\t\tresp.StatusCode != http.StatusNoContent {\n\t\trespTrace, err = httputil.DumpResponse(resp, true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\trespTrace, err = httputil.DumpResponse(resp, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Write response to trace output.\n\t_, err = fmt.Fprint(c.traceOutput, strings.TrimSuffix(string(respTrace), \"\\r\\n\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Ends the http dump.\n\t_, err = fmt.Fprintln(c.traceOutput, \"---------END-HTTP---------\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Returns success.\n\treturn nil\n}\n\n// do - execute http request.\nfunc (c *Client) do(req *http.Request) (resp *http.Response, err error) {\n\tdefer func() {\n\t\tif IsNetworkOrHostDown(err, false) {\n\t\t\tc.markOffline()\n\t\t}\n\t}()\n\n\tresp, err = c.httpClient.Do(req)\n\tif err != nil {\n\t\t// Handle this specifically for now until future Golang versions fix this issue properly.\n\t\tif urlErr, ok := err.(*url.Error); ok {\n\t\t\tif strings.Contains(urlErr.Err.Error(), \"EOF\") {\n\t\t\t\treturn nil, &url.Error{\n\t\t\t\t\tOp:  urlErr.Op,\n\t\t\t\t\tURL: urlErr.URL,\n\t\t\t\t\tErr: errors.New(\"Connection closed by foreign host \" + urlErr.URL + \". Retry again.\"),\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// Response cannot be non-nil, report error if thats the case.\n\tif resp == nil {\n\t\tmsg := \"Response is empty. \" + reportIssue\n\t\treturn nil, errInvalidArgument(msg)\n\t}\n\n\t// If trace is enabled, dump http request and response,\n\t// except when the traceErrorsOnly enabled and the response's status code is ok\n\tif c.isTraceEnabled && !(c.traceErrorsOnly && resp.StatusCode == http.StatusOK) {\n\t\terr = c.dumpHTTP(req, resp)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\n// List of success status.\nvar successStatus = []int{\n\thttp.StatusOK,\n\thttp.StatusNoContent,\n\thttp.StatusPartialContent,\n}\n\n// executeMethod - instantiates a given method, and retries the\n// request upon any error up to maxRetries attempts in a binomially\n// delayed manner using a standard back off algorithm.\nfunc (c *Client) executeMethod(ctx context.Context, method string, metadata requestMetadata) (res *http.Response, err error) {\n\tif c.IsOffline() {\n\t\treturn nil, errors.New(c.endpointURL.String() + \" is offline.\")\n\t}\n\n\tvar retryable bool       // Indicates if request can be retried.\n\tvar bodySeeker io.Seeker // Extracted seeker from io.Reader.\n\treqRetry := c.maxRetries // Indicates how many times we can retry the request\n\n\tif metadata.contentBody != nil {\n\t\t// Check if body is seekable then it is retryable.\n\t\tbodySeeker, retryable = metadata.contentBody.(io.Seeker)\n\t\tswitch bodySeeker {\n\t\tcase os.Stdin, os.Stdout, os.Stderr:\n\t\t\tretryable = false\n\t\t}\n\t\t// Retry only when reader is seekable\n\t\tif !retryable {\n\t\t\treqRetry = 1\n\t\t}\n\n\t\t// Figure out if the body can be closed - if yes\n\t\t// we will definitely close it upon the function\n\t\t// return.\n\t\tbodyCloser, ok := metadata.contentBody.(io.Closer)\n\t\tif ok {\n\t\t\tdefer bodyCloser.Close()\n\t\t}\n\t}\n\n\tif metadata.addCrc != nil && metadata.contentLength > 0 {\n\t\tif metadata.trailer == nil {\n\t\t\tmetadata.trailer = make(http.Header, 1)\n\t\t}\n\t\tcrc := metadata.addCrc.Hasher()\n\t\tmetadata.contentBody = newHashReaderWrapper(metadata.contentBody, crc, func(hash []byte) {\n\t\t\t// Update trailer when done.\n\t\t\tmetadata.trailer.Set(metadata.addCrc.Key(), base64.StdEncoding.EncodeToString(hash))\n\t\t})\n\t\tmetadata.trailer.Set(metadata.addCrc.Key(), base64.StdEncoding.EncodeToString(crc.Sum(nil)))\n\t}\n\n\t// Create cancel context to control 'newRetryTimer' go routine.\n\tretryCtx, cancel := context.WithCancel(ctx)\n\n\t// Indicate to our routine to exit cleanly upon return.\n\tdefer cancel()\n\n\tfor range c.newRetryTimer(retryCtx, reqRetry, DefaultRetryUnit, DefaultRetryCap, MaxJitter) {\n\t\t// Retry executes the following function body if request has an\n\t\t// error until maxRetries have been exhausted, retry attempts are\n\t\t// performed after waiting for a given period of time in a\n\t\t// binomial fashion.\n\t\tif retryable {\n\t\t\t// Seek back to beginning for each attempt.\n\t\t\tif _, err = bodySeeker.Seek(0, 0); err != nil {\n\t\t\t\t// If seek failed, no need to retry.\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\t// Instantiate a new request.\n\t\tvar req *http.Request\n\t\treq, err = c.newRequest(ctx, method, metadata)\n\t\tif err != nil {\n\t\t\terrResponse := ToErrorResponse(err)\n\t\t\tif isS3CodeRetryable(errResponse.Code) {\n\t\t\t\tcontinue // Retry.\n\t\t\t}\n\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Initiate the request.\n\t\tres, err = c.do(req)\n\t\tif err != nil {\n\t\t\tif isRequestErrorRetryable(ctx, err) {\n\t\t\t\t// Retry the request\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// For any known successful http status, return quickly.\n\t\tfor _, httpStatus := range successStatus {\n\t\t\tif httpStatus == res.StatusCode {\n\t\t\t\treturn res, nil\n\t\t\t}\n\t\t}\n\n\t\t// Read the body to be saved later.\n\t\terrBodyBytes, err := io.ReadAll(res.Body)\n\t\t// res.Body should be closed\n\t\tcloseResponse(res)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Save the body.\n\t\terrBodySeeker := bytes.NewReader(errBodyBytes)\n\t\tres.Body = io.NopCloser(errBodySeeker)\n\n\t\t// For errors verify if its retryable otherwise fail quickly.\n\t\terrResponse := ToErrorResponse(httpRespToErrorResponse(res, metadata.bucketName, metadata.objectName))\n\n\t\t// Save the body back again.\n\t\terrBodySeeker.Seek(0, 0) // Seek back to starting point.\n\t\tres.Body = io.NopCloser(errBodySeeker)\n\n\t\t// Bucket region if set in error response and the error\n\t\t// code dictates invalid region, we can retry the request\n\t\t// with the new region.\n\t\t//\n\t\t// Additionally, we should only retry if bucketLocation and custom\n\t\t// region is empty.\n\t\tif c.region == \"\" {\n\t\t\tswitch errResponse.Code {\n\t\t\tcase \"AuthorizationHeaderMalformed\":\n\t\t\t\tfallthrough\n\t\t\tcase \"InvalidRegion\":\n\t\t\t\tfallthrough\n\t\t\tcase \"AccessDenied\":\n\t\t\t\tif errResponse.Region == \"\" {\n\t\t\t\t\t// Region is empty we simply return the error.\n\t\t\t\t\treturn res, err\n\t\t\t\t}\n\t\t\t\t// Region is not empty figure out a way to\n\t\t\t\t// handle this appropriately.\n\t\t\t\tif metadata.bucketName != \"\" {\n\t\t\t\t\t// Gather Cached location only if bucketName is present.\n\t\t\t\t\tif location, cachedOk := c.bucketLocCache.Get(metadata.bucketName); cachedOk && location != errResponse.Region {\n\t\t\t\t\t\tc.bucketLocCache.Set(metadata.bucketName, errResponse.Region)\n\t\t\t\t\t\tcontinue // Retry.\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// This is for ListBuckets() fallback.\n\t\t\t\t\tif errResponse.Region != metadata.bucketLocation {\n\t\t\t\t\t\t// Retry if the error response has a different region\n\t\t\t\t\t\t// than the request we just made.\n\t\t\t\t\t\tmetadata.bucketLocation = errResponse.Region\n\t\t\t\t\t\tcontinue // Retry\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Verify if error response code is retryable.\n\t\tif isS3CodeRetryable(errResponse.Code) {\n\t\t\tcontinue // Retry.\n\t\t}\n\n\t\t// Verify if http status code is retryable.\n\t\tif isHTTPStatusRetryable(res.StatusCode) {\n\t\t\tcontinue // Retry.\n\t\t}\n\n\t\t// For all other cases break out of the retry loop.\n\t\tbreak\n\t}\n\n\t// Return an error when retry is canceled or deadlined\n\tif e := retryCtx.Err(); e != nil {\n\t\treturn nil, e\n\t}\n\n\treturn res, err\n}\n\n// newRequest - instantiate a new HTTP request for a given method.\nfunc (c *Client) newRequest(ctx context.Context, method string, metadata requestMetadata) (req *http.Request, err error) {\n\t// If no method is supplied default to 'POST'.\n\tif method == \"\" {\n\t\tmethod = http.MethodPost\n\t}\n\n\tlocation := metadata.bucketLocation\n\tif location == \"\" {\n\t\tif metadata.bucketName != \"\" {\n\t\t\t// Gather location only if bucketName is present.\n\t\t\tlocation, err = c.getBucketLocation(ctx, metadata.bucketName)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tif location == \"\" {\n\t\t\tlocation = getDefaultLocation(*c.endpointURL, c.region)\n\t\t}\n\t}\n\n\t// Look if target url supports virtual host.\n\t// We explicitly disallow MakeBucket calls to not use virtual DNS style,\n\t// since the resolution may fail.\n\tisMakeBucket := (metadata.objectName == \"\" && method == http.MethodPut && len(metadata.queryValues) == 0)\n\tisVirtualHost := c.isVirtualHostStyleRequest(*c.endpointURL, metadata.bucketName) && !isMakeBucket\n\n\t// Construct a new target URL.\n\ttargetURL, err := c.makeTargetURL(metadata.bucketName, metadata.objectName, location,\n\t\tisVirtualHost, metadata.queryValues)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif c.httpTrace != nil {\n\t\tctx = httptrace.WithClientTrace(ctx, c.httpTrace)\n\t}\n\n\t// Initialize a new HTTP request for the method.\n\treq, err = http.NewRequestWithContext(ctx, method, targetURL.String(), nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get credentials from the configured credentials provider.\n\tvalue, err := c.credsProvider.GetWithContext(c.CredContext())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tsignerType      = value.SignerType\n\t\taccessKeyID     = value.AccessKeyID\n\t\tsecretAccessKey = value.SecretAccessKey\n\t\tsessionToken    = value.SessionToken\n\t)\n\n\t// Custom signer set then override the behavior.\n\tif c.overrideSignerType != credentials.SignatureDefault {\n\t\tsignerType = c.overrideSignerType\n\t}\n\n\t// If signerType returned by credentials helper is anonymous,\n\t// then do not sign regardless of signerType override.\n\tif value.SignerType == credentials.SignatureAnonymous {\n\t\tsignerType = credentials.SignatureAnonymous\n\t}\n\n\t// Generate presign url if needed, return right here.\n\tif metadata.expires != 0 && metadata.presignURL {\n\t\tif signerType.IsAnonymous() {\n\t\t\treturn nil, errInvalidArgument(\"Presigned URLs cannot be generated with anonymous credentials.\")\n\t\t}\n\t\tif metadata.extraPresignHeader != nil {\n\t\t\tif signerType.IsV2() {\n\t\t\t\treturn nil, errInvalidArgument(\"Extra signed headers for Presign with Signature V2 is not supported.\")\n\t\t\t}\n\t\t\tfor k, v := range metadata.extraPresignHeader {\n\t\t\t\treq.Header.Set(k, v[0])\n\t\t\t}\n\t\t}\n\t\tif signerType.IsV2() {\n\t\t\t// Presign URL with signature v2.\n\t\t\treq = signer.PreSignV2(*req, accessKeyID, secretAccessKey, metadata.expires, isVirtualHost)\n\t\t} else if signerType.IsV4() {\n\t\t\t// Presign URL with signature v4.\n\t\t\treq = signer.PreSignV4(*req, accessKeyID, secretAccessKey, sessionToken, location, metadata.expires)\n\t\t}\n\t\treturn req, nil\n\t}\n\n\t// Set 'User-Agent' header for the request.\n\tc.setUserAgent(req)\n\n\t// Set all headers.\n\tfor k, v := range metadata.customHeader {\n\t\treq.Header.Set(k, v[0])\n\t}\n\n\t// Go net/http notoriously closes the request body.\n\t// - The request Body, if non-nil, will be closed by the underlying Transport, even on errors.\n\t// This can cause underlying *os.File seekers to fail, avoid that\n\t// by making sure to wrap the closer as a nop.\n\tif metadata.contentLength == 0 {\n\t\treq.Body = nil\n\t} else {\n\t\treq.Body = io.NopCloser(metadata.contentBody)\n\t}\n\n\t// Set incoming content-length.\n\treq.ContentLength = metadata.contentLength\n\tif req.ContentLength <= -1 {\n\t\t// For unknown content length, we upload using transfer-encoding: chunked.\n\t\treq.TransferEncoding = []string{\"chunked\"}\n\t}\n\n\t// set md5Sum for content protection.\n\tif len(metadata.contentMD5Base64) > 0 {\n\t\treq.Header.Set(\"Content-Md5\", metadata.contentMD5Base64)\n\t}\n\n\t// For anonymous requests just return.\n\tif signerType.IsAnonymous() {\n\t\treturn req, nil\n\t}\n\n\tswitch {\n\tcase signerType.IsV2():\n\t\t// Add signature version '2' authorization header.\n\t\treq = signer.SignV2(*req, accessKeyID, secretAccessKey, isVirtualHost)\n\tcase metadata.streamSha256 && !c.secure:\n\t\tif len(metadata.trailer) > 0 {\n\t\t\treq.Trailer = metadata.trailer\n\t\t}\n\t\t// Streaming signature is used by default for a PUT object request.\n\t\t// Additionally, we also look if the initialized client is secure,\n\t\t// if yes then we don't need to perform streaming signature.\n\t\treq = signer.StreamingSignV4(req, accessKeyID,\n\t\t\tsecretAccessKey, sessionToken, location, metadata.contentLength, time.Now().UTC(), c.sha256Hasher())\n\tdefault:\n\t\t// Set sha256 sum for signature calculation only with signature version '4'.\n\t\tshaHeader := unsignedPayload\n\t\tif metadata.contentSHA256Hex != \"\" {\n\t\t\tshaHeader = metadata.contentSHA256Hex\n\t\t\tif len(metadata.trailer) > 0 {\n\t\t\t\t// Sanity check, we should not end up here if upstream is sane.\n\t\t\t\treturn nil, errors.New(\"internal error: contentSHA256Hex with trailer not supported\")\n\t\t\t}\n\t\t} else if len(metadata.trailer) > 0 {\n\t\t\tshaHeader = unsignedPayloadTrailer\n\t\t}\n\t\treq.Header.Set(\"X-Amz-Content-Sha256\", shaHeader)\n\n\t\t// Add signature version '4' authorization header.\n\t\treq = signer.SignV4Trailer(*req, accessKeyID, secretAccessKey, sessionToken, location, metadata.trailer)\n\t}\n\n\t// Return request.\n\treturn req, nil\n}\n\n// set User agent.\nfunc (c *Client) setUserAgent(req *http.Request) {\n\treq.Header.Set(\"User-Agent\", libraryUserAgent)\n\tif c.appInfo.appName != \"\" && c.appInfo.appVersion != \"\" {\n\t\treq.Header.Set(\"User-Agent\", libraryUserAgent+\" \"+c.appInfo.appName+\"/\"+c.appInfo.appVersion)\n\t}\n}\n\n// makeTargetURL make a new target url.\nfunc (c *Client) makeTargetURL(bucketName, objectName, bucketLocation string, isVirtualHostStyle bool, queryValues url.Values) (*url.URL, error) {\n\thost := c.endpointURL.Host\n\t// For Amazon S3 endpoint, try to fetch location based endpoint.\n\tif s3utils.IsAmazonEndpoint(*c.endpointURL) {\n\t\tif c.s3AccelerateEndpoint != \"\" && bucketName != \"\" {\n\t\t\t// http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\t\t\t// Disable transfer acceleration for non-compliant bucket names.\n\t\t\tif strings.Contains(bucketName, \".\") {\n\t\t\t\treturn nil, errTransferAccelerationBucket(bucketName)\n\t\t\t}\n\t\t\t// If transfer acceleration is requested set new host.\n\t\t\t// For more details about enabling transfer acceleration read here.\n\t\t\t// http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\t\t\thost = c.s3AccelerateEndpoint\n\t\t} else {\n\t\t\t// Do not change the host if the endpoint URL is a FIPS S3 endpoint or a S3 PrivateLink interface endpoint\n\t\t\tif !s3utils.IsAmazonFIPSEndpoint(*c.endpointURL) && !s3utils.IsAmazonPrivateLinkEndpoint(*c.endpointURL) {\n\t\t\t\t// Fetch new host based on the bucket location.\n\t\t\t\thost = getS3Endpoint(bucketLocation, c.s3DualstackEnabled)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Save scheme.\n\tscheme := c.endpointURL.Scheme\n\n\t// Strip port 80 and 443 so we won't send these ports in Host header.\n\t// The reason is that browsers and curl automatically remove :80 and :443\n\t// with the generated presigned urls, then a signature mismatch error.\n\tif h, p, err := net.SplitHostPort(host); err == nil {\n\t\tif scheme == \"http\" && p == \"80\" || scheme == \"https\" && p == \"443\" {\n\t\t\thost = h\n\t\t\tif ip := net.ParseIP(h); ip != nil && ip.To4() == nil {\n\t\t\t\thost = \"[\" + h + \"]\"\n\t\t\t}\n\t\t}\n\t}\n\n\turlStr := scheme + \"://\" + host + \"/\"\n\n\t// Make URL only if bucketName is available, otherwise use the\n\t// endpoint URL.\n\tif bucketName != \"\" {\n\t\t// If endpoint supports virtual host style use that always.\n\t\t// Currently only S3 and Google Cloud Storage would support\n\t\t// virtual host style.\n\t\tif isVirtualHostStyle {\n\t\t\turlStr = scheme + \"://\" + bucketName + \".\" + host + \"/\"\n\t\t\tif objectName != \"\" {\n\t\t\t\turlStr += s3utils.EncodePath(objectName)\n\t\t\t}\n\t\t} else {\n\t\t\t// If not fall back to using path style.\n\t\t\turlStr = urlStr + bucketName + \"/\"\n\t\t\tif objectName != \"\" {\n\t\t\t\turlStr += s3utils.EncodePath(objectName)\n\t\t\t}\n\t\t}\n\t}\n\n\t// If there are any query values, add them to the end.\n\tif len(queryValues) > 0 {\n\t\turlStr = urlStr + \"?\" + s3utils.QueryEncode(queryValues)\n\t}\n\n\treturn url.Parse(urlStr)\n}\n\n// returns true if virtual hosted style requests are to be used.\nfunc (c *Client) isVirtualHostStyleRequest(url url.URL, bucketName string) bool {\n\tif bucketName == \"\" {\n\t\treturn false\n\t}\n\n\tif c.lookup == BucketLookupDNS {\n\t\treturn true\n\t}\n\tif c.lookup == BucketLookupPath {\n\t\treturn false\n\t}\n\n\t// default to virtual only for Amazon/Google  storage. In all other cases use\n\t// path style requests\n\treturn s3utils.IsVirtualHostSupported(url, bucketName)\n}\n\n// CredContext returns the context for fetching credentials\nfunc (c *Client) CredContext() *credentials.CredContext {\n\thttpClient := c.httpClient\n\tif httpClient == nil {\n\t\thttpClient = http.DefaultClient\n\t}\n\treturn &credentials.CredContext{\n\t\tClient:   httpClient,\n\t\tEndpoint: c.endpointURL.String(),\n\t}\n}\n"
        },
        {
          "name": "api_unit_test.go",
          "type": "blob",
          "size": 8.1923828125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2024 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"net/url\"\n\t\"testing\"\n\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/policy\"\n)\n\n// Tests valid hosts for location.\nfunc TestValidBucketLocation(t *testing.T) {\n\ts3Hosts := []struct {\n\t\tbucketLocation string\n\t\tuseDualstack   bool\n\t\tendpoint       string\n\t}{\n\t\t{\"us-east-1\", true, \"s3.dualstack.us-east-1.amazonaws.com\"},\n\t\t{\"us-east-1\", false, \"s3.us-east-1.amazonaws.com\"},\n\t\t{\"unknown\", true, \"s3.dualstack.us-east-1.amazonaws.com\"},\n\t\t{\"unknown\", false, \"s3.us-east-1.amazonaws.com\"},\n\t\t{\"ap-southeast-1\", true, \"s3.dualstack.ap-southeast-1.amazonaws.com\"},\n\t\t{\"ap-southeast-1\", false, \"s3.ap-southeast-1.amazonaws.com\"},\n\t}\n\tfor _, s3Host := range s3Hosts {\n\t\tendpoint := getS3Endpoint(s3Host.bucketLocation, s3Host.useDualstack)\n\t\tif endpoint != s3Host.endpoint {\n\t\t\tt.Fatal(\"Error: invalid bucket location\", endpoint)\n\t\t}\n\t}\n}\n\n// Tests error response structure.\nfunc TestErrorResponse(t *testing.T) {\n\tvar err error\n\terr = ErrorResponse{\n\t\tCode: \"Testing\",\n\t}\n\terrResp := ToErrorResponse(err)\n\tif errResp.Code != \"Testing\" {\n\t\tt.Fatal(\"Type conversion failed, we have an empty struct.\")\n\t}\n\n\t// Should fail with invalid argument.\n\terr = httpRespToErrorResponse(nil, \"\", \"\")\n\terrResp = ToErrorResponse(err)\n\tif errResp.Code != \"InvalidArgument\" {\n\t\tt.Fatal(\"Empty response input should return invalid argument.\")\n\t}\n}\n\n// Tests signature type.\nfunc TestSignatureType(t *testing.T) {\n\tclnt := Client{}\n\tif !clnt.overrideSignerType.IsV4() {\n\t\tt.Fatal(\"Error\")\n\t}\n\tclnt.overrideSignerType = credentials.SignatureV2\n\tif !clnt.overrideSignerType.IsV2() {\n\t\tt.Fatal(\"Error\")\n\t}\n\tif clnt.overrideSignerType.IsV4() {\n\t\tt.Fatal(\"Error\")\n\t}\n\tclnt.overrideSignerType = credentials.SignatureV4\n\tif !clnt.overrideSignerType.IsV4() {\n\t\tt.Fatal(\"Error\")\n\t}\n}\n\n// Tests bucket policy types.\nfunc TestBucketPolicyTypes(t *testing.T) {\n\twant := map[string]bool{\n\t\t\"none\":      true,\n\t\t\"readonly\":  true,\n\t\t\"writeonly\": true,\n\t\t\"readwrite\": true,\n\t\t\"invalid\":   false,\n\t}\n\tfor bucketPolicy, ok := range want {\n\t\tif policy.BucketPolicy(bucketPolicy).IsValidBucketPolicy() != ok {\n\t\t\tt.Fatal(\"Error\")\n\t\t}\n\t}\n}\n\n// Tests optimal part size.\nfunc TestPartSize(t *testing.T) {\n\t_, _, _, err := OptimalPartInfo(5000000000000000000, minPartSize)\n\tif err == nil {\n\t\tt.Fatal(\"Error: should fail\")\n\t}\n\ttotalPartsCount, partSize, lastPartSize, err := OptimalPartInfo(5243928576, 5*1024*1024)\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\tif totalPartsCount != 1001 {\n\t\tt.Fatalf(\"Error: expecting total parts count of 1001: got %v instead\", totalPartsCount)\n\t}\n\tif partSize != 5242880 {\n\t\tt.Fatalf(\"Error: expecting part size of 5242880: got %v instead\", partSize)\n\t}\n\tif lastPartSize != 1048576 {\n\t\tt.Fatalf(\"Error: expecting last part size of 1048576: got %v instead\", lastPartSize)\n\t}\n\ttotalPartsCount, partSize, lastPartSize, err = OptimalPartInfo(5243928576, 0)\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\tif totalPartsCount != 313 {\n\t\tt.Fatalf(\"Error: expecting total parts count of 313: got %v instead\", totalPartsCount)\n\t}\n\tif partSize != 16777216 {\n\t\tt.Fatalf(\"Error: expecting part size of 16777216: got %v instead\", partSize)\n\t}\n\tif lastPartSize != 9437184 {\n\t\tt.Fatalf(\"Error: expecting last part size of 9437184: got %v instead\", lastPartSize)\n\t}\n\t_, partSize, _, err = OptimalPartInfo(5000000000, minPartSize)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\tif partSize != minPartSize {\n\t\tt.Fatalf(\"Error: expecting part size of %v: got %v instead\", minPartSize, partSize)\n\t}\n\t// if stream and using default optimal part size determined by sdk\n\ttotalPartsCount, partSize, lastPartSize, err = OptimalPartInfo(-1, 0)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\tif totalPartsCount != 9930 {\n\t\tt.Fatalf(\"Error: expecting total parts count of 9930: got %v instead\", totalPartsCount)\n\t}\n\tif partSize != 553648128 {\n\t\tt.Fatalf(\"Error: expecting part size of 553648128: got %v instead\", partSize)\n\t}\n\tif lastPartSize != 385875968 {\n\t\tt.Fatalf(\"Error: expecting last part size of 385875968: got %v instead\", lastPartSize)\n\t}\n\n\ttotalPartsCount, partSize, lastPartSize, err = OptimalPartInfo(-1, 64*1024*1024)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\tif totalPartsCount != 10000 {\n\t\tt.Fatalf(\"Error: expecting total parts count of 10000: got %v instead\", totalPartsCount)\n\t}\n\tif partSize != 67108864 {\n\t\tt.Fatalf(\"Error: expecting part size of 67108864: got %v instead\", partSize)\n\t}\n\tif lastPartSize != 67108864 {\n\t\tt.Fatalf(\"Error: expecting part size of 67108864: got %v instead\", lastPartSize)\n\t}\n}\n\n// TestMakeTargetURL - testing makeTargetURL()\nfunc TestMakeTargetURL(t *testing.T) {\n\ttestCases := []struct {\n\t\taddr           string\n\t\tsecure         bool\n\t\tbucketName     string\n\t\tobjectName     string\n\t\tbucketLocation string\n\t\tqueryValues    map[string][]string\n\t\texpectedURL    url.URL\n\t\texpectedErr    error\n\t}{\n\t\t// Test 1\n\t\t{\"localhost:9000\", false, \"\", \"\", \"\", nil, url.URL{Host: \"localhost:9000\", Scheme: \"http\", Path: \"/\"}, nil},\n\t\t// Test 2\n\t\t{\"localhost\", true, \"\", \"\", \"\", nil, url.URL{Host: \"localhost\", Scheme: \"https\", Path: \"/\"}, nil},\n\t\t// Test 3\n\t\t{\"localhost:9000\", true, \"mybucket\", \"\", \"\", nil, url.URL{Host: \"localhost:9000\", Scheme: \"https\", Path: \"/mybucket/\"}, nil},\n\t\t// Test 4, testing against google storage API\n\t\t{\"storage.googleapis.com\", true, \"mybucket\", \"\", \"\", nil, url.URL{Host: \"mybucket.storage.googleapis.com\", Scheme: \"https\", Path: \"/\"}, nil},\n\t\t// Test 5, testing against AWS S3 API\n\t\t{\"s3.amazonaws.com\", true, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"mybucket.s3.dualstack.us-east-1.amazonaws.com\", Scheme: \"https\", Path: \"/myobject\"}, nil},\n\t\t// Test 6\n\t\t{\"localhost:9000\", false, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"localhost:9000\", Scheme: \"http\", Path: \"/mybucket/myobject\"}, nil},\n\t\t// Test 7, testing with query\n\t\t{\"localhost:9000\", false, \"mybucket\", \"myobject\", \"\", map[string][]string{\"param\": {\"val\"}}, url.URL{Host: \"localhost:9000\", Scheme: \"http\", Path: \"/mybucket/myobject\", RawQuery: \"param=val\"}, nil},\n\t\t// Test 8, testing with port 80\n\t\t{\"localhost:80\", false, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"localhost\", Scheme: \"http\", Path: \"/mybucket/myobject\"}, nil},\n\t\t// Test 9, testing with port 443\n\t\t{\"localhost:443\", true, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"localhost\", Scheme: \"https\", Path: \"/mybucket/myobject\"}, nil},\n\t\t{\"[240b:c0e0:102:54C0:1c05:c2c1:19:5001]:443\", true, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"[240b:c0e0:102:54C0:1c05:c2c1:19:5001]\", Scheme: \"https\", Path: \"/mybucket/myobject\"}, nil},\n\t\t{\"[240b:c0e0:102:54C0:1c05:c2c1:19:5001]:9000\", true, \"mybucket\", \"myobject\", \"\", nil, url.URL{Host: \"[240b:c0e0:102:54C0:1c05:c2c1:19:5001]:9000\", Scheme: \"https\", Path: \"/mybucket/myobject\"}, nil},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Initialize a MinIO client\n\t\tc, _ := New(testCase.addr, &Options{\n\t\t\tCreds:  credentials.NewStaticV4(\"foo\", \"bar\", \"\"),\n\t\t\tSecure: testCase.secure,\n\t\t})\n\t\tisVirtualHost := c.isVirtualHostStyleRequest(*c.endpointURL, testCase.bucketName)\n\t\tu, err := c.makeTargetURL(testCase.bucketName, testCase.objectName, testCase.bucketLocation, isVirtualHost, testCase.queryValues)\n\t\t// Check the returned error\n\t\tif testCase.expectedErr == nil && err != nil {\n\t\t\tt.Fatalf(\"Test %d: Should succeed but failed with err = %v\", i+1, err)\n\t\t}\n\t\tif testCase.expectedErr != nil && err == nil {\n\t\t\tt.Fatalf(\"Test %d: Should fail but succeeded\", i+1)\n\t\t}\n\t\tif err == nil {\n\t\t\t// Check if the returned url is equal to what we expect\n\t\t\tif u.String() != testCase.expectedURL.String() {\n\t\t\t\tt.Fatalf(\"Test %d: Mismatched target url: expected = `%v`, found = `%v`\",\n\t\t\t\t\ti+1, testCase.expectedURL.String(), u.String())\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "bucket-cache.go",
          "type": "blob",
          "size": 6.8857421875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"path\"\n\t\"sync\"\n\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n\t\"github.com/minio/minio-go/v7/pkg/signer\"\n)\n\n// bucketLocationCache - Provides simple mechanism to hold bucket\n// locations in memory.\ntype bucketLocationCache struct {\n\t// mutex is used for handling the concurrent\n\t// read/write requests for cache.\n\tsync.RWMutex\n\n\t// items holds the cached bucket locations.\n\titems map[string]string\n}\n\n// newBucketLocationCache - Provides a new bucket location cache to be\n// used internally with the client object.\nfunc newBucketLocationCache() *bucketLocationCache {\n\treturn &bucketLocationCache{\n\t\titems: make(map[string]string),\n\t}\n}\n\n// Get - Returns a value of a given key if it exists.\nfunc (r *bucketLocationCache) Get(bucketName string) (location string, ok bool) {\n\tr.RLock()\n\tdefer r.RUnlock()\n\tlocation, ok = r.items[bucketName]\n\treturn\n}\n\n// Set - Will persist a value into cache.\nfunc (r *bucketLocationCache) Set(bucketName, location string) {\n\tr.Lock()\n\tdefer r.Unlock()\n\tr.items[bucketName] = location\n}\n\n// Delete - Deletes a bucket name from cache.\nfunc (r *bucketLocationCache) Delete(bucketName string) {\n\tr.Lock()\n\tdefer r.Unlock()\n\tdelete(r.items, bucketName)\n}\n\n// GetBucketLocation - get location for the bucket name from location cache, if not\n// fetch freshly by making a new request.\nfunc (c *Client) GetBucketLocation(ctx context.Context, bucketName string) (string, error) {\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn c.getBucketLocation(ctx, bucketName)\n}\n\n// getBucketLocation - Get location for the bucketName from location map cache, if not\n// fetch freshly by making a new request.\nfunc (c *Client) getBucketLocation(ctx context.Context, bucketName string) (string, error) {\n\tif err := s3utils.CheckValidBucketName(bucketName); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Region set then no need to fetch bucket location.\n\tif c.region != \"\" {\n\t\treturn c.region, nil\n\t}\n\n\tif location, ok := c.bucketLocCache.Get(bucketName); ok {\n\t\treturn location, nil\n\t}\n\n\t// Initialize a new request.\n\treq, err := c.getBucketLocationRequest(ctx, bucketName)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Initiate the request.\n\tresp, err := c.do(req)\n\tdefer closeResponse(resp)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tlocation, err := processBucketLocationResponse(resp, bucketName)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tc.bucketLocCache.Set(bucketName, location)\n\treturn location, nil\n}\n\n// processes the getBucketLocation http response from the server.\nfunc processBucketLocationResponse(resp *http.Response, bucketName string) (bucketLocation string, err error) {\n\tif resp != nil {\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\terr = httpRespToErrorResponse(resp, bucketName, \"\")\n\t\t\terrResp := ToErrorResponse(err)\n\t\t\t// For access denied error, it could be an anonymous\n\t\t\t// request. Move forward and let the top level callers\n\t\t\t// succeed if possible based on their policy.\n\t\t\tswitch errResp.Code {\n\t\t\tcase \"NotImplemented\":\n\t\t\t\tswitch errResp.Server {\n\t\t\t\tcase \"AmazonSnowball\":\n\t\t\t\t\treturn \"snowball\", nil\n\t\t\t\tcase \"cloudflare\":\n\t\t\t\t\treturn \"us-east-1\", nil\n\t\t\t\t}\n\t\t\tcase \"AuthorizationHeaderMalformed\":\n\t\t\t\tfallthrough\n\t\t\tcase \"InvalidRegion\":\n\t\t\t\tfallthrough\n\t\t\tcase \"AccessDenied\":\n\t\t\t\tif errResp.Region == \"\" {\n\t\t\t\t\treturn \"us-east-1\", nil\n\t\t\t\t}\n\t\t\t\treturn errResp.Region, nil\n\t\t\t}\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\t// Extract location.\n\tvar locationConstraint string\n\terr = xmlDecoder(resp.Body, &locationConstraint)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tlocation := locationConstraint\n\t// Location is empty will be 'us-east-1'.\n\tif location == \"\" {\n\t\tlocation = \"us-east-1\"\n\t}\n\n\t// Location can be 'EU' convert it to meaningful 'eu-west-1'.\n\tif location == \"EU\" {\n\t\tlocation = \"eu-west-1\"\n\t}\n\n\t// Save the location into cache.\n\n\t// Return.\n\treturn location, nil\n}\n\n// getBucketLocationRequest - Wrapper creates a new getBucketLocation request.\nfunc (c *Client) getBucketLocationRequest(ctx context.Context, bucketName string) (*http.Request, error) {\n\t// Set location query.\n\turlValues := make(url.Values)\n\turlValues.Set(\"location\", \"\")\n\n\t// Set get bucket location always as path style.\n\ttargetURL := *c.endpointURL\n\n\t// as it works in makeTargetURL method from api.go file\n\tif h, p, err := net.SplitHostPort(targetURL.Host); err == nil {\n\t\tif targetURL.Scheme == \"http\" && p == \"80\" || targetURL.Scheme == \"https\" && p == \"443\" {\n\t\t\ttargetURL.Host = h\n\t\t\tif ip := net.ParseIP(h); ip != nil && ip.To16() != nil {\n\t\t\t\ttargetURL.Host = \"[\" + h + \"]\"\n\t\t\t}\n\t\t}\n\t}\n\n\tisVirtualStyle := c.isVirtualHostStyleRequest(targetURL, bucketName)\n\n\tvar urlStr string\n\n\tif isVirtualStyle {\n\t\turlStr = c.endpointURL.Scheme + \"://\" + bucketName + \".\" + targetURL.Host + \"/?location\"\n\t} else {\n\t\ttargetURL.Path = path.Join(bucketName, \"\") + \"/\"\n\t\ttargetURL.RawQuery = urlValues.Encode()\n\t\turlStr = targetURL.String()\n\t}\n\n\t// Get a new HTTP request for the method.\n\treq, err := http.NewRequestWithContext(ctx, http.MethodGet, urlStr, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Set UserAgent for the request.\n\tc.setUserAgent(req)\n\n\t// Get credentials from the configured credentials provider.\n\tvalue, err := c.credsProvider.GetWithContext(c.CredContext())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tsignerType      = value.SignerType\n\t\taccessKeyID     = value.AccessKeyID\n\t\tsecretAccessKey = value.SecretAccessKey\n\t\tsessionToken    = value.SessionToken\n\t)\n\n\t// Custom signer set then override the behavior.\n\tif c.overrideSignerType != credentials.SignatureDefault {\n\t\tsignerType = c.overrideSignerType\n\t}\n\n\t// If signerType returned by credentials helper is anonymous,\n\t// then do not sign regardless of signerType override.\n\tif value.SignerType == credentials.SignatureAnonymous {\n\t\tsignerType = credentials.SignatureAnonymous\n\t}\n\n\tif signerType.IsAnonymous() {\n\t\treturn req, nil\n\t}\n\n\tif signerType.IsV2() {\n\t\treq = signer.SignV2(*req, accessKeyID, secretAccessKey, isVirtualStyle)\n\t\treturn req, nil\n\t}\n\n\t// Set sha256 sum for signature calculation only with signature version '4'.\n\tcontentSha256 := emptySHA256Hex\n\tif c.secure {\n\t\tcontentSha256 = unsignedPayload\n\t}\n\n\treq.Header.Set(\"X-Amz-Content-Sha256\", contentSha256)\n\treq = signer.SignV4(*req, accessKeyID, secretAccessKey, sessionToken, \"us-east-1\")\n\treturn req, nil\n}\n"
        },
        {
          "name": "bucket-cache_test.go",
          "type": "blob",
          "size": 12.1005859375,
          "content": "/*\n * Copyright\n *  2015, 2016, 2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"path\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/signer\"\n)\n\n// Test validates `newBucketLocationCache`.\nfunc TestNewBucketLocationCache(t *testing.T) {\n\texpectedBucketLocationcache := &bucketLocationCache{\n\t\titems: make(map[string]string),\n\t}\n\tactualBucketLocationCache := newBucketLocationCache()\n\n\tif !reflect.DeepEqual(actualBucketLocationCache, expectedBucketLocationcache) {\n\t\tt.Errorf(\"Unexpected return value\")\n\t}\n}\n\n// Tests validate bucketLocationCache operations.\nfunc TestBucketLocationCacheOps(t *testing.T) {\n\ttestBucketLocationCache := newBucketLocationCache()\n\texpectedBucketName := \"minio-bucket\"\n\texpectedLocation := \"us-east-1\"\n\ttestBucketLocationCache.Set(expectedBucketName, expectedLocation)\n\tactualLocation, ok := testBucketLocationCache.Get(expectedBucketName)\n\tif !ok {\n\t\tt.Errorf(\"Bucket location cache not set\")\n\t}\n\tif expectedLocation != actualLocation {\n\t\tt.Errorf(\"Bucket location cache not set to expected value\")\n\t}\n\ttestBucketLocationCache.Delete(expectedBucketName)\n\t_, ok = testBucketLocationCache.Get(expectedBucketName)\n\tif ok {\n\t\tt.Errorf(\"Bucket location cache not deleted as expected\")\n\t}\n}\n\n// Tests validate http request generation for 'getBucketLocation'.\nfunc TestGetBucketLocationRequest(t *testing.T) {\n\t// Generates expected http request for getBucketLocation.\n\t// Used for asserting with the actual request generated.\n\tcreateExpectedRequest := func(c *Client, bucketName string) (*http.Request, error) {\n\t\t// Set location query.\n\t\turlValues := make(url.Values)\n\t\turlValues.Set(\"location\", \"\")\n\n\t\t// Set get bucket location always as path style.\n\t\ttargetURL := *c.endpointURL\n\n\t\tisVirtualStyle := c.isVirtualHostStyleRequest(targetURL, bucketName)\n\n\t\tvar urlStr string\n\t\tif isVirtualStyle {\n\t\t\turlStr = targetURL.Scheme + \"://\" + bucketName + \".\" + targetURL.Host + \"/?location\"\n\t\t} else {\n\t\t\ttargetURL.Path = path.Join(bucketName, \"\") + \"/\"\n\t\t\ttargetURL.RawQuery = urlValues.Encode()\n\t\t\turlStr = targetURL.String()\n\t\t}\n\n\t\t// Get a new HTTP request for the method.\n\t\treq, err := http.NewRequest(http.MethodGet, urlStr, nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Set UserAgent for the request.\n\t\tc.setUserAgent(req)\n\n\t\t// Get credentials from the configured credentials provider.\n\t\tvalue, err := c.credsProvider.GetWithContext(c.CredContext())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar (\n\t\t\tsignerType      = value.SignerType\n\t\t\taccessKeyID     = value.AccessKeyID\n\t\t\tsecretAccessKey = value.SecretAccessKey\n\t\t\tsessionToken    = value.SessionToken\n\t\t)\n\n\t\t// Custom signer set then override the behavior.\n\t\tif c.overrideSignerType != credentials.SignatureDefault {\n\t\t\tsignerType = c.overrideSignerType\n\t\t}\n\n\t\t// If signerType returned by credentials helper is anonymous,\n\t\t// then do not sign regardless of signerType override.\n\t\tif value.SignerType == credentials.SignatureAnonymous {\n\t\t\tsignerType = credentials.SignatureAnonymous\n\t\t}\n\n\t\t// Set sha256 sum for signature calculation only\n\t\t// with signature version '4'.\n\t\tswitch {\n\t\tcase signerType.IsV4():\n\t\t\tcontentSha256 := emptySHA256Hex\n\t\t\tif c.secure {\n\t\t\t\tcontentSha256 = unsignedPayload\n\t\t\t}\n\t\t\treq.Header.Set(\"X-Amz-Content-Sha256\", contentSha256)\n\t\t\treq = signer.SignV4(*req, accessKeyID, secretAccessKey, sessionToken, \"us-east-1\")\n\t\tcase signerType.IsV2():\n\t\t\treq = signer.SignV2(*req, accessKeyID, secretAccessKey, false)\n\t\t}\n\n\t\treturn req, nil\n\t}\n\t// Info for 'Client' creation.\n\t// Will be used as arguments for 'NewClient'.\n\ttype infoForClient struct {\n\t\tendPoint       string\n\t\taccessKey      string\n\t\tsecretKey      string\n\t\tenableInsecure bool\n\t}\n\t// dataset for 'NewClient' call.\n\tinfo := []infoForClient{\n\t\t// endpoint localhost.\n\t\t// both access-key and secret-key are empty.\n\t\t{\"localhost:9000\", \"\", \"\", false},\n\t\t// both access-key are secret-key exists.\n\t\t{\"localhost:9000\", \"my-access-key\", \"my-secret-key\", false},\n\t\t// one of acess-key and secret-key are empty.\n\t\t{\"localhost:9000\", \"\", \"my-secret-key\", false},\n\n\t\t// endpoint amazon s3.\n\t\t{\"s3.amazonaws.com\", \"\", \"\", false},\n\t\t{\"s3.amazonaws.com\", \"my-access-key\", \"my-secret-key\", false},\n\t\t{\"s3.amazonaws.com\", \"my-acess-key\", \"\", false},\n\n\t\t// endpoint google cloud storage.\n\t\t{\"storage.googleapis.com\", \"\", \"\", false},\n\t\t{\"storage.googleapis.com\", \"my-access-key\", \"my-secret-key\", false},\n\t\t{\"storage.googleapis.com\", \"\", \"my-secret-key\", false},\n\n\t\t// endpoint custom domain running MinIO server.\n\t\t{\"play.min.io\", \"\", \"\", false},\n\t\t{\"play.min.io\", \"my-access-key\", \"my-secret-key\", false},\n\t\t{\"play.min.io\", \"my-acess-key\", \"\", false},\n\t}\n\ttestCases := []struct {\n\t\tbucketName string\n\t\t// data for new client creation.\n\t\tinfo infoForClient\n\t\t// error in the output.\n\t\terr error\n\t\t// flag indicating whether tests should pass.\n\t\tshouldPass bool\n\t}{\n\t\t// Client is constructed using the info struct.\n\t\t// case with empty location.\n\t\t{\"my-bucket\", info[0], nil, true},\n\t\t// case with location set to standard 'us-east-1'.\n\t\t{\"my-bucket\", info[0], nil, true},\n\t\t// case with location set to a value different from 'us-east-1'.\n\t\t{\"my-bucket\", info[0], nil, true},\n\n\t\t{\"my-bucket\", info[1], nil, true},\n\t\t{\"my-bucket\", info[1], nil, true},\n\t\t{\"my-bucket\", info[1], nil, true},\n\n\t\t{\"my-bucket\", info[2], nil, true},\n\t\t{\"my-bucket\", info[2], nil, true},\n\t\t{\"my-bucket\", info[2], nil, true},\n\n\t\t{\"my-bucket\", info[3], nil, true},\n\t\t{\"my-bucket\", info[3], nil, true},\n\t\t{\"my-bucket\", info[3], nil, true},\n\n\t\t{\"my-bucket\", info[4], nil, true},\n\t\t{\"my-bucket\", info[4], nil, true},\n\t\t{\"my-bucket\", info[4], nil, true},\n\n\t\t{\"my-bucket\", info[5], nil, true},\n\t\t{\"my-bucket\", info[5], nil, true},\n\t\t{\"my-bucket\", info[5], nil, true},\n\n\t\t{\"my-bucket\", info[6], nil, true},\n\t\t{\"my-bucket\", info[6], nil, true},\n\t\t{\"my-bucket\", info[6], nil, true},\n\n\t\t{\"my-bucket\", info[7], nil, true},\n\t\t{\"my-bucket\", info[7], nil, true},\n\t\t{\"my-bucket\", info[7], nil, true},\n\n\t\t{\"my-bucket\", info[8], nil, true},\n\t\t{\"my-bucket\", info[8], nil, true},\n\t\t{\"my-bucket\", info[8], nil, true},\n\n\t\t{\"my-bucket\", info[9], nil, true},\n\t\t{\"my-bucket\", info[9], nil, true},\n\t\t{\"my-bucket\", info[9], nil, true},\n\n\t\t{\"my-bucket\", info[10], nil, true},\n\t\t{\"my-bucket\", info[10], nil, true},\n\t\t{\"my-bucket\", info[10], nil, true},\n\n\t\t{\"my-bucket\", info[11], nil, true},\n\t\t{\"my-bucket\", info[11], nil, true},\n\t\t{\"my-bucket\", info[11], nil, true},\n\t}\n\tfor i, testCase := range testCases {\n\t\t// cannot create a newclient with empty endPoint value.\n\t\t// validates and creates a new client only if the endPoint value is not empty.\n\t\tclient := &Client{}\n\t\tvar err error\n\t\tif testCase.info.endPoint != \"\" {\n\t\t\tclient, err = New(testCase.info.endPoint, &Options{\n\t\t\t\tCreds:  credentials.NewStaticV4(testCase.info.accessKey, testCase.info.secretKey, \"\"),\n\t\t\t\tSecure: testCase.info.enableInsecure,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Test %d: Failed to create new Client: %s\", i+1, err.Error())\n\t\t\t}\n\t\t}\n\n\t\tactualReq, err := client.getBucketLocationRequest(context.Background(), testCase.bucketName)\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err.Error())\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err.Error())\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err.Error(), err.Error())\n\t\t\t}\n\t\t}\n\n\t\t// Test passes as expected, but the output values are verified for correctness here.\n\t\tif err == nil && testCase.shouldPass {\n\t\t\texpectedReq, err := createExpectedRequest(client, testCase.bucketName)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Test %d: Expected request Creation failed\", i+1)\n\t\t\t}\n\t\t\tif expectedReq.Method != actualReq.Method {\n\t\t\t\tt.Errorf(\"Test %d: The expected Request method doesn't match with the actual one\", i+1)\n\t\t\t}\n\t\t\tif expectedReq.URL.String() != actualReq.URL.String() {\n\t\t\t\tt.Errorf(\"Test %d: Expected the request URL to be '%s', but instead found '%s'\", i+1, expectedReq.URL.String(), actualReq.URL.String())\n\t\t\t}\n\t\t\tif expectedReq.ContentLength != actualReq.ContentLength {\n\t\t\t\tt.Errorf(\"Test %d: Expected the request body Content-Length to be '%d', but found '%d' instead\", i+1, expectedReq.ContentLength, actualReq.ContentLength)\n\t\t\t}\n\n\t\t\tif expectedReq.Header.Get(\"X-Amz-Content-Sha256\") != actualReq.Header.Get(\"X-Amz-Content-Sha256\") {\n\t\t\t\tt.Errorf(\"Test %d: 'X-Amz-Content-Sha256' header of the expected request doesn't match with that of the actual request\", i+1)\n\t\t\t}\n\t\t\tif expectedReq.Header.Get(\"User-Agent\") != actualReq.Header.Get(\"User-Agent\") {\n\t\t\t\tt.Errorf(\"Test %d: Expected 'User-Agent' header to be \\\"%s\\\",but found \\\"%s\\\" instead\", i+1, expectedReq.Header.Get(\"User-Agent\"), actualReq.Header.Get(\"User-Agent\"))\n\t\t\t}\n\t\t}\n\t}\n}\n\n// generates http response with bucket location set in the body.\nfunc generateLocationResponse(resp *http.Response, bodyContent []byte) (*http.Response, error) {\n\tresp.StatusCode = http.StatusOK\n\tresp.Body = io.NopCloser(bytes.NewBuffer(bodyContent))\n\treturn resp, nil\n}\n\n// Tests the processing of GetPolicy response from server.\nfunc TestProcessBucketLocationResponse(t *testing.T) {\n\t// LocationResponse - format for location response.\n\ttype LocationResponse struct {\n\t\tXMLName  xml.Name `xml:\"http://s3.amazonaws.com/doc/2006-03-01/ LocationConstraint\" json:\"-\"`\n\t\tLocation string   `xml:\",chardata\"`\n\t}\n\n\tAPIErrors := []APIError{\n\t\t{\n\t\t\tCode:           \"AccessDenied\",\n\t\t\tDescription:    \"Access Denied\",\n\t\t\tHTTPStatusCode: http.StatusUnauthorized,\n\t\t},\n\t}\n\ttestCases := []struct {\n\t\tbucketName    string\n\t\tinputLocation string\n\t\tisAPIError    bool\n\t\tapiErr        APIError\n\t\t// expected results.\n\t\texpectedResult string\n\t\terr            error\n\t\t// flag indicating whether tests should pass.\n\t\tshouldPass bool\n\t}{\n\t\t{\"my-bucket\", \"\", true, APIErrors[0], \"us-east-1\", nil, true},\n\t\t{\"my-bucket\", \"\", false, APIError{}, \"us-east-1\", nil, true},\n\t\t{\"my-bucket\", \"EU\", false, APIError{}, \"eu-west-1\", nil, true},\n\t\t{\"my-bucket\", \"eu-central-1\", false, APIError{}, \"eu-central-1\", nil, true},\n\t\t{\"my-bucket\", \"us-east-1\", false, APIError{}, \"us-east-1\", nil, true},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tinputResponse := &http.Response{}\n\t\tvar err error\n\t\tif testCase.isAPIError {\n\t\t\tinputResponse = generateErrorResponse(inputResponse, testCase.apiErr, testCase.bucketName)\n\t\t} else {\n\t\t\tinputResponse, err = generateLocationResponse(inputResponse, encodeResponse(LocationResponse{\n\t\t\t\tLocation: testCase.inputLocation,\n\t\t\t}))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Test %d: Creation of valid response failed\", i+1)\n\t\t\t}\n\t\t}\n\t\tactualResult, err := processBucketLocationResponse(inputResponse, \"my-bucket\")\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err.Error())\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err.Error())\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err.Error(), err.Error())\n\t\t\t}\n\t\t}\n\t\tif err == nil && testCase.shouldPass {\n\t\t\tif !reflect.DeepEqual(testCase.expectedResult, actualResult) {\n\t\t\t\tt.Errorf(\"Test %d: The expected BucketPolicy doesn't match the actual BucketPolicy\", i+1)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "checksum.go",
          "type": "blob",
          "size": 10.8701171875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2023 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"crypto/sha1\"\n\t\"crypto/sha256\"\n\t\"encoding/base64\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"hash\"\n\t\"hash/crc32\"\n\t\"hash/crc64\"\n\t\"io\"\n\t\"math/bits\"\n\t\"net/http\"\n\t\"sort\"\n)\n\n// ChecksumType contains information about the checksum type.\ntype ChecksumType uint32\n\nconst (\n\n\t// ChecksumSHA256 indicates a SHA256 checksum.\n\tChecksumSHA256 ChecksumType = 1 << iota\n\t// ChecksumSHA1 indicates a SHA-1 checksum.\n\tChecksumSHA1\n\t// ChecksumCRC32 indicates a CRC32 checksum with IEEE table.\n\tChecksumCRC32\n\t// ChecksumCRC32C indicates a CRC32 checksum with Castagnoli table.\n\tChecksumCRC32C\n\t// ChecksumCRC64NVME indicates CRC64 with 0xad93d23594c93659 polynomial.\n\tChecksumCRC64NVME\n\n\t// Keep after all valid checksums\n\tchecksumLast\n\n\t// ChecksumFullObject is a modifier that can be used on CRC32 and CRC32C\n\t// to indicate full object checksums.\n\tChecksumFullObject\n\n\t// checksumMask is a mask for valid checksum types.\n\tchecksumMask = checksumLast - 1\n\n\t// ChecksumNone indicates no checksum.\n\tChecksumNone ChecksumType = 0\n\n\t// ChecksumFullObjectCRC32 indicates full object CRC32\n\tChecksumFullObjectCRC32 = ChecksumCRC32 | ChecksumFullObject\n\n\t// ChecksumFullObjectCRC32C indicates full object CRC32C\n\tChecksumFullObjectCRC32C = ChecksumCRC32C | ChecksumFullObject\n\n\tamzChecksumAlgo      = \"x-amz-checksum-algorithm\"\n\tamzChecksumCRC32     = \"x-amz-checksum-crc32\"\n\tamzChecksumCRC32C    = \"x-amz-checksum-crc32c\"\n\tamzChecksumSHA1      = \"x-amz-checksum-sha1\"\n\tamzChecksumSHA256    = \"x-amz-checksum-sha256\"\n\tamzChecksumCRC64NVME = \"x-amz-checksum-crc64nvme\"\n)\n\n// Base returns the base type, without modifiers.\nfunc (c ChecksumType) Base() ChecksumType {\n\treturn c & checksumMask\n}\n\n// Is returns if c is all of t.\nfunc (c ChecksumType) Is(t ChecksumType) bool {\n\treturn c&t == t\n}\n\n// Key returns the header key.\n// returns empty string if invalid or none.\nfunc (c ChecksumType) Key() string {\n\tswitch c & checksumMask {\n\tcase ChecksumCRC32:\n\t\treturn amzChecksumCRC32\n\tcase ChecksumCRC32C:\n\t\treturn amzChecksumCRC32C\n\tcase ChecksumSHA1:\n\t\treturn amzChecksumSHA1\n\tcase ChecksumSHA256:\n\t\treturn amzChecksumSHA256\n\tcase ChecksumCRC64NVME:\n\t\treturn amzChecksumCRC64NVME\n\t}\n\treturn \"\"\n}\n\n// CanComposite will return if the checksum type can be used for composite multipart upload on AWS.\nfunc (c ChecksumType) CanComposite() bool {\n\tswitch c & checksumMask {\n\tcase ChecksumSHA256, ChecksumSHA1, ChecksumCRC32, ChecksumCRC32C:\n\t\treturn true\n\t}\n\treturn false\n}\n\n// CanMergeCRC will return if the checksum type can be used for multipart upload on AWS.\nfunc (c ChecksumType) CanMergeCRC() bool {\n\tswitch c & checksumMask {\n\tcase ChecksumCRC32, ChecksumCRC32C, ChecksumCRC64NVME:\n\t\treturn true\n\t}\n\treturn false\n}\n\n// FullObjectRequested will return if the checksum type indicates full object checksum was requested.\nfunc (c ChecksumType) FullObjectRequested() bool {\n\tswitch c & (ChecksumFullObject | checksumMask) {\n\tcase ChecksumFullObjectCRC32C, ChecksumFullObjectCRC32, ChecksumCRC64NVME:\n\t\treturn true\n\t}\n\treturn false\n}\n\n// KeyCapitalized returns the capitalized key as used in HTTP headers.\nfunc (c ChecksumType) KeyCapitalized() string {\n\treturn http.CanonicalHeaderKey(c.Key())\n}\n\n// RawByteLen returns the size of the un-encoded checksum.\nfunc (c ChecksumType) RawByteLen() int {\n\tswitch c & checksumMask {\n\tcase ChecksumCRC32, ChecksumCRC32C:\n\t\treturn 4\n\tcase ChecksumSHA1:\n\t\treturn sha1.Size\n\tcase ChecksumSHA256:\n\t\treturn sha256.Size\n\tcase ChecksumCRC64NVME:\n\t\treturn crc64.Size\n\t}\n\treturn 0\n}\n\nconst crc64NVMEPolynomial = 0xad93d23594c93659\n\n// crc64 uses reversed polynomials.\nvar crc64Table = crc64.MakeTable(bits.Reverse64(crc64NVMEPolynomial))\n\n// Hasher returns a hasher corresponding to the checksum type.\n// Returns nil if no checksum.\nfunc (c ChecksumType) Hasher() hash.Hash {\n\tswitch c & checksumMask {\n\tcase ChecksumCRC32:\n\t\treturn crc32.NewIEEE()\n\tcase ChecksumCRC32C:\n\t\treturn crc32.New(crc32.MakeTable(crc32.Castagnoli))\n\tcase ChecksumSHA1:\n\t\treturn sha1.New()\n\tcase ChecksumSHA256:\n\t\treturn sha256.New()\n\tcase ChecksumCRC64NVME:\n\t\treturn crc64.New(crc64Table)\n\t}\n\treturn nil\n}\n\n// IsSet returns whether the type is valid and known.\nfunc (c ChecksumType) IsSet() bool {\n\treturn bits.OnesCount32(uint32(c&checksumMask)) == 1\n}\n\n// SetDefault will set the checksum if not already set.\nfunc (c *ChecksumType) SetDefault(t ChecksumType) {\n\tif !c.IsSet() {\n\t\t*c = t\n\t}\n}\n\n// EncodeToString the encoded hash value of the content provided in b.\nfunc (c ChecksumType) EncodeToString(b []byte) string {\n\tif !c.IsSet() {\n\t\treturn \"\"\n\t}\n\th := c.Hasher()\n\th.Write(b)\n\treturn base64.StdEncoding.EncodeToString(h.Sum(nil))\n}\n\n// String returns the type as a string.\n// CRC32, CRC32C, SHA1, and SHA256 for valid values.\n// Empty string for unset and \"<invalid>\" if not valid.\nfunc (c ChecksumType) String() string {\n\tswitch c & checksumMask {\n\tcase ChecksumCRC32:\n\t\treturn \"CRC32\"\n\tcase ChecksumCRC32C:\n\t\treturn \"CRC32C\"\n\tcase ChecksumSHA1:\n\t\treturn \"SHA1\"\n\tcase ChecksumSHA256:\n\t\treturn \"SHA256\"\n\tcase ChecksumNone:\n\t\treturn \"\"\n\tcase ChecksumCRC64NVME:\n\t\treturn \"CRC64NVME\"\n\t}\n\treturn \"<invalid>\"\n}\n\n// ChecksumReader reads all of r and returns a checksum of type c.\n// Returns any error that may have occurred while reading.\nfunc (c ChecksumType) ChecksumReader(r io.Reader) (Checksum, error) {\n\th := c.Hasher()\n\tif h == nil {\n\t\treturn Checksum{}, nil\n\t}\n\t_, err := io.Copy(h, r)\n\tif err != nil {\n\t\treturn Checksum{}, err\n\t}\n\treturn NewChecksum(c, h.Sum(nil)), nil\n}\n\n// ChecksumBytes returns a checksum of the content b with type c.\nfunc (c ChecksumType) ChecksumBytes(b []byte) Checksum {\n\th := c.Hasher()\n\tif h == nil {\n\t\treturn Checksum{}\n\t}\n\tn, err := h.Write(b)\n\tif err != nil || n != len(b) {\n\t\t// Shouldn't happen with these checksummers.\n\t\treturn Checksum{}\n\t}\n\treturn NewChecksum(c, h.Sum(nil))\n}\n\n// Checksum is a type and encoded value.\ntype Checksum struct {\n\tType ChecksumType\n\tr    []byte\n}\n\n// NewChecksum sets the checksum to the value of b,\n// which is the raw hash output.\n// If the length of c does not match t.RawByteLen,\n// a checksum with ChecksumNone is returned.\nfunc NewChecksum(t ChecksumType, b []byte) Checksum {\n\tif t.IsSet() && len(b) == t.RawByteLen() {\n\t\treturn Checksum{Type: t, r: b}\n\t}\n\treturn Checksum{}\n}\n\n// NewChecksumString sets the checksum to the value of s,\n// which is the base 64 encoded raw hash output.\n// If the length of c does not match t.RawByteLen, it is not added.\nfunc NewChecksumString(t ChecksumType, s string) Checksum {\n\tb, _ := base64.StdEncoding.DecodeString(s)\n\tif t.IsSet() && len(b) == t.RawByteLen() {\n\t\treturn Checksum{Type: t, r: b}\n\t}\n\treturn Checksum{}\n}\n\n// IsSet returns whether the checksum is valid and known.\nfunc (c Checksum) IsSet() bool {\n\treturn c.Type.IsSet() && len(c.r) == c.Type.RawByteLen()\n}\n\n// Encoded returns the encoded value.\n// Returns the empty string if not set or valid.\nfunc (c Checksum) Encoded() string {\n\tif !c.IsSet() {\n\t\treturn \"\"\n\t}\n\treturn base64.StdEncoding.EncodeToString(c.r)\n}\n\n// Raw returns the raw checksum value if set.\nfunc (c Checksum) Raw() []byte {\n\tif !c.IsSet() {\n\t\treturn nil\n\t}\n\treturn c.r\n}\n\n// CompositeChecksum returns the composite checksum of all provided parts.\nfunc (c ChecksumType) CompositeChecksum(p []ObjectPart) (*Checksum, error) {\n\tif !c.CanComposite() {\n\t\treturn nil, errors.New(\"cannot do composite checksum\")\n\t}\n\tsort.Slice(p, func(i, j int) bool {\n\t\treturn p[i].PartNumber < p[j].PartNumber\n\t})\n\tc = c.Base()\n\tcrcBytes := make([]byte, 0, len(p)*c.RawByteLen())\n\tfor _, part := range p {\n\t\tpCrc, err := part.ChecksumRaw(c)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcrcBytes = append(crcBytes, pCrc...)\n\t}\n\th := c.Hasher()\n\th.Write(crcBytes)\n\treturn &Checksum{Type: c, r: h.Sum(nil)}, nil\n}\n\n// FullObjectChecksum will return the full object checksum from provided parts.\nfunc (c ChecksumType) FullObjectChecksum(p []ObjectPart) (*Checksum, error) {\n\tif !c.CanMergeCRC() {\n\t\treturn nil, errors.New(\"cannot merge this checksum type\")\n\t}\n\tc = c.Base()\n\tsort.Slice(p, func(i, j int) bool {\n\t\treturn p[i].PartNumber < p[j].PartNumber\n\t})\n\n\tswitch len(p) {\n\tcase 0:\n\t\treturn nil, errors.New(\"no parts given\")\n\tcase 1:\n\t\tcheck, err := p[0].ChecksumRaw(c)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &Checksum{\n\t\t\tType: c,\n\t\t\tr:    check,\n\t\t}, nil\n\t}\n\tvar merged uint32\n\tvar merged64 uint64\n\tfirst, err := p[0].ChecksumRaw(c)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsz := p[0].Size\n\tswitch c {\n\tcase ChecksumCRC32, ChecksumCRC32C:\n\t\tmerged = binary.BigEndian.Uint32(first)\n\tcase ChecksumCRC64NVME:\n\t\tmerged64 = binary.BigEndian.Uint64(first)\n\t}\n\n\tpoly32 := uint32(crc32.IEEE)\n\tif c.Is(ChecksumCRC32C) {\n\t\tpoly32 = crc32.Castagnoli\n\t}\n\tfor _, part := range p[1:] {\n\t\tif part.Size == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tsz += part.Size\n\t\tpCrc, err := part.ChecksumRaw(c)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tswitch c {\n\t\tcase ChecksumCRC32, ChecksumCRC32C:\n\t\t\tmerged = crc32Combine(poly32, merged, binary.BigEndian.Uint32(pCrc), part.Size)\n\t\tcase ChecksumCRC64NVME:\n\t\t\tmerged64 = crc64Combine(bits.Reverse64(crc64NVMEPolynomial), merged64, binary.BigEndian.Uint64(pCrc), part.Size)\n\t\t}\n\t}\n\tvar tmp [8]byte\n\tswitch c {\n\tcase ChecksumCRC32, ChecksumCRC32C:\n\t\tbinary.BigEndian.PutUint32(tmp[:], merged)\n\t\treturn &Checksum{\n\t\t\tType: c,\n\t\t\tr:    tmp[:4],\n\t\t}, nil\n\tcase ChecksumCRC64NVME:\n\t\tbinary.BigEndian.PutUint64(tmp[:], merged64)\n\t\treturn &Checksum{\n\t\t\tType: c,\n\t\t\tr:    tmp[:8],\n\t\t}, nil\n\tdefault:\n\t\treturn nil, errors.New(\"unknown checksum type\")\n\t}\n}\n\nfunc addAutoChecksumHeaders(opts *PutObjectOptions) {\n\tif opts.UserMetadata == nil {\n\t\topts.UserMetadata = make(map[string]string, 1)\n\t}\n\topts.UserMetadata[\"X-Amz-Checksum-Algorithm\"] = opts.AutoChecksum.String()\n\tif opts.AutoChecksum.FullObjectRequested() {\n\t\topts.UserMetadata[\"X-Amz-Checksum-Type\"] = \"FULL_OBJECT\"\n\t}\n}\n\nfunc applyAutoChecksum(opts *PutObjectOptions, allParts []ObjectPart) {\n\tif !opts.AutoChecksum.IsSet() {\n\t\treturn\n\t}\n\tif opts.AutoChecksum.CanComposite() && !opts.AutoChecksum.Is(ChecksumFullObject) {\n\t\t// Add composite hash of hashes.\n\t\tcrc, err := opts.AutoChecksum.CompositeChecksum(allParts)\n\t\tif err == nil {\n\t\t\topts.UserMetadata = map[string]string{opts.AutoChecksum.Key(): crc.Encoded()}\n\t\t}\n\t} else if opts.AutoChecksum.CanMergeCRC() {\n\t\tcrc, err := opts.AutoChecksum.FullObjectChecksum(allParts)\n\t\tif err == nil {\n\t\t\topts.UserMetadata = map[string]string{opts.AutoChecksum.KeyCapitalized(): crc.Encoded(), \"X-Amz-Checksum-Type\": \"FULL_OBJECT\"}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "code_of_conduct.md",
          "type": "blob",
          "size": 3.486328125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior, in compliance with the\nlicensing terms applying to the Project developments.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful. However, these actions shall respect the\nlicensing terms of the Project Developments that will always supersede such\nCode of Conduct.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at dev@min.io. The project team\nwill review and investigate all complaints, and will respond in a way that it deems\nappropriate to the circumstances. The project team is obligated to maintain\nconfidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at [http://contributor-covenant.org/version/1/4][version]\n\nThis version includes a clarification to ensure that the code of conduct is in\ncompliance with the free software licensing terms of the project.\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "constants.go",
          "type": "blob",
          "size": 4.8623046875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\n// Multipart upload defaults.\n\n// absMinPartSize - absolute minimum part size (5 MiB) below which\n// a part in a multipart upload may not be uploaded.\nconst absMinPartSize = 1024 * 1024 * 5\n\n// minPartSize - minimum part size 16MiB per object after which\n// putObject behaves internally as multipart.\nconst minPartSize = 1024 * 1024 * 16\n\n// maxPartsCount - maximum number of parts for a single multipart session.\nconst maxPartsCount = 10000\n\n// maxPartSize - maximum part size 5GiB for a single multipart upload\n// operation.\nconst maxPartSize = 1024 * 1024 * 1024 * 5\n\n// maxSinglePutObjectSize - maximum size 5GiB of object per PUT\n// operation.\nconst maxSinglePutObjectSize = 1024 * 1024 * 1024 * 5\n\n// maxMultipartPutObjectSize - maximum size 5TiB of object for\n// Multipart operation.\nconst maxMultipartPutObjectSize = 1024 * 1024 * 1024 * 1024 * 5\n\n// unsignedPayload - value to be set to X-Amz-Content-Sha256 header when\n// we don't want to sign the request payload\nconst unsignedPayload = \"UNSIGNED-PAYLOAD\"\n\n// unsignedPayloadTrailer value to be set to X-Amz-Content-Sha256 header when\n// we don't want to sign the request payload, but have a trailer.\nconst unsignedPayloadTrailer = \"STREAMING-UNSIGNED-PAYLOAD-TRAILER\"\n\n// Total number of parallel workers used for multipart operation.\nconst totalWorkers = 4\n\n// Signature related constants.\nconst (\n\tsignV4Algorithm   = \"AWS4-HMAC-SHA256\"\n\tiso8601DateFormat = \"20060102T150405Z\"\n)\n\nconst (\n\t// GetObjectAttributesTags are tags used to defined\n\t// return values for the GetObjectAttributes API\n\tGetObjectAttributesTags = \"ETag,Checksum,StorageClass,ObjectSize,ObjectParts\"\n\t// GetObjectAttributesMaxParts defined the default maximum\n\t// number of parts returned by GetObjectAttributes\n\tGetObjectAttributesMaxParts = 1000\n)\n\nconst (\n\t// Response Headers\n\n\t// ETag is a common response header\n\tETag = \"ETag\"\n\n\t// Storage class header.\n\tamzStorageClass = \"X-Amz-Storage-Class\"\n\n\t// Website redirect location header\n\tamzWebsiteRedirectLocation = \"X-Amz-Website-Redirect-Location\"\n\n\t// GetObjectAttributes headers\n\tamzPartNumberMarker    = \"X-Amz-Part-Number-Marker\"\n\tamzExpectedBucketOnwer = \"X-Amz-Expected-Bucket-Owner\"\n\tamzMaxParts            = \"X-Amz-Max-Parts\"\n\tamzObjectAttributes    = \"X-Amz-Object-Attributes\"\n\n\t// Object Tagging headers\n\tamzTaggingHeader          = \"X-Amz-Tagging\"\n\tamzTaggingHeaderDirective = \"X-Amz-Tagging-Directive\"\n\n\tamzVersionID         = \"X-Amz-Version-Id\"\n\tamzTaggingCount      = \"X-Amz-Tagging-Count\"\n\tamzExpiration        = \"X-Amz-Expiration\"\n\tamzRestore           = \"X-Amz-Restore\"\n\tamzReplicationStatus = \"X-Amz-Replication-Status\"\n\tamzDeleteMarker      = \"X-Amz-Delete-Marker\"\n\n\t// Object legal hold header\n\tamzLegalHoldHeader = \"X-Amz-Object-Lock-Legal-Hold\"\n\n\t// Object retention header\n\tamzLockMode         = \"X-Amz-Object-Lock-Mode\"\n\tamzLockRetainUntil  = \"X-Amz-Object-Lock-Retain-Until-Date\"\n\tamzBypassGovernance = \"X-Amz-Bypass-Governance-Retention\"\n\n\t// Replication status\n\tamzBucketReplicationStatus = \"X-Amz-Replication-Status\"\n\t// Minio specific Replication/lifecycle transition extension\n\tminIOBucketSourceMTime = \"X-Minio-Source-Mtime\"\n\n\tminIOBucketSourceETag              = \"X-Minio-Source-Etag\"\n\tminIOBucketReplicationDeleteMarker = \"X-Minio-Source-DeleteMarker\"\n\tminIOBucketReplicationProxyRequest = \"X-Minio-Source-Proxy-Request\"\n\tminIOBucketReplicationRequest      = \"X-Minio-Source-Replication-Request\"\n\tminIOBucketReplicationCheck        = \"X-Minio-Source-Replication-Check\"\n\n\t// Header indicates last tag update time on source\n\tminIOBucketReplicationTaggingTimestamp = \"X-Minio-Source-Replication-Tagging-Timestamp\"\n\t// Header indicates last retention update time on source\n\tminIOBucketReplicationObjectRetentionTimestamp = \"X-Minio-Source-Replication-Retention-Timestamp\"\n\t// Header indicates last legalhold update time on source\n\tminIOBucketReplicationObjectLegalHoldTimestamp = \"X-Minio-Source-Replication-LegalHold-Timestamp\"\n\tminIOForceDelete                               = \"x-minio-force-delete\"\n\t// Header indicates delete marker replication request can be sent by source now.\n\tminioTgtReplicationReady = \"X-Minio-Replication-Ready\"\n\t// Header asks if delete marker replication request can be sent by source now.\n\tisMinioTgtReplicationReady = \"X-Minio-Check-Replication-Ready\"\n)\n"
        },
        {
          "name": "core.go",
          "type": "blob",
          "size": 6.388671875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"net/http\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\n// Core - Inherits Client and adds new methods to expose the low level S3 APIs.\ntype Core struct {\n\t*Client\n}\n\n// NewCore - Returns new initialized a Core client, this CoreClient should be\n// only used under special conditions such as need to access lower primitives\n// and being able to use them to write your own wrappers.\nfunc NewCore(endpoint string, opts *Options) (*Core, error) {\n\tvar s3Client Core\n\tclient, err := New(endpoint, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts3Client.Client = client\n\treturn &s3Client, nil\n}\n\n// ListObjects - List all the objects at a prefix, optionally with marker and delimiter\n// you can further filter the results.\nfunc (c Core) ListObjects(bucket, prefix, marker, delimiter string, maxKeys int) (result ListBucketResult, err error) {\n\treturn c.listObjectsQuery(context.Background(), bucket, prefix, marker, delimiter, maxKeys, nil)\n}\n\n// ListObjectsV2 - Lists all the objects at a prefix, similar to ListObjects() but uses\n// continuationToken instead of marker to support iteration over the results.\nfunc (c Core) ListObjectsV2(bucketName, objectPrefix, startAfter, continuationToken, delimiter string, maxkeys int) (ListBucketV2Result, error) {\n\treturn c.listObjectsV2Query(context.Background(), bucketName, objectPrefix, continuationToken, true, false, delimiter, startAfter, maxkeys, nil)\n}\n\n// CopyObject - copies an object from source object to destination object on server side.\nfunc (c Core) CopyObject(ctx context.Context, sourceBucket, sourceObject, destBucket, destObject string, metadata map[string]string, srcOpts CopySrcOptions, dstOpts PutObjectOptions) (ObjectInfo, error) {\n\treturn c.copyObjectDo(ctx, sourceBucket, sourceObject, destBucket, destObject, metadata, srcOpts, dstOpts)\n}\n\n// CopyObjectPart - creates a part in a multipart upload by copying (a\n// part of) an existing object.\nfunc (c Core) CopyObjectPart(ctx context.Context, srcBucket, srcObject, destBucket, destObject, uploadID string,\n\tpartID int, startOffset, length int64, metadata map[string]string,\n) (p CompletePart, err error) {\n\treturn c.copyObjectPartDo(ctx, srcBucket, srcObject, destBucket, destObject, uploadID,\n\t\tpartID, startOffset, length, metadata)\n}\n\n// PutObject - Upload object. Uploads using single PUT call.\nfunc (c Core) PutObject(ctx context.Context, bucket, object string, data io.Reader, size int64, md5Base64, sha256Hex string, opts PutObjectOptions) (UploadInfo, error) {\n\thookReader := newHook(data, opts.Progress)\n\treturn c.putObjectDo(ctx, bucket, object, hookReader, md5Base64, sha256Hex, size, opts)\n}\n\n// NewMultipartUpload - Initiates new multipart upload and returns the new uploadID.\nfunc (c Core) NewMultipartUpload(ctx context.Context, bucket, object string, opts PutObjectOptions) (uploadID string, err error) {\n\tresult, err := c.initiateMultipartUpload(ctx, bucket, object, opts)\n\treturn result.UploadID, err\n}\n\n// ListMultipartUploads - List incomplete uploads.\nfunc (c Core) ListMultipartUploads(ctx context.Context, bucket, prefix, keyMarker, uploadIDMarker, delimiter string, maxUploads int) (result ListMultipartUploadsResult, err error) {\n\treturn c.listMultipartUploadsQuery(ctx, bucket, keyMarker, uploadIDMarker, prefix, delimiter, maxUploads)\n}\n\n// PutObjectPartOptions contains options for PutObjectPart API\ntype PutObjectPartOptions struct {\n\tMd5Base64, Sha256Hex  string\n\tSSE                   encrypt.ServerSide\n\tCustomHeader, Trailer http.Header\n\tDisableContentSha256  bool\n}\n\n// PutObjectPart - Upload an object part.\nfunc (c Core) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int,\n\tdata io.Reader, size int64, opts PutObjectPartOptions,\n) (ObjectPart, error) {\n\tp := uploadPartParams{\n\t\tbucketName:   bucket,\n\t\tobjectName:   object,\n\t\tuploadID:     uploadID,\n\t\treader:       data,\n\t\tpartNumber:   partID,\n\t\tmd5Base64:    opts.Md5Base64,\n\t\tsha256Hex:    opts.Sha256Hex,\n\t\tsize:         size,\n\t\tsse:          opts.SSE,\n\t\tstreamSha256: !opts.DisableContentSha256,\n\t\tcustomHeader: opts.CustomHeader,\n\t\ttrailer:      opts.Trailer,\n\t}\n\treturn c.uploadPart(ctx, p)\n}\n\n// ListObjectParts - List uploaded parts of an incomplete upload.x\nfunc (c Core) ListObjectParts(ctx context.Context, bucket, object, uploadID string, partNumberMarker, maxParts int) (result ListObjectPartsResult, err error) {\n\treturn c.listObjectPartsQuery(ctx, bucket, object, uploadID, partNumberMarker, maxParts)\n}\n\n// CompleteMultipartUpload - Concatenate uploaded parts and commit to an object.\nfunc (c Core) CompleteMultipartUpload(ctx context.Context, bucket, object, uploadID string, parts []CompletePart, opts PutObjectOptions) (UploadInfo, error) {\n\tres, err := c.completeMultipartUpload(ctx, bucket, object, uploadID, completeMultipartUpload{\n\t\tParts: parts,\n\t}, opts)\n\treturn res, err\n}\n\n// AbortMultipartUpload - Abort an incomplete upload.\nfunc (c Core) AbortMultipartUpload(ctx context.Context, bucket, object, uploadID string) error {\n\treturn c.abortMultipartUpload(ctx, bucket, object, uploadID)\n}\n\n// GetBucketPolicy - fetches bucket access policy for a given bucket.\nfunc (c Core) GetBucketPolicy(ctx context.Context, bucket string) (string, error) {\n\treturn c.getBucketPolicy(ctx, bucket)\n}\n\n// PutBucketPolicy - applies a new bucket access policy for a given bucket.\nfunc (c Core) PutBucketPolicy(ctx context.Context, bucket, bucketPolicy string) error {\n\treturn c.putBucketPolicy(ctx, bucket, bucketPolicy)\n}\n\n// GetObject is a lower level API implemented to support reading\n// partial objects and also downloading objects with special conditions\n// matching etag, modtime etc.\nfunc (c Core) GetObject(ctx context.Context, bucketName, objectName string, opts GetObjectOptions) (io.ReadCloser, ObjectInfo, http.Header, error) {\n\treturn c.getObject(ctx, bucketName, objectName, opts)\n}\n"
        },
        {
          "name": "core_test.go",
          "type": "blob",
          "size": 25.5732421875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\nconst (\n\tserverEndpoint = \"SERVER_ENDPOINT\"\n\taccessKey      = \"ACCESS_KEY\"\n\tsecretKey      = \"SECRET_KEY\"\n\tenableSecurity = \"ENABLE_HTTPS\"\n)\n\n// Tests for Core GetObject() function.\nfunc TestGetObjectCore(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for the short runs\")\n\t}\n\n\t// Instantiate new minio core client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable tracing, write to stderr.\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\t// Generate data more than 32K\n\tbuf := bytes.Repeat([]byte(\"3\"), rand.Intn(1<<20)+32*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t_, err = c.Client.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), PutObjectOptions{\n\t\tContentType: \"binary/octet-stream\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tst, err := c.Client.StatObject(context.Background(), bucketName, objectName, StatObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Stat error:\", err, bucketName, objectName)\n\t}\n\tif st.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size)\n\t}\n\n\toffset := int64(2048)\n\n\t// read directly\n\tbuf1 := make([]byte, 512)\n\tbuf2 := make([]byte, 512)\n\tbuf3 := make([]byte, st.Size)\n\tbuf4 := make([]byte, 1)\n\n\topts := GetObjectOptions{}\n\topts.SetRange(offset, offset+int64(len(buf1))-1)\n\treader, objectInfo, _, err := c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tm, err := readFull(reader, buf1)\n\treader.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif objectInfo.Size != int64(m) {\n\t\tt.Fatalf(\"Error: GetObject read shorter bytes before reaching EOF, want %v, got %v\\n\", objectInfo.Size, m)\n\t}\n\tif !bytes.Equal(buf1, buf[offset:offset+512]) {\n\t\tt.Fatal(\"Error: Incorrect read between two GetObject from same offset.\")\n\t}\n\toffset += 512\n\n\topts.SetRange(offset, offset+int64(len(buf2))-1)\n\treader, objectInfo, _, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm, err = readFull(reader, buf2)\n\treader.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif objectInfo.Size != int64(m) {\n\t\tt.Fatalf(\"Error: GetObject read shorter bytes before reaching EOF, want %v, got %v\\n\", objectInfo.Size, m)\n\t}\n\tif !bytes.Equal(buf2, buf[offset:offset+512]) {\n\t\tt.Fatal(\"Error: Incorrect read between two GetObject from same offset.\")\n\t}\n\n\topts.SetRange(0, int64(len(buf3)))\n\treader, objectInfo, _, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm, err = readFull(reader, buf3)\n\tif err != nil {\n\t\treader.Close()\n\t\tt.Fatal(err)\n\t}\n\treader.Close()\n\n\tif objectInfo.Size != int64(m) {\n\t\tt.Fatalf(\"Error: GetObject read shorter bytes before reaching EOF, want %v, got %v\\n\", objectInfo.Size, m)\n\t}\n\tif !bytes.Equal(buf3, buf) {\n\t\tt.Fatal(\"Error: Incorrect data read in GetObject, than what was previously upoaded.\")\n\t}\n\n\topts = GetObjectOptions{}\n\topts.SetMatchETag(\"etag\")\n\t_, _, _, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err == nil {\n\t\tt.Fatal(\"Unexpected GetObject should fail with mismatching etags\")\n\t}\n\tif errResp := ToErrorResponse(err); errResp.Code != \"PreconditionFailed\" {\n\t\tt.Fatalf(\"Expected \\\"PreconditionFailed\\\" as code, got %s instead\", errResp.Code)\n\t}\n\n\topts = GetObjectOptions{}\n\topts.SetMatchETagExcept(\"etag\")\n\treader, objectInfo, _, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm, err = readFull(reader, buf3)\n\treader.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif objectInfo.Size != int64(m) {\n\t\tt.Fatalf(\"Error: GetObject read shorter bytes before reaching EOF, want %v, got %v\\n\", objectInfo.Size, m)\n\t}\n\tif !bytes.Equal(buf3, buf) {\n\t\tt.Fatal(\"Error: Incorrect data read in GetObject, than what was previously upoaded.\")\n\t}\n\n\topts = GetObjectOptions{}\n\topts.SetRange(0, 0)\n\treader, objectInfo, _, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm, err = readFull(reader, buf4)\n\treader.Close()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif objectInfo.Size != int64(m) {\n\t\tt.Fatalf(\"Error: GetObject read shorter bytes before reaching EOF, want %v, got %v\\n\", objectInfo.Size, m)\n\t}\n\n\topts = GetObjectOptions{}\n\topts.SetRange(offset, offset+int64(len(buf2))-1)\n\tcontentLength := len(buf2)\n\tvar header http.Header\n\t_, _, header, err = c.GetObject(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcontentLengthValue, err := strconv.Atoi(header.Get(\"Content-Length\"))\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\tif contentLength != contentLengthValue {\n\t\tt.Fatalf(\"Error: Content Length in response header %v, not equal to set content length %v\\n\", contentLengthValue, contentLength)\n\t}\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n}\n\n// Tests GetObject to return Content-Encoding properly set\n// and overrides any auto decoding.\nfunc TestGetObjectContentEncoding(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for the short runs\")\n\t}\n\n\t// Instantiate new minio core client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable tracing, write to stderr.\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\t// Generate data more than 32K\n\tbuf := bytes.Repeat([]byte(\"3\"), rand.Intn(1<<20)+32*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t_, err = c.Client.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), PutObjectOptions{\n\t\tContentEncoding: \"gzip\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\trwc, objInfo, _, err := c.GetObject(context.Background(), bucketName, objectName, GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"Error: %v\", err)\n\t}\n\trwc.Close()\n\tif objInfo.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Unexpected size of the object %v, expected %v\", objInfo.Size, len(buf))\n\t}\n\tvalue, ok := objInfo.Metadata[\"Content-Encoding\"]\n\tif !ok {\n\t\tt.Fatalf(\"Expected Content-Encoding metadata to be set.\")\n\t}\n\tif value[0] != \"gzip\" {\n\t\tt.Fatalf(\"Unexpected content-encoding found, want gzip, got %v\", value)\n\t}\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n}\n\n// Tests get bucket policy core API.\nfunc TestGetBucketPolicy(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for short runs\")\n\t}\n\n\t// Instantiate new minio client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable to debug\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\t// Verify if bucket exits and you have access.\n\tvar exists bool\n\texists, err = c.BucketExists(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\tif !exists {\n\t\tt.Fatal(\"Error: could not find \", bucketName)\n\t}\n\n\t// Asserting the default bucket policy.\n\tbucketPolicy, err := c.GetBucketPolicy(context.Background(), bucketName)\n\tif err != nil {\n\t\terrResp := ToErrorResponse(err)\n\t\tif errResp.Code != \"NoSuchBucketPolicy\" {\n\t\t\tt.Error(\"Error:\", err, bucketName)\n\t\t}\n\t}\n\tif bucketPolicy != \"\" {\n\t\tt.Errorf(\"Bucket policy expected %#v, got %#v\", \"\", bucketPolicy)\n\t}\n\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n}\n\n// Tests Core CopyObject API implementation.\nfunc TestCoreCopyObject(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for short runs\")\n\t}\n\n\t// Instantiate new minio client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable tracing, write to stderr.\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\tbuf := bytes.Repeat([]byte(\"a\"), 32*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tputopts := PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", putopts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, StatObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size)\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\n\tcuploadInfo, err := c.CopyObject(context.Background(), bucketName, objectName, destBucketName, destObjectName, map[string]string{\n\t\t\"X-Amz-Metadata-Directive\": \"REPLACE\",\n\t\t\"Content-Type\":             \"application/javascript\",\n\t}, CopySrcOptions{}, PutObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName, destBucketName, destObjectName)\n\t}\n\tif cuploadInfo.ETag != uploadInfo.ETag {\n\t\tt.Fatalf(\"Error: expected etag to be same as source object %s, but found different etag %s\", uploadInfo.ETag, cuploadInfo.ETag)\n\t}\n\n\t// Attempt to read from destBucketName and object name.\n\tr, err := c.Client.GetObject(context.Background(), destBucketName, destObjectName, GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tst, err = r.Stat()\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Error: number of bytes in stat does not match, want %v, got %v\\n\",\n\t\t\tlen(buf), st.Size)\n\t}\n\n\tif st.ContentType != \"application/javascript\" {\n\t\tt.Fatalf(\"Error: Content types don't match, expected: application/javascript, found: %+v\\n\", st.ContentType)\n\t}\n\n\tif st.ETag != uploadInfo.ETag {\n\t\tt.Fatalf(\"Error: expected etag to be same as source object %s, but found different etag :%s\", uploadInfo.ETag, st.ETag)\n\t}\n\n\tif err := r.Close(); err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\tif err := r.Close(); err == nil {\n\t\tt.Fatal(\"Error: object is already closed, should return error\")\n\t}\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\terr = c.RemoveObject(context.Background(), destBucketName, destObjectName, RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation\nfunc TestCoreCopyObjectPart(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for short runs\")\n\t}\n\n\t// Instantiate new minio client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable tracing, write to stderr.\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\tmetadata := map[string]string{\n\t\t\"Content-Type\": \"binary/octet-stream\",\n\t}\n\tputopts := PutObjectOptions{\n\t\tUserMetadata: metadata,\n\t}\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", putopts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, StatObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size)\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, PutObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, nil)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, nil)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, nil)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []CompletePart{fstPart, sndPart, lstPart}, PutObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, StatObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tt.Fatal(\"Destination object has incorrect size!\")\n\t}\n\n\t// Now we read the data back\n\tgetOpts := GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tt.Fatal(\"Got unexpected data in first 5MB\")\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, destBucketName, destObjectName)\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tt.Fatal(\"Got unexpected data in second 5MB\")\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tt.Fatal(\"Got unexpected data in last byte of copied object!\")\n\t}\n\n\tif err := c.RemoveObject(context.Background(), destBucketName, destObjectName, RemoveObjectOptions{}); err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\tif err := c.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{}); err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\tif err := c.RemoveBucket(context.Background(), bucketName); err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core PutObject.\nfunc TestCorePutObject(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for short runs\")\n\t}\n\n\t// Instantiate new minio client object.\n\tc, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\t// Enable tracing, write to stderr.\n\t// c.TraceOn(os.Stderr)\n\n\t// Set user agent.\n\tc.SetAppInfo(\"MinIO-go-FunctionalTest\", \"0.1.0\")\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\tbuf := bytes.Repeat([]byte(\"a\"), 32*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t// Object content type\n\tobjectContentType := \"binary/octet-stream\"\n\tmetadata := make(map[string]string)\n\tmetadata[\"Content-Type\"] = objectContentType\n\tputopts := PutObjectOptions{\n\t\tUserMetadata: metadata,\n\t}\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"1B2M2Y8AsgTpgAmY7PhCfg==\", \"\", putopts)\n\tif err == nil {\n\t\tt.Fatal(\"Error expected: error, got: nil(success)\")\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", putopts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\t// Read the data back\n\tr, err := c.Client.GetObject(context.Background(), bucketName, objectName, GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tt.Fatalf(\"Error: number of bytes in stat does not match, want %v, got %v\\n\",\n\t\t\tlen(buf), st.Size)\n\t}\n\n\tif st.ContentType != objectContentType {\n\t\tt.Fatalf(\"Error: Content types don't match, expected: %+v, found: %+v\\n\", objectContentType, st.ContentType)\n\t}\n\n\tif err := r.Close(); err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\tif err := r.Close(); err == nil {\n\t\tt.Fatal(\"Error: object is already closed, should return error\")\n\t}\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n}\n\nfunc TestCoreGetObjectMetadata(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for the short runs\")\n\t}\n\n\tcore, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = core.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\n\tmetadata := map[string]string{\n\t\t\"X-Amz-Meta-Key-1\": \"Val-1\",\n\t}\n\tputopts := PutObjectOptions{\n\t\tUserMetadata: metadata,\n\t}\n\n\t_, err = core.PutObject(context.Background(), bucketName, \"my-objectname\",\n\t\tbytes.NewReader([]byte(\"hello\")), 5, \"\", \"\", putopts)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treader, objInfo, _, err := core.GetObject(context.Background(), bucketName, \"my-objectname\", GetObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\treader.Close()\n\n\tif objInfo.Metadata.Get(\"X-Amz-Meta-Key-1\") != \"Val-1\" {\n\t\tt.Fatal(\"Expected metadata to be available but wasn't\")\n\t}\n\n\terr = core.RemoveObject(context.Background(), bucketName, \"my-objectname\", RemoveObjectOptions{})\n\tif err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\terr = core.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n}\n\nfunc TestCoreMultipartUpload(t *testing.T) {\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tt.Skip(\"SERVER_ENDPOINT not set\")\n\t}\n\tif testing.Short() {\n\t\tt.Skip(\"skipping functional tests for the short runs\")\n\t}\n\n\t// Instantiate new minio client object.\n\tcore, err := NewCore(\n\t\tos.Getenv(serverEndpoint),\n\t\t&Options{\n\t\t\tCreds:  credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\"),\n\t\t\tSecure: mustParseBool(os.Getenv(enableSecurity)),\n\t\t})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\t// Make a new bucket.\n\terr = core.MakeBucket(context.Background(), bucketName, MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName)\n\t}\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tobjectContentType := \"binary/octet-stream\"\n\tmetadata := make(map[string]string)\n\tmetadata[\"Content-Type\"] = objectContentType\n\tputopts := PutObjectOptions{\n\t\tUserMetadata: metadata,\n\t}\n\tuploadID, err := core.NewMultipartUpload(context.Background(), bucketName, objectName, putopts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t}\n\tbuf := bytes.Repeat([]byte(\"a\"), 32*1024*1024)\n\tr := bytes.NewReader(buf)\n\tpartBuf := make([]byte, 100*1024*1024)\n\tparts := make([]CompletePart, 0, 5)\n\tpartID := 0\n\tfor {\n\t\tn, err := r.Read(partBuf)\n\t\tif err != nil && err != io.EOF {\n\t\t\tt.Fatal(\"Error:\", err)\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif n > 0 {\n\t\t\tpartID++\n\t\t\tdata := bytes.NewReader(partBuf[:n])\n\t\t\tdataLen := int64(len(partBuf[:n]))\n\t\t\tobjectPart, err := core.PutObjectPart(context.Background(), bucketName, objectName, uploadID, partID,\n\t\t\t\tdata, dataLen,\n\t\t\t\tPutObjectPartOptions{\n\t\t\t\t\tMd5Base64:    \"\",\n\t\t\t\t\tSha256Hex:    \"\",\n\t\t\t\t\tSSE:          encrypt.NewSSE(),\n\t\t\t\t\tCustomHeader: nil,\n\t\t\t\t\tTrailer:      nil,\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(\"Error:\", err, bucketName, objectName)\n\t\t\t}\n\t\t\tparts = append(parts, CompletePart{\n\t\t\t\tPartNumber: partID,\n\t\t\t\tETag:       objectPart.ETag,\n\t\t\t})\n\t\t}\n\t}\n\tobjectParts, err := core.listObjectParts(context.Background(), bucketName, objectName, uploadID)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\tif len(objectParts) != len(parts) {\n\t\tt.Fatal(\"Error\", len(objectParts), len(parts))\n\t}\n\t_, err = core.CompleteMultipartUpload(context.Background(), bucketName, objectName, uploadID, parts, putopts)\n\tif err != nil {\n\t\tt.Fatal(\"Error:\", err)\n\t}\n\n\tif err := core.RemoveObject(context.Background(), bucketName, objectName, RemoveObjectOptions{}); err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n\n\tif err := core.RemoveBucket(context.Background(), bucketName); err != nil {\n\t\tt.Fatal(\"Error: \", err)\n\t}\n}\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "functional_tests.go",
          "type": "blob",
          "size": 444.64453125,
          "content": "//go:build mint\n// +build mint\n\n/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2020 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage main\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math/rand\"\n\t\"mime/multipart\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/dustin/go-humanize\"\n\t\"github.com/google/uuid\"\n\n\t\"github.com/minio/minio-go/v7\"\n\t\"github.com/minio/minio-go/v7/pkg/cors\"\n\t\"github.com/minio/minio-go/v7/pkg/credentials\"\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/notification\"\n\t\"github.com/minio/minio-go/v7/pkg/tags\"\n)\n\nconst letterBytes = \"abcdefghijklmnopqrstuvwxyz01234569\"\nconst (\n\tletterIdxBits = 6                    // 6 bits to represent a letter index\n\tletterIdxMask = 1<<letterIdxBits - 1 // All 1-bits, as many as letterIdxBits\n\tletterIdxMax  = 63 / letterIdxBits   // # of letter indices fitting in 63 bits\n)\n\nconst (\n\tserverEndpoint     = \"SERVER_ENDPOINT\"\n\taccessKey          = \"ACCESS_KEY\"\n\tsecretKey          = \"SECRET_KEY\"\n\tenableHTTPS        = \"ENABLE_HTTPS\"\n\tenableKMS          = \"ENABLE_KMS\"\n\tappVersion         = \"0.1.0\"\n\tskipCERTValidation = \"SKIP_CERT_VALIDATION\"\n)\n\nfunc createHTTPTransport() (transport *http.Transport) {\n\tvar err error\n\ttransport, err = minio.DefaultTransport(mustParseBool(os.Getenv(enableHTTPS)))\n\tif err != nil {\n\t\tlogError(\"http-transport\", getFuncName(), nil, time.Now(), \"\", \"could not create http transport\", err)\n\t\treturn nil\n\t}\n\n\tif mustParseBool(os.Getenv(enableHTTPS)) && mustParseBool(os.Getenv(skipCERTValidation)) {\n\t\ttransport.TLSClientConfig.InsecureSkipVerify = true\n\t}\n\n\treturn\n}\n\nvar readFull = func(r io.Reader, buf []byte) (n int, err error) {\n\t// ReadFull reads exactly len(buf) bytes from r into buf.\n\t// It returns the number of bytes copied and an error if\n\t// fewer bytes were read. The error is EOF only if no bytes\n\t// were read. If an EOF happens after reading some but not\n\t// all the bytes, ReadFull returns ErrUnexpectedEOF.\n\t// On return, n == len(buf) if and only if err == nil.\n\t// If r returns an error having read at least len(buf) bytes,\n\t// the error is dropped.\n\tfor n < len(buf) && err == nil {\n\t\tvar nn int\n\t\tnn, err = r.Read(buf[n:])\n\t\t// Some spurious io.Reader's return\n\t\t// io.ErrUnexpectedEOF when nn == 0\n\t\t// this behavior is undocumented\n\t\t// so we are on purpose not using io.ReadFull\n\t\t// implementation because this can lead\n\t\t// to custom handling, to avoid that\n\t\t// we simply modify the original io.ReadFull\n\t\t// implementation to avoid this issue.\n\t\t// io.ErrUnexpectedEOF with nn == 0 really\n\t\t// means that io.EOF\n\t\tif err == io.ErrUnexpectedEOF && nn == 0 {\n\t\t\terr = io.EOF\n\t\t}\n\t\tn += nn\n\t}\n\tif n >= len(buf) {\n\t\terr = nil\n\t} else if n > 0 && err == io.EOF {\n\t\terr = io.ErrUnexpectedEOF\n\t}\n\treturn\n}\n\nfunc baseLogger(testName, function string, args map[string]interface{}, startTime time.Time) *slog.Logger {\n\t// calculate the test case duration\n\tduration := time.Since(startTime)\n\t// log with the fields as per mint\n\tl := slog.With(\n\t\t\"name\", \"minio-go: \"+testName,\n\t\t\"duration\", duration.Nanoseconds()/1000000,\n\t)\n\tif function != \"\" {\n\t\tl = l.With(\"function\", function)\n\t}\n\tif len(args) > 0 {\n\t\tl = l.With(\"args\", args)\n\t}\n\treturn l\n}\n\n// log successful test runs\nfunc logSuccess(testName, function string, args map[string]interface{}, startTime time.Time) {\n\tbaseLogger(testName, function, args, startTime).\n\t\tWith(\"status\", \"PASS\").\n\t\tInfo(\"\")\n}\n\n// As few of the features are not available in Gateway(s) currently, Check if err value is NotImplemented,\n// and log as NA in that case and continue execution. Otherwise log as failure and return\nfunc logError(testName, function string, args map[string]interface{}, startTime time.Time, alert, message string, err error) {\n\t// If server returns NotImplemented we assume it is gateway mode and hence log it as info and move on to next tests\n\t// Special case for ComposeObject API as it is implemented on client side and adds specific error details like `Error in upload-part-copy` in\n\t// addition to NotImplemented error returned from server\n\tif isErrNotImplemented(err) {\n\t\tlogIgnored(testName, function, args, startTime, message)\n\t} else {\n\t\tlogFailure(testName, function, args, startTime, alert, message, err)\n\t\tif !isRunOnFail() {\n\t\t\tpanic(fmt.Sprintf(\"Test failed with message: %s, err: %v\", message, err))\n\t\t}\n\t}\n}\n\n// Log failed test runs, do not call this directly, use logError instead, as that correctly stops the test run\nfunc logFailure(testName, function string, args map[string]interface{}, startTime time.Time, alert, message string, err error) {\n\tl := baseLogger(testName, function, args, startTime).With(\n\t\t\"status\", \"FAIL\",\n\t\t\"alert\", alert,\n\t\t\"message\", message,\n\t)\n\n\tif err != nil {\n\t\tl = l.With(\"error\", err)\n\t}\n\n\tl.Error(\"\")\n}\n\n// log not applicable test runs\nfunc logIgnored(testName, function string, args map[string]interface{}, startTime time.Time, alert string) {\n\tbaseLogger(testName, function, args, startTime).\n\t\tWith(\n\t\t\t\"status\", \"NA\",\n\t\t\t\"alert\", strings.Split(alert, \" \")[0]+\" is NotImplemented\",\n\t\t).Info(\"\")\n}\n\n// Delete objects in given bucket, recursively\nfunc cleanupBucket(bucketName string, c *minio.Client) error {\n\t// Create a done channel to control 'ListObjectsV2' go routine.\n\tdoneCh := make(chan struct{})\n\t// Exit cleanly upon return.\n\tdefer close(doneCh)\n\t// Iterate over all objects in the bucket via listObjectsV2 and delete\n\tfor objCh := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{Recursive: true}) {\n\t\tif objCh.Err != nil {\n\t\t\treturn objCh.Err\n\t\t}\n\t\tif objCh.Key != \"\" {\n\t\t\terr := c.RemoveObject(context.Background(), bucketName, objCh.Key, minio.RemoveObjectOptions{})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tfor objPartInfo := range c.ListIncompleteUploads(context.Background(), bucketName, \"\", true) {\n\t\tif objPartInfo.Err != nil {\n\t\t\treturn objPartInfo.Err\n\t\t}\n\t\tif objPartInfo.Key != \"\" {\n\t\t\terr := c.RemoveIncompleteUpload(context.Background(), bucketName, objPartInfo.Key)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\t// objects are already deleted, clear the buckets now\n\treturn c.RemoveBucket(context.Background(), bucketName)\n}\n\nfunc cleanupVersionedBucket(bucketName string, c *minio.Client) error {\n\tdoneCh := make(chan struct{})\n\tdefer close(doneCh)\n\tfor obj := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true}) {\n\t\tif obj.Err != nil {\n\t\t\treturn obj.Err\n\t\t}\n\t\tif obj.Key != \"\" {\n\t\t\terr := c.RemoveObject(context.Background(), bucketName, obj.Key,\n\t\t\t\tminio.RemoveObjectOptions{VersionID: obj.VersionID, GovernanceBypass: true})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tfor objPartInfo := range c.ListIncompleteUploads(context.Background(), bucketName, \"\", true) {\n\t\tif objPartInfo.Err != nil {\n\t\t\treturn objPartInfo.Err\n\t\t}\n\t\tif objPartInfo.Key != \"\" {\n\t\t\terr := c.RemoveIncompleteUpload(context.Background(), bucketName, objPartInfo.Key)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\t// objects are already deleted, clear the buckets now\n\terr := c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tfor obj := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true}) {\n\t\t\tslog.Info(\"found object\", \"key\", obj.Key, \"version\", obj.VersionID)\n\t\t}\n\t}\n\treturn err\n}\n\nfunc isErrNotImplemented(err error) bool {\n\treturn minio.ToErrorResponse(err).Code == \"NotImplemented\"\n}\n\nfunc isRunOnFail() bool {\n\treturn os.Getenv(\"RUN_ON_FAIL\") == \"1\"\n}\n\nfunc init() {\n\t// If server endpoint is not set, all tests default to\n\t// using https://play.min.io\n\tif os.Getenv(serverEndpoint) == \"\" {\n\t\tos.Setenv(serverEndpoint, \"play.min.io\")\n\t\tos.Setenv(accessKey, \"Q3AM3UQ867SPQQA43P2F\")\n\t\tos.Setenv(secretKey, \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\")\n\t\tos.Setenv(enableHTTPS, \"1\")\n\t}\n}\n\nvar mintDataDir = os.Getenv(\"MINT_DATA_DIR\")\n\nfunc getMintDataDirFilePath(filename string) (fp string) {\n\tif mintDataDir == \"\" {\n\t\treturn\n\t}\n\treturn filepath.Join(mintDataDir, filename)\n}\n\nfunc newRandomReader(seed, size int64) io.Reader {\n\treturn io.LimitReader(rand.New(rand.NewSource(seed)), size)\n}\n\nfunc mustCrcReader(r io.Reader) uint32 {\n\tcrc := crc32.NewIEEE()\n\t_, err := io.Copy(crc, r)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn crc.Sum32()\n}\n\nfunc crcMatches(r io.Reader, want uint32) error {\n\tcrc := crc32.NewIEEE()\n\t_, err := io.Copy(crc, r)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tgot := crc.Sum32()\n\tif got != want {\n\t\treturn fmt.Errorf(\"crc mismatch, want %x, got %x\", want, got)\n\t}\n\treturn nil\n}\n\nfunc crcMatchesName(r io.Reader, name string) error {\n\twant := dataFileCRC32[name]\n\tcrc := crc32.NewIEEE()\n\t_, err := io.Copy(crc, r)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tgot := crc.Sum32()\n\tif got != want {\n\t\treturn fmt.Errorf(\"crc mismatch, want %x, got %x\", want, got)\n\t}\n\treturn nil\n}\n\n// read data from file if it exists or optionally create a buffer of particular size\nfunc getDataReader(fileName string) io.ReadCloser {\n\tif mintDataDir == \"\" {\n\t\tsize := int64(dataFileMap[fileName])\n\t\tif _, ok := dataFileCRC32[fileName]; !ok {\n\t\t\tdataFileCRC32[fileName] = mustCrcReader(newRandomReader(size, size))\n\t\t}\n\t\treturn io.NopCloser(newRandomReader(size, size))\n\t}\n\treader, _ := os.Open(getMintDataDirFilePath(fileName))\n\tif _, ok := dataFileCRC32[fileName]; !ok {\n\t\tdataFileCRC32[fileName] = mustCrcReader(reader)\n\t\treader.Close()\n\t\treader, _ = os.Open(getMintDataDirFilePath(fileName))\n\t}\n\treturn reader\n}\n\n// randString generates random names and prepends them with a known prefix.\nfunc randString(n int, src rand.Source, prefix string) string {\n\tb := make([]byte, n)\n\t// A rand.Int63() generates 63 random bits, enough for letterIdxMax letters!\n\tfor i, cache, remain := n-1, src.Int63(), letterIdxMax; i >= 0; {\n\t\tif remain == 0 {\n\t\t\tcache, remain = src.Int63(), letterIdxMax\n\t\t}\n\t\tif idx := int(cache & letterIdxMask); idx < len(letterBytes) {\n\t\t\tb[i] = letterBytes[idx]\n\t\t\ti--\n\t\t}\n\t\tcache >>= letterIdxBits\n\t\tremain--\n\t}\n\treturn prefix + string(b[0:30-len(prefix)])\n}\n\nvar dataFileMap = map[string]int{\n\t\"datafile-0-b\":     0,\n\t\"datafile-1-b\":     1,\n\t\"datafile-1-kB\":    1 * humanize.KiByte,\n\t\"datafile-10-kB\":   10 * humanize.KiByte,\n\t\"datafile-33-kB\":   33 * humanize.KiByte,\n\t\"datafile-100-kB\":  100 * humanize.KiByte,\n\t\"datafile-1.03-MB\": 1056 * humanize.KiByte,\n\t\"datafile-1-MB\":    1 * humanize.MiByte,\n\t\"datafile-5-MB\":    5 * humanize.MiByte,\n\t\"datafile-6-MB\":    6 * humanize.MiByte,\n\t\"datafile-11-MB\":   11 * humanize.MiByte,\n\t\"datafile-65-MB\":   65 * humanize.MiByte,\n\t\"datafile-129-MB\":  129 * humanize.MiByte,\n}\n\nvar dataFileCRC32 = map[string]uint32{}\n\nfunc isFullMode() bool {\n\treturn os.Getenv(\"MINT_MODE\") == \"full\"\n}\n\nfunc getFuncName() string {\n\treturn getFuncNameLoc(2)\n}\n\nfunc getFuncNameLoc(caller int) string {\n\tpc, _, _, _ := runtime.Caller(caller)\n\treturn strings.TrimPrefix(runtime.FuncForPC(pc).Name(), \"main.\")\n}\n\ntype ClientConfig struct {\n\t// MinIO client configuration\n\tTraceOn         bool // Turn on tracing of HTTP requests and responses to stderr\n\tCredsV2         bool // Use V2 credentials if true, otherwise use v4\n\tTrailingHeaders bool // Send trailing headers in requests\n}\n\nfunc NewClient(config ClientConfig) (*minio.Client, error) {\n\t// Instantiate new MinIO client\n\tvar creds *credentials.Credentials\n\tif config.CredsV2 {\n\t\tcreds = credentials.NewStaticV2(os.Getenv(accessKey), os.Getenv(secretKey), \"\")\n\t} else {\n\t\tcreds = credentials.NewStaticV4(os.Getenv(accessKey), os.Getenv(secretKey), \"\")\n\t}\n\topts := &minio.Options{\n\t\tCreds:           creds,\n\t\tTransport:       createHTTPTransport(),\n\t\tSecure:          mustParseBool(os.Getenv(enableHTTPS)),\n\t\tTrailingHeaders: config.TrailingHeaders,\n\t}\n\tclient, err := minio.New(os.Getenv(serverEndpoint), opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif config.TraceOn {\n\t\tclient.TraceOn(os.Stderr)\n\t}\n\n\t// Set user agent.\n\tclient.SetAppInfo(\"MinIO-go-FunctionalTest\", appVersion)\n\n\treturn client, nil\n}\n\n// Tests bucket re-create errors.\nfunc testMakeBucketError() {\n\tregion := \"eu-central-1\"\n\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"MakeBucket(bucketName, region)\"\n\t// initialize logging params\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"region\":     region,\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket in 'eu-central-1'.\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: region}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket Failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: region}); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bucket already exists\", err)\n\t\treturn\n\t}\n\t// Verify valid error response from server.\n\tif minio.ToErrorResponse(err).Code != \"BucketAlreadyExists\" &&\n\t\tminio.ToErrorResponse(err).Code != \"BucketAlreadyOwnedByYou\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error returned by server\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testMetadataSizeLimit() {\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, objectSize, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":        \"\",\n\t\t\"objectName\":        \"\",\n\t\t\"opts.UserMetadata\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client creation failed\", err)\n\t\treturn\n\t}\n\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tconst HeaderSizeLimit = 8 * 1024\n\tconst UserMetadataLimit = 2 * 1024\n\n\t// Meta-data greater than the 2 KB limit of AWS - PUT calls with this meta-data should fail\n\tmetadata := make(map[string]string)\n\tmetadata[\"X-Amz-Meta-Mint-Test\"] = string(bytes.Repeat([]byte(\"m\"), 1+UserMetadataLimit-len(\"X-Amz-Meta-Mint-Test\")))\n\targs[\"metadata\"] = fmt.Sprint(metadata)\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(nil), 0, minio.PutObjectOptions{UserMetadata: metadata})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Created object with user-defined metadata exceeding metadata size limits\", nil)\n\t\treturn\n\t}\n\n\t// Meta-data (headers) greater than the 8 KB limit of AWS - PUT calls with this meta-data should fail\n\tmetadata = make(map[string]string)\n\tmetadata[\"X-Amz-Mint-Test\"] = string(bytes.Repeat([]byte(\"m\"), 1+HeaderSizeLimit-len(\"X-Amz-Mint-Test\")))\n\targs[\"metadata\"] = fmt.Sprint(metadata)\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(nil), 0, minio.PutObjectOptions{UserMetadata: metadata})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Created object with headers exceeding header size limits\", nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests various bucket supported formats.\nfunc testMakeBucketRegions() {\n\tregion := \"eu-central-1\"\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"MakeBucket(bucketName, region)\"\n\t// initialize logging params\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"region\":     region,\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket in 'eu-central-1'.\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: region}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket with '.' in its name, in 'us-west-2'. This\n\t// request is internally staged into a path style instead of\n\t// virtual host style.\n\tregion = \"us-west-2\"\n\targs[\"region\"] = region\n\tif err = c.MakeBucket(context.Background(), bucketName+\".withperiod\", minio.MakeBucketOptions{Region: region}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupBucket(bucketName+\".withperiod\", c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test PutObject using a large data to trigger multipart readat\nfunc testPutObjectReadAt() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"objectContentType\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Object content type\n\tobjectContentType := \"binary/octet-stream\"\n\targs[\"objectContentType\"] = objectContentType\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: objectContentType})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Get Object failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat Object failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Number of bytes in stat does not match, expected %d got %d\", bufSize, st.Size), err)\n\t\treturn\n\t}\n\tif st.ContentType != objectContentType && st.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Content types don't match\", err)\n\t\treturn\n\t}\n\tif err := crcMatchesName(r, \"datafile-129-MB\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"data CRC check failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object Close failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object is already closed, didn't return error on Close\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testListObjectVersions() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ListObjectVersions(bucketName, prefix, recursive)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"prefix\":     \"\",\n\t\t\"recursive\":  \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbufSize := dataFileMap[\"datafile-10-kB\"]\n\treader := getDataReader(\"datafile-10-kB\")\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\treader.Close()\n\n\tbufSize = dataFileMap[\"datafile-1-b\"]\n\treader = getDataReader(\"datafile-1-b\")\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\treader.Close()\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected object deletion\", err)\n\t\treturn\n\t}\n\n\tvar deleteMarkers, versions int\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tif info.Key != objectName {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected object name in listing objects\", nil)\n\t\t\treturn\n\t\t}\n\t\tif info.VersionID == \"\" {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected version id in listing objects\", nil)\n\t\t\treturn\n\t\t}\n\t\tif info.IsDeleteMarker {\n\t\t\tdeleteMarkers++\n\t\t\tif !info.IsLatest {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected IsLatest field in listing objects\", nil)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tversions++\n\t\t}\n\t}\n\n\tif deleteMarkers != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of DeleteMarker elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\tif versions != 2 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of Version elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testStatObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"StatObject\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbufSize := dataFileMap[\"datafile-10-kB\"]\n\treader := getDataReader(\"datafile-10-kB\")\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\treader.Close()\n\n\tbufSize = dataFileMap[\"datafile-1-b\"]\n\treader = getDataReader(\"datafile-1-b\")\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\treader.Close()\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\n\tvar results []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tresults = append(results, info)\n\t}\n\n\tif len(results) != 2 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of Version elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\tfor i := 0; i < len(results); i++ {\n\t\topts := minio.StatObjectOptions{VersionID: results[i].VersionID}\n\t\tstatInfo, err := c.StatObject(context.Background(), bucketName, objectName, opts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.VersionID == \"\" || statInfo.VersionID != results[i].VersionID {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected version id\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.ETag != results[i].ETag {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected ETag\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.LastModified.Unix() != results[i].LastModified.Unix() {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Last-Modified\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.Size != results[i].Size {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Content-Length\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testGetObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Save the contents of datafiles to check with GetObject() reader output later\n\tvar buffers [][]byte\n\ttestFiles := []string{\"datafile-1-b\", \"datafile-10-kB\"}\n\n\tfor _, testFile := range testFiles {\n\t\tr := getDataReader(testFile)\n\t\tbuf, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected failure\", err)\n\t\t\treturn\n\t\t}\n\t\tr.Close()\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tbuffers = append(buffers, buf)\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\n\tvar results []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tresults = append(results, info)\n\t}\n\n\tif len(results) != 2 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of Version elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\tsort.SliceStable(results, func(i, j int) bool {\n\t\treturn results[i].Size < results[j].Size\n\t})\n\n\tsort.SliceStable(buffers, func(i, j int) bool {\n\t\treturn len(buffers[i]) < len(buffers[j])\n\t})\n\n\tfor i := 0; i < len(results); i++ {\n\t\topts := minio.GetObjectOptions{VersionID: results[i].VersionID}\n\t\treader, err := c.GetObject(context.Background(), bucketName, objectName, opts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during  GET object\", err)\n\t\t\treturn\n\t\t}\n\t\tstatInfo, err := reader.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during calling reader.Stat()\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.ETag != results[i].ETag {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected ETag\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.LastModified.Unix() != results[i].LastModified.Unix() {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Last-Modified\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.Size != results[i].Size {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Content-Length\", err)\n\t\t\treturn\n\t\t}\n\n\t\ttmpBuffer := bytes.NewBuffer([]byte{})\n\t\t_, err = io.Copy(tmpBuffer, reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected io.Copy()\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif !bytes.Equal(tmpBuffer.Bytes(), buffers[i]) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected content of GetObject()\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testPutObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tconst n = 10\n\t// Read input...\n\n\t// Save the data concurrently.\n\tvar wg sync.WaitGroup\n\twg.Add(n)\n\tbuffers := make([][]byte, n)\n\tvar errs [n]error\n\tfor i := 0; i < n; i++ {\n\t\tr := newRandomReader(int64((1<<20)*i+i), int64(i))\n\t\tbuf, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected failure\", err)\n\t\t\treturn\n\t\t}\n\t\tbuffers[i] = buf\n\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\t_, errs[i] = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{PartSize: 5 << 20})\n\t\t}(i)\n\t}\n\twg.Wait()\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tvar results []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tresults = append(results, info)\n\t}\n\n\tif len(results) != n {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of Version elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\tsort.Slice(results, func(i, j int) bool {\n\t\treturn results[i].Size < results[j].Size\n\t})\n\n\tsort.Slice(buffers, func(i, j int) bool {\n\t\treturn len(buffers[i]) < len(buffers[j])\n\t})\n\n\tfor i := 0; i < len(results); i++ {\n\t\topts := minio.GetObjectOptions{VersionID: results[i].VersionID}\n\t\treader, err := c.GetObject(context.Background(), bucketName, objectName, opts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during  GET object\", err)\n\t\t\treturn\n\t\t}\n\t\tstatInfo, err := reader.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during calling reader.Stat()\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.ETag != results[i].ETag {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected ETag\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.LastModified.Unix() != results[i].LastModified.Unix() {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Last-Modified\", err)\n\t\t\treturn\n\t\t}\n\t\tif statInfo.Size != results[i].Size {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"error during HEAD object, unexpected Content-Length\", err)\n\t\t\treturn\n\t\t}\n\n\t\ttmpBuffer := bytes.NewBuffer([]byte{})\n\t\t_, err = io.Copy(tmpBuffer, reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected io.Copy()\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif !bytes.Equal(tmpBuffer.Bytes(), buffers[i]) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected content of GetObject()\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testListMultipartUpload() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\tcore := minio.Core{Client: c}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\tctx := context.Background()\n\terr = c.MakeBucket(ctx, bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\t}\n\t}()\n\tobjName := \"prefix/objectName\"\n\n\twant := minio.ListMultipartUploadsResult{\n\t\tBucket:             bucketName,\n\t\tKeyMarker:          \"\",\n\t\tUploadIDMarker:     \"\",\n\t\tNextKeyMarker:      \"\",\n\t\tNextUploadIDMarker: \"\",\n\t\tEncodingType:       \"url\",\n\t\tMaxUploads:         1000,\n\t\tIsTruncated:        false,\n\t\tPrefix:             \"prefix/objectName\",\n\t\tDelimiter:          \"/\",\n\t\tCommonPrefixes:     nil,\n\t}\n\tfor i := 0; i < 5; i++ {\n\t\tuid, err := core.NewMultipartUpload(ctx, bucketName, objName, minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload failed\", err)\n\t\t\treturn\n\t\t}\n\t\twant.Uploads = append(want.Uploads, minio.ObjectMultipartInfo{\n\t\t\tInitiated:    time.Time{},\n\t\t\tStorageClass: \"\",\n\t\t\tKey:          objName,\n\t\t\tSize:         0,\n\t\t\tUploadID:     uid,\n\t\t\tErr:          nil,\n\t\t})\n\n\t\tfor j := 0; j < 5; j++ {\n\t\t\tcmpGot := func(call string, got minio.ListMultipartUploadsResult) bool {\n\t\t\t\tfor i := range got.Uploads {\n\t\t\t\t\tgot.Uploads[i].Initiated = time.Time{}\n\t\t\t\t}\n\t\t\t\tif !reflect.DeepEqual(want, got) {\n\t\t\t\t\terr := fmt.Errorf(\"want: %#v\\ngot : %#v\", want, got)\n\t\t\t\t\tlogError(testName, function, args, startTime, \"\", call+\" failed\", err)\n\t\t\t\t}\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tgot, err := core.ListMultipartUploads(ctx, bucketName, objName, \"\", \"\", \"/\", 1000)\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"ListMultipartUploads failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !cmpGot(\"ListMultipartUploads-prefix\", got) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tgot, err = core.ListMultipartUploads(ctx, bucketName, objName, objName, \"\", \"/\", 1000)\n\t\t\tgot.KeyMarker = \"\"\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"ListMultipartUploads failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !cmpGot(\"ListMultipartUploads-marker\", got) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif i > 2 {\n\t\t\terr = core.AbortMultipartUpload(ctx, bucketName, objName, uid)\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"AbortMultipartUpload failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\twant.Uploads = want.Uploads[:len(want.Uploads)-1]\n\t\t}\n\t}\n\tfor _, up := range want.Uploads {\n\t\terr = core.AbortMultipartUpload(ctx, bucketName, objName, up.UploadID)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"AbortMultipartUpload failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testCopyObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\ttestFiles := []string{\"datafile-1-b\", \"datafile-10-kB\"}\n\tfor _, testFile := range testFiles {\n\t\tr := getDataReader(testFile)\n\t\tbuf, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected failure\", err)\n\t\t\treturn\n\t\t}\n\t\tr.Close()\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tvar infos []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tinfos = append(infos, info)\n\t}\n\n\tsort.Slice(infos, func(i, j int) bool {\n\t\treturn infos[i].Size < infos[j].Size\n\t})\n\n\treader, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{VersionID: infos[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject of the oldest version content failed\", err)\n\t\treturn\n\t}\n\n\toldestContent, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Reading the oldest object version failed\", err)\n\t\treturn\n\t}\n\n\t// Copy Source\n\tsrcOpts := minio.CopySrcOptions{\n\t\tBucket:    bucketName,\n\t\tObject:    objectName,\n\t\tVersionID: infos[0].VersionID,\n\t}\n\targs[\"src\"] = srcOpts\n\n\tdstOpts := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: objectName + \"-copy\",\n\t}\n\targs[\"dst\"] = dstOpts\n\n\t// Perform the Copy\n\tif _, err = c.CopyObject(context.Background(), dstOpts, srcOpts); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\t// Destination object\n\treaderCopy, err := c.GetObject(context.Background(), bucketName, objectName+\"-copy\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer readerCopy.Close()\n\n\tnewestContent, err := io.ReadAll(readerCopy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Reading from GetObject reader failed\", err)\n\t\treturn\n\t}\n\n\tif len(newestContent) == 0 || !bytes.Equal(oldestContent, newestContent) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected destination object content\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testConcurrentCopyObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\ttestFiles := []string{\"datafile-10-kB\"}\n\tfor _, testFile := range testFiles {\n\t\tr := getDataReader(testFile)\n\t\tbuf, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected failure\", err)\n\t\t\treturn\n\t\t}\n\t\tr.Close()\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tvar infos []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tinfos = append(infos, info)\n\t}\n\n\tsort.Slice(infos, func(i, j int) bool {\n\t\treturn infos[i].Size < infos[j].Size\n\t})\n\n\treader, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{VersionID: infos[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject of the oldest version content failed\", err)\n\t\treturn\n\t}\n\n\toldestContent, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Reading the oldest object version failed\", err)\n\t\treturn\n\t}\n\n\t// Copy Source\n\tsrcOpts := minio.CopySrcOptions{\n\t\tBucket:    bucketName,\n\t\tObject:    objectName,\n\t\tVersionID: infos[0].VersionID,\n\t}\n\targs[\"src\"] = srcOpts\n\n\tdstOpts := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: objectName + \"-copy\",\n\t}\n\targs[\"dst\"] = dstOpts\n\n\t// Perform the Copy concurrently\n\tconst n = 10\n\tvar wg sync.WaitGroup\n\twg.Add(n)\n\tvar errs [n]error\n\tfor i := 0; i < n; i++ {\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\t_, errs[i] = c.CopyObject(context.Background(), dstOpts, srcOpts)\n\t\t}(i)\n\t}\n\twg.Wait()\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsInfo = c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: false, Prefix: dstOpts.Object})\n\tinfos = []minio.ObjectInfo{}\n\tfor info := range objectsInfo {\n\t\t// Destination object\n\t\treaderCopy, err := c.GetObject(context.Background(), bucketName, objectName+\"-copy\", minio.GetObjectOptions{VersionID: info.VersionID})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer readerCopy.Close()\n\n\t\tnewestContent, err := io.ReadAll(readerCopy)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Reading from GetObject reader failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif len(newestContent) == 0 || !bytes.Equal(oldestContent, newestContent) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected destination object content\", err)\n\t\t\treturn\n\t\t}\n\t\tinfos = append(infos, info)\n\t}\n\n\tif len(infos) != n {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of Version elements in listing objects\", nil)\n\t\treturn\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testComposeObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// var testFiles = []string{\"datafile-5-MB\", \"datafile-10-kB\"}\n\ttestFiles := []string{\"datafile-5-MB\", \"datafile-10-kB\"}\n\tvar testFilesBytes [][]byte\n\n\tfor _, testFile := range testFiles {\n\t\tr := getDataReader(testFile)\n\t\tbuf, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"unexpected failure\", err)\n\t\t\treturn\n\t\t}\n\t\tr.Close()\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\ttestFilesBytes = append(testFilesBytes, buf)\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\n\tvar results []minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tresults = append(results, info)\n\t}\n\n\tsort.SliceStable(results, func(i, j int) bool {\n\t\treturn results[i].Size > results[j].Size\n\t})\n\n\t// Source objects to concatenate. We also specify decryption\n\t// key for each\n\tsrc1 := minio.CopySrcOptions{\n\t\tBucket:    bucketName,\n\t\tObject:    objectName,\n\t\tVersionID: results[0].VersionID,\n\t}\n\n\tsrc2 := minio.CopySrcOptions{\n\t\tBucket:    bucketName,\n\t\tObject:    objectName,\n\t\tVersionID: results[1].VersionID,\n\t}\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: objectName + \"-copy\",\n\t}\n\n\t_, err = c.ComposeObject(context.Background(), dst, src1, src2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject failed\", err)\n\t\treturn\n\t}\n\n\t// Destination object\n\treaderCopy, err := c.GetObject(context.Background(), bucketName, objectName+\"-copy\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject of the copy object failed\", err)\n\t\treturn\n\t}\n\tdefer readerCopy.Close()\n\n\tcopyContentBytes, err := io.ReadAll(readerCopy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Reading from the copy object reader failed\", err)\n\t\treturn\n\t}\n\n\tvar expectedContent []byte\n\tfor _, fileBytes := range testFilesBytes {\n\t\texpectedContent = append(expectedContent, fileBytes...)\n\t}\n\n\tif len(copyContentBytes) == 0 || !bytes.Equal(copyContentBytes, expectedContent) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected destination object content\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testRemoveObjectWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"DeleteObject()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, getDataReader(\"datafile-10-kB\"), int64(dataFileMap[\"datafile-10-kB\"]), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tobjectsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tvar version minio.ObjectInfo\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tversion = info\n\t\tbreak\n\t}\n\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, minio.RemoveObjectOptions{VersionID: version.VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"DeleteObject failed\", err)\n\t\treturn\n\t}\n\n\tobjectsInfo = c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tfor range objectsInfo {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected versioning info, should not have any one \", err)\n\t\treturn\n\t}\n\t// test delete marker version id is non-null\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, getDataReader(\"datafile-10-kB\"), int64(dataFileMap[\"datafile-10-kB\"]), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\t// create delete marker\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"DeleteObject failed\", err)\n\t\treturn\n\t}\n\tobjectsInfo = c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tidx := 0\n\tfor info := range objectsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tif idx == 0 {\n\t\t\tif !info.IsDeleteMarker {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error - expected delete marker to have been created\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif info.VersionID == \"\" {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error - expected delete marker to be versioned\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tidx++\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testRemoveObjectsWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"DeleteObjects()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, getDataReader(\"datafile-10-kB\"), int64(dataFileMap[\"datafile-10-kB\"]), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tobjectsVersions := make(chan minio.ObjectInfo)\n\tgo func() {\n\t\tobjectsVersionsInfo := c.ListObjects(context.Background(), bucketName,\n\t\t\tminio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\t\tfor info := range objectsVersionsInfo {\n\t\t\tif info.Err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tobjectsVersions <- info\n\t\t}\n\t\tclose(objectsVersions)\n\t}()\n\n\tremoveErrors := c.RemoveObjects(context.Background(), bucketName, objectsVersions, minio.RemoveObjectsOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"DeleteObjects call failed\", err)\n\t\treturn\n\t}\n\n\tfor e := range removeErrors {\n\t\tif e.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Single delete operation failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsVersionsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\tfor range objectsVersionsInfo {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected versioning info, should not have any one \", err)\n\t\treturn\n\t}\n\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testObjectTaggingWithVersioning() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"{Get,Set,Remove}ObjectTagging()\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\terr = c.EnableVersioning(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Enable versioning failed\", err)\n\t\treturn\n\t}\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tfor _, file := range []string{\"datafile-1-b\", \"datafile-10-kB\"} {\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, getDataReader(file), int64(dataFileMap[file]), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tversionsInfo := c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{WithVersions: true, Recursive: true})\n\n\tvar versions []minio.ObjectInfo\n\tfor info := range versionsInfo {\n\t\tif info.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error during listing objects\", err)\n\t\t\treturn\n\t\t}\n\t\tversions = append(versions, info)\n\t}\n\n\tsort.SliceStable(versions, func(i, j int) bool {\n\t\treturn versions[i].Size < versions[j].Size\n\t})\n\n\ttagsV1 := map[string]string{\"key1\": \"val1\"}\n\tt1, err := tags.MapToObjectTags(tagsV1)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectTagging (1) failed\", err)\n\t\treturn\n\t}\n\n\terr = c.PutObjectTagging(context.Background(), bucketName, objectName, t1, minio.PutObjectTaggingOptions{VersionID: versions[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectTagging (1) failed\", err)\n\t\treturn\n\t}\n\n\ttagsV2 := map[string]string{\"key2\": \"val2\"}\n\tt2, err := tags.MapToObjectTags(tagsV2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectTagging (1) failed\", err)\n\t\treturn\n\t}\n\n\terr = c.PutObjectTagging(context.Background(), bucketName, objectName, t2, minio.PutObjectTaggingOptions{VersionID: versions[1].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectTagging (2) failed\", err)\n\t\treturn\n\t}\n\n\ttagsEqual := func(tags1, tags2 map[string]string) bool {\n\t\tfor k1, v1 := range tags1 {\n\t\t\tv2, found := tags2[k1]\n\t\t\tif found {\n\t\t\t\tif v1 != v2 {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tgotTagsV1, err := c.GetObjectTagging(context.Background(), bucketName, objectName, minio.GetObjectTaggingOptions{VersionID: versions[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectTagging failed\", err)\n\t\treturn\n\t}\n\n\tif !tagsEqual(t1.ToMap(), gotTagsV1.ToMap()) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected tags content (1)\", err)\n\t\treturn\n\t}\n\n\tgotTagsV2, err := c.GetObjectTagging(context.Background(), bucketName, objectName, minio.GetObjectTaggingOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectTaggingContext failed\", err)\n\t\treturn\n\t}\n\n\tif !tagsEqual(t2.ToMap(), gotTagsV2.ToMap()) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected tags content (2)\", err)\n\t\treturn\n\t}\n\n\terr = c.RemoveObjectTagging(context.Background(), bucketName, objectName, minio.RemoveObjectTaggingOptions{VersionID: versions[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectTagging (2) failed\", err)\n\t\treturn\n\t}\n\n\temptyTags, err := c.GetObjectTagging(context.Background(), bucketName, objectName,\n\t\tminio.GetObjectTaggingOptions{VersionID: versions[0].VersionID})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectTagging failed\", err)\n\t\treturn\n\t}\n\n\tif len(emptyTags.ToMap()) != 0 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected tags content (2)\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and their versions as long as the bucket itself\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test PutObject with custom checksums.\nfunc testPutObjectWithChecksums() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{UserMetadata: metadata, Progress: progress}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\ttests := []struct {\n\t\tcs minio.ChecksumType\n\t}{\n\t\t{cs: minio.ChecksumCRC32C},\n\t\t{cs: minio.ChecksumCRC32},\n\t\t{cs: minio.ChecksumSHA1},\n\t\t{cs: minio.ChecksumSHA256},\n\t\t{cs: minio.ChecksumCRC64NVME},\n\t}\n\n\tfor _, test := range tests {\n\t\tif os.Getenv(\"MINT_NO_FULL_OBJECT\") != \"\" && test.cs.FullObjectRequested() {\n\t\t\tcontinue\n\t\t}\n\t\tbufSize := dataFileMap[\"datafile-10-kB\"]\n\n\t\t// Save the data\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\tcmpChecksum := func(got, want string) {\n\t\t\tif want != got {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"checksum mismatch\", fmt.Errorf(\"want %s, got %s\", want, got))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tmeta := map[string]string{}\n\t\treader := getDataReader(\"datafile-10-kB\")\n\t\tb, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\th := test.cs.Hasher()\n\t\th.Reset()\n\n\t\t// Test with a bad CRC - we haven't called h.Write(b), so this is a checksum of empty data\n\t\tmeta[test.cs.Key()] = base64.StdEncoding.EncodeToString(h.Sum(nil))\n\t\targs[\"metadata\"] = meta\n\t\targs[\"range\"] = \"false\"\n\t\targs[\"checksum\"] = test.cs.String()\n\n\t\tresp, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart: true,\n\t\t\tUserMetadata:     meta,\n\t\t})\n\t\tif err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject did not fail on wrong CRC\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Set correct CRC.\n\t\th.Write(b)\n\t\tmeta[test.cs.Key()] = base64.StdEncoding.EncodeToString(h.Sum(nil))\n\t\treader.Close()\n\n\t\tresp, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart:     true,\n\t\t\tDisableContentSha256: true,\n\t\t\tUserMetadata:         meta,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(resp.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(resp.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(resp.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(resp.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\tcmpChecksum(resp.ChecksumCRC64NVME, meta[\"x-amz-checksum-crc64nvme\"])\n\n\t\t// Read the data back\n\t\tgopts := minio.GetObjectOptions{Checksum: true}\n\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tst, err := r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(st.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(st.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(st.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(st.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\tcmpChecksum(st.ChecksumCRC64NVME, meta[\"x-amz-checksum-crc64nvme\"])\n\n\t\tif st.Size != int64(bufSize) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes returned by PutObject does not match GetObject, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\t\treturn\n\t\t}\n\n\t\tif err := r.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object Close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif err := r.Close(); err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object already closed, should respond with error\", err)\n\t\t\treturn\n\t\t}\n\n\t\targs[\"range\"] = \"true\"\n\t\terr = gopts.SetRange(100, 1000)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"SetRange failed\", err)\n\t\t\treturn\n\t\t}\n\t\tr, err = c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tb, err = io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\tst, err = r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Range requests should return empty checksums...\n\t\tcmpChecksum(st.ChecksumSHA256, \"\")\n\t\tcmpChecksum(st.ChecksumSHA1, \"\")\n\t\tcmpChecksum(st.ChecksumCRC32, \"\")\n\t\tcmpChecksum(st.ChecksumCRC32C, \"\")\n\t\tcmpChecksum(st.ChecksumCRC64NVME, \"\")\n\n\t\tdelete(args, \"range\")\n\t\tdelete(args, \"metadata\")\n\t\tlogSuccess(testName, function, args, startTime)\n\t}\n}\n\n// Test PutObject with custom checksums.\nfunc testPutObjectWithTrailingChecksums() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{UserMetadata: metadata, Progress: progress, TrailChecksum: xxx}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\ttests := []struct {\n\t\tcs minio.ChecksumType\n\t}{\n\t\t{cs: minio.ChecksumCRC64NVME},\n\t\t{cs: minio.ChecksumCRC32C},\n\t\t{cs: minio.ChecksumCRC32},\n\t\t{cs: minio.ChecksumSHA1},\n\t\t{cs: minio.ChecksumSHA256},\n\t}\n\tfor _, test := range tests {\n\t\tif os.Getenv(\"MINT_NO_FULL_OBJECT\") != \"\" && test.cs.FullObjectRequested() {\n\t\t\tcontinue\n\t\t}\n\t\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\t\tbufSize := dataFileMap[\"datafile-10-kB\"]\n\n\t\t// Save the data\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\tcmpChecksum := func(got, want string) {\n\t\t\tif want != got {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"checksum mismatch\", fmt.Errorf(\"want %s, got %s\", want, got))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tmeta := map[string]string{}\n\t\treader := getDataReader(\"datafile-10-kB\")\n\t\tb, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\th := test.cs.Hasher()\n\t\th.Reset()\n\n\t\t// Test with Wrong CRC.\n\t\targs[\"metadata\"] = meta\n\t\targs[\"range\"] = \"false\"\n\t\targs[\"checksum\"] = test.cs.String()\n\n\t\tresp, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart:     true,\n\t\t\tDisableContentSha256: true,\n\t\t\tUserMetadata:         meta,\n\t\t\tChecksum:             test.cs,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\th.Write(b)\n\t\tmeta[test.cs.Key()] = base64.StdEncoding.EncodeToString(h.Sum(nil))\n\n\t\tcmpChecksum(resp.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(resp.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(resp.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(resp.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\tcmpChecksum(resp.ChecksumCRC64NVME, meta[\"x-amz-checksum-crc64nvme\"])\n\n\t\t// Read the data back\n\t\tgopts := minio.GetObjectOptions{Checksum: true}\n\n\t\tfunction = \"GetObject(...)\"\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tst, err := r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(st.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(st.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(st.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(st.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\tcmpChecksum(resp.ChecksumCRC64NVME, meta[\"x-amz-checksum-crc64nvme\"])\n\n\t\tif st.Size != int64(bufSize) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes returned by PutObject does not match GetObject, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\t\treturn\n\t\t}\n\n\t\tif err := r.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object Close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif err := r.Close(); err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object already closed, should respond with error\", err)\n\t\t\treturn\n\t\t}\n\n\t\tfunction = \"GetObject( Range...)\"\n\t\targs[\"range\"] = \"true\"\n\t\terr = gopts.SetRange(100, 1000)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"SetRange failed\", err)\n\t\t\treturn\n\t\t}\n\t\tr, err = c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tb, err = io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\tst, err = r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Range requests should return empty checksums...\n\t\tcmpChecksum(st.ChecksumSHA256, \"\")\n\t\tcmpChecksum(st.ChecksumSHA1, \"\")\n\t\tcmpChecksum(st.ChecksumCRC32, \"\")\n\t\tcmpChecksum(st.ChecksumCRC32C, \"\")\n\t\tcmpChecksum(st.ChecksumCRC64NVME, \"\")\n\n\t\tfunction = \"GetObjectAttributes(...)\"\n\t\ts, err := c.GetObjectAttributes(context.Background(), bucketName, objectName, minio.ObjectAttributesOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(s.Checksum.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(s.Checksum.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(s.Checksum.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(s.Checksum.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\n\t\tdelete(args, \"range\")\n\t\tdelete(args, \"metadata\")\n\t\tlogSuccess(testName, function, args, startTime)\n\t}\n}\n\n// Test PutObject with custom checksums.\nfunc testPutMultipartObjectWithChecksums(trailing bool) {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       fmt.Sprintf(\"minio.PutObjectOptions{UserMetadata: metadata, Trailing: %v}\", trailing),\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: trailing})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\thashMultiPart := func(b []byte, partSize int, cs minio.ChecksumType) string {\n\t\tr := bytes.NewReader(b)\n\t\thasher := cs.Hasher()\n\t\tif cs.FullObjectRequested() {\n\t\t\tpartSize = len(b)\n\t\t}\n\t\ttmp := make([]byte, partSize)\n\t\tparts := 0\n\t\tvar all []byte\n\t\tfor {\n\t\t\tn, err := io.ReadFull(r, tmp)\n\t\t\tif err != nil && err != io.ErrUnexpectedEOF && err != io.EOF {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Calc crc failed\", err)\n\t\t\t}\n\t\t\tif n == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tparts++\n\t\t\thasher.Reset()\n\t\t\thasher.Write(tmp[:n])\n\t\t\tall = append(all, hasher.Sum(nil)...)\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif parts == 1 {\n\t\t\treturn base64.StdEncoding.EncodeToString(hasher.Sum(nil))\n\t\t}\n\t\thasher.Reset()\n\t\thasher.Write(all)\n\t\treturn fmt.Sprintf(\"%s-%d\", base64.StdEncoding.EncodeToString(hasher.Sum(nil)), parts)\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\ttests := []struct {\n\t\tcs minio.ChecksumType\n\t}{\n\t\t{cs: minio.ChecksumFullObjectCRC32},\n\t\t{cs: minio.ChecksumFullObjectCRC32C},\n\t\t{cs: minio.ChecksumCRC64NVME},\n\t\t{cs: minio.ChecksumCRC32C},\n\t\t{cs: minio.ChecksumCRC32},\n\t\t{cs: minio.ChecksumSHA1},\n\t\t{cs: minio.ChecksumSHA256},\n\t}\n\n\tfor _, test := range tests {\n\t\tif os.Getenv(\"MINT_NO_FULL_OBJECT\") != \"\" && test.cs.FullObjectRequested() {\n\t\t\tcontinue\n\t\t}\n\n\t\targs[\"section\"] = \"prep\"\n\t\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\t\t// Save the data\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\t\targs[\"checksum\"] = test.cs.String()\n\n\t\tcmpChecksum := func(got, want string) {\n\t\t\tif want != got {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"checksum mismatch\", fmt.Errorf(\"want %s, got %s\", want, got))\n\t\t\t\t// fmt.Printf(\"want %s, got %s\\n\", want, got)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tconst partSize = 10 << 20\n\t\treader := getDataReader(\"datafile-129-MB\")\n\t\tb, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\treader.Close()\n\t\th := test.cs.Hasher()\n\t\th.Reset()\n\t\twant := hashMultiPart(b, partSize, test.cs)\n\n\t\tvar cs minio.ChecksumType\n\t\trd := io.Reader(io.NopCloser(bytes.NewReader(b)))\n\t\tif trailing {\n\t\t\tcs = test.cs\n\t\t\trd = bytes.NewReader(b)\n\t\t}\n\n\t\t// Set correct CRC.\n\t\targs[\"section\"] = \"PutObject\"\n\t\tresp, err := c.PutObject(context.Background(), bucketName, objectName, rd, int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableContentSha256: true,\n\t\t\tDisableMultipart:     false,\n\t\t\tUserMetadata:         nil,\n\t\t\tPartSize:             partSize,\n\t\t\tAutoChecksum:         test.cs,\n\t\t\tChecksum:             cs,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tswitch test.cs.Base() {\n\t\tcase minio.ChecksumCRC32C:\n\t\t\tcmpChecksum(resp.ChecksumCRC32C, want)\n\t\tcase minio.ChecksumCRC32:\n\t\t\tcmpChecksum(resp.ChecksumCRC32, want)\n\t\tcase minio.ChecksumSHA1:\n\t\t\tcmpChecksum(resp.ChecksumSHA1, want)\n\t\tcase minio.ChecksumSHA256:\n\t\t\tcmpChecksum(resp.ChecksumSHA256, want)\n\t\tcase minio.ChecksumCRC64NVME:\n\t\t\tcmpChecksum(resp.ChecksumCRC64NVME, want)\n\t\t}\n\n\t\targs[\"section\"] = \"HeadObject\"\n\t\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{Checksum: true})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tswitch test.cs.Base() {\n\t\tcase minio.ChecksumCRC32C:\n\t\t\tcmpChecksum(st.ChecksumCRC32C, want)\n\t\tcase minio.ChecksumCRC32:\n\t\t\tcmpChecksum(st.ChecksumCRC32, want)\n\t\tcase minio.ChecksumSHA1:\n\t\t\tcmpChecksum(st.ChecksumSHA1, want)\n\t\tcase minio.ChecksumSHA256:\n\t\t\tcmpChecksum(st.ChecksumSHA256, want)\n\t\tcase minio.ChecksumCRC64NVME:\n\t\t\tcmpChecksum(st.ChecksumCRC64NVME, want)\n\t\t}\n\n\t\targs[\"section\"] = \"GetObjectAttributes\"\n\t\ts, err := c.GetObjectAttributes(context.Background(), bucketName, objectName, minio.ObjectAttributesOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif strings.ContainsRune(want, '-') {\n\t\t\twant = want[:strings.IndexByte(want, '-')]\n\t\t}\n\t\tswitch test.cs {\n\t\t// Full Object CRC does not return anything with GetObjectAttributes\n\t\tcase minio.ChecksumCRC32C:\n\t\t\tcmpChecksum(s.Checksum.ChecksumCRC32C, want)\n\t\tcase minio.ChecksumCRC32:\n\t\t\tcmpChecksum(s.Checksum.ChecksumCRC32, want)\n\t\tcase minio.ChecksumSHA1:\n\t\t\tcmpChecksum(s.Checksum.ChecksumSHA1, want)\n\t\tcase minio.ChecksumSHA256:\n\t\t\tcmpChecksum(s.Checksum.ChecksumSHA256, want)\n\t\t}\n\n\t\t// Read the data back\n\t\tgopts := minio.GetObjectOptions{Checksum: true}\n\t\tgopts.PartNumber = 2\n\n\t\t// We cannot use StatObject, since it ignores partnumber.\n\t\targs[\"section\"] = \"GetObject-Part\"\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tio.Copy(io.Discard, r)\n\t\tst, err = r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Test part 2 checksum...\n\t\th.Reset()\n\t\th.Write(b[partSize : 2*partSize])\n\t\twant = base64.StdEncoding.EncodeToString(h.Sum(nil))\n\n\t\tswitch test.cs {\n\t\t// Full Object CRC does not return any part CRC for whatever reason.\n\t\tcase minio.ChecksumCRC32C:\n\t\t\tcmpChecksum(st.ChecksumCRC32C, want)\n\t\tcase minio.ChecksumCRC32:\n\t\t\tcmpChecksum(st.ChecksumCRC32, want)\n\t\tcase minio.ChecksumSHA1:\n\t\t\tcmpChecksum(st.ChecksumSHA1, want)\n\t\tcase minio.ChecksumSHA256:\n\t\t\tcmpChecksum(st.ChecksumSHA256, want)\n\t\tcase minio.ChecksumCRC64NVME:\n\t\t\t// AWS doesn't return part checksum, but may in the future.\n\t\t\tif st.ChecksumCRC64NVME != \"\" {\n\t\t\t\tcmpChecksum(st.ChecksumCRC64NVME, want)\n\t\t\t}\n\t\t}\n\n\t\tdelete(args, \"metadata\")\n\t\tdelete(args, \"section\")\n\t\tlogSuccess(testName, function, args, startTime)\n\t}\n}\n\n// Test PutObject with trailing checksums.\nfunc testTrailingChecksums() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{UserMetadata: metadata, Progress: progress}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\thashMultiPart := func(b []byte, partSize int, hasher hash.Hash) string {\n\t\tr := bytes.NewReader(b)\n\t\ttmp := make([]byte, partSize)\n\t\tparts := 0\n\t\tvar all []byte\n\t\tfor {\n\t\t\tn, err := io.ReadFull(r, tmp)\n\t\t\tif err != nil && err != io.ErrUnexpectedEOF {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Calc crc failed\", err)\n\t\t\t}\n\t\t\tif n == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tparts++\n\t\t\thasher.Reset()\n\t\t\thasher.Write(tmp[:n])\n\t\t\tall = append(all, hasher.Sum(nil)...)\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\thasher.Reset()\n\t\thasher.Write(all)\n\t\treturn fmt.Sprintf(\"%s-%d\", base64.StdEncoding.EncodeToString(hasher.Sum(nil)), parts)\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\ttests := []struct {\n\t\theader string\n\t\thasher hash.Hash\n\n\t\t// Checksum values\n\t\tChecksumCRC32  string\n\t\tChecksumCRC32C string\n\t\tChecksumSHA1   string\n\t\tChecksumSHA256 string\n\t\tPO             minio.PutObjectOptions\n\t}{\n\t\t// Currently there is no way to override the checksum type.\n\t\t{\n\t\t\theader:         \"x-amz-checksum-crc32c\",\n\t\t\thasher:         crc32.New(crc32.MakeTable(crc32.Castagnoli)),\n\t\t\tChecksumCRC32C: \"set\",\n\t\t\tPO: minio.PutObjectOptions{\n\t\t\t\tDisableContentSha256: true,\n\t\t\t\tDisableMultipart:     false,\n\t\t\t\tUserMetadata:         nil,\n\t\t\t\tPartSize:             5 << 20,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\theader:         \"x-amz-checksum-crc32c\",\n\t\t\thasher:         crc32.New(crc32.MakeTable(crc32.Castagnoli)),\n\t\t\tChecksumCRC32C: \"set\",\n\t\t\tPO: minio.PutObjectOptions{\n\t\t\t\tDisableContentSha256: true,\n\t\t\t\tDisableMultipart:     false,\n\t\t\t\tUserMetadata:         nil,\n\t\t\t\tPartSize:             6_645_654, // Rather arbitrary size\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\theader:         \"x-amz-checksum-crc32c\",\n\t\t\thasher:         crc32.New(crc32.MakeTable(crc32.Castagnoli)),\n\t\t\tChecksumCRC32C: \"set\",\n\t\t\tPO: minio.PutObjectOptions{\n\t\t\t\tDisableContentSha256: false,\n\t\t\t\tDisableMultipart:     false,\n\t\t\t\tUserMetadata:         nil,\n\t\t\t\tPartSize:             5 << 20,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\theader:         \"x-amz-checksum-crc32c\",\n\t\t\thasher:         crc32.New(crc32.MakeTable(crc32.Castagnoli)),\n\t\t\tChecksumCRC32C: \"set\",\n\t\t\tPO: minio.PutObjectOptions{\n\t\t\t\tDisableContentSha256: false,\n\t\t\t\tDisableMultipart:     false,\n\t\t\t\tUserMetadata:         nil,\n\t\t\t\tPartSize:             6_645_654, // Rather arbitrary size\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tbufSize := dataFileMap[\"datafile-11-MB\"]\n\n\t\t// Save the data\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\tcmpChecksum := func(got, want string) {\n\t\t\tif want != got {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"checksum mismatch\", fmt.Errorf(\"want %q, got %q\", want, got))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\treader := getDataReader(\"datafile-11-MB\")\n\t\tb, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t\treader.Close()\n\t\th := test.hasher\n\t\th.Reset()\n\t\ttest.ChecksumCRC32C = hashMultiPart(b, int(test.PO.PartSize), test.hasher)\n\n\t\t// Set correct CRC.\n\t\tresp, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), test.PO)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\t// c.TraceOff()\n\t\tcmpChecksum(resp.ChecksumSHA256, test.ChecksumSHA256)\n\t\tcmpChecksum(resp.ChecksumSHA1, test.ChecksumSHA1)\n\t\tcmpChecksum(resp.ChecksumCRC32, test.ChecksumCRC32)\n\t\tcmpChecksum(resp.ChecksumCRC32C, test.ChecksumCRC32C)\n\n\t\t// Read the data back\n\t\tgopts := minio.GetObjectOptions{Checksum: true}\n\t\tgopts.PartNumber = 2\n\n\t\t// We cannot use StatObject, since it ignores partnumber.\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, gopts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tio.Copy(io.Discard, r)\n\t\tst, err := r.Stat()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Test part 2 checksum...\n\t\th.Reset()\n\t\tp2 := b[test.PO.PartSize:]\n\t\tif len(p2) > int(test.PO.PartSize) {\n\t\t\tp2 = p2[:test.PO.PartSize]\n\t\t}\n\t\th.Write(p2)\n\t\tgot := base64.StdEncoding.EncodeToString(h.Sum(nil))\n\t\tif test.ChecksumSHA256 != \"\" {\n\t\t\tcmpChecksum(st.ChecksumSHA256, got)\n\t\t}\n\t\tif test.ChecksumSHA1 != \"\" {\n\t\t\tcmpChecksum(st.ChecksumSHA1, got)\n\t\t}\n\t\tif test.ChecksumCRC32 != \"\" {\n\t\t\tcmpChecksum(st.ChecksumCRC32, got)\n\t\t}\n\t\tif test.ChecksumCRC32C != \"\" {\n\t\t\tcmpChecksum(st.ChecksumCRC32C, got)\n\t\t}\n\n\t\tdelete(args, \"metadata\")\n\t\tlogSuccess(testName, function, args, startTime)\n\t}\n}\n\n// Test PutObject with custom checksums.\nfunc testPutObjectWithAutomaticChecksums() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{UserMetadata: metadata, Progress: progress}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\ttests := []struct {\n\t\theader string\n\t\thasher hash.Hash\n\n\t\t// Checksum values\n\t\tChecksumCRC32  string\n\t\tChecksumCRC32C string\n\t\tChecksumSHA1   string\n\t\tChecksumSHA256 string\n\t}{\n\t\t// Built-in will only add crc32c, when no MD5 nor SHA256.\n\t\t{header: \"x-amz-checksum-crc32c\", hasher: crc32.New(crc32.MakeTable(crc32.Castagnoli))},\n\t}\n\n\t// defer c.TraceOff()\n\n\tfor i, test := range tests {\n\t\tbufSize := dataFileMap[\"datafile-10-kB\"]\n\n\t\t// Save the data\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\tcmpChecksum := func(got, want string) {\n\t\t\tif want != got {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"checksum mismatch\", fmt.Errorf(\"want %s, got %s\", want, got))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tmeta := map[string]string{}\n\t\treader := getDataReader(\"datafile-10-kB\")\n\t\tb, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\th := test.hasher\n\t\th.Reset()\n\t\th.Write(b)\n\t\tmeta[test.header] = base64.StdEncoding.EncodeToString(h.Sum(nil))\n\t\targs[\"metadata\"] = meta\n\n\t\tresp, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart:     true,\n\t\t\tUserMetadata:         nil,\n\t\t\tDisableContentSha256: true,\n\t\t\tSendContentMd5:       false,\n\t\t})\n\t\tif err == nil {\n\t\t\tif i == 0 && resp.ChecksumCRC32C == \"\" {\n\t\t\t\tlogIgnored(testName, function, args, startTime, \"Checksums does not appear to be supported by backend\")\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(resp.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(resp.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(resp.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(resp.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\n\t\t// Usually this will be the same as above, since we skip automatic checksum when SHA256 content is sent.\n\t\t// When/if we add a checksum control to PutObjectOptions this will make more sense.\n\t\tresp, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart:     true,\n\t\t\tUserMetadata:         nil,\n\t\t\tDisableContentSha256: false,\n\t\t\tSendContentMd5:       false,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\t// The checksum will not be enabled on HTTP, since it uses SHA256 blocks.\n\t\tif mustParseBool(os.Getenv(enableHTTPS)) {\n\t\t\tcmpChecksum(resp.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\t\tcmpChecksum(resp.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\t\tcmpChecksum(resp.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\t\tcmpChecksum(resp.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\t}\n\n\t\t// Set SHA256 header manually\n\t\tsh256 := sha256.Sum256(b)\n\t\tmeta = map[string]string{\"x-amz-checksum-sha256\": base64.StdEncoding.EncodeToString(sh256[:])}\n\t\targs[\"metadata\"] = meta\n\t\tresp, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(b), int64(bufSize), minio.PutObjectOptions{\n\t\t\tDisableMultipart:     true,\n\t\t\tUserMetadata:         meta,\n\t\t\tDisableContentSha256: true,\n\t\t\tSendContentMd5:       false,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tcmpChecksum(resp.ChecksumSHA256, meta[\"x-amz-checksum-sha256\"])\n\t\tcmpChecksum(resp.ChecksumSHA1, meta[\"x-amz-checksum-sha1\"])\n\t\tcmpChecksum(resp.ChecksumCRC32, meta[\"x-amz-checksum-crc32\"])\n\t\tcmpChecksum(resp.ChecksumCRC32C, meta[\"x-amz-checksum-crc32c\"])\n\t\tdelete(args, \"metadata\")\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testGetObjectAttributes() {\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObjectAttributes(ctx, bucketName, objectName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.ObjectAttributesOptions{}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\terr = c.MakeBucket(\n\t\tcontext.Background(),\n\t\tbucketName,\n\t\tminio.MakeBucketOptions{Region: \"us-east-1\"},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tbucketNameV := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-versioned-\")\n\targs[\"bucketName\"] = bucketNameV\n\terr = c.MakeBucket(\n\t\tcontext.Background(),\n\t\tbucketNameV,\n\t\tminio.MakeBucketOptions{Region: \"us-east-1\"},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\terr = c.EnableVersioning(context.Background(), bucketNameV)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unable to enable versioning\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\tdefer cleanupVersionedBucket(bucketNameV, c)\n\n\ttestFiles := make(map[string]*objectAttributesNewObject)\n\ttestFiles[\"file1\"] = &objectAttributesNewObject{\n\t\tObject:           \"file1\",\n\t\tObjectReaderType: \"datafile-1.03-MB\",\n\t\tBucket:           bucketNameV,\n\t\tContentType:      \"custom/contenttype\",\n\t\tSendContentMd5:   false,\n\t}\n\n\ttestFiles[\"file2\"] = &objectAttributesNewObject{\n\t\tObject:           \"file2\",\n\t\tObjectReaderType: \"datafile-129-MB\",\n\t\tBucket:           bucketName,\n\t\tContentType:      \"custom/contenttype\",\n\t\tSendContentMd5:   false,\n\t}\n\n\tfor i, v := range testFiles {\n\t\tbufSize := dataFileMap[v.ObjectReaderType]\n\n\t\treader := getDataReader(v.ObjectReaderType)\n\n\t\targs[\"objectName\"] = v.Object\n\t\ttestFiles[i].UploadInfo, err = c.PutObject(context.Background(), v.Bucket, v.Object, reader, int64(bufSize), minio.PutObjectOptions{\n\t\t\tContentType:    v.ContentType,\n\t\t\tSendContentMd5: v.SendContentMd5,\n\t\t\tChecksum:       minio.ChecksumCRC32C,\n\t\t})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\ttestTable := make(map[string]objectAttributesTableTest)\n\n\ttestTable[\"none-versioned\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:     \"file2\",\n\t\t\tStorageClass:     \"STANDARD\",\n\t\t\tHasFullChecksum:  true,\n\t\t\tHasPartChecksums: true,\n\t\t\tHasParts:         true,\n\t\t},\n\t}\n\n\ttestTable[\"0-to-0-marker\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{\n\t\t\tPartNumberMarker: 0,\n\t\t\tMaxParts:         0,\n\t\t},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:     \"file2\",\n\t\t\tStorageClass:     \"STANDARD\",\n\t\t\tHasFullChecksum:  true,\n\t\t\tHasPartChecksums: true,\n\t\t\tHasParts:         true,\n\t\t},\n\t}\n\n\ttestTable[\"0-marker-to-max\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{\n\t\t\tPartNumberMarker: 0,\n\t\t\tMaxParts:         10000,\n\t\t},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:     \"file2\",\n\t\t\tStorageClass:     \"STANDARD\",\n\t\t\tHasFullChecksum:  true,\n\t\t\tHasPartChecksums: true,\n\t\t\tHasParts:         true,\n\t\t},\n\t}\n\n\ttestTable[\"0-to-1-marker\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{\n\t\t\tPartNumberMarker: 0,\n\t\t\tMaxParts:         1,\n\t\t},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:     \"file2\",\n\t\t\tStorageClass:     \"STANDARD\",\n\t\t\tHasFullChecksum:  true,\n\t\t\tHasPartChecksums: true,\n\t\t\tHasParts:         true,\n\t\t},\n\t}\n\n\ttestTable[\"7-to-6-marker\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{\n\t\t\tPartNumberMarker: 7,\n\t\t\tMaxParts:         6,\n\t\t},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:     \"file2\",\n\t\t\tStorageClass:     \"STANDARD\",\n\t\t\tHasFullChecksum:  true,\n\t\t\tHasPartChecksums: true,\n\t\t\tHasParts:         true,\n\t\t},\n\t}\n\n\ttestTable[\"versioned\"] = objectAttributesTableTest{\n\t\topts: minio.ObjectAttributesOptions{},\n\t\ttest: objectAttributesTestOptions{\n\t\t\tTestFileName:    \"file1\",\n\t\t\tStorageClass:    \"STANDARD\",\n\t\t\tHasFullChecksum: true,\n\t\t},\n\t}\n\n\tfor i, v := range testTable {\n\n\t\ttf, ok := testFiles[v.test.TestFileName]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\targs[\"objectName\"] = tf.Object\n\t\targs[\"bucketName\"] = tf.Bucket\n\t\tif tf.UploadInfo.VersionID != \"\" {\n\t\t\tv.opts.VersionID = tf.UploadInfo.VersionID\n\t\t}\n\n\t\ts, err := c.GetObjectAttributes(context.Background(), tf.Bucket, tf.Object, v.opts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tv.test.NumberOfParts = s.ObjectParts.PartsCount\n\t\tv.test.ETag = tf.UploadInfo.ETag\n\t\tv.test.ObjectSize = int(tf.UploadInfo.Size)\n\n\t\terr = validateObjectAttributeRequest(s, &v.opts, &v.test)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Validating GetObjectsAttributes response failed, table test: \"+i, err)\n\t\t\treturn\n\t\t}\n\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testGetObjectAttributesSSECEncryption() {\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObjectAttributes(ctx, bucketName, objectName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.ObjectAttributesOptions{}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\terr = c.MakeBucket(\n\t\tcontext.Background(),\n\t\tbucketName,\n\t\tminio.MakeBucketOptions{Region: \"us-east-1\"},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tobjectName := \"encrypted-object\"\n\targs[\"objectName\"] = objectName\n\tbufSize := dataFileMap[\"datafile-11-MB\"]\n\treader := getDataReader(\"datafile-11-MB\")\n\n\tsse := encrypt.DefaultPBKDF([]byte(\"word1 word2 word3 word4\"), []byte(bucketName+objectName))\n\n\tinfo, err := c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{\n\t\tContentType:          \"content/custom\",\n\t\tSendContentMd5:       false,\n\t\tServerSideEncryption: sse,\n\t\tPartSize:             uint64(bufSize) / 2,\n\t\tChecksum:             minio.ChecksumCRC32C,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\topts := minio.ObjectAttributesOptions{\n\t\tServerSideEncryption: sse,\n\t}\n\tattr, err := c.GetObjectAttributes(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes with empty bucket name should have failed\", nil)\n\t\treturn\n\t}\n\terr = validateObjectAttributeRequest(attr, &opts, &objectAttributesTestOptions{\n\t\tTestFileName:     info.Key,\n\t\tETag:             info.ETag,\n\t\tNumberOfParts:    2,\n\t\tObjectSize:       int(info.Size),\n\t\tHasFullChecksum:  true,\n\t\tHasParts:         true,\n\t\tHasPartChecksums: true,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Validating GetObjectsAttributes response failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testGetObjectAttributesErrorCases() {\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObjectAttributes(ctx, bucketName, objectName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.ObjectAttributesOptions{}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{TrailingHeaders: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\tunknownBucket := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-bucket-\")\n\tunknownObject := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-object-\")\n\n\t_, err = c.GetObjectAttributes(context.Background(), unknownBucket, unknownObject, minio.ObjectAttributesOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes failed\", nil)\n\t\treturn\n\t}\n\n\terrorResponse := err.(minio.ErrorResponse)\n\tif errorResponse.Code != \"NoSuchBucket\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error code, expected NoSuchBucket but got \"+errorResponse.Code, nil)\n\t\treturn\n\t}\n\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\terr = c.MakeBucket(\n\t\tcontext.Background(),\n\t\tbucketName,\n\t\tminio.MakeBucketOptions{Region: \"us-east-1\"},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tbucketNameV := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-versioned-\")\n\targs[\"bucketName\"] = bucketNameV\n\terr = c.MakeBucket(\n\t\tcontext.Background(),\n\t\tbucketNameV,\n\t\tminio.MakeBucketOptions{Region: \"us-east-1\"},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\terr = c.EnableVersioning(context.Background(), bucketNameV)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\tdefer cleanupVersionedBucket(bucketNameV, c)\n\n\t_, err = c.GetObjectAttributes(context.Background(), bucketName, unknownObject, minio.ObjectAttributesOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes failed\", nil)\n\t\treturn\n\t}\n\n\terrorResponse = err.(minio.ErrorResponse)\n\tif errorResponse.Code != \"NoSuchKey\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error code, expected NoSuchKey but got \"+errorResponse.Code, nil)\n\t\treturn\n\t}\n\n\t_, err = c.GetObjectAttributes(context.Background(), bucketName, \"\", minio.ObjectAttributesOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes with empty object name should have failed\", nil)\n\t\treturn\n\t}\n\n\t_, err = c.GetObjectAttributes(context.Background(), \"\", unknownObject, minio.ObjectAttributesOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes with empty bucket name should have failed\", nil)\n\t\treturn\n\t}\n\n\t_, err = c.GetObjectAttributes(context.Background(), bucketNameV, unknownObject, minio.ObjectAttributesOptions{\n\t\tVersionID: uuid.NewString(),\n\t})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectAttributes with empty bucket name should have failed\", nil)\n\t\treturn\n\t}\n\terrorResponse = err.(minio.ErrorResponse)\n\tif errorResponse.Code != \"NoSuchVersion\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error code, expected NoSuchVersion but got \"+errorResponse.Code, nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\ntype objectAttributesNewObject struct {\n\tObject           string\n\tObjectReaderType string\n\tBucket           string\n\tContentType      string\n\tSendContentMd5   bool\n\tUploadInfo       minio.UploadInfo\n}\n\ntype objectAttributesTableTest struct {\n\topts minio.ObjectAttributesOptions\n\ttest objectAttributesTestOptions\n}\n\ntype objectAttributesTestOptions struct {\n\tTestFileName     string\n\tETag             string\n\tNumberOfParts    int\n\tStorageClass     string\n\tObjectSize       int\n\tHasPartChecksums bool\n\tHasFullChecksum  bool\n\tHasParts         bool\n}\n\nfunc validateObjectAttributeRequest(OA *minio.ObjectAttributes, opts *minio.ObjectAttributesOptions, test *objectAttributesTestOptions) (err error) {\n\tif opts.VersionID != \"\" {\n\t\tif OA.VersionID != opts.VersionID {\n\t\t\terr = fmt.Errorf(\"Expected versionId %s but got versionId %s\", opts.VersionID, OA.VersionID)\n\t\t\treturn\n\t\t}\n\t}\n\n\tpartsMissingChecksum := false\n\tfoundPartChecksum := false\n\tfor _, v := range OA.ObjectParts.Parts {\n\t\tchecksumFound := false\n\t\tif v.ChecksumSHA256 != \"\" {\n\t\t\tchecksumFound = true\n\t\t} else if v.ChecksumSHA1 != \"\" {\n\t\t\tchecksumFound = true\n\t\t} else if v.ChecksumCRC32 != \"\" {\n\t\t\tchecksumFound = true\n\t\t} else if v.ChecksumCRC32C != \"\" {\n\t\t\tchecksumFound = true\n\t\t}\n\t\tif !checksumFound {\n\t\t\tpartsMissingChecksum = true\n\t\t} else {\n\t\t\tfoundPartChecksum = true\n\t\t}\n\t}\n\n\tif test.HasPartChecksums {\n\t\tif partsMissingChecksum {\n\t\t\terr = fmt.Errorf(\"One or all parts were missing a checksum\")\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tif foundPartChecksum {\n\t\t\terr = fmt.Errorf(\"Did not expect ObjectParts to have checksums but found one\")\n\t\t\treturn\n\t\t}\n\t}\n\n\thasFullObjectChecksum := (OA.Checksum.ChecksumCRC32 != \"\" ||\n\t\tOA.Checksum.ChecksumCRC32C != \"\" ||\n\t\tOA.Checksum.ChecksumSHA1 != \"\" ||\n\t\tOA.Checksum.ChecksumSHA256 != \"\")\n\n\tif test.HasFullChecksum {\n\t\tif !hasFullObjectChecksum {\n\t\t\terr = fmt.Errorf(\"Full object checksum not found\")\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tif hasFullObjectChecksum {\n\t\t\terr = fmt.Errorf(\"Did not expect a full object checksum but we got one\")\n\t\t\treturn\n\t\t}\n\t}\n\n\tif OA.ETag != test.ETag {\n\t\terr = fmt.Errorf(\"Etags do not match, got %s but expected %s\", OA.ETag, test.ETag)\n\t\treturn\n\t}\n\n\tif test.HasParts {\n\t\tif len(OA.ObjectParts.Parts) < 1 {\n\t\t\terr = fmt.Errorf(\"Was expecting ObjectParts but none were present\")\n\t\t\treturn\n\t\t}\n\t}\n\n\tif OA.StorageClass == \"\" {\n\t\terr = fmt.Errorf(\"Was expecting a StorageClass but got none\")\n\t\treturn\n\t}\n\n\tif OA.ObjectSize != test.ObjectSize {\n\t\terr = fmt.Errorf(\"Was expecting a ObjectSize but got none\")\n\t\treturn\n\t}\n\n\tif test.HasParts {\n\t\tif opts.MaxParts == 0 {\n\t\t\tif len(OA.ObjectParts.Parts) != OA.ObjectParts.PartsCount {\n\t\t\t\terr = fmt.Errorf(\"expected %s parts but got %d\", OA.ObjectParts.PartsCount, len(OA.ObjectParts.Parts))\n\t\t\t\treturn\n\t\t\t}\n\t\t} else if (opts.MaxParts + opts.PartNumberMarker) > OA.ObjectParts.PartsCount {\n\t\t\tif len(OA.ObjectParts.Parts) != (OA.ObjectParts.PartsCount - opts.PartNumberMarker) {\n\t\t\t\terr = fmt.Errorf(\"expected %d parts but got %d\", (OA.ObjectParts.PartsCount - opts.PartNumberMarker), len(OA.ObjectParts.Parts))\n\t\t\t\treturn\n\t\t\t}\n\t\t} else if opts.MaxParts != 0 {\n\t\t\tif opts.MaxParts != len(OA.ObjectParts.Parts) {\n\t\t\t\terr = fmt.Errorf(\"expected %d parts but got %d\", opts.MaxParts, len(OA.ObjectParts.Parts))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tif OA.ObjectParts.NextPartNumberMarker == OA.ObjectParts.PartsCount {\n\t\tif OA.ObjectParts.IsTruncated {\n\t\t\terr = fmt.Errorf(\"Expected ObjectParts to NOT be truncated, but it was\")\n\t\t\treturn\n\t\t}\n\t}\n\n\tif OA.ObjectParts.NextPartNumberMarker != OA.ObjectParts.PartsCount {\n\t\tif !OA.ObjectParts.IsTruncated {\n\t\t\terr = fmt.Errorf(\"Expected ObjectParts to be truncated, but it was NOT\")\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\n// Test PutObject using a large data to trigger multipart readat\nfunc testPutObjectWithMetadata() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{UserMetadata: metadata, Progress: progress}\",\n\t}\n\n\tif !isFullMode() {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipping functional tests for short/quick runs\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Make bucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Object custom metadata\n\tcustomContentType := \"custom/contenttype\"\n\n\targs[\"metadata\"] = map[string][]string{\n\t\t\"Content-Type\":         {customContentType},\n\t\t\"X-Amz-Meta-CustomKey\": {\"extra  spaces  in   value\"},\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{\n\t\tContentType: customContentType,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes returned by PutObject does not match GetObject, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\tif st.ContentType != customContentType && st.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"ContentType does not match, expected \"+customContentType+\" got \"+st.ContentType, err)\n\t\treturn\n\t}\n\tif err := crcMatchesName(r, \"datafile-129-MB\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"data CRC check failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object Close failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object already closed, should respond with error\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testPutObjectWithContentLanguage() {\n\t// initialize logging params\n\tobjectName := \"test-object\"\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": objectName,\n\t\t\"size\":       -1,\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tdata := []byte{}\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(data), int64(0), minio.PutObjectOptions{\n\t\tContentLanguage: \"en\",\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tobjInfo, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Metadata.Get(\"Content-Language\") != \"en\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected content-language 'en' doesn't match with StatObject return value\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test put object with streaming signature.\nfunc testPutObjectStreaming() {\n\t// initialize logging params\n\tobjectName := \"test-object\"\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size,opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": objectName,\n\t\t\"size\":       -1,\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload an object.\n\tsizes := []int64{0, 64*1024 - 1, 64 * 1024}\n\n\tfor _, size := range sizes {\n\t\tdata := newRandomReader(size, size)\n\t\tui, err := c.PutObject(context.Background(), bucketName, objectName, data, int64(size), minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectStreaming failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif ui.Size != size {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectStreaming result has unexpected size\", nil)\n\t\t\treturn\n\t\t}\n\n\t\tobjInfo, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif objInfo.Size != size {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected size\", err)\n\t\t\treturn\n\t\t}\n\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object seeker from the end, using whence set to '2'.\nfunc testGetObjectSeekEnd() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes read does not match, expected \"+string(int64(bufSize))+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tpos, err := r.Seek(-100, 2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object Seek failed\", err)\n\t\treturn\n\t}\n\tif pos != st.Size-100 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect position\", err)\n\t\treturn\n\t}\n\tbuf2 := make([]byte, 100)\n\tm, err := readFull(r, buf2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Error reading through readFull\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes dont match, expected \"+string(len(buf2))+\" got \"+string(m), err)\n\t\treturn\n\t}\n\thexBuf1 := fmt.Sprintf(\"%02x\", buf[len(buf)-100:])\n\thexBuf2 := fmt.Sprintf(\"%02x\", buf2[:m])\n\tif hexBuf1 != hexBuf2 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Values at same index dont match\", err)\n\t\treturn\n\t}\n\tpos, err = r.Seek(-100, 2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object Seek failed\", err)\n\t\treturn\n\t}\n\tif pos != st.Size-100 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect position\", err)\n\t\treturn\n\t}\n\tif err = r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ObjectClose failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object reader to not throw error on being closed twice.\nfunc testGetObjectClosedTwice() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\tif err := crcMatchesName(r, \"datafile-33-kB\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"data CRC check failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object Close failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Already closed object. No error returned\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test RemoveObjects request where context cancels after timeout\nfunc testRemoveObjectsContext() {\n\t// Initialize logging params.\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"RemoveObjects(ctx, bucketName, objectsCh)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\t// Instantiate new minio client.\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate put data.\n\tr := bytes.NewReader(bytes.Repeat([]byte(\"a\"), 8))\n\n\t// Multi remove of 20 objects.\n\tnrObjects := 20\n\tobjectsCh := make(chan minio.ObjectInfo)\n\tgo func() {\n\t\tdefer close(objectsCh)\n\t\tfor i := 0; i < nrObjects; i++ {\n\t\t\tobjectName := \"sample\" + strconv.Itoa(i) + \".txt\"\n\t\t\tinfo, err := c.PutObject(context.Background(), bucketName, objectName, r, 8,\n\t\t\t\tminio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tobjectsCh <- minio.ObjectInfo{\n\t\t\t\tKey:       info.Key,\n\t\t\t\tVersionID: info.VersionID,\n\t\t\t}\n\t\t}\n\t}()\n\t// Set context to cancel in 1 nanosecond.\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Call RemoveObjects API with short timeout.\n\terrorCh := c.RemoveObjects(ctx, bucketName, objectsCh, minio.RemoveObjectsOptions{})\n\t// Check for error.\n\tselect {\n\tcase r := <-errorCh:\n\t\tif r.Err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObjects should fail on short timeout\", err)\n\t\t\treturn\n\t\t}\n\t}\n\t// Set context with longer timeout.\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\t// Perform RemoveObjects with the longer timeout. Expect the removals to succeed.\n\terrorCh = c.RemoveObjects(ctx, bucketName, objectsCh, minio.RemoveObjectsOptions{})\n\tselect {\n\tcase r, more := <-errorCh:\n\t\tif more || r.Err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error\", r.Err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test removing multiple objects with Remove API\nfunc testRemoveMultipleObjects() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"RemoveObjects(bucketName, objectsCh)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tr := bytes.NewReader(bytes.Repeat([]byte(\"a\"), 8))\n\n\t// Multi remove of 1100 objects\n\tnrObjects := 200\n\n\tobjectsCh := make(chan minio.ObjectInfo)\n\n\tgo func() {\n\t\tdefer close(objectsCh)\n\t\t// Upload objects and send them to objectsCh\n\t\tfor i := 0; i < nrObjects; i++ {\n\t\t\tobjectName := \"sample\" + strconv.Itoa(i) + \".txt\"\n\t\t\tinfo, err := c.PutObject(context.Background(), bucketName, objectName, r, 8,\n\t\t\t\tminio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tobjectsCh <- minio.ObjectInfo{\n\t\t\t\tKey:       info.Key,\n\t\t\t\tVersionID: info.VersionID,\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Call RemoveObjects API\n\terrorCh := c.RemoveObjects(context.Background(), bucketName, objectsCh, minio.RemoveObjectsOptions{})\n\n\t// Check if errorCh doesn't receive any error\n\tselect {\n\tcase r, more := <-errorCh:\n\t\tif more {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error\", r.Err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test removing multiple objects and check for results\nfunc testRemoveMultipleObjectsWithResult() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"RemoveObjects(bucketName, objectsCh)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupVersionedBucket(bucketName, c)\n\n\tr := bytes.NewReader(bytes.Repeat([]byte(\"a\"), 8))\n\n\tnrObjects := 10\n\tnrLockedObjects := 5\n\n\tobjectsCh := make(chan minio.ObjectInfo)\n\n\tgo func() {\n\t\tdefer close(objectsCh)\n\t\t// Upload objects and send them to objectsCh\n\t\tfor i := 0; i < nrObjects; i++ {\n\t\t\tobjectName := \"sample\" + strconv.Itoa(i) + \".txt\"\n\t\t\tinfo, err := c.PutObject(context.Background(), bucketName, objectName, r, 8,\n\t\t\t\tminio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif i < nrLockedObjects {\n\t\t\t\t// t := time.Date(2130, time.April, 25, 14, 0, 0, 0, time.UTC)\n\t\t\t\tt := time.Now().Add(5 * time.Minute)\n\t\t\t\tm := minio.RetentionMode(minio.Governance)\n\t\t\t\topts := minio.PutObjectRetentionOptions{\n\t\t\t\t\tGovernanceBypass: false,\n\t\t\t\t\tRetainUntilDate:  &t,\n\t\t\t\t\tMode:             &m,\n\t\t\t\t\tVersionID:        info.VersionID,\n\t\t\t\t}\n\t\t\t\terr = c.PutObjectRetention(context.Background(), bucketName, objectName, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Error setting retention\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tobjectsCh <- minio.ObjectInfo{\n\t\t\t\tKey:       info.Key,\n\t\t\t\tVersionID: info.VersionID,\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Call RemoveObjects API\n\tresultCh := c.RemoveObjectsWithResult(context.Background(), bucketName, objectsCh, minio.RemoveObjectsOptions{})\n\n\tvar foundNil, foundErr int\n\n\tfor {\n\t\t// Check if errorCh doesn't receive any error\n\t\tselect {\n\t\tcase deleteRes, ok := <-resultCh:\n\t\t\tif !ok {\n\t\t\t\tgoto out\n\t\t\t}\n\t\t\tif deleteRes.ObjectName == \"\" {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected object name\", nil)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif deleteRes.ObjectVersionID == \"\" {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected object version ID\", nil)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif deleteRes.Err == nil {\n\t\t\t\tfoundNil++\n\t\t\t} else {\n\t\t\t\tfoundErr++\n\t\t\t}\n\t\t}\n\t}\nout:\n\tif foundNil+foundErr != nrObjects {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of results\", nil)\n\t\treturn\n\t}\n\n\tif foundNil != nrObjects-nrLockedObjects {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of nil errors\", nil)\n\t\treturn\n\t}\n\n\tif foundErr != nrLockedObjects {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected number of errors\", nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests FPutObject of a big file to trigger multipart\nfunc testFPutObjectMultipart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutObject(bucketName, objectName, fileName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload 4 parts to utilize all 3 'workers' in multipart and still have a part to upload.\n\tfileName := getMintDataDirFilePath(\"datafile-129-MB\")\n\tif fileName == \"\" {\n\t\t// Make a temp file with minPartSize bytes of data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"FPutObjectTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\t\treturn\n\t\t}\n\t\t// Upload 2 parts to utilize all 3 'workers' in multipart and still have a part to upload.\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-129-MB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File Close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tfileName = file.Name()\n\t\targs[\"fileName\"] = fileName\n\t}\n\ttotalSize := dataFileMap[\"datafile-129-MB\"]\n\t// Set base object name\n\tobjectName := bucketName + \"FPutObject\" + \"-standard\"\n\targs[\"objectName\"] = objectName\n\n\tobjectContentType := \"testapplication/octet-stream\"\n\targs[\"objectContentType\"] = objectContentType\n\n\t// Perform standard FPutObject with contentType provided (Expecting application/octet-stream)\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName, fileName, minio.PutObjectOptions{ContentType: objectContentType})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tobjInfo, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected error\", err)\n\t\treturn\n\t}\n\tif objInfo.Size != int64(totalSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(int64(totalSize))+\" got \"+string(objInfo.Size), err)\n\t\treturn\n\t}\n\tif objInfo.ContentType != objectContentType && objInfo.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"ContentType doesn't match\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests FPutObject with null contentType (default = application/octet-stream)\nfunc testFPutObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutObject(bucketName, objectName, fileName, opts)\"\n\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\tlocation := \"us-east-1\"\n\n\t// Make a new bucket.\n\targs[\"bucketName\"] = bucketName\n\targs[\"location\"] = location\n\tfunction = \"MakeBucket(bucketName, location)\"\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: location})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload 3 parts worth of data to use all 3 of multiparts 'workers' and have an extra part.\n\t// Use different data in part for multipart tests to check parts are uploaded in correct order.\n\tfName := getMintDataDirFilePath(\"datafile-129-MB\")\n\tif fName == \"\" {\n\t\t// Make a temp file with minPartSize bytes of data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"FPutObjectTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Upload 3 parts to utilize all 3 'workers' in multipart and still have a part to upload.\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-129-MB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File copy failed\", err)\n\t\t\treturn\n\t\t}\n\t\t// Close the file pro-actively for windows.\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.Remove(file.Name())\n\t\tfName = file.Name()\n\t}\n\n\t// Set base object name\n\tfunction = \"FPutObject(bucketName, objectName, fileName, opts)\"\n\tobjectName := bucketName + \"FPutObject\"\n\targs[\"objectName\"] = objectName + \"-standard\"\n\targs[\"fileName\"] = fName\n\targs[\"opts\"] = minio.PutObjectOptions{ContentType: \"application/octet-stream\"}\n\n\t// Perform standard FPutObject with contentType provided (Expecting application/octet-stream)\n\tui, err := c.FPutObject(context.Background(), bucketName, objectName+\"-standard\", fName, minio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\tif ui.Size != int64(dataFileMap[\"datafile-129-MB\"]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject returned an unexpected upload size\", err)\n\t\treturn\n\t}\n\n\t// Perform FPutObject with no contentType provided (Expecting application/octet-stream)\n\targs[\"objectName\"] = objectName + \"-Octet\"\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName+\"-Octet\", fName, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File close failed\", err)\n\t\treturn\n\t}\n\n\tsrcFile, err := os.Open(fName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File open failed\", err)\n\t\treturn\n\t}\n\tdefer srcFile.Close()\n\t// Add extension to temp file name\n\ttmpFile, err := os.Create(fName + \".gtar\")\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File create failed\", err)\n\t\treturn\n\t}\n\t_, err = io.Copy(tmpFile, srcFile)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File copy failed\", err)\n\t\treturn\n\t}\n\ttmpFile.Close()\n\n\t// Perform FPutObject with no contentType provided (Expecting application/x-gtar)\n\targs[\"objectName\"] = objectName + \"-GTar\"\n\targs[\"opts\"] = minio.PutObjectOptions{}\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName+\"-GTar\", fName+\".gtar\", minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Check headers\n\tfunction = \"StatObject(bucketName, objectName, opts)\"\n\targs[\"objectName\"] = objectName + \"-standard\"\n\trStandard, err := c.StatObject(context.Background(), bucketName, objectName+\"-standard\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif rStandard.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"ContentType does not match, expected application/octet-stream, got \"+rStandard.ContentType, err)\n\t\treturn\n\t}\n\n\tfunction = \"StatObject(bucketName, objectName, opts)\"\n\targs[\"objectName\"] = objectName + \"-Octet\"\n\trOctet, err := c.StatObject(context.Background(), bucketName, objectName+\"-Octet\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif rOctet.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"ContentType does not match, expected application/octet-stream, got \"+rOctet.ContentType, err)\n\t\treturn\n\t}\n\n\tfunction = \"StatObject(bucketName, objectName, opts)\"\n\targs[\"objectName\"] = objectName + \"-GTar\"\n\trGTar, err := c.StatObject(context.Background(), bucketName, objectName+\"-GTar\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif rGTar.ContentType != \"application/x-gtar\" && rGTar.ContentType != \"application/octet-stream\" && rGTar.ContentType != \"application/x-tar\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"ContentType does not match, expected application/x-tar or application/octet-stream, got \"+rGTar.ContentType, err)\n\t\treturn\n\t}\n\n\tos.Remove(fName + \".gtar\")\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests FPutObject request when context cancels after timeout\nfunc testFPutObjectContext() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutObject(bucketName, objectName, fileName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload 1 parts worth of data to use multipart upload.\n\t// Use different data in part for multipart tests to check parts are uploaded in correct order.\n\tfName := getMintDataDirFilePath(\"datafile-1-MB\")\n\tif fName == \"\" {\n\t\t// Make a temp file with 1 MiB bytes of data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"FPutObjectContextTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Upload 1 parts to trigger multipart upload\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-1-MB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File copy failed\", err)\n\t\t\treturn\n\t\t}\n\t\t// Close the file pro-actively for windows.\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.Remove(file.Name())\n\t\tfName = file.Name()\n\t}\n\n\t// Set base object name\n\tobjectName := bucketName + \"FPutObjectContext\"\n\targs[\"objectName\"] = objectName\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Perform FPutObject with contentType provided (Expecting application/octet-stream)\n\t_, err = c.FPutObject(ctx, bucketName, objectName+\"-Shorttimeout\", fName, minio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\tdefer cancel()\n\t// Perform FPutObject with a long timeout. Expect the put object to succeed\n\t_, err = c.FPutObject(ctx, bucketName, objectName+\"-Longtimeout\", fName, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject shouldn't fail on long timeout\", err)\n\t\treturn\n\t}\n\n\t_, err = c.StatObject(context.Background(), bucketName, objectName+\"-Longtimeout\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests FPutObject request when context cancels after timeout\nfunc testFPutObjectContextV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutObjectContext(ctx, bucketName, objectName, fileName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"minio.PutObjectOptions{ContentType:objectContentType}\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload 1 parts worth of data to use multipart upload.\n\t// Use different data in part for multipart tests to check parts are uploaded in correct order.\n\tfName := getMintDataDirFilePath(\"datafile-1-MB\")\n\tif fName == \"\" {\n\t\t// Make a temp file with 1 MiB bytes of data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"FPutObjectContextTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Temp file creation failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Upload 1 parts to trigger multipart upload\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-1-MB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File copy failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Close the file pro-actively for windows.\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.Remove(file.Name())\n\t\tfName = file.Name()\n\t}\n\n\t// Set base object name\n\tobjectName := bucketName + \"FPutObjectContext\"\n\targs[\"objectName\"] = objectName\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Perform FPutObject with contentType provided (Expecting application/octet-stream)\n\t_, err = c.FPutObject(ctx, bucketName, objectName+\"-Shorttimeout\", fName, minio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\tdefer cancel()\n\t// Perform FPutObject with a long timeout. Expect the put object to succeed\n\t_, err = c.FPutObject(ctx, bucketName, objectName+\"-Longtimeout\", fName, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject shouldn't fail on longer timeout\", err)\n\t\treturn\n\t}\n\n\t_, err = c.StatObject(context.Background(), bucketName, objectName+\"-Longtimeout\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test validates putObject with context to see if request cancellation is honored.\nfunc testPutObjectContext() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(ctx, bucketName, objectName, fileName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket call failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\tobjectName := fmt.Sprintf(\"test-file-%v\", rand.Uint32())\n\targs[\"objectName\"] = objectName\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\tcancel()\n\targs[\"ctx\"] = ctx\n\targs[\"opts\"] = minio.PutObjectOptions{ContentType: \"binary/octet-stream\"}\n\n\t_, err = c.PutObject(ctx, bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\targs[\"ctx\"] = ctx\n\n\tdefer cancel()\n\treader = getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\t_, err = c.PutObject(ctx, bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject with long timeout failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests get object with s3zip extensions.\nfunc testGetObjectS3Zip() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{\"x-minio-extract\": true}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\t// Delete all objects and buckets\n\t\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\") + \".zip\"\n\targs[\"objectName\"] = objectName\n\n\tvar zipFile bytes.Buffer\n\tzw := zip.NewWriter(&zipFile)\n\trng := rand.New(rand.NewSource(0xc0cac01a))\n\tconst nFiles = 500\n\tfor i := 0; i <= nFiles; i++ {\n\t\tif i == nFiles {\n\t\t\t// Make one large, compressible file.\n\t\t\ti = 1000000\n\t\t}\n\t\tb := make([]byte, i)\n\t\tif i < nFiles {\n\t\t\trng.Read(b)\n\t\t}\n\t\twc, err := zw.Create(fmt.Sprintf(\"test/small/file-%d.bin\", i))\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"zw.Create failed\", err)\n\t\t\treturn\n\t\t}\n\t\twc.Write(b)\n\t}\n\terr = zw.Close()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"zw.Close failed\", err)\n\t\treturn\n\t}\n\tbuf := zipFile.Bytes()\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat object failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(len(buf))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\tr.Close()\n\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"zip.NewReader failed\", err)\n\t\treturn\n\t}\n\tlOpts := minio.ListObjectsOptions{}\n\tlOpts.Set(\"x-minio-extract\", \"true\")\n\tlOpts.Prefix = objectName + \"/\"\n\tlOpts.Recursive = true\n\tlist := c.ListObjects(context.Background(), bucketName, lOpts)\n\tlisted := map[string]minio.ObjectInfo{}\n\tfor item := range list {\n\t\tif item.Err != nil {\n\t\t\tbreak\n\t\t}\n\t\tlisted[item.Key] = item\n\t}\n\tif len(listed) == 0 {\n\t\t// Assume we are running against non-minio.\n\t\targs[\"SKIPPED\"] = true\n\t\tlogIgnored(testName, function, args, startTime, \"s3zip does not appear to be present\")\n\t\treturn\n\t}\n\n\tfor _, file := range zr.File {\n\t\tif file.FileInfo().IsDir() {\n\t\t\tcontinue\n\t\t}\n\t\targs[\"zipfile\"] = file.Name\n\t\tzfr, err := file.Open()\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file.Open failed\", err)\n\t\t\treturn\n\t\t}\n\t\twant, err := io.ReadAll(zfr)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"fzip file read failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\topts := minio.GetObjectOptions{}\n\t\topts.Set(\"x-minio-extract\", \"true\")\n\t\tkey := path.Join(objectName, file.Name)\n\t\tr, err = c.GetObject(context.Background(), bucketName, key, opts)\n\t\tif err != nil {\n\t\t\tterr := minio.ToErrorResponse(err)\n\t\t\tif terr.StatusCode != http.StatusNotFound {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tgot, err := io.ReadAll(r)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\t\treturn\n\t\t}\n\t\tr.Close()\n\t\tif !bytes.Equal(want, got) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Content mismatch\", err)\n\t\t\treturn\n\t\t}\n\t\toi, ok := listed[key]\n\t\tif !ok {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object Missing\", fmt.Errorf(\"%s not present in listing\", key))\n\t\t\treturn\n\t\t}\n\t\tif int(oi.Size) != len(got) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Object Size Incorrect\", fmt.Errorf(\"listing %d, read %d\", oi.Size, len(got)))\n\t\t\treturn\n\t\t}\n\t\tdelete(listed, key)\n\t}\n\tdelete(args, \"zipfile\")\n\tif len(listed) > 0 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Extra listed objects\", fmt.Errorf(\"left over: %v\", listed))\n\t\treturn\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests get object ReaderSeeker interface methods.\nfunc testGetObjectReadSeekFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\t// Delete all objects and buckets\n\t\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat object failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\t// This following function helps us to compare data from the reader after seek\n\t// with the data from the original buffer\n\tcmpData := func(r io.Reader, start, end int) {\n\t\tif end-start == 0 {\n\t\t\treturn\n\t\t}\n\t\tbuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err := io.CopyN(buffer, r, int64(bufSize)); err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyN failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif !bytes.Equal(buf[start:end], buffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read bytes v/s original buffer\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Generic seek error for errors other than io.EOF\n\tseekErr := errors.New(\"seek error\")\n\n\ttestCases := []struct {\n\t\toffset    int64\n\t\twhence    int\n\t\tpos       int64\n\t\terr       error\n\t\tshouldCmp bool\n\t\tstart     int\n\t\tend       int\n\t}{\n\t\t// Start from offset 0, fetch data and compare\n\t\t{0, 0, 0, nil, true, 0, 0},\n\t\t// Start from offset 2048, fetch data and compare\n\t\t{2048, 0, 2048, nil, true, 2048, bufSize},\n\t\t// Start from offset larger than possible\n\t\t{int64(bufSize) + 1024, 0, 0, seekErr, false, 0, 0},\n\t\t// Move to offset 0 without comparing\n\t\t{0, 0, 0, nil, false, 0, 0},\n\t\t// Move one step forward and compare\n\t\t{1, 1, 1, nil, true, 1, bufSize},\n\t\t// Move larger than possible\n\t\t{int64(bufSize), 1, 0, seekErr, false, 0, 0},\n\t\t// Provide negative offset with CUR_SEEK\n\t\t{int64(-1), 1, 0, seekErr, false, 0, 0},\n\t\t// Test with whence SEEK_END and with positive offset\n\t\t{1024, 2, int64(bufSize) - 1024, io.EOF, true, 0, 0},\n\t\t// Test with whence SEEK_END and with negative offset\n\t\t{-1024, 2, int64(bufSize) - 1024, nil, true, bufSize - 1024, bufSize},\n\t\t// Test with whence SEEK_END and with large negative offset\n\t\t{-int64(bufSize) * 2, 2, 0, seekErr, true, 0, 0},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Perform seek operation\n\t\tn, err := r.Seek(testCase.offset, testCase.whence)\n\t\t// We expect an error\n\t\tif testCase.err == seekErr && err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", unexpected err value: expected: \"+testCase.err.Error()+\", found: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\t// We expect a specific error\n\t\tif testCase.err != seekErr && testCase.err != err {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", unexpected err value: expected: \"+testCase.err.Error()+\", found: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\t// If we expect an error go to the next loop\n\t\tif testCase.err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t// Check the returned seek pos\n\t\tif n != testCase.pos {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", number of bytes seeked does not match, expected \"+string(testCase.pos)+\", got \"+string(n), err)\n\t\t\treturn\n\t\t}\n\t\t// Compare only if shouldCmp is activated\n\t\tif testCase.shouldCmp {\n\t\t\tcmpData(r, testCase.start, testCase.end)\n\t\t}\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests get object ReaderAt interface methods.\nfunc testGetObjectReadAtFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\toffset := int64(2048)\n\n\t// read directly\n\tbuf1 := make([]byte, 512)\n\tbuf2 := make([]byte, 512)\n\tbuf3 := make([]byte, 512)\n\tbuf4 := make([]byte, 512)\n\n\t// Test readAt before stat is called such that objectInfo doesn't change.\n\tm, err := r.ReadAt(buf1, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf1) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf1))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf1, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tm, err = r.ReadAt(buf2, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf2))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf2, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\n\toffset += 512\n\tm, err = r.ReadAt(buf3, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf3) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf3))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf3, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf4, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf4) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf4))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf4, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\n\tbuf5 := make([]byte, len(buf))\n\t// Read the whole object.\n\tm, err = r.ReadAt(buf5, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m != len(buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf5))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf, buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect data read in GetObject, than what was previously uploaded\", err)\n\t\treturn\n\t}\n\n\tbuf6 := make([]byte, len(buf)+1)\n\t// Read the whole object and beyond.\n\t_, err = r.ReadAt(buf6, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Reproduces issue https://github.com/minio/minio-go/issues/1137\nfunc testGetObjectReadAtWhenEOFWasReached() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// read directly\n\tbuf1 := make([]byte, len(buf))\n\tbuf2 := make([]byte, 512)\n\n\tm, err := r.Read(buf1)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Read failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m != len(buf1) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read read shorter bytes before reaching EOF, expected \"+string(len(buf1))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf1, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect count of Read data\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tm, err = r.ReadAt(buf2, 512)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf2))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf2, buf[512:1024]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect count of ReadAt data\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test Presigned Post Policy\nfunc testPresignedPostPolicy() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PresignedPostPolicy(policy)\"\n\targs := map[string]interface{}{\n\t\t\"policy\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t// Azure requires the key to not start with a number\n\tmetadataKey := randString(60, rand.NewSource(time.Now().UnixNano()), \"user\")\n\tmetadataValue := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\tpolicy := minio.NewPostPolicy()\n\tpolicy.SetBucket(bucketName)\n\tpolicy.SetKey(objectName)\n\tpolicy.SetExpires(time.Now().UTC().AddDate(0, 0, 10)) // expires in 10 days\n\tpolicy.SetContentType(\"binary/octet-stream\")\n\tpolicy.SetContentLengthRange(10, 1024*1024)\n\tpolicy.SetUserMetadata(metadataKey, metadataValue)\n\tpolicy.SetContentEncoding(\"gzip\")\n\n\t// Add CRC32C\n\tchecksum := minio.ChecksumCRC32C.ChecksumBytes(buf)\n\terr = policy.SetChecksum(checksum)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetChecksum failed\", err)\n\t\treturn\n\t}\n\n\targs[\"policy\"] = policy.String()\n\n\tpresignedPostPolicyURL, formData, err := c.PresignedPostPolicy(context.Background(), policy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPostPolicy failed\", err)\n\t\treturn\n\t}\n\n\tvar formBuf bytes.Buffer\n\twriter := multipart.NewWriter(&formBuf)\n\tfor k, v := range formData {\n\t\twriter.WriteField(k, v)\n\t}\n\n\t// Get a 33KB file to upload and test if set post policy works\n\tfilePath := getMintDataDirFilePath(\"datafile-33-kB\")\n\tif filePath == \"\" {\n\t\t// Make a temp file with 33 KB data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"PresignedPostPolicyTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-33-kB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File Close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tfilePath = file.Name()\n\t}\n\n\t// add file to post request\n\tf, err := os.Open(filePath)\n\tdefer f.Close()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File open failed\", err)\n\t\treturn\n\t}\n\tw, err := writer.CreateFormFile(\"file\", filePath)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CreateFormFile failed\", err)\n\t\treturn\n\t}\n\n\t_, err = io.Copy(w, f)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\treturn\n\t}\n\twriter.Close()\n\n\thttpClient := &http.Client{\n\t\t// Setting a sensible time out of 30secs to wait for response\n\t\t// headers. Request is pro-actively canceled after 30secs\n\t\t// with no response.\n\t\tTimeout:   30 * time.Second,\n\t\tTransport: createHTTPTransport(),\n\t}\n\targs[\"url\"] = presignedPostPolicyURL.String()\n\n\treq, err := http.NewRequest(http.MethodPost, presignedPostPolicyURL.String(), bytes.NewReader(formBuf.Bytes()))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Http request failed\", err)\n\t\treturn\n\t}\n\n\treq.Header.Set(\"Content-Type\", writer.FormDataContentType())\n\n\t// make post request with correct form data\n\tres, err := httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Http request failed\", err)\n\t\treturn\n\t}\n\tdefer res.Body.Close()\n\tif res.StatusCode != http.StatusNoContent {\n\t\tlogError(testName, function, args, startTime, \"\", \"Http request failed\", errors.New(res.Status))\n\t\treturn\n\t}\n\n\t// expected path should be absolute path of the object\n\tvar scheme string\n\tif mustParseBool(os.Getenv(enableHTTPS)) {\n\t\tscheme = \"https://\"\n\t} else {\n\t\tscheme = \"http://\"\n\t}\n\n\texpectedLocation := scheme + os.Getenv(serverEndpoint) + \"/\" + bucketName + \"/\" + objectName\n\texpectedLocationBucketDNS := scheme + bucketName + \".\" + os.Getenv(serverEndpoint) + \"/\" + objectName\n\n\tif !strings.Contains(expectedLocation, \".amazonaws.com/\") {\n\t\t// Test when not against AWS S3.\n\t\tif val, ok := res.Header[\"Location\"]; ok {\n\t\t\tif val[0] != expectedLocation && val[0] != expectedLocationBucketDNS {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Location in header response is incorrect. Want %q or %q, got %q\", expectedLocation, expectedLocationBucketDNS, val[0]), err)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Location not found in header response\", err)\n\t\t\treturn\n\t\t}\n\t}\n\twantChecksumCrc32c := checksum.Encoded()\n\tif got := res.Header.Get(\"X-Amz-Checksum-Crc32c\"); got != wantChecksumCrc32c {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Want checksum %q, got %q\", wantChecksumCrc32c, got), nil)\n\t\treturn\n\t}\n\n\t// Ensure that when we subsequently GetObject, the checksum is returned\n\tgopts := minio.GetObjectOptions{Checksum: true}\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, gopts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif st.ChecksumCRC32C != wantChecksumCrc32c {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Want checksum %s, got %s\", wantChecksumCrc32c, st.ChecksumCRC32C), nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// testPresignedPostPolicyWrongFile tests that when we have a policy with a checksum, we cannot POST the wrong file\nfunc testPresignedPostPolicyWrongFile() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PresignedPostPolicy(policy)\"\n\targs := map[string]interface{}{\n\t\t\"policy\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t// Azure requires the key to not start with a number\n\tmetadataKey := randString(60, rand.NewSource(time.Now().UnixNano()), \"user\")\n\tmetadataValue := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tpolicy := minio.NewPostPolicy()\n\tpolicy.SetBucket(bucketName)\n\tpolicy.SetKey(objectName)\n\tpolicy.SetExpires(time.Now().UTC().AddDate(0, 0, 10)) // expires in 10 days\n\tpolicy.SetContentType(\"binary/octet-stream\")\n\tpolicy.SetContentLengthRange(10, 1024*1024)\n\tpolicy.SetUserMetadata(metadataKey, metadataValue)\n\n\t// Add CRC32C of some data that the policy will explicitly allow.\n\tchecksum := minio.ChecksumCRC32C.ChecksumBytes([]byte{0x01, 0x02, 0x03})\n\terr = policy.SetChecksum(checksum)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetChecksum failed\", err)\n\t\treturn\n\t}\n\n\targs[\"policy\"] = policy.String()\n\n\tpresignedPostPolicyURL, formData, err := c.PresignedPostPolicy(context.Background(), policy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPostPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// At this stage, we have a policy that allows us to upload for a specific checksum.\n\t// Test that uploading datafile-10-kB, with a different checksum, fails as expected\n\tfilePath := getMintDataDirFilePath(\"datafile-10-kB\")\n\tif filePath == \"\" {\n\t\t// Make a temp file with 10 KB data.\n\t\tfile, err := os.CreateTemp(os.TempDir(), \"PresignedPostPolicyTest\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif _, err = io.Copy(file, getDataReader(\"datafile-10-kB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif err = file.Close(); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File Close failed\", err)\n\t\t\treturn\n\t\t}\n\t\tfilePath = file.Name()\n\t}\n\tfileReader := getDataReader(\"datafile-10-kB\")\n\tdefer fileReader.Close()\n\tbuf10k, err := io.ReadAll(fileReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\totherChecksum := minio.ChecksumCRC32C.ChecksumBytes(buf10k)\n\n\tvar formBuf bytes.Buffer\n\twriter := multipart.NewWriter(&formBuf)\n\tfor k, v := range formData {\n\t\tif k == \"x-amz-checksum-crc32c\" {\n\t\t\tv = otherChecksum.Encoded()\n\t\t}\n\t\twriter.WriteField(k, v)\n\t}\n\n\t// Add file to post request\n\tf, err := os.Open(filePath)\n\tdefer f.Close()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File open failed\", err)\n\t\treturn\n\t}\n\tw, err := writer.CreateFormFile(\"file\", filePath)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CreateFormFile failed\", err)\n\t\treturn\n\t}\n\t_, err = io.Copy(w, f)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\treturn\n\t}\n\twriter.Close()\n\n\thttpClient := &http.Client{\n\t\tTimeout:   30 * time.Second,\n\t\tTransport: createHTTPTransport(),\n\t}\n\targs[\"url\"] = presignedPostPolicyURL.String()\n\n\treq, err := http.NewRequest(http.MethodPost, presignedPostPolicyURL.String(), bytes.NewReader(formBuf.Bytes()))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request failed\", err)\n\t\treturn\n\t}\n\n\treq.Header.Set(\"Content-Type\", writer.FormDataContentType())\n\n\t// Make the POST request with the form data.\n\tres, err := httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request failed\", err)\n\t\treturn\n\t}\n\tdefer res.Body.Close()\n\tif res.StatusCode != http.StatusForbidden {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request unexpected status\", errors.New(res.Status))\n\t\treturn\n\t}\n\n\t// Read the response body, ensure it has checksum failure message\n\tresBody, err := io.ReadAll(res.Body)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Normalize the response body, because S3 uses quotes around the policy condition components\n\t// in the error message, MinIO does not.\n\tresBodyStr := strings.ReplaceAll(string(resBody), `\"`, \"\")\n\tif !strings.Contains(resBodyStr, \"Policy Condition failed: [eq, $x-amz-checksum-crc32c, 8TDyHg=\") {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected response body\", errors.New(resBodyStr))\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests copy object\nfunc testCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(dst, src)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Make a new bucket in 'us-east-1' (destination bucket).\n\terr = c.MakeBucket(context.Background(), bucketName+\"-copy\", minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName+\"-copy\", c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\t// Check the various fields of source object against destination object.\n\tobjInfo, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\t// Copy Source\n\tsrc := minio.CopySrcOptions{\n\t\tBucket: bucketName,\n\t\tObject: objectName,\n\t\t// Set copy conditions.\n\t\tMatchETag:          objInfo.ETag,\n\t\tMatchModifiedSince: time.Date(2014, time.April, 0, 0, 0, 0, 0, time.UTC),\n\t}\n\targs[\"src\"] = src\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName + \"-copy\",\n\t\tObject: objectName + \"-copy\",\n\t}\n\n\t// Perform the Copy\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\t// Source object\n\tr, err = c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Destination object\n\treaderCopy, err := c.GetObject(context.Background(), bucketName+\"-copy\", objectName+\"-copy\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Check the various fields of source object against destination object.\n\tobjInfo, err = r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tobjInfoCopy, err := readerCopy.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif objInfo.Size != objInfoCopy.Size {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(objInfoCopy.Size)+\", got \"+string(objInfo.Size), err)\n\t\treturn\n\t}\n\n\tif err := crcMatchesName(r, \"datafile-33-kB\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"data CRC check failed\", err)\n\t\treturn\n\t}\n\tif err := crcMatchesName(readerCopy, \"datafile-33-kB\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"copy data CRC check failed\", err)\n\t\treturn\n\t}\n\t// Close all the get readers before proceeding with CopyObject operations.\n\tr.Close()\n\treaderCopy.Close()\n\n\t// CopyObject again but with wrong conditions\n\tsrc = minio.CopySrcOptions{\n\t\tBucket:               bucketName,\n\t\tObject:               objectName,\n\t\tMatchUnmodifiedSince: time.Date(2014, time.April, 0, 0, 0, 0, 0, time.UTC),\n\t\tNoMatchETag:          objInfo.ETag,\n\t}\n\n\t// Perform the Copy which should fail\n\t_, err = c.CopyObject(context.Background(), dst, src)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject did not fail for invalid conditions\", err)\n\t\treturn\n\t}\n\n\tsrc = minio.CopySrcOptions{\n\t\tBucket: bucketName,\n\t\tObject: objectName,\n\t}\n\n\tdst = minio.CopyDestOptions{\n\t\tBucket:          bucketName,\n\t\tObject:          objectName,\n\t\tReplaceMetadata: true,\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Copy\": \"should be same\",\n\t\t},\n\t}\n\targs[\"dst\"] = dst\n\targs[\"src\"] = src\n\n\t_, err = c.CopyObject(context.Background(), dst, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject shouldn't fail\", err)\n\t\treturn\n\t}\n\n\toi, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tstOpts := minio.StatObjectOptions{}\n\tstOpts.SetMatchETag(oi.ETag)\n\tobjInfo, err = c.StatObject(context.Background(), bucketName, objectName, stOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject ETag should match and not fail\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Metadata.Get(\"x-amz-meta-copy\") != \"should be same\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject modified metadata should match\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests SSE-C get object ReaderSeeker interface methods.\nfunc testSSECEncryptedGetObjectReadSeekFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\t// Delete all objects and buckets\n\t\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Generate 129MiB of data.\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{\n\t\tContentType:          \"binary/octet-stream\",\n\t\tServerSideEncryption: encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+objectName)),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{\n\t\tServerSideEncryption: encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+objectName)),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat object failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\t// This following function helps us to compare data from the reader after seek\n\t// with the data from the original buffer\n\tcmpData := func(r io.Reader, start, end int) {\n\t\tif end-start == 0 {\n\t\t\treturn\n\t\t}\n\t\tbuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err := io.CopyN(buffer, r, int64(bufSize)); err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyN failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif !bytes.Equal(buf[start:end], buffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read bytes v/s original buffer\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\ttestCases := []struct {\n\t\toffset    int64\n\t\twhence    int\n\t\tpos       int64\n\t\terr       error\n\t\tshouldCmp bool\n\t\tstart     int\n\t\tend       int\n\t}{\n\t\t// Start from offset 0, fetch data and compare\n\t\t{0, 0, 0, nil, true, 0, 0},\n\t\t// Start from offset 2048, fetch data and compare\n\t\t{2048, 0, 2048, nil, true, 2048, bufSize},\n\t\t// Start from offset larger than possible\n\t\t{int64(bufSize) + 1024, 0, 0, io.EOF, false, 0, 0},\n\t\t// Move to offset 0 without comparing\n\t\t{0, 0, 0, nil, false, 0, 0},\n\t\t// Move one step forward and compare\n\t\t{1, 1, 1, nil, true, 1, bufSize},\n\t\t// Move larger than possible\n\t\t{int64(bufSize), 1, 0, io.EOF, false, 0, 0},\n\t\t// Provide negative offset with CUR_SEEK\n\t\t{int64(-1), 1, 0, fmt.Errorf(\"Negative position not allowed for 1\"), false, 0, 0},\n\t\t// Test with whence SEEK_END and with positive offset\n\t\t{1024, 2, 0, io.EOF, false, 0, 0},\n\t\t// Test with whence SEEK_END and with negative offset\n\t\t{-1024, 2, int64(bufSize) - 1024, nil, true, bufSize - 1024, bufSize},\n\t\t// Test with whence SEEK_END and with large negative offset\n\t\t{-int64(bufSize) * 2, 2, 0, fmt.Errorf(\"Seeking at negative offset not allowed for 2\"), false, 0, 0},\n\t\t// Test with invalid whence\n\t\t{0, 3, 0, fmt.Errorf(\"Invalid whence 3\"), false, 0, 0},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Perform seek operation\n\t\tn, err := r.Seek(testCase.offset, testCase.whence)\n\t\tif err != nil && testCase.err == nil {\n\t\t\t// We expected success.\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\treturn\n\t\t}\n\t\tif err == nil && testCase.err != nil {\n\t\t\t// We expected failure, but got success.\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\treturn\n\t\t}\n\t\tif err != nil && testCase.err != nil {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\t// We expect a specific error\n\t\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// Check the returned seek pos\n\t\tif n != testCase.pos {\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, number of bytes seeked does not match, expected %d, got %d\", i+1, testCase.pos, n), err)\n\t\t\treturn\n\t\t}\n\t\t// Compare only if shouldCmp is activated\n\t\tif testCase.shouldCmp {\n\t\t\tcmpData(r, testCase.start, testCase.end)\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests SSE-S3 get object ReaderSeeker interface methods.\nfunc testSSES3EncryptedGetObjectReadSeekFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\t// Delete all objects and buckets\n\t\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Generate 129MiB of data.\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{\n\t\tContentType:          \"binary/octet-stream\",\n\t\tServerSideEncryption: encrypt.NewSSE(),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat object failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\t// This following function helps us to compare data from the reader after seek\n\t// with the data from the original buffer\n\tcmpData := func(r io.Reader, start, end int) {\n\t\tif end-start == 0 {\n\t\t\treturn\n\t\t}\n\t\tbuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err := io.CopyN(buffer, r, int64(bufSize)); err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyN failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif !bytes.Equal(buf[start:end], buffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read bytes v/s original buffer\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\ttestCases := []struct {\n\t\toffset    int64\n\t\twhence    int\n\t\tpos       int64\n\t\terr       error\n\t\tshouldCmp bool\n\t\tstart     int\n\t\tend       int\n\t}{\n\t\t// Start from offset 0, fetch data and compare\n\t\t{0, 0, 0, nil, true, 0, 0},\n\t\t// Start from offset 2048, fetch data and compare\n\t\t{2048, 0, 2048, nil, true, 2048, bufSize},\n\t\t// Start from offset larger than possible\n\t\t{int64(bufSize) + 1024, 0, 0, io.EOF, false, 0, 0},\n\t\t// Move to offset 0 without comparing\n\t\t{0, 0, 0, nil, false, 0, 0},\n\t\t// Move one step forward and compare\n\t\t{1, 1, 1, nil, true, 1, bufSize},\n\t\t// Move larger than possible\n\t\t{int64(bufSize), 1, 0, io.EOF, false, 0, 0},\n\t\t// Provide negative offset with CUR_SEEK\n\t\t{int64(-1), 1, 0, fmt.Errorf(\"Negative position not allowed for 1\"), false, 0, 0},\n\t\t// Test with whence SEEK_END and with positive offset\n\t\t{1024, 2, 0, io.EOF, false, 0, 0},\n\t\t// Test with whence SEEK_END and with negative offset\n\t\t{-1024, 2, int64(bufSize) - 1024, nil, true, bufSize - 1024, bufSize},\n\t\t// Test with whence SEEK_END and with large negative offset\n\t\t{-int64(bufSize) * 2, 2, 0, fmt.Errorf(\"Seeking at negative offset not allowed for 2\"), false, 0, 0},\n\t\t// Test with invalid whence\n\t\t{0, 3, 0, fmt.Errorf(\"Invalid whence 3\"), false, 0, 0},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Perform seek operation\n\t\tn, err := r.Seek(testCase.offset, testCase.whence)\n\t\tif err != nil && testCase.err == nil {\n\t\t\t// We expected success.\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\treturn\n\t\t}\n\t\tif err == nil && testCase.err != nil {\n\t\t\t// We expected failure, but got success.\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\treturn\n\t\t}\n\t\tif err != nil && testCase.err != nil {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\t// We expect a specific error\n\t\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\t\tfmt.Sprintf(\"Test %d, unexpected err value: expected: %s, found: %s\", i+1, testCase.err, err), err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// Check the returned seek pos\n\t\tif n != testCase.pos {\n\t\t\tlogError(testName, function, args, startTime, \"\",\n\t\t\t\tfmt.Sprintf(\"Test %d, number of bytes seeked does not match, expected %d, got %d\", i+1, testCase.pos, n), err)\n\t\t\treturn\n\t\t}\n\t\t// Compare only if shouldCmp is activated\n\t\tif testCase.shouldCmp {\n\t\t\tcmpData(r, testCase.start, testCase.end)\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests SSE-C get object ReaderAt interface methods.\nfunc testSSECEncryptedGetObjectReadAtFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 129MiB of data.\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{\n\t\tContentType:          \"binary/octet-stream\",\n\t\tServerSideEncryption: encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+objectName)),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{\n\t\tServerSideEncryption: encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+objectName)),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\toffset := int64(2048)\n\n\t// read directly\n\tbuf1 := make([]byte, 512)\n\tbuf2 := make([]byte, 512)\n\tbuf3 := make([]byte, 512)\n\tbuf4 := make([]byte, 512)\n\n\t// Test readAt before stat is called such that objectInfo doesn't change.\n\tm, err := r.ReadAt(buf1, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf1) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf1))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf1, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tm, err = r.ReadAt(buf2, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf2))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf2, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf3, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf3) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf3))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf3, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf4, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf4) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf4))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf4, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\n\tbuf5 := make([]byte, len(buf))\n\t// Read the whole object.\n\tm, err = r.ReadAt(buf5, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m != len(buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf5))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf, buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect data read in GetObject, than what was previously uploaded\", err)\n\t\treturn\n\t}\n\n\tbuf6 := make([]byte, len(buf)+1)\n\t// Read the whole object and beyond.\n\t_, err = r.ReadAt(buf6, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests SSE-S3 get object ReaderAt interface methods.\nfunc testSSES3EncryptedGetObjectReadAtFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 129MiB of data.\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{\n\t\tContentType:          \"binary/octet-stream\",\n\t\tServerSideEncryption: encrypt.NewSSE(),\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\toffset := int64(2048)\n\n\t// read directly\n\tbuf1 := make([]byte, 512)\n\tbuf2 := make([]byte, 512)\n\tbuf3 := make([]byte, 512)\n\tbuf4 := make([]byte, 512)\n\n\t// Test readAt before stat is called such that objectInfo doesn't change.\n\tm, err := r.ReadAt(buf1, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf1) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf1))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf1, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\", got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tm, err = r.ReadAt(buf2, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf2))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf2, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf3, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf3) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf3))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf3, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf4, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf4) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf4))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf4, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\n\tbuf5 := make([]byte, len(buf))\n\t// Read the whole object.\n\tm, err = r.ReadAt(buf5, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m != len(buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf5))+\", got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf, buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect data read in GetObject, than what was previously uploaded\", err)\n\t\treturn\n\t}\n\n\tbuf6 := make([]byte, len(buf)+1)\n\t// Read the whole object and beyond.\n\t_, err = r.ReadAt(buf6, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// testSSECEncryptionPutGet tests encryption with customer provided encryption keys\nfunc testSSECEncryptionPutGet() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutEncryptedObject(bucketName, objectName, reader, sse)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"sse\":        \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\ttestCases := []struct {\n\t\tbuf []byte\n\t}{\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 15)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 16)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 17)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 31)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 32)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 33)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*2)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*1024)},\n\t}\n\n\tconst password = \"correct horse battery staple\" // https://xkcd.com/936/\n\n\tfor i, testCase := range testCases {\n\t\t// Generate a random object name\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\t// Secured object\n\t\tsse := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\t\targs[\"sse\"] = sse\n\n\t\t// Put encrypted data\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(testCase.buf), int64(len(testCase.buf)), minio.PutObjectOptions{ServerSideEncryption: sse})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Read the data back\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{ServerSideEncryption: sse})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Compare the sent object with the received one\n\t\trecvBuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err = io.Copy(recvBuffer, r); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", error: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\tif recvBuffer.Len() != len(testCase.buf) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Number of bytes of received object does not match, expected \"+string(len(testCase.buf))+\", got \"+string(recvBuffer.Len()), err)\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(testCase.buf, recvBuffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Encrypted sent is not equal to decrypted, expected \"+string(testCase.buf)+\", got \"+string(recvBuffer.Bytes()), err)\n\t\t\treturn\n\t\t}\n\n\t\tlogSuccess(testName, function, args, startTime)\n\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// TestEncryptionFPut tests encryption with customer specified encryption keys\nfunc testSSECEncryptionFPut() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutEncryptedObject(bucketName, objectName, filePath, contentType, sse)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":  \"\",\n\t\t\"objectName\":  \"\",\n\t\t\"filePath\":    \"\",\n\t\t\"contentType\": \"\",\n\t\t\"sse\":         \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Object custom metadata\n\tcustomContentType := \"custom/contenttype\"\n\targs[\"metadata\"] = customContentType\n\n\ttestCases := []struct {\n\t\tbuf []byte\n\t}{\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 0)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 15)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 16)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 17)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 31)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 32)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 33)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*2)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*1024)},\n\t}\n\n\tconst password = \"correct horse battery staple\" // https://xkcd.com/936/\n\tfor i, testCase := range testCases {\n\t\t// Generate a random object name\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\t// Secured object\n\t\tsse := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\t\targs[\"sse\"] = sse\n\n\t\t// Generate a random file name.\n\t\tfileName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\tfile, err := os.Create(fileName)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file create failed\", err)\n\t\t\treturn\n\t\t}\n\t\t_, err = file.Write(testCase.buf)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file write failed\", err)\n\t\t\treturn\n\t\t}\n\t\tfile.Close()\n\t\t// Put encrypted data\n\t\tif _, err = c.FPutObject(context.Background(), bucketName, objectName, fileName, minio.PutObjectOptions{ServerSideEncryption: sse}); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"FPutEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Read the data back\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{ServerSideEncryption: sse})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Compare the sent object with the received one\n\t\trecvBuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err = io.Copy(recvBuffer, r); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", error: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\tif recvBuffer.Len() != len(testCase.buf) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Number of bytes of received object does not match, expected \"+string(len(testCase.buf))+\", got \"+string(recvBuffer.Len()), err)\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(testCase.buf, recvBuffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Encrypted sent is not equal to decrypted, expected \"+string(testCase.buf)+\", got \"+string(recvBuffer.Bytes()), err)\n\t\t\treturn\n\t\t}\n\n\t\tos.Remove(fileName)\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// testSSES3EncryptionPutGet tests SSE-S3 encryption\nfunc testSSES3EncryptionPutGet() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutEncryptedObject(bucketName, objectName, reader, sse)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"sse\":        \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\ttestCases := []struct {\n\t\tbuf []byte\n\t}{\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 15)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 16)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 17)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 31)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 32)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 33)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*2)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*1024)},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Generate a random object name\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\t// Secured object\n\t\tsse := encrypt.NewSSE()\n\t\targs[\"sse\"] = sse\n\n\t\t// Put encrypted data\n\t\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(testCase.buf), int64(len(testCase.buf)), minio.PutObjectOptions{ServerSideEncryption: sse})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Read the data back without any encryption headers\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Compare the sent object with the received one\n\t\trecvBuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err = io.Copy(recvBuffer, r); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", error: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\tif recvBuffer.Len() != len(testCase.buf) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Number of bytes of received object does not match, expected \"+string(len(testCase.buf))+\", got \"+string(recvBuffer.Len()), err)\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(testCase.buf, recvBuffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Encrypted sent is not equal to decrypted, expected \"+string(testCase.buf)+\", got \"+string(recvBuffer.Bytes()), err)\n\t\t\treturn\n\t\t}\n\n\t\tlogSuccess(testName, function, args, startTime)\n\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// TestSSES3EncryptionFPut tests server side encryption\nfunc testSSES3EncryptionFPut() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutEncryptedObject(bucketName, objectName, filePath, contentType, sse)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":  \"\",\n\t\t\"objectName\":  \"\",\n\t\t\"filePath\":    \"\",\n\t\t\"contentType\": \"\",\n\t\t\"sse\":         \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Object custom metadata\n\tcustomContentType := \"custom/contenttype\"\n\targs[\"metadata\"] = customContentType\n\n\ttestCases := []struct {\n\t\tbuf []byte\n\t}{\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 0)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 15)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 16)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 17)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 31)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 32)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 33)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*2)},\n\t\t{buf: bytes.Repeat([]byte(\"F\"), 1024*1024)},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\t// Generate a random object name\n\t\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\targs[\"objectName\"] = objectName\n\n\t\t// Secured object\n\t\tsse := encrypt.NewSSE()\n\t\targs[\"sse\"] = sse\n\n\t\t// Generate a random file name.\n\t\tfileName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t\tfile, err := os.Create(fileName)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file create failed\", err)\n\t\t\treturn\n\t\t}\n\t\t_, err = file.Write(testCase.buf)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file write failed\", err)\n\t\t\treturn\n\t\t}\n\t\tfile.Close()\n\t\t// Put encrypted data\n\t\tif _, err = c.FPutObject(context.Background(), bucketName, objectName, fileName, minio.PutObjectOptions{ServerSideEncryption: sse}); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"FPutEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Read the data back\n\t\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetEncryptedObject failed\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Compare the sent object with the received one\n\t\trecvBuffer := bytes.NewBuffer([]byte{})\n\t\tif _, err = io.Copy(recvBuffer, r); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", error: \"+err.Error(), err)\n\t\t\treturn\n\t\t}\n\t\tif recvBuffer.Len() != len(testCase.buf) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Number of bytes of received object does not match, expected \"+string(len(testCase.buf))+\", got \"+string(recvBuffer.Len()), err)\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(testCase.buf, recvBuffer.Bytes()) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Test \"+string(i+1)+\", Encrypted sent is not equal to decrypted, expected \"+string(testCase.buf)+\", got \"+string(recvBuffer.Bytes()), err)\n\t\t\treturn\n\t\t}\n\n\t\tos.Remove(fileName)\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testBucketNotification() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"SetBucketNotification(bucketName)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\tif os.Getenv(\"NOTIFY_BUCKET\") == \"\" ||\n\t\tos.Getenv(\"NOTIFY_SERVICE\") == \"\" ||\n\t\tos.Getenv(\"NOTIFY_REGION\") == \"\" ||\n\t\tos.Getenv(\"NOTIFY_ACCOUNTID\") == \"\" ||\n\t\tos.Getenv(\"NOTIFY_RESOURCE\") == \"\" {\n\t\tlogIgnored(testName, function, args, startTime, \"Skipped notification test as it is not configured\")\n\t\treturn\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\tbucketName := os.Getenv(\"NOTIFY_BUCKET\")\n\targs[\"bucketName\"] = bucketName\n\n\ttopicArn := notification.NewArn(\"aws\", os.Getenv(\"NOTIFY_SERVICE\"), os.Getenv(\"NOTIFY_REGION\"), os.Getenv(\"NOTIFY_ACCOUNTID\"), os.Getenv(\"NOTIFY_RESOURCE\"))\n\tqueueArn := notification.NewArn(\"aws\", \"dummy-service\", \"dummy-region\", \"dummy-accountid\", \"dummy-resource\")\n\n\ttopicConfig := notification.NewConfig(topicArn)\n\ttopicConfig.AddEvents(notification.ObjectCreatedAll, notification.ObjectRemovedAll)\n\ttopicConfig.AddFilterSuffix(\"jpg\")\n\n\tqueueConfig := notification.NewConfig(queueArn)\n\tqueueConfig.AddEvents(notification.ObjectCreatedAll)\n\tqueueConfig.AddFilterPrefix(\"photos/\")\n\n\tconfig := notification.Configuration{}\n\tconfig.AddTopic(topicConfig)\n\n\t// Add the same topicConfig again, should have no effect\n\t// because it is duplicated\n\tconfig.AddTopic(topicConfig)\n\tif len(config.TopicConfigs) != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Duplicate entry added\", err)\n\t\treturn\n\t}\n\n\t// Add and remove a queue config\n\tconfig.AddQueue(queueConfig)\n\tconfig.RemoveQueueByArn(queueArn)\n\n\terr = c.SetBucketNotification(context.Background(), bucketName, config)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketNotification failed\", err)\n\t\treturn\n\t}\n\n\tconfig, err = c.GetBucketNotification(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketNotification failed\", err)\n\t\treturn\n\t}\n\n\tif len(config.TopicConfigs) != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Topic config is empty\", err)\n\t\treturn\n\t}\n\n\tif config.TopicConfigs[0].Filter.S3Key.FilterRules[0].Value != \"jpg\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Couldn't get the suffix\", err)\n\t\treturn\n\t}\n\n\terr = c.RemoveAllBucketNotification(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveAllBucketNotification failed\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests comprehensive list of all methods.\nfunc testFunctional() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"testFunctional()\"\n\tfunctionAll := \"\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, nil, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket.\n\tfunction = \"MakeBucket(bucketName, region)\"\n\tfunctionAll = \"MakeBucket(bucketName, region)\"\n\targs[\"bucketName\"] = bucketName\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\n\tdefer cleanupBucket(bucketName, c)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a random file name.\n\tfileName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tfile, err := os.Create(fileName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File creation failed\", err)\n\t\treturn\n\t}\n\tfor i := 0; i < 3; i++ {\n\t\tbuf := make([]byte, rand.Intn(1<<19))\n\t\t_, err = file.Write(buf)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File write failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tfile.Close()\n\n\t// Verify if bucket exits and you have access.\n\tvar exists bool\n\tfunction = \"BucketExists(bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\texists, err = c.BucketExists(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"BucketExists failed\", err)\n\t\treturn\n\t}\n\tif !exists {\n\t\tlogError(testName, function, args, startTime, \"\", \"Could not find the bucket\", err)\n\t\treturn\n\t}\n\n\t// Asserting the default bucket policy.\n\tfunction = \"GetBucketPolicy(ctx, bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\tnilPolicy, err := c.GetBucketPolicy(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\tif nilPolicy != \"\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"policy should be set to nil\", err)\n\t\treturn\n\t}\n\n\t// Set the bucket policy to 'public readonly'.\n\tfunction = \"SetBucketPolicy(bucketName, readOnlyPolicy)\"\n\tfunctionAll += \", \" + function\n\n\treadOnlyPolicy := `{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"*\"]},\"Action\":[\"s3:ListBucket\"],\"Resource\":[\"arn:aws:s3:::` + bucketName + `\"]}]}`\n\targs = map[string]interface{}{\n\t\t\"bucketName\":   bucketName,\n\t\t\"bucketPolicy\": readOnlyPolicy,\n\t}\n\n\terr = c.SetBucketPolicy(context.Background(), bucketName, readOnlyPolicy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\t// should return policy `readonly`.\n\tfunction = \"GetBucketPolicy(ctx, bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\t_, err = c.GetBucketPolicy(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// Make the bucket 'public writeonly'.\n\tfunction = \"SetBucketPolicy(bucketName, writeOnlyPolicy)\"\n\tfunctionAll += \", \" + function\n\n\twriteOnlyPolicy := `{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"*\"]},\"Action\":[\"s3:ListBucketMultipartUploads\"],\"Resource\":[\"arn:aws:s3:::` + bucketName + `\"]}]}`\n\targs = map[string]interface{}{\n\t\t\"bucketName\":   bucketName,\n\t\t\"bucketPolicy\": writeOnlyPolicy,\n\t}\n\terr = c.SetBucketPolicy(context.Background(), bucketName, writeOnlyPolicy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\t// should return policy `writeonly`.\n\tfunction = \"GetBucketPolicy(ctx, bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\n\t_, err = c.GetBucketPolicy(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// Make the bucket 'public read/write'.\n\tfunction = \"SetBucketPolicy(bucketName, readWritePolicy)\"\n\tfunctionAll += \", \" + function\n\n\treadWritePolicy := `{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"*\"]},\"Action\":[\"s3:ListBucket\",\"s3:ListBucketMultipartUploads\"],\"Resource\":[\"arn:aws:s3:::` + bucketName + `\"]}]}`\n\n\targs = map[string]interface{}{\n\t\t\"bucketName\":   bucketName,\n\t\t\"bucketPolicy\": readWritePolicy,\n\t}\n\terr = c.SetBucketPolicy(context.Background(), bucketName, readWritePolicy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\t// should return policy `readwrite`.\n\tfunction = \"GetBucketPolicy(bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\t_, err = c.GetBucketPolicy(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// List all buckets.\n\tfunction = \"ListBuckets()\"\n\tfunctionAll += \", \" + function\n\targs = nil\n\tbuckets, err := c.ListBuckets(context.Background())\n\n\tif len(buckets) == 0 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Found bucket list to be empty\", err)\n\t\treturn\n\t}\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ListBuckets failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if previously created bucket is listed in list buckets.\n\tbucketFound := false\n\tfor _, bucket := range buckets {\n\t\tif bucket.Name == bucketName {\n\t\t\tbucketFound = true\n\t\t}\n\t}\n\n\t// If bucket not found error out.\n\tif !bucketFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bucket: \"+bucketName+\" not found\", err)\n\t\treturn\n\t}\n\n\tobjectName := bucketName + \"unique\"\n\n\t// Generate data\n\tbuf := bytes.Repeat([]byte(\"f\"), 1<<19)\n\n\tfunction = \"PutObject(bucketName, objectName, reader, contentType)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"contentType\": \"\",\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName + \"-nolength\",\n\t\t\"contentType\": \"binary/octet-stream\",\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName+\"-nolength\", bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate a done channel to close all listing.\n\tdoneCh := make(chan struct{})\n\tdefer close(doneCh)\n\n\tobjFound := false\n\tisRecursive := true // Recursive is true.\n\n\tfunction = \"ListObjects(bucketName, objectName, isRecursive, doneCh)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"isRecursive\": isRecursive,\n\t}\n\n\tfor obj := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{UseV1: true, Prefix: objectName, Recursive: true}) {\n\t\tif obj.Key == objectName {\n\t\t\tobjFound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !objFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object \"+objectName+\" not found\", err)\n\t\treturn\n\t}\n\n\tobjFound = false\n\tisRecursive = true // Recursive is true.\n\tfunction = \"ListObjects()\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"isRecursive\": isRecursive,\n\t}\n\n\tfor obj := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{Prefix: objectName, Recursive: isRecursive}) {\n\t\tif obj.Key == objectName {\n\t\t\tobjFound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !objFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object \"+objectName+\" not found\", err)\n\t\treturn\n\t}\n\n\tincompObjNotFound := true\n\n\tfunction = \"ListIncompleteUploads(bucketName, objectName, isRecursive, doneCh)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"isRecursive\": isRecursive,\n\t}\n\n\tfor objIncompl := range c.ListIncompleteUploads(context.Background(), bucketName, objectName, isRecursive) {\n\t\tif objIncompl.Key != \"\" {\n\t\t\tincompObjNotFound = false\n\t\t\tbreak\n\t\t}\n\t}\n\tif !incompObjNotFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected dangling incomplete upload found\", err)\n\t\treturn\n\t}\n\n\tfunction = \"GetObject(bucketName, objectName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t}\n\tnewReader, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tnewReadBytes, err := io.ReadAll(newReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\tif !bytes.Equal(newReadBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject bytes mismatch\", err)\n\t\treturn\n\t}\n\tnewReader.Close()\n\n\tfunction = \"FGetObject(bucketName, objectName, fileName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"fileName\":   fileName + \"-f\",\n\t}\n\terr = c.FGetObject(context.Background(), bucketName, objectName, fileName+\"-f\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject failed\", err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignedHeadObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": \"\",\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tif _, err = c.PresignedHeadObject(context.Background(), bucketName, \"\", 3600*time.Second, nil); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject success\", err)\n\t\treturn\n\t}\n\n\t// Generate presigned HEAD object url.\n\tfunction = \"PresignedHeadObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedHeadURL, err := c.PresignedHeadObject(context.Background(), bucketName, objectName, 3600*time.Second, nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject failed\", err)\n\t\treturn\n\t}\n\n\ttransport := createHTTPTransport()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"DefaultTransport failed\", err)\n\t\treturn\n\t}\n\n\thttpClient := &http.Client{\n\t\t// Setting a sensible time out of 30secs to wait for response\n\t\t// headers. Request is pro-actively canceled after 30secs\n\t\t// with no response.\n\t\tTimeout:   30 * time.Second,\n\t\tTransport: transport,\n\t}\n\n\treq, err := http.NewRequest(http.MethodHead, presignedHeadURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject request was incorrect\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\tresp, err := httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject response incorrect\", err)\n\t\treturn\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject response incorrect, status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tif resp.Header.Get(\"ETag\") == \"\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject response incorrect\", err)\n\t\treturn\n\t}\n\tresp.Body.Close()\n\n\tfunction = \"PresignedGetObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": \"\",\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\t_, err = c.PresignedGetObject(context.Background(), bucketName, \"\", 3600*time.Second, nil)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject success\", err)\n\t\treturn\n\t}\n\n\t// Generate presigned GET object url.\n\tfunction = \"PresignedGetObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedGetURL, err := c.PresignedGetObject(context.Background(), bucketName, objectName, 3600*time.Second, nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\treq, err = http.NewRequest(http.MethodGet, presignedGetURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject request incorrect\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect, status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tnewPresignedBytes, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\tresp.Body.Close()\n\tif !bytes.Equal(newPresignedBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\n\t// Set request parameters.\n\treqParams := make(url.Values)\n\treqParams.Set(\"response-content-disposition\", \"attachment; filename=\\\"test.txt\\\"\")\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"expires\":    3600 * time.Second,\n\t\t\"reqParams\":  reqParams,\n\t}\n\tpresignedGetURL, err = c.PresignedGetObject(context.Background(), bucketName, objectName, 3600*time.Second, reqParams)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\treq, err = http.NewRequest(http.MethodGet, presignedGetURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject request incorrect\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect, status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tnewPresignedBytes, err = io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(newPresignedBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch for presigned GET URL\", err)\n\t\treturn\n\t}\n\tif resp.Header.Get(\"Content-Disposition\") != \"attachment; filename=\\\"test.txt\\\"\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"wrong Content-Disposition received \"+string(resp.Header.Get(\"Content-Disposition\")), err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignedPutObject(bucketName, objectName, expires)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": \"\",\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\t_, err = c.PresignedPutObject(context.Background(), bucketName, \"\", 3600*time.Second)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPutObject success\", err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignedPutObject(bucketName, objectName, expires)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName + \"-presigned\",\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedPutURL, err := c.PresignedPutObject(context.Background(), bucketName, objectName+\"-presigned\", 3600*time.Second)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPutObject failed\", err)\n\t\treturn\n\t}\n\n\tbuf = bytes.Repeat([]byte(\"g\"), 1<<19)\n\n\treq, err = http.NewRequest(http.MethodPut, presignedPutURL.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Couldn't make HTTP request with PresignedPutObject URL\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPutObject failed\", err)\n\t\treturn\n\t}\n\n\tnewReader, err = c.GetObject(context.Background(), bucketName, objectName+\"-presigned\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject after PresignedPutObject failed\", err)\n\t\treturn\n\t}\n\n\tnewReadBytes, err = io.ReadAll(newReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll after GetObject failed\", err)\n\t\treturn\n\t}\n\n\tif !bytes.Equal(newReadBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch\", err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignHeader(method, bucketName, objectName, expires, reqParams, extraHeaders)\"\n\tfunctionAll += \", \" + function\n\tpresignExtraHeaders := map[string][]string{\n\t\t\"mysecret\": {\"abcxxx\"},\n\t}\n\targs = map[string]interface{}{\n\t\t\"method\":       \"PUT\",\n\t\t\"bucketName\":   bucketName,\n\t\t\"objectName\":   objectName + \"-presign-custom\",\n\t\t\"expires\":      3600 * time.Second,\n\t\t\"extraHeaders\": presignExtraHeaders,\n\t}\n\tpresignedURL, err := c.PresignHeader(context.Background(), \"PUT\", bucketName, objectName+\"-presign-custom\", 3600*time.Second, nil, presignExtraHeaders)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Presigned failed\", err)\n\t\treturn\n\t}\n\n\t// Generate data more than 32K\n\tbuf = bytes.Repeat([]byte(\"1\"), rand.Intn(1<<10)+32*1024)\n\n\treq, err = http.NewRequest(http.MethodPut, presignedURL.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request to Presigned URL failed\", err)\n\t\treturn\n\t}\n\n\treq.Header.Add(\"mysecret\", \"abcxxx\")\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request to Presigned URL failed\", err)\n\t\treturn\n\t}\n\n\t// Download the uploaded object to verify\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName + \"-presign-custom\",\n\t}\n\tnewReader, err = c.GetObject(context.Background(), bucketName, objectName+\"-presign-custom\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject of uploaded custom-presigned object failed\", err)\n\t\treturn\n\t}\n\n\tnewReadBytes, err = io.ReadAll(newReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed during get on custom-presigned put object\", err)\n\t\treturn\n\t}\n\tnewReader.Close()\n\n\tif !bytes.Equal(newReadBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch on custom-presigned object upload verification\", err)\n\t\treturn\n\t}\n\n\tfunction = \"RemoveObject(bucketName, objectName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t}\n\terr = c.RemoveObject(context.Background(), bucketName, objectName, minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObject failed\", err)\n\t\treturn\n\t}\n\targs[\"objectName\"] = objectName + \"-f\"\n\terr = c.RemoveObject(context.Background(), bucketName, objectName+\"-f\", minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObject failed\", err)\n\t\treturn\n\t}\n\n\targs[\"objectName\"] = objectName + \"-nolength\"\n\terr = c.RemoveObject(context.Background(), bucketName, objectName+\"-nolength\", minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObject failed\", err)\n\t\treturn\n\t}\n\n\targs[\"objectName\"] = objectName + \"-presigned\"\n\terr = c.RemoveObject(context.Background(), bucketName, objectName+\"-presigned\", minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObject failed\", err)\n\t\treturn\n\t}\n\n\targs[\"objectName\"] = objectName + \"-presign-custom\"\n\terr = c.RemoveObject(context.Background(), bucketName, objectName+\"-presign-custom\", minio.RemoveObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveObject failed\", err)\n\t\treturn\n\t}\n\n\tfunction = \"RemoveBucket(bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveBucket failed\", err)\n\t\treturn\n\t}\n\terr = c.RemoveBucket(context.Background(), bucketName)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveBucket did not fail for invalid bucket name\", err)\n\t\treturn\n\t}\n\tif err.Error() != \"The specified bucket does not exist\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveBucket failed\", err)\n\t\treturn\n\t}\n\n\tos.Remove(fileName)\n\tos.Remove(fileName + \"-f\")\n\tlogSuccess(testName, functionAll, args, startTime)\n}\n\n// Test for validating GetObject Reader* methods functioning when the\n// object is modified in the object store.\nfunc testGetObjectModified() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload an object.\n\tobjectName := \"myobject\"\n\targs[\"objectName\"] = objectName\n\tcontent := \"helloworld\"\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, strings.NewReader(content), int64(len(content)), minio.PutObjectOptions{ContentType: \"application/text\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Failed to upload \"+objectName+\", to bucket \"+bucketName, err)\n\t\treturn\n\t}\n\n\tdefer c.RemoveObject(context.Background(), bucketName, objectName, minio.RemoveObjectOptions{})\n\n\treader, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Failed to GetObject \"+objectName+\", from bucket \"+bucketName, err)\n\t\treturn\n\t}\n\tdefer reader.Close()\n\n\t// Read a few bytes of the object.\n\tb := make([]byte, 5)\n\tn, err := reader.ReadAt(b, 0)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Failed to read object \"+objectName+\", from bucket \"+bucketName+\" at an offset\", err)\n\t\treturn\n\t}\n\n\t// Upload different contents to the same object while object is being read.\n\tnewContent := \"goodbyeworld\"\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, strings.NewReader(newContent), int64(len(newContent)), minio.PutObjectOptions{ContentType: \"application/text\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Failed to upload \"+objectName+\", to bucket \"+bucketName, err)\n\t\treturn\n\t}\n\n\t// Confirm that a Stat() call in between doesn't change the Object's cached etag.\n\t_, err = reader.Stat()\n\texpectedError := \"At least one of the pre-conditions you specified did not hold\"\n\tif err.Error() != expectedError {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected Stat to fail with error \"+expectedError+\", but received \"+err.Error(), err)\n\t\treturn\n\t}\n\n\t// Read again only to find object contents have been modified since last read.\n\t_, err = reader.ReadAt(b, int64(n))\n\tif err.Error() != expectedError {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected ReadAt to fail with error \"+expectedError+\", but received \"+err.Error(), err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test validates putObject to upload a file seeked at a given offset.\nfunc testPutObjectUploadSeekedObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, fileToUpload, contentType)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":   \"\",\n\t\t\"objectName\":   \"\",\n\t\t\"fileToUpload\": \"\",\n\t\t\"contentType\":  \"binary/octet-stream\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\n\tvar tempfile *os.File\n\n\tif fileName := getMintDataDirFilePath(\"datafile-100-kB\"); fileName != \"\" {\n\t\ttempfile, err = os.Open(fileName)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File open failed\", err)\n\t\t\treturn\n\t\t}\n\t\targs[\"fileToUpload\"] = fileName\n\t} else {\n\t\ttempfile, err = os.CreateTemp(\"\", \"minio-go-upload-test-\")\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"TempFile create failed\", err)\n\t\t\treturn\n\t\t}\n\t\targs[\"fileToUpload\"] = tempfile.Name()\n\n\t\t// Generate 100kB data\n\t\tif _, err = io.Copy(tempfile, getDataReader(\"datafile-100-kB\")); err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"File copy failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tdefer os.Remove(tempfile.Name())\n\n\t\t// Seek back to the beginning of the file.\n\t\ttempfile.Seek(0, 0)\n\t}\n\tlength := 100 * humanize.KiByte\n\tobjectName := fmt.Sprintf(\"test-file-%v\", rand.Uint32())\n\targs[\"objectName\"] = objectName\n\n\toffset := length / 2\n\tif _, err = tempfile.Seek(int64(offset), 0); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"TempFile seek failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, tempfile, int64(length-offset), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\ttempfile.Close()\n\n\tobj, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer obj.Close()\n\n\tn, err := obj.Seek(int64(offset), 0)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek failed\", err)\n\t\treturn\n\t}\n\tif n != int64(offset) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Invalid offset returned, expected %d got %d\", int64(offset), n), err)\n\t\treturn\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName+\"getobject\", obj, int64(length-offset), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName+\"getobject\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(length-offset) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Invalid offset returned, expected %d got %d\", int64(length-offset), n), err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests bucket re-create errors.\nfunc testMakeBucketErrorV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"MakeBucket(bucketName, region)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"region\":     \"eu-west-1\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\tregion := \"eu-west-1\"\n\targs[\"bucketName\"] = bucketName\n\targs[\"region\"] = region\n\n\t// Make a new bucket in 'eu-west-1'.\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: region}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: region}); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket did not fail for existing bucket name\", err)\n\t\treturn\n\t}\n\t// Verify valid error response from server.\n\tif minio.ToErrorResponse(err).Code != \"BucketAlreadyExists\" &&\n\t\tminio.ToErrorResponse(err).Code != \"BucketAlreadyOwnedByYou\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error returned by server\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object reader to not throw error on being closed twice.\nfunc testGetObjectClosedTwiceV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"MakeBucket(bucketName, region)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"region\":     \"eu-west-1\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif err := r.Close(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Object is already closed, should return error\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests FPutObject hidden contentType setting\nfunc testFPutObjectV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FPutObject(bucketName, objectName, fileName, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Make a temp file with 11*1024*1024 bytes of data.\n\tfile, err := os.CreateTemp(os.TempDir(), \"FPutObjectTest\")\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"TempFile creation failed\", err)\n\t\treturn\n\t}\n\n\tr := bytes.NewReader(bytes.Repeat([]byte(\"b\"), 11*1024*1024))\n\tn, err := io.CopyN(file, r, 11*1024*1024)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\treturn\n\t}\n\tif n != int64(11*1024*1024) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(int64(11*1024*1024))+\" got \"+string(n), err)\n\t\treturn\n\t}\n\n\t// Close the file pro-actively for windows.\n\terr = file.Close()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"File close failed\", err)\n\t\treturn\n\t}\n\n\t// Set base object name\n\tobjectName := bucketName + \"FPutObject\"\n\targs[\"objectName\"] = objectName\n\targs[\"fileName\"] = file.Name()\n\n\t// Perform standard FPutObject with contentType provided (Expecting application/octet-stream)\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName+\"-standard\", file.Name(), minio.PutObjectOptions{ContentType: \"application/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Perform FPutObject with no contentType provided (Expecting application/octet-stream)\n\targs[\"objectName\"] = objectName + \"-Octet\"\n\targs[\"contentType\"] = \"\"\n\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName+\"-Octet\", file.Name(), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Add extension to temp file name\n\tfileName := file.Name()\n\terr = os.Rename(fileName, fileName+\".gtar\")\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Rename failed\", err)\n\t\treturn\n\t}\n\n\t// Perform FPutObject with no contentType provided (Expecting application/x-gtar)\n\targs[\"objectName\"] = objectName + \"-Octet\"\n\targs[\"contentType\"] = \"\"\n\targs[\"fileName\"] = fileName + \".gtar\"\n\n\t_, err = c.FPutObject(context.Background(), bucketName, objectName+\"-GTar\", fileName+\".gtar\", minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FPutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Check headers and sizes\n\trStandard, err := c.StatObject(context.Background(), bucketName, objectName+\"-standard\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tif rStandard.Size != 11*1024*1024 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected size\", nil)\n\t\treturn\n\t}\n\n\tif rStandard.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Content-Type headers mismatched, expected: application/octet-stream , got \"+rStandard.ContentType, err)\n\t\treturn\n\t}\n\n\trOctet, err := c.StatObject(context.Background(), bucketName, objectName+\"-Octet\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif rOctet.ContentType != \"application/octet-stream\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Content-Type headers mismatched, expected: application/octet-stream , got \"+rOctet.ContentType, err)\n\t\treturn\n\t}\n\n\tif rOctet.Size != 11*1024*1024 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected size\", nil)\n\t\treturn\n\t}\n\n\trGTar, err := c.StatObject(context.Background(), bucketName, objectName+\"-GTar\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif rGTar.Size != 11*1024*1024 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected size\", nil)\n\t\treturn\n\t}\n\tif rGTar.ContentType != \"application/x-gtar\" && rGTar.ContentType != \"application/octet-stream\" && rGTar.ContentType != \"application/x-tar\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Content-Type headers mismatched, expected: application/x-tar , got \"+rGTar.ContentType, err)\n\t\treturn\n\t}\n\n\tos.Remove(fileName + \".gtar\")\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests various bucket supported formats.\nfunc testMakeBucketRegionsV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"MakeBucket(bucketName, region)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"region\":     \"eu-west-1\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket in 'eu-central-1'.\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"eu-west-1\"}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tif err = cleanupBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed while removing bucket recursively\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket with '.' in its name, in 'us-west-2'. This\n\t// request is internally staged into a path style instead of\n\t// virtual host style.\n\tif err = c.MakeBucket(context.Background(), bucketName+\".withperiod\", minio.MakeBucketOptions{Region: \"us-west-2\"}); err != nil {\n\t\targs[\"bucketName\"] = bucketName + \".withperiod\"\n\t\targs[\"region\"] = \"us-west-2\"\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket test with a bucket name with period, '.', failed\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupBucket(bucketName+\".withperiod\", c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed while removing bucket recursively\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests get object ReaderSeeker interface methods.\nfunc testGetObjectReadSeekFunctionalV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data.\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(int64(bufSize))+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\toffset := int64(2048)\n\tn, err := r.Seek(offset, 0)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek failed\", err)\n\t\treturn\n\t}\n\tif n != offset {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of seeked bytes does not match, expected \"+string(offset)+\" got \"+string(n), err)\n\t\treturn\n\t}\n\tn, err = r.Seek(0, 1)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek failed\", err)\n\t\treturn\n\t}\n\tif n != offset {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of seeked bytes does not match, expected \"+string(offset)+\" got \"+string(n), err)\n\t\treturn\n\t}\n\t_, err = r.Seek(offset, 2)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek on positive offset for whence '2' should error out\", err)\n\t\treturn\n\t}\n\tn, err = r.Seek(-offset, 2)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek failed\", err)\n\t\treturn\n\t}\n\tif n != st.Size-offset {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of seeked bytes does not match, expected \"+string(st.Size-offset)+\" got \"+string(n), err)\n\t\treturn\n\t}\n\n\tvar buffer1 bytes.Buffer\n\tif _, err = io.CopyN(&buffer1, r, st.Size); err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif !bytes.Equal(buf[len(buf)-int(offset):], buffer1.Bytes()) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read bytes v/s original buffer\", err)\n\t\treturn\n\t}\n\n\t// Seek again and read again.\n\tn, err = r.Seek(offset-1, 0)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Seek failed\", err)\n\t\treturn\n\t}\n\tif n != (offset - 1) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of seeked bytes does not match, expected \"+string(offset-1)+\" got \"+string(n), err)\n\t\treturn\n\t}\n\n\tvar buffer2 bytes.Buffer\n\tif _, err = io.CopyN(&buffer2, r, st.Size); err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Copy failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\t// Verify now lesser bytes.\n\tif !bytes.Equal(buf[2047:], buffer2.Bytes()) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read bytes v/s original buffer\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests get object ReaderAt interface methods.\nfunc testGetObjectReadAtFunctionalV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(bucketName, objectName)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\tbuf, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\n\t// Save the data\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer r.Close()\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\toffset := int64(2048)\n\n\t// Read directly\n\tbuf2 := make([]byte, 512)\n\tbuf3 := make([]byte, 512)\n\tbuf4 := make([]byte, 512)\n\n\tm, err := r.ReadAt(buf2, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf2) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf2))+\" got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf2, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf3, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf3) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf3))+\" got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf3, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\toffset += 512\n\tm, err = r.ReadAt(buf4, offset)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\treturn\n\t}\n\tif m != len(buf4) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf4))+\" got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf4, buf[offset:offset+512]) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect read between two ReadAt from same offset\", err)\n\t\treturn\n\t}\n\n\tbuf5 := make([]byte, bufSize)\n\t// Read the whole object.\n\tm, err = r.ReadAt(buf5, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m != len(buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt read shorter bytes before reaching EOF, expected \"+string(len(buf5))+\" got \"+string(m), err)\n\t\treturn\n\t}\n\tif !bytes.Equal(buf, buf5) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Incorrect data read in GetObject, than what was previously uploaded\", err)\n\t\treturn\n\t}\n\n\tbuf6 := make([]byte, bufSize+1)\n\t// Read the whole object and beyond.\n\t_, err = r.ReadAt(buf6, 0)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAt failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Tests copy object\nfunc testCopyObjectV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Make a new bucket in 'us-east-1' (destination bucket).\n\terr = c.MakeBucket(context.Background(), bucketName+\"-copy\", minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName+\"-copy\", c)\n\n\t// Generate 33K of data.\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tr, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\t// Check the various fields of source object against destination object.\n\tobjInfo, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tr.Close()\n\n\t// Copy Source\n\tsrc := minio.CopySrcOptions{\n\t\tBucket:             bucketName,\n\t\tObject:             objectName,\n\t\tMatchModifiedSince: time.Date(2014, time.April, 0, 0, 0, 0, 0, time.UTC),\n\t\tMatchETag:          objInfo.ETag,\n\t}\n\targs[\"source\"] = src\n\n\t// Set copy conditions.\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName + \"-copy\",\n\t\tObject: objectName + \"-copy\",\n\t}\n\targs[\"destination\"] = dst\n\n\t// Perform the Copy\n\t_, err = c.CopyObject(context.Background(), dst, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\t// Source object\n\tr, err = c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\t// Destination object\n\treaderCopy, err := c.GetObject(context.Background(), bucketName+\"-copy\", objectName+\"-copy\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\t// Check the various fields of source object against destination object.\n\tobjInfo, err = r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tobjInfoCopy, err := readerCopy.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\treturn\n\t}\n\tif objInfo.Size != objInfoCopy.Size {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes does not match, expected \"+string(objInfoCopy.Size)+\" got \"+string(objInfo.Size), err)\n\t\treturn\n\t}\n\n\t// Close all the readers.\n\tr.Close()\n\treaderCopy.Close()\n\n\t// CopyObject again but with wrong conditions\n\tsrc = minio.CopySrcOptions{\n\t\tBucket:               bucketName,\n\t\tObject:               objectName,\n\t\tMatchUnmodifiedSince: time.Date(2014, time.April, 0, 0, 0, 0, 0, time.UTC),\n\t\tNoMatchETag:          objInfo.ETag,\n\t}\n\n\t// Perform the Copy which should fail\n\t_, err = c.CopyObject(context.Background(), dst, src)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject did not fail for invalid conditions\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testComposeObjectErrorCasesWrapper(c *minio.Client) {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr := c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Test that more than 10K source objects cannot be\n\t// concatenated.\n\tsrcArr := [10001]minio.CopySrcOptions{}\n\tsrcSlice := srcArr[:]\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"object\",\n\t}\n\n\targs[\"destination\"] = dst\n\t// Just explain about srcArr in args[\"sourceList\"]\n\t// to stop having 10,001 null headers logged\n\targs[\"sourceList\"] = \"source array of 10,001 elements\"\n\tif _, err := c.ComposeObject(context.Background(), dst, srcSlice...); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected error in ComposeObject\", err)\n\t\treturn\n\t} else if err.Error() != \"There must be as least one and up to 10000 source objects.\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected error\", err)\n\t\treturn\n\t}\n\n\t// Create a source with invalid offset spec and check that\n\t// error is returned:\n\t// 1. Create the source object.\n\tconst badSrcSize = 5 * 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"1\"), badSrcSize)\n\t_, err = c.PutObject(context.Background(), bucketName, \"badObject\", bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\t// 2. Set invalid range spec on the object (going beyond\n\t// object size)\n\tbadSrc := minio.CopySrcOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"badObject\",\n\t\tMatchRange: true,\n\t\tStart:      1,\n\t\tEnd:        badSrcSize,\n\t}\n\n\t// 3. ComposeObject call should fail.\n\tif _, err := c.ComposeObject(context.Background(), dst, badSrc); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject expected to fail\", err)\n\t\treturn\n\t} else if !strings.Contains(err.Error(), \"has invalid segment-to-copy\") {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got invalid error\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test expected error cases\nfunc testComposeObjectErrorCasesV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestComposeObjectErrorCasesWrapper(c)\n}\n\nfunc testComposeMultipleSources(c *minio.Client) {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{\n\t\t\"destination\": \"\",\n\t\t\"sourceList\":  \"\",\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr := c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Upload a small source object\n\tconst srcSize = 1024 * 1024 * 5\n\tbuf := bytes.Repeat([]byte(\"1\"), srcSize)\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObject\", bytes.NewReader(buf), int64(srcSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// We will append 10 copies of the object.\n\tsrcs := []minio.CopySrcOptions{}\n\tfor i := 0; i < 10; i++ {\n\t\tsrcs = append(srcs, minio.CopySrcOptions{\n\t\t\tBucket: bucketName,\n\t\t\tObject: \"srcObject\",\n\t\t})\n\t}\n\n\t// make the last part very small\n\tsrcs[9].MatchRange = true\n\n\targs[\"sourceList\"] = srcs\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"dstObject\",\n\t}\n\targs[\"destination\"] = dst\n\n\tui, err := c.ComposeObject(context.Background(), dst, srcs...)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject failed\", err)\n\t\treturn\n\t}\n\n\tif ui.Size != 9*srcSize+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject returned unexpected size\", err)\n\t\treturn\n\t}\n\n\tobjProps, err := c.StatObject(context.Background(), bucketName, \"dstObject\", minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tif objProps.Size != 9*srcSize+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Size mismatched! Expected \"+string(10000*srcSize)+\" got \"+string(objProps.Size), err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test concatenating multiple 10K objects V2\nfunc testCompose10KSourcesV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestComposeMultipleSources(c)\n}\n\nfunc testEncryptedEmptyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, objectSize, opts)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tsse := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"object\"))\n\n\t// 1. create an sse-c encrypted object to copy by uploading\n\tconst srcSize = 0\n\tvar buf []byte // Empty buffer\n\targs[\"objectName\"] = \"object\"\n\t_, err = c.PutObject(context.Background(), bucketName, \"object\", bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ServerSideEncryption: sse})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\t// 2. Test CopyObject for an empty object\n\tsrc := minio.CopySrcOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"object\",\n\t\tEncryption: sse,\n\t}\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"new-object\",\n\t\tEncryption: sse,\n\t}\n\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tfunction = \"CopyObject(dst, src)\"\n\t\tlogError(testName, function, map[string]interface{}{}, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\t// 3. Test Key rotation\n\tnewSSE := encrypt.DefaultPBKDF([]byte(\"Don't Panic\"), []byte(bucketName+\"new-object\"))\n\tsrc = minio.CopySrcOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"new-object\",\n\t\tEncryption: sse,\n\t}\n\n\tdst = minio.CopyDestOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"new-object\",\n\t\tEncryption: newSSE,\n\t}\n\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tfunction = \"CopyObject(dst, src)\"\n\t\tlogError(testName, function, map[string]interface{}{}, startTime, \"\", \"CopyObject with key rotation failed\", err)\n\t\treturn\n\t}\n\n\t// 4. Download the object.\n\treader, err := c.GetObject(context.Background(), bucketName, \"new-object\", minio.GetObjectOptions{ServerSideEncryption: newSSE})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer reader.Close()\n\n\tdecBytes, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, map[string]interface{}{}, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(decBytes, buf) {\n\t\tlogError(testName, function, map[string]interface{}{}, startTime, \"\", \"Downloaded object doesn't match the empty encrypted object\", err)\n\t\treturn\n\t}\n\n\tdelete(args, \"objectName\")\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testEncryptedCopyObjectWrapper(c *minio.Client, bucketName string, sseSrc, sseDst encrypt.ServerSide) {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncNameLoc(2)\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\tvar srcEncryption, dstEncryption encrypt.ServerSide\n\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr := c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// 1. create an sse-c encrypted object to copy by uploading\n\tconst srcSize = 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"abcde\"), srcSize) // gives a buffer of 5MiB\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObject\", bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{\n\t\tServerSideEncryption: sseSrc,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tif sseSrc != nil && sseSrc.Type() != encrypt.S3 {\n\t\tsrcEncryption = sseSrc\n\t}\n\n\t// 2. copy object and change encryption key\n\tsrc := minio.CopySrcOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"srcObject\",\n\t\tEncryption: srcEncryption,\n\t}\n\targs[\"source\"] = src\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     \"dstObject\",\n\t\tEncryption: sseDst,\n\t}\n\targs[\"destination\"] = dst\n\n\t_, err = c.CopyObject(context.Background(), dst, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\tif sseDst != nil && sseDst.Type() != encrypt.S3 {\n\t\tdstEncryption = sseDst\n\t}\n\t// 3. get copied object and check if content is equal\n\tcoreClient := minio.Core{Client: c}\n\treader, _, _, err := coreClient.GetObject(context.Background(), bucketName, \"dstObject\", minio.GetObjectOptions{ServerSideEncryption: dstEncryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tdecBytes, err := io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(decBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Downloaded object mismatched for encrypted object\", err)\n\t\treturn\n\t}\n\treader.Close()\n\n\t// Test key rotation for source object in-place.\n\tvar newSSE encrypt.ServerSide\n\tif sseSrc != nil && sseSrc.Type() == encrypt.SSEC {\n\t\tnewSSE = encrypt.DefaultPBKDF([]byte(\"Don't Panic\"), []byte(bucketName+\"srcObject\")) // replace key\n\t}\n\tif sseSrc != nil && sseSrc.Type() == encrypt.S3 {\n\t\tnewSSE = encrypt.NewSSE()\n\t}\n\tif newSSE != nil {\n\t\tdst = minio.CopyDestOptions{\n\t\t\tBucket:     bucketName,\n\t\t\tObject:     \"srcObject\",\n\t\t\tEncryption: newSSE,\n\t\t}\n\t\targs[\"destination\"] = dst\n\n\t\t_, err = c.CopyObject(context.Background(), dst, src)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Get copied object and check if content is equal\n\t\treader, _, _, err = coreClient.GetObject(context.Background(), bucketName, \"srcObject\", minio.GetObjectOptions{ServerSideEncryption: newSSE})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tdecBytes, err = io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\t\treturn\n\t\t}\n\t\tif !bytes.Equal(decBytes, buf) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Downloaded object mismatched for encrypted object\", err)\n\t\t\treturn\n\t\t}\n\t\treader.Close()\n\n\t\t// Test in-place decryption.\n\t\tdst = minio.CopyDestOptions{\n\t\t\tBucket: bucketName,\n\t\t\tObject: \"srcObject\",\n\t\t}\n\t\targs[\"destination\"] = dst\n\n\t\tsrc = minio.CopySrcOptions{\n\t\t\tBucket:     bucketName,\n\t\t\tObject:     \"srcObject\",\n\t\t\tEncryption: newSSE,\n\t\t}\n\t\targs[\"source\"] = src\n\t\t_, err = c.CopyObject(context.Background(), dst, src)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject Key rotation failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Get copied decrypted object and check if content is equal\n\treader, _, _, err = coreClient.GetObject(context.Background(), bucketName, \"srcObject\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tdefer reader.Close()\n\n\tdecBytes, err = io.ReadAll(reader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(decBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Downloaded object mismatched for encrypted object\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test encrypted copy object\nfunc testUnencryptedToSSECCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseDst := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"dstObject\"))\n\ttestEncryptedCopyObjectWrapper(c, bucketName, nil, sseDst)\n}\n\n// Test encrypted copy object\nfunc testUnencryptedToSSES3CopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tvar sseSrc encrypt.ServerSide\n\tsseDst := encrypt.NewSSE()\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testUnencryptedToUnencryptedCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tvar sseSrc, sseDst encrypt.ServerSide\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSECToSSECCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"srcObject\"))\n\tsseDst := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"dstObject\"))\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSECToSSES3CopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"srcObject\"))\n\tsseDst := encrypt.NewSSE()\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSECToUnencryptedCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"srcObject\"))\n\tvar sseDst encrypt.ServerSide\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSES3ToSSECCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.NewSSE()\n\tsseDst := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"dstObject\"))\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSES3ToSSES3CopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.NewSSE()\n\tsseDst := encrypt.NewSSE()\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedSSES3ToUnencryptedCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.NewSSE()\n\tvar sseDst encrypt.ServerSide\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\n// Test encrypted copy object\nfunc testEncryptedCopyObjectV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\n\tsseSrc := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"srcObject\"))\n\tsseDst := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+\"dstObject\"))\n\ttestEncryptedCopyObjectWrapper(c, bucketName, sseSrc, sseDst)\n}\n\nfunc testDecryptedCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\tbucketName, objectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\"), \"object\"\n\tif err = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tencryption := encrypt.DefaultPBKDF([]byte(\"correct horse battery staple\"), []byte(bucketName+objectName))\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(bytes.Repeat([]byte(\"a\"), 1024*1024)), 1024*1024, minio.PutObjectOptions{\n\t\tServerSideEncryption: encryption,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tsrc := minio.CopySrcOptions{\n\t\tBucket:     bucketName,\n\t\tObject:     objectName,\n\t\tEncryption: encrypt.SSECopy(encryption),\n\t}\n\targs[\"source\"] = src\n\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"decrypted-\" + objectName,\n\t}\n\targs[\"destination\"] = dst\n\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\tif _, err = c.GetObject(context.Background(), bucketName, \"decrypted-\"+objectName, minio.GetObjectOptions{}); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testSSECMultipartEncryptedToSSECCopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 6MB of data\n\tbuf := bytes.Repeat([]byte(\"abcdef\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tsrcencryption := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\n\t// Upload a 6MB object using multipart mechanism\n\tuploadID, err := c.NewMultipartUpload(context.Background(), bucketName, objectName, minio.PutObjectOptions{ServerSideEncryption: srcencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\tvar completeParts []minio.CompletePart\n\n\tpart, err := c.PutObjectPart(context.Background(), bucketName, objectName, uploadID, 1,\n\t\tbytes.NewReader(buf[:5*1024*1024]), 5*1024*1024,\n\t\tminio.PutObjectPartOptions{SSE: srcencryption},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectPart call failed\", err)\n\t\treturn\n\t}\n\tcompleteParts = append(completeParts, minio.CompletePart{PartNumber: part.PartNumber, ETag: part.ETag})\n\n\tpart, err = c.PutObjectPart(context.Background(), bucketName, objectName, uploadID, 2,\n\t\tbytes.NewReader(buf[5*1024*1024:]), 1024*1024,\n\t\tminio.PutObjectPartOptions{SSE: srcencryption},\n\t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectPart call failed\", err)\n\t\treturn\n\t}\n\tcompleteParts = append(completeParts, minio.CompletePart{PartNumber: part.PartNumber, ETag: part.ETag})\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), bucketName, objectName, uploadID, completeParts, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.DefaultPBKDF([]byte(password), []byte(destBucketName+destObjectName))\n\n\tuploadID, err = c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tencrypt.SSECopy(srcencryption).Marshal(header)\n\tdstencryption.Marshal(header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = objInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err = c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (6*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{ServerSideEncryption: dstencryption}\n\tgetOpts.SetRange(0, 6*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 6*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 6MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(6*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 6*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:6*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 6MB\", err)\n\t\treturn\n\t}\n\tif getBuf[6*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation\nfunc testSSECEncryptedToSSECCopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tsrcencryption := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\tputmetadata := map[string]string{\n\t\t\"Content-Type\": \"binary/octet-stream\",\n\t}\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata:         putmetadata,\n\t\tServerSideEncryption: srcencryption,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.DefaultPBKDF([]byte(password), []byte(destBucketName+destObjectName))\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tencrypt.SSECopy(srcencryption).Marshal(header)\n\tdstencryption.Marshal(header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{ServerSideEncryption: dstencryption}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for SSEC encrypted to unencrypted copy\nfunc testSSECEncryptedToUnencryptedCopyPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tsrcencryption := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t\tServerSideEncryption: srcencryption,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tvar dstencryption encrypt.ServerSide\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tencrypt.SSECopy(srcencryption).Marshal(header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for SSEC encrypted to SSE-S3 encrypted copy\nfunc testSSECEncryptedToSSES3CopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tsrcencryption := encrypt.DefaultPBKDF([]byte(password), []byte(bucketName+objectName))\n\tputmetadata := map[string]string{\n\t\t\"Content-Type\": \"binary/octet-stream\",\n\t}\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata:         putmetadata,\n\t\tServerSideEncryption: srcencryption,\n\t}\n\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.NewSSE()\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tencrypt.SSECopy(srcencryption).Marshal(header)\n\tdstencryption.Marshal(header)\n\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for unencrypted to SSEC encryption copy part\nfunc testUnencryptedToSSECCopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tputmetadata := map[string]string{\n\t\t\"Content-Type\": \"binary/octet-stream\",\n\t}\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: putmetadata,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.DefaultPBKDF([]byte(password), []byte(destBucketName+destObjectName))\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tdstencryption.Marshal(header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{ServerSideEncryption: dstencryption}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for unencrypted to unencrypted copy\nfunc testUnencryptedToUnencryptedCopyPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tputmetadata := map[string]string{\n\t\t\"Content-Type\": \"binary/octet-stream\",\n\t}\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: putmetadata,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for unencrypted to SSE-S3 encrypted copy\nfunc testUnencryptedToSSES3CopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.NewSSE()\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tdstencryption.Marshal(header)\n\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for SSE-S3 to SSEC encryption copy part\nfunc testSSES3EncryptedToSSECCopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tpassword := \"correct horse battery staple\"\n\tsrcEncryption := encrypt.NewSSE()\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t\tServerSideEncryption: srcEncryption,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcEncryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.DefaultPBKDF([]byte(password), []byte(destBucketName+destObjectName))\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tdstencryption.Marshal(header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{ServerSideEncryption: dstencryption}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for unencrypted to unencrypted copy\nfunc testSSES3EncryptedToUnencryptedCopyPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tsrcEncryption := encrypt.NewSSE()\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t\tServerSideEncryption: srcEncryption,\n\t}\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcEncryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\n// Test Core CopyObjectPart implementation for unencrypted to SSE-S3 encrypted copy\nfunc testSSES3EncryptedToSSES3CopyObjectPart() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObjectPart(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tclient, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Instantiate new core client object.\n\tc := minio.Core{Client: client}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, client)\n\t// Make a buffer with 5MB of data\n\tbuf := bytes.Repeat([]byte(\"abcde\"), 1024*1024)\n\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tsrcEncryption := encrypt.NewSSE()\n\topts := minio.PutObjectOptions{\n\t\tUserMetadata: map[string]string{\n\t\t\t\"Content-Type\": \"binary/octet-stream\",\n\t\t},\n\t\tServerSideEncryption: srcEncryption,\n\t}\n\n\tuploadInfo, err := c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), \"\", \"\", opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{ServerSideEncryption: srcEncryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"Error: number of bytes does not match, want %v, got %v\\n\", len(buf), st.Size), err)\n\t\treturn\n\t}\n\n\tdestBucketName := bucketName\n\tdestObjectName := objectName + \"-dest\"\n\tdstencryption := encrypt.NewSSE()\n\n\tuploadID, err := c.NewMultipartUpload(context.Background(), destBucketName, destObjectName, minio.PutObjectOptions{ServerSideEncryption: dstencryption})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"NewMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Content of the destination object will be two copies of\n\t// `objectName` concatenated, followed by first byte of\n\t// `objectName`.\n\tmetadata := make(map[string]string)\n\theader := make(http.Header)\n\tdstencryption.Marshal(header)\n\n\tfor k, v := range header {\n\t\tmetadata[k] = v[0]\n\t}\n\n\tmetadata[\"x-amz-copy-source-if-match\"] = uploadInfo.ETag\n\n\t// First of three parts\n\tfstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 1, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Second of three parts\n\tsndPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 2, 0, -1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Last of three parts\n\tlstPart, err := c.CopyObjectPart(context.Background(), bucketName, objectName, destBucketName, destObjectName, uploadID, 3, 0, 1, metadata)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObjectPart call failed\", err)\n\t\treturn\n\t}\n\n\t// Complete the multipart upload\n\t_, err = c.CompleteMultipartUpload(context.Background(), destBucketName, destObjectName, uploadID, []minio.CompletePart{fstPart, sndPart, lstPart}, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CompleteMultipartUpload call failed\", err)\n\t\treturn\n\t}\n\n\t// Stat the object and check its length matches\n\tobjInfo, err := c.StatObject(context.Background(), destBucketName, destObjectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject call failed\", err)\n\t\treturn\n\t}\n\n\tif objInfo.Size != (5*1024*1024)*2+1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Destination object has incorrect size!\", err)\n\t\treturn\n\t}\n\n\t// Now we read the data back\n\tgetOpts := minio.GetObjectOptions{}\n\tgetOpts.SetRange(0, 5*1024*1024-1)\n\tr, _, _, err := c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf := make([]byte, 5*1024*1024)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in first 5MB\", err)\n\t\treturn\n\t}\n\n\tgetOpts.SetRange(5*1024*1024, 0)\n\tr, _, _, err = c.GetObject(context.Background(), destBucketName, destObjectName, getOpts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject call failed\", err)\n\t\treturn\n\t}\n\tgetBuf = make([]byte, 5*1024*1024+1)\n\t_, err = readFull(r, getBuf)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Read buffer failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(getBuf[:5*1024*1024], buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in second 5MB\", err)\n\t\treturn\n\t}\n\tif getBuf[5*1024*1024] != buf[0] {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got unexpected data in last byte of copied object!\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n\n\t// Do not need to remove destBucketName its same as bucketName.\n}\n\nfunc testUserMetadataCopying() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestUserMetadataCopyingWrapper(c)\n}\n\nfunc testUserMetadataCopyingWrapper(c *minio.Client) {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr := c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tfetchMeta := func(object string) (h http.Header) {\n\t\tobjInfo, err := c.StatObject(context.Background(), bucketName, object, minio.StatObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\t\th = make(http.Header)\n\t\tfor k, vs := range objInfo.Metadata {\n\t\t\tif strings.HasPrefix(strings.ToLower(k), \"x-amz-meta-\") {\n\t\t\t\th.Add(k, vs[0])\n\t\t\t}\n\t\t}\n\t\treturn h\n\t}\n\n\t// 1. create a client encrypted object to copy by uploading\n\tconst srcSize = 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"abcde\"), srcSize) // gives a buffer of 5MiB\n\tmetadata := make(http.Header)\n\tmetadata.Set(\"x-amz-meta-myheader\", \"myvalue\")\n\tm := make(map[string]string)\n\tm[\"x-amz-meta-myheader\"] = \"myvalue\"\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObject\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{UserMetadata: m})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectWithMetadata failed\", err)\n\t\treturn\n\t}\n\tif !reflect.DeepEqual(metadata, fetchMeta(\"srcObject\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\t// 2. create source\n\tsrc := minio.CopySrcOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"srcObject\",\n\t}\n\n\t// 2.1 create destination with metadata set\n\tdst1 := minio.CopyDestOptions{\n\t\tBucket:          bucketName,\n\t\tObject:          \"dstObject-1\",\n\t\tUserMetadata:    map[string]string{\"notmyheader\": \"notmyvalue\"},\n\t\tReplaceMetadata: true,\n\t}\n\n\t// 3. Check that copying to an object with metadata set resets\n\t// the headers on the copy.\n\targs[\"source\"] = src\n\targs[\"destination\"] = dst1\n\t_, err = c.CopyObject(context.Background(), dst1, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\texpectedHeaders := make(http.Header)\n\texpectedHeaders.Set(\"x-amz-meta-notmyheader\", \"notmyvalue\")\n\tif !reflect.DeepEqual(expectedHeaders, fetchMeta(\"dstObject-1\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\t// 4. create destination with no metadata set and same source\n\tdst2 := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"dstObject-2\",\n\t}\n\n\t// 5. Check that copying to an object with no metadata set,\n\t// copies metadata.\n\targs[\"source\"] = src\n\targs[\"destination\"] = dst2\n\t_, err = c.CopyObject(context.Background(), dst2, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed\", err)\n\t\treturn\n\t}\n\n\texpectedHeaders = metadata\n\tif !reflect.DeepEqual(expectedHeaders, fetchMeta(\"dstObject-2\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\t// 6. Compose a pair of sources.\n\tdst3 := minio.CopyDestOptions{\n\t\tBucket:          bucketName,\n\t\tObject:          \"dstObject-3\",\n\t\tReplaceMetadata: true,\n\t}\n\n\tfunction = \"ComposeObject(destination, sources)\"\n\targs[\"source\"] = []minio.CopySrcOptions{src, src}\n\targs[\"destination\"] = dst3\n\t_, err = c.ComposeObject(context.Background(), dst3, src, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject failed\", err)\n\t\treturn\n\t}\n\n\t// Check that no headers are copied in this case\n\tif !reflect.DeepEqual(make(http.Header), fetchMeta(\"dstObject-3\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\t// 7. Compose a pair of sources with dest user metadata set.\n\tdst4 := minio.CopyDestOptions{\n\t\tBucket:          bucketName,\n\t\tObject:          \"dstObject-4\",\n\t\tUserMetadata:    map[string]string{\"notmyheader\": \"notmyvalue\"},\n\t\tReplaceMetadata: true,\n\t}\n\n\tfunction = \"ComposeObject(destination, sources)\"\n\targs[\"source\"] = []minio.CopySrcOptions{src, src}\n\targs[\"destination\"] = dst4\n\t_, err = c.ComposeObject(context.Background(), dst4, src, src)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ComposeObject failed\", err)\n\t\treturn\n\t}\n\n\t// Check that no headers are copied in this case\n\texpectedHeaders = make(http.Header)\n\texpectedHeaders.Set(\"x-amz-meta-notmyheader\", \"notmyvalue\")\n\tif !reflect.DeepEqual(expectedHeaders, fetchMeta(\"dstObject-4\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testUserMetadataCopyingV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"CopyObject(destination, source)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v2 object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestUserMetadataCopyingWrapper(c)\n}\n\nfunc testStorageClassMetadataPutObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\tfunction := \"testStorageClassMetadataPutObject()\"\n\targs := map[string]interface{}{}\n\ttestName := getFuncName()\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tfetchMeta := func(object string) (h http.Header) {\n\t\tobjInfo, err := c.StatObject(context.Background(), bucketName, object, minio.StatObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\t\th = make(http.Header)\n\t\tfor k, vs := range objInfo.Metadata {\n\t\t\tif strings.HasPrefix(strings.ToLower(k), \"x-amz-storage-class\") {\n\t\t\t\tfor _, v := range vs {\n\t\t\t\t\th.Add(k, v)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn h\n\t}\n\n\tmetadata := make(http.Header)\n\tmetadata.Set(\"x-amz-storage-class\", \"REDUCED_REDUNDANCY\")\n\n\temptyMetadata := make(http.Header)\n\n\tconst srcSize = 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"abcde\"), srcSize) // gives a buffer of 1MiB\n\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObjectRRSClass\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{StorageClass: \"REDUCED_REDUNDANCY\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Get the returned metadata\n\treturnedMeta := fetchMeta(\"srcObjectRRSClass\")\n\n\t// The response metada should either be equal to metadata (with REDUCED_REDUNDANCY) or emptyMetadata (in case of gateways)\n\tif !reflect.DeepEqual(metadata, returnedMeta) && !reflect.DeepEqual(emptyMetadata, returnedMeta) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\tmetadata = make(http.Header)\n\tmetadata.Set(\"x-amz-storage-class\", \"STANDARD\")\n\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObjectSSClass\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{StorageClass: \"STANDARD\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\tif reflect.DeepEqual(metadata, fetchMeta(\"srcObjectSSClass\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata verification failed, STANDARD storage class should not be a part of response metadata\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testStorageClassInvalidMetadataPutObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\tfunction := \"testStorageClassInvalidMetadataPutObject()\"\n\targs := map[string]interface{}{}\n\ttestName := getFuncName()\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tconst srcSize = 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"abcde\"), srcSize) // gives a buffer of 1MiB\n\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObjectRRSClass\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{StorageClass: \"INVALID_STORAGE_CLASS\"})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject with invalid storage class passed, was expected to fail\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testStorageClassMetadataCopyObject() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\tfunction := \"testStorageClassMetadataCopyObject()\"\n\targs := map[string]interface{}{}\n\ttestName := getFuncName()\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v4 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test\")\n\t// Make a new bucket in 'us-east-1' (source bucket).\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tfetchMeta := func(object string) (h http.Header) {\n\t\tobjInfo, err := c.StatObject(context.Background(), bucketName, object, minio.StatObjectOptions{})\n\t\targs[\"bucket\"] = bucketName\n\t\targs[\"object\"] = object\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Stat failed\", err)\n\t\t\treturn\n\t\t}\n\t\th = make(http.Header)\n\t\tfor k, vs := range objInfo.Metadata {\n\t\t\tif strings.HasPrefix(strings.ToLower(k), \"x-amz-storage-class\") {\n\t\t\t\tfor _, v := range vs {\n\t\t\t\t\th.Add(k, v)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn h\n\t}\n\n\tmetadata := make(http.Header)\n\tmetadata.Set(\"x-amz-storage-class\", \"REDUCED_REDUNDANCY\")\n\n\temptyMetadata := make(http.Header)\n\n\tconst srcSize = 1024 * 1024\n\tbuf := bytes.Repeat([]byte(\"abcde\"), srcSize)\n\n\t// Put an object with RRS Storage class\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObjectRRSClass\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{StorageClass: \"REDUCED_REDUNDANCY\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Make server side copy of object uploaded in previous step\n\tsrc := minio.CopySrcOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"srcObjectRRSClass\",\n\t}\n\tdst := minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"srcObjectRRSClassCopy\",\n\t}\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed on RRS\", err)\n\t\treturn\n\t}\n\n\t// Get the returned metadata\n\treturnedMeta := fetchMeta(\"srcObjectRRSClassCopy\")\n\n\t// The response metada should either be equal to metadata (with REDUCED_REDUNDANCY) or emptyMetadata (in case of gateways)\n\tif !reflect.DeepEqual(metadata, returnedMeta) && !reflect.DeepEqual(emptyMetadata, returnedMeta) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata match failed\", err)\n\t\treturn\n\t}\n\n\tmetadata = make(http.Header)\n\tmetadata.Set(\"x-amz-storage-class\", \"STANDARD\")\n\n\t// Put an object with Standard Storage class\n\t_, err = c.PutObject(context.Background(), bucketName, \"srcObjectSSClass\",\n\t\tbytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{StorageClass: \"STANDARD\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Make server side copy of object uploaded in previous step\n\tsrc = minio.CopySrcOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"srcObjectSSClass\",\n\t}\n\tdst = minio.CopyDestOptions{\n\t\tBucket: bucketName,\n\t\tObject: \"srcObjectSSClassCopy\",\n\t}\n\tif _, err = c.CopyObject(context.Background(), dst, src); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CopyObject failed on SS\", err)\n\t\treturn\n\t}\n\t// Fetch the meta data of copied object\n\tif reflect.DeepEqual(metadata, fetchMeta(\"srcObjectSSClassCopy\")) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Metadata verification failed, STANDARD storage class should not be a part of response metadata\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test put object with size -1 byte object.\nfunc testPutObjectNoLengthV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"size\":       -1,\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tobjectName := bucketName + \"unique\"\n\targs[\"objectName\"] = objectName\n\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\targs[\"size\"] = bufSize\n\n\t// Upload an object.\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, -1, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectWithSize failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected upload object size \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test put objects of unknown size.\nfunc testPutObjectsUnknownV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader,size,opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"size\":       \"\",\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Issues are revealed by trying to upload multiple files of unknown size\n\t// sequentially (on 4GB machines)\n\tfor i := 1; i <= 4; i++ {\n\t\t// Simulate that we could be receiving byte slices of data that we want\n\t\t// to upload as a file\n\t\trpipe, wpipe := io.Pipe()\n\t\tdefer rpipe.Close()\n\t\tgo func() {\n\t\t\tb := []byte(\"test\")\n\t\t\twpipe.Write(b)\n\t\t\twpipe.Close()\n\t\t}()\n\n\t\t// Upload the object.\n\t\tobjectName := fmt.Sprintf(\"%sunique%d\", bucketName, i)\n\t\targs[\"objectName\"] = objectName\n\n\t\tui, err := c.PutObject(context.Background(), bucketName, objectName, rpipe, -1, minio.PutObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectStreaming failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif ui.Size != 4 {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Expected upload object size \"+string(4)+\" got \"+string(ui.Size), nil)\n\t\t\treturn\n\t\t}\n\n\t\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"StatObjectStreaming failed\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif st.Size != int64(4) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Expected upload object size \"+string(4)+\" got \"+string(st.Size), err)\n\t\t\treturn\n\t\t}\n\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test put object with 0 byte object.\nfunc testPutObject0ByteV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(bucketName, objectName, reader, size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"size\":       0,\n\t\t\"opts\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tobjectName := bucketName + \"unique\"\n\targs[\"objectName\"] = objectName\n\targs[\"opts\"] = minio.PutObjectOptions{}\n\n\t// Upload an object.\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader([]byte(\"\")), 0, minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObjectWithSize failed\", err)\n\t\treturn\n\t}\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObjectWithSize failed\", err)\n\t\treturn\n\t}\n\tif st.Size != 0 {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected upload object size 0 but got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test expected error cases\nfunc testComposeObjectErrorCases() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestComposeObjectErrorCasesWrapper(c)\n}\n\n// Test concatenating multiple 10K objects V4\nfunc testCompose10KSources() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ComposeObject(destination, sourceList)\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\ttestComposeMultipleSources(c)\n}\n\n// Tests comprehensive list of all methods.\nfunc testFunctionalV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"testFunctionalV2()\"\n\tfunctionAll := \"\"\n\targs := map[string]interface{}{}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v2 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\tlocation := \"us-east-1\"\n\t// Make a new bucket.\n\tfunction = \"MakeBucket(bucketName, location)\"\n\tfunctionAll = \"MakeBucket(bucketName, location)\"\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"location\":   location,\n\t}\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: location})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Generate a random file name.\n\tfileName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\tfile, err := os.Create(fileName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"file create failed\", err)\n\t\treturn\n\t}\n\tfor i := 0; i < 3; i++ {\n\t\tbuf := make([]byte, rand.Intn(1<<19))\n\t\t_, err = file.Write(buf)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"file write failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tfile.Close()\n\n\t// Verify if bucket exits and you have access.\n\tvar exists bool\n\tfunction = \"BucketExists(bucketName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t}\n\texists, err = c.BucketExists(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"BucketExists failed\", err)\n\t\treturn\n\t}\n\tif !exists {\n\t\tlogError(testName, function, args, startTime, \"\", \"Could not find existing bucket \"+bucketName, err)\n\t\treturn\n\t}\n\n\t// Make the bucket 'public read/write'.\n\tfunction = \"SetBucketPolicy(bucketName, bucketPolicy)\"\n\tfunctionAll += \", \" + function\n\n\treadWritePolicy := `{\"Version\": \"2012-10-17\",\"Statement\": [{\"Action\": [\"s3:ListBucketMultipartUploads\", \"s3:ListBucket\"],\"Effect\": \"Allow\",\"Principal\": {\"AWS\": [\"*\"]},\"Resource\": [\"arn:aws:s3:::` + bucketName + `\"],\"Sid\": \"\"}]}`\n\n\targs = map[string]interface{}{\n\t\t\"bucketName\":   bucketName,\n\t\t\"bucketPolicy\": readWritePolicy,\n\t}\n\terr = c.SetBucketPolicy(context.Background(), bucketName, readWritePolicy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// List all buckets.\n\tfunction = \"ListBuckets()\"\n\tfunctionAll += \", \" + function\n\targs = nil\n\tbuckets, err := c.ListBuckets(context.Background())\n\tif len(buckets) == 0 {\n\t\tlogError(testName, function, args, startTime, \"\", \"List buckets cannot be empty\", err)\n\t\treturn\n\t}\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ListBuckets failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if previously created bucket is listed in list buckets.\n\tbucketFound := false\n\tfor _, bucket := range buckets {\n\t\tif bucket.Name == bucketName {\n\t\t\tbucketFound = true\n\t\t}\n\t}\n\n\t// If bucket not found error out.\n\tif !bucketFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bucket \"+bucketName+\"not found\", err)\n\t\treturn\n\t}\n\n\tobjectName := bucketName + \"unique\"\n\n\t// Generate data\n\tbuf := bytes.Repeat([]byte(\"n\"), rand.Intn(1<<19))\n\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"contentType\": \"\",\n\t}\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := c.StatObject(context.Background(), bucketName, objectName, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected uploaded object length \"+string(len(buf))+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\tobjectNameNoLength := objectName + \"-nolength\"\n\targs[\"objectName\"] = objectNameNoLength\n\t_, err = c.PutObject(context.Background(), bucketName, objectNameNoLength, bytes.NewReader(buf), int64(len(buf)), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\tst, err = c.StatObject(context.Background(), bucketName, objectNameNoLength, minio.StatObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"StatObject failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(len(buf)) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Expected uploaded object length \"+string(len(buf))+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\n\t// Instantiate a done channel to close all listing.\n\tdoneCh := make(chan struct{})\n\tdefer close(doneCh)\n\n\tobjFound := false\n\tisRecursive := true // Recursive is true.\n\tfunction = \"ListObjects(bucketName, objectName, isRecursive, doneCh)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"isRecursive\": isRecursive,\n\t}\n\tfor obj := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{UseV1: true, Prefix: objectName, Recursive: isRecursive}) {\n\t\tif obj.Key == objectName {\n\t\t\tobjFound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !objFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Could not find existing object \"+objectName, err)\n\t\treturn\n\t}\n\n\tincompObjNotFound := true\n\tfunction = \"ListIncompleteUploads(bucketName, objectName, isRecursive, doneCh)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\":  bucketName,\n\t\t\"objectName\":  objectName,\n\t\t\"isRecursive\": isRecursive,\n\t}\n\tfor objIncompl := range c.ListIncompleteUploads(context.Background(), bucketName, objectName, isRecursive) {\n\t\tif objIncompl.Key != \"\" {\n\t\t\tincompObjNotFound = false\n\t\t\tbreak\n\t\t}\n\t}\n\tif !incompObjNotFound {\n\t\tlogError(testName, function, args, startTime, \"\", \"Unexpected dangling incomplete upload found\", err)\n\t\treturn\n\t}\n\n\tfunction = \"GetObject(bucketName, objectName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t}\n\tnewReader, err := c.GetObject(context.Background(), bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tnewReadBytes, err := io.ReadAll(newReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tnewReader.Close()\n\n\tif !bytes.Equal(newReadBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch\", err)\n\t\treturn\n\t}\n\n\tfunction = \"FGetObject(bucketName, objectName, fileName)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"fileName\":   fileName + \"-f\",\n\t}\n\terr = c.FGetObject(context.Background(), bucketName, objectName, fileName+\"-f\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FgetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Generate presigned HEAD object url.\n\tfunction = \"PresignedHeadObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedHeadURL, err := c.PresignedHeadObject(context.Background(), bucketName, objectName, 3600*time.Second, nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject failed\", err)\n\t\treturn\n\t}\n\n\thttpClient := &http.Client{\n\t\t// Setting a sensible time out of 30secs to wait for response\n\t\t// headers. Request is pro-actively canceled after 30secs\n\t\t// with no response.\n\t\tTimeout:   30 * time.Second,\n\t\tTransport: createHTTPTransport(),\n\t}\n\n\treq, err := http.NewRequest(http.MethodHead, presignedHeadURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject URL head request failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\tresp, err := httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject URL head request failed\", err)\n\t\treturn\n\t}\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedHeadObject URL returns status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tif resp.Header.Get(\"ETag\") == \"\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Got empty ETag\", err)\n\t\treturn\n\t}\n\tresp.Body.Close()\n\n\t// Generate presigned GET object url.\n\tfunction = \"PresignedGetObject(bucketName, objectName, expires, reqParams)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName,\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedGetURL, err := c.PresignedGetObject(context.Background(), bucketName, objectName, 3600*time.Second, nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\treq, err = http.NewRequest(http.MethodGet, presignedGetURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject request incorrect\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject URL returns status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tnewPresignedBytes, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tresp.Body.Close()\n\tif !bytes.Equal(newPresignedBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch\", err)\n\t\treturn\n\t}\n\n\t// Set request parameters.\n\treqParams := make(url.Values)\n\treqParams.Set(\"response-content-disposition\", \"attachment; filename=\\\"test.txt\\\"\")\n\t// Generate presigned GET object url.\n\targs[\"reqParams\"] = reqParams\n\tpresignedGetURL, err = c.PresignedGetObject(context.Background(), bucketName, objectName, 3600*time.Second, reqParams)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject failed\", err)\n\t\treturn\n\t}\n\n\t// Verify if presigned url works.\n\treq, err = http.NewRequest(http.MethodGet, presignedGetURL.String(), nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject request incorrect\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject response incorrect\", err)\n\t\treturn\n\t}\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedGetObject URL returns status \"+string(resp.StatusCode), err)\n\t\treturn\n\t}\n\tnewPresignedBytes, err = io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed\", err)\n\t\treturn\n\t}\n\tif !bytes.Equal(newPresignedBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch\", err)\n\t\treturn\n\t}\n\t// Verify content disposition.\n\tif resp.Header.Get(\"Content-Disposition\") != \"attachment; filename=\\\"test.txt\\\"\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"wrong Content-Disposition received \", err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignedPutObject(bucketName, objectName, expires)\"\n\tfunctionAll += \", \" + function\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName + \"-presigned\",\n\t\t\"expires\":    3600 * time.Second,\n\t}\n\tpresignedPutURL, err := c.PresignedPutObject(context.Background(), bucketName, objectName+\"-presigned\", 3600*time.Second)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PresignedPutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Generate data more than 32K\n\tbuf = bytes.Repeat([]byte(\"1\"), rand.Intn(1<<10)+32*1024)\n\n\treq, err = http.NewRequest(http.MethodPut, presignedPutURL.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request to PresignedPutObject URL failed\", err)\n\t\treturn\n\t}\n\n\tresp, err = httpClient.Do(req)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request to PresignedPutObject URL failed\", err)\n\t\treturn\n\t}\n\n\t// Download the uploaded object to verify\n\targs = map[string]interface{}{\n\t\t\"bucketName\": bucketName,\n\t\t\"objectName\": objectName + \"-presigned\",\n\t}\n\tnewReader, err = c.GetObject(context.Background(), bucketName, objectName+\"-presigned\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject of uploaded presigned object failed\", err)\n\t\treturn\n\t}\n\n\tnewReadBytes, err = io.ReadAll(newReader)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"ReadAll failed during get on presigned put object\", err)\n\t\treturn\n\t}\n\tnewReader.Close()\n\n\tif !bytes.Equal(newReadBytes, buf) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Bytes mismatch on presigned object upload verification\", err)\n\t\treturn\n\t}\n\n\tfunction = \"PresignHeader(method, bucketName, objectName, expires, reqParams, extraHeaders)\"\n\tfunctionAll += \", \" + function\n\tpresignExtraHeaders := map[string][]string{\n\t\t\"mysecret\": {\"abcxxx\"},\n\t}\n\targs = map[string]interface{}{\n\t\t\"method\":       \"PUT\",\n\t\t\"bucketName\":   bucketName,\n\t\t\"objectName\":   objectName + \"-presign-custom\",\n\t\t\"expires\":      3600 * time.Second,\n\t\t\"extraHeaders\": presignExtraHeaders,\n\t}\n\t_, err = c.PresignHeader(context.Background(), \"PUT\", bucketName, objectName+\"-presign-custom\", 3600*time.Second, nil, presignExtraHeaders)\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Presigned with extra headers succeeded\", err)\n\t\treturn\n\t}\n\n\tos.Remove(fileName)\n\tos.Remove(fileName + \"-f\")\n\tlogSuccess(testName, functionAll, args, startTime)\n}\n\n// Test get object with GetObject with context\nfunc testGetObjectContext() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(ctx, bucketName, objectName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tcancel()\n\n\tr, err := c.GetObject(ctx, bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed unexpectedly\", err)\n\t\treturn\n\t}\n\n\tif _, err = r.Stat(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tr.Close()\n\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Read the data back\n\tr, err = c.GetObject(ctx, bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"object Stat call failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match: want \"+string(bufSize)+\", got\"+string(st.Size), err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"object Close() call failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object with FGetObject with a user provided context\nfunc testFGetObjectContext() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FGetObject(ctx, bucketName, objectName, fileName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-1-MB\"]\n\treader := getDataReader(\"datafile-1-MB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\tfileName := \"tempfile-context\"\n\targs[\"fileName\"] = fileName\n\t// Read the data back\n\terr = c.FGetObject(ctx, bucketName, objectName, fileName+\"-f\", minio.GetObjectOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\tdefer cancel()\n\n\t// Read the data back\n\terr = c.FGetObject(ctx, bucketName, objectName, fileName+\"-fcontext\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject with long timeout failed\", err)\n\t\treturn\n\t}\n\tif err = os.Remove(fileName + \"-fcontext\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Remove file failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object with GetObject with a user provided context\nfunc testGetObjectRanges() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(ctx, bucketName, objectName, fileName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t}\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)\n\tdefer cancel()\n\n\trng := rand.NewSource(time.Now().UnixNano())\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rng, \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rng, \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\t// Read the data back\n\ttests := []struct {\n\t\tstart int64\n\t\tend   int64\n\t}{\n\t\t{\n\t\t\tstart: 1024,\n\t\t\tend:   1024 + 1<<20,\n\t\t},\n\t\t{\n\t\t\tstart: 20e6,\n\t\t\tend:   20e6 + 10000,\n\t\t},\n\t\t{\n\t\t\tstart: 40e6,\n\t\t\tend:   40e6 + 10000,\n\t\t},\n\t\t{\n\t\t\tstart: 60e6,\n\t\t\tend:   60e6 + 10000,\n\t\t},\n\t\t{\n\t\t\tstart: 80e6,\n\t\t\tend:   80e6 + 10000,\n\t\t},\n\t\t{\n\t\t\tstart: 120e6,\n\t\t\tend:   int64(bufSize),\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\twantRC := getDataReader(\"datafile-129-MB\")\n\t\tio.CopyN(io.Discard, wantRC, test.start)\n\t\twant := mustCrcReader(io.LimitReader(wantRC, test.end-test.start+1))\n\t\topts := minio.GetObjectOptions{}\n\t\topts.SetRange(test.start, test.end)\n\t\targs[\"opts\"] = fmt.Sprintf(\"%+v\", test)\n\t\tobj, err := c.GetObject(ctx, bucketName, objectName, opts)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject with long timeout failed\", err)\n\t\t\treturn\n\t\t}\n\t\terr = crcMatches(obj, want)\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"GetObject offset %d -> %d\", test.start, test.end), err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object ACLs with GetObjectACL with custom provided context\nfunc testGetObjectACLContext() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObjectACL(ctx, bucketName, objectName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-1-MB\"]\n\treader := getDataReader(\"datafile-1-MB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Add meta data to add a canned acl\n\tmetaData := map[string]string{\n\t\t\"X-Amz-Acl\": \"public-read-write\",\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName,\n\t\tobjectName, reader, int64(bufSize),\n\t\tminio.PutObjectOptions{\n\t\t\tContentType:  \"binary/octet-stream\",\n\t\t\tUserMetadata: metaData,\n\t\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Read the data back\n\tobjectInfo, getObjectACLErr := c.GetObjectACL(ctx, bucketName, objectName)\n\tif getObjectACLErr != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL failed. \", getObjectACLErr)\n\t\treturn\n\t}\n\n\ts, ok := objectInfo.Metadata[\"X-Amz-Acl\"]\n\tif !ok {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail unable to find \\\"X-Amz-Acl\\\"\", nil)\n\t\treturn\n\t}\n\n\tif len(s) != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Acl\\\" canned acl expected \\\"1\\\" got \"+fmt.Sprintf(`\"%d\"`, len(s)), nil)\n\t\treturn\n\t}\n\n\t// Do a very limited testing if this is not AWS S3\n\tif os.Getenv(serverEndpoint) != \"s3.amazonaws.com\" {\n\t\tif s[0] != \"private\" {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Acl\\\" expected \\\"private\\\" but got\"+fmt.Sprintf(\"%q\", s[0]), nil)\n\t\t\treturn\n\t\t}\n\n\t\tlogSuccess(testName, function, args, startTime)\n\t\treturn\n\t}\n\n\tif s[0] != \"public-read-write\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Acl\\\" expected \\\"public-read-write\\\" but got\"+fmt.Sprintf(\"%q\", s[0]), nil)\n\t\treturn\n\t}\n\n\tbufSize = dataFileMap[\"datafile-1-MB\"]\n\treader2 := getDataReader(\"datafile-1-MB\")\n\tdefer reader2.Close()\n\t// Save the data\n\tobjectName = randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Add meta data to add a canned acl\n\tmetaData = map[string]string{\n\t\t\"X-Amz-Grant-Read\":  \"id=fooread@minio.go\",\n\t\t\"X-Amz-Grant-Write\": \"id=foowrite@minio.go\",\n\t}\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader2, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\", UserMetadata: metaData})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel = context.WithTimeout(context.Background(), 10*time.Second)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\t// Read the data back\n\tobjectInfo, getObjectACLErr = c.GetObjectACL(ctx, bucketName, objectName)\n\tif getObjectACLErr == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail\", getObjectACLErr)\n\t\treturn\n\t}\n\n\tif len(objectInfo.Metadata) != 3 {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail expected \\\"3\\\" ACLs but got \"+fmt.Sprintf(`\"%d\"`, len(objectInfo.Metadata)), nil)\n\t\treturn\n\t}\n\n\ts, ok = objectInfo.Metadata[\"X-Amz-Grant-Read\"]\n\tif !ok {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail unable to find \\\"X-Amz-Grant-Read\\\"\", nil)\n\t\treturn\n\t}\n\n\tif len(s) != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Grant-Read\\\" acl expected \\\"1\\\" got \"+fmt.Sprintf(`\"%d\"`, len(s)), nil)\n\t\treturn\n\t}\n\n\tif s[0] != \"fooread@minio.go\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Grant-Read\\\" acl expected \\\"fooread@minio.go\\\" got \"+fmt.Sprintf(\"%q\", s), nil)\n\t\treturn\n\t}\n\n\ts, ok = objectInfo.Metadata[\"X-Amz-Grant-Write\"]\n\tif !ok {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail unable to find \\\"X-Amz-Grant-Write\\\"\", nil)\n\t\treturn\n\t}\n\n\tif len(s) != 1 {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Grant-Write\\\" acl expected \\\"1\\\" got \"+fmt.Sprintf(`\"%d\"`, len(s)), nil)\n\t\treturn\n\t}\n\n\tif s[0] != \"foowrite@minio.go\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObjectACL fail \\\"X-Amz-Grant-Write\\\" acl expected \\\"foowrite@minio.go\\\" got \"+fmt.Sprintf(\"%q\", s), nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test validates putObject with context to see if request cancellation is honored for V2.\nfunc testPutObjectContextV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"PutObject(ctx, bucketName, objectName, reader, size, opts)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"size\":       \"\",\n\t\t\"opts\":       \"\",\n\t}\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Make a new bucket.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\tbufSize := dataFileMap[\"datatfile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\n\tobjectName := fmt.Sprintf(\"test-file-%v\", rand.Uint32())\n\targs[\"objectName\"] = objectName\n\n\tctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n\targs[\"ctx\"] = ctx\n\targs[\"size\"] = bufSize\n\tdefer cancel()\n\n\t_, err = c.PutObject(ctx, bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject with short timeout failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\targs[\"ctx\"] = ctx\n\n\tdefer cancel()\n\treader = getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\t_, err = c.PutObject(ctx, bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject with long timeout failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object with GetObject with custom context\nfunc testGetObjectContextV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetObject(ctx, bucketName, objectName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\treader := getDataReader(\"datafile-33-kB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tcancel()\n\n\tr, err := c.GetObject(ctx, bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject failed unexpectedly\", err)\n\t\treturn\n\t}\n\tif _, err = r.Stat(); err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tr.Close()\n\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\tdefer cancel()\n\n\t// Read the data back\n\tr, err = c.GetObject(ctx, bucketName, objectName, minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetObject shouldn't fail on longer timeout\", err)\n\t\treturn\n\t}\n\n\tst, err := r.Stat()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"object Stat call failed\", err)\n\t\treturn\n\t}\n\tif st.Size != int64(bufSize) {\n\t\tlogError(testName, function, args, startTime, \"\", \"Number of bytes in stat does not match, expected \"+string(bufSize)+\" got \"+string(st.Size), err)\n\t\treturn\n\t}\n\tif err := r.Close(); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \" object Close() call failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get object with FGetObject with custom context\nfunc testFGetObjectContextV2() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"FGetObject(ctx, bucketName, objectName,fileName)\"\n\targs := map[string]interface{}{\n\t\t\"ctx\":        \"\",\n\t\t\"bucketName\": \"\",\n\t\t\"objectName\": \"\",\n\t\t\"fileName\":   \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{CredsV2: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO v2 client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket call failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\tbufSize := dataFileMap[\"datatfile-1-MB\"]\n\treader := getDataReader(\"datafile-1-MB\")\n\tdefer reader.Close()\n\t// Save the data\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Nanosecond)\n\targs[\"ctx\"] = ctx\n\tdefer cancel()\n\n\tfileName := \"tempfile-context\"\n\targs[\"fileName\"] = fileName\n\n\t// Read the data back\n\terr = c.FGetObject(ctx, bucketName, objectName, fileName+\"-f\", minio.GetObjectOptions{})\n\tif err == nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject should fail on short timeout\", err)\n\t\treturn\n\t}\n\tctx, cancel = context.WithTimeout(context.Background(), 1*time.Hour)\n\tdefer cancel()\n\n\t// Read the data back\n\terr = c.FGetObject(ctx, bucketName, objectName, fileName+\"-fcontext\", minio.GetObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"FGetObject call shouldn't fail on long timeout\", err)\n\t\treturn\n\t}\n\n\tif err = os.Remove(fileName + \"-fcontext\"); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Remove file failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test list object v1 and V2\nfunc testListObjects() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"ListObjects(bucketName, objectPrefix, recursive, doneCh)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":   \"\",\n\t\t\"objectPrefix\": \"\",\n\t\t\"recursive\":    \"true\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tdefer cleanupBucket(bucketName, c)\n\n\ttestObjects := []struct {\n\t\tname         string\n\t\tstorageClass string\n\t}{\n\t\t// Special characters\n\t\t{\"foo bar\", \"STANDARD\"},\n\t\t{\"foo-%\", \"STANDARD\"},\n\t\t{\"random-object-1\", \"STANDARD\"},\n\t\t{\"random-object-2\", \"REDUCED_REDUNDANCY\"},\n\t}\n\n\tfor i, object := range testObjects {\n\t\tbufSize := dataFileMap[\"datafile-33-kB\"]\n\t\treader := getDataReader(\"datafile-33-kB\")\n\t\tdefer reader.Close()\n\t\t_, err = c.PutObject(context.Background(), bucketName, object.name, reader, int64(bufSize),\n\t\t\tminio.PutObjectOptions{ContentType: \"binary/octet-stream\", StorageClass: object.storageClass})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", fmt.Sprintf(\"PutObject %d call failed\", i+1), err)\n\t\t\treturn\n\t\t}\n\t}\n\n\ttestList := func(listFn func(context.Context, string, minio.ListObjectsOptions) <-chan minio.ObjectInfo, bucket string, opts minio.ListObjectsOptions) {\n\t\tvar objCursor int\n\n\t\t// check for object name and storage-class from listing object result\n\t\tfor objInfo := range listFn(context.Background(), bucket, opts) {\n\t\t\tif objInfo.Err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"ListObjects failed unexpectedly\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif objInfo.Key != testObjects[objCursor].name {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"ListObjects does not return expected object name\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif objInfo.StorageClass != testObjects[objCursor].storageClass {\n\t\t\t\t// Ignored as Gateways (Azure/GCS etc) wont return storage class\n\t\t\t\tlogIgnored(testName, function, args, startTime, \"ListObjects doesn't return expected storage class\")\n\t\t\t}\n\t\t\tobjCursor++\n\t\t}\n\n\t\tif objCursor != len(testObjects) {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"ListObjects returned unexpected number of items\", errors.New(\"\"))\n\t\t\treturn\n\t\t}\n\t}\n\n\ttestList(c.ListObjects, bucketName, minio.ListObjectsOptions{Recursive: true, UseV1: true})\n\ttestList(c.ListObjects, bucketName, minio.ListObjectsOptions{Recursive: true})\n\ttestList(c.ListObjects, bucketName, minio.ListObjectsOptions{Recursive: true, WithMetadata: true})\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// testCors is runnable against S3 itself.\n// Just provide the env var MINIO_GO_TEST_BUCKET_CORS with bucket that is public and WILL BE DELETED.\n// Recreate this manually each time. Minio-go SDK does not support calling\n// SetPublicBucket (put-public-access-block) on S3, otherwise we could script the whole thing.\nfunc testCors() {\n\tctx := context.Background()\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"SetBucketCors(bucketName, cors)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"cors\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Create or reuse a bucket that will get cors settings applied to it and deleted when done\n\tbucketName := os.Getenv(\"MINIO_GO_TEST_BUCKET_CORS\")\n\tif bucketName == \"\" {\n\t\tbucketName = randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\t\terr = c.MakeBucket(ctx, bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\t\tif err != nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\t\treturn\n\t\t}\n\t}\n\targs[\"bucketName\"] = bucketName\n\tdefer cleanupBucket(bucketName, c)\n\n\tpublicPolicy := `{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"*\"]},\"Action\":[\"s3:*\"],\"Resource\":[\"arn:aws:s3:::` + bucketName + `\", \"arn:aws:s3:::` + bucketName + `/*\"]}]}`\n\terr = c.SetBucketPolicy(ctx, bucketName, publicPolicy)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketPolicy failed\", err)\n\t\treturn\n\t}\n\n\t// Upload an object for testing.\n\tobjectContents := `some-text-file-contents`\n\treader := strings.NewReader(objectContents)\n\tbufSize := int64(len(objectContents))\n\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t_, err = c.PutObject(ctx, bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{ContentType: \"binary/octet-stream\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"PutObject call failed\", err)\n\t\treturn\n\t}\n\tbucketURL := c.EndpointURL().String() + \"/\" + bucketName + \"/\"\n\tobjectURL := bucketURL + objectName\n\n\thttpClient := &http.Client{\n\t\tTimeout:   30 * time.Second,\n\t\tTransport: createHTTPTransport(),\n\t}\n\n\terrStrAccessForbidden := `<Error><Code>AccessForbidden</Code><Message>CORSResponse: This CORS request is not allowed. This is usually because the evalution of Origin, request method / Access-Control-Request-Method or Access-Control-Request-Headers are not whitelisted`\n\ttestCases := []struct {\n\t\tname string\n\n\t\t// Cors rules to apply\n\t\tapplyCorsRules []cors.Rule\n\n\t\t// Outbound request info\n\t\tmethod  string\n\t\turl     string\n\t\theaders map[string]string\n\n\t\t// Wanted response\n\t\twantStatus       int\n\t\twantHeaders      map[string]string\n\t\twantBodyContains string\n\t}{\n\t\t{\n\t\t\tname: \"apply bucket rules\",\n\t\t\tapplyCorsRules: []cors.Rule{\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"https\"}, // S3 documents 'https' origin, but it does not actually work, see test below.\n\t\t\t\t\tAllowedMethod: []string{\"PUT\"},\n\t\t\t\t\tAllowedHeader: []string{\"*\"},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"http://www.example1.com\"},\n\t\t\t\t\tAllowedMethod: []string{\"PUT\"},\n\t\t\t\t\tAllowedHeader: []string{\"*\"},\n\t\t\t\t\tExposeHeader:  []string{\"x-amz-server-side-encryption\", \"x-amz-request-id\"},\n\t\t\t\t\tMaxAgeSeconds: 3600,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"http://www.example2.com\"},\n\t\t\t\t\tAllowedMethod: []string{\"POST\"},\n\t\t\t\t\tAllowedHeader: []string{\"X-My-Special-Header\"},\n\t\t\t\t\tExposeHeader:  []string{\"X-AMZ-Request-ID\"},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"http://www.example3.com\"},\n\t\t\t\t\tAllowedMethod: []string{\"PUT\"},\n\t\t\t\t\tAllowedHeader: []string{\"X-Example-3-Special-Header\"},\n\t\t\t\t\tMaxAgeSeconds: 10,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"*\"},\n\t\t\t\t\tAllowedMethod: []string{\"GET\"},\n\t\t\t\t\tAllowedHeader: []string{\"*\"},\n\t\t\t\t\tExposeHeader:  []string{\"x-amz-request-id\", \"X-AMZ-server-side-encryption\"},\n\t\t\t\t\tMaxAgeSeconds: 3600,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"http://multiplemethodstest.com\"},\n\t\t\t\t\tAllowedMethod: []string{\"POST\", \"PUT\", \"DELETE\"},\n\t\t\t\t\tAllowedHeader: []string{\"x-abc-*\", \"x-def-*\"},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"http://UPPERCASEEXAMPLE.com\"},\n\t\t\t\t\tAllowedMethod: []string{\"DELETE\"},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tAllowedOrigin: []string{\"https://*\"},\n\t\t\t\t\tAllowedMethod: []string{\"DELETE\"},\n\t\t\t\t\tAllowedHeader: []string{\"x-abc-*\", \"x-def-*\"},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight to object url matches example1 rule\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"PUT\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-another-header,x-could-be-anything\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-another-header,x-could-be-anything\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t\t// S3 additionally sets the following headers here, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Expose-Headers\":    \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight to bucket url matches example1 rule\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    bucketURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"PUT\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-another-header,x-could-be-anything\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-another-header,x-could-be-anything\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight matches example2 rule with header given\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.example2.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"POST\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"X-My-Special-Header\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example2.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"POST\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-my-special-header\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight matches example2 rule with no header given\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.example2.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"POST\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example2.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"POST\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight matches wildcard origin rule\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.couldbeanything.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"GET\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-custom-header,x-other-custom-header\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"*\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"GET\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-custom-header,x-other-custom-header\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight does not match any rule\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.couldbeanything.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"DELETE\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight does not match example1 rule because of method\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"POST\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"s3 processes cors rules even when request is not preflight if cors headers present test get\",\n\t\t\tmethod: http.MethodGet,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-another-header,x-could-be-anything\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"PUT\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"x-amz-server-side-encryption,x-amz-request-id\",\n\t\t\t\t// S3 additionally sets the following headers here, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Allow-Headers\":     \"x-another-header,x-could-be-anything\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t// \"Access-Control-Max-Age\":           \"3600\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"s3 processes cors rules even when request is not preflight if cors headers present test put\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"GET\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"*\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"x-amz-request-id,x-amz-server-side-encryption\",\n\t\t\t\t// S3 additionally sets the following headers here, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Allow-Headers\":     \"x-another-header,x-could-be-anything\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t// \"Access-Control-Max-Age\":           \"3600\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"s3 processes cors rules even when request is not preflight but there is no rule match\",\n\t\t\tmethod: http.MethodGet,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-another-header,x-could-be-anything\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"DELETE\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"get request matches wildcard origin rule and returns cors headers\",\n\t\t\tmethod: http.MethodGet,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.example1.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"*\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"x-amz-request-id,X-AMZ-server-side-encryption\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"GET\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"head request does not match rule and returns no cors headers\",\n\t\t\tmethod: http.MethodHead,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.nomatchingdomainfound.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"put request with origin does not match rule and returns no cors headers\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.nomatchingdomainfound.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:       \"put request with no origin does not match rule and returns no cors headers\",\n\t\t\tmethod:     http.MethodPut,\n\t\t\turl:        objectURL,\n\t\t\theaders:    map[string]string{},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight for delete request with wildcard origin does not match\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.notsecureexample.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"DELETE\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight for delete request with wildcard https origin matches secureexample\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"https://www.secureexample.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"DELETE\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"DELETE\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"https://www.secureexample.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight for delete request matches secureexample with wildcard https origin and request headers\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"https://www.secureexample.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"DELETE\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-abc-1,x-abc-second,x-def-1\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"DELETE\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"https://www.secureexample.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-abc-1,x-abc-second,x-def-1\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight for delete request matches secureexample rejected because request header does not match\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"https://www.secureexample.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"DELETE\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-abc-1,x-abc-second,x-def-1,x-does-not-match\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight with https origin is documented by s3 as matching but it does not match\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"https://www.securebutdoesnotmatch.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"PUT\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:       \"put no origin no match returns no cors headers\",\n\t\t\tmethod:     http.MethodPut,\n\t\t\turl:        objectURL,\n\t\t\theaders:    map[string]string{},\n\t\t\twantStatus: http.StatusOK,\n\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"put with origin match example1 returns cors headers\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.example1.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"x-amz-server-side-encryption,x-amz-request-id\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"put with origin and header match example1 returns cors headers\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":              \"http://www.example1.com\",\n\t\t\t\t\"x-could-be-anything\": \"myvalue\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"x-amz-server-side-encryption,x-amz-request-id\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"put no match found returns no cors headers\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.unmatchingdomain.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"put with origin match example3 returns cors headers\",\n\t\t\tmethod: http.MethodPut,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":              \"http://www.example3.com\",\n\t\t\t\t\"X-My-Special-Header\": \"myvalue\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example3.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Max-Age\":           \"10\",\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight matches example1 rule headers case is incorrect\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"PUT\",\n\t\t\t\t// Fetch standard guarantees that these are sent lowercase, here we test what happens when they are not.\n\t\t\t\t\"Access-Control-Request-Headers\": \"X-Another-Header,X-Could-Be-Anything\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-another-header,x-could-be-anything\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Expose-Headers\":    \"x-amz-server-side-encryption,x-amz-request-id\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight matches example1 rule headers are not sorted\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"PUT\",\n\t\t\t\t// Fetch standard guarantees that these are sorted, test what happens when they are not.\n\t\t\t\t\"Access-Control-Request-Headers\": \"a-customer-header,b-should-be-last\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://www.example1.com\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"PUT\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"a-customer-header,b-should-be-last\",\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"3600\",\n\t\t\t\t\"Content-Length\":                   \"0\",\n\t\t\t\t// S3 returns the following headers, MinIO follows fetch spec and does not:\n\t\t\t\t// \"Access-Control-Expose-Headers\":    \"x-amz-server-side-encryption,x-amz-request-id\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight with case sensitivity in origin matches uppercase\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://UPPERCASEEXAMPLE.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"DELETE\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Methods\":     \"DELETE\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://UPPERCASEEXAMPLE.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight with case sensitivity in origin does not match when lowercase\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                        \"http://uppercaseexample.com\",\n\t\t\t\t\"Access-Control-Request-Method\": \"DELETE\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight match upper case with unknown header but no header restrictions\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://UPPERCASEEXAMPLE.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"DELETE\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-unknown-1\",\n\t\t\t},\n\t\t\twantStatus:       http.StatusForbidden,\n\t\t\twantBodyContains: errStrAccessForbidden,\n\t\t},\n\t\t{\n\t\t\tname:   \"preflight for delete request matches multiplemethodstest.com origin and request headers\",\n\t\t\tmethod: http.MethodOptions,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\":                         \"http://multiplemethodstest.com\",\n\t\t\t\t\"Access-Control-Request-Method\":  \"DELETE\",\n\t\t\t\t\"Access-Control-Request-Headers\": \"x-abc-1\",\n\t\t\t},\n\t\t\twantStatus: http.StatusOK,\n\t\t\twantHeaders: map[string]string{\n\t\t\t\t\"Access-Control-Allow-Credentials\": \"true\",\n\t\t\t\t\"Access-Control-Allow-Origin\":      \"http://multiplemethodstest.com\",\n\t\t\t\t\"Access-Control-Allow-Headers\":     \"x-abc-1\",\n\t\t\t\t\"Access-Control-Expose-Headers\":    \"\",\n\t\t\t\t\"Access-Control-Max-Age\":           \"\",\n\t\t\t\t// S3 returns POST, PUT, DELETE here, MinIO does not as spec does not require it.\n\t\t\t\t// \"Access-Control-Allow-Methods\":     \"DELETE\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"delete request goes ahead because cors is only for browsers and does not block on the server side\",\n\t\t\tmethod: http.MethodDelete,\n\t\t\turl:    objectURL,\n\t\t\theaders: map[string]string{\n\t\t\t\t\"Origin\": \"http://www.justrandom.com\",\n\t\t\t},\n\t\t\twantStatus: http.StatusNoContent,\n\t\t},\n\t}\n\n\tfor i, test := range testCases {\n\t\ttestName := fmt.Sprintf(\"%s_%d_%s\", testName, i+1, strings.ReplaceAll(test.name, \" \", \"_\"))\n\n\t\t// Apply the CORS rules\n\t\tif test.applyCorsRules != nil {\n\t\t\tcorsConfig := &cors.Config{\n\t\t\t\tCORSRules: test.applyCorsRules,\n\t\t\t}\n\t\t\terr = c.SetBucketCors(ctx, bucketName, corsConfig)\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketCors failed to apply\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Make request\n\t\tif test.method != \"\" && test.url != \"\" {\n\t\t\treq, err := http.NewRequestWithContext(ctx, test.method, test.url, nil)\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request creation failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\treq.Header.Set(\"User-Agent\", \"MinIO-go-FunctionalTest/\"+appVersion)\n\n\t\t\tfor k, v := range test.headers {\n\t\t\t\treq.Header.Set(k, v)\n\t\t\t}\n\t\t\tresp, err := httpClient.Do(req)\n\t\t\tif err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"HTTP request failed\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer resp.Body.Close()\n\n\t\t\t// Check returned status code\n\t\t\tif resp.StatusCode != test.wantStatus {\n\t\t\t\terrStr := fmt.Sprintf(\" incorrect status code in response, want: %d, got: %d\", test.wantStatus, resp.StatusCode)\n\t\t\t\tlogError(testName, function, args, startTime, \"\", errStr, nil)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Check returned body\n\t\t\tif test.wantBodyContains != \"\" {\n\t\t\t\tbody, err := io.ReadAll(resp.Body)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Failed to read response body\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif !strings.Contains(string(body), test.wantBodyContains) {\n\t\t\t\t\terrStr := fmt.Sprintf(\" incorrect body in response, want: %s, in got: %s\", test.wantBodyContains, string(body))\n\t\t\t\t\tlogError(testName, function, args, startTime, \"\", errStr, nil)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check returned response headers\n\t\t\tfor k, v := range test.wantHeaders {\n\t\t\t\tgotVal := resp.Header.Get(k)\n\t\t\t\tif k == \"Access-Control-Expose-Headers\" {\n\t\t\t\t\t// MinIO returns this in canonical form, S3 does not.\n\t\t\t\t\tgotVal = strings.ToLower(gotVal)\n\t\t\t\t\tv = strings.ToLower(v)\n\t\t\t\t}\n\t\t\t\t// Remove all spaces, S3 adds spaces after CSV values in headers, MinIO does not.\n\t\t\t\tgotVal = strings.ReplaceAll(gotVal, \" \", \"\")\n\t\t\t\tif gotVal != v {\n\t\t\t\t\terrStr := fmt.Sprintf(\" incorrect header in response, want: %s: '%s', got: '%s'\", k, v, gotVal)\n\t\t\t\t\tlogError(testName, function, args, startTime, \"\", errStr, nil)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tlogSuccess(testName, function, args, startTime)\n\t}\n\tlogSuccess(testName, function, args, startTime)\n}\n\nfunc testCorsSetGetDelete() {\n\tctx := context.Background()\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"SetBucketCors(bucketName, cors)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"cors\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(ctx, bucketName, minio.MakeBucketOptions{Region: \"us-east-1\"})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\tdefer cleanupBucket(bucketName, c)\n\n\t// Set the CORS rules on the new bucket\n\tcorsRules := []cors.Rule{\n\t\t{\n\t\t\tAllowedOrigin: []string{\"http://www.example1.com\"},\n\t\t\tAllowedMethod: []string{\"PUT\"},\n\t\t\tAllowedHeader: []string{\"*\"},\n\t\t},\n\t\t{\n\t\t\tAllowedOrigin: []string{\"http://www.example2.com\"},\n\t\t\tAllowedMethod: []string{\"POST\"},\n\t\t\tAllowedHeader: []string{\"X-My-Special-Header\"},\n\t\t},\n\t\t{\n\t\t\tAllowedOrigin: []string{\"*\"},\n\t\t\tAllowedMethod: []string{\"GET\"},\n\t\t\tAllowedHeader: []string{\"*\"},\n\t\t},\n\t}\n\tcorsConfig := cors.NewConfig(corsRules)\n\terr = c.SetBucketCors(ctx, bucketName, corsConfig)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketCors failed to apply\", err)\n\t\treturn\n\t}\n\n\t// Get the rules and check they match what we set\n\tgotCorsConfig, err := c.GetBucketCors(ctx, bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketCors failed\", err)\n\t\treturn\n\t}\n\tif !reflect.DeepEqual(corsConfig, gotCorsConfig) {\n\t\tmsg := fmt.Sprintf(\"GetBucketCors returned unexpected rules, expected: %+v, got: %+v\", corsConfig, gotCorsConfig)\n\t\tlogError(testName, function, args, startTime, \"\", msg, nil)\n\t\treturn\n\t}\n\n\t// Delete the rules\n\terr = c.SetBucketCors(ctx, bucketName, nil)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketCors failed to delete\", err)\n\t\treturn\n\t}\n\n\t// Get the rules and check they are now empty\n\tgotCorsConfig, err = c.GetBucketCors(ctx, bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketCors failed\", err)\n\t\treturn\n\t}\n\tif gotCorsConfig != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketCors returned unexpected rules\", nil)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test deleting multiple objects with object retention set in Governance mode\nfunc testRemoveObjects() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"RemoveObjects(bucketName, objectsCh, opts)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\":   \"\",\n\t\t\"objectPrefix\": \"\",\n\t\t\"recursive\":    \"true\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\tobjectName := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\targs[\"objectName\"] = objectName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\tbufSize := dataFileMap[\"datafile-129-MB\"]\n\treader := getDataReader(\"datafile-129-MB\")\n\tdefer reader.Close()\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Error uploading object\", err)\n\t\treturn\n\t}\n\n\t// Replace with smaller...\n\tbufSize = dataFileMap[\"datafile-10-kB\"]\n\treader = getDataReader(\"datafile-10-kB\")\n\tdefer reader.Close()\n\n\t_, err = c.PutObject(context.Background(), bucketName, objectName, reader, int64(bufSize), minio.PutObjectOptions{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Error uploading object\", err)\n\t}\n\n\tt := time.Date(2030, time.April, 25, 14, 0, 0, 0, time.UTC)\n\tm := minio.RetentionMode(minio.Governance)\n\topts := minio.PutObjectRetentionOptions{\n\t\tGovernanceBypass: false,\n\t\tRetainUntilDate:  &t,\n\t\tMode:             &m,\n\t}\n\terr = c.PutObjectRetention(context.Background(), bucketName, objectName, opts)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"Error setting retention\", err)\n\t\treturn\n\t}\n\n\tobjectsCh := make(chan minio.ObjectInfo)\n\t// Send object names that are needed to be removed to objectsCh\n\tgo func() {\n\t\tdefer close(objectsCh)\n\t\t// List all objects from a bucket-name with a matching prefix.\n\t\tfor object := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{UseV1: true, Recursive: true}) {\n\t\t\tif object.Err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Error listing objects\", object.Err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tobjectsCh <- object\n\t\t}\n\t}()\n\n\tfor rErr := range c.RemoveObjects(context.Background(), bucketName, objectsCh, minio.RemoveObjectsOptions{}) {\n\t\t// Error is expected here because Retention is set on the object\n\t\t// and RemoveObjects is called without Bypass Governance\n\t\tif rErr.Err == nil {\n\t\t\tlogError(testName, function, args, startTime, \"\", \"Expected error during deletion\", nil)\n\t\t\treturn\n\t\t}\n\t}\n\n\tobjectsCh1 := make(chan minio.ObjectInfo)\n\n\t// Send object names that are needed to be removed to objectsCh\n\tgo func() {\n\t\tdefer close(objectsCh1)\n\t\t// List all objects from a bucket-name with a matching prefix.\n\t\tfor object := range c.ListObjects(context.Background(), bucketName, minio.ListObjectsOptions{UseV1: true, Recursive: true}) {\n\t\t\tif object.Err != nil {\n\t\t\t\tlogError(testName, function, args, startTime, \"\", \"Error listing objects\", object.Err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tobjectsCh1 <- object\n\t\t}\n\t}()\n\n\topts1 := minio.RemoveObjectsOptions{\n\t\tGovernanceBypass: true,\n\t}\n\n\tfor rErr := range c.RemoveObjects(context.Background(), bucketName, objectsCh1, opts1) {\n\t\t// Error is not expected here because Retention is set on the object\n\t\t// and RemoveObjects is called with Bypass Governance\n\t\tlogError(testName, function, args, startTime, \"\", \"Error detected during deletion\", rErr.Err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test get bucket tags\nfunc testGetBucketTagging() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"GetBucketTagging(bucketName)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.GetBucketTagging(context.Background(), bucketName)\n\tif minio.ToErrorResponse(err).Code != \"NoSuchTagSet\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error from server failed\", err)\n\t\treturn\n\t}\n\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test setting tags for bucket\nfunc testSetBucketTagging() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"SetBucketTagging(bucketName, tags)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t\t\"tags\":       \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.GetBucketTagging(context.Background(), bucketName)\n\tif minio.ToErrorResponse(err).Code != \"NoSuchTagSet\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error from server\", err)\n\t\treturn\n\t}\n\n\ttag := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\texpectedValue := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tt, err := tags.MapToBucketTags(map[string]string{\n\t\ttag: expectedValue,\n\t})\n\targs[\"tags\"] = t.String()\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"tags.MapToBucketTags failed\", err)\n\t\treturn\n\t}\n\n\terr = c.SetBucketTagging(context.Background(), bucketName, t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketTagging failed\", err)\n\t\treturn\n\t}\n\n\ttagging, err := c.GetBucketTagging(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketTagging failed\", err)\n\t\treturn\n\t}\n\n\tif tagging.ToMap()[tag] != expectedValue {\n\t\tmsg := fmt.Sprintf(\"Tag %s; got value %s; wanted %s\", tag, tagging.ToMap()[tag], expectedValue)\n\t\tlogError(testName, function, args, startTime, \"\", msg, err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Test removing bucket tags\nfunc testRemoveBucketTagging() {\n\t// initialize logging params\n\tstartTime := time.Now()\n\ttestName := getFuncName()\n\tfunction := \"RemoveBucketTagging(bucketName)\"\n\targs := map[string]interface{}{\n\t\t\"bucketName\": \"\",\n\t}\n\n\tc, err := NewClient(ClientConfig{})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MinIO client v4 object creation failed\", err)\n\t\treturn\n\t}\n\n\t// Generate a new random bucket name.\n\tbucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"minio-go-test-\")\n\targs[\"bucketName\"] = bucketName\n\n\t// Make a new bucket.\n\terr = c.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{Region: \"us-east-1\", ObjectLocking: true})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"MakeBucket failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.GetBucketTagging(context.Background(), bucketName)\n\tif minio.ToErrorResponse(err).Code != \"NoSuchTagSet\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error from server\", err)\n\t\treturn\n\t}\n\n\ttag := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\texpectedValue := randString(60, rand.NewSource(time.Now().UnixNano()), \"\")\n\n\tt, err := tags.MapToBucketTags(map[string]string{\n\t\ttag: expectedValue,\n\t})\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"tags.MapToBucketTags failed\", err)\n\t\treturn\n\t}\n\n\terr = c.SetBucketTagging(context.Background(), bucketName, t)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"SetBucketTagging failed\", err)\n\t\treturn\n\t}\n\n\ttagging, err := c.GetBucketTagging(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"GetBucketTagging failed\", err)\n\t\treturn\n\t}\n\n\tif tagging.ToMap()[tag] != expectedValue {\n\t\tmsg := fmt.Sprintf(\"Tag %s; got value %s; wanted %s\", tag, tagging.ToMap()[tag], expectedValue)\n\t\tlogError(testName, function, args, startTime, \"\", msg, err)\n\t\treturn\n\t}\n\n\terr = c.RemoveBucketTagging(context.Background(), bucketName)\n\tif err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"RemoveBucketTagging failed\", err)\n\t\treturn\n\t}\n\n\t_, err = c.GetBucketTagging(context.Background(), bucketName)\n\tif minio.ToErrorResponse(err).Code != \"NoSuchTagSet\" {\n\t\tlogError(testName, function, args, startTime, \"\", \"Invalid error from server\", err)\n\t\treturn\n\t}\n\n\t// Delete all objects and buckets\n\tif err = cleanupVersionedBucket(bucketName, c); err != nil {\n\t\tlogError(testName, function, args, startTime, \"\", \"CleanupBucket failed\", err)\n\t\treturn\n\t}\n\n\tlogSuccess(testName, function, args, startTime)\n}\n\n// Convert string to bool and always return false if any error\nfunc mustParseBool(str string) bool {\n\tb, err := strconv.ParseBool(str)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn b\n}\n\nfunc main() {\n\tslog.SetDefault(slog.New(slog.NewJSONHandler(\n\t\tos.Stdout,\n\t\t&slog.HandlerOptions{\n\t\t\tLevel: slog.LevelInfo,\n\t\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t\tif a.Key == slog.MessageKey || a.Value.String() == \"\" {\n\t\t\t\t\treturn slog.Attr{}\n\t\t\t\t}\n\n\t\t\t\treturn a\n\t\t\t},\n\t\t},\n\t)))\n\n\ttls := mustParseBool(os.Getenv(enableHTTPS))\n\tkms := mustParseBool(os.Getenv(enableKMS))\n\tif os.Getenv(enableKMS) == \"\" {\n\t\t// Default to KMS tests.\n\t\tkms = true\n\t}\n\n\t// execute tests\n\tif isFullMode() {\n\t\ttestCorsSetGetDelete()\n\t\ttestCors()\n\t\ttestListMultipartUpload()\n\t\ttestGetObjectAttributes()\n\t\ttestGetObjectAttributesErrorCases()\n\t\ttestMakeBucketErrorV2()\n\t\ttestGetObjectClosedTwiceV2()\n\t\ttestFPutObjectV2()\n\t\ttestMakeBucketRegionsV2()\n\t\ttestGetObjectReadSeekFunctionalV2()\n\t\ttestGetObjectReadAtFunctionalV2()\n\t\ttestGetObjectRanges()\n\t\ttestCopyObjectV2()\n\t\ttestFunctionalV2()\n\t\ttestComposeObjectErrorCasesV2()\n\t\ttestCompose10KSourcesV2()\n\t\ttestUserMetadataCopyingV2()\n\t\ttestPutObjectWithChecksums()\n\t\ttestPutObjectWithTrailingChecksums()\n\t\ttestPutMultipartObjectWithChecksums(false)\n\t\ttestPutMultipartObjectWithChecksums(true)\n\t\ttestPutObject0ByteV2()\n\t\ttestPutObjectNoLengthV2()\n\t\ttestPutObjectsUnknownV2()\n\t\ttestGetObjectContextV2()\n\t\ttestFPutObjectContextV2()\n\t\ttestFGetObjectContextV2()\n\t\ttestPutObjectContextV2()\n\t\ttestPutObjectWithVersioning()\n\t\ttestMakeBucketError()\n\t\ttestMakeBucketRegions()\n\t\ttestPutObjectWithMetadata()\n\t\ttestPutObjectReadAt()\n\t\ttestPutObjectStreaming()\n\t\ttestGetObjectSeekEnd()\n\t\ttestGetObjectClosedTwice()\n\t\ttestGetObjectS3Zip()\n\t\ttestRemoveMultipleObjects()\n\t\ttestRemoveMultipleObjectsWithResult()\n\t\ttestFPutObjectMultipart()\n\t\ttestFPutObject()\n\t\ttestGetObjectReadSeekFunctional()\n\t\ttestGetObjectReadAtFunctional()\n\t\ttestGetObjectReadAtWhenEOFWasReached()\n\t\ttestPresignedPostPolicy()\n\t\ttestPresignedPostPolicyWrongFile()\n\t\ttestCopyObject()\n\t\ttestComposeObjectErrorCases()\n\t\ttestCompose10KSources()\n\t\ttestUserMetadataCopying()\n\t\ttestBucketNotification()\n\t\ttestFunctional()\n\t\ttestGetObjectModified()\n\t\ttestPutObjectUploadSeekedObject()\n\t\ttestGetObjectContext()\n\t\ttestFPutObjectContext()\n\t\ttestFGetObjectContext()\n\t\ttestGetObjectACLContext()\n\t\ttestPutObjectContext()\n\t\ttestStorageClassMetadataPutObject()\n\t\ttestStorageClassInvalidMetadataPutObject()\n\t\ttestStorageClassMetadataCopyObject()\n\t\ttestPutObjectWithContentLanguage()\n\t\ttestListObjects()\n\t\ttestRemoveObjects()\n\t\ttestListObjectVersions()\n\t\ttestStatObjectWithVersioning()\n\t\ttestGetObjectWithVersioning()\n\t\ttestCopyObjectWithVersioning()\n\t\ttestConcurrentCopyObjectWithVersioning()\n\t\ttestComposeObjectWithVersioning()\n\t\ttestRemoveObjectWithVersioning()\n\t\ttestRemoveObjectsWithVersioning()\n\t\ttestObjectTaggingWithVersioning()\n\t\ttestTrailingChecksums()\n\t\ttestPutObjectWithAutomaticChecksums()\n\t\ttestGetBucketTagging()\n\t\ttestSetBucketTagging()\n\t\ttestRemoveBucketTagging()\n\n\t\t// SSE-C tests will only work over TLS connection.\n\t\tif tls {\n\t\t\ttestGetObjectAttributesSSECEncryption()\n\t\t\ttestSSECEncryptionPutGet()\n\t\t\ttestSSECEncryptionFPut()\n\t\t\ttestSSECEncryptedGetObjectReadAtFunctional()\n\t\t\ttestSSECEncryptedGetObjectReadSeekFunctional()\n\t\t\ttestEncryptedCopyObjectV2()\n\t\t\ttestEncryptedSSECToSSECCopyObject()\n\t\t\ttestEncryptedSSECToUnencryptedCopyObject()\n\t\t\ttestUnencryptedToSSECCopyObject()\n\t\t\ttestUnencryptedToUnencryptedCopyObject()\n\t\t\ttestEncryptedEmptyObject()\n\t\t\ttestDecryptedCopyObject()\n\t\t\ttestSSECEncryptedToSSECCopyObjectPart()\n\t\t\ttestSSECMultipartEncryptedToSSECCopyObjectPart()\n\t\t\ttestSSECEncryptedToUnencryptedCopyPart()\n\t\t\ttestUnencryptedToSSECCopyObjectPart()\n\t\t\ttestUnencryptedToUnencryptedCopyPart()\n\t\t\ttestEncryptedSSECToSSES3CopyObject()\n\t\t\ttestEncryptedSSES3ToSSECCopyObject()\n\t\t\ttestSSECEncryptedToSSES3CopyObjectPart()\n\t\t\ttestSSES3EncryptedToSSECCopyObjectPart()\n\t\t}\n\n\t\t// KMS tests\n\t\tif kms {\n\t\t\ttestSSES3EncryptionPutGet()\n\t\t\ttestSSES3EncryptionFPut()\n\t\t\ttestSSES3EncryptedGetObjectReadAtFunctional()\n\t\t\ttestSSES3EncryptedGetObjectReadSeekFunctional()\n\t\t\ttestEncryptedSSES3ToSSES3CopyObject()\n\t\t\ttestEncryptedSSES3ToUnencryptedCopyObject()\n\t\t\ttestUnencryptedToSSES3CopyObject()\n\t\t\ttestUnencryptedToSSES3CopyObjectPart()\n\t\t\ttestSSES3EncryptedToUnencryptedCopyPart()\n\t\t\ttestSSES3EncryptedToSSES3CopyObjectPart()\n\t\t}\n\t} else {\n\t\ttestFunctional()\n\t\ttestFunctionalV2()\n\t}\n}\n"
        },
        {
          "name": "get-options_test.go",
          "type": "blob",
          "size": 3.095703125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestSetHeader(t *testing.T) {\n\ttestCases := []struct {\n\t\tstart    int64\n\t\tend      int64\n\t\terrVal   error\n\t\texpected string\n\t}{\n\t\t{0, 10, nil, \"bytes=0-10\"},\n\t\t{1, 10, nil, \"bytes=1-10\"},\n\t\t{5, 0, nil, \"bytes=5-\"},\n\t\t{0, -5, nil, \"bytes=-5\"},\n\t\t{0, 0, nil, \"bytes=0-0\"},\n\t\t{\n\t\t\t11, 10, fmt.Errorf(\"Invalid range specified: start=11 end=10\"),\n\t\t\t\"\",\n\t\t},\n\t\t{-1, 10, fmt.Errorf(\"Invalid range specified: start=-1 end=10\"), \"\"},\n\t\t{-1, 0, fmt.Errorf(\"Invalid range specified: start=-1 end=0\"), \"\"},\n\t\t{1, -5, fmt.Errorf(\"Invalid range specified: start=1 end=-5\"), \"\"},\n\t}\n\tfor i, testCase := range testCases {\n\t\topts := GetObjectOptions{}\n\t\terr := opts.SetRange(testCase.start, testCase.end)\n\t\tif err == nil && testCase.errVal != nil {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with '%v' but it passed\",\n\t\t\t\ti+1, testCase.errVal)\n\t\t} else if err != nil && testCase.errVal.Error() != err.Error() {\n\t\t\tt.Errorf(\"Test %d: Expected error '%v' but got error '%v'\",\n\t\t\t\ti+1, testCase.errVal, err)\n\t\t} else if err == nil && opts.headers[\"Range\"] != testCase.expected {\n\t\t\tt.Errorf(\"Test %d: Expected range header '%s', but got '%s'\",\n\t\t\t\ti+1, testCase.expected, opts.headers[\"Range\"])\n\t\t}\n\t}\n}\n\nfunc TestCustomQueryParameters(t *testing.T) {\n\tvar (\n\t\tparamKey   = \"x-test-param\"\n\t\tparamValue = \"test-value\"\n\n\t\tinvalidParamKey   = \"invalid-test-param\"\n\t\tinvalidParamValue = \"invalid-test-param\"\n\t)\n\n\ttestCases := []struct {\n\t\tsetParamsFunc func(o *GetObjectOptions)\n\t}{\n\t\t{func(o *GetObjectOptions) {\n\t\t\to.AddReqParam(paramKey, paramValue)\n\t\t\to.AddReqParam(invalidParamKey, invalidParamValue)\n\t\t}},\n\t\t{func(o *GetObjectOptions) {\n\t\t\to.SetReqParam(paramKey, paramValue)\n\t\t\to.SetReqParam(invalidParamKey, invalidParamValue)\n\t\t}},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\topts := GetObjectOptions{}\n\t\ttestCase.setParamsFunc(&opts)\n\n\t\t// This and the following checks indirectly ensure that only the expected\n\t\t// valid header is added.\n\t\tif len(opts.reqParams) != 1 {\n\t\t\tt.Errorf(\"Test %d: Expected 1 kv-pair in query parameters, got %v\", i+1, len(opts.reqParams))\n\t\t}\n\n\t\tif v, ok := opts.reqParams[paramKey]; !ok {\n\t\t\tt.Errorf(\"Test %d: Expected query parameter with key %s missing\", i+1, paramKey)\n\t\t} else if len(v) != 1 {\n\t\t\tt.Errorf(\"Test %d: Expected 1 value for query parameter with key %s, got %d values\", i+1, paramKey, len(v))\n\t\t} else if v[0] != paramValue {\n\t\t\tt.Errorf(\"Test %d: Expected query value %s for key %s, got %s\", i+1, paramValue, paramKey, v[0])\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.5224609375,
          "content": "module github.com/minio/minio-go/v7\n\ngo 1.22\n\nrequire (\n\tgithub.com/dustin/go-humanize v1.0.1\n\tgithub.com/go-ini/ini v1.67.0\n\tgithub.com/goccy/go-json v0.10.4\n\tgithub.com/google/uuid v1.6.0\n\tgithub.com/klauspost/compress v1.17.11\n\tgithub.com/minio/md5-simd v1.1.2\n\tgithub.com/rs/xid v1.6.0\n\tgolang.org/x/crypto v0.31.0\n\tgolang.org/x/net v0.33.0\n)\n\nrequire (\n\tgithub.com/klauspost/cpuid/v2 v2.2.9 // indirect\n\tgithub.com/stretchr/testify v1.9.0 // indirect\n\tgolang.org/x/sys v0.28.0 // indirect\n\tgolang.org/x/text v0.21.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 2.671875,
          "content": "github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=\ngithub.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=\ngithub.com/go-ini/ini v1.67.0 h1:z6ZrTEZqSWOTyH2FlglNbNgARyHG8oLW9gMELqKr06A=\ngithub.com/go-ini/ini v1.67.0/go.mod h1:ByCAeIL28uOIIG0E3PJtZPDL8WnHpFKFOtgjp+3Ies8=\ngithub.com/goccy/go-json v0.10.4 h1:JSwxQzIqKfmFX1swYPpUThQZp/Ka4wzJdK0LWVytLPM=\ngithub.com/goccy/go-json v0.10.4/go.mod h1:oq7eo15ShAhp70Anwd5lgX2pLfOS3QCiwU/PULtXL6M=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/klauspost/compress v1.17.11 h1:In6xLpyWOi1+C7tXUUWv2ot1QvBjxevKAaI6IXrJmUc=\ngithub.com/klauspost/compress v1.17.11/go.mod h1:pMDklpSncoRMuLFrf1W9Ss9KT+0rH90U12bZKk7uwG0=\ngithub.com/klauspost/cpuid/v2 v2.0.1/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=\ngithub.com/klauspost/cpuid/v2 v2.2.9 h1:66ze0taIn2H33fBvCkXuv9BmCwDfafmiIVpKV9kKGuY=\ngithub.com/klauspost/cpuid/v2 v2.2.9/go.mod h1:rqkxqrZ1EhYM9G+hXH7YdowN5R5RGN6NK4QwQ3WMXF8=\ngithub.com/minio/md5-simd v1.1.2 h1:Gdi1DZK69+ZVMoNHRXJyNcxrMA4dSxoYHZSQbirFg34=\ngithub.com/minio/md5-simd v1.1.2/go.mod h1:MzdKDxYpY2BT9XQFocsiZf/NKVtR7nkE4RoEpN+20RM=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/rs/xid v1.6.0 h1:fV591PaemRlL6JfRxGDEPl69wICngIQ3shQtzfy2gxU=\ngithub.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=\ngithub.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=\ngithub.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngolang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=\ngolang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\ngolang.org/x/net v0.33.0 h1:74SYHlV8BIgHIFC/LrYkOGIwL19eTYXQ5wc6TBuO36I=\ngolang.org/x/net v0.33.0/go.mod h1:HXLR5J+9DxmrqMwG9qjGCxZ+zKXxBru04zlTvWlWuN4=\ngolang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\ngolang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "healthcheck_test.go",
          "type": "blob",
          "size": 1.693359375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2021 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestHealthCheck(t *testing.T) {\n\tsrv := httptest.NewUnstartedServer(http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t}))\n\tdefer srv.Close()\n\n\t// New - instantiate minio client with options\n\tclnt, err := New(srv.Listener.Addr().String(), &Options{\n\t\tRegion: \"us-east-1\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\thcancel, err := clnt.HealthCheck(1 * time.Second)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tprobeBucketName := randString(60, rand.NewSource(time.Now().UnixNano()), \"probe-health-\")\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer cancel()\n\tclnt.BucketExists(ctx, probeBucketName)\n\n\tif !clnt.IsOffline() {\n\t\tt.Fatal(\"Expected offline but found online\")\n\t}\n\n\tsrv.Start()\n\ttime.Sleep(2 * time.Second)\n\n\tif clnt.IsOffline() {\n\t\tt.Fatal(\"Expected online but found offline\")\n\t}\n\n\thcancel() // healthcheck is canceled.\n\n\tif !clnt.IsOnline() {\n\t\tt.Fatal(\"Expected online but found offline\")\n\t}\n}\n"
        },
        {
          "name": "hook-reader.go",
          "type": "blob",
          "size": 2.5986328125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n)\n\n// hookReader hooks additional reader in the source stream. It is\n// useful for making progress bars. Second reader is appropriately\n// notified about the exact number of bytes read from the primary\n// source on each Read operation.\ntype hookReader struct {\n\tmu     sync.RWMutex\n\tsource io.Reader\n\thook   io.Reader\n}\n\n// Seek implements io.Seeker. Seeks source first, and if necessary\n// seeks hook if Seek method is appropriately found.\nfunc (hr *hookReader) Seek(offset int64, whence int) (n int64, err error) {\n\thr.mu.Lock()\n\tdefer hr.mu.Unlock()\n\n\t// Verify for source has embedded Seeker, use it.\n\tsourceSeeker, ok := hr.source.(io.Seeker)\n\tif ok {\n\t\tn, err = sourceSeeker.Seek(offset, whence)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\tif hr.hook != nil {\n\t\t// Verify if hook has embedded Seeker, use it.\n\t\thookSeeker, ok := hr.hook.(io.Seeker)\n\t\tif ok {\n\t\t\tvar m int64\n\t\t\tm, err = hookSeeker.Seek(offset, whence)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\tif n != m {\n\t\t\t\treturn 0, fmt.Errorf(\"hook seeker seeked %d bytes, expected source %d bytes\", m, n)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn n, nil\n}\n\n// Read implements io.Reader. Always reads from the source, the return\n// value 'n' number of bytes are reported through the hook. Returns\n// error for all non io.EOF conditions.\nfunc (hr *hookReader) Read(b []byte) (n int, err error) {\n\thr.mu.RLock()\n\tdefer hr.mu.RUnlock()\n\n\tn, err = hr.source.Read(b)\n\tif err != nil && err != io.EOF {\n\t\treturn n, err\n\t}\n\tif hr.hook != nil {\n\t\t// Progress the hook with the total read bytes from the source.\n\t\tif _, herr := hr.hook.Read(b[:n]); herr != nil {\n\t\t\tif herr != io.EOF {\n\t\t\t\treturn n, herr\n\t\t\t}\n\t\t}\n\t}\n\treturn n, err\n}\n\n// newHook returns a io.ReadSeeker which implements hookReader that\n// reports the data read from the source to the hook.\nfunc newHook(source, hook io.Reader) io.Reader {\n\tif hook == nil {\n\t\treturn &hookReader{source: source}\n\t}\n\treturn &hookReader{\n\t\tsource: source,\n\t\thook:   hook,\n\t}\n}\n"
        },
        {
          "name": "pkg",
          "type": "tree",
          "content": null
        },
        {
          "name": "post-policy.go",
          "type": "blob",
          "size": 12.390625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2023 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n\t\"github.com/minio/minio-go/v7/pkg/tags\"\n)\n\n// expirationDateFormat date format for expiration key in json policy.\nconst expirationDateFormat = \"2006-01-02T15:04:05.000Z\"\n\n// policyCondition explanation:\n// http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html\n//\n// Example:\n//\n//\tpolicyCondition {\n//\t    matchType: \"$eq\",\n//\t    key: \"$Content-Type\",\n//\t    value: \"image/png\",\n//\t}\ntype policyCondition struct {\n\tmatchType string\n\tcondition string\n\tvalue     string\n}\n\n// PostPolicy - Provides strict static type conversion and validation\n// for Amazon S3's POST policy JSON string.\ntype PostPolicy struct {\n\t// Expiration date and time of the POST policy.\n\texpiration time.Time\n\t// Collection of different policy conditions.\n\tconditions []policyCondition\n\t// ContentLengthRange minimum and maximum allowable size for the\n\t// uploaded content.\n\tcontentLengthRange struct {\n\t\tmin int64\n\t\tmax int64\n\t}\n\n\t// Post form data.\n\tformData map[string]string\n}\n\n// NewPostPolicy - Instantiate new post policy.\nfunc NewPostPolicy() *PostPolicy {\n\tp := &PostPolicy{}\n\tp.conditions = make([]policyCondition, 0)\n\tp.formData = make(map[string]string)\n\treturn p\n}\n\n// SetExpires - Sets expiration time for the new policy.\nfunc (p *PostPolicy) SetExpires(t time.Time) error {\n\tif t.IsZero() {\n\t\treturn errInvalidArgument(\"No expiry time set.\")\n\t}\n\tp.expiration = t\n\treturn nil\n}\n\n// SetKey - Sets an object name for the policy based upload.\nfunc (p *PostPolicy) SetKey(key string) error {\n\tif strings.TrimSpace(key) == \"\" {\n\t\treturn errInvalidArgument(\"Object name is empty.\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$key\",\n\t\tvalue:     key,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"key\"] = key\n\treturn nil\n}\n\n// SetKeyStartsWith - Sets an object name that an policy based upload\n// can start with.\n// Can use an empty value (\"\") to allow any key.\nfunc (p *PostPolicy) SetKeyStartsWith(keyStartsWith string) error {\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"starts-with\",\n\t\tcondition: \"$key\",\n\t\tvalue:     keyStartsWith,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"key\"] = keyStartsWith\n\treturn nil\n}\n\n// SetBucket - Sets bucket at which objects will be uploaded to.\nfunc (p *PostPolicy) SetBucket(bucketName string) error {\n\tif strings.TrimSpace(bucketName) == \"\" {\n\t\treturn errInvalidArgument(\"Bucket name is empty.\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$bucket\",\n\t\tvalue:     bucketName,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"bucket\"] = bucketName\n\treturn nil\n}\n\n// SetCondition - Sets condition for credentials, date and algorithm\nfunc (p *PostPolicy) SetCondition(matchType, condition, value string) error {\n\tif strings.TrimSpace(value) == \"\" {\n\t\treturn errInvalidArgument(\"No value specified for condition\")\n\t}\n\n\tpolicyCond := policyCondition{\n\t\tmatchType: matchType,\n\t\tcondition: \"$\" + condition,\n\t\tvalue:     value,\n\t}\n\tif condition == \"X-Amz-Credential\" || condition == \"X-Amz-Date\" || condition == \"X-Amz-Algorithm\" {\n\t\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tp.formData[condition] = value\n\t\treturn nil\n\t}\n\treturn errInvalidArgument(\"Invalid condition in policy\")\n}\n\n// SetTagging - Sets tagging for the object for this policy based upload.\nfunc (p *PostPolicy) SetTagging(tagging string) error {\n\tif strings.TrimSpace(tagging) == \"\" {\n\t\treturn errInvalidArgument(\"No tagging specified.\")\n\t}\n\t_, err := tags.ParseObjectXML(strings.NewReader(tagging))\n\tif err != nil {\n\t\treturn errors.New(\"The XML you provided was not well-formed or did not validate against our published schema.\") //nolint\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$tagging\",\n\t\tvalue:     tagging,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"tagging\"] = tagging\n\treturn nil\n}\n\n// SetContentType - Sets content-type of the object for this policy\n// based upload.\nfunc (p *PostPolicy) SetContentType(contentType string) error {\n\tif strings.TrimSpace(contentType) == \"\" {\n\t\treturn errInvalidArgument(\"No content type specified.\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$Content-Type\",\n\t\tvalue:     contentType,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"Content-Type\"] = contentType\n\treturn nil\n}\n\n// SetContentTypeStartsWith - Sets what content-type of the object for this policy\n// based upload can start with.\n// Can use an empty value (\"\") to allow any content-type.\nfunc (p *PostPolicy) SetContentTypeStartsWith(contentTypeStartsWith string) error {\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"starts-with\",\n\t\tcondition: \"$Content-Type\",\n\t\tvalue:     contentTypeStartsWith,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"Content-Type\"] = contentTypeStartsWith\n\treturn nil\n}\n\n// SetContentDisposition - Sets content-disposition of the object for this policy\nfunc (p *PostPolicy) SetContentDisposition(contentDisposition string) error {\n\tif strings.TrimSpace(contentDisposition) == \"\" {\n\t\treturn errInvalidArgument(\"No content disposition specified.\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$Content-Disposition\",\n\t\tvalue:     contentDisposition,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"Content-Disposition\"] = contentDisposition\n\treturn nil\n}\n\n// SetContentEncoding - Sets content-encoding of the object for this policy\nfunc (p *PostPolicy) SetContentEncoding(contentEncoding string) error {\n\tif strings.TrimSpace(contentEncoding) == \"\" {\n\t\treturn errInvalidArgument(\"No content encoding specified.\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$Content-Encoding\",\n\t\tvalue:     contentEncoding,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"Content-Encoding\"] = contentEncoding\n\treturn nil\n}\n\n// SetContentLengthRange - Set new min and max content length\n// condition for all incoming uploads.\nfunc (p *PostPolicy) SetContentLengthRange(minLen, maxLen int64) error {\n\tif minLen > maxLen {\n\t\treturn errInvalidArgument(\"Minimum limit is larger than maximum limit.\")\n\t}\n\tif minLen < 0 {\n\t\treturn errInvalidArgument(\"Minimum limit cannot be negative.\")\n\t}\n\tif maxLen <= 0 {\n\t\treturn errInvalidArgument(\"Maximum limit cannot be non-positive.\")\n\t}\n\tp.contentLengthRange.min = minLen\n\tp.contentLengthRange.max = maxLen\n\treturn nil\n}\n\n// SetSuccessActionRedirect - Sets the redirect success url of the object for this policy\n// based upload.\nfunc (p *PostPolicy) SetSuccessActionRedirect(redirect string) error {\n\tif strings.TrimSpace(redirect) == \"\" {\n\t\treturn errInvalidArgument(\"Redirect is empty\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$success_action_redirect\",\n\t\tvalue:     redirect,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"success_action_redirect\"] = redirect\n\treturn nil\n}\n\n// SetSuccessStatusAction - Sets the status success code of the object for this policy\n// based upload.\nfunc (p *PostPolicy) SetSuccessStatusAction(status string) error {\n\tif strings.TrimSpace(status) == \"\" {\n\t\treturn errInvalidArgument(\"Status is empty\")\n\t}\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: \"$success_action_status\",\n\t\tvalue:     status,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[\"success_action_status\"] = status\n\treturn nil\n}\n\n// SetUserMetadata - Set user metadata as a key/value couple.\n// Can be retrieved through a HEAD request or an event.\nfunc (p *PostPolicy) SetUserMetadata(key, value string) error {\n\tif strings.TrimSpace(key) == \"\" {\n\t\treturn errInvalidArgument(\"Key is empty\")\n\t}\n\tif strings.TrimSpace(value) == \"\" {\n\t\treturn errInvalidArgument(\"Value is empty\")\n\t}\n\theaderName := fmt.Sprintf(\"x-amz-meta-%s\", key)\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: fmt.Sprintf(\"$%s\", headerName),\n\t\tvalue:     value,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[headerName] = value\n\treturn nil\n}\n\n// SetUserMetadataStartsWith - Set how an user metadata should starts with.\n// Can be retrieved through a HEAD request or an event.\nfunc (p *PostPolicy) SetUserMetadataStartsWith(key, value string) error {\n\tif strings.TrimSpace(key) == \"\" {\n\t\treturn errInvalidArgument(\"Key is empty\")\n\t}\n\theaderName := fmt.Sprintf(\"x-amz-meta-%s\", key)\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"starts-with\",\n\t\tcondition: fmt.Sprintf(\"$%s\", headerName),\n\t\tvalue:     value,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[headerName] = value\n\treturn nil\n}\n\n// SetChecksum sets the checksum of the request.\nfunc (p *PostPolicy) SetChecksum(c Checksum) error {\n\tif c.IsSet() {\n\t\tp.formData[amzChecksumAlgo] = c.Type.String()\n\t\tp.formData[c.Type.Key()] = c.Encoded()\n\n\t\tpolicyCond := policyCondition{\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: fmt.Sprintf(\"$%s\", amzChecksumAlgo),\n\t\t\tvalue:     c.Type.String(),\n\t\t}\n\t\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpolicyCond = policyCondition{\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: fmt.Sprintf(\"$%s\", c.Type.Key()),\n\t\t\tvalue:     c.Encoded(),\n\t\t}\n\t\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// SetEncryption - sets encryption headers for POST API\nfunc (p *PostPolicy) SetEncryption(sse encrypt.ServerSide) {\n\tif sse == nil {\n\t\treturn\n\t}\n\th := http.Header{}\n\tsse.Marshal(h)\n\tfor k, v := range h {\n\t\tp.formData[k] = v[0]\n\t}\n}\n\n// SetUserData - Set user data as a key/value couple.\n// Can be retrieved through a HEAD request or an event.\nfunc (p *PostPolicy) SetUserData(key, value string) error {\n\tif key == \"\" {\n\t\treturn errInvalidArgument(\"Key is empty\")\n\t}\n\tif value == \"\" {\n\t\treturn errInvalidArgument(\"Value is empty\")\n\t}\n\theaderName := fmt.Sprintf(\"x-amz-%s\", key)\n\tpolicyCond := policyCondition{\n\t\tmatchType: \"eq\",\n\t\tcondition: fmt.Sprintf(\"$%s\", headerName),\n\t\tvalue:     value,\n\t}\n\tif err := p.addNewPolicy(policyCond); err != nil {\n\t\treturn err\n\t}\n\tp.formData[headerName] = value\n\treturn nil\n}\n\n// addNewPolicy - internal helper to validate adding new policies.\n// Can use starts-with with an empty value (\"\") to allow any content within a form field.\nfunc (p *PostPolicy) addNewPolicy(policyCond policyCondition) error {\n\tif policyCond.matchType == \"\" || policyCond.condition == \"\" {\n\t\treturn errInvalidArgument(\"Policy fields are empty.\")\n\t}\n\tif policyCond.matchType != \"starts-with\" && policyCond.value == \"\" {\n\t\treturn errInvalidArgument(\"Policy value is empty.\")\n\t}\n\tp.conditions = append(p.conditions, policyCond)\n\treturn nil\n}\n\n// String function for printing policy in json formatted string.\nfunc (p PostPolicy) String() string {\n\treturn string(p.marshalJSON())\n}\n\n// marshalJSON - Provides Marshaled JSON in bytes.\nfunc (p PostPolicy) marshalJSON() []byte {\n\texpirationStr := `\"expiration\":\"` + p.expiration.Format(expirationDateFormat) + `\"`\n\tvar conditionsStr string\n\tconditions := []string{}\n\tfor _, po := range p.conditions {\n\t\tconditions = append(conditions, fmt.Sprintf(\"[\\\"%s\\\",\\\"%s\\\",\\\"%s\\\"]\", po.matchType, po.condition, po.value))\n\t}\n\tif p.contentLengthRange.min != 0 || p.contentLengthRange.max != 0 {\n\t\tconditions = append(conditions, fmt.Sprintf(\"[\\\"content-length-range\\\", %d, %d]\",\n\t\t\tp.contentLengthRange.min, p.contentLengthRange.max))\n\t}\n\tif len(conditions) > 0 {\n\t\tconditionsStr = `\"conditions\":[` + strings.Join(conditions, \",\") + \"]\"\n\t}\n\tretStr := \"{\"\n\tretStr = retStr + expirationStr + \",\"\n\tretStr += conditionsStr\n\tretStr += \"}\"\n\treturn []byte(retStr)\n}\n\n// base64 - Produces base64 of PostPolicy's Marshaled json.\nfunc (p PostPolicy) base64() string {\n\treturn base64.StdEncoding.EncodeToString(p.marshalJSON())\n}\n"
        },
        {
          "name": "post-policy_test.go",
          "type": "blob",
          "size": 10.580078125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2023 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/encrypt\"\n)\n\nfunc TestPostPolicySetExpires(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tinput      time.Time\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid time\",\n\t\t\tinput:      time.Date(2023, time.March, 2, 15, 4, 5, 0, time.UTC),\n\t\t\twantErr:    false,\n\t\t\twantResult: \"2023-03-02T15:04:05\",\n\t\t},\n\t\t{\n\t\t\tname:    \"time before 1970\",\n\t\t\tinput:   time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC),\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetExpires(tt.input)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetKey(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tinput      string\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid key\",\n\t\t\tinput:      \"my-object\",\n\t\t\twantResult: `\"eq\",\"$key\",\"my-object\"`,\n\t\t},\n\t\t{\n\t\t\tname:    \"empty key\",\n\t\t\tinput:   \"\",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"key with spaces\",\n\t\t\tinput:   \"  \",\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetKey(tt.input)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetKeyStartsWith(t *testing.T) {\n\ttests := []struct {\n\t\tname  string\n\t\tinput string\n\t\twant  string\n\t}{\n\t\t{\n\t\t\tname:  \"valid key prefix\",\n\t\t\tinput: \"my-prefix/\",\n\t\t\twant:  `[\"starts-with\",\"$key\",\"my-prefix/\"]`,\n\t\t},\n\t\t{\n\t\t\tname:  \"empty prefix (allow any key)\",\n\t\t\tinput: \"\",\n\t\t\twant:  `[\"starts-with\",\"$key\",\"\"]`,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetKeyStartsWith(tt.input)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"%s: want no error, got: %v\", tt.name, err)\n\t\t\t}\n\n\t\t\tif tt.want != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.want) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.want, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetBucket(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tinput      string\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid bucket\",\n\t\t\tinput:      \"my-bucket\",\n\t\t\twantResult: `\"eq\",\"$bucket\",\"my-bucket\"`,\n\t\t},\n\t\t{\n\t\t\tname:    \"empty bucket\",\n\t\t\tinput:   \"\",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"bucket with spaces\",\n\t\t\tinput:   \"   \",\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetBucket(tt.input)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetCondition(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tmatchType  string\n\t\tcondition  string\n\t\tvalue      string\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid eq condition\",\n\t\t\tmatchType:  \"eq\",\n\t\t\tcondition:  \"X-Amz-Date\",\n\t\t\tvalue:      \"20210324T000000Z\",\n\t\t\twantResult: `\"eq\",\"$X-Amz-Date\",\"20210324T000000Z\"`,\n\t\t},\n\t\t{\n\t\t\tname:      \"empty value\",\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: \"X-Amz-Date\",\n\t\t\tvalue:     \"\",\n\t\t\twantErr:   true,\n\t\t},\n\t\t{\n\t\t\tname:      \"invalid condition\",\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: \"Invalid-Condition\",\n\t\t\tvalue:     \"somevalue\",\n\t\t\twantErr:   true,\n\t\t},\n\t\t{\n\t\t\tname:       \"valid starts-with condition\",\n\t\t\tmatchType:  \"starts-with\",\n\t\t\tcondition:  \"X-Amz-Credential\",\n\t\t\tvalue:      \"my-access-key\",\n\t\t\twantResult: `\"starts-with\",\"$X-Amz-Credential\",\"my-access-key\"`,\n\t\t},\n\t\t{\n\t\t\tname:      \"empty condition\",\n\t\t\tmatchType: \"eq\",\n\t\t\tcondition: \"\",\n\t\t\tvalue:     \"somevalue\",\n\t\t\twantErr:   true,\n\t\t},\n\t\t{\n\t\t\tname:      \"empty matchType\",\n\t\t\tmatchType: \"\",\n\t\t\tcondition: \"X-Amz-Date\",\n\t\t\tvalue:     \"somevalue\",\n\t\t\twantErr:   true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetCondition(tt.matchType, tt.condition, tt.value)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetTagging(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\ttagging    string\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid tagging\",\n\t\t\ttagging:    `<Tagging><TagSet><Tag><Key>key1</Key><Value>value1</Value></Tag></TagSet></Tagging>`,\n\t\t\twantResult: `\"eq\",\"$tagging\",\"<Tagging><TagSet><Tag><Key>key1</Key><Value>value1</Value></Tag></TagSet></Tagging>\"`,\n\t\t},\n\t\t{\n\t\t\tname:    \"empty tagging\",\n\t\t\ttagging: \"\",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"whitespace tagging\",\n\t\t\ttagging: \"   \",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"invalid XML\",\n\t\t\ttagging: `<Tagging><TagSet><Tag><Key>key1</Key><Value>value1</Value></Tag></TagSet>`,\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"invalid schema\",\n\t\t\ttagging: `<InvalidTagging></InvalidTagging>`,\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetTagging(tt.tagging)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetUserMetadata(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tkey        string\n\t\tvalue      string\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid metadata\",\n\t\t\tkey:        \"user-key\",\n\t\t\tvalue:      \"user-value\",\n\t\t\twantResult: `\"eq\",\"$x-amz-meta-user-key\",\"user-value\"`,\n\t\t},\n\t\t{\n\t\t\tname:    \"empty key\",\n\t\t\tkey:     \"\",\n\t\t\tvalue:   \"somevalue\",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"empty value\",\n\t\t\tkey:     \"user-key\",\n\t\t\tvalue:   \"\",\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"key with spaces\",\n\t\t\tkey:     \"   \",\n\t\t\tvalue:   \"somevalue\",\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetUserMetadata(tt.key, tt.value)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetChecksum(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tchecksum   Checksum\n\t\twantErr    bool\n\t\twantResult string\n\t}{\n\t\t{\n\t\t\tname:       \"valid checksum SHA256\",\n\t\t\tchecksum:   ChecksumSHA256.ChecksumBytes([]byte(\"somerandomdata\")),\n\t\t\twantResult: `[[\"eq\",\"$x-amz-checksum-algorithm\",\"SHA256\"],[\"eq\",\"$x-amz-checksum-sha256\",\"29/7Qm/iMzZ1O3zMbO0luv6mYWyS6JIqPYV9lc8w1PA=\"]]`,\n\t\t},\n\t\t{\n\t\t\tname:       \"valid checksum CRC32\",\n\t\t\tchecksum:   ChecksumCRC32.ChecksumBytes([]byte(\"somerandomdata\")),\n\t\t\twantResult: `[[\"eq\",\"$x-amz-checksum-algorithm\",\"CRC32\"],[\"eq\",\"$x-amz-checksum-crc32\",\"7sOPnw==\"]]`,\n\t\t},\n\t\t{\n\t\t\tname:       \"empty checksum\",\n\t\t\tchecksum:   Checksum{},\n\t\t\twantResult: \"\",\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\terr := pp.SetChecksum(tt.checksum)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"%s: want error: %v, got: %v\", tt.name, tt.wantErr, err)\n\t\t\t}\n\n\t\t\tif tt.wantResult != \"\" {\n\t\t\t\tresult := pp.String()\n\t\t\t\tif !strings.Contains(result, tt.wantResult) {\n\t\t\t\t\tt.Errorf(\"%s: want result to contain: '%s', got: '%s'\", tt.name, tt.wantResult, result)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostPolicySetEncryption(t *testing.T) {\n\ttests := []struct {\n\t\tname    string\n\t\tsseType string\n\t\tkeyID   string\n\t\twant    map[string]string\n\t}{\n\t\t{\n\t\t\tname:    \"SSE-S3 encryption\",\n\t\t\tsseType: \"SSE-S3\",\n\t\t\tkeyID:   \"my-key-id\",\n\t\t\twant: map[string]string{\n\t\t\t\t\"X-Amz-Server-Side-Encryption\":                \"aws:kms\",\n\t\t\t\t\"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"my-key-id\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"SSE-C encryption with Key ID\",\n\t\t\tsseType: \"SSE-C\",\n\t\t\tkeyID:   \"my-key-id\",\n\t\t\twant: map[string]string{\n\t\t\t\t\"X-Amz-Server-Side-Encryption-Customer-Key\":       \"bXktc2VjcmV0LWtleTEyMzQ1Njc4OTBhYmNkZWZnaGk=\",\n\t\t\t\t\"X-Amz-Server-Side-Encryption-Customer-Key-Md5\":   \"T1mefJwyXBH43sRtfEgRZQ==\",\n\t\t\t\t\"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"AES256\",\n\t\t\t},\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tpp := NewPostPolicy()\n\n\t\t\tvar sse encrypt.ServerSide\n\t\t\tvar err error\n\t\t\tif tt.sseType == \"SSE-S3\" {\n\t\t\t\tsse, err = encrypt.NewSSEKMS(tt.keyID, nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"Failed to create SSE-KMS: %v\", err)\n\t\t\t\t}\n\t\t\t} else if tt.sseType == \"SSE-C\" {\n\t\t\t\tsse, err = encrypt.NewSSEC([]byte(\"my-secret-key1234567890abcdefghi\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"Failed to create SSE-C: %v\", err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tt.Fatalf(\"Unknown SSE type: %s\", tt.sseType)\n\t\t\t}\n\n\t\t\tpp.SetEncryption(sse)\n\n\t\t\tfor k, v := range tt.want {\n\t\t\t\tif pp.formData[k] != v {\n\t\t\t\t\tt.Errorf(\"%s: want %s: %s, got: %s\", tt.name, k, v, pp.formData[k])\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "retry-continous.go",
          "type": "blob",
          "size": 1.9423828125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport \"time\"\n\n// newRetryTimerContinous creates a timer with exponentially increasing delays forever.\nfunc (c *Client) newRetryTimerContinous(baseSleep, maxSleep time.Duration, jitter float64, doneCh chan struct{}) <-chan int {\n\tattemptCh := make(chan int)\n\n\t// normalize jitter to the range [0, 1.0]\n\tif jitter < NoJitter {\n\t\tjitter = NoJitter\n\t}\n\tif jitter > MaxJitter {\n\t\tjitter = MaxJitter\n\t}\n\n\t// computes the exponential backoff duration according to\n\t// https://www.awsarchitectureblog.com/2015/03/backoff.html\n\texponentialBackoffWait := func(attempt int) time.Duration {\n\t\t// 1<<uint(attempt) below could overflow, so limit the value of attempt\n\t\tmaxAttempt := 30\n\t\tif attempt > maxAttempt {\n\t\t\tattempt = maxAttempt\n\t\t}\n\t\t// sleep = random_between(0, min(maxSleep, base * 2 ** attempt))\n\t\tsleep := baseSleep * time.Duration(1<<uint(attempt))\n\t\tif sleep > maxSleep {\n\t\t\tsleep = maxSleep\n\t\t}\n\t\tif jitter != NoJitter {\n\t\t\tsleep -= time.Duration(c.random.Float64() * float64(sleep) * jitter)\n\t\t}\n\t\treturn sleep\n\t}\n\n\tgo func() {\n\t\tdefer close(attemptCh)\n\t\tvar nextBackoff int\n\t\tfor {\n\t\t\tselect {\n\t\t\t// Attempts starts.\n\t\t\tcase attemptCh <- nextBackoff:\n\t\t\t\tnextBackoff++\n\t\t\tcase <-doneCh:\n\t\t\t\t// Stop the routine.\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttime.Sleep(exponentialBackoffWait(nextBackoff))\n\t\t}\n\t}()\n\treturn attemptCh\n}\n"
        },
        {
          "name": "retry.go",
          "type": "blob",
          "size": 4.44140625,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"time\"\n)\n\n// MaxRetry is the maximum number of retries before stopping.\nvar MaxRetry = 10\n\n// MaxJitter will randomize over the full exponential backoff time\nconst MaxJitter = 1.0\n\n// NoJitter disables the use of jitter for randomizing the exponential backoff time\nconst NoJitter = 0.0\n\n// DefaultRetryUnit - default unit multiplicative per retry.\n// defaults to 200 * time.Millisecond\nvar DefaultRetryUnit = 200 * time.Millisecond\n\n// DefaultRetryCap - Each retry attempt never waits no longer than\n// this maximum time duration.\nvar DefaultRetryCap = time.Second\n\n// newRetryTimer creates a timer with exponentially increasing\n// delays until the maximum retry attempts are reached.\nfunc (c *Client) newRetryTimer(ctx context.Context, maxRetry int, baseSleep, maxSleep time.Duration, jitter float64) <-chan int {\n\tattemptCh := make(chan int)\n\n\t// computes the exponential backoff duration according to\n\t// https://www.awsarchitectureblog.com/2015/03/backoff.html\n\texponentialBackoffWait := func(attempt int) time.Duration {\n\t\t// normalize jitter to the range [0, 1.0]\n\t\tif jitter < NoJitter {\n\t\t\tjitter = NoJitter\n\t\t}\n\t\tif jitter > MaxJitter {\n\t\t\tjitter = MaxJitter\n\t\t}\n\n\t\t// sleep = random_between(0, min(maxSleep, base * 2 ** attempt))\n\t\tsleep := baseSleep * time.Duration(1<<uint(attempt))\n\t\tif sleep > maxSleep {\n\t\t\tsleep = maxSleep\n\t\t}\n\t\tif jitter != NoJitter {\n\t\t\tsleep -= time.Duration(c.random.Float64() * float64(sleep) * jitter)\n\t\t}\n\t\treturn sleep\n\t}\n\n\tgo func() {\n\t\tdefer close(attemptCh)\n\t\tfor i := 0; i < maxRetry; i++ {\n\t\t\tselect {\n\t\t\tcase attemptCh <- i + 1:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-time.After(exponentialBackoffWait(i)):\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\treturn attemptCh\n}\n\n// List of AWS S3 error codes which are retryable.\nvar retryableS3Codes = map[string]struct{}{\n\t\"RequestError\":          {},\n\t\"RequestTimeout\":        {},\n\t\"Throttling\":            {},\n\t\"ThrottlingException\":   {},\n\t\"RequestLimitExceeded\":  {},\n\t\"RequestThrottled\":      {},\n\t\"InternalError\":         {},\n\t\"ExpiredToken\":          {},\n\t\"ExpiredTokenException\": {},\n\t\"SlowDown\":              {},\n\t// Add more AWS S3 codes here.\n}\n\n// isS3CodeRetryable - is s3 error code retryable.\nfunc isS3CodeRetryable(s3Code string) (ok bool) {\n\t_, ok = retryableS3Codes[s3Code]\n\treturn ok\n}\n\n// List of HTTP status codes which are retryable.\nvar retryableHTTPStatusCodes = map[int]struct{}{\n\thttp.StatusRequestTimeout:      {},\n\t429:                            {}, // http.StatusTooManyRequests is not part of the Go 1.5 library, yet\n\t499:                            {}, // client closed request, retry. A non-standard status code introduced by nginx.\n\thttp.StatusInternalServerError: {},\n\thttp.StatusBadGateway:          {},\n\thttp.StatusServiceUnavailable:  {},\n\thttp.StatusGatewayTimeout:      {},\n\t520:                            {}, // It is used by Cloudflare as a catch-all response for when the origin server sends something unexpected.\n\t// Add more HTTP status codes here.\n}\n\n// isHTTPStatusRetryable - is HTTP error code retryable.\nfunc isHTTPStatusRetryable(httpStatusCode int) (ok bool) {\n\t_, ok = retryableHTTPStatusCodes[httpStatusCode]\n\treturn ok\n}\n\n// For now, all http Do() requests are retriable except some well defined errors\nfunc isRequestErrorRetryable(ctx context.Context, err error) bool {\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\t// Retry if internal timeout in the HTTP call.\n\t\treturn ctx.Err() == nil\n\t}\n\tif ue, ok := err.(*url.Error); ok {\n\t\te := ue.Unwrap()\n\t\tswitch e.(type) {\n\t\t// x509: certificate signed by unknown authority\n\t\tcase x509.UnknownAuthorityError:\n\t\t\treturn false\n\t\t}\n\t\tswitch e.Error() {\n\t\tcase \"http: server gave HTTP response to HTTPS client\":\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n"
        },
        {
          "name": "s3-endpoints.go",
          "type": "blob",
          "size": 4.5439453125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2024 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\ntype awsS3Endpoint struct {\n\tendpoint          string\n\tdualstackEndpoint string\n}\n\n// awsS3EndpointMap Amazon S3 endpoint map.\nvar awsS3EndpointMap = map[string]awsS3Endpoint{\n\t\"us-east-1\": {\n\t\t\"s3.us-east-1.amazonaws.com\",\n\t\t\"s3.dualstack.us-east-1.amazonaws.com\",\n\t},\n\t\"us-east-2\": {\n\t\t\"s3.us-east-2.amazonaws.com\",\n\t\t\"s3.dualstack.us-east-2.amazonaws.com\",\n\t},\n\t\"us-west-2\": {\n\t\t\"s3.us-west-2.amazonaws.com\",\n\t\t\"s3.dualstack.us-west-2.amazonaws.com\",\n\t},\n\t\"us-west-1\": {\n\t\t\"s3.us-west-1.amazonaws.com\",\n\t\t\"s3.dualstack.us-west-1.amazonaws.com\",\n\t},\n\t\"ca-central-1\": {\n\t\t\"s3.ca-central-1.amazonaws.com\",\n\t\t\"s3.dualstack.ca-central-1.amazonaws.com\",\n\t},\n\t\"ca-west-1\": {\n\t\t\"s3.ca-west-1.amazonaws.com\",\n\t\t\"s3.dualstack.ca-west-1.amazonaws.com\",\n\t},\n\t\"eu-west-1\": {\n\t\t\"s3.eu-west-1.amazonaws.com\",\n\t\t\"s3.dualstack.eu-west-1.amazonaws.com\",\n\t},\n\t\"eu-west-2\": {\n\t\t\"s3.eu-west-2.amazonaws.com\",\n\t\t\"s3.dualstack.eu-west-2.amazonaws.com\",\n\t},\n\t\"eu-west-3\": {\n\t\t\"s3.eu-west-3.amazonaws.com\",\n\t\t\"s3.dualstack.eu-west-3.amazonaws.com\",\n\t},\n\t\"eu-central-1\": {\n\t\t\"s3.eu-central-1.amazonaws.com\",\n\t\t\"s3.dualstack.eu-central-1.amazonaws.com\",\n\t},\n\t\"eu-central-2\": {\n\t\t\"s3.eu-central-2.amazonaws.com\",\n\t\t\"s3.dualstack.eu-central-2.amazonaws.com\",\n\t},\n\t\"eu-north-1\": {\n\t\t\"s3.eu-north-1.amazonaws.com\",\n\t\t\"s3.dualstack.eu-north-1.amazonaws.com\",\n\t},\n\t\"eu-south-1\": {\n\t\t\"s3.eu-south-1.amazonaws.com\",\n\t\t\"s3.dualstack.eu-south-1.amazonaws.com\",\n\t},\n\t\"eu-south-2\": {\n\t\t\"s3.eu-south-2.amazonaws.com\",\n\t\t\"s3.dualstack.eu-south-2.amazonaws.com\",\n\t},\n\t\"ap-east-1\": {\n\t\t\"s3.ap-east-1.amazonaws.com\",\n\t\t\"s3.dualstack.ap-east-1.amazonaws.com\",\n\t},\n\t\"ap-south-1\": {\n\t\t\"s3.ap-south-1.amazonaws.com\",\n\t\t\"s3.dualstack.ap-south-1.amazonaws.com\",\n\t},\n\t\"ap-south-2\": {\n\t\t\"s3.ap-south-2.amazonaws.com\",\n\t\t\"s3.dualstack.ap-south-2.amazonaws.com\",\n\t},\n\t\"ap-southeast-1\": {\n\t\t\"s3.ap-southeast-1.amazonaws.com\",\n\t\t\"s3.dualstack.ap-southeast-1.amazonaws.com\",\n\t},\n\t\"ap-southeast-2\": {\n\t\t\"s3.ap-southeast-2.amazonaws.com\",\n\t\t\"s3.dualstack.ap-southeast-2.amazonaws.com\",\n\t},\n\t\"ap-southeast-3\": {\n\t\t\"s3.ap-southeast-3.amazonaws.com\",\n\t\t\"s3.dualstack.ap-southeast-3.amazonaws.com\",\n\t},\n\t\"ap-southeast-4\": {\n\t\t\"s3.ap-southeast-4.amazonaws.com\",\n\t\t\"s3.dualstack.ap-southeast-4.amazonaws.com\",\n\t},\n\t\"ap-northeast-1\": {\n\t\t\"s3.ap-northeast-1.amazonaws.com\",\n\t\t\"s3.dualstack.ap-northeast-1.amazonaws.com\",\n\t},\n\t\"ap-northeast-2\": {\n\t\t\"s3.ap-northeast-2.amazonaws.com\",\n\t\t\"s3.dualstack.ap-northeast-2.amazonaws.com\",\n\t},\n\t\"ap-northeast-3\": {\n\t\t\"s3.ap-northeast-3.amazonaws.com\",\n\t\t\"s3.dualstack.ap-northeast-3.amazonaws.com\",\n\t},\n\t\"af-south-1\": {\n\t\t\"s3.af-south-1.amazonaws.com\",\n\t\t\"s3.dualstack.af-south-1.amazonaws.com\",\n\t},\n\t\"me-central-1\": {\n\t\t\"s3.me-central-1.amazonaws.com\",\n\t\t\"s3.dualstack.me-central-1.amazonaws.com\",\n\t},\n\t\"me-south-1\": {\n\t\t\"s3.me-south-1.amazonaws.com\",\n\t\t\"s3.dualstack.me-south-1.amazonaws.com\",\n\t},\n\t\"sa-east-1\": {\n\t\t\"s3.sa-east-1.amazonaws.com\",\n\t\t\"s3.dualstack.sa-east-1.amazonaws.com\",\n\t},\n\t\"us-gov-west-1\": {\n\t\t\"s3.us-gov-west-1.amazonaws.com\",\n\t\t\"s3.dualstack.us-gov-west-1.amazonaws.com\",\n\t},\n\t\"us-gov-east-1\": {\n\t\t\"s3.us-gov-east-1.amazonaws.com\",\n\t\t\"s3.dualstack.us-gov-east-1.amazonaws.com\",\n\t},\n\t\"cn-north-1\": {\n\t\t\"s3.cn-north-1.amazonaws.com.cn\",\n\t\t\"s3.dualstack.cn-north-1.amazonaws.com.cn\",\n\t},\n\t\"cn-northwest-1\": {\n\t\t\"s3.cn-northwest-1.amazonaws.com.cn\",\n\t\t\"s3.dualstack.cn-northwest-1.amazonaws.com.cn\",\n\t},\n\t\"il-central-1\": {\n\t\t\"s3.il-central-1.amazonaws.com\",\n\t\t\"s3.dualstack.il-central-1.amazonaws.com\",\n\t},\n}\n\n// getS3Endpoint get Amazon S3 endpoint based on the bucket location.\nfunc getS3Endpoint(bucketLocation string, useDualstack bool) (endpoint string) {\n\ts3Endpoint, ok := awsS3EndpointMap[bucketLocation]\n\tif !ok {\n\t\t// Default to 's3.us-east-1.amazonaws.com' endpoint.\n\t\tif useDualstack {\n\t\t\treturn \"s3.dualstack.us-east-1.amazonaws.com\"\n\t\t}\n\t\treturn \"s3.us-east-1.amazonaws.com\"\n\t}\n\tif useDualstack {\n\t\treturn s3Endpoint.dualstackEndpoint\n\t}\n\treturn s3Endpoint.endpoint\n}\n"
        },
        {
          "name": "s3-error.go",
          "type": "blob",
          "size": 4.66796875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\n// Non exhaustive list of AWS S3 standard error responses -\n// http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html\nvar s3ErrorResponseMap = map[string]string{\n\t\"AccessDenied\":                      \"Access Denied.\",\n\t\"BadDigest\":                         \"The Content-Md5 you specified did not match what we received.\",\n\t\"EntityTooSmall\":                    \"Your proposed upload is smaller than the minimum allowed object size.\",\n\t\"EntityTooLarge\":                    \"Your proposed upload exceeds the maximum allowed object size.\",\n\t\"IncompleteBody\":                    \"You did not provide the number of bytes specified by the Content-Length HTTP header.\",\n\t\"InternalError\":                     \"We encountered an internal error, please try again.\",\n\t\"InvalidAccessKeyId\":                \"The access key ID you provided does not exist in our records.\",\n\t\"InvalidBucketName\":                 \"The specified bucket is not valid.\",\n\t\"InvalidDigest\":                     \"The Content-Md5 you specified is not valid.\",\n\t\"InvalidRange\":                      \"The requested range is not satisfiable\",\n\t\"MalformedXML\":                      \"The XML you provided was not well-formed or did not validate against our published schema.\",\n\t\"MissingContentLength\":              \"You must provide the Content-Length HTTP header.\",\n\t\"MissingContentMD5\":                 \"Missing required header for this request: Content-Md5.\",\n\t\"MissingRequestBodyError\":           \"Request body is empty.\",\n\t\"NoSuchBucket\":                      \"The specified bucket does not exist.\",\n\t\"NoSuchBucketPolicy\":                \"The bucket policy does not exist\",\n\t\"NoSuchKey\":                         \"The specified key does not exist.\",\n\t\"NoSuchUpload\":                      \"The specified multipart upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed.\",\n\t\"NotImplemented\":                    \"A header you provided implies functionality that is not implemented\",\n\t\"PreconditionFailed\":                \"At least one of the pre-conditions you specified did not hold\",\n\t\"RequestTimeTooSkewed\":              \"The difference between the request time and the server's time is too large.\",\n\t\"SignatureDoesNotMatch\":             \"The request signature we calculated does not match the signature you provided. Check your key and signing method.\",\n\t\"MethodNotAllowed\":                  \"The specified method is not allowed against this resource.\",\n\t\"InvalidPart\":                       \"One or more of the specified parts could not be found.\",\n\t\"InvalidPartOrder\":                  \"The list of parts was not in ascending order. The parts list must be specified in order by part number.\",\n\t\"InvalidObjectState\":                \"The operation is not valid for the current state of the object.\",\n\t\"AuthorizationHeaderMalformed\":      \"The authorization header is malformed; the region is wrong.\",\n\t\"MalformedPOSTRequest\":              \"The body of your POST request is not well-formed multipart/form-data.\",\n\t\"BucketNotEmpty\":                    \"The bucket you tried to delete is not empty\",\n\t\"AllAccessDisabled\":                 \"All access to this bucket has been disabled.\",\n\t\"MalformedPolicy\":                   \"Policy has invalid resource.\",\n\t\"MissingFields\":                     \"Missing fields in request.\",\n\t\"AuthorizationQueryParametersError\": \"Error parsing the X-Amz-Credential parameter; the Credential is mal-formed; expecting \\\"<YOUR-AKID>/YYYYMMDD/REGION/SERVICE/aws4_request\\\".\",\n\t\"MalformedDate\":                     \"Invalid date format header, expected to be in ISO8601, RFC1123 or RFC1123Z time format.\",\n\t\"BucketAlreadyOwnedByYou\":           \"Your previous request to create the named bucket succeeded and you already own it.\",\n\t\"InvalidDuration\":                   \"Duration provided in the request is invalid.\",\n\t\"XAmzContentSHA256Mismatch\":         \"The provided 'x-amz-content-sha256' header does not match what was computed.\",\n\t\"NoSuchCORSConfiguration\":           \"The specified bucket does not have a CORS configuration.\",\n\t// Add new API errors here.\n}\n"
        },
        {
          "name": "test-utils_test.go",
          "type": "blob",
          "size": 2.0908203125,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"bytes\"\n\t\"encoding/xml\"\n\t\"io\"\n\t\"net/http\"\n\t\"strconv\"\n)\n\n// Contains common used utilities for tests.\n\n// APIError Used for mocking error response from server.\ntype APIError struct {\n\tCode           string\n\tDescription    string\n\tHTTPStatusCode int\n}\n\n// Mocks XML error response from the server.\nfunc generateErrorResponse(resp *http.Response, APIErr APIError, bucketName string) *http.Response {\n\t// generate error response.\n\terrorResponse := getAPIErrorResponse(APIErr, bucketName)\n\tencodedErrorResponse := encodeResponse(errorResponse)\n\t// write Header.\n\tresp.StatusCode = APIErr.HTTPStatusCode\n\tresp.Body = io.NopCloser(bytes.NewBuffer(encodedErrorResponse))\n\n\treturn resp\n}\n\n// getErrorResponse gets in standard error and resource value and\n// provides a encodable populated response values.\nfunc getAPIErrorResponse(err APIError, bucketName string) ErrorResponse {\n\terrResp := ErrorResponse{}\n\terrResp.Code = err.Code\n\terrResp.Message = err.Description\n\terrResp.BucketName = bucketName\n\treturn errResp\n}\n\n// Encodes the response headers into XML format.\nfunc encodeResponse(response interface{}) []byte {\n\tvar bytesBuffer bytes.Buffer\n\tbytesBuffer.WriteString(xml.Header)\n\tencode := xml.NewEncoder(&bytesBuffer)\n\tencode.Encode(response)\n\treturn bytesBuffer.Bytes()\n}\n\n// Convert string to bool and always return false if any error\nfunc mustParseBool(str string) bool {\n\tb, err := strconv.ParseBool(str)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "testcerts",
          "type": "tree",
          "content": null
        },
        {
          "name": "transport.go",
          "type": "blob",
          "size": 2.4169921875,
          "content": "//go:build go1.7 || go1.8\n// +build go1.7 go1.8\n\n/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2017-2018 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"time\"\n)\n\n// mustGetSystemCertPool - return system CAs or empty pool in case of error (or windows)\nfunc mustGetSystemCertPool() *x509.CertPool {\n\tpool, err := x509.SystemCertPool()\n\tif err != nil {\n\t\treturn x509.NewCertPool()\n\t}\n\treturn pool\n}\n\n// DefaultTransport - this default transport is similar to\n// http.DefaultTransport but with additional param  DisableCompression\n// is set to true to avoid decompressing content with 'gzip' encoding.\nvar DefaultTransport = func(secure bool) (*http.Transport, error) {\n\ttr := &http.Transport{\n\t\tProxy: http.ProxyFromEnvironment,\n\t\tDialContext: (&net.Dialer{\n\t\t\tTimeout:   30 * time.Second,\n\t\t\tKeepAlive: 30 * time.Second,\n\t\t}).DialContext,\n\t\tMaxIdleConns:          256,\n\t\tMaxIdleConnsPerHost:   16,\n\t\tResponseHeaderTimeout: time.Minute,\n\t\tIdleConnTimeout:       time.Minute,\n\t\tTLSHandshakeTimeout:   10 * time.Second,\n\t\tExpectContinueTimeout: 10 * time.Second,\n\t\t// Set this value so that the underlying transport round-tripper\n\t\t// doesn't try to auto decode the body of objects with\n\t\t// content-encoding set to `gzip`.\n\t\t//\n\t\t// Refer:\n\t\t//    https://golang.org/src/net/http/transport.go?h=roundTrip#L1843\n\t\tDisableCompression: true,\n\t}\n\n\tif secure {\n\t\ttr.TLSClientConfig = &tls.Config{\n\t\t\t// Can't use SSLv3 because of POODLE and BEAST\n\t\t\t// Can't use TLSv1.0 because of POODLE and BEAST using CBC cipher\n\t\t\t// Can't use TLSv1.1 because of RC4 cipher usage\n\t\t\tMinVersion: tls.VersionTLS12,\n\t\t}\n\t\tif f := os.Getenv(\"SSL_CERT_FILE\"); f != \"\" {\n\t\t\trootCAs := mustGetSystemCertPool()\n\t\t\tdata, err := os.ReadFile(f)\n\t\t\tif err == nil {\n\t\t\t\trootCAs.AppendCertsFromPEM(data)\n\t\t\t}\n\t\t\ttr.TLSClientConfig.RootCAs = rootCAs\n\t\t}\n\t}\n\treturn tr, nil\n}\n"
        },
        {
          "name": "utils.go",
          "type": "blob",
          "size": 24.3623046875,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"context\"\n\t\"crypto/md5\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"encoding/hex\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\tmd5simd \"github.com/minio/md5-simd\"\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\nfunc trimEtag(etag string) string {\n\tetag = strings.TrimPrefix(etag, \"\\\"\")\n\treturn strings.TrimSuffix(etag, \"\\\"\")\n}\n\nvar expirationRegex = regexp.MustCompile(`expiry-date=\"(.*?)\", rule-id=\"(.*?)\"`)\n\nfunc amzExpirationToExpiryDateRuleID(expiration string) (time.Time, string) {\n\tif matches := expirationRegex.FindStringSubmatch(expiration); len(matches) == 3 {\n\t\texpTime, err := parseRFC7231Time(matches[1])\n\t\tif err != nil {\n\t\t\treturn time.Time{}, \"\"\n\t\t}\n\t\treturn expTime, matches[2]\n\t}\n\treturn time.Time{}, \"\"\n}\n\nvar restoreRegex = regexp.MustCompile(`ongoing-request=\"(.*?)\"(, expiry-date=\"(.*?)\")?`)\n\nfunc amzRestoreToStruct(restore string) (ongoing bool, expTime time.Time, err error) {\n\tmatches := restoreRegex.FindStringSubmatch(restore)\n\tif len(matches) != 4 {\n\t\treturn false, time.Time{}, errors.New(\"unexpected restore header\")\n\t}\n\tongoing, err = strconv.ParseBool(matches[1])\n\tif err != nil {\n\t\treturn false, time.Time{}, err\n\t}\n\tif matches[3] != \"\" {\n\t\texpTime, err = parseRFC7231Time(matches[3])\n\t\tif err != nil {\n\t\t\treturn false, time.Time{}, err\n\t\t}\n\t}\n\treturn\n}\n\n// xmlDecoder provide decoded value in xml.\nfunc xmlDecoder(body io.Reader, v interface{}) error {\n\td := xml.NewDecoder(body)\n\treturn d.Decode(v)\n}\n\n// sum256 calculate sha256sum for an input byte array, returns hex encoded.\nfunc sum256Hex(data []byte) string {\n\thash := newSHA256Hasher()\n\tdefer hash.Close()\n\thash.Write(data)\n\treturn hex.EncodeToString(hash.Sum(nil))\n}\n\n// sumMD5Base64 calculate md5sum for an input byte array, returns base64 encoded.\nfunc sumMD5Base64(data []byte) string {\n\thash := newMd5Hasher()\n\tdefer hash.Close()\n\thash.Write(data)\n\treturn base64.StdEncoding.EncodeToString(hash.Sum(nil))\n}\n\n// getEndpointURL - construct a new endpoint.\nfunc getEndpointURL(endpoint string, secure bool) (*url.URL, error) {\n\t// If secure is false, use 'http' scheme.\n\tscheme := \"https\"\n\tif !secure {\n\t\tscheme = \"http\"\n\t}\n\n\t// Construct a secured endpoint URL.\n\tendpointURLStr := scheme + \"://\" + endpoint\n\tendpointURL, err := url.Parse(endpointURLStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Validate incoming endpoint URL.\n\tif err := isValidEndpointURL(*endpointURL); err != nil {\n\t\treturn nil, err\n\t}\n\treturn endpointURL, nil\n}\n\n// closeResponse close non nil response with any response Body.\n// convenient wrapper to drain any remaining data on response body.\n//\n// Subsequently this allows golang http RoundTripper\n// to re-use the same connection for future requests.\nfunc closeResponse(resp *http.Response) {\n\t// Callers should close resp.Body when done reading from it.\n\t// If resp.Body is not closed, the Client's underlying RoundTripper\n\t// (typically Transport) may not be able to re-use a persistent TCP\n\t// connection to the server for a subsequent \"keep-alive\" request.\n\tif resp != nil && resp.Body != nil {\n\t\t// Drain any remaining Body and then close the connection.\n\t\t// Without this closing connection would disallow re-using\n\t\t// the same connection for future uses.\n\t\t//  - http://stackoverflow.com/a/17961593/4465767\n\t\tio.Copy(io.Discard, resp.Body)\n\t\tresp.Body.Close()\n\t}\n}\n\nvar (\n\t// Hex encoded string of nil sha256sum bytes.\n\temptySHA256Hex = \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n\n\t// Sentinel URL is the default url value which is invalid.\n\tsentinelURL = url.URL{}\n)\n\n// Verify if input endpoint URL is valid.\nfunc isValidEndpointURL(endpointURL url.URL) error {\n\tif endpointURL == sentinelURL {\n\t\treturn errInvalidArgument(\"Endpoint url cannot be empty.\")\n\t}\n\tif endpointURL.Path != \"/\" && endpointURL.Path != \"\" {\n\t\treturn errInvalidArgument(\"Endpoint url cannot have fully qualified paths.\")\n\t}\n\thost := endpointURL.Hostname()\n\tif !s3utils.IsValidIP(host) && !s3utils.IsValidDomain(host) {\n\t\tmsg := \"Endpoint: \" + endpointURL.Host + \" does not follow ip address or domain name standards.\"\n\t\treturn errInvalidArgument(msg)\n\t}\n\n\tif strings.Contains(host, \".s3.amazonaws.com\") {\n\t\tif !s3utils.IsAmazonEndpoint(endpointURL) {\n\t\t\treturn errInvalidArgument(\"Amazon S3 endpoint should be 's3.amazonaws.com'.\")\n\t\t}\n\t}\n\tif strings.Contains(host, \".googleapis.com\") {\n\t\tif !s3utils.IsGoogleEndpoint(endpointURL) {\n\t\t\treturn errInvalidArgument(\"Google Cloud Storage endpoint should be 'storage.googleapis.com'.\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Verify if input expires value is valid.\nfunc isValidExpiry(expires time.Duration) error {\n\texpireSeconds := int64(expires / time.Second)\n\tif expireSeconds < 1 {\n\t\treturn errInvalidArgument(\"Expires cannot be lesser than 1 second.\")\n\t}\n\tif expireSeconds > 604800 {\n\t\treturn errInvalidArgument(\"Expires cannot be greater than 7 days.\")\n\t}\n\treturn nil\n}\n\n// Extract only necessary metadata header key/values by\n// filtering them out with a list of custom header keys.\nfunc extractObjMetadata(header http.Header) http.Header {\n\tpreserveKeys := []string{\n\t\t\"Content-Type\",\n\t\t\"Cache-Control\",\n\t\t\"Content-Encoding\",\n\t\t\"Content-Language\",\n\t\t\"Content-Disposition\",\n\t\t\"X-Amz-Storage-Class\",\n\t\t\"X-Amz-Object-Lock-Mode\",\n\t\t\"X-Amz-Object-Lock-Retain-Until-Date\",\n\t\t\"X-Amz-Object-Lock-Legal-Hold\",\n\t\t\"X-Amz-Website-Redirect-Location\",\n\t\t\"X-Amz-Server-Side-Encryption\",\n\t\t\"X-Amz-Tagging-Count\",\n\t\t\"X-Amz-Meta-\",\n\t\t// Add new headers to be preserved.\n\t\t// if you add new headers here, please extend\n\t\t// PutObjectOptions{} to preserve them\n\t\t// upon upload as well.\n\t}\n\tfilteredHeader := make(http.Header)\n\tfor k, v := range header {\n\t\tvar found bool\n\t\tfor _, prefix := range preserveKeys {\n\t\t\tif !strings.HasPrefix(k, prefix) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t\tif found {\n\t\t\tfilteredHeader[k] = v\n\t\t}\n\t}\n\treturn filteredHeader\n}\n\nconst (\n\t// RFC 7231#section-7.1.1.1 timetamp format. e.g Tue, 29 Apr 2014 18:30:38 GMT\n\trfc822TimeFormat                           = \"Mon, 2 Jan 2006 15:04:05 GMT\"\n\trfc822TimeFormatSingleDigitDay             = \"Mon, _2 Jan 2006 15:04:05 GMT\"\n\trfc822TimeFormatSingleDigitDayTwoDigitYear = \"Mon, _2 Jan 06 15:04:05 GMT\"\n)\n\nfunc parseTime(t string, formats ...string) (time.Time, error) {\n\tfor _, format := range formats {\n\t\ttt, err := time.Parse(format, t)\n\t\tif err == nil {\n\t\t\treturn tt, nil\n\t\t}\n\t}\n\treturn time.Time{}, fmt.Errorf(\"unable to parse %s in any of the input formats: %s\", t, formats)\n}\n\nfunc parseRFC7231Time(lastModified string) (time.Time, error) {\n\treturn parseTime(lastModified, rfc822TimeFormat, rfc822TimeFormatSingleDigitDay, rfc822TimeFormatSingleDigitDayTwoDigitYear)\n}\n\n// ToObjectInfo converts http header values into ObjectInfo type,\n// extracts metadata and fills in all the necessary fields in ObjectInfo.\nfunc ToObjectInfo(bucketName, objectName string, h http.Header) (ObjectInfo, error) {\n\tvar err error\n\t// Trim off the odd double quotes from ETag in the beginning and end.\n\tetag := trimEtag(h.Get(\"ETag\"))\n\n\t// Parse content length is exists\n\tvar size int64 = -1\n\tcontentLengthStr := h.Get(\"Content-Length\")\n\tif contentLengthStr != \"\" {\n\t\tsize, err = strconv.ParseInt(contentLengthStr, 10, 64)\n\t\tif err != nil {\n\t\t\t// Content-Length is not valid\n\t\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\t\tCode:       \"InternalError\",\n\t\t\t\tMessage:    fmt.Sprintf(\"Content-Length is not an integer, failed with %v\", err),\n\t\t\t\tBucketName: bucketName,\n\t\t\t\tKey:        objectName,\n\t\t\t\tRequestID:  h.Get(\"x-amz-request-id\"),\n\t\t\t\tHostID:     h.Get(\"x-amz-id-2\"),\n\t\t\t\tRegion:     h.Get(\"x-amz-bucket-region\"),\n\t\t\t}\n\t\t}\n\t}\n\n\t// Parse Last-Modified has http time format.\n\tmtime, err := parseRFC7231Time(h.Get(\"Last-Modified\"))\n\tif err != nil {\n\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\tCode:       \"InternalError\",\n\t\t\tMessage:    fmt.Sprintf(\"Last-Modified time format is invalid, failed with %v\", err),\n\t\t\tBucketName: bucketName,\n\t\t\tKey:        objectName,\n\t\t\tRequestID:  h.Get(\"x-amz-request-id\"),\n\t\t\tHostID:     h.Get(\"x-amz-id-2\"),\n\t\t\tRegion:     h.Get(\"x-amz-bucket-region\"),\n\t\t}\n\t}\n\n\t// Fetch content type if any present.\n\tcontentType := strings.TrimSpace(h.Get(\"Content-Type\"))\n\tif contentType == \"\" {\n\t\tcontentType = \"application/octet-stream\"\n\t}\n\n\texpiryStr := h.Get(\"Expires\")\n\tvar expiry time.Time\n\tif expiryStr != \"\" {\n\t\texpiry, err = parseRFC7231Time(expiryStr)\n\t\tif err != nil {\n\t\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\t\tCode:       \"InternalError\",\n\t\t\t\tMessage:    fmt.Sprintf(\"'Expiry' is not in supported format: %v\", err),\n\t\t\t\tBucketName: bucketName,\n\t\t\t\tKey:        objectName,\n\t\t\t\tRequestID:  h.Get(\"x-amz-request-id\"),\n\t\t\t\tHostID:     h.Get(\"x-amz-id-2\"),\n\t\t\t\tRegion:     h.Get(\"x-amz-bucket-region\"),\n\t\t\t}\n\t\t}\n\t}\n\n\tmetadata := extractObjMetadata(h)\n\tuserMetadata := make(map[string]string)\n\tfor k, v := range metadata {\n\t\tif strings.HasPrefix(k, \"X-Amz-Meta-\") {\n\t\t\tuserMetadata[strings.TrimPrefix(k, \"X-Amz-Meta-\")] = v[0]\n\t\t}\n\t}\n\tuserTags := s3utils.TagDecode(h.Get(amzTaggingHeader))\n\n\tvar tagCount int\n\tif count := h.Get(amzTaggingCount); count != \"\" {\n\t\ttagCount, err = strconv.Atoi(count)\n\t\tif err != nil {\n\t\t\treturn ObjectInfo{}, ErrorResponse{\n\t\t\t\tCode:       \"InternalError\",\n\t\t\t\tMessage:    fmt.Sprintf(\"x-amz-tagging-count is not an integer, failed with %v\", err),\n\t\t\t\tBucketName: bucketName,\n\t\t\t\tKey:        objectName,\n\t\t\t\tRequestID:  h.Get(\"x-amz-request-id\"),\n\t\t\t\tHostID:     h.Get(\"x-amz-id-2\"),\n\t\t\t\tRegion:     h.Get(\"x-amz-bucket-region\"),\n\t\t\t}\n\t\t}\n\t}\n\n\t// Nil if not found\n\tvar restore *RestoreInfo\n\tif restoreHdr := h.Get(amzRestore); restoreHdr != \"\" {\n\t\tongoing, expTime, err := amzRestoreToStruct(restoreHdr)\n\t\tif err != nil {\n\t\t\treturn ObjectInfo{}, err\n\t\t}\n\t\trestore = &RestoreInfo{OngoingRestore: ongoing, ExpiryTime: expTime}\n\t}\n\n\t// extract lifecycle expiry date and rule ID\n\texpTime, ruleID := amzExpirationToExpiryDateRuleID(h.Get(amzExpiration))\n\n\tdeleteMarker := h.Get(amzDeleteMarker) == \"true\"\n\n\t// Save object metadata info.\n\treturn ObjectInfo{\n\t\tETag:              etag,\n\t\tKey:               objectName,\n\t\tSize:              size,\n\t\tLastModified:      mtime,\n\t\tContentType:       contentType,\n\t\tExpires:           expiry,\n\t\tVersionID:         h.Get(amzVersionID),\n\t\tIsDeleteMarker:    deleteMarker,\n\t\tReplicationStatus: h.Get(amzReplicationStatus),\n\t\tExpiration:        expTime,\n\t\tExpirationRuleID:  ruleID,\n\t\t// Extract only the relevant header keys describing the object.\n\t\t// following function filters out a list of standard set of keys\n\t\t// which are not part of object metadata.\n\t\tMetadata:     metadata,\n\t\tUserMetadata: userMetadata,\n\t\tUserTags:     userTags,\n\t\tUserTagCount: tagCount,\n\t\tRestore:      restore,\n\n\t\t// Checksum values\n\t\tChecksumCRC32:     h.Get(ChecksumCRC32.Key()),\n\t\tChecksumCRC32C:    h.Get(ChecksumCRC32C.Key()),\n\t\tChecksumSHA1:      h.Get(ChecksumSHA1.Key()),\n\t\tChecksumSHA256:    h.Get(ChecksumSHA256.Key()),\n\t\tChecksumCRC64NVME: h.Get(ChecksumCRC64NVME.Key()),\n\t}, nil\n}\n\nvar readFull = func(r io.Reader, buf []byte) (n int, err error) {\n\t// ReadFull reads exactly len(buf) bytes from r into buf.\n\t// It returns the number of bytes copied and an error if\n\t// fewer bytes were read. The error is EOF only if no bytes\n\t// were read. If an EOF happens after reading some but not\n\t// all the bytes, ReadFull returns ErrUnexpectedEOF.\n\t// On return, n == len(buf) if and only if err == nil.\n\t// If r returns an error having read at least len(buf) bytes,\n\t// the error is dropped.\n\tfor n < len(buf) && err == nil {\n\t\tvar nn int\n\t\tnn, err = r.Read(buf[n:])\n\t\t// Some spurious io.Reader's return\n\t\t// io.ErrUnexpectedEOF when nn == 0\n\t\t// this behavior is undocumented\n\t\t// so we are on purpose not using io.ReadFull\n\t\t// implementation because this can lead\n\t\t// to custom handling, to avoid that\n\t\t// we simply modify the original io.ReadFull\n\t\t// implementation to avoid this issue.\n\t\t// io.ErrUnexpectedEOF with nn == 0 really\n\t\t// means that io.EOF\n\t\tif err == io.ErrUnexpectedEOF && nn == 0 {\n\t\t\terr = io.EOF\n\t\t}\n\t\tn += nn\n\t}\n\tif n >= len(buf) {\n\t\terr = nil\n\t} else if n > 0 && err == io.EOF {\n\t\terr = io.ErrUnexpectedEOF\n\t}\n\treturn\n}\n\n// regCred matches credential string in HTTP header\nvar regCred = regexp.MustCompile(\"Credential=([A-Z0-9]+)/\")\n\n// regCred matches signature string in HTTP header\nvar regSign = regexp.MustCompile(\"Signature=([[0-9a-f]+)\")\n\n// Redact out signature value from authorization string.\nfunc redactSignature(origAuth string) string {\n\tif !strings.HasPrefix(origAuth, signV4Algorithm) {\n\t\t// Set a temporary redacted auth\n\t\treturn \"AWS **REDACTED**:**REDACTED**\"\n\t}\n\n\t// Signature V4 authorization header.\n\n\t// Strip out accessKeyID from:\n\t// Credential=<access-key-id>/<date>/<aws-region>/<aws-service>/aws4_request\n\tnewAuth := regCred.ReplaceAllString(origAuth, \"Credential=**REDACTED**/\")\n\n\t// Strip out 256-bit signature from: Signature=<256-bit signature>\n\treturn regSign.ReplaceAllString(newAuth, \"Signature=**REDACTED**\")\n}\n\n// Get default location returns the location based on the input\n// URL `u`, if region override is provided then all location\n// defaults to regionOverride.\n//\n// If no other cases match then the location is set to `us-east-1`\n// as a last resort.\nfunc getDefaultLocation(u url.URL, regionOverride string) (location string) {\n\tif regionOverride != \"\" {\n\t\treturn regionOverride\n\t}\n\tregion := s3utils.GetRegionFromURL(u)\n\tif region == \"\" {\n\t\tregion = \"us-east-1\"\n\t}\n\treturn region\n}\n\nvar supportedHeaders = map[string]bool{\n\t\"content-type\":                        true,\n\t\"cache-control\":                       true,\n\t\"content-encoding\":                    true,\n\t\"content-disposition\":                 true,\n\t\"content-language\":                    true,\n\t\"x-amz-website-redirect-location\":     true,\n\t\"x-amz-object-lock-mode\":              true,\n\t\"x-amz-metadata-directive\":            true,\n\t\"x-amz-object-lock-retain-until-date\": true,\n\t\"expires\":                             true,\n\t\"x-amz-replication-status\":            true,\n\t// Add more supported headers here.\n\t// Must be lower case.\n}\n\n// isStorageClassHeader returns true if the header is a supported storage class header\nfunc isStorageClassHeader(headerKey string) bool {\n\treturn strings.EqualFold(amzStorageClass, headerKey)\n}\n\n// isStandardHeader returns true if header is a supported header and not a custom header\nfunc isStandardHeader(headerKey string) bool {\n\treturn supportedHeaders[strings.ToLower(headerKey)]\n}\n\n// sseHeaders is list of server side encryption headers\nvar sseHeaders = map[string]bool{\n\t\"x-amz-server-side-encryption\":                    true,\n\t\"x-amz-server-side-encryption-aws-kms-key-id\":     true,\n\t\"x-amz-server-side-encryption-context\":            true,\n\t\"x-amz-server-side-encryption-customer-algorithm\": true,\n\t\"x-amz-server-side-encryption-customer-key\":       true,\n\t\"x-amz-server-side-encryption-customer-key-md5\":   true,\n\t// Add more supported headers here.\n\t// Must be lower case.\n}\n\n// isSSEHeader returns true if header is a server side encryption header.\nfunc isSSEHeader(headerKey string) bool {\n\treturn sseHeaders[strings.ToLower(headerKey)]\n}\n\n// isAmzHeader returns true if header is a x-amz-meta-* or x-amz-acl header.\nfunc isAmzHeader(headerKey string) bool {\n\tkey := strings.ToLower(headerKey)\n\n\treturn strings.HasPrefix(key, \"x-amz-meta-\") || strings.HasPrefix(key, \"x-amz-grant-\") || key == \"x-amz-acl\" || isSSEHeader(headerKey) || strings.HasPrefix(key, \"x-amz-checksum-\")\n}\n\n// isMinioHeader returns true if header is x-minio- header.\nfunc isMinioHeader(headerKey string) bool {\n\treturn strings.HasPrefix(strings.ToLower(headerKey), \"x-minio-\")\n}\n\n// supportedQueryValues is a list of query strings that can be passed in when using GetObject.\nvar supportedQueryValues = map[string]bool{\n\t\"attributes\":                   true,\n\t\"partNumber\":                   true,\n\t\"versionId\":                    true,\n\t\"response-cache-control\":       true,\n\t\"response-content-disposition\": true,\n\t\"response-content-encoding\":    true,\n\t\"response-content-language\":    true,\n\t\"response-content-type\":        true,\n\t\"response-expires\":             true,\n}\n\n// isStandardQueryValue will return true when the passed in query string parameter is supported rather than customized.\nfunc isStandardQueryValue(qsKey string) bool {\n\treturn supportedQueryValues[qsKey]\n}\n\n// Per documentation at https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#LogFormatCustom, the\n// set of query params starting with \"x-\" are ignored by S3.\nconst allowedCustomQueryPrefix = \"x-\"\n\nfunc isCustomQueryValue(qsKey string) bool {\n\treturn strings.HasPrefix(qsKey, allowedCustomQueryPrefix)\n}\n\nvar (\n\tmd5Pool    = sync.Pool{New: func() interface{} { return md5.New() }}\n\tsha256Pool = sync.Pool{New: func() interface{} { return sha256.New() }}\n)\n\nfunc newMd5Hasher() md5simd.Hasher {\n\treturn &hashWrapper{Hash: md5Pool.Get().(hash.Hash), isMD5: true}\n}\n\nfunc newSHA256Hasher() md5simd.Hasher {\n\treturn &hashWrapper{Hash: sha256Pool.Get().(hash.Hash), isSHA256: true}\n}\n\n// hashWrapper implements the md5simd.Hasher interface.\ntype hashWrapper struct {\n\thash.Hash\n\tisMD5    bool\n\tisSHA256 bool\n}\n\n// Close will put the hasher back into the pool.\nfunc (m *hashWrapper) Close() {\n\tif m.isMD5 && m.Hash != nil {\n\t\tm.Reset()\n\t\tmd5Pool.Put(m.Hash)\n\t}\n\tif m.isSHA256 && m.Hash != nil {\n\t\tm.Reset()\n\t\tsha256Pool.Put(m.Hash)\n\t}\n\tm.Hash = nil\n}\n\nconst letterBytes = \"abcdefghijklmnopqrstuvwxyz01234569\"\nconst (\n\tletterIdxBits = 6                    // 6 bits to represent a letter index\n\tletterIdxMask = 1<<letterIdxBits - 1 // All 1-bits, as many as letterIdxBits\n\tletterIdxMax  = 63 / letterIdxBits   // # of letter indices fitting in 63 bits\n)\n\n// randString generates random names and prepends them with a known prefix.\nfunc randString(n int, src rand.Source, prefix string) string {\n\tb := make([]byte, n)\n\t// A rand.Int63() generates 63 random bits, enough for letterIdxMax letters!\n\tfor i, cache, remain := n-1, src.Int63(), letterIdxMax; i >= 0; {\n\t\tif remain == 0 {\n\t\t\tcache, remain = src.Int63(), letterIdxMax\n\t\t}\n\t\tif idx := int(cache & letterIdxMask); idx < len(letterBytes) {\n\t\t\tb[i] = letterBytes[idx]\n\t\t\ti--\n\t\t}\n\t\tcache >>= letterIdxBits\n\t\tremain--\n\t}\n\treturn prefix + string(b[0:30-len(prefix)])\n}\n\n// IsNetworkOrHostDown - if there was a network error or if the host is down.\n// expectTimeouts indicates that *context* timeouts are expected and does not\n// indicate a downed host. Other timeouts still returns down.\nfunc IsNetworkOrHostDown(err error, expectTimeouts bool) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\n\tif errors.Is(err, context.Canceled) {\n\t\treturn false\n\t}\n\n\tif expectTimeouts && errors.Is(err, context.DeadlineExceeded) {\n\t\treturn false\n\t}\n\n\tif errors.Is(err, context.DeadlineExceeded) {\n\t\treturn true\n\t}\n\n\t// We need to figure if the error either a timeout\n\t// or a non-temporary error.\n\turlErr := &url.Error{}\n\tif errors.As(err, &urlErr) {\n\t\tswitch urlErr.Err.(type) {\n\t\tcase *net.DNSError, *net.OpError, net.UnknownNetworkError, *tls.CertificateVerificationError:\n\t\t\treturn true\n\t\t}\n\t}\n\tvar e net.Error\n\tif errors.As(err, &e) {\n\t\tif e.Timeout() {\n\t\t\treturn true\n\t\t}\n\t}\n\n\t// Fallback to other mechanisms.\n\tswitch {\n\tcase strings.Contains(err.Error(), \"Connection closed by foreign host\"):\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"TLS handshake timeout\"):\n\t\t// If error is - tlsHandshakeTimeoutError.\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"i/o timeout\"):\n\t\t// If error is - tcp timeoutError.\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"connection timed out\"):\n\t\t// If err is a net.Dial timeout.\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"connection refused\"):\n\t\t// If err is connection refused\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"server gave HTTP response to HTTPS client\"):\n\t\t// If err is TLS client is used with HTTP server\n\t\treturn true\n\tcase strings.Contains(err.Error(), \"Client sent an HTTP request to an HTTPS server\"):\n\t\t// If err is plain-text Client is used with a HTTPS server\n\t\treturn true\n\tcase strings.Contains(strings.ToLower(err.Error()), \"503 service unavailable\"):\n\t\t// Denial errors\n\t\treturn true\n\t}\n\treturn false\n}\n\n// newHashReaderWrapper will hash all reads done through r.\n// When r returns io.EOF the done function will be called with the sum.\nfunc newHashReaderWrapper(r io.Reader, h hash.Hash, done func(hash []byte)) *hashReaderWrapper {\n\treturn &hashReaderWrapper{\n\t\tr:    r,\n\t\th:    h,\n\t\tdone: done,\n\t}\n}\n\ntype hashReaderWrapper struct {\n\tr    io.Reader\n\th    hash.Hash\n\tdone func(hash []byte)\n}\n\n// Read implements the io.Reader interface.\nfunc (h *hashReaderWrapper) Read(p []byte) (n int, err error) {\n\tn, err = h.r.Read(p)\n\tif n > 0 {\n\t\tn2, err := h.h.Write(p[:n])\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif n2 != n {\n\t\t\treturn 0, io.ErrShortWrite\n\t\t}\n\t}\n\tif err == io.EOF {\n\t\t// Call back\n\t\th.done(h.h.Sum(nil))\n\t}\n\treturn n, err\n}\n\n// Following is ported from C to Go in 2016 by Justin Ruggles, with minimal alteration.\n// Used uint for unsigned long. Used uint32 for input arguments in order to match\n// the Go hash/crc32 package. zlib CRC32 combine (https://github.com/madler/zlib)\n// Modified for hash/crc64 by Klaus Post, 2024.\nfunc gf2MatrixTimes(mat []uint64, vec uint64) uint64 {\n\tvar sum uint64\n\n\tfor vec != 0 {\n\t\tif vec&1 != 0 {\n\t\t\tsum ^= mat[0]\n\t\t}\n\t\tvec >>= 1\n\t\tmat = mat[1:]\n\t}\n\treturn sum\n}\n\nfunc gf2MatrixSquare(square, mat []uint64) {\n\tif len(square) != len(mat) {\n\t\tpanic(\"square matrix size mismatch\")\n\t}\n\tfor n := range mat {\n\t\tsquare[n] = gf2MatrixTimes(mat, mat[n])\n\t}\n}\n\n// crc32Combine returns the combined CRC-32 hash value of the two passed CRC-32\n// hash values crc1 and crc2. poly represents the generator polynomial\n// and len2 specifies the byte length that the crc2 hash covers.\nfunc crc32Combine(poly uint32, crc1, crc2 uint32, len2 int64) uint32 {\n\t// degenerate case (also disallow negative lengths)\n\tif len2 <= 0 {\n\t\treturn crc1\n\t}\n\n\teven := make([]uint64, 32) // even-power-of-two zeros operator\n\todd := make([]uint64, 32)  // odd-power-of-two zeros operator\n\n\t// put operator for one zero bit in odd\n\todd[0] = uint64(poly) // CRC-32 polynomial\n\trow := uint64(1)\n\tfor n := 1; n < 32; n++ {\n\t\todd[n] = row\n\t\trow <<= 1\n\t}\n\n\t// put operator for two zero bits in even\n\tgf2MatrixSquare(even, odd)\n\n\t// put operator for four zero bits in odd\n\tgf2MatrixSquare(odd, even)\n\n\t// apply len2 zeros to crc1 (first square will put the operator for one\n\t// zero byte, eight zero bits, in even)\n\tcrc1n := uint64(crc1)\n\tfor {\n\t\t// apply zeros operator for this bit of len2\n\t\tgf2MatrixSquare(even, odd)\n\t\tif len2&1 != 0 {\n\t\t\tcrc1n = gf2MatrixTimes(even, crc1n)\n\t\t}\n\t\tlen2 >>= 1\n\n\t\t// if no more bits set, then done\n\t\tif len2 == 0 {\n\t\t\tbreak\n\t\t}\n\n\t\t// another iteration of the loop with odd and even swapped\n\t\tgf2MatrixSquare(odd, even)\n\t\tif len2&1 != 0 {\n\t\t\tcrc1n = gf2MatrixTimes(odd, crc1n)\n\t\t}\n\t\tlen2 >>= 1\n\n\t\t// if no more bits set, then done\n\t\tif len2 == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// return combined crc\n\tcrc1n ^= uint64(crc2)\n\treturn uint32(crc1n)\n}\n\nfunc crc64Combine(poly uint64, crc1, crc2 uint64, len2 int64) uint64 {\n\t// degenerate case (also disallow negative lengths)\n\tif len2 <= 0 {\n\t\treturn crc1\n\t}\n\n\teven := make([]uint64, 64) // even-power-of-two zeros operator\n\todd := make([]uint64, 64)  // odd-power-of-two zeros operator\n\n\t// put operator for one zero bit in odd\n\todd[0] = poly // CRC-64 polynomial\n\trow := uint64(1)\n\tfor n := 1; n < 64; n++ {\n\t\todd[n] = row\n\t\trow <<= 1\n\t}\n\n\t// put operator for two zero bits in even\n\tgf2MatrixSquare(even, odd)\n\n\t// put operator for four zero bits in odd\n\tgf2MatrixSquare(odd, even)\n\n\t// apply len2 zeros to crc1 (first square will put the operator for one\n\t// zero byte, eight zero bits, in even)\n\tcrc1n := crc1\n\tfor {\n\t\t// apply zeros operator for this bit of len2\n\t\tgf2MatrixSquare(even, odd)\n\t\tif len2&1 != 0 {\n\t\t\tcrc1n = gf2MatrixTimes(even, crc1n)\n\t\t}\n\t\tlen2 >>= 1\n\n\t\t// if no more bits set, then done\n\t\tif len2 == 0 {\n\t\t\tbreak\n\t\t}\n\n\t\t// another iteration of the loop with odd and even swapped\n\t\tgf2MatrixSquare(odd, even)\n\t\tif len2&1 != 0 {\n\t\t\tcrc1n = gf2MatrixTimes(odd, crc1n)\n\t\t}\n\t\tlen2 >>= 1\n\n\t\t// if no more bits set, then done\n\t\tif len2 == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// return combined crc\n\tcrc1n ^= crc2\n\treturn crc1n\n}\n"
        },
        {
          "name": "utils_test.go",
          "type": "blob",
          "size": 16.177734375,
          "content": "/*\n * MinIO Go Library for Amazon S3 Compatible Cloud Storage\n * Copyright 2015-2017 MinIO, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage minio\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net/url\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/minio/minio-go/v7/pkg/s3utils\"\n)\n\nfunc TestParseRFC7231Time(t *testing.T) {\n\ttestCases := []struct {\n\t\ttimeStr         string\n\t\texpectedSuccess bool\n\t}{\n\t\t{\n\t\t\ttimeStr:         \"Sun, 2 Jan 2000 20:34:56 GMT\",\n\t\t\texpectedSuccess: true,\n\t\t},\n\t\t{\n\t\t\ttimeStr:         \"Sun, 02 Jan 2000 20:34:56 GMT\",\n\t\t\texpectedSuccess: true,\n\t\t},\n\t\t{\n\t\t\ttimeStr:         \"Sun, 2 Jan 00 20:34:56 GMT\",\n\t\t\texpectedSuccess: true,\n\t\t},\n\t\t{\n\t\t\ttimeStr:         \"Sun, 02 Jan 00 20:34:56 GMT\",\n\t\t\texpectedSuccess: true,\n\t\t},\n\t\t{\n\t\t\ttimeStr:         \"Su, 2 Jan 00 20:34:56 GMT\",\n\t\t\texpectedSuccess: false,\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\ttestCase := testCase\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\t_, err := parseRFC7231Time(testCase.timeStr)\n\t\t\tif err != nil && testCase.expectedSuccess {\n\t\t\t\tt.Errorf(\"expected success found failure %v\", err)\n\t\t\t}\n\t\t\tif err == nil && !testCase.expectedSuccess {\n\t\t\t\tt.Errorf(\"expected failure found success\")\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Tests signature redacting function used\n// in filtering on-wire Authorization header.\nfunc TestRedactSignature(t *testing.T) {\n\ttestCases := []struct {\n\t\tauthValue                 string\n\t\texpectedRedactedAuthValue string\n\t}{\n\t\t{\n\t\t\tauthValue:                 \"AWS 1231313:888x000231==\",\n\t\t\texpectedRedactedAuthValue: \"AWS **REDACTED**:**REDACTED**\",\n\t\t},\n\t\t{\n\t\t\tauthValue:                 \"AWS4-HMAC-SHA256 Credential=12312313/20170613/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=02131231312313213\",\n\t\t\texpectedRedactedAuthValue: \"AWS4-HMAC-SHA256 Credential=**REDACTED**/20170613/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=**REDACTED**\",\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tredactedAuthValue := redactSignature(testCase.authValue)\n\t\tif redactedAuthValue != testCase.expectedRedactedAuthValue {\n\t\t\tt.Errorf(\"Test %d: Expected %s, got %s\", i+1, testCase.expectedRedactedAuthValue, redactedAuthValue)\n\t\t}\n\t}\n}\n\n// Tests for 'getEndpointURL(endpoint string, inSecure bool)'.\nfunc TestGetEndpointURL(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Inputs.\n\t\tendPoint string\n\t\tsecure   bool\n\n\t\t// Expected result.\n\t\tresult string\n\t\terr    error\n\t\t// Flag indicating whether the test is expected to pass or not.\n\t\tshouldPass bool\n\t}{\n\t\t{\"s3.amazonaws.com\", true, \"https://s3.amazonaws.com\", nil, true},\n\t\t{\"s3.cn-north-1.amazonaws.com.cn\", true, \"https://s3.cn-north-1.amazonaws.com.cn\", nil, true},\n\t\t{\"s3.cn-northwest-1.amazonaws.com.cn\", true, \"https://s3.cn-northwest-1.amazonaws.com.cn\", nil, true},\n\t\t{\"s3.amazonaws.com\", false, \"http://s3.amazonaws.com\", nil, true},\n\t\t{\"s3.cn-north-1.amazonaws.com.cn\", false, \"http://s3.cn-north-1.amazonaws.com.cn\", nil, true},\n\t\t{\"s3.cn-northwest-1.amazonaws.com.cn\", false, \"http://s3.cn-northwest-1.amazonaws.com.cn\", nil, true},\n\t\t{\"192.168.1.1:9000\", false, \"http://192.168.1.1:9000\", nil, true},\n\t\t{\"192.168.1.1:9000\", true, \"https://192.168.1.1:9000\", nil, true},\n\t\t{\"s3.amazonaws.com:443\", true, \"https://s3.amazonaws.com:443\", nil, true},\n\t\t{\"storage.googleapis.com:443\", true, \"https://storage.googleapis.com:443\", nil, true},\n\t\t{\"[::1]\", false, \"http://[::1]\", nil, true},\n\t\t{\"[::1]\", true, \"https://[::1]\", nil, true},\n\t\t{\"[::1]:80\", false, \"http://[::1]:80\", nil, true},\n\t\t{\"[::1]:443\", true, \"https://[::1]:443\", nil, true},\n\t\t{\"[::1]:9000\", false, \"http://[::1]:9000\", nil, true},\n\t\t{\"[::1]:9000\", true, \"https://[::1]:9000\", nil, true},\n\t\t{\"13333.123123.-\", true, \"\", errInvalidArgument(fmt.Sprintf(\"Endpoint: %s does not follow ip address or domain name standards.\", \"13333.123123.-\")), false},\n\t\t{\"13333.123123.-\", true, \"\", errInvalidArgument(fmt.Sprintf(\"Endpoint: %s does not follow ip address or domain name standards.\", \"13333.123123.-\")), false},\n\t\t{\"s3.aamzza.-\", true, \"\", errInvalidArgument(fmt.Sprintf(\"Endpoint: %s does not follow ip address or domain name standards.\", \"s3.aamzza.-\")), false},\n\t\t{\"\", true, \"\", errInvalidArgument(\"Endpoint:  does not follow ip address or domain name standards.\"), false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tresult, err := getEndpointURL(testCase.endPoint, testCase.secure)\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err.Error())\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err.Error())\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err.Error(), err.Error())\n\t\t\t}\n\t\t}\n\n\t\t// Test passes as expected, but the output values are verified for correctness here.\n\t\tif err == nil && testCase.shouldPass {\n\t\t\tif testCase.result != result.String() {\n\t\t\t\tt.Errorf(\"Test %d: Expected the result Url to be \\\"%s\\\", but found \\\"%s\\\" instead\", i+1, testCase.result, result.String())\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Tests validate end point validator.\nfunc TestIsValidEndpointURL(t *testing.T) {\n\ttestCases := []struct {\n\t\turl string\n\t\terr error\n\t\t// Flag indicating whether the test is expected to pass or not.\n\t\tshouldPass bool\n\t}{\n\t\t{\"\", errInvalidArgument(\"Endpoint url cannot be empty.\"), false},\n\t\t{\"https://s3.amazonaws.com\", nil, true},\n\t\t{\"https://s3.cn-north-1.amazonaws.com.cn\", nil, true},\n\t\t{\"https://s3-us-gov-west-1.amazonaws.com\", nil, true},\n\t\t{\"https://s3-fips-us-gov-west-1.amazonaws.com\", nil, true},\n\t\t{\"https://s3-fips.us-gov-west-1.amazonaws.com\", nil, true},\n\t\t{\"https://s3-fips.us-gov-east-1.amazonaws.com\", nil, true},\n\t\t{\"https://s3.amazonaws.com/\", nil, true},\n\t\t{\"https://storage.googleapis.com/\", nil, true},\n\t\t{\"https://z3.amazonaws.com\", nil, true},\n\t\t{\"https://mybalancer.us-east-1.elb.amazonaws.com\", nil, true},\n\t\t{\"192.168.1.1\", errInvalidArgument(\"Endpoint url cannot have fully qualified paths.\"), false},\n\t\t{\"https://amazon.googleapis.com/\", errInvalidArgument(\"Google Cloud Storage endpoint should be 'storage.googleapis.com'.\"), false},\n\t\t{\"https://storage.googleapis.com/bucket/\", errInvalidArgument(\"Endpoint url cannot have fully qualified paths.\"), false},\n\t\t{\"https://s3.amazonaws.com/bucket/object\", errInvalidArgument(\"Endpoint url cannot have fully qualified paths.\"), false},\n\t\t{\"https://.s3server.example.com/\", errInvalidArgument(\"Endpoint: .s3server.example.com does not follow ip address or domain name standards.\"), false},\n\t\t{\"https://s3server.example_/\", errInvalidArgument(\"Endpoint: s3server.example_ does not follow ip address or domain name standards.\"), false},\n\t\t{\"https://_s3server.example.com/\", errInvalidArgument(\"Endpoint: _s3server.example.com does not follow ip address or domain name standards.\"), false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tvar u url.URL\n\t\tif testCase.url == \"\" {\n\t\t\tu = sentinelURL\n\t\t} else {\n\t\t\tu1, err := url.Parse(testCase.url)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err)\n\t\t\t}\n\t\t\tu = *u1\n\t\t}\n\t\terr := isValidEndpointURL(u)\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err)\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err)\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err, err)\n\t\t\t}\n\t\t}\n\n\t}\n}\n\nfunc TestDefaultBucketLocation(t *testing.T) {\n\ttestCases := []struct {\n\t\tendpointURL      url.URL\n\t\tregionOverride   string\n\t\texpectedLocation string\n\t}{\n\t\t// Region override is set URL is ignored. - Test 1.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3-fips-us-gov-west-1.amazonaws.com\"},\n\t\t\tregionOverride:   \"us-west-1\",\n\t\t\texpectedLocation: \"us-west-1\",\n\t\t},\n\t\t// No region override, url based preferenced is honored - Test 2.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3-fips-us-gov-west-1.amazonaws.com\"},\n\t\t\tregionOverride:   \"\",\n\t\t\texpectedLocation: \"us-gov-west-1\",\n\t\t},\n\t\t// Region override is honored - Test 3.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3.amazonaws.com\"},\n\t\t\tregionOverride:   \"us-west-1\",\n\t\t\texpectedLocation: \"us-west-1\",\n\t\t},\n\t\t// China region should be honored, region override not provided. - Test 4.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3.cn-north-1.amazonaws.com.cn\"},\n\t\t\tregionOverride:   \"\",\n\t\t\texpectedLocation: \"cn-north-1\",\n\t\t},\n\t\t// China region should be honored, region override not provided. - Test 5.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3.cn-northwest-1.amazonaws.com.cn\"},\n\t\t\tregionOverride:   \"\",\n\t\t\texpectedLocation: \"cn-northwest-1\",\n\t\t},\n\t\t// No region provided, no standard region strings provided as well. - Test 6.\n\t\t{\n\t\t\tendpointURL:      url.URL{Host: \"s3.amazonaws.com\"},\n\t\t\tregionOverride:   \"\",\n\t\t\texpectedLocation: \"us-east-1\",\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tretLocation := getDefaultLocation(testCase.endpointURL, testCase.regionOverride)\n\t\tif testCase.expectedLocation != retLocation {\n\t\t\tt.Errorf(\"Test %d: Expected location %s, got %s\", i+1, testCase.expectedLocation, retLocation)\n\t\t}\n\t}\n}\n\n// Tests validate the expiry time validator.\nfunc TestIsValidExpiry(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\tduration time.Duration\n\t\t// Expected result.\n\t\terr error\n\t\t// Flag to indicate whether the test should pass.\n\t\tshouldPass bool\n\t}{\n\t\t{100 * time.Millisecond, errInvalidArgument(\"Expires cannot be lesser than 1 second.\"), false},\n\t\t{604801 * time.Second, errInvalidArgument(\"Expires cannot be greater than 7 days.\"), false},\n\t\t{0 * time.Second, errInvalidArgument(\"Expires cannot be lesser than 1 second.\"), false},\n\t\t{1 * time.Second, nil, true},\n\t\t{10000 * time.Second, nil, true},\n\t\t{999 * time.Second, nil, true},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\terr := isValidExpiry(testCase.duration)\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err.Error())\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err.Error())\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err.Error(), err.Error())\n\t\t\t}\n\t\t}\n\n\t}\n}\n\n// Tests validate the bucket name validator.\nfunc TestIsValidBucketName(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\tbucketName string\n\t\t// Expected result.\n\t\terr error\n\t\t// Flag to indicate whether test should Pass.\n\t\tshouldPass bool\n\t}{\n\t\t{\".mybucket\", errors.New(\"Bucket name contains invalid characters\"), false},\n\t\t{\"mybucket.\", errors.New(\"Bucket name contains invalid characters\"), false},\n\t\t{\"mybucket-\", errors.New(\"Bucket name contains invalid characters\"), false},\n\t\t{\"my\", errors.New(\"Bucket name cannot be shorter than 3 characters\"), false},\n\t\t{\"\", errors.New(\"Bucket name cannot be empty\"), false},\n\t\t{\"my..bucket\", errors.New(\"Bucket name contains invalid characters\"), false},\n\t\t{\"my.bucket.com\", nil, true},\n\t\t{\"my-bucket\", nil, true},\n\t\t{\"123my-bucket\", nil, true},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\terr := s3utils.CheckValidBucketName(testCase.bucketName)\n\t\tif err != nil && testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed with: <ERROR> %s\", i+1, err.Error())\n\t\t}\n\t\tif err == nil && !testCase.shouldPass {\n\t\t\tt.Errorf(\"Test %d: Expected to fail with <ERROR> \\\"%s\\\", but passed instead\", i+1, testCase.err.Error())\n\t\t}\n\t\t// Failed as expected, but does it fail for the expected reason.\n\t\tif err != nil && !testCase.shouldPass {\n\t\t\tif err.Error() != testCase.err.Error() {\n\t\t\t\tt.Errorf(\"Test %d: Expected to fail with error \\\"%s\\\", but instead failed with error \\\"%s\\\" instead\", i+1, testCase.err.Error(), err.Error())\n\t\t\t}\n\t\t}\n\n\t}\n}\n\n// Tests if header is standard supported header\nfunc TestIsStandardHeader(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\theader string\n\t\t// Expected result.\n\t\texpectedValue bool\n\t}{\n\t\t{\"content-encoding\", true},\n\t\t{\"content-type\", true},\n\t\t{\"cache-control\", true},\n\t\t{\"content-disposition\", true},\n\t\t{\"content-language\", true},\n\t\t{\"random-header\", false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactual := isStandardHeader(testCase.header)\n\t\tif actual != testCase.expectedValue {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed\", i+1)\n\t\t}\n\t}\n}\n\n// Tests if header is server encryption header\nfunc TestIsSSEHeader(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\theader string\n\t\t// Expected result.\n\t\texpectedValue bool\n\t}{\n\t\t{\"x-amz-server-side-encryption\", true},\n\t\t{\"x-amz-server-side-encryption-aws-kms-key-id\", true},\n\t\t{\"x-amz-server-side-encryption-context\", true},\n\t\t{\"x-amz-server-side-encryption-customer-algorithm\", true},\n\t\t{\"x-amz-server-side-encryption-customer-key\", true},\n\t\t{\"x-amz-server-side-encryption-customer-key-MD5\", true},\n\t\t{\"random-header\", false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactual := isSSEHeader(testCase.header)\n\t\tif actual != testCase.expectedValue {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed\", i+1)\n\t\t}\n\t}\n}\n\n// Tests if header is x-amz-meta or x-amz-acl\nfunc TestIsAmzHeader(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\theader string\n\t\t// Expected result.\n\t\texpectedValue bool\n\t}{\n\t\t{\"x-amz-iv\", false},\n\t\t{\"x-amz-key\", false},\n\t\t{\"x-amz-matdesc\", false},\n\t\t{\"x-amz-meta-x-amz-iv\", true},\n\t\t{\"x-amz-meta-x-amz-key\", true},\n\t\t{\"x-amz-meta-x-amz-matdesc\", true},\n\t\t{\"x-amz-acl\", true},\n\t\t{\"random-header\", false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactual := isAmzHeader(testCase.header)\n\t\tif actual != testCase.expectedValue {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed\", i+1)\n\t\t}\n\t}\n}\n\n// Tests if query parameter starts with \"x-\" and will be ignored by S3.\nfunc TestIsCustomQueryValue(t *testing.T) {\n\ttestCases := []struct {\n\t\t// Input.\n\t\tqueryParamKey string\n\t\t// Expected result.\n\t\texpectedValue bool\n\t}{\n\t\t{\"x-custom-key\", true},\n\t\t{\"xcustom-key\", false},\n\t\t{\"random-header\", false},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactual := isCustomQueryValue(testCase.queryParamKey)\n\t\tif actual != testCase.expectedValue {\n\t\t\tt.Errorf(\"Test %d: Expected to pass, but failed\", i+1)\n\t\t}\n\t}\n}\n\nfunc TestFullObjectChecksum64(t *testing.T) {\n\ttests := []ChecksumType{\n\t\tChecksumCRC32,\n\t\tChecksumCRC32C,\n\t\tChecksumCRC64NVME,\n\t}\n\tfor _, cs := range tests {\n\t\tt.Run(cs.String(), func(t *testing.T) {\n\t\t\tb := make([]byte, 1024000)\n\t\t\trng := rand.New(rand.NewSource(time.Now().UnixNano()))\n\t\t\trng.Read(b)\n\t\t\tsum := cs.EncodeToString\n\t\t\twant := sum(b)\n\t\t\tvar parts []ObjectPart\n\t\t\tfor len(b) > 0 {\n\t\t\t\tsz := rng.Intn(len(b) / 2)\n\t\t\t\tif len(b)-sz < 1024 {\n\t\t\t\t\tsz = len(b)\n\t\t\t\t}\n\t\t\t\tswitch cs {\n\t\t\t\tcase ChecksumCRC32:\n\t\t\t\t\tparts = append(parts, ObjectPart{PartNumber: len(parts) + 1, ChecksumCRC32: cs.EncodeToString(b[:sz]), Size: int64(sz)})\n\t\t\t\tcase ChecksumCRC32C:\n\t\t\t\t\tparts = append(parts, ObjectPart{PartNumber: len(parts) + 1, ChecksumCRC32C: cs.EncodeToString(b[:sz]), Size: int64(sz)})\n\t\t\t\tcase ChecksumCRC64NVME:\n\t\t\t\t\tparts = append(parts, ObjectPart{PartNumber: len(parts) + 1, ChecksumCRC64NVME: cs.EncodeToString(b[:sz]), Size: int64(sz)})\n\t\t\t\t}\n\t\t\t\tb = b[sz:]\n\t\t\t}\n\t\t\tgotCRC, err := cs.FullObjectChecksum(parts)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif gotCRC.Encoded() != want {\n\t\t\t\tt.Errorf(\"Checksum %v does not match the expected CRC got:%s want:%s\", cs.String(), gotCRC.Encoded(), want)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        }
      ]
    }
  ]
}