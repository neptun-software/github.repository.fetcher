{
  "metadata": {
    "timestamp": 1736567129207,
    "page": 725,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjczMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "go-ego/gse",
      "stars": 2610,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8505859375,
          "content": "# Object files\n.DS_Store\n.vscode\n.idea\n\nexamples/examples\nexamples/jp/jp\n# \ndata/dict/zh/dict.txt\ndata/dict/zh/dictionary.txt\n\nvendor\n# vendor/github.com/vcaesar/tt\n\n# Debug files\n*.dSYM/\n*.su\ndebug\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.o\n*.ko\n*.obj\n*.elf\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Libraries\n*.lib\n*.a\n*.la\n*.lo\n\n# Shared objects (inc. Windows DLLs)\n*.dll\n*.so\n*.so.*\n*.dylib\n\n# Executables\n*.exe\n*.out\n*.app\n*.i*86\n*.x86_64\n*.hex\n\n# Binaries for programs and plugins\n\n# Test binary, build with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n# *.out\n\n# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736\n.glide/\nexamples/dict/embed/embed\nexamples/dict/embed/main\noryxBuildBinary"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.3544921875,
          "content": "language: go\n\ngo:\n  # - 1.7.x\n  # - 1.8.x\n  # - 1.9.x\n  # - 1.10.x\n  # - 1.11.x\n  # - 1.12.x\n  # - 1.13.x\n  # - 1.16.x\n  # - 1.17.x\n  # - 1.18.x\n  # - 1.19.x\n  - 1.20.x\n  # - tip\n\ninstall:\n  - export PATH=$PATH:$HOME/gopath/bin\n  # - go get -u github.com/go-ego/cedar\n  - go get -t -d -v ./...\n  # - go get -u -v -t ./...\n# notifications:\n#   email:\n#     - .com\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.2431640625,
          "content": "# Contribution Guidelines\n\n## Introduction\n\nThis document explains how to contribute changes to the Ego project. It assumes you have followed the README.md and [API Document](https://github.com/go-ego/gse/blob/master/docs/doc.md). <!--Sensitive security-related issues should be reported to [security@Ego.io](mailto:security@Ego.io.)-->\n\n## Bug reports\n\nPlease search the issues on the issue tracker with a variety of keywords to ensure your bug is not already reported.\n\nIf unique, [open an issue](https://github.com/go-ego/gse/issues/new) and answer the questions so we can understand and reproduce the problematic behavior.\n\nThe burden is on you to convince us that it is actually a bug in Ego. This is easiest to do when you write clear, concise instructions so we can reproduce the behavior (even if it seems obvious). The more detailed and specific you are, the faster we will be able to help you. Check out [How to Report Bugs Effectively](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html).\n\nPlease be kind, remember that Ego comes at no cost to you, and you're getting free help.\n\n## Discuss your design\n\nThe project welcomes submissions but please let everyone know what you're working on if you want to change or add something to the Ego repositories.\n\nBefore starting to write something new for the Ego project, please [file an issue](https://github.com/go-ego/gse/issues/new). Significant changes must go through the [change proposal process](https://github.com/go-ego/proposals) before they can be accepted.\n\nThis process gives everyone a chance to validate the design, helps prevent duplication of effort, and ensures that the idea fits inside the goals for the project and tools. It also checks that the design is sound before code is written; the code review tool is not the place for high-level discussions.\n\n## Testing redux\n\nBefore sending code out for review, run all the tests for the whole tree to make sure the changes don't break other usage and keep the compatibility on upgrade. You must be test on Mac, Windows, Linux and other. You should install the CLI for Circle CI, as we are using the server for continous testing.\n\n## Code review\n\nIn addition to the owner, Changes to Ego must be reviewed before they are accepted, no matter who makes the change even if it is a maintainer. We use GitHub's pull request workflow to do that and we also use [LGTM](http://lgtm.co) to ensure every PR is reviewed by vz or least 2 maintainers.\n\n\n## Sign your work\n\nThe sign-off is a simple line at the end of the explanation for the patch. Your signature certifies that you wrote the patch or otherwise have the right to pass it on as an open-source patch. \n\n## Maintainers\n\nTo make sure every PR is checked, we got team maintainers. A maintainer should be a contributor of Ego and contributed at least 4 accepted PRs. \n\n## Owners\n\nSince Ego is a pure community organization without any company support, Copyright 2016 The go-ego Project Developers.\n\n\n## Versions\n\nEgo has the `master` branch as a tip branch and has version branches such as `v0.30.0`. `v0.40.0` is a release branch and we will tag `v0.40.0` for binary download. If `v0.40.0` has bugs, we will accept pull requests on the `v0.40.0` branch and publish a `v0.40.1` tag, after bringing the bug fix also to the master branch.\n\nSince the `master` branch is a tip version, if you wish to use Ego in production, please download the latest release tag version. All the branches will be protected via GitHub, all the PRs to every branch must be reviewed by two maintainers and must pass the automatic tests.\n\n## Copyright\n\nCode that you contribute should use the standard copyright header:\n\n```\n// Copyright 2016 The go-ego Project Developers. See the COPYRIGHT\n// file at the top-level directory of this distribution and at\n// https://github.com/go-ego/gse/blob/master/LICENSE\n//\n// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n// option. This file may not be copied, modified, or distributed\n// except according to those terms.\n```\n\nFiles in the repository contain copyright from the year they are added to the year they are last changed. If the copyright author is changed, just paste the header below the old one.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.3486328125,
          "content": "# gse\r\n\r\nGo efficient multilingual NLP and text segmentation; support English, Chinese, Japanese and others.\r\nAnd supports with [elasticsearch](https://github.com/vcaesar/go-gse-elastic) and [bleve](https://github.com/vcaesar/gse-bleve).\r\n\r\n<!--<img align=\"right\" src=\"https://raw.githubusercontent.com/go-ego/ego/master/logo.jpg\">-->\r\n<!--<a href=\"https://circleci.com/gh/go-ego/ego/tree/dev\"><img src=\"https://img.shields.io/circleci/project/go-ego/ego/dev.svg\" alt=\"Build Status\"></a>-->\r\n\r\n[![Build Status](https://github.com/go-ego/gse/workflows/Go/badge.svg)](https://github.com/go-ego/gse/commits/master)\r\n[![CircleCI Status](https://circleci.com/gh/go-ego/gse.svg?style=shield)](https://circleci.com/gh/go-ego/gse)\r\n[![codecov](https://codecov.io/gh/go-ego/gse/branch/master/graph/badge.svg)](https://codecov.io/gh/go-ego/gse)\r\n[![Build Status](https://travis-ci.org/go-ego/gse.svg)](https://travis-ci.org/go-ego/gse)\r\n[![Go Report Card](https://goreportcard.com/badge/github.com/go-ego/gse)](https://goreportcard.com/report/github.com/go-ego/gse)\r\n[![GoDoc](https://godoc.org/github.com/go-ego/gse?status.svg)](https://godoc.org/github.com/go-ego/gse)\r\n[![GitHub release](https://img.shields.io/github/release/go-ego/gse.svg)](https://github.com/go-ego/gse/releases/latest)\r\n[![Join the chat at https://gitter.im/go-ego/ego](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/go-ego/ego?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\r\n\r\n<!-- [![Release](https://github-release-version.herokuapp.com/github/go-ego/gse/release.svg?style=flat)](https://github.com/go-ego/gse/releases/latest) -->\r\n<!--<a href=\"https://github.com/go-ego/ego/releases\"><img src=\"https://img.shields.io/badge/%20version%20-%206.0.0%20-blue.svg?style=flat-square\" alt=\"Releases\"></a>-->\r\n\r\n[简体中文](https://github.com/go-ego/gse/blob/master/README_zh.md)\r\n\r\nGse is implements jieba by golang, and try add NLP support and more feature\r\n\r\n## Feature:\r\n\r\n- Support common, search engine, full mode, precise mode and HMM mode multiple word segmentation modes;\r\n- Support user and embed dictionary, Part-of-speech/POS tagging, analyze segment info, stop and trim words\r\n- Support multilingual: English, Chinese, Japanese and others\r\n- Support Traditional Chinese\r\n- Support HMM cut text use Viterbi algorithm\r\n- Support NLP by TensorFlow (in work)\r\n- Named Entity Recognition (in work)\r\n- Supports with [elasticsearch](https://github.com/vcaesar/go-gse-elastic) and bleve\r\n- run<a href=\"https://github.com/go-ego/gse/blob/master/tools/server/server.go\"> JSON RPC service</a>.\r\n\r\n## Algorithm:\r\n\r\n- [Dictionary](https://github.com/go-ego/gse/blob/master/dictionary.go) with double array trie (Double-Array Trie) to achieve\r\n- [Segmenter](https://github.com/go-ego/gse/blob/master/dag.go) algorithm is the shortest path (based on word frequency and dynamic programming), and DAG and HMM algorithm word segmentation.\r\n\r\n## Text Segmentation speed:\r\n\r\n- <a href=\"https://github.com/go-ego/gse/blob/master/tools/benchmark/benchmark.go\"> single thread</a> 9.2MB/s\r\n- <a href=\"https://github.com/go-ego/gse/blob/master/tools/benchmark/goroutines/goroutines.go\">goroutines concurrent</a> 26.8MB/s.\r\n- HMM text segmentation single thread 3.2MB/s. (2core 4threads Macbook Pro).\r\n\r\n## Binding:\r\n\r\n[gse-bind](https://github.com/vcaesar/gse-bind), binding JavaScript and other, support more language.\r\n\r\n## Install / update\r\n\r\nWith Go module support (Go 1.11+), just import:\r\n\r\n```go\r\nimport \"github.com/go-ego/gse\"\r\n```\r\n\r\nOtherwise, to install the gse package, run the command:\r\n\r\n```\r\ngo get -u github.com/go-ego/gse\r\n```\r\n\r\n## Use\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t_ \"embed\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/go-ego/gse\"\r\n)\r\n\r\n//go:embed testdata/test_en2.txt\r\nvar testDict string\r\n\r\n//go:embed testdata/test_en.txt\r\nvar testEn string\r\n\r\nvar (\r\n\ttext  = \"To be or not to be, that's the question!\"\r\n\ttest1 = \"Hiworld, Helloworld!\"\r\n)\r\n\r\nfunc main() {\r\n\tvar seg1 gse.Segmenter\r\n\tseg1.DictSep = \",\"\r\n\terr := seg1.LoadDict(\"./testdata/test_en.txt\")\r\n\tif err != nil {\r\n\t\tfmt.Println(\"Load dictionary error: \", err)\r\n\t}\r\n\r\n\ts1 := seg1.Cut(text)\r\n\tfmt.Println(\"seg1 Cut: \", s1)\r\n\t// seg1 Cut:  [to be   or   not to be ,   that's the question!]\r\n\r\n\tvar seg2 gse.Segmenter\r\n\tseg2.AlphaNum = true\r\n\tseg2.LoadDict(\"./testdata/test_en_dict3.txt\")\r\n\r\n\ts2 := seg2.Cut(test1)\r\n\tfmt.Println(\"seg2 Cut: \", s2)\r\n\t// seg2 Cut:  [hi world ,   hello world !]\r\n\r\n\tvar seg3 gse.Segmenter\r\n\tseg3.AlphaNum = true\r\n\tseg3.DictSep = \",\"\r\n\terr = seg3.LoadDictEmbed(testDict + \"\\n\" + testEn)\r\n\tif err != nil {\r\n\t\tfmt.Println(\"loadDictEmbed error: \", err)\r\n\t}\r\n\ts3 := seg3.Cut(text + test1)\r\n\tfmt.Println(\"seg3 Cut: \", s3)\r\n\t// seg3 Cut:  [to be   or   not to be ,   that's the question! hi world ,   hello world !]\r\n\r\n\t// example2()\r\n}\r\n```\r\n\r\nExample2:\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"regexp\"\r\n\r\n\t\"github.com/go-ego/gse\"\r\n\t\"github.com/go-ego/gse/hmm/pos\"\r\n)\r\n\r\nvar (\r\n\ttext = \"Hello world, Helloworld. Winter is coming! こんにちは世界, 你好世界.\"\r\n\r\n\tnew, _ = gse.New(\"zh,testdata/test_en_dict3.txt\", \"alpha\")\r\n\r\n\tseg gse.Segmenter\r\n\tposSeg pos.Segmenter\r\n)\r\n\r\nfunc main() {\r\n\t// Loading the default dictionary\r\n\tseg.LoadDict()\r\n\t// Loading the default dictionary with embed\r\n\t// seg.LoadDictEmbed()\r\n\t//\r\n\t// Loading the Simplified Chinese dictionary\r\n\t// seg.LoadDict(\"zh_s\")\r\n\t// seg.LoadDictEmbed(\"zh_s\")\r\n\t//\r\n\t// Loading the Traditional Chinese dictionary\r\n\t// seg.LoadDict(\"zh_t\")\r\n\t//\r\n\t// Loading the Japanese dictionary\r\n\t// seg.LoadDict(\"jp\")\r\n\t//\r\n\t// Load the dictionary\r\n\t// seg.LoadDict(\"your gopath\"+\"/src/github.com/go-ego/gse/data/dict/dictionary.txt\")\r\n\r\n\tcut()\r\n\r\n\tsegCut()\r\n}\r\n\r\nfunc cut() {\r\n\thmm := new.Cut(text, true)\r\n\tfmt.Println(\"cut use hmm: \", hmm)\r\n\r\n\thmm = new.CutSearch(text, true)\r\n\tfmt.Println(\"cut search use hmm: \", hmm)\r\n\tfmt.Println(\"analyze: \", new.Analyze(hmm, text))\r\n\r\n\thmm = new.CutAll(text)\r\n\tfmt.Println(\"cut all: \", hmm)\r\n\r\n\treg := regexp.MustCompile(`(\\d+年|\\d+月|\\d+日|[\\p{Latin}]+|[\\p{Hangul}]+|\\d+\\.\\d+|[a-zA-Z0-9]+)`)\r\n\ttext1 := `헬로월드 헬로 서울, 2021年09月10日, 3.14`\r\n\thmm = seg.CutDAG(text1, reg)\r\n\tfmt.Println(\"Cut with hmm and regexp: \", hmm, hmm[0], hmm[6])\r\n}\r\n\r\nfunc analyzeAndTrim(cut []string) {\r\n\ta := seg.Analyze(cut, \"\")\r\n\tfmt.Println(\"analyze the segment: \", a)\r\n\r\n\tcut = seg.Trim(cut)\r\n\tfmt.Println(\"cut all: \", cut)\r\n\r\n\tfmt.Println(seg.String(text, true))\r\n\tfmt.Println(seg.Slice(text, true))\r\n}\r\n\r\nfunc cutPos() {\r\n\tpo := seg.Pos(text, true)\r\n\tfmt.Println(\"pos: \", po)\r\n\tpo = seg.TrimPos(po)\r\n\tfmt.Println(\"trim pos: \", po)\r\n\r\n\tpos.WithGse(seg)\r\n\tpo = posSeg.Cut(text, true)\r\n\tfmt.Println(\"pos: \", po)\r\n\r\n\tpo = posSeg.TrimWithPos(po, \"zg\")\r\n\tfmt.Println(\"trim pos: \", po)\r\n}\r\n\r\nfunc segCut() {\r\n\t// Text Segmentation\r\n\ttb := []byte(text)\r\n\tfmt.Println(seg.String(text, true))\r\n\r\n\tsegments := seg.Segment(tb)\r\n\t// Handle word segmentation results, search mode\r\n\tfmt.Println(gse.ToString(segments, true))\r\n}\r\n\r\n```\r\n\r\n[Look at an custom dictionary example](/examples/dict/main.go)\r\n\r\n```Go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t_ \"embed\"\r\n\r\n\t\"github.com/go-ego/gse\"\r\n)\r\n\r\n//go:embed test_en_dict3.txt\r\nvar testDict string\r\n\r\nfunc main() {\r\n\t// var seg gse.Segmenter\r\n\t// seg.LoadDict(\"zh, testdata/zh/test_dict.txt, testdata/zh/test_dict1.txt\")\r\n\t// seg.LoadStop()\r\n\tseg, err := gse.NewEmbed(\"zh, word 20 n\"+testDict, \"en\")\r\n\t// seg.LoadDictEmbed()\r\n\tseg.LoadStopEmbed()\r\n\r\n\ttext1 := \"Hello world, こんにちは世界, 你好世界!\"\r\n\ts1 := seg.Cut(text1, true)\r\n\tfmt.Println(s1)\r\n\tfmt.Println(\"trim: \", seg.Trim(s1))\r\n\tfmt.Println(\"stop: \", seg.Stop(s1))\r\n\tfmt.Println(seg.String(text1, true))\r\n\r\n\tsegments := seg.Segment([]byte(text1))\r\n\tfmt.Println(gse.ToString(segments))\r\n}\r\n```\r\n\r\n[Look at an Chinese example](/examples/main.go)\r\n\r\n[Look at an Japanese example](/examples/jp/main.go)\r\n\r\n## Elasticsearch\r\n\r\nHow to use it with elasticsearch?\r\n\r\n[go-gse-elastic](https://github.com/vcaesar/go-gse-elastic)\r\n\r\n## Authors\r\n\r\n- [Maintainers](https://github.com/orgs/go-ego/people)\r\n- [Contributors](https://github.com/go-ego/gse/graphs/contributors)\r\n\r\n## License\r\n\r\nGse is primarily distributed under the terms of \"both the MIT license and the Apache License (Version 2.0)\".\r\nSee [LICENSE-APACHE](http://www.apache.org/licenses/LICENSE-2.0), [LICENSE-MIT](https://github.com/go-vgo/robotgo/blob/master/LICENSE).\r\n\r\nThanks for [sego](https://github.com/huichen/sego) and [jieba](https://github.com/fxsjy/jieba)([jiebago](https://github.com/wangbin/jiebago)).\r\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 6.59765625,
          "content": "# [gse](https://github.com/go-ego/gse)\n\nGo 高性能多语言 NLP 和分词, 支持英文、中文、日文等, 支持接入 [elasticsearch](https://github.com/vcaesar/go-gse-elastic) 和 [bleve](https://github.com/vcaesar/gse-bleve)\n\n<!--<img align=\"right\" src=\"https://raw.githubusercontent.com/go-ego/ego/master/logo.jpg\">-->\n<!--<a href=\"https://circleci.com/gh/go-ego/ego/tree/dev\"><img src=\"https://img.shields.io/circleci/project/go-ego/ego/dev.svg\" alt=\"Build Status\"></a>-->\n\n[![Build Status](https://github.com/go-ego/gse/workflows/Go/badge.svg)](https://github.com/go-ego/gse/commits/master)\n[![CircleCI Status](https://circleci.com/gh/go-ego/gse.svg?style=shield)](https://circleci.com/gh/go-ego/gse)\n[![codecov](https://codecov.io/gh/go-ego/gse/branch/master/graph/badge.svg)](https://codecov.io/gh/go-ego/gse)\n[![Build Status](https://travis-ci.org/go-ego/gse.svg)](https://travis-ci.org/go-ego/gse)\n[![Go Report Card](https://goreportcard.com/badge/github.com/go-ego/gse)](https://goreportcard.com/report/github.com/go-ego/gse)\n[![GoDoc](https://godoc.org/github.com/go-ego/gse?status.svg)](https://godoc.org/github.com/go-ego/gse)\n[![GitHub release](https://img.shields.io/github/release/go-ego/gse.svg)](https://github.com/go-ego/gse/releases/latest)\n[![Join the chat at https://gitter.im/go-ego/ego](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/go-ego/ego?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n<!--<a href=\"https://github.com/go-ego/ego/releases\"><img src=\"https://img.shields.io/badge/%20version%20-%206.0.0%20-blue.svg?style=flat-square\" alt=\"Releases\"></a>-->\n\nGse 是结巴分词(jieba)的 golang 实现, 并尝试添加 NLP 功能和更多属性\n\n## 特征:\n\n- 支持普通、搜索引擎、全模式、精确模式和 HMM 模式多种分词模式\n- 支持自定义词典、embed 词典、词性标注、停用词、整理分析分词\n- 多语言支持: 英文, 中文, 日文等\n- 支持繁体字\n- NLP 和 TensorFlow 支持 (进行中)\n- 命名实体识别 (进行中)\n- 支持接入 Elasticsearch 和 bleve\n- 可运行<a href=\"https://github.com/go-ego/gse/blob/master/tools/server/server.go\"> JSON RPC 服务</a>\n\n## 算法:\n\n- [词典](https://github.com/go-ego/gse/blob/master/dictionary.go)用双数组 trie（Double-Array Trie）实现，\n- [分词器](https://github.com/go-ego/gse/blob/master/segmenter.go)算法为基于词频的最短路径加动态规划, 以及 DAG 和 HMM 算法分词.\n- 支持 HMM 分词, 使用 viterbi 算法.\n\n## 分词速度:\n\n- <a href=\"https://github.com/go-ego/gse/blob/master/tools/benchmark/benchmark.go\">单线程</a> 9.2MB/s\n- <a href=\"https://github.com/go-ego/gse/blob/master/tools/benchmark/goroutines/goroutines.go\">goroutines 并发</a> 26.8MB/s.\n- HMM 模式单线程分词速度 3.2MB/s.（双核 4 线程 Macbook Pro）。\n\n## Binding:\n\n[gse-bind](https://github.com/vcaesar/gse-bind), binding JavaScript and other, support more language.\n\n## 安装/更新\n\nWith Go module support (Go 1.11+), just import:\n\n```go\nimport \"github.com/go-ego/gse\"\n```\n\nOtherwise, to install the gse package, run the command:\n\n```\ngo get -u github.com/go-ego/gse\n```\n\n## 使用\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"regexp\"\n\n\t\"github.com/go-ego/gse\"\n\t\"github.com/go-ego/gse/hmm/pos\"\n)\n\nvar (\n\tseg gse.Segmenter\n\tposSeg pos.Segmenter\n\n\tnew, _ = gse.New(\"zh,testdata/test_en_dict3.txt\", \"alpha\")\n\n\ttext = \"你好世界, Hello world, Helloworld.\"\n)\n\nfunc main() {\n\t// 加载默认词典\n\tseg.LoadDict()\n\t// 加载默认 embed 词典\n\t// seg.LoadDictEmbed()\n\t//\n\t// 加载简体中文词典\n\t// seg.LoadDict(\"zh_s\")\n\t// seg.LoadDictEmbed(\"zh_s\")\n\t//\n\t// 加载繁体中文词典\n\t// seg.LoadDict(\"zh_t\")\n\t//\n\t// 加载日文词典\n\t// seg.LoadDict(\"jp\")\n\t//\n\t// 载入词典\n\t// seg.LoadDict(\"your gopath\"+\"/src/github.com/go-ego/gse/data/dict/dictionary.txt\")\n\n\tcut()\n\n\tsegCut()\n}\n\n\nfunc cut() {\n\thmm := new.Cut(text, true)\n\tfmt.Println(\"cut use hmm: \", hmm)\n\n\thmm = new.CutSearch(text, true)\n\tfmt.Println(\"cut search use hmm: \", hmm)\n\tfmt.Println(\"analyze: \", new.Analyze(hmm, text))\n\n\thmm = new.CutAll(text)\n\tfmt.Println(\"cut all: \", hmm)\n\n\treg := regexp.MustCompile(`(\\d+年|\\d+月|\\d+日|[\\p{Latin}]+|[\\p{Hangul}]+|\\d+\\.\\d+|[a-zA-Z0-9]+)`)\n\ttext1 := `헬로월드 헬로 서울, 2021年09月10日, 3.14`\n\thmm = seg.CutDAG(text1, reg)\n\tfmt.Println(\"Cut with hmm and regexp: \", hmm, hmm[0], hmm[6])\n}\n\nfunc analyzeAndTrim(cut []string) {\n\ta := seg.Analyze(cut, \"\")\n\tfmt.Println(\"analyze the segment: \", a)\n\n\tcut = seg.Trim(cut)\n\tfmt.Println(\"cut all: \", cut)\n\n\tfmt.Println(seg.String(text, true))\n\tfmt.Println(seg.Slice(text, true))\n}\n\nfunc cutPos() {\n\tpo := seg.Pos(text, true)\n\tfmt.Println(\"pos: \", po)\n\tpo = seg.TrimPos(po)\n\tfmt.Println(\"trim pos: \", po)\n\n\tposSeg.WithGse(seg)\n\tpo = posSeg.Cut(text, true)\n\tfmt.Println(\"pos: \", po)\n\n\tpo = posSeg.TrimWithPos(po, \"zg\")\n\tfmt.Println(\"trim pos: \", po)\n}\n\nfunc segCut() {\n\t// 分词文本\n\ttb := []byte(\"山达尔星联邦共和国联邦政府\")\n\n\t// 处理分词结果\n\tfmt.Println(\"输出分词结果, 类型为字符串, 使用搜索模式: \", seg.String(tb, true))\n\tfmt.Println(\"输出分词结果, 类型为 slice: \", seg.Slice(tb))\n\n\tsegments := seg.Segment(tb)\n\t// 处理分词结果, 普通模式\n\tfmt.Println(gse.ToString(segments))\n\n\tsegments1 := seg.Segment([]byte(text))\n\t// 搜索模式\n\tfmt.Println(gse.ToString(segments1, true))\n}\n\n```\n\n[自定义词典分词示例](/examples/dict/main.go)\n\n```Go\n\npackage main\n\nimport (\n\t\"fmt\"\n\t_ \"embed\"\n\n\t\"github.com/go-ego/gse\"\n)\n\n//go:embed test_en_dict3.txt\nvar testDict string\n\nfunc main() {\n\t// var seg gse.Segmenter\n\t// seg.LoadDict(\"zh, testdata/test_dict.txt, testdata/test_dict1.txt\")\n\t// seg.LoadStop()\n\tseg, err := gse.NewEmbed(\"zh, word 20 n\"+testDict, \"en\")\n\t// seg.LoadDictEmbed()\n\tseg.LoadStopEmbed()\n\n\ttext1 := \"所以, 你好, 再见\"\n\tfmt.Println(seg.Cut(text1, true))\n\tfmt.Println(seg.String(text1, true))\n\n\tsegments := seg.Segment([]byte(text1))\n\tfmt.Println(gse.ToString(segments))\n}\n```\n\n[中文分词示例](/examples/main.go)\n\n[日文分词示例](/examples/jp/main.go)\n\n## Elasticsearch\n\nHow to use it with elasticsearch?\n\n[go-gse-elastic](https://github.com/vcaesar/go-gse-elastic)\n\n## Authors\n\n- [Maintainers](https://github.com/orgs/go-ego/people)\n- [Contributors](https://github.com/go-ego/gse/graphs/contributors)\n\n## License\n\nGse is primarily distributed under the terms of \"both the MIT license and the Apache License (Version 2.0)\".\nSee [LICENSE-APACHE](http://www.apache.org/licenses/LICENSE-2.0), [LICENSE-MIT](https://github.com/go-vgo/robotgo/blob/master/LICENSE).\n\nThanks for [sego](https://github.com/huichen/sego) and [jieba](https://github.com/fxsjy/jieba)([jiebago](https://github.com/wangbin/jiebago)).\n"
        },
        {
          "name": "circle.yml",
          "type": "blob",
          "size": 0.4560546875,
          "content": "version: 2\n\njobs:\n  build:\n    docker:\n      - image: golang:1.23.0\n    working_directory: /gopath/src/github.com/go-ego/gse\n    steps:\n      - checkout\n      # specify any bash command here prefixed with `run: `\n      # - run: go get -u github.com/go-ego/cedar\n      - run: go get -v -t -d ./...\n      - run: go test -v ./...\n      # codecov.io\n      - run: go test -v -covermode=count -coverprofile=coverage.out\n      - run: bash <(curl -s https://codecov.io/bash)\n"
        },
        {
          "name": "crf",
          "type": "tree",
          "content": null
        },
        {
          "name": "dag.go",
          "type": "blob",
          "size": 7.7587890625,
          "content": "// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"bytes\"\n\t\"math\"\n\t\"regexp\"\n\t\"strings\"\n)\n\nconst (\n\t// RatioWord ratio words and letters\n\tRatioWord float32 = 1.5\n\t// RatioWordFull full ratio words and letters\n\tRatioWordFull float32 = 1\n)\n\nvar reEng = regexp.MustCompile(`[[:alnum:]]`)\n\ntype route struct {\n\tfreq  float64\n\tindex int\n}\n\n// Find find word in dictionary return word's freq, pos and existence\nfunc (seg *Segmenter) Find(str string) (float64, string, bool) {\n\treturn seg.Dict.Find([]byte(str))\n}\n\n// Value find word in dictionary return word's value\nfunc (seg *Segmenter) Value(str string) (int, int, error) {\n\treturn seg.Dict.Value([]byte(str))\n}\n\n// FindAllOccs find the all search byte start in data\nfunc FindAllOccs(data []byte, searches []string) map[string][]int {\n\tresults := make(map[string][]int, 0)\n\ttmp := data\n\tfor _, search := range searches {\n\t\tindex := len(data)\n\t\tfor {\n\t\t\tmatch := bytes.LastIndex(tmp[0:index], []byte(search))\n\t\t\tif match == -1 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tindex = match\n\t\t\tresults[search] = append(results[search], match)\n\t\t}\n\t}\n\n\treturn results\n}\n\n// Analyze analyze the token segment info\nfunc (seg *Segmenter) Analyze(text []string, t1 string, by ...bool) (az []AnalyzeToken) {\n\tif len(text) <= 0 {\n\t\treturn\n\t}\n\n\tstart, end := 0, 0\n\tif t1 == \"\" {\n\t\tif len(by) > 0 {\n\t\t\tend = len([]rune(text[0]))\n\t\t} else {\n\t\t\tend = len([]byte(text[0]))\n\t\t}\n\t}\n\n\tisEx := make(map[string]int, 0)\n\tif ToLower {\n\t\tt1 = strings.ToLower(t1)\n\t}\n\tall := FindAllOccs([]byte(t1), text)\n\tfor k, v := range text {\n\t\tif k > 0 && t1 == \"\" {\n\t\t\tstart = az[k-1].End\n\t\t\tif len(by) > 0 {\n\t\t\t\tend = az[k-1].End + len([]rune(v))\n\t\t\t} else {\n\t\t\t\tend = az[k-1].End + len([]byte(v))\n\t\t\t}\n\t\t}\n\n\t\tif t1 != \"\" {\n\t\t\tif _, ok := isEx[v]; ok {\n\t\t\t\tisEx[v]++\n\t\t\t} else {\n\t\t\t\tisEx[v] = 0\n\t\t\t}\n\n\t\t\tif len(all[v]) > 0 {\n\t\t\t\tstart = all[v][isEx[v]]\n\t\t\t}\n\t\t\tend = start + len([]byte(v))\n\t\t}\n\n\t\tfreq, pos, _ := seg.Find(v)\n\t\taz = append(az, AnalyzeToken{\n\t\t\tPosition: k,\n\t\t\tStart:    start,\n\t\t\tEnd:      end,\n\n\t\t\tText: v,\n\t\t\tFreq: freq,\n\t\t\tPos:  pos,\n\t\t})\n\t}\n\n\treturn\n}\n\n// getDag get a directed acyclic graph (DAG) from slice of runes(containing Unicode characters)\nfunc (seg *Segmenter) getDag(runes []rune) map[int][]int {\n\tdag := make(map[int][]int)\n\tn := len(runes)\n\n\tvar (\n\t\tfrag []rune\n\t\ti    int\n\t)\n\n\tfor k := 0; k < n; k++ {\n\t\tdag[k] = make([]int, 0)\n\t\ti = k\n\t\tfrag = runes[k : k+1]\n\n\t\tfor {\n\t\t\tfreq, _, ok := seg.Find(string(frag))\n\t\t\tif !ok {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tif freq > 0 {\n\t\t\t\tdag[k] = append(dag[k], i)\n\t\t\t}\n\n\t\t\ti++\n\t\t\tif i >= n {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tfrag = runes[k : i+1]\n\t\t}\n\n\t\tif len(dag[k]) == 0 {\n\t\t\tdag[k] = append(dag[k], k)\n\t\t}\n\t}\n\n\treturn dag\n}\n\nfunc (seg *Segmenter) calc(runes []rune) map[int]route {\n\tdag := seg.getDag(runes)\n\n\tn := len(runes)\n\trs := make(map[int]route)\n\n\trs[n] = route{freq: 0.0, index: 0}\n\tvar r route\n\n\tlogT := math.Log(seg.Dict.totalFreq)\n\tfor idx := n - 1; idx >= 0; idx-- {\n\t\tfor _, i := range dag[idx] {\n\t\t\tfreq, _, ok := seg.Find(string(runes[idx : i+1]))\n\n\t\t\tif ok {\n\t\t\t\tf := math.Log(freq) - logT + rs[i+1].freq\n\t\t\t\tr = route{freq: f, index: i}\n\t\t\t} else {\n\t\t\t\tf := math.Log(1.0) - logT + rs[i+1].freq\n\t\t\t\tr = route{freq: f, index: i}\n\t\t\t}\n\n\t\t\tif v, ok := rs[idx]; !ok {\n\t\t\t\trs[idx] = r\n\t\t\t} else {\n\t\t\t\tf := v.freq == r.freq && v.index < r.index\n\t\t\t\tif v.freq < r.freq || f {\n\t\t\t\t\trs[idx] = r\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn rs\n}\n\nfunc (seg *Segmenter) hmm(bufString string, buf []rune, reg ...*regexp.Regexp) (result []string) {\n\n\tv, _, ok := seg.Find(bufString)\n\tif !ok || v == 0 {\n\t\tresult = append(result, seg.HMMCut(bufString, reg...)...)\n\t\treturn\n\t}\n\n\tfor _, elem := range buf {\n\t\tresult = append(result, string(elem))\n\t}\n\treturn\n}\n\nfunc (seg *Segmenter) cutDAG(str string, reg ...*regexp.Regexp) []string {\n\n\tmLen := int(float32(len(str))/RatioWord) + 1\n\tresult := make([]string, 0, mLen)\n\n\tif ToLower {\n\t\tstr = strings.ToLower(str)\n\t}\n\trunes := []rune(str)\n\troutes := seg.calc(runes)\n\n\tvar y int\n\tlength := len(runes)\n\tvar buf []rune\n\n\tfor x := 0; x < length; {\n\t\ty = routes[x].index + 1\n\t\tfrag := runes[x:y]\n\n\t\tif y-x == 1 {\n\t\t\tbuf = append(buf, frag...)\n\t\t} else {\n\t\t\tif len(buf) > 0 {\n\t\t\t\tbufString := string(buf)\n\t\t\t\tif len(buf) == 1 {\n\t\t\t\t\tresult = append(result, bufString)\n\t\t\t\t} else {\n\t\t\t\t\tresult = append(result, seg.hmm(bufString, buf, reg...)...)\n\t\t\t\t}\n\n\t\t\t\tbuf = make([]rune, 0)\n\t\t\t}\n\n\t\t\tresult = append(result, string(frag))\n\t\t}\n\n\t\tx = y\n\t}\n\n\tif len(buf) > 0 {\n\t\tbufString := string(buf)\n\n\t\tif len(buf) == 1 {\n\t\t\tresult = append(result, bufString)\n\t\t} else {\n\t\t\tresult = append(result, seg.hmm(bufString, buf, reg...)...)\n\t\t}\n\t}\n\n\treturn result\n}\n\nfunc (seg *Segmenter) cutDAGNoHMM(str string) []string {\n\tmLen := int(float32(len(str))/RatioWord) + 1\n\tresult := make([]string, 0, mLen)\n\n\tif ToLower {\n\t\tstr = strings.ToLower(str)\n\t}\n\trunes := []rune(str)\n\troutes := seg.calc(runes)\n\tlength := len(runes)\n\n\tvar y int\n\tvar buf []rune\n\n\tfor x := 0; x < length; {\n\t\ty = routes[x].index + 1\n\t\tfrag := runes[x:y]\n\n\t\tif reEng.MatchString(string(frag)) && len(frag) == 1 {\n\t\t\tbuf = append(buf, frag...)\n\t\t\tx = y\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(buf) > 0 {\n\t\t\tresult = append(result, string(buf))\n\t\t\tbuf = make([]rune, 0)\n\t\t}\n\n\t\tresult = append(result, string(frag))\n\t\tx = y\n\t}\n\n\tif len(buf) > 0 {\n\t\tresult = append(result, string(buf))\n\t\t// buf = make([]rune, 0)\n\t}\n\n\treturn result\n}\n\nfunc (seg *Segmenter) cutAll(str string) []string {\n\tmLen := int(float32(len(str))/RatioWord) + 1\n\tresult := make([]string, 0, mLen)\n\n\tif ToLower {\n\t\tstr = strings.ToLower(str)\n\t}\n\trunes := []rune(str)\n\tdag := seg.getDag(runes)\n\tstart := -1\n\tks := make([]int, len(dag))\n\n\tfor k := range dag {\n\t\tks[k] = k\n\t}\n\n\tvar l []int\n\tfor k := range ks {\n\t\tl = dag[k]\n\n\t\tif len(l) == 1 && k > start {\n\t\t\tresult = append(result, string(runes[k:l[0]+1]))\n\t\t\tstart = l[0]\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, j := range l {\n\t\t\tif j > k {\n\t\t\t\tresult = append(result, string(runes[k:j+1]))\n\t\t\t\tstart = j\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result\n}\n\nfunc (seg *Segmenter) cutForSearch(str string, hmm ...bool) []string {\n\n\tmLen := int(float32(len(str))/RatioWordFull) + 1\n\tresult := make([]string, 0, mLen)\n\n\tws := seg.Cut(str, hmm...)\n\tfor _, word := range ws {\n\t\trunes := []rune(word)\n\t\tfor _, incr := range []int{2, 3} {\n\t\t\tif len(runes) <= incr {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar gram string\n\t\t\tfor i := 0; i < len(runes)-incr+1; i++ {\n\t\t\t\tgram = string(runes[i : i+incr])\n\t\t\t\tv, _, ok := seg.Find(gram)\n\t\t\t\tif ok && v > 0 {\n\t\t\t\t\tresult = append(result, gram)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tresult = append(result, word)\n\t}\n\n\treturn result\n}\n\n// SuggestFreq suggest the words frequency\n// return a suggested frequency of a word cutted to short words.\nfunc (seg *Segmenter) SuggestFreq(words ...string) float64 {\n\tfreq := 1.0\n\ttotal := seg.Dict.totalFreq\n\n\tif len(words) > 1 {\n\t\tfor _, word := range words {\n\t\t\tv, _, ok := seg.Find(word)\n\t\t\tif ok {\n\t\t\t\tfreq *= v\n\t\t\t}\n\n\t\t\tfreq /= total\n\t\t}\n\n\t\tfreq, _ = math.Modf(freq * total)\n\t\twordFreq := 0.0\n\t\tv, _, ok := seg.Find(strings.Join(words, \"\"))\n\t\tif ok {\n\t\t\twordFreq = v\n\t\t}\n\n\t\tif wordFreq < freq {\n\t\t\tfreq = wordFreq\n\t\t}\n\n\t\treturn freq\n\t}\n\n\tword := words[0]\n\tfor _, segment := range seg.Cut(word, false) {\n\t\tv, _, ok := seg.Find(segment)\n\t\tif ok {\n\t\t\tfreq *= v\n\t\t}\n\n\t\tfreq /= total\n\t}\n\n\tfreq, _ = math.Modf(freq * total)\n\tfreq += 1.0\n\twordFreq := 1.0\n\n\tv, _, ok := seg.Find(word)\n\tif ok {\n\t\twordFreq = v\n\t}\n\n\tif wordFreq > freq {\n\t\tfreq = wordFreq\n\t}\n\n\treturn freq\n}\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dict_1.16.go",
          "type": "blob",
          "size": 3.5078125,
          "content": "// Copyright 2016 The go-ego Project Developers. See the COPYRIGHT\n// file at the top-level directory of this distribution and at\n// https://github.com/go-ego/gse/blob/master/LICENSE\n//\n// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n// option. This file may not be copied, modified, or distributed\n// except according to those terms.\n\npackage gse\n\nimport (\n\t\"strings\"\n)\n\n// //go:embed data/dict/dictionary.txt\n// var dataDict string\n\n// NewEmbed return new gse segmenter by embed dictionary\nfunc NewEmbed(dict ...string) (seg Segmenter, err error) {\n\tif len(dict) > 1 && (dict[1] == \"alpha\" || dict[1] == \"en\") {\n\t\tseg.AlphaNum = true\n\t}\n\n\terr = seg.LoadDictEmbed(dict...)\n\treturn\n}\n\nfunc (seg *Segmenter) loadZh() error {\n\treturn seg.LoadDictStr(zhS + zhT)\n}\n\nfunc (seg *Segmenter) loadZhST(d string) (begin int, err error) {\n\tif strings.Contains(d, \"zh,\") {\n\t\tbegin = 1\n\t\t// err = seg.LoadDictStr(dataDict)\n\t\terr = seg.loadZh()\n\t}\n\n\tif strings.Contains(d, \"zh_s,\") {\n\t\tbegin = 1\n\t\terr = seg.LoadDictStr(zhS)\n\t}\n\tif strings.Contains(d, \"zh_t,\") {\n\t\tbegin = 1\n\t\terr = seg.LoadDictStr(zhT)\n\t}\n\n\treturn\n}\n\n// LoadDictEmbed load the dictionary by embed file\nfunc (seg *Segmenter) LoadDictEmbed(dict ...string) (err error) {\n\tif len(dict) > 0 {\n\t\td := dict[0]\n\t\tif d == \"ja\" {\n\t\t\treturn seg.LoadDictStr(ja)\n\t\t}\n\n\t\tif d == \"zh\" {\n\t\t\treturn seg.loadZh()\n\t\t}\n\t\tif d == \"zh_s\" {\n\t\t\treturn seg.LoadDictStr(zhS)\n\t\t}\n\t\tif d == \"zh_t\" {\n\t\t\treturn seg.LoadDictStr(zhT)\n\t\t}\n\n\t\tif strings.Contains(d, \", \") && seg.DictSep != \",\" {\n\t\t\tbegin := 0\n\t\t\ts := strings.Split(d, \", \")\n\t\t\tbegin, err = seg.loadZhST(d)\n\n\t\t\tfor i := begin; i < len(s); i++ {\n\t\t\t\terr = seg.LoadDictStr(s[i])\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\terr = seg.LoadDictStr(d)\n\t\treturn\n\t}\n\n\t// return seg.LoadDictStr(dataDict)\n\treturn seg.loadZh()\n}\n\n// LoadDictStr load the dictionary from dict path\nfunc (seg *Segmenter) LoadDictStr(dict string) error {\n\tif seg.Dict == nil {\n\t\tseg.Dict = NewDict()\n\t\tseg.Init()\n\t}\n\n\tarr := strings.Split(dict, \"\\n\")\n\tfor i := 0; i < len(arr); i++ {\n\t\ts1 := strings.Split(arr[i], seg.DictSep+\" \")\n\t\tsize := len(s1)\n\t\tif size == 0 {\n\t\t\tcontinue\n\t\t}\n\t\ttext := strings.TrimSpace(s1[0])\n\n\t\tfreqText := \"\"\n\t\tif len(s1) > 1 {\n\t\t\tfreqText = strings.TrimSpace(s1[1])\n\t\t}\n\n\t\tfreq := seg.Size(size, text, freqText)\n\t\tif freq == 0.0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tpos := \"\"\n\t\tif size > 2 {\n\t\t\tpos = strings.TrimSpace(strings.Trim(s1[2], \"\\n\"))\n\t\t}\n\n\t\t// add the words to the token\n\t\twords := seg.SplitTextToWords([]byte(text))\n\t\ttoken := Token{text: words, freq: freq, pos: pos}\n\t\tseg.Dict.AddToken(token)\n\t}\n\n\tseg.CalcToken()\n\treturn nil\n}\n\n// LoadStopEmbed load the stop dictionary from embed file\nfunc (seg *Segmenter) LoadStopEmbed(dict ...string) (err error) {\n\tif len(dict) > 0 {\n\t\td := dict[0]\n\t\tif strings.Contains(d, \", \") {\n\t\t\tbegin := 0\n\t\t\ts := strings.Split(d, \", \")\n\t\t\tif strings.Contains(d, \"zh,\") {\n\t\t\t\tbegin = 1\n\t\t\t\terr = seg.LoadStopStr(stopDict)\n\t\t\t}\n\n\t\t\tfor i := begin; i < len(s); i++ {\n\t\t\t\terr = seg.LoadStopStr(s[i])\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\terr = seg.LoadStopStr(d)\n\t\treturn\n\t}\n\n\treturn seg.LoadStopStr(stopDict)\n}\n\n// LoadStopStr load the stop dictionary from dict path\nfunc (seg *Segmenter) LoadStopStr(dict string) error {\n\tif seg.StopWordMap == nil {\n\t\tseg.StopWordMap = make(map[string]bool)\n\t}\n\n\tarr := strings.Split(dict, \"\\n\")\n\tfor i := 0; i < len(arr); i++ {\n\t\tkey := strings.TrimSpace(arr[i])\n\t\tif key != \"\" {\n\t\t\tseg.StopWordMap[key] = true\n\t\t}\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "dict_1.16_test.go",
          "type": "blob",
          "size": 1.8701171875,
          "content": "//go:build go1.16\n// +build go1.16\n\npackage gse\n\nimport (\n\t_ \"embed\"\n\t\"testing\"\n\n\t\"github.com/vcaesar/tt\"\n)\n\n//go:embed testdata/test_en_dict3.txt\nvar testDict string\n\n//go:embed testdata/test_en.txt\nvar testEn string\n\n//go:embed testdata/zh/test_zh_dict2.txt\nvar testDict2 string\n\n//go:embed testdata/stop.txt\nvar testStop string\n\nfunc TestLoadDictEmbed(t *testing.T) {\n\tvar seg2 Segmenter\n\terr := seg2.LoadDictEmbed(testDict)\n\ttt.Nil(t, err)\n\n\tseg1, err := NewEmbed(\"zh, word1 20 n, \"+testDict+\", \"+testDict2, \"en\")\n\ttt.Nil(t, err)\n\n\tf, pos, ok := seg1.Find(\"1号店\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"n\", pos)\n\ttt.Equal(t, 3, f)\n\n\tf, pos, ok = seg1.Find(\"hello\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"\", pos)\n\ttt.Equal(t, 20, f)\n\n\tf, pos, ok = seg1.Find(\"world\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"n\", pos)\n\ttt.Equal(t, 20, f)\n\n\tf, pos, ok = seg1.Find(\"word1\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"n\", pos)\n\ttt.Equal(t, 20, f)\n\n\tf, pos, ok = seg1.Find(\"新星共和国\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"ns\", pos)\n\ttt.Equal(t, 32, f)\n\n\tf, _, ok = seg1.Find(\"八千一百三十七万七千二百三十六口\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, 2, f)\n}\n\nfunc TestLoadDictSTEmbed(t *testing.T) {\n\tvar seg1 Segmenter\n\terr := seg1.LoadDictEmbed(\"zh_s\")\n\ttt.Nil(t, err)\n\ttt.Equal(t, 352275, len(seg1.Dict.Tokens))\n\ttt.Equal(t, 3.3335153e+07, seg1.Dict.totalFreq)\n\n\terr = seg1.LoadDictEmbed(\"zh_t, word1 20 n, \" + testDict)\n\ttt.Nil(t, err)\n\ttt.Equal(t, 587211, len(seg1.Dict.Tokens))\n\ttt.Equal(t, 5.3226834e+07, seg1.Dict.totalFreq)\n}\n\nfunc TestLoadStopEmbed(t *testing.T) {\n\tvar seg1 Segmenter\n\terr := seg1.LoadStopEmbed(\"zh, \" + testStop)\n\ttt.Nil(t, err)\n\ttt.Bool(t, seg1.IsStop(\"比如\"))\n\ttt.Bool(t, seg1.IsStop(\"离开\"))\n}\n\nfunc TestDictSep(t *testing.T) {\n\tvar seg1 Segmenter\n\tseg1.DictSep = \",\"\n\terr := seg1.LoadDictEmbed(testEn)\n\ttt.Nil(t, err)\n\n\tf, pos, ok := seg1.Find(\"to be\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"x\", pos)\n\ttt.Equal(t, 10, f)\n}\n"
        },
        {
          "name": "dict_embed.go",
          "type": "blob",
          "size": 0.328125,
          "content": "//go:build go1.16 && !ne\n// +build go1.16,!ne\n\npackage gse\n\nimport (\n\t_ \"embed\"\n)\n\nvar (\n\t//go:embed data/dict/jp/dict.txt\n\tja string\n\n\t//go:embed data/dict/zh/t_1.txt\n\tzhT string\n\t//go:embed data/dict/zh/s_1.txt\n\tzhS string\n\n\t//go:embed data/dict/zh/idf.txt\n\tZhIdf string\n)\n\n//go:embed data/dict/zh/stop_tokens.txt\nvar stopDict string\n"
        },
        {
          "name": "dict_no_embed.go",
          "type": "blob",
          "size": 0.0888671875,
          "content": "//go:build ne\n// +build ne\n\npackage gse\n\nvar (\n\tja, zhT, zhS string\n\tstopDict     string\n)\n"
        },
        {
          "name": "dict_util.go",
          "type": "blob",
          "size": 10.8359375,
          "content": "// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"math\"\n\t\"os\"\n\t\"path\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n)\n\nvar (\n\t// ToLower set alpha to lowercase\n\tToLower = true\n)\n\nconst (\n\tzhS1 = \"dict/zh/s_1.txt\"\n\tzhT1 = \"dict/zh/t_1.txt\"\n)\n\n// Init initializes the segmenter config\nfunc (seg *Segmenter) Init() {\n\tif seg.MinTokenFreq == 0 {\n\t\tseg.MinTokenFreq = 2.0\n\t}\n\n\tif seg.TextFreq == \"\" {\n\t\tseg.TextFreq = \"2.0\"\n\t}\n\n\t// init the model of hmm cut\n\tif !seg.NotLoadHMM {\n\t\tseg.LoadModel()\n\t}\n}\n\n// Dictionary returns the dictionary used by the tokenizer\nfunc (seg *Segmenter) Dictionary() *Dictionary {\n\treturn seg.Dict\n}\n\n// ToToken make the text, freq and pos to token structure\nfunc (seg *Segmenter) ToToken(text string, freq float64, pos ...string) Token {\n\tvar po string\n\tif len(pos) > 0 {\n\t\tpo = pos[0]\n\t}\n\n\twords := seg.SplitTextToWords([]byte(text))\n\ttoken := Token{text: words, freq: freq, pos: po}\n\treturn token\n}\n\n// AddToken add a new text to the token\nfunc (seg *Segmenter) AddToken(text string, freq float64, pos ...string) error {\n\ttoken := seg.ToToken(text, freq, pos...)\n\treturn seg.Dict.AddToken(token)\n}\n\n// AddTokenForce add new text to token and force\n// time-consuming\nfunc (seg *Segmenter) AddTokenForce(text string, freq float64, pos ...string) (err error) {\n\terr = seg.AddToken(text, freq, pos...)\n\tseg.CalcToken()\n\treturn\n}\n\n// ReAddToken remove and add token again\nfunc (seg *Segmenter) ReAddToken(text string, freq float64, pos ...string) error {\n\ttoken := seg.ToToken(text, freq, pos...)\n\terr := seg.Dict.RemoveToken(token)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn seg.Dict.AddToken(token)\n}\n\n// RemoveToken remove token in dictionary\nfunc (seg *Segmenter) RemoveToken(text string) error {\n\twords := seg.SplitTextToWords([]byte(text))\n\ttoken := Token{text: words}\n\n\treturn seg.Dict.RemoveToken(token)\n}\n\n// Empty empty the seg dictionary\nfunc (seg *Segmenter) Empty() error {\n\tseg.Dict = nil\n\treturn nil\n}\n\n// LoadDictMap load dictionary from []map[string]string\nfunc (seg *Segmenter) LoadDictMap(dict []map[string]string) error {\n\tif seg.Dict == nil {\n\t\tseg.Dict = NewDict()\n\t\tseg.Init()\n\t}\n\n\tfor _, d := range dict {\n\t\t// Parse the word frequency\n\t\tfreq := seg.Size(len(d), d[\"text\"], d[\"freq\"])\n\t\tif freq == 0.0 {\n\t\t\tcontinue\n\t\t}\n\n\t\twords := seg.SplitTextToWords([]byte(d[\"text\"]))\n\t\ttoken := Token{text: words, freq: freq, pos: d[\"pos\"]}\n\t\tseg.Dict.AddToken(token)\n\t}\n\n\tseg.CalcToken()\n\treturn nil\n}\n\n// LoadDict load the dictionary from the file\n//\n// The format of the dictionary is (one for each participle):\n//\n//\tparticiple text, frequency, part of speech\n//\n// # And you can option the dictionary separator by seg.DictSep = \",\"\n//\n// Can load multiple dictionary files, the file name separated by \",\" or \", \"\n// the front of the dictionary preferentially load the participle,\n//\n//\tsuch as: \"user_dictionary.txt,common_dictionary.txt\"\n//\n// When a participle appears both in the user dictionary and\n// in the `common dictionary`, the `user dictionary` is given priority.\nfunc (seg *Segmenter) LoadDict(files ...string) error {\n\tif !seg.Load {\n\t\tseg.Dict = NewDict()\n\t\tseg.Load = true\n\t\tseg.Init()\n\t}\n\n\tvar (\n\t\tdictDir  = path.Join(path.Dir(seg.GetCurrentFilePath()), \"data\")\n\t\tdictPath string\n\t\t// load     bool\n\t)\n\n\tif len(files) > 0 {\n\t\tdictFiles := DictPaths(dictDir, files[0])\n\t\tif !seg.SkipLog {\n\t\t\tlog.Println(\"Dict files path: \", dictFiles)\n\t\t}\n\n\t\tif len(dictFiles) == 0 {\n\t\t\tlog.Println(\"Warning: dict files is nil.\")\n\t\t\t// return errors.New(\"Dict files is nil.\")\n\t\t}\n\n\t\tif len(dictFiles) > 0 {\n\t\t\t// load = true\n\t\t\t// files = dictFiles\n\t\t\tfor i := 0; i < len(dictFiles); i++ {\n\t\t\t\terr := seg.Read(dictFiles[i])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(files) == 0 {\n\t\tdictPath = path.Join(dictDir, zhS1)\n\t\tpath1 := path.Join(dictDir, zhT1)\n\t\t// files = []string{dictPath}\n\t\terr := seg.Read(dictPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = seg.Read(path1)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if files[0] != \"\" && files[0] != \"en\" && !load {\n\t// \tfor _, file := range strings.Split(files[0], \",\") {\n\t// \t\t// for _, file := range files {\n\t// \t\terr := seg.Read(file)\n\t// \t\tif err != nil {\n\t// \t\t\treturn err\n\t// \t\t}\n\t// \t}\n\t// }\n\n\tseg.CalcToken()\n\tif !seg.SkipLog {\n\t\tlog.Println(\"Gse dictionary loaded finished.\")\n\t}\n\n\treturn nil\n}\n\n// GetCurrentFilePath get the current file path\nfunc (seg *Segmenter) GetCurrentFilePath() string {\n\tif seg.DictPath != \"\" {\n\t\treturn seg.DictPath\n\t}\n\n\t_, filePath, _, _ := runtime.Caller(1)\n\treturn filePath\n}\n\n// GetIdfPath get the idf path\nfunc (seg *Segmenter) GetIdfPath(files ...string) []string {\n\tvar (\n\t\tdictDir  = path.Join(path.Dir(seg.GetCurrentFilePath()), \"data\")\n\t\tdictPath = path.Join(dictDir, \"dict/zh/idf.txt\")\n\t)\n\n\tfiles = append(files, dictPath)\n\n\treturn files\n}\n\n// Read read the dict file\nfunc (seg *Segmenter) Read(file string) error {\n\tif !seg.SkipLog {\n\t\tlog.Printf(\"Load the gse dictionary: \\\"%s\\\" \", file)\n\t}\n\n\tdictFile, err := os.Open(file)\n\tif err != nil {\n\t\tlog.Printf(\"Could not load dictionaries: \\\"%s\\\", %v \\n\", file, err)\n\t\treturn err\n\t}\n\tdefer dictFile.Close()\n\n\treader := bufio.NewReader(dictFile)\n\treturn seg.Reader(reader, file)\n}\n\n// Size frequency is calculated based on the size of the text\nfunc (seg *Segmenter) Size(size int, text, freqText string) (freq float64) {\n\tif size == 0 {\n\t\t// End of file or error line\n\t\t// continue\n\t\treturn\n\t}\n\n\tif size < 2 {\n\t\tif !seg.LoadNoFreq {\n\t\t\t// invalid row line\n\t\t\treturn\n\t\t}\n\n\t\tfreqText = seg.TextFreq\n\t}\n\n\t// Analyze the word frequency\n\tvar err error\n\tfreq, err = strconv.ParseFloat(freqText, 64)\n\tif err != nil {\n\t\t// continue\n\t\treturn\n\t}\n\n\t// Filter out the words that are too infrequent\n\tif freq < seg.MinTokenFreq {\n\t\treturn 0.0\n\t}\n\n\t// Filter words with a length of 1 to reduce the word frequency\n\tif len([]rune(text)) < 2 {\n\t\tfreq = 2\n\t}\n\n\treturn\n}\n\n// ReadN read the tokens by '\\n'\nfunc (seg *Segmenter) ReadN(reader *bufio.Reader) (size int,\n\ttext, freqText, pos string, fsErr error) {\n\tvar txt string\n\ttxt, fsErr = reader.ReadString('\\n')\n\n\tparts := strings.Split(txt, seg.DictSep+\" \")\n\tsize = len(parts)\n\n\ttext = parts[0]\n\tif size > 1 {\n\t\tfreqText = strings.TrimSpace(parts[1])\n\t}\n\tif size > 2 {\n\t\tpos = strings.TrimSpace(strings.Trim(parts[2], \"\\n\"))\n\t}\n\n\treturn\n}\n\n// Reader load dictionary from io.Reader\nfunc (seg *Segmenter) Reader(reader *bufio.Reader, files ...string) error {\n\tvar (\n\t\tfile           string\n\t\ttext, freqText string\n\t\tfreq           float64\n\t\tpos            string\n\t)\n\n\tif len(files) > 0 {\n\t\tfile = files[0]\n\t}\n\n\t// Read the word segmentation line by line\n\tline := 0\n\tfor {\n\t\tline++\n\t\tvar (\n\t\t\tsize  int\n\t\t\tfsErr error\n\t\t)\n\t\tif seg.DictSep == \"\" {\n\t\t\tsize, fsErr = fmt.Fscanln(reader, &text, &freqText, &pos)\n\t\t} else {\n\t\t\tsize, text, freqText, pos, fsErr = seg.ReadN(reader)\n\t\t}\n\n\t\tif fsErr != nil {\n\t\t\tif fsErr == io.EOF {\n\t\t\t\t// End of file\n\t\t\t\tif seg.DictSep == \"\" {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tif seg.DictSep != \"\" && text == \"\" {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif size > 0 {\n\t\t\t\tif seg.MoreLog {\n\t\t\t\t\tlog.Printf(\"File '%v' line \\\"%v\\\" read error: %v, skip\",\n\t\t\t\t\t\tfile, line, fsErr.Error())\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tlog.Printf(\"File '%v' line \\\"%v\\\" is empty, read error: %v, skip\",\n\t\t\t\t\tfile, line, fsErr.Error())\n\t\t\t}\n\t\t}\n\n\t\tfreq = seg.Size(size, text, freqText)\n\t\tif freq == 0.0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif size == 2 {\n\t\t\t// No part of speech, marked as an empty string\n\t\t\tpos = \"\"\n\t\t}\n\n\t\t// Add participle tokens to the dictionary\n\t\twords := seg.SplitTextToWords([]byte(text))\n\t\ttoken := Token{text: words, freq: freq, pos: pos}\n\t\tseg.Dict.AddToken(token)\n\t}\n\n\treturn nil\n}\n\n// DictPaths get the dict's paths\nfunc DictPaths(dictDir, filePath string) (files []string) {\n\tvar dictPath string\n\n\tif filePath == \"en\" {\n\t\treturn\n\t}\n\n\tvar fileName []string\n\tif strings.Contains(filePath, \", \") {\n\t\tfileName = strings.Split(filePath, \", \")\n\t} else {\n\t\tfileName = strings.Split(filePath, \",\")\n\t}\n\n\tfor i := 0; i < len(fileName); i++ {\n\t\tif fileName[i] == \"ja\" || fileName[i] == \"jp\" {\n\t\t\tdictPath = path.Join(dictDir, \"dict/jp/dict.txt\")\n\t\t}\n\n\t\tif fileName[i] == \"zh\" {\n\t\t\tdictPath = path.Join(dictDir, zhS1)\n\t\t\tpath1 := path.Join(dictDir, zhT1)\n\t\t\tfiles = append(files, path1)\n\t\t}\n\n\t\tif fileName[i] == \"zh_s\" {\n\t\t\tdictPath = path.Join(dictDir, zhS1)\n\t\t}\n\n\t\tif fileName[i] == \"zh_t\" {\n\t\t\tdictPath = path.Join(dictDir, zhT1)\n\t\t}\n\n\t\t// if str[i] == \"ti\" {\n\t\t// }\n\n\t\tdictName := fileName[i] != \"en\" &&\n\t\t\tfileName[i] != \"zh\" &&\n\t\t\tfileName[i] != \"zh_s\" && fileName[i] != \"zh_t\" &&\n\t\t\tfileName[i] != \"ja\" && fileName[i] != \"jp\" &&\n\t\t\tfileName[i] != \"ko\" && fileName[i] != \"ti\"\n\n\t\tif dictName {\n\t\t\tdictPath = fileName[i]\n\t\t}\n\n\t\tif dictPath != \"\" && dictPath != \" \" {\n\t\t\tfiles = append(files, dictPath)\n\t\t}\n\t}\n\t// }\n\n\treturn\n}\n\n// IsJp is Japan char return true\nfunc IsJp(segText string) bool {\n\tfor _, r := range segText {\n\t\tjp := unicode.Is(unicode.Scripts[\"Hiragana\"], r) ||\n\t\t\tunicode.Is(unicode.Scripts[\"Katakana\"], r)\n\t\tif jp {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// CalcToken calc the segmenter token\nfunc (seg *Segmenter) CalcToken() {\n\t// Calculate the path value of each word segment.\n\t// For the meaning of the path value, see the notes of the Token structure\n\tlogTotalFreq := float32(math.Log2(seg.Dict.totalFreq))\n\tfor i := range seg.Dict.Tokens {\n\t\ttoken := &seg.Dict.Tokens[i]\n\t\ttoken.distance = logTotalFreq - float32(math.Log2(token.freq))\n\t}\n\n\t// Each word segmentation is carefully divided for search engine mode,\n\t// For the usage of this mode, see the comments of the Token structure.\n\tfor i := range seg.Dict.Tokens {\n\t\ttoken := &seg.Dict.Tokens[i]\n\t\tsegments := seg.segmentWords(token.text, true)\n\n\t\t// Calculate the number of sub-segments that need to be added\n\t\tnumTokensToAdd := 0\n\t\tfor iToken := 0; iToken < len(segments); iToken++ {\n\t\t\tif len(segments[iToken].token.text) > 0 {\n\t\t\t\thasJp := false\n\t\t\t\tif len(segments[iToken].token.text) == 1 {\n\t\t\t\t\tsegText := string(segments[iToken].token.text[0])\n\t\t\t\t\thasJp = IsJp(segText)\n\t\t\t\t}\n\n\t\t\t\tif !hasJp {\n\t\t\t\t\tnumTokensToAdd++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttoken.segments = make([]*Segment, numTokensToAdd)\n\n\t\t// add sub-segments subparticiple\n\t\tiSegmentsToAdd := 0\n\t\tfor iToken := 0; iToken < len(segments); iToken++ {\n\t\t\tif len(segments[iToken].token.text) > 0 {\n\t\t\t\thasJp := false\n\t\t\t\tif len(segments[iToken].token.text) == 1 {\n\t\t\t\t\tsegText := string(segments[iToken].token.text[0])\n\t\t\t\t\thasJp = IsJp(segText)\n\t\t\t\t}\n\n\t\t\t\tif !hasJp {\n\t\t\t\t\ttoken.segments[iSegmentsToAdd] = &segments[iToken]\n\t\t\t\t\tiSegmentsToAdd++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "dictionary.go",
          "type": "blob",
          "size": 3.2841796875,
          "content": "// Copyright 2013 Hui Chen\n// Copyright 2016 ego authors\n//\n// Copyright 2016 The go-ego Project Developers. See the COPYRIGHT\n// file at the top-level directory of this distribution and at\n// https://github.com/go-ego/gse/blob/master/LICENSE\n//\n// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n// option. This file may not be copied, modified, or distributed\n// except according to those terms.\n\npackage gse\n\nimport (\n\t\"github.com/vcaesar/cedar\"\n)\n\n// Dictionary struct implements a string double array trie.\n// one segment maybe in leaf node or not\ntype Dictionary struct {\n\ttrie *cedar.Cedar // Cedar double array trie\n\n\tmaxTokenLen int     // the maximum length of the dictionary\n\tTokens      []Token // the all tokens in the dictionary, to traverse\n\ttotalFreq   float64 // the total number of tokens in the dictionary\n}\n\n// NewDict a new dictionary trie\nfunc NewDict() *Dictionary {\n\treturn &Dictionary{trie: cedar.New()}\n}\n\n// MaxTokenLen the maximum length of the dictionary\nfunc (dict *Dictionary) MaxTokenLen() int {\n\treturn dict.maxTokenLen\n}\n\n// NumTokens the number of tokens in the dictionary\nfunc (dict *Dictionary) NumTokens() int {\n\treturn len(dict.Tokens)\n}\n\n// TotalFreq the total frequency of the dictionary\nfunc (dict *Dictionary) TotalFreq() float64 {\n\treturn dict.totalFreq\n}\n\n// AddToken add a token to the dictionary\nfunc (dict *Dictionary) AddToken(token Token) error {\n\tbytes := textSliceToBytes(token.text)\n\tval, err := dict.trie.Get(bytes)\n\tif err == nil || val > 0 {\n\t\treturn nil\n\t}\n\n\terr = dict.trie.Insert(bytes, dict.NumTokens())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdict.Tokens = append(dict.Tokens, token)\n\tdict.totalFreq += token.freq\n\n\tif len(token.text) > dict.maxTokenLen {\n\t\tdict.maxTokenLen = len(token.text)\n\t}\n\n\treturn nil\n}\n\n// RemoveToken remove token in dictionary\nfunc (dict *Dictionary) RemoveToken(token Token) error {\n\tbytes := textSliceToBytes(token.text)\n\n\treturn dict.trie.Delete(bytes)\n}\n\n// LookupTokens finds tokens and words in the dictionary, matching the given pattern\n// and returns the number of tokens\nfunc (dict *Dictionary) LookupTokens(\n\twords []Text, tokens []*Token) (numOfTokens int) {\n\tvar (\n\t\tid, value int\n\t\terr       error\n\t)\n\n\tfor _, word := range words {\n\t\tid, err = dict.trie.Jump(word, id)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\tvalue, err = dict.trie.Value(id)\n\t\tif err == nil {\n\t\t\ttokens[numOfTokens] = &dict.Tokens[value]\n\t\t\tnumOfTokens++\n\t\t}\n\t}\n\n\treturn\n}\n\n// Find find the word in the dictionary is non-existent\n// and the word's frequency and pos\nfunc (dict *Dictionary) Find(word []byte) (float64, string, bool) {\n\tvar (\n\t\tid, value int\n\t\tfreq      float64\n\t\terr       error\n\t)\n\n\tid, err = dict.trie.Jump(word, id)\n\tif err != nil {\n\t\treturn 0, \"\", false\n\t}\n\n\tvalue, err = dict.trie.Value(id)\n\tif err != nil && id != 0 {\n\t\treturn 0, \"\", true\n\t}\n\n\tif err != nil {\n\t\treturn 0, \"\", false\n\t}\n\n\tfreq = dict.Tokens[value].freq\n\tpos := dict.Tokens[value].pos\n\treturn freq, pos, true\n}\n\n// Value find word in the dictionary\n// return the word's value and id\nfunc (dict *Dictionary) Value(word []byte) (val, id int, err error) {\n\tid, err = dict.trie.Jump(word, id)\n\tif err != nil {\n\t\treturn 0, id, err\n\t}\n\n\tval, err = dict.trie.Value(id)\n\treturn\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.11328125,
          "content": "module github.com/go-ego/gse\n\ngo 1.17\n\nrequire (\n\tgithub.com/vcaesar/cedar v0.20.2\n\tgithub.com/vcaesar/tt v0.20.1\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 0.32421875,
          "content": "github.com/vcaesar/cedar v0.20.2 h1:TDx7AdZhilKcfE1WvdToTJf5VrC/FXcUOW+KY1upLZ4=\ngithub.com/vcaesar/cedar v0.20.2/go.mod h1:lyuGvALuZZDPNXwpzv/9LyxW+8Y6faN7zauFezNsnik=\ngithub.com/vcaesar/tt v0.20.1 h1:D/jUeeVCNbq3ad8M7hhtB3J9x5RZ6I1n1eZ0BJp7M+4=\ngithub.com/vcaesar/tt v0.20.1/go.mod h1:cH2+AwGAJm19Wa6xvEa+0r+sXDJBT0QgNQey6mwqLeU=\n"
        },
        {
          "name": "gonn",
          "type": "tree",
          "content": null
        },
        {
          "name": "gse.go",
          "type": "blob",
          "size": 4.3720703125,
          "content": "// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\n/*\nPackage gse Go efficient multilingual NLP and text segmentation\n*/\npackage gse\n\nimport (\n\t\"regexp\"\n\n\t\"github.com/go-ego/gse/hmm\"\n)\n\nconst (\n\t// Version get the gse version\n\tVersion = \"v0.80.2.705, Green Lake!\"\n\n\t// minTokenFrequency = 2 // only read tokens with frequency >= 2 from the dictionary\n)\n\n// GetVersion get the version of gse\nfunc GetVersion() string {\n\treturn Version\n}\n\n// Prob define the hmm model struct\ntype Prob struct {\n\tB, E, M, S map[rune]float64\n}\n\n// New return a new gse segmenter\nfunc New(files ...string) (seg Segmenter, err error) {\n\tif len(files) > 1 && files[1] == \"alpha\" {\n\t\tseg.AlphaNum = true\n\t}\n\n\terr = seg.LoadDict(files...)\n\treturn\n}\n\n// Cut cuts a str into words using accurate mode.\n// Parameter hmm controls whether to use the HMM(Hidden Markov Model)\n// or use the user's model.\n//\n// seg.Cut(text):\n//\n//\tuse the shortest path\n//\n// seg.Cut(text, false):\n//\n//\tuse cut dag not hmm\n//\n// seg.Cut(text, true):\n//\n//\tuse cut dag and hmm mode\nfunc (seg *Segmenter) Cut(str string, hmm ...bool) []string {\n\tif len(hmm) <= 0 {\n\t\treturn seg.Slice(str)\n\t\t// return seg.cutDAGNoHMM(str)\n\t}\n\n\tif len(hmm) > 0 && !hmm[0] {\n\t\treturn seg.cutDAGNoHMM(str)\n\t}\n\n\treturn seg.cutDAG(str)\n}\n\n// CutSearch cuts str into words using search engine mode.\nfunc (seg *Segmenter) CutSearch(str string, hmm ...bool) []string {\n\tif len(hmm) <= 0 {\n\t\treturn seg.Slice(str, true)\n\t}\n\n\treturn seg.cutForSearch(str, hmm...)\n}\n\n// CutAll cuts a str into words using full mode.\nfunc (seg *Segmenter) CutAll(str string) []string {\n\treturn seg.cutAll(str)\n}\n\n// CutDAG cut string with DAG use hmm and regexp\nfunc (seg *Segmenter) CutDAG(str string, reg ...*regexp.Regexp) []string {\n\treturn seg.cutDAG(str, reg...)\n}\n\n// CutDAGNoHMM cut string with DAG not use hmm\nfunc (seg *Segmenter) CutDAGNoHMM(str string) []string {\n\treturn seg.cutDAGNoHMM(str)\n}\n\n// CutStr cut []string with Cut return string\nfunc (seg *Segmenter) CutStr(str []string, separator ...string) (r string) {\n\tsep := \" \"\n\tif len(separator) > 0 {\n\t\tsep = separator[0]\n\t}\n\n\tfor i := 0; i < len(str); i++ {\n\t\tif i == len(str)-1 {\n\t\t\tr += str[i]\n\t\t} else {\n\t\t\tr += str[i] + sep\n\t\t}\n\t}\n\n\treturn\n}\n\n// LoadModel load the hmm model (default is Chinese char)\n//\n// Use the user's model:\n//\n//\tseg.LoadModel(B, E, M, S map[rune]float64)\nfunc (seg *Segmenter) LoadModel(prob ...map[rune]float64) {\n\thmm.LoadModel(prob...)\n}\n\n// HMMCut cut sentence string use HMM with Viterbi\nfunc (seg *Segmenter) HMMCut(str string, reg ...*regexp.Regexp) []string {\n\t// hmm.LoadModel(prob...)\n\treturn hmm.Cut(str, reg...)\n}\n\n// HMMCutMod cut sentence string use HMM with Viterbi\nfunc (seg *Segmenter) HMMCutMod(str string, prob ...map[rune]float64) []string {\n\thmm.LoadModel(prob...)\n\treturn hmm.Cut(str)\n}\n\n// Slice use modeSegment segment return []string\n// using search mode if searchMode is true\nfunc (seg *Segmenter) Slice(s string, searchMode ...bool) []string {\n\tsegs := seg.ModeSegment([]byte(s), searchMode...)\n\treturn ToSlice(segs, searchMode...)\n}\n\n// Slice use modeSegment segment return string\n// using search mode if searchMode is true\nfunc (seg *Segmenter) String(s string, searchMode ...bool) string {\n\tsegs := seg.ModeSegment([]byte(s), searchMode...)\n\treturn ToString(segs, searchMode...)\n}\n\n// SegPos type a POS struct\ntype SegPos struct {\n\tText, Pos string\n}\n\n// Pos return text and pos array\nfunc (seg *Segmenter) Pos(s string, searchMode ...bool) []SegPos {\n\tsa := seg.ModeSegment([]byte(s), searchMode...)\n\treturn ToPos(sa, searchMode...)\n}\n\n// PosStr cut []SegPos with Pos return string\nfunc (seg *Segmenter) PosStr(str []SegPos, separator ...string) (r string) {\n\tsep := \" \"\n\tif len(separator) > 0 {\n\t\tsep = separator[0]\n\t}\n\n\tfor i := 0; i < len(str); i++ {\n\t\tadd := str[i].Text\n\t\tif !seg.SkipPos {\n\t\t\tadd += \"/\" + str[i].Pos\n\t\t}\n\n\t\tif i == len(str)-1 {\n\t\t\tr += add\n\t\t} else {\n\t\t\tr += add + sep\n\t\t}\n\t}\n\n\treturn\n}\n"
        },
        {
          "name": "gse_bm_test.go",
          "type": "blob",
          "size": 1.453125,
          "content": "package gse\n\nimport (\n\t\"testing\"\n\n\t\"github.com/vcaesar/tt\"\n)\n\nvar text = \"纽约时代广场, 纽约帝国大厦, 旧金山湾金门大桥\"\n\nfunc BenchmarkCut(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.Cut(text)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutTrim(t *testing.B) {\n\tfn := func() {\n\t\ts := prodSeg.Cut(text)\n\t\tprodSeg.Trim(s)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCut_NoHMM(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.Cut(text, false)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutHMM(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.Cut(text, true)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutAll(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.CutAll(text)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutSearch(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.CutSearch(text)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutSearch_NoHMM(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.CutSearch(text, false)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkCutSearchHMM(t *testing.B) {\n\tfn := func() {\n\t\tprodSeg.CutSearch(text, true)\n\t}\n\n\ttt.BM(t, fn)\n}\n\nfunc BenchmarkSlice(b *testing.B) {\n\tfn := func() {\n\t\tprodSeg.Slice(text)\n\t}\n\n\ttt.BM(b, fn)\n}\n\nfunc BenchmarkString(b *testing.B) {\n\tfn := func() {\n\t\tprodSeg.String(text)\n\t}\n\n\ttt.BM(b, fn)\n}\n\nfunc BenchmarkAddToken(b *testing.B) {\n\tfn := func() {\n\t\tprodSeg.AddToken(text, 10)\n\t}\n\n\ttt.BM(b, fn)\n}\n\nfunc BenchmarkRemoveToken(b *testing.B) {\n\tfn := func() {\n\t\tprodSeg.RemoveToken(text)\n\t}\n\n\ttt.BM(b, fn)\n}\n\nfunc BenchmarkFind(b *testing.B) {\n\tfn := func() {\n\t\tprodSeg.Find(\"帝国大厦\")\n\t}\n\n\ttt.BM(b, fn)\n}\n"
        },
        {
          "name": "gse_test.go",
          "type": "blob",
          "size": 11.3212890625,
          "content": "package gse\n\nimport (\n\t\"regexp\"\n\t\"testing\"\n\n\t\"github.com/vcaesar/tt\"\n)\n\nfunc init() {\n\tprodSeg.LoadDict()\n\tprodSeg.LoadStop(\"zh\")\n}\n\nfunc TestLoadDictMap(t *testing.T) {\n\tm := []map[string]string{\n\t\t{\n\t\t\t\"text\": \"to be\",\n\t\t\t\"freq\": \"10\",\n\t\t\t\"pos\":  \"n\",\n\t\t},\n\t\t{\n\t\t\t\"text\": \"or not\",\n\t\t\t\"freq\": \"13\",\n\t\t},\n\t}\n\n\terr := prodSeg.LoadDictMap(m)\n\ttt.Nil(t, err)\n\n\tf, pos, ok := prodSeg.Find(\"to be\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"n\", pos)\n\ttt.Equal(t, 10, f)\n\n\tf, _, ok = prodSeg.Find(\"or not\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, 13, f)\n}\n\nfunc TestAnalyze(t *testing.T) {\n\ttxt := `城市地标建筑: 纽约帝国大厦, 旧金山湾金门大桥, Seattle Space Needle; Toronto CN Tower, 伦敦大笨钟`\n\n\ts := prodSeg.Cut(txt, true)\n\ttt.Equal(t, 23, len(s))\n\ttt.Equal(t, \"[城市地标 建筑 :  纽约 帝国大厦 ,  旧金山湾 金门大桥 ,  seattle   space   needle ;  toronto   cn   tower ,  伦敦 大笨钟]\", s)\n\n\ta := prodSeg.Analyze(s, \"\", true)\n\ttt.Equal(t, 23, len(a))\n\ttt.Equal(t, \"[{0 4 0 0  城市地标 3 j} {4 6 1 0  建筑 14397 n} {6 8 2 0  :  0 } {8 10 3 0  纽约 1758 ns} {10 14 4 0  帝国大厦 3 nr} {14 16 5 0  ,  0 } {16 20 6 0  旧金山湾 3 ns} {20 24 7 0  金门大桥 38 nz} {24 26 8 0  ,  0 } {26 33 9 0  seattle 0 } {33 34 10 0    0 } {34 39 11 0  space 0 } {39 40 12 0    0 } {40 46 13 0  needle 0 } {46 48 14 0  ;  0 } {48 55 15 0  toronto 0 } {55 56 16 0    0 } {56 58 17 0  cn 0 } {58 59 18 0    0 } {59 64 19 0  tower 0 } {64 66 20 0  ,  0 } {66 68 21 0  伦敦 2255 ns} {68 71 22 0  大笨钟 0 }]\", a)\n\n\ttt.Equal(t, 0, a[0].Start)\n\ttt.Equal(t, 4, a[0].End)\n\ttt.Equal(t, 0, a[0].Position)\n\ttt.Equal(t, 0, a[0].Len)\n\ttt.Equal(t, \"城市地标\", a[0].Text)\n\ttt.Equal(t, 3, a[0].Freq)\n\ttt.Equal(t, \"\", a[0].Type)\n\n\ts = prodSeg.CutSearch(txt, true)\n\ttt.Equal(t, 34, len(s))\n\ttt.Equal(t, \"[城市 市地 地标 城市地标 建筑 :  纽约 帝国 国大 大厦 帝国大厦 ,  金山 山湾 旧金山 旧金山湾 金门 大桥 金门大桥 ,  seattle   space   needle ;  toronto   cn   tower ,  伦敦 大笨钟]\", s)\n\n\ta = prodSeg.Analyze(s, txt)\n\ttt.Equal(t, 34, len(a))\n\ttt.Equal(t, \"[{0 6 0 0  城市 25084 ns} {3 9 1 0  市地 11 n} {6 12 2 0  地标 32 n} {0 12 3 0  城市地标 3 j} {12 18 4 0  建筑 14397 n} {18 20 5 0  :  0 } {20 26 6 0  纽约 1758 ns} {26 32 7 0  帝国 3655 n} {29 35 8 0  国大 114 j} {32 38 9 0  大厦 777 n} {26 38 10 0  帝国大厦 3 nr} {104 106 11 0  ,  0 } {43 49 12 0  金山 291 nr} {46 52 13 0  山湾 7 ns} {40 49 14 0  旧金山 238 ns} {40 52 15 0  旧金山湾 3 ns} {52 58 16 0  金门 149 n} {58 64 17 0  大桥 3288 ns} {52 64 18 0  金门大桥 38 nz} {64 66 19 0  ,  0 } {66 73 20 0  seattle 0 } {105 106 21 0    0 } {74 79 22 0  space 0 } {98 99 23 0    0 } {80 86 24 0  needle 0 } {86 88 25 0  ;  0 } {88 95 26 0  toronto 0 } {95 96 27 0    0 } {96 98 28 0  cn 0 } {87 88 29 0    0 } {99 104 30 0  tower 0 } {38 40 31 0  ,  0 } {106 112 32 0  伦敦 2255 ns} {112 121 33 0  大笨钟 0 }]\", a)\n}\n\nfunc TestHMM(t *testing.T) {\n\ttt.Equal(t, 587209, len(prodSeg.Dict.Tokens))\n\ttt.Equal(t, 5.3226765e+07, prodSeg.Dict.totalFreq)\n\n\thmm := prodSeg.HMMCutMod(\"纽约时代广场\")\n\ttt.Equal(t, 2, len(hmm))\n\ttt.Equal(t, \"纽约\", hmm[0])\n\ttt.Equal(t, \"时代广场\", hmm[1])\n\n\t// text := \"纽约时代广场, 纽约帝国大厦, 旧金山湾金门大桥\"\n\ttx := prodSeg.Cut(text, true)\n\ttt.Equal(t, 7, len(tx))\n\ttt.Equal(t, \"[纽约时代广场 ,  纽约 帝国大厦 ,  旧金山湾 金门大桥]\", tx)\n\n\ttx = prodSeg.TrimPunct(tx)\n\ttt.Equal(t, 5, len(tx))\n\ttt.Equal(t, \"[纽约时代广场 纽约 帝国大厦 旧金山湾 金门大桥]\", tx)\n\n\ttx = prodSeg.cutDAGNoHMM(text)\n\ttt.Equal(t, 9, len(tx))\n\ttt.Equal(t, \"[纽约时代广场 ,   纽约 帝国大厦 ,   旧金山湾 金门大桥]\", tx)\n\n\ttx = append(tx, \" 广场\")\n\ttx = append(tx, \"ok👌\", \"的\")\n\ttt.Bool(t, prodSeg.IsStop(\"的\"))\n\n\ttx = prodSeg.Trim(tx)\n\ttt.Equal(t, 7, len(tx))\n\ttt.Equal(t, \"[纽约时代广场 纽约 帝国大厦 旧金山湾 金门大桥 广场 ok]\", tx)\n\n\ttx1 := prodSeg.CutTrim(text, true)\n\ttt.Equal(t, 5, len(tx1))\n\ttt.Equal(t, \"[纽约时代广场 纽约 帝国大厦 旧金山湾 金门大桥]\", tx1)\n\n\ts := prodSeg.CutStr(tx, \", \")\n\ttt.Equal(t, 80, len(s))\n\ttt.Equal(t, \"纽约时代广场, 纽约, 帝国大厦, 旧金山湾, 金门大桥, 广场, ok\", s)\n\n\ttx = prodSeg.CutAll(text)\n\ttt.Equal(t, 21, len(tx))\n\ttt.Equal(t,\n\t\t\"[纽约 纽约时代广场 时代 时代广场 广场 ,   纽约 帝国 帝国大厦 国大 大厦 ,   旧金山 旧金山湾 金山 山湾 金门 金门大桥 大桥]\",\n\t\ttx)\n\n\ttx = prodSeg.CutSearch(text, false)\n\ttt.Equal(t, 20, len(tx))\n\ttt.Equal(t,\n\t\t\"[纽约 时代 广场 纽约时代广场 ,   纽约 帝国 国大 大厦 帝国大厦 ,   金山 山湾 旧金山 旧金山湾 金门 大桥 金门大桥]\",\n\t\ttx)\n\n\ttx = prodSeg.CutSearch(text, true)\n\ttt.Equal(t, 18, len(tx))\n\ttt.Equal(t,\n\t\t\"[纽约 时代 广场 纽约时代广场 ,  纽约 帝国 国大 大厦 帝国大厦 ,  金山 山湾 旧金山 旧金山湾 金门 大桥 金门大桥]\",\n\t\ttx)\n\n\tf1 := prodSeg.SuggestFreq(\"西雅图\")\n\ttt.Equal(t, 79, f1)\n\n\tf1 = prodSeg.SuggestFreq(\"西雅图\", \"西雅图都会区\", \"旧金山湾\")\n\ttt.Equal(t, 0, f1)\n\n\treg := regexp.MustCompile(`(\\d+年|\\d+月|\\d+日|[\\p{Latin}]+|[\\p{Hangul}]+|\\d+\\.\\d+|[a-zA-Z0-9]+)`)\n\ttext1 := `헬로월드 헬로 서울, 2021年09月10日, 3.14`\n\ttx = prodSeg.CutDAG(text1, reg)\n\ttt.Equal(t, 11, len(tx))\n\ttt.Equal(t, \"[헬로월드   헬로   서울 ,  2021年 09月 10日 ,  3.14]\", tx)\n\n\ttx = prodSeg.CutDAGNoHMM(text)\n\ttt.Equal(t, 9, len(tx))\n\ttt.Equal(t, \"[纽约时代广场 ,   纽约 帝国大厦 ,   旧金山湾 金门大桥]\", tx)\n}\n\nfunc TestPos(t *testing.T) {\n\ts := prodSeg.String(text, true)\n\ttt.Equal(t, 206, len(s))\n\ttt.Equal(t,\n\t\t\"纽约/ns 时代/n 广场/n 时代广场/n 纽约时代广场/nt ,/x  /x 纽约/ns 帝国/n 大厦/n 帝国大厦/nr ,/x  /x 金山/nr 旧金山/ns 湾/zg 旧金山湾/ns 金门/n 大桥/ns 金门大桥/nz \", s)\n\n\tc := prodSeg.Slice(text, true)\n\ttt.Equal(t, 20, len(c))\n\n\tpos := prodSeg.Pos(text, false)\n\ttt.Equal(t, 9, len(pos))\n\ttt.Equal(t,\n\t\t\"[{纽约时代广场 nt} {, x} {  x} {纽约 ns} {帝国大厦 nr} {, x} {  x} {旧金山湾 ns} {金门大桥 nz}]\", pos)\n\n\tpos = prodSeg.TrimPosPunct(pos)\n\ttt.Equal(t, 5, len(pos))\n\ttt.Equal(t, \"[{纽约时代广场 nt} {纽约 ns} {帝国大厦 nr} {旧金山湾 ns} {金门大桥 nz}]\", pos)\n\n\tpos = prodSeg.Pos(text, true)\n\ttt.Equal(t, 20, len(pos))\n\ttt.Equal(t,\n\t\t\"[{纽约 ns} {时代 n} {广场 n} {时代广场 n} {纽约时代广场 nt} {, x} {  x} {纽约 ns} {帝国 n} {大厦 n} {帝国大厦 nr} {, x} {  x} {金山 nr} {旧金山 ns} {湾 zg} {旧金山湾 ns} {金门 n} {大桥 ns} {金门大桥 nz}]\", pos)\n\n\tpos1 := prodSeg.PosTrim(text, true, \"zg\")\n\ttt.Equal(t, 15, len(pos1))\n\ttt.Equal(t,\n\t\t\"[{纽约 ns} {时代 n} {广场 n} {时代广场 n} {纽约时代广场 nt} {纽约 ns} {帝国 n} {大厦 n} {帝国大厦 nr} {金山 nr} {旧金山 ns} {旧金山湾 ns} {金门 n} {大桥 ns} {金门大桥 nz}]\", pos1)\n\n\tpos = append(pos, SegPos{Text: \"👌\", Pos: \"x\"})\n\tpos = prodSeg.TrimPos(pos)\n\ttt.Equal(t, 16, len(pos))\n\ttt.Equal(t,\n\t\t\"[{纽约 ns} {时代 n} {广场 n} {时代广场 n} {纽约时代广场 nt} {纽约 ns} {帝国 n} {大厦 n} {帝国大厦 nr} {金山 nr} {旧金山 ns} {湾 zg} {旧金山湾 ns} {金门 n} {大桥 ns} {金门大桥 nz}]\", pos)\n\n\ts = prodSeg.PosStr(pos, \", \")\n\ttt.Equal(t, 204, len(s))\n\ttt.Equal(t,\n\t\t\"纽约/ns, 时代/n, 广场/n, 时代广场/n, 纽约时代广场/nt, 纽约/ns, 帝国/n, 大厦/n, 帝国大厦/nr, 金山/nr, 旧金山/ns, 湾/zg, 旧金山湾/ns, 金门/n, 大桥/ns, 金门大桥/nz\", s)\n\n\tprodSeg.SkipPos = true\n\ts = prodSeg.PosStr(pos, \", \")\n\ttt.Equal(t, 162, len(s))\n\ttt.Equal(t,\n\t\t\"纽约, 时代, 广场, 时代广场, 纽约时代广场, 纽约, 帝国, 大厦, 帝国大厦, 金山, 旧金山, 湾, 旧金山湾, 金门, 大桥, 金门大桥\", s)\n\n\tpos = prodSeg.TrimWithPos(pos, \"n\", \"zg\")\n\ttt.Equal(t, 9, len(pos))\n\ttt.Equal(t,\n\t\t\"[{纽约 ns} {纽约时代广场 nt} {纽约 ns} {帝国大厦 nr} {金山 nr} {旧金山 ns} {旧金山湾 ns} {大桥 ns} {金门大桥 nz}]\", pos)\n\n\tpos2 := prodSeg.PosTrimArr(text, false, \"n\", \"zg\")\n\ttt.Equal(t, 5, len(pos2))\n\ttt.Equal(t,\n\t\t\"[纽约时代广场 纽约 帝国大厦 旧金山湾 金门大桥]\", pos2)\n\n\tpos3 := prodSeg.PosTrimStr(text, false, \"n\", \"zg\")\n\ttt.Equal(t, 64, len(pos3))\n\ttt.Equal(t,\n\t\t\"纽约时代广场 纽约 帝国大厦 旧金山湾 金门大桥\", pos3)\n}\n\nfunc TestLoadST(t *testing.T) {\n\tvar seg Segmenter\n\terr := seg.LoadDict(\"zh_s\")\n\ttt.Nil(t, err)\n\ttt.Equal(t, 352275, len(seg.Dict.Tokens))\n\ttt.Equal(t, 3.3335153e+07, seg.Dict.totalFreq)\n\n\terr = seg.LoadDict(\"zh_t, ./testdata/test_en_dict3.txt\")\n\ttt.Nil(t, err)\n\ttt.Equal(t, 587210, len(seg.Dict.Tokens))\n\ttt.Equal(t, 5.3226814e+07, seg.Dict.totalFreq)\n}\n\nfunc TestStop(t *testing.T) {\n\tvar seg Segmenter\n\terr := seg.LoadStop()\n\ttt.Nil(t, err)\n\ttt.Equal(t, 88, len(seg.StopWordMap))\n\n\terr = seg.LoadStop(\"testdata/stop.txt\")\n\ttt.Nil(t, err)\n\ttt.Equal(t, 90, len(seg.StopWordMap))\n\ttt.Bool(t, seg.IsStop(\"离开\"))\n\n\terr = seg.EmptyStop()\n\ttt.Nil(t, err)\n\ttt.Equal(t, \"map[]\", seg.StopWordMap)\n\n\t// err := prodSeg.LoadStop(\"zh\")\n\t// tt.Nil(t, err)\n\ttt.Equal(t, 1161, len(prodSeg.StopWordMap))\n\n\tb := prodSeg.IsStop(\"阿\")\n\ttt.True(t, b)\n\n\ttt.True(t, prodSeg.IsStop(\"哎\"))\n\tb = prodSeg.IsStop(\"的\")\n\ttt.True(t, b)\n\n\tprodSeg.AddStop(\"lol\")\n\tb = prodSeg.IsStop(\"lol\")\n\ttt.True(t, b)\n\n\tprodSeg.RemoveStop(\"lol\")\n\tb = prodSeg.IsStop(\"lol\")\n\ttt.False(t, b)\n\n\tm := []string{\"abc\", \"123\"}\n\tprodSeg.LoadStopArr(m)\n\ttt.True(t, prodSeg.IsStop(\"abc\"))\n\ttt.True(t, prodSeg.IsStop(\"123\"))\n\n\tt1 := `hi, bot, 123; 🤖, 机器人; 👌^_^😆`\n\ts := FilterEmoji(t1)\n\ttt.Equal(t, \"hi, bot, 123; , 机器人; ^_^\", s)\n\n\ts = FilterSymbol(t1)\n\ttt.Equal(t, \"hibot123机器人\", s)\n\n\ts = FilterLang(t1, \"Han\")\n\ttt.Equal(t, \"hibot机器人\", s)\n\n\tt2 := `<p>test: </p> <div class=\"bot\"> bot 机器人 <<银河系漫游指南>> </div>`\n\ts = FilterHtml(t2)\n\ttt.Equal(t, \"test:   bot 机器人 <<银河系漫游指南>> \", s)\n\n\tprodSeg.AddStop(`\"`)\n\tprodSeg.AddStopArr(\"class\", \"div\", \"=\")\n\ttt.True(t, prodSeg.IsStop(\"=\"))\n\ts1 := prodSeg.CutStop(t2, false)\n\ttt.Equal(t, \"[p test : p bot bot 机器人 银河系 漫游 指南]\", s1)\n\n\ts = prodSeg.CutTrimHtmls(t2, true)\n\ttt.Equal(t, \"test bot 机器人 银河系 漫游 指南\", s)\n\n\ts1 = Range(\"hibot, 机器人\")\n\ttt.Equal(t, \"[h i b o t ,   机 器 人]\", s1)\n\ts = RangeText(\"hibot, 机器人\")\n\ttt.Equal(t, \"h i b o t ,   机 器 人 \", s)\n}\n\nfunc TestNum(t *testing.T) {\n\tseg, err := New(\"./testdata/test_en_dict3.txt\")\n\ttt.Nil(t, err)\n\n\tseg.Num = true\n\ttext := \"t123test123 num123-1\"\n\ts := seg.Cut(text)\n\ttt.Equal(t, \"[t 1 2 3 test 1 2 3   num 1 2 3 - 1]\", s)\n\n\ts = seg.CutAll(text)\n\ttt.Equal(t, \"[t 1 2 3 t e s t 1 2 3   n u m 1 2 3 - 1]\", s)\n\n\tseg.Alpha = true\n\ts = seg.CutSearch(text)\n\ttt.Equal(t, \"[t 1 2 3 t e s t 1 2 3   n u m 1 2 3 - 1]\", s)\n\n\terr = seg.Empty()\n\ttt.Nil(t, err)\n\ttt.Nil(t, seg.Dict)\n}\n\nfunc TestUrl(t *testing.T) {\n\tseg, err := New(\"./testdata/test_en_dict3.txt\")\n\ttt.Nil(t, err)\n\n\ts1 := seg.CutUrls(\"https://www.g.com/search?q=test%m11.42&ie=UTF-8\")\n\ttt.Equal(t, \"https www g com search q test m 11 42 ie utf 8\", s1)\n}\n\nfunc TestLoadDictSep(t *testing.T) {\n\tvar seg1 Segmenter\n\tseg1.DictSep = \",\"\n\tseg1.NotLoadHMM = true\n\terr := seg1.LoadDict(\"./testdata/test_en.txt\")\n\ttt.Nil(t, err)\n\n\tf, pos, ok := seg1.Find(\"not to be\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"x\", pos)\n\ttt.Equal(t, 5, f)\n}\n"
        },
        {
          "name": "hmm",
          "type": "tree",
          "content": null
        },
        {
          "name": "seg_utils.go",
          "type": "blob",
          "size": 5.17578125,
          "content": "// Copyright 2013 Hui Chen\n// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n)\n\n// ToString converts a segments slice to string return the string\n//\n//\t two output modes:\n//\n//\t\tnormal mode (searchMode=false）\n//\t\tsearch mode（searchMode=true）\n//\n// default searchMode=false\n// search mode is used search engine, and will output more results\nfunc ToString(segs []Segment, searchMode ...bool) (output string) {\n\tvar mode bool\n\tif len(searchMode) > 0 {\n\t\tmode = searchMode[0]\n\t}\n\n\tif mode {\n\t\tfor _, seg := range segs {\n\t\t\toutput += tokenToString(seg.token)\n\t\t}\n\t\treturn\n\t}\n\n\tfor _, seg := range segs {\n\t\toutput += fmt.Sprintf(\"%s/%s \",\n\t\t\ttextSliceToString(seg.token.text), seg.token.pos)\n\t}\n\treturn\n}\n\nfunc tokenToString(token *Token) (output string) {\n\thasOnlyTerminalToken := true\n\tfor _, s := range token.segments {\n\t\tif len(s.token.segments) > 1 || IsJp(string(s.token.text[0])) {\n\t\t\thasOnlyTerminalToken = false\n\t\t}\n\n\t\tif !hasOnlyTerminalToken && s != nil {\n\t\t\toutput += tokenToString(s.token)\n\t\t}\n\t}\n\n\toutput += fmt.Sprintf(\"%s/%s \", textSliceToString(token.text), token.pos)\n\treturn\n}\n\nfunc tokenToBytes(token *Token) (output []byte) {\n\tfor _, s := range token.segments {\n\t\toutput = append(output, tokenToBytes(s.token)...)\n\t}\n\toutput = append(output,\n\t\t[]byte(fmt.Sprintf(\"%s/%s \", textSliceToString(token.text), token.pos))...)\n\n\treturn\n}\n\n// ToSlice converts a segments to slice return string slice\nfunc ToSlice(segs []Segment, searchMode ...bool) (output []string) {\n\tvar mode bool\n\tif len(searchMode) > 0 {\n\t\tmode = searchMode[0]\n\t}\n\n\tif mode {\n\t\tfor _, seg := range segs {\n\t\t\toutput = append(output, tokenToSlice(seg.token)...)\n\t\t}\n\t\treturn\n\t}\n\n\tfor _, seg := range segs {\n\t\toutput = append(output, seg.token.Text())\n\t}\n\treturn\n}\n\nfunc tokenToSlice(token *Token) (output []string) {\n\thasOnlyTerminalToken := true\n\tfor _, s := range token.segments {\n\t\tif len(s.token.segments) > 1 || IsJp(string(s.token.text[0])) {\n\t\t\thasOnlyTerminalToken = false\n\t\t}\n\n\t\tif !hasOnlyTerminalToken {\n\t\t\toutput = append(output, tokenToSlice(s.token)...)\n\t\t}\n\t}\n\n\toutput = append(output, textSliceToString(token.text))\n\treturn\n}\n\n// ToPos converts a segments slice to []SegPos\nfunc ToPos(segs []Segment, searchMode ...bool) (output []SegPos) {\n\tvar mode bool\n\tif len(searchMode) > 0 {\n\t\tmode = searchMode[0]\n\t}\n\n\tif mode {\n\t\tfor _, seg := range segs {\n\t\t\toutput = append(output, tokenToPos(seg.token)...)\n\t\t}\n\t\treturn\n\t}\n\n\tfor _, seg := range segs {\n\t\tpos1 := SegPos{\n\t\t\tText: textSliceToString(seg.token.text),\n\t\t\tPos:  seg.token.pos,\n\t\t}\n\n\t\toutput = append(output, pos1)\n\t}\n\n\treturn\n}\n\nfunc tokenToPos(token *Token) (output []SegPos) {\n\thasOnlyTerminalToken := true\n\tfor _, s := range token.segments {\n\t\tif len(s.token.segments) > 1 || IsJp(string(s.token.text[0])) {\n\t\t\thasOnlyTerminalToken = false\n\t\t}\n\n\t\tif !hasOnlyTerminalToken {\n\t\t\toutput = append(output, tokenToPos(s.token)...)\n\t\t}\n\t}\n\n\tpos1 := SegPos{\n\t\tText: textSliceToString(token.text),\n\t\tPos:  token.pos,\n\t}\n\toutput = append(output, pos1)\n\n\treturn\n}\n\n// let make multiple []Text into one string output\nfunc textToString(text []Text) (output string) {\n\tfor _, word := range text {\n\t\toutput += string(word)\n\t}\n\treturn\n}\n\n// let make []Text toString returns a string output\nfunc textSliceToString(text []Text) string {\n\treturn Join(text)\n}\n\n// return total length of text slice\nfunc textSliceByteLen(text []Text) (length int) {\n\tfor _, word := range text {\n\t\tlength += len(word)\n\t}\n\treturn\n}\n\nfunc textSliceToBytes(text []Text) []byte {\n\tvar buf bytes.Buffer\n\tfor _, word := range text {\n\t\tbuf.Write(word)\n\t}\n\n\treturn buf.Bytes()\n}\n\n// Join is better string splicing\nfunc Join(text []Text) string {\n\tswitch len(text) {\n\tcase 0:\n\t\treturn \"\"\n\tcase 1:\n\t\treturn string(text[0])\n\tcase 2:\n\t\t// Special case for common small values.\n\t\t// Remove if github.com/golang/go/issues/6714 is fixed\n\t\treturn string(text[0]) + string(text[1])\n\tcase 3:\n\t\t// Special case for common small values.\n\t\t// Remove if #6714 is fixed\n\t\treturn string(text[0]) + string(text[1]) + string(text[2])\n\t}\n\n\tn := 0\n\tfor i := 0; i < len(text); i++ {\n\t\tn += len(text[i])\n\t}\n\n\tb := make([]byte, n)\n\tbp := copy(b, text[0])\n\tfor _, str := range text[1:] {\n\t\tbp += copy(b[bp:], str)\n\t}\n\treturn string(b)\n}\n\nfunc printTokens(tokens []*Token, numTokens int) (output string) {\n\tfor iToken := 0; iToken < numTokens; iToken++ {\n\t\tfor _, word := range tokens[iToken].text {\n\t\t\toutput += fmt.Sprint(string(word))\n\t\t}\n\t\toutput += \" \"\n\t}\n\treturn\n}\n\nfunc toWords(strings ...string) []Text {\n\twords := []Text{}\n\tfor _, s := range strings {\n\t\twords = append(words, []byte(s))\n\t}\n\treturn words\n}\n\nfunc bytesToString(bytes []Text) (output string) {\n\tfor _, b := range bytes {\n\t\toutput += (string(b) + \"/\")\n\t}\n\treturn\n}\n"
        },
        {
          "name": "segmenter.go",
          "type": "blob",
          "size": 7.24609375,
          "content": "// Copyright 2013 Hui Chen\n// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\n// Segmenter define the segmenter structure\ntype Segmenter struct {\n\tDict     *Dictionary\n\tLoad     bool\n\tDictSep  string\n\tDictPath string\n\n\t// NotLoadHMM option load the default hmm model config (Chinese char)\n\tNotLoadHMM bool\n\n\t// AlphaNum set splitTextToWords can add token\n\t// when words in alphanum\n\t// set up alphanum dictionary word segmentation\n\tAlphaNum bool\n\tAlpha    bool\n\tNum      bool\n\t// ToLower set alpha tolower\n\t// ToLower bool\n\n\t// LoadNoFreq load not have freq dict word\n\tLoadNoFreq bool\n\t// MinTokenFreq load min freq token\n\tMinTokenFreq float64\n\t// TextFreq add token frequency when not specified freq\n\tTextFreq string\n\n\t// SkipLog set skip log print\n\tSkipLog bool\n\tMoreLog bool\n\n\t// SkipPos skip PosStr pos\n\tSkipPos bool\n\n\tNotStop bool\n\t// StopWordMap the stop word map\n\tStopWordMap map[string]bool\n}\n\n// jumper this structure is used to record information\n// about the forward leap at a word in the Viterbi algorithm\ntype jumper struct {\n\tminDistance float32\n\ttoken       *Token\n}\n\n// Segment use the shortest path to segment the text\n//\n// input parameter：\n//\n// bytes\tUTF8 text []byte\n//\n// output：\n//\n// []Segment return segments result\nfunc (seg *Segmenter) Segment(bytes []byte) []Segment {\n\treturn seg.internalSegment(bytes, false)\n}\n\n// ModeSegment segment using search mode if searchMode is true\nfunc (seg *Segmenter) ModeSegment(bytes []byte, searchMode ...bool) []Segment {\n\tvar mode bool\n\tif len(searchMode) > 0 {\n\t\tmode = searchMode[0]\n\t}\n\n\treturn seg.internalSegment(bytes, mode)\n}\n\nfunc (seg *Segmenter) internalSegment(bytes []byte, searchMode bool) []Segment {\n\t// special cases\n\tif len(bytes) == 0 {\n\t\t// return []Segment{}\n\t\treturn nil\n\t}\n\n\t// split text to words\n\ttext := seg.SplitTextToWords(bytes)\n\n\treturn seg.segmentWords(text, searchMode)\n}\n\nfunc (seg *Segmenter) segmentWords(text []Text, searchMode bool) []Segment {\n\t// The case where the division is no longer possible in the search mode\n\tif searchMode && len(text) == 1 {\n\t\treturn nil\n\t}\n\n\t// jumpers defines the forward jump information at each literal,\n\t// including the subword corresponding to this jump,\n\t// the and the value of the shortest path from the start\n\t// of the text segment to that literal\n\t//\n\tjumpers := make([]jumper, len(text))\n\n\tif seg.Dict == nil {\n\t\treturn nil\n\t}\n\n\ttokens := make([]*Token, seg.Dict.maxTokenLen)\n\tfor current := 0; current < len(text); current++ {\n\t\t// find the shortest path of the previous token,\n\t\t// to calculate the subsequent path values\n\t\tvar baseDistance float32\n\t\tif current == 0 {\n\t\t\t// When this character is at the beginning of the text,\n\t\t\t// the base distance should be zero\n\t\t\tbaseDistance = 0\n\t\t} else {\n\t\t\tbaseDistance = jumpers[current-1].minDistance\n\t\t}\n\n\t\t// find all the segments starting with this token\n\t\ttx := text[current:minInt(current+seg.Dict.maxTokenLen, len(text))]\n\t\tnumTokens := seg.Dict.LookupTokens(tx, tokens)\n\n\t\t// Update the jump information at the end of the split word\n\t\t// for all possible splits\n\t\tfor iToken := 0; iToken < numTokens; iToken++ {\n\t\t\tlocation := current + len(tokens[iToken].text) - 1\n\t\t\tif !searchMode || current != 0 || location != len(text)-1 {\n\t\t\t\tupdateJumper(&jumpers[location], baseDistance, tokens[iToken])\n\t\t\t}\n\t\t}\n\n\t\t// Add a pseudo-syllable if there is no corresponding syllable\n\t\t// for the current character\n\t\tif numTokens == 0 || len(tokens[0].text) > 1 {\n\t\t\tupdateJumper(&jumpers[current], baseDistance,\n\t\t\t\t&Token{text: []Text{text[current]}, freq: 1, distance: 32, pos: \"x\"})\n\t\t}\n\t}\n\n\t// Scan the first pass from back to front\n\t// to get the number of subwords to be added\n\tnumSeg := 0\n\tfor index := len(text) - 1; index >= 0; {\n\t\tlocation := index - len(jumpers[index].token.text) + 1\n\t\tnumSeg++\n\t\tindex = location - 1\n\t}\n\n\t// Scan from back to front for a second time\n\t// to add the split to the final result\n\toutputSegments := make([]Segment, numSeg)\n\tfor index := len(text) - 1; index >= 0; {\n\t\tlocation := index - len(jumpers[index].token.text) + 1\n\t\tnumSeg--\n\t\toutputSegments[numSeg].token = jumpers[index].token\n\t\tindex = location - 1\n\t}\n\n\t// Calculate the byte position of each participle\n\tbytePosition := 0\n\tfor iSeg := 0; iSeg < len(outputSegments); iSeg++ {\n\t\toutputSegments[iSeg].start = bytePosition\n\t\tbytePosition += textSliceByteLen(outputSegments[iSeg].token.text)\n\t\toutputSegments[iSeg].end = bytePosition\n\t}\n\n\treturn outputSegments\n}\n\n// updateJumper Update the jump information:\n//  1. When the location has never been visited\n//     (the case where jumper.minDistance is zero), or\n//  2. When the current shortest path at the location\n//     is greater than the new shortest path\n//\n// Update the shortest path value of the current location to baseDistance\n// add the probability of the new split\nfunc updateJumper(jumper *jumper, baseDistance float32, token *Token) {\n\tnewDistance := baseDistance + token.distance\n\tif jumper.minDistance == 0 || jumper.minDistance > newDistance {\n\t\tjumper.minDistance = newDistance\n\t\tjumper.token = token\n\t}\n}\n\n// SplitWords splits a string to token words\nfunc SplitWords(text Text) []Text {\n\tvar seg Segmenter\n\treturn seg.SplitTextToWords(text)\n}\n\n// SplitTextToWords splits a string to token words\nfunc (seg *Segmenter) SplitTextToWords(text Text) []Text {\n\toutput := make([]Text, 0, len(text)/3)\n\tcurrent, alphanumericStart := 0, 0\n\tinAlphanumeric := true\n\n\tfor current < len(text) {\n\t\tr, size := utf8.DecodeRune(text[current:])\n\t\tisNum := unicode.IsNumber(r) && !seg.Num\n\t\tisAlpha := unicode.IsLetter(r) && !seg.Alpha\n\t\tif size <= 2 && (isAlpha || isNum) {\n\t\t\t// Currently is Latin alphabet or numbers (not in CJK)\n\t\t\tif !inAlphanumeric {\n\t\t\t\talphanumericStart = current\n\t\t\t\tinAlphanumeric = true\n\t\t\t}\n\n\t\t\tif seg.AlphaNum {\n\t\t\t\toutput = append(output, toLow(text[current:current+size]))\n\t\t\t}\n\t\t} else {\n\t\t\tif inAlphanumeric {\n\t\t\t\tinAlphanumeric = false\n\t\t\t\tif current != 0 && !seg.AlphaNum {\n\t\t\t\t\toutput = append(output, toLow(text[alphanumericStart:current]))\n\t\t\t\t}\n\t\t\t}\n\n\t\t\toutput = append(output, text[current:current+size])\n\t\t}\n\t\tcurrent += size\n\t}\n\n\t// process last byte is alpha and num\n\tif inAlphanumeric && !seg.AlphaNum {\n\t\tif current != 0 {\n\t\t\toutput = append(output, toLow(text[alphanumericStart:current]))\n\t\t}\n\t}\n\n\treturn output\n}\n\nfunc toLow(text []byte) []byte {\n\tif ToLower {\n\t\treturn toLower(text)\n\t}\n\n\treturn text\n}\n\n// toLower converts a string to lower\nfunc toLower(text []byte) []byte {\n\toutput := make([]byte, len(text))\n\tfor i, t := range text {\n\t\tif t >= 'A' && t <= 'Z' {\n\t\t\toutput[i] = t - 'A' + 'a'\n\t\t} else {\n\t\t\toutput[i] = t\n\t\t}\n\t}\n\n\treturn output\n}\n\n// minInt get min value of int\nfunc minInt(a, b int) int {\n\tif a > b {\n\t\treturn b\n\t}\n\treturn a\n}\n\n// maxInt get max value of int\nfunc maxInt(a, b int) int {\n\tif a > b {\n\t\treturn a\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "segmenter_test.go",
          "type": "blob",
          "size": 7.5673828125,
          "content": "package gse\n\nimport (\n\t\"fmt\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/vcaesar/tt\"\n)\n\nvar (\n\tprodSeg = Segmenter{}\n\n\ttestH = []byte(\"こんにちは世界\")\n)\n\nfunc TestGetVer(t *testing.T) {\n\tfmt.Println(\"go version: \", runtime.Version())\n\tver := GetVersion()\n\n\ttt.Expect(t, Version, ver)\n\ttt.Expect(t, Version, ver)\n\ttt.Equal(t, Version, ver)\n}\n\nfunc TestSplit(t *testing.T) {\n\tvar seg1 Segmenter\n\ttt.Expect(t, \"世/界/有/七/十/亿/人/口/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"世界有七十亿人口\"))))\n\n\ttt.Expect(t, \"github/ /is/ /a/ /web/-/based/ /hosting/ /service/,/ /for/ /software/ /development/ /projects/./\",\n\t\tbytesToString(SplitWords([]byte(\n\t\t\t\"GitHub is a web-based hosting service, for software development projects.\"))))\n\n\ttt.Expect(t, \"雅/虎/yahoo/!/ /致/力/于/，/领/先/的/门/户/网/站/。/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\n\t\t\t\"雅虎Yahoo! 致力于，领先的门户网站。\"))))\n\n\ttt.Expect(t, \"こ/ん/に/ち/は/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"こんにちは\"))))\n\n\ttt.Expect(t, \"안/녕/하/세/요/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"안녕하세요\"))))\n\n\ttt.Expect(t, \"Я/ /тоже/ /рада/ /Вас/ /видеть/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"Я тоже рада Вас видеть\"))))\n\n\ttt.Expect(t, \"¿/cómo/ /van/ /las/ /cosas/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"¿Cómo van las cosas\"))))\n\n\ttt.Expect(t, \"wie/ /geht/ /es/ /ihnen/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"Wie geht es Ihnen\"))))\n\n\ttt.Expect(t, \"je/ /suis/ /enchanté/ /de/ /cette/ /pièce/\",\n\t\tbytesToString(seg1.SplitTextToWords([]byte(\"Je suis enchanté de cette pièce\"))))\n\n\ttt.Expect(t, \"[[116 111 32 119 111 114 100 115]]\", toWords(\"to words\"))\n}\n\nfunc TestSegment(t *testing.T) {\n\tvar seg Segmenter\n\tseg.LoadDict(\"testdata/zh/test_dict1.txt,testdata/zh/test_zh_dict2.txt\")\n\t// seg.LoadDict(\"testdata/zh/test_dict1.txt\", \"testdata/zh/test_zh_dict2.txt\")\n\ttt.Expect(t, \"16\", seg.Dict.NumTokens())\n\t// tt.Expect(t, \"5\", seg.Dict.NumTokens())\n\tsegments := seg.Segment([]byte(\"世界有七十亿人口\"))\n\ttt.Expect(t, \"世界/ 有/p3 七十亿/ 人口/p12 \", ToString(segments, false))\n\t// tt.Expect(t, \"世界/ 有/x 七十亿/ 人口/p12 \", ToString(segments, false))\n\n\ttt.Expect(t, \"4\", len(segments))\n\ttt.Expect(t, \"0\", segments[0].start)\n\ttt.Expect(t, \"6\", segments[0].end)\n\ttt.Expect(t, \"6\", segments[1].start)\n\ttt.Expect(t, \"9\", segments[1].end)\n\n\ttt.Expect(t, \"9\", segments[2].start)\n\ttt.Expect(t, \"18\", segments[2].end)\n\ttt.Expect(t, \"18\", segments[3].start)\n\ttt.Expect(t, \"24\", segments[3].end)\n}\n\nfunc TestSegmentJp(t *testing.T) {\n\tvar seg Segmenter\n\t// SkipLog = true\n\terr := seg.LoadDict(\"data/dict/jp/dict.txt\")\n\ttt.Nil(t, err)\n\ttt.Equal(t, 794146, len(seg.Dict.Tokens))\n\ttt.Equal(t, 4.784183005e+09, seg.Dict.totalFreq)\n\n\tf, pos, ok := seg.Find(\"自由\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"名詞\", pos)\n\ttt.Equal(t, 3636, f)\n\n\tf, pos, ok = seg.Find(\"此の度\")\n\ttt.Bool(t, ok)\n\ttt.Equal(t, \"名詞\", pos)\n\ttt.Equal(t, 5257, f)\n\n\tsegments := seg.Segment(testH)\n\n\ttt.Expect(t, \"こんにちは/感動詞 世界/名詞 \", ToString(segments, false))\n\ttt.Expect(t, \"こん/名詞 こんにちは/感動詞 世界/名詞 \", ToString(segments, true))\n\ttt.Expect(t, \"[こん こんにちは 世界]\", ToSlice(segments, true))\n\ttt.Expect(t, \"[こんにちは 世界]\", ToSlice(segments, false))\n\ttt.True(t, IsJp(ToSlice(segments)[0]))\n\n\ttt.Expect(t, \"2\", len(segments))\n\ttt.Expect(t, \"0\", segments[0].start)\n\ttt.Expect(t, \"15\", segments[0].end)\n\n\ttoken := segments[0].Token()\n\ttt.Expect(t, \"こんにちは\", token.Text())\n\ttt.Expect(t, \"5704\", token.Freq())\n\ttt.Expect(t, \"感動詞\", token.Pos())\n\n\tvar tokenArr []*Token\n\tfor i := 0; i < len(segments); i++ {\n\t\ttokenArr = append(tokenArr, segments[i].Token())\n\t}\n\ttt.Expect(t, \"こんにちは 世界 \", printTokens(tokenArr, 2))\n\n\ttseg := token.Segments()\n\ttt.Expect(t, \"0\", tseg[0].Start())\n\ttt.Expect(t, \"6\", tseg[0].End())\n}\n\nfunc TestLoadDictionary(t *testing.T) {\n\tvar seg, seg1 Segmenter\n\n\terr := seg.LoadDict(\"zh\")\n\ttt.Nil(t, err)\n\n\terr = seg1.LoadDict(\"jp\")\n\ttt.Nil(t, err)\n\tseg1.Load = false\n\n\terr = seg1.LoadDict()\n\ttt.Nil(t, err)\n\n\terr = seg.LoadDict(\"en\")\n\ttt.Nil(t, err)\n}\n\nfunc TestToken(t *testing.T) {\n\tToLower = false\n\tdefer func() { ToLower = true }()\n\n\tvar seg = prodSeg\n\tseg.Load = false\n\n\tseg.LoadNoFreq = true\n\tseg.TextFreq = \"2.0\"\n\tseg.MinTokenFreq = 1.0\n\tseg.MoreLog = true\n\tseg.SkipLog = true\n\n\ttt.Expect(t, \"世界/n 人口/n \", ToString(prodSeg.Segment(\n\t\t[]byte(\"世界人口\")), false))\n\n\tdict := seg.Dictionary()\n\ttt.Expect(t, \"16\", dict.MaxTokenLen())\n\ttt.Equal(t, 5.3226765e+07, dict.TotalFreq())\n\n\tfreq, pos, ok := dict.Find([]byte(\"世界\"))\n\ttt.Equal(t, 34387, freq)\n\ttt.Equal(t, \"n\", pos)\n\ttt.True(t, ok)\n\n\tfreq, _, ok = dict.Find([]byte(\"帝国大\"))\n\ttt.Equal(t, 0, freq)\n\ttt.True(t, ok)\n\n\tfreq, _, ok = dict.Find([]byte(\"帝国大厦\"))\n\ttt.Equal(t, 3, freq)\n\ttt.True(t, ok)\n\n\tfreq, _, ok = seg.Find(\"帝国大厦大\")\n\ttt.Equal(t, 0, freq)\n\ttt.False(t, ok)\n\n\tval, id, err := seg.Value(\"帝国\")\n\ttt.Equal(t, 96493, val)\n\ttt.Equal(t, 597213, id)\n\ttt.Nil(t, err)\n\n\terr = seg.AddToken(\"伦敦摘星塔\", 100)\n\ttt.Nil(t, err)\n\terr = seg.AddToken(\"Winter is coming\", 100)\n\ttt.Nil(t, err)\n\terr = seg.AddToken(\"Winter is coming\", 200)\n\ttt.Nil(t, err)\n\n\tfreq, _, ok = seg.Find(\"Winter is coming\")\n\ttt.Equal(t, 100, freq)\n\ttt.True(t, ok)\n\n\tfreq, _, ok = prodSeg.Find(\"伦敦摘星塔\")\n\ttt.Equal(t, 100, freq)\n\ttt.True(t, ok)\n\n\terr = prodSeg.AddToken(\"西雅图中心\", 100)\n\ttt.Nil(t, err)\n\n\terr = prodSeg.AddToken(\"西雅图太空针\", 100)\n\ttt.Nil(t, err)\n\tfreq, pos, ok = prodSeg.Find(\"西雅图太空针\")\n\ttt.Equal(t, 100, freq)\n\ttt.Equal(t, \"\", pos)\n\ttt.True(t, ok)\n\n\terr = prodSeg.ReAddToken(\"西雅图太空针\", 200, \"n\")\n\ttt.Nil(t, err)\n\tfreq, pos, ok = prodSeg.Find(\"西雅图太空针\")\n\ttt.Equal(t, 200, freq)\n\ttt.Equal(t, \"n\", pos)\n\ttt.True(t, ok)\n\n\tprodSeg.AddTokenForce(\"Space Needle\", 100, \"n\")\n\terr = prodSeg.RemoveToken(\"西雅图太空针\")\n\ttt.Nil(t, err)\n\tfreq, _, ok = dict.Find([]byte(\"西雅图太空针\"))\n\ttt.Equal(t, 0, freq)\n\ttt.False(t, ok)\n}\n\nfunc TestDictPaths(t *testing.T) {\n\tvar seg1 Segmenter\n\t// seg.SkipLog = true\n\tpaths := DictPaths(\"./dictDir\", \"zh, jp\")\n\ttt.Expect(t, \"3\", len(paths))\n\n\ttt.Expect(t, \"dictDir/dict/zh/t_1.txt\", paths[0])\n\ttt.Expect(t, \"dictDir/dict/zh/s_1.txt\", paths[1])\n\ttt.Expect(t, \"dictDir/dict/jp/dict.txt\", paths[2])\n\n\tpaths1 := DictPaths(\"./dictDir\", \"zh, jp\")\n\ttt.Expect(t, \"3\", len(paths))\n\ttt.Equal(t, paths, paths1)\n\n\tp := strings.ReplaceAll(seg1.GetCurrentFilePath(), \"/segmenter_test.go\", \"\") +\n\t\t`/data/dict/zh/idf.txt`\n\ttt.Equal(t, \"[\"+p+\"]\", seg1.GetIdfPath([]string{}...))\n\n\tseg1.DictPath = \"testdata/zh\"\n\ttt.Equal(t, \"testdata/zh\", seg1.GetCurrentFilePath())\n\ttt.Equal(t, \"[testdata/data/dict/zh/idf.txt]\", seg1.GetIdfPath([]string{}...))\n}\n\nfunc TestInAlphaNum(t *testing.T) {\n\t// var seg Segmenter\n\t// AlphaNum = true\n\t// seg.LoadDict(\"zh,./testdata/test_en_dict3.txt\")\n\t//\n\t// AlphaNum = true\n\t// ToLower = true\n\tseg, err := New(\"zh,./testdata/test_en_dict3.txt\", \"alpha\")\n\ttt.Nil(t, err)\n\n\tfreq, _, ok := seg.Find(\"hello\")\n\ttt.Equal(t, 20, freq)\n\ttt.True(t, ok)\n\n\tfreq, _, ok = seg.Find(\"world\")\n\ttt.Equal(t, 20, freq)\n\ttt.True(t, ok)\n\n\ttext := \"helloworld! 你好世界, Helloworld.\"\n\ttx := seg.Cut(text)\n\ttt.Equal(t, 11, len(tx))\n\ttt.Equal(t, \"[hello world !   你好 世界 ,   hello world .]\", tx)\n\n\ttx = seg.Cut(text, false)\n\ttt.Equal(t, 11, len(tx))\n\ttt.Equal(t, \"[hello world !   你好 世界 ,   hello world .]\", tx)\n\n\ttx = seg.Cut(text, true)\n\ttt.Equal(t, 9, len(tx))\n\ttt.Equal(t, \"[hello world !  你好 世界 ,  hello world .]\", tx)\n}\n"
        },
        {
          "name": "stop.go",
          "type": "blob",
          "size": 2.529296875,
          "content": "// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"bufio\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"strings\"\n)\n\n// StopWordMap the default stop words.\nvar StopWordMap = map[string]bool{\n\t\" \": true,\n}\n\n// LoadStopArr load stop word by []string\nfunc (seg *Segmenter) LoadStopArr(dict []string) {\n\tif seg.StopWordMap == nil {\n\t\tseg.StopWordMap = make(map[string]bool)\n\t}\n\n\tfor _, d := range dict {\n\t\tseg.StopWordMap[d] = true\n\t}\n}\n\n// LoadStop load stop word files add token to map\nfunc (seg *Segmenter) LoadStop(files ...string) error {\n\tif seg.StopWordMap == nil {\n\t\tseg.StopWordMap = make(map[string]bool)\n\t}\n\n\tdictDir := path.Join(path.Dir(seg.GetCurrentFilePath()), \"data\")\n\tif len(files) <= 0 {\n\t\tdictPath := path.Join(dictDir, \"dict/zh/stop_word.txt\")\n\t\tfiles = append(files, dictPath)\n\t}\n\n\tname := strings.Split(files[0], \", \")\n\tif name[0] == \"zh\" {\n\t\tname[0] = path.Join(dictDir, \"dict/zh/stop_tokens.txt\")\n\t}\n\n\tfor i := 0; i < len(name); i++ {\n\t\tif !seg.SkipLog {\n\t\t\tlog.Printf(\"Load the stop word dictionary: \\\"%s\\\" \", name[i])\n\t\t}\n\n\t\tfile, err := os.Open(name[i])\n\t\tif err != nil {\n\t\t\tlog.Printf(\"Could not load dictionaries: \\\"%s\\\", %v \\n\", name[i], err)\n\t\t\treturn err\n\t\t}\n\t\tdefer file.Close()\n\n\t\tscanner := bufio.NewScanner(file)\n\t\tfor scanner.Scan() {\n\t\t\ttext := scanner.Text()\n\t\t\tif text != \"\" {\n\t\t\t\tseg.StopWordMap[text] = true\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// AddStop add a token to the StopWord dictionary.\nfunc (seg *Segmenter) AddStop(text string) {\n\tseg.StopWordMap[text] = true\n}\n\n// AddStopArr add array stop token to stop dictionaries\nfunc (seg *Segmenter) AddStopArr(text ...string) {\n\tseg.LoadStopArr(text)\n}\n\n// RemoveStop remove a token from the StopWord dictionary.\nfunc (seg *Segmenter) RemoveStop(text string) {\n\tdelete(seg.StopWordMap, text)\n}\n\n// EmptyStop empty the stop dictionary\nfunc (seg *Segmenter) EmptyStop() error {\n\tseg.StopWordMap = nil\n\treturn nil\n}\n\n// IsStop check the word is a stop word.\nfunc (seg *Segmenter) IsStop(s string) bool {\n\t_, ok := seg.StopWordMap[s]\n\treturn ok\n\t// return StopWordMap[s]\n}\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "tf",
          "type": "tree",
          "content": null
        },
        {
          "name": "token.go",
          "type": "blob",
          "size": 3.4892578125,
          "content": "// Copyright 2013 Hui Chen\n// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\n// AnalyzeToken analyze the segment info structure\ntype AnalyzeToken struct {\n\t// the start of the segment in the text\n\tStart int\n\tEnd   int\n\n\tPosition int\n\tLen      int\n\n\tType string\n\n\tText string\n\tFreq float64\n\tPos  string\n}\n\n// Segment a segment in the text\ntype Segment struct {\n\t// the start of the segment in the text\n\tstart int\n\n\t// the bytes end of the segment in the text (not including this)\n\tend int\n\n\tPosition int\n\n\t// segment information\n\ttoken *Token\n}\n\n// Start returns the start byte position of the segment\nfunc (s *Segment) Start() int {\n\treturn s.start\n}\n\n// End return the end byte position of the segment (not including this)\nfunc (s *Segment) End() int {\n\treturn s.end\n}\n\n// Token return the segment token information\nfunc (s *Segment) Token() *Token {\n\treturn s.token\n}\n\n// Text a string type，used to parse text\n// 1. a word, such as \"world\" or \"boundary\", in English a word is a word\n// 2. a participle, such as \"world\" a.k.a. \"population\"\n// 3. a text, such as \"the world has seven billion people\"\ntype Text []byte\n\n// Token define a segment token structure\ntype Token struct {\n\t// a segment string，it's []Text\n\ttext []Text\n\n\t// a frequency of the token\n\tfreq float64\n\n\t// part of speech label\n\tpos string\n\n\t// log2(total frequency/this segment frequency)，equal to log2(1/p(segment)))，\n\t// used by the short path as the path length of the clause in dynamic programming.\n\t// Solving for the maximum of prod(p(segment)) is equivalent to solving for the minimum of\n\t// the minimum of sum(distance(segment)),\n\t// which is where \"shortest path\" comes from.\n\tdistance float32\n\n\t// For further segmentation of this segmented text,\n\t// see the Segments function comment.\n\tsegments []*Segment\n}\n\n// Text return the text of the segment\nfunc (token *Token) Text() string {\n\treturn textSliceToString(token.text)\n}\n\n// Freq returns the frequency in the dictionary token\nfunc (token *Token) Freq() float64 {\n\treturn token.freq\n}\n\n// Pos returns the part of speech in the dictionary token\nfunc (token *Token) Pos() string {\n\treturn token.pos\n}\n\n// Segments will segment further subdivisions of the text of this participle,\n// the participle has two subclauses.\n//\n// Subclauses can also have further subclauses forming a tree structure,\n// which can be traversed to get all the detailed subdivisions of the participle,\n// which is mainly Used by search engines to perform full-text searches on a piece of text.\nfunc (token *Token) Segments() []*Segment {\n\treturn token.segments\n}\n\n// Equals compare str split tokens\nfunc (token *Token) Equals(str string) bool {\n\ttokenLen := 0\n\tfor _, t := range token.text {\n\t\ttokenLen += len(t)\n\t}\n\tif tokenLen != len(str) {\n\t\treturn false\n\t}\n\n\tbytStr := []byte(str)\n\tindex := 0\n\tfor i := 0; i < len(token.text); i++ {\n\t\ttextArray := []byte(token.text[i])\n\t\tfor j := 0; j < len(textArray); j++ {\n\t\t\tif textArray[j] != bytStr[index] {\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\tindex++\n\t\t}\n\t}\n\n\treturn true\n}\n"
        },
        {
          "name": "token_test.go",
          "type": "blob",
          "size": 0.8505859375,
          "content": "package gse\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/vcaesar/tt\"\n)\n\nvar token = Token{\n\ttext: []Text{\n\t\t[]byte(\"one\"),\n\t\t[]byte(\"two\"),\n\t},\n}\n\nfunc TestTokenEquals(t *testing.T) {\n\ttt.True(t, token.Equals(\"onetwo\"))\n}\n\nfunc TestTokenNotEquals(t *testing.T) {\n\ttt.False(t, token.Equals(\"one-two\"))\n}\n\nvar strs = []Text{\n\tText(\"one\"),\n\tText(\"two\"),\n\tText(\"three\"),\n\tText(\"four\"),\n}\n\nfunc TextSliceToString(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\ttextSliceToString(strs)\n\t}\n}\n\nfunc TextToString(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\ttextToString(strs)\n\t}\n}\n\nfunc TestBenchmark(t *testing.T) {\n\tfmt.Println(\"textToString: \")\n\tfmt.Println(testing.Benchmark(TextToString))\n\n\tfmt.Println(\"textSliceToString: \")\n\tfmt.Println(testing.Benchmark(TextSliceToString))\n}\n\nfunc BenchmarkEquals(t *testing.B) {\n\tfn := func() {\n\t\ttoken.Equals(\"onetwo\")\n\t}\n\n\ttt.BM(t, fn)\n}\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "trim.go",
          "type": "blob",
          "size": 5.931640625,
          "content": "// Copyright 2016 ego authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n// License for the specific language governing permissions and limitations\n// under the License.\n\npackage gse\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\nfunc notPunct(ru []rune) bool {\n\tfor i := 0; i < len(ru); i++ {\n\t\tif !unicode.IsSpace(ru[i]) && !unicode.IsPunct(ru[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// TrimPunct trim []string exclude space and punct\nfunc (seg *Segmenter) TrimPunct(s []string) (r []string) {\n\tfor i := 0; i < len(s); i++ {\n\t\tru := []rune(s[i])\n\t\tif len(ru) > 0 {\n\t\t\tr0 := ru[0]\n\t\t\tif !unicode.IsSpace(r0) && !unicode.IsPunct(r0) {\n\t\t\t\tr = append(r, s[i])\n\t\t\t} else if len(ru) > 1 && notPunct(ru) {\n\t\t\t\tr = append(r, s[i])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// TrimPosPunct trim SegPos not space and punct\nfunc (seg *Segmenter) TrimPosPunct(se []SegPos) (re []SegPos) {\n\tfor i := 0; i < len(se); i++ {\n\t\tif !seg.NotStop && seg.IsStop(se[i].Text) {\n\t\t\tse[i].Text = \"\"\n\t\t}\n\n\t\tif se[i].Text != \"\" && len(se[i].Text) > 0 {\n\t\t\tru := []rune(se[i].Text)[0]\n\t\t\tif !unicode.IsSpace(ru) && !unicode.IsPunct(ru) {\n\t\t\t\tre = append(re, se[i])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// TrimWithPos trim some seg with pos\nfunc (seg *Segmenter) TrimWithPos(se []SegPos, pos ...string) (re []SegPos) {\n\tfor h := 0; h < len(pos); h++ {\n\t\tif h > 0 {\n\t\t\tse = re\n\t\t\tre = nil\n\t\t}\n\n\t\tfor i := 0; i < len(se); i++ {\n\t\t\tif se[i].Pos != pos[h] {\n\t\t\t\tre = append(re, se[i])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// Stop trim []string stop word\nfunc (seg *Segmenter) Stop(s []string) (r []string) {\n\tfor _, v := range s {\n\t\tif !seg.IsStop(v) && v != \"\" {\n\t\t\tr = append(r, v)\n\t\t}\n\t}\n\treturn\n}\n\n// Trim trim []string exclude symbol, space and punct\nfunc (seg *Segmenter) Trim(s []string) (r []string) {\n\tfor i := 0; i < len(s); i++ {\n\t\tsi := FilterSymbol(s[i])\n\t\tif !seg.NotStop && seg.IsStop(si) {\n\t\t\tsi = \"\"\n\t\t}\n\n\t\tif si != \"\" {\n\t\t\tr = append(r, si)\n\t\t}\n\t}\n\n\treturn\n}\n\n// TrimSymbol trim []string exclude symbol, space and punct\nfunc (seg *Segmenter) TrimSymbol(s []string) (r []string) {\n\tfor i := 0; i < len(s); i++ {\n\t\tsi := FilterSymbol(s[i])\n\t\tif si != \"\" {\n\t\t\tr = append(r, si)\n\t\t}\n\t}\n\n\treturn\n}\n\n// TrimPos trim SegPos not symbol, space and punct\nfunc (seg *Segmenter) TrimPos(s []SegPos) (r []SegPos) {\n\tfor i := 0; i < len(s); i++ {\n\t\tsi := FilterSymbol(s[i].Text)\n\t\tif !seg.NotStop && seg.IsStop(si) {\n\t\t\tsi = \"\"\n\t\t}\n\n\t\tif si != \"\" {\n\t\t\tr = append(r, s[i])\n\t\t}\n\t}\n\n\treturn\n}\n\n// CutStop cut string and tirm stop\nfunc (seg *Segmenter) CutStop(str string, hmm ...bool) []string {\n\treturn seg.Stop(seg.Cut(str, hmm...))\n}\n\n// CutTrim cut string and tirm\nfunc (seg *Segmenter) CutTrim(str string, hmm ...bool) []string {\n\ts := seg.Cut(str, hmm...)\n\treturn seg.Trim(s)\n}\n\n// PosTrim cut string pos and trim\nfunc (seg *Segmenter) PosTrim(str string, search bool, pos ...string) []SegPos {\n\tp := seg.Pos(str, search)\n\tp = seg.TrimWithPos(p, pos...)\n\treturn seg.TrimPos(p)\n}\n\n// PosTrimArr cut string return pos.Text []string\nfunc (seg *Segmenter) PosTrimArr(str string, search bool, pos ...string) (re []string) {\n\tp1 := seg.PosTrim(str, search, pos...)\n\tfor i := 0; i < len(p1); i++ {\n\t\tre = append(re, p1[i].Text)\n\t}\n\n\treturn\n}\n\n// PosTrimStr cut string return pos.Text string\nfunc (seg *Segmenter) PosTrimStr(str string, search bool, pos ...string) string {\n\tpa := seg.PosTrimArr(str, search, pos...)\n\treturn seg.CutStr(pa)\n}\n\n// CutTrimHtml cut string trim html and symbol return []string\nfunc (seg *Segmenter) CutTrimHtml(str string, hmm ...bool) []string {\n\tstr = FilterHtml(str)\n\ts := seg.Cut(str, hmm...)\n\treturn seg.TrimSymbol(s)\n}\n\n// CutTrimHtmls cut string trim html and symbol return string\nfunc (seg *Segmenter) CutTrimHtmls(str string, hmm ...bool) string {\n\ts := seg.CutTrimHtml(str, hmm...)\n\treturn seg.CutStr(s, \" \")\n}\n\n// CutUrl cut url string trim symbol return []string\nfunc (seg *Segmenter) CutUrl(str string, num ...bool) []string {\n\tif len(num) <= 0 {\n\t\t// seg.Num = true\n\t\tstr = SplitNums(str)\n\t}\n\ts := seg.Cut(str)\n\treturn seg.TrimSymbol(s)\n}\n\n// CutUrls cut url string trim symbol return string\nfunc (seg *Segmenter) CutUrls(str string, num ...bool) string {\n\treturn seg.CutStr(seg.CutUrl(str, num...), \" \")\n}\n\n// SplitNum cut string by num to []string\nfunc SplitNum(text string) []string {\n\tr := regexp.MustCompile(`\\d+|\\D+`)\n\treturn r.FindAllString(text, -1)\n}\n\n// SplitNums cut string by num to string\nfunc SplitNums(text string) string {\n\treturn strings.Join(SplitNum(text), \" \")\n}\n\n// FilterEmoji filter the emoji\nfunc FilterEmoji(text string) (new string) {\n\tfor _, value := range text {\n\t\t_, size := utf8.DecodeRuneInString(string(value))\n\t\tif size <= 3 {\n\t\t\tnew += string(value)\n\t\t}\n\t}\n\n\treturn\n}\n\n// FilterSymbol filter the symbol\nfunc FilterSymbol(text string) (new string) {\n\tfor _, value := range text {\n\t\tif !unicode.IsSymbol(value) &&\n\t\t\t!unicode.IsSpace(value) && !unicode.IsPunct(value) {\n\t\t\tnew += string(value)\n\t\t}\n\t}\n\n\treturn\n}\n\n// FilterHtml filter the html tag\nfunc FilterHtml(text string) string {\n\tregHtml := regexp.MustCompile(`(?U)\\<[^>]*[\\w|=|\"]+\\>`)\n\ttext = regHtml.ReplaceAllString(text, \"\")\n\treturn text\n}\n\n// FilterLang filter the language\nfunc FilterLang(text, lang string) (new string) {\n\tfor _, value := range text {\n\t\tif unicode.IsLetter(value) || unicode.Is(unicode.Scripts[lang], value) {\n\t\t\tnew += string(value)\n\t\t}\n\t}\n\n\treturn\n}\n\n// Range range text to []string\nfunc Range(text string) (new []string) {\n\tfor _, value := range text {\n\t\tnew = append(new, string(value))\n\t}\n\n\treturn\n}\n\n// RangeText range text to string\nfunc RangeText(text string) (new string) {\n\tfor _, value := range text {\n\t\tnew += string(value) + \" \"\n\t}\n\n\treturn\n}\n"
        }
      ]
    }
  ]
}