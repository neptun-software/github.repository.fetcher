{
  "metadata": {
    "timestamp": 1736567220417,
    "page": 816,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "goraft/raft",
      "stars": 2429,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.27734375,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n\ncoverage.html\ncoverprofile.out\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.03515625,
          "content": "Copyright 2013 go-raft contributors\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.2451171875,
          "content": "COVERPROFILE=cover.out\n\ndefault: test\n\ncover:\n\tgo test -coverprofile=$(COVERPROFILE) .\n\tgo tool cover -html=$(COVERPROFILE)\n\trm $(COVERPROFILE)\n\ndependencies:\n\tgo get -d .\n\ntest:\n\tgo test -i ./...\n\tgo test -v ./...\n\n.PHONY: coverage dependencies test\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.5048828125,
          "content": "go-raft [![Build Status](https://drone.io/github.com/goraft/raft/status.png)](https://drone.io/github.com/goraft/raft/latest) [![Coverage Status](https://coveralls.io/repos/goraft/raft/badge.png?branch=master)](https://coveralls.io/r/goraft/raft?branch=master)\n=======\n\n## Overview\n\n![unmaintained](http://img.shields.io/badge/status-unmaintained-red.png)\n\n\n**NOTE**: This project is unmaintained. If you are using goraft in a project\nand want to carry the project forward please file an issue with your ideas and\nintentions. The original project authors have created new raft implementations\nnow used in [etcd](https://godoc.org/github.com/coreos/etcd/raft) and [InfluxDB](https://godoc.org/github.com/influxdb/influxdb/raft).\n\nThis is a Go implementation of the Raft distributed consensus protocol.\nRaft is a protocol by which a cluster of nodes can maintain a replicated state machine.\nThe state machine is kept in sync through the use of a replicated log.\n\nFor more details on Raft, you can read [In Search of an Understandable Consensus Algorithm][raft-paper] by Diego Ongaro and John Ousterhout.\n\n## Project Status\n\nThis library is feature complete but should be considered experimental until it has seen more usage.\nIf you have any questions on implementing go-raft in your project please file an issue.\nThere is an [active community][community] of developers who can help.\ngo-raft is under the MIT license.\n\n[community]: https://github.com/goraft/raft/contributors\n\n### Features\n\n- Leader election\n- Log replication\n- Configuration changes\n- Log compaction\n- Unit tests\n- Fast Protobuf Log Encoding\n- HTTP transport\n\n### Projects\n\nThese projects are built on go-raft:\n\n\n- [goraft/raftd](https://github.com/goraft/raftd) - A reference implementation for using the go-raft library for distributed consensus.\n- [Weed File System](https://weed-fs.googlecode.com) - A scalable distributed key-to-file system with O(1) disk access for each read.\n- [rqlite](https://github.com/otoolep/rqlite) - A replicated SQLite database, distributing the database replicas across multiple nodes.\n\nIf you have a project that you're using go-raft in, please add it to this README so others can see implementation examples.\n\n## Contact and Resources\n\n- [raft-dev][raft-dev] is a mailing list for discussion about best practices\n  and implementation of Raft. Not goraft specific but helpful if you have\n  questions.\n- [Slides from Ben's talk][bens-talk] which includes easy to understand\n  diagrams of leader election and replication\n- The [Raft Consensus homepage][raft-home] has links to additional raft\n  implementations, slides to talks on Raft and general information\n\n[raft-home]:  http://raftconsensus.github.io/\n[raft-dev]: https://groups.google.com/forum/#!forum/raft-dev\n[bens-talk]: https://speakerdeck.com/benbjohnson/raft-the-understandable-distributed-consensus-protocol\n\n## The Raft Protocol\n\nThis section provides a summary of the Raft protocol from a high level.\nFor a more detailed explanation on the failover process and election terms please see the full paper describing the protocol: [In Search of an Understandable Consensus Algorithm][raft-paper].\n\n### Overview\n\nMaintaining state in a single process on a single server is easy.\nYour process is a single point of authority so there are no conflicts when reading and writing state.\nEven multi-threaded processes can rely on locks or coroutines to serialize access to the data.\n\nHowever, in a distributed system there is no single point of authority.\nServers can crash or the network between two machines can become unavailable or any number of other problems can occur.\n\nA distributed consensus protocol is used for maintaining a consistent state across multiple servers in a cluster.\nMany distributed systems are built upon the Paxos protocol but Paxos can be difficult to understand and there are many gaps between Paxos and real world implementation.\n\nAn alternative is the [Raft distributed consensus protocol][raft-paper] by Diego Ongaro and John Ousterhout.\nRaft is a protocol built with understandability as a primary tenet and it centers around two things:\n\n1. Leader Election\n2. Replicated Log\n\nWith these two constructs, you can build a system that can maintain state across multiple servers -- even in the event of multiple failures.\n\n### Leader Election\n\nThe Raft protocol effectively works as a master-slave system whereby state changes are written to a single server in the cluster and are distributed out to the rest of the servers in the cluster.\nThis simplifies the protocol since there is only one data authority and conflicts will not have to be resolved.\n\nRaft ensures that there is only one leader at a time.\nIt does this by performing elections among the nodes in the cluster and requiring that a node must receive a majority of the votes in order to become leader.\nFor example, if you have 3 nodes in your cluster then a single node would need 2 votes in order to become the leader.\nFor a 5 node cluster, a server would need 3 votes to become leader.\n\n### Replicated Log\n\nTo maintain state, a log of commands is maintained.\nEach command makes a change to the state of the server and the command is deterministic.\nBy ensuring that this log is replicated identically between all the nodes in the cluster we can replicate the state at any point in time in the log by running each command sequentially.\n\nReplicating the log under normal conditions is done by sending an `AppendEntries` RPC from the leader to each of the other servers in the cluster (called Peers).\nEach peer will append the entries from the leader through a 2-phase commit process which ensure that a majority of servers in the cluster have entries written to log.\n\n\n## Raft in Practice\n\n### Optimal Cluster Size\n\nThe primary consideration when choosing the node count in your Raft cluster is the number of nodes that can simultaneously fail.\nBecause Raft requires a majority of nodes to be available to make progress, the number of node failures the cluster can tolerate is `(n / 2) - 1`.\n\nThis means that a 3-node cluster can tolerate 1 node failure.\nIf 2 nodes fail then the cluster cannot commit entries or elect a new leader so progress stops.\nA 5-node cluster can tolerate 2 node failures. A 9-node cluster can tolerate 4 node failures.\nIt is unlikely that 4 nodes will simultaneously fail so clusters larger than 9 nodes are not common.\n\nAnother consideration is performance.\nThe leader must replicate log entries for each follower node so CPU and networking resources can quickly be bottlenecked under stress in a large cluster.\n\n\n### Scaling Raft\n\nOnce you grow beyond the maximum size of your cluster there are a few options for scaling Raft:\n\n1. *Core nodes with dumb replication.*\n   This option requires you to maintain a small cluster (e.g. 5 nodes) that is involved in the Raft process and then replicate only committed log entries to the remaining nodes in the cluster.\n   This works well if you have reads in your system that can be stale.\n\n2. *Sharding.*\n   This option requires that you segment your data into different clusters.\n   This option works well if you need very strong consistency and therefore need to read and write heavily from the leader.\n\nIf you have a very large cluster that you need to replicate to using Option 1 then you may want to look at performing hierarchical replication so that nodes can better share the load.\n\n\n## History\n\nBen Johnson started this library for use in his behavioral analytics database called [Sky](https://github.com/skydb/sky).\nHe put it under the MIT license in the hopes that it would be useful for other projects too.\n\n[raft-paper]: https://ramcloud.stanford.edu/raft.pdf\n"
        },
        {
          "name": "append_entries.go",
          "type": "blob",
          "size": 3.591796875,
          "content": "package raft\n\nimport (\n\t\"io\"\n\t\"io/ioutil\"\n\n\t\"code.google.com/p/gogoprotobuf/proto\"\n\t\"github.com/goraft/raft/protobuf\"\n)\n\n// The request sent to a server to append entries to the log.\ntype AppendEntriesRequest struct {\n\tTerm         uint64\n\tPrevLogIndex uint64\n\tPrevLogTerm  uint64\n\tCommitIndex  uint64\n\tLeaderName   string\n\tEntries      []*protobuf.LogEntry\n}\n\n// The response returned from a server appending entries to the log.\ntype AppendEntriesResponse struct {\n\tpb     *protobuf.AppendEntriesResponse\n\tpeer   string\n\tappend bool\n}\n\n// Creates a new AppendEntries request.\nfunc newAppendEntriesRequest(term uint64, prevLogIndex uint64, prevLogTerm uint64,\n\tcommitIndex uint64, leaderName string, entries []*LogEntry) *AppendEntriesRequest {\n\tpbEntries := make([]*protobuf.LogEntry, len(entries))\n\n\tfor i := range entries {\n\t\tpbEntries[i] = entries[i].pb\n\t}\n\n\treturn &AppendEntriesRequest{\n\t\tTerm:         term,\n\t\tPrevLogIndex: prevLogIndex,\n\t\tPrevLogTerm:  prevLogTerm,\n\t\tCommitIndex:  commitIndex,\n\t\tLeaderName:   leaderName,\n\t\tEntries:      pbEntries,\n\t}\n}\n\n// Encodes the AppendEntriesRequest to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (req *AppendEntriesRequest) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.AppendEntriesRequest{\n\t\tTerm:         proto.Uint64(req.Term),\n\t\tPrevLogIndex: proto.Uint64(req.PrevLogIndex),\n\t\tPrevLogTerm:  proto.Uint64(req.PrevLogTerm),\n\t\tCommitIndex:  proto.Uint64(req.CommitIndex),\n\t\tLeaderName:   proto.String(req.LeaderName),\n\t\tEntries:      req.Entries,\n\t}\n\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the AppendEntriesRequest from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (req *AppendEntriesRequest) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\tpb := new(protobuf.AppendEntriesRequest)\n\tif err := proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treq.Term = pb.GetTerm()\n\treq.PrevLogIndex = pb.GetPrevLogIndex()\n\treq.PrevLogTerm = pb.GetPrevLogTerm()\n\treq.CommitIndex = pb.GetCommitIndex()\n\treq.LeaderName = pb.GetLeaderName()\n\treq.Entries = pb.GetEntries()\n\n\treturn len(data), nil\n}\n\n// Creates a new AppendEntries response.\nfunc newAppendEntriesResponse(term uint64, success bool, index uint64, commitIndex uint64) *AppendEntriesResponse {\n\tpb := &protobuf.AppendEntriesResponse{\n\t\tTerm:        proto.Uint64(term),\n\t\tIndex:       proto.Uint64(index),\n\t\tSuccess:     proto.Bool(success),\n\t\tCommitIndex: proto.Uint64(commitIndex),\n\t}\n\n\treturn &AppendEntriesResponse{\n\t\tpb: pb,\n\t}\n}\n\nfunc (aer *AppendEntriesResponse) Index() uint64 {\n\treturn aer.pb.GetIndex()\n}\n\nfunc (aer *AppendEntriesResponse) CommitIndex() uint64 {\n\treturn aer.pb.GetCommitIndex()\n}\n\nfunc (aer *AppendEntriesResponse) Term() uint64 {\n\treturn aer.pb.GetTerm()\n}\n\nfunc (aer *AppendEntriesResponse) Success() bool {\n\treturn aer.pb.GetSuccess()\n}\n\n// Encodes the AppendEntriesResponse to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (resp *AppendEntriesResponse) Encode(w io.Writer) (int, error) {\n\tb, err := proto.Marshal(resp.pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(b)\n}\n\n// Decodes the AppendEntriesResponse from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (resp *AppendEntriesResponse) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\tresp.pb = new(protobuf.AppendEntriesResponse)\n\tif err := proto.Unmarshal(data, resp.pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn len(data), nil\n}\n"
        },
        {
          "name": "append_entries_test.go",
          "type": "blob",
          "size": 1.5595703125,
          "content": "package raft\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n)\n\nfunc BenchmarkAppendEntriesRequestEncoding(b *testing.B) {\n\treq, tmp := createTestAppendEntriesRequest(2000)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tvar buf bytes.Buffer\n\t\treq.Encode(&buf)\n\t}\n\tb.SetBytes(int64(len(tmp)))\n}\n\nfunc BenchmarkAppendEntriesRequestDecoding(b *testing.B) {\n\treq, buf := createTestAppendEntriesRequest(2000)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\treq.Decode(bytes.NewReader(buf))\n\t}\n\tb.SetBytes(int64(len(buf)))\n}\n\nfunc BenchmarkAppendEntriesResponseEncoding(b *testing.B) {\n\treq, tmp := createTestAppendEntriesResponse(2000)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tvar buf bytes.Buffer\n\t\treq.Encode(&buf)\n\t}\n\tb.SetBytes(int64(len(tmp)))\n}\n\nfunc BenchmarkAppendEntriesResponseDecoding(b *testing.B) {\n\treq, buf := createTestAppendEntriesResponse(2000)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\treq.Decode(bytes.NewReader(buf))\n\t}\n\tb.SetBytes(int64(len(buf)))\n}\n\nfunc createTestAppendEntriesRequest(entryCount int) (*AppendEntriesRequest, []byte) {\n\tentries := make([]*LogEntry, 0)\n\tfor i := 0; i < entryCount; i++ {\n\t\tcommand := &DefaultJoinCommand{Name: \"localhost:1000\"}\n\t\tentry, _ := newLogEntry(nil, nil, 1, 2, command)\n\t\tentries = append(entries, entry)\n\t}\n\treq := newAppendEntriesRequest(1, 1, 1, 1, \"leader\", entries)\n\n\tvar buf bytes.Buffer\n\treq.Encode(&buf)\n\n\treturn req, buf.Bytes()\n}\n\nfunc createTestAppendEntriesResponse(entryCount int) (*AppendEntriesResponse, []byte) {\n\tresp := newAppendEntriesResponse(1, true, 1, 1)\n\n\tvar buf bytes.Buffer\n\tresp.Encode(&buf)\n\n\treturn resp, buf.Bytes()\n}\n"
        },
        {
          "name": "command.go",
          "type": "blob",
          "size": 1.9228515625,
          "content": "package raft\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"reflect\"\n)\n\nvar commandTypes map[string]Command\n\nfunc init() {\n\tcommandTypes = map[string]Command{}\n}\n\n// Command represents an action to be taken on the replicated state machine.\ntype Command interface {\n\tCommandName() string\n}\n\n// CommandApply represents the interface to apply a command to the server.\ntype CommandApply interface {\n\tApply(Context) (interface{}, error)\n}\n\n// deprecatedCommandApply represents the old interface to apply a command to the server.\ntype deprecatedCommandApply interface {\n\tApply(Server) (interface{}, error)\n}\n\ntype CommandEncoder interface {\n\tEncode(w io.Writer) error\n\tDecode(r io.Reader) error\n}\n\n// Creates a new instance of a command by name.\nfunc newCommand(name string, data []byte) (Command, error) {\n\t// Find the registered command.\n\tcommand := commandTypes[name]\n\tif command == nil {\n\t\treturn nil, fmt.Errorf(\"raft.Command: Unregistered command type: %s\", name)\n\t}\n\n\t// Make a copy of the command.\n\tv := reflect.New(reflect.Indirect(reflect.ValueOf(command)).Type()).Interface()\n\tcopy, ok := v.(Command)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"raft: Unable to copy command: %s (%v)\", command.CommandName(), reflect.ValueOf(v).Kind().String()))\n\t}\n\n\t// If data for the command was passed in the decode it.\n\tif data != nil {\n\t\tif encoder, ok := copy.(CommandEncoder); ok {\n\t\t\tif err := encoder.Decode(bytes.NewReader(data)); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := json.NewDecoder(bytes.NewReader(data)).Decode(copy); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn copy, nil\n}\n\n// Registers a command by storing a reference to an instance of it.\nfunc RegisterCommand(command Command) {\n\tif command == nil {\n\t\tpanic(fmt.Sprintf(\"raft: Cannot register nil\"))\n\t} else if commandTypes[command.CommandName()] != nil {\n\t\tpanic(fmt.Sprintf(\"raft: Duplicate registration: %s\", command.CommandName()))\n\t}\n\tcommandTypes[command.CommandName()] = command\n}\n"
        },
        {
          "name": "commands.go",
          "type": "blob",
          "size": 1.423828125,
          "content": "package raft\n\nimport (\n\t\"io\"\n)\n\n// Join command interface\ntype JoinCommand interface {\n\tCommand\n\tNodeName() string\n}\n\n// Join command\ntype DefaultJoinCommand struct {\n\tName             string `json:\"name\"`\n\tConnectionString string `json:\"connectionString\"`\n}\n\n// Leave command interface\ntype LeaveCommand interface {\n\tCommand\n\tNodeName() string\n}\n\n// Leave command\ntype DefaultLeaveCommand struct {\n\tName string `json:\"name\"`\n}\n\n// NOP command\ntype NOPCommand struct {\n}\n\n// The name of the Join command in the log\nfunc (c *DefaultJoinCommand) CommandName() string {\n\treturn \"raft:join\"\n}\n\nfunc (c *DefaultJoinCommand) Apply(server Server) (interface{}, error) {\n\terr := server.AddPeer(c.Name, c.ConnectionString)\n\n\treturn []byte(\"join\"), err\n}\n\nfunc (c *DefaultJoinCommand) NodeName() string {\n\treturn c.Name\n}\n\n// The name of the Leave command in the log\nfunc (c *DefaultLeaveCommand) CommandName() string {\n\treturn \"raft:leave\"\n}\n\nfunc (c *DefaultLeaveCommand) Apply(server Server) (interface{}, error) {\n\terr := server.RemovePeer(c.Name)\n\n\treturn []byte(\"leave\"), err\n}\nfunc (c *DefaultLeaveCommand) NodeName() string {\n\treturn c.Name\n}\n\n// The name of the NOP command in the log\nfunc (c NOPCommand) CommandName() string {\n\treturn \"raft:nop\"\n}\n\nfunc (c NOPCommand) Apply(server Server) (interface{}, error) {\n\treturn nil, nil\n}\n\nfunc (c NOPCommand) Encode(w io.Writer) error {\n\treturn nil\n}\n\nfunc (c NOPCommand) Decode(r io.Reader) error {\n\treturn nil\n}\n"
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 0.1572265625,
          "content": "package raft\n\ntype Config struct {\n\tCommitIndex uint64 `json:\"commitIndex\"`\n\t// TODO decide what we need to store in peer struct\n\tPeers []*Peer `json:\"peers\"`\n}\n"
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 0.896484375,
          "content": "package raft\n\n// Context represents the current state of the server. It is passed into\n// a command when the command is being applied since the server methods\n// are locked.\ntype Context interface {\n\tServer() Server\n\tCurrentTerm() uint64\n\tCurrentIndex() uint64\n\tCommitIndex() uint64\n}\n\n// context is the concrete implementation of Context.\ntype context struct {\n\tserver       Server\n\tcurrentIndex uint64\n\tcurrentTerm  uint64\n\tcommitIndex  uint64\n}\n\n// Server returns a reference to the server.\nfunc (c *context) Server() Server {\n\treturn c.server\n}\n\n// CurrentTerm returns current term the server is in.\nfunc (c *context) CurrentTerm() uint64 {\n\treturn c.currentTerm\n}\n\n// CurrentIndex returns current index the server is at.\nfunc (c *context) CurrentIndex() uint64 {\n\treturn c.currentIndex\n}\n\n// CommitIndex returns last commit index the server is at.\nfunc (c *context) CommitIndex() uint64 {\n\treturn c.commitIndex\n}\n"
        },
        {
          "name": "debug.go",
          "type": "blob",
          "size": 2.58203125,
          "content": "package raft\n\nimport (\n\t\"log\"\n\t\"os\"\n)\n\n//------------------------------------------------------------------------------\n//\n// Variables\n//\n//------------------------------------------------------------------------------\n\nconst (\n\tDebug = 1\n\tTrace = 2\n)\n\nvar logLevel int = 0\nvar logger *log.Logger\n\nfunc init() {\n\tlogger = log.New(os.Stdout, \"[raft]\", log.Lmicroseconds)\n}\n\n//------------------------------------------------------------------------------\n//\n// Functions\n//\n//------------------------------------------------------------------------------\n\nfunc LogLevel() int {\n\treturn logLevel\n}\n\nfunc SetLogLevel(level int) {\n\tlogLevel = level\n}\n\n//--------------------------------------\n// Warnings\n//--------------------------------------\n\n// Prints to the standard logger. Arguments are handled in the manner of\n// fmt.Print.\nfunc warn(v ...interface{}) {\n\tlogger.Print(v...)\n}\n\n// Prints to the standard logger. Arguments are handled in the manner of\n// fmt.Printf.\nfunc warnf(format string, v ...interface{}) {\n\tlogger.Printf(format, v...)\n}\n\n// Prints to the standard logger. Arguments are handled in the manner of\n// fmt.Println.\nfunc warnln(v ...interface{}) {\n\tlogger.Println(v...)\n}\n\n//--------------------------------------\n// Basic debugging\n//--------------------------------------\n\n// Prints to the standard logger if debug mode is enabled. Arguments\n// are handled in the manner of fmt.Print.\nfunc debug(v ...interface{}) {\n\tif logLevel >= Debug {\n\t\tlogger.Print(v...)\n\t}\n}\n\n// Prints to the standard logger if debug mode is enabled. Arguments\n// are handled in the manner of fmt.Printf.\nfunc debugf(format string, v ...interface{}) {\n\tif logLevel >= Debug {\n\t\tlogger.Printf(format, v...)\n\t}\n}\n\n// Prints to the standard logger if debug mode is enabled. Arguments\n// are handled in the manner of fmt.Println.\nfunc debugln(v ...interface{}) {\n\tif logLevel >= Debug {\n\t\tlogger.Println(v...)\n\t}\n}\n\n//--------------------------------------\n// Trace-level debugging\n//--------------------------------------\n\n// Prints to the standard logger if trace debugging is enabled. Arguments\n// are handled in the manner of fmt.Print.\nfunc trace(v ...interface{}) {\n\tif logLevel >= Trace {\n\t\tlogger.Print(v...)\n\t}\n}\n\n// Prints to the standard logger if trace debugging is enabled. Arguments\n// are handled in the manner of fmt.Printf.\nfunc tracef(format string, v ...interface{}) {\n\tif logLevel >= Trace {\n\t\tlogger.Printf(format, v...)\n\t}\n}\n\n// Prints to the standard logger if trace debugging is enabled. Arguments\n// are handled in the manner of debugln.\nfunc traceln(v ...interface{}) {\n\tif logLevel >= Trace {\n\t\tlogger.Println(v...)\n\t}\n}\n"
        },
        {
          "name": "event.go",
          "type": "blob",
          "size": 1.49609375,
          "content": "package raft\n\nconst (\n\tStateChangeEventType  = \"stateChange\"\n\tLeaderChangeEventType = \"leaderChange\"\n\tTermChangeEventType   = \"termChange\"\n\tCommitEventType   = \"commit\"\n\tAddPeerEventType      = \"addPeer\"\n\tRemovePeerEventType   = \"removePeer\"\n\n\tHeartbeatIntervalEventType        = \"heartbeatInterval\"\n\tElectionTimeoutThresholdEventType = \"electionTimeoutThreshold\"\n\n\tHeartbeatEventType = \"heartbeat\"\n)\n\n// Event represents an action that occurred within the Raft library.\n// Listeners can subscribe to event types by using the Server.AddEventListener() function.\ntype Event interface {\n\tType() string\n\tSource() interface{}\n\tValue() interface{}\n\tPrevValue() interface{}\n}\n\n// event is the concrete implementation of the Event interface.\ntype event struct {\n\ttyp       string\n\tsource    interface{}\n\tvalue     interface{}\n\tprevValue interface{}\n}\n\n// newEvent creates a new event.\nfunc newEvent(typ string, value interface{}, prevValue interface{}) *event {\n\treturn &event{\n\t\ttyp:       typ,\n\t\tvalue:     value,\n\t\tprevValue: prevValue,\n\t}\n}\n\n// Type returns the type of event that occurred.\nfunc (e *event) Type() string {\n\treturn e.typ\n}\n\n// Source returns the object that dispatched the event.\nfunc (e *event) Source() interface{} {\n\treturn e.source\n}\n\n// Value returns the current value associated with the event, if applicable.\nfunc (e *event) Value() interface{} {\n\treturn e.value\n}\n\n// PrevValue returns the previous value associated with the event, if applicable.\nfunc (e *event) PrevValue() interface{} {\n\treturn e.prevValue\n}\n"
        },
        {
          "name": "event_dispatcher.go",
          "type": "blob",
          "size": 1.7275390625,
          "content": "package raft\n\nimport (\n\t\"reflect\"\n\t\"sync\"\n)\n\n// eventDispatcher is responsible for managing listeners for named events\n// and dispatching event notifications to those listeners.\ntype eventDispatcher struct {\n\tsync.RWMutex\n\tsource    interface{}\n\tlisteners map[string]eventListeners\n}\n\n// EventListener is a function that can receive event notifications.\ntype EventListener func(Event)\n\n// EventListeners represents a collection of individual listeners.\ntype eventListeners []EventListener\n\n// newEventDispatcher creates a new eventDispatcher instance.\nfunc newEventDispatcher(source interface{}) *eventDispatcher {\n\treturn &eventDispatcher{\n\t\tsource:    source,\n\t\tlisteners: make(map[string]eventListeners),\n\t}\n}\n\n// AddEventListener adds a listener function for a given event type.\nfunc (d *eventDispatcher) AddEventListener(typ string, listener EventListener) {\n\td.Lock()\n\tdefer d.Unlock()\n\td.listeners[typ] = append(d.listeners[typ], listener)\n}\n\n// RemoveEventListener removes a listener function for a given event type.\nfunc (d *eventDispatcher) RemoveEventListener(typ string, listener EventListener) {\n\td.Lock()\n\tdefer d.Unlock()\n\n\t// Grab a reference to the function pointer once.\n\tptr := reflect.ValueOf(listener).Pointer()\n\n\t// Find listener by pointer and remove it.\n\tlisteners := d.listeners[typ]\n\tfor i, l := range listeners {\n\t\tif reflect.ValueOf(l).Pointer() == ptr {\n\t\t\td.listeners[typ] = append(listeners[:i], listeners[i+1:]...)\n\t\t}\n\t}\n}\n\n// DispatchEvent dispatches an event.\nfunc (d *eventDispatcher) DispatchEvent(e Event) {\n\td.RLock()\n\tdefer d.RUnlock()\n\n\t// Automatically set the event source.\n\tif e, ok := e.(*event); ok {\n\t\te.source = d.source\n\t}\n\n\t// Dispatch the event to all listeners.\n\tfor _, l := range d.listeners[e.Type()] {\n\t\tl(e)\n\t}\n}\n"
        },
        {
          "name": "event_dispatcher_test.go",
          "type": "blob",
          "size": 1.6611328125,
          "content": "package raft\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\n// Ensure that we can listen and dispatch events.\nfunc TestDispatchEvent(t *testing.T) {\n\tvar count int\n\tdispatcher := newEventDispatcher(nil)\n\tdispatcher.AddEventListener(\"foo\", func(e Event) {\n\t\tcount += 1\n\t})\n\tdispatcher.AddEventListener(\"foo\", func(e Event) {\n\t\tcount += 10\n\t})\n\tdispatcher.AddEventListener(\"bar\", func(e Event) {\n\t\tcount += 100\n\t})\n\tdispatcher.DispatchEvent(&event{typ: \"foo\", value: nil, prevValue: nil})\n\tassert.Equal(t, 11, count)\n}\n\n// Ensure that we can add and remove a listener.\nfunc TestRemoveEventListener(t *testing.T) {\n\tvar count int\n\tf0 := func(e Event) {\n\t\tcount += 1\n\t}\n\tf1 := func(e Event) {\n\t\tcount += 10\n\t}\n\n\tdispatcher := newEventDispatcher(nil)\n\tdispatcher.AddEventListener(\"foo\", f0)\n\tdispatcher.AddEventListener(\"foo\", f1)\n\tdispatcher.DispatchEvent(&event{typ: \"foo\"})\n\tdispatcher.RemoveEventListener(\"foo\", f0)\n\tdispatcher.DispatchEvent(&event{typ: \"foo\"})\n\tassert.Equal(t, 21, count)\n}\n\n// Ensure that event is properly passed to listener.\nfunc TestEventListener(t *testing.T) {\n\tdispatcher := newEventDispatcher(\"X\")\n\tdispatcher.AddEventListener(\"foo\", func(e Event) {\n\t\tassert.Equal(t, \"foo\", e.Type())\n\t\tassert.Equal(t, \"X\", e.Source())\n\t\tassert.Equal(t, 10, e.Value())\n\t\tassert.Equal(t, 20, e.PrevValue())\n\t})\n\tdispatcher.DispatchEvent(&event{typ: \"foo\", value: 10, prevValue: 20})\n}\n\n// Benchmark the performance of event dispatch.\nfunc BenchmarkEventDispatch(b *testing.B) {\n\tdispatcher := newEventDispatcher(nil)\n\tdispatcher.AddEventListener(\"xxx\", func(e Event) {})\n\tfor i := 0; i < b.N; i++ {\n\t\tdispatcher.DispatchEvent(&event{typ: \"foo\", value: 10, prevValue: 20})\n\t}\n}\n"
        },
        {
          "name": "http_transporter.go",
          "type": "blob",
          "size": 8.9716796875,
          "content": "package raft\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"path\"\n\t\"time\"\n)\n\n// Parts from this transporter were heavily influenced by Peter Bougon's\n// raft implementation: https://github.com/peterbourgon/raft\n\n//------------------------------------------------------------------------------\n//\n// Typedefs\n//\n//------------------------------------------------------------------------------\n\n// An HTTPTransporter is a default transport layer used to communicate between\n// multiple servers.\ntype HTTPTransporter struct {\n\tDisableKeepAlives    bool\n\tprefix               string\n\tappendEntriesPath    string\n\trequestVotePath      string\n\tsnapshotPath         string\n\tsnapshotRecoveryPath string\n\thttpClient           http.Client\n\tTransport            *http.Transport\n}\n\ntype HTTPMuxer interface {\n\tHandleFunc(string, func(http.ResponseWriter, *http.Request))\n}\n\n//------------------------------------------------------------------------------\n//\n// Constructor\n//\n//------------------------------------------------------------------------------\n\n// Creates a new HTTP transporter with the given path prefix.\nfunc NewHTTPTransporter(prefix string, timeout time.Duration) *HTTPTransporter {\n\tt := &HTTPTransporter{\n\t\tDisableKeepAlives:    false,\n\t\tprefix:               prefix,\n\t\tappendEntriesPath:    joinPath(prefix, \"/appendEntries\"),\n\t\trequestVotePath:      joinPath(prefix, \"/requestVote\"),\n\t\tsnapshotPath:         joinPath(prefix, \"/snapshot\"),\n\t\tsnapshotRecoveryPath: joinPath(prefix, \"/snapshotRecovery\"),\n\t\tTransport:            &http.Transport{DisableKeepAlives: false},\n\t}\n\tt.httpClient.Transport = t.Transport\n\tt.Transport.ResponseHeaderTimeout = timeout\n\treturn t\n}\n\n//------------------------------------------------------------------------------\n//\n// Accessors\n//\n//------------------------------------------------------------------------------\n\n// Retrieves the path prefix used by the transporter.\nfunc (t *HTTPTransporter) Prefix() string {\n\treturn t.prefix\n}\n\n// Retrieves the AppendEntries path.\nfunc (t *HTTPTransporter) AppendEntriesPath() string {\n\treturn t.appendEntriesPath\n}\n\n// Retrieves the RequestVote path.\nfunc (t *HTTPTransporter) RequestVotePath() string {\n\treturn t.requestVotePath\n}\n\n// Retrieves the Snapshot path.\nfunc (t *HTTPTransporter) SnapshotPath() string {\n\treturn t.snapshotPath\n}\n\n// Retrieves the SnapshotRecovery path.\nfunc (t *HTTPTransporter) SnapshotRecoveryPath() string {\n\treturn t.snapshotRecoveryPath\n}\n\n//------------------------------------------------------------------------------\n//\n// Methods\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Installation\n//--------------------------------------\n\n// Applies Raft routes to an HTTP router for a given server.\nfunc (t *HTTPTransporter) Install(server Server, mux HTTPMuxer) {\n\tmux.HandleFunc(t.AppendEntriesPath(), t.appendEntriesHandler(server))\n\tmux.HandleFunc(t.RequestVotePath(), t.requestVoteHandler(server))\n\tmux.HandleFunc(t.SnapshotPath(), t.snapshotHandler(server))\n\tmux.HandleFunc(t.SnapshotRecoveryPath(), t.snapshotRecoveryHandler(server))\n}\n\n//--------------------------------------\n// Outgoing\n//--------------------------------------\n\n// Sends an AppendEntries RPC to a peer.\nfunc (t *HTTPTransporter) SendAppendEntriesRequest(server Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\tvar b bytes.Buffer\n\tif _, err := req.Encode(&b); err != nil {\n\t\ttraceln(\"transporter.ae.encoding.error:\", err)\n\t\treturn nil\n\t}\n\n\turl := joinPath(peer.ConnectionString, t.AppendEntriesPath())\n\ttraceln(server.Name(), \"POST\", url)\n\n\thttpResp, err := t.httpClient.Post(url, \"application/protobuf\", &b)\n\tif httpResp == nil || err != nil {\n\t\ttraceln(\"transporter.ae.response.error:\", err)\n\t\treturn nil\n\t}\n\tdefer httpResp.Body.Close()\n\n\tresp := &AppendEntriesResponse{}\n\tif _, err = resp.Decode(httpResp.Body); err != nil && err != io.EOF {\n\t\ttraceln(\"transporter.ae.decoding.error:\", err)\n\t\treturn nil\n\t}\n\n\treturn resp\n}\n\n// Sends a RequestVote RPC to a peer.\nfunc (t *HTTPTransporter) SendVoteRequest(server Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\tvar b bytes.Buffer\n\tif _, err := req.Encode(&b); err != nil {\n\t\ttraceln(\"transporter.rv.encoding.error:\", err)\n\t\treturn nil\n\t}\n\n\turl := fmt.Sprintf(\"%s%s\", peer.ConnectionString, t.RequestVotePath())\n\ttraceln(server.Name(), \"POST\", url)\n\n\thttpResp, err := t.httpClient.Post(url, \"application/protobuf\", &b)\n\tif httpResp == nil || err != nil {\n\t\ttraceln(\"transporter.rv.response.error:\", err)\n\t\treturn nil\n\t}\n\tdefer httpResp.Body.Close()\n\n\tresp := &RequestVoteResponse{}\n\tif _, err = resp.Decode(httpResp.Body); err != nil && err != io.EOF {\n\t\ttraceln(\"transporter.rv.decoding.error:\", err)\n\t\treturn nil\n\t}\n\n\treturn resp\n}\n\nfunc joinPath(connectionString, thePath string) string {\n\tu, err := url.Parse(connectionString)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tu.Path = path.Join(u.Path, thePath)\n\treturn u.String()\n}\n\n// Sends a SnapshotRequest RPC to a peer.\nfunc (t *HTTPTransporter) SendSnapshotRequest(server Server, peer *Peer, req *SnapshotRequest) *SnapshotResponse {\n\tvar b bytes.Buffer\n\tif _, err := req.Encode(&b); err != nil {\n\t\ttraceln(\"transporter.rv.encoding.error:\", err)\n\t\treturn nil\n\t}\n\n\turl := joinPath(peer.ConnectionString, t.snapshotPath)\n\ttraceln(server.Name(), \"POST\", url)\n\n\thttpResp, err := t.httpClient.Post(url, \"application/protobuf\", &b)\n\tif httpResp == nil || err != nil {\n\t\ttraceln(\"transporter.rv.response.error:\", err)\n\t\treturn nil\n\t}\n\tdefer httpResp.Body.Close()\n\n\tresp := &SnapshotResponse{}\n\tif _, err = resp.Decode(httpResp.Body); err != nil && err != io.EOF {\n\t\ttraceln(\"transporter.rv.decoding.error:\", err)\n\t\treturn nil\n\t}\n\n\treturn resp\n}\n\n// Sends a SnapshotRequest RPC to a peer.\nfunc (t *HTTPTransporter) SendSnapshotRecoveryRequest(server Server, peer *Peer, req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse {\n\tvar b bytes.Buffer\n\tif _, err := req.Encode(&b); err != nil {\n\t\ttraceln(\"transporter.rv.encoding.error:\", err)\n\t\treturn nil\n\t}\n\n\turl := joinPath(peer.ConnectionString, t.snapshotRecoveryPath)\n\ttraceln(server.Name(), \"POST\", url)\n\n\thttpResp, err := t.httpClient.Post(url, \"application/protobuf\", &b)\n\tif httpResp == nil || err != nil {\n\t\ttraceln(\"transporter.rv.response.error:\", err)\n\t\treturn nil\n\t}\n\tdefer httpResp.Body.Close()\n\n\tresp := &SnapshotRecoveryResponse{}\n\tif _, err = resp.Decode(httpResp.Body); err != nil && err != io.EOF {\n\t\ttraceln(\"transporter.rv.decoding.error:\", err)\n\t\treturn nil\n\t}\n\n\treturn resp\n}\n\n//--------------------------------------\n// Incoming\n//--------------------------------------\n\n// Handles incoming AppendEntries requests.\nfunc (t *HTTPTransporter) appendEntriesHandler(server Server) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\ttraceln(server.Name(), \"RECV /appendEntries\")\n\n\t\treq := &AppendEntriesRequest{}\n\t\tif _, err := req.Decode(r.Body); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tresp := server.AppendEntries(req)\n\t\tif resp == nil {\n\t\t\thttp.Error(w, \"Failed creating response.\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tif _, err := resp.Encode(w); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Handles incoming RequestVote requests.\nfunc (t *HTTPTransporter) requestVoteHandler(server Server) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\ttraceln(server.Name(), \"RECV /requestVote\")\n\n\t\treq := &RequestVoteRequest{}\n\t\tif _, err := req.Decode(r.Body); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tresp := server.RequestVote(req)\n\t\tif resp == nil {\n\t\t\thttp.Error(w, \"Failed creating response.\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tif _, err := resp.Encode(w); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Handles incoming Snapshot requests.\nfunc (t *HTTPTransporter) snapshotHandler(server Server) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\ttraceln(server.Name(), \"RECV /snapshot\")\n\n\t\treq := &SnapshotRequest{}\n\t\tif _, err := req.Decode(r.Body); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tresp := server.RequestSnapshot(req)\n\t\tif resp == nil {\n\t\t\thttp.Error(w, \"Failed creating response.\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tif _, err := resp.Encode(w); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Handles incoming SnapshotRecovery requests.\nfunc (t *HTTPTransporter) snapshotRecoveryHandler(server Server) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\ttraceln(server.Name(), \"RECV /snapshotRecovery\")\n\n\t\treq := &SnapshotRecoveryRequest{}\n\t\tif _, err := req.Decode(r.Body); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tresp := server.SnapshotRecoveryRequest(req)\n\t\tif resp == nil {\n\t\t\thttp.Error(w, \"Failed creating response.\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tif _, err := resp.Encode(w); err != nil {\n\t\t\thttp.Error(w, \"\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "http_transporter_test.go",
          "type": "blob",
          "size": 3.8505859375,
          "content": "package raft\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n)\n\n// Ensure that we can start several servers and have them communicate.\nfunc TestHTTPTransporter(t *testing.T) {\n\ttransporter := NewHTTPTransporter(\"/raft\", testElectionTimeout)\n\ttransporter.DisableKeepAlives = true\n\n\tservers := []Server{}\n\tf0 := func(server Server, httpServer *http.Server) {\n\t\t// Stop the leader and wait for an election.\n\t\tserver.Stop()\n\t\ttime.Sleep(testElectionTimeout * 2)\n\n\t\tif servers[1].State() != Leader && servers[2].State() != Leader {\n\t\t\tt.Fatal(\"Expected re-election:\", servers[1].State(), servers[2].State())\n\t\t}\n\t\tserver.Start()\n\t}\n\tf1 := func(server Server, httpServer *http.Server) {\n\t}\n\tf2 := func(server Server, httpServer *http.Server) {\n\t}\n\trunTestHttpServers(t, &servers, transporter, f0, f1, f2)\n}\n\n// Starts multiple independent Raft servers wrapped with HTTP servers.\nfunc runTestHttpServers(t *testing.T, servers *[]Server, transporter *HTTPTransporter, callbacks ...func(Server, *http.Server)) {\n\tvar wg sync.WaitGroup\n\thttpServers := []*http.Server{}\n\tlisteners := []net.Listener{}\n\tfor i := range callbacks {\n\t\twg.Add(1)\n\t\tport := 9000 + i\n\n\t\t// Create raft server.\n\t\tserver := newTestServer(fmt.Sprintf(\"localhost:%d\", port), transporter)\n\t\tserver.SetHeartbeatInterval(testHeartbeatInterval)\n\t\tserver.SetElectionTimeout(testElectionTimeout)\n\t\tserver.Start()\n\n\t\tdefer server.Stop()\n\t\t*servers = append(*servers, server)\n\n\t\t// Create listener for HTTP server and start it.\n\t\tlistener, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", port))\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tdefer listener.Close()\n\t\tlisteners = append(listeners, listener)\n\n\t\t// Create wrapping HTTP server.\n\t\tmux := http.NewServeMux()\n\t\ttransporter.Install(server, mux)\n\t\thttpServer := &http.Server{Addr: fmt.Sprintf(\":%d\", port), Handler: mux}\n\t\thttpServers = append(httpServers, httpServer)\n\t\tgo func() { httpServer.Serve(listener) }()\n\t}\n\n\t// Setup configuration.\n\tfor _, server := range *servers {\n\t\tif _, err := (*servers)[0].Do(&DefaultJoinCommand{Name: server.Name(), ConnectionString: fmt.Sprintf(\"http://%s\", server.Name())}); err != nil {\n\t\t\tt.Fatalf(\"Server %s unable to join: %v\", server.Name(), err)\n\t\t}\n\t}\n\n\t// Wait for configuration to propagate.\n\ttime.Sleep(testHeartbeatInterval * 2)\n\n\t// Execute all the callbacks at the same time.\n\tfor _i, _f := range callbacks {\n\t\ti, f := _i, _f\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tf((*servers)[i], httpServers[i])\n\t\t}()\n\t}\n\n\t// Wait until everything is done.\n\twg.Wait()\n}\n\nfunc BenchmarkSpeed(b *testing.B) {\n\n\ttransporter := NewHTTPTransporter(\"/raft\", testElectionTimeout)\n\ttransporter.DisableKeepAlives = true\n\n\tservers := []Server{}\n\n\tfor i := 0; i < 3; i++ {\n\t\tport := 9000 + i\n\n\t\t// Create raft server.\n\t\tserver := newTestServer(fmt.Sprintf(\"localhost:%d\", port), transporter)\n\t\tserver.SetHeartbeatInterval(testHeartbeatInterval)\n\t\tserver.SetElectionTimeout(testElectionTimeout)\n\t\tserver.Start()\n\n\t\tdefer server.Stop()\n\t\tservers = append(servers, server)\n\n\t\t// Create listener for HTTP server and start it.\n\t\tlistener, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", port))\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tdefer listener.Close()\n\n\t\t// Create wrapping HTTP server.\n\t\tmux := http.NewServeMux()\n\t\ttransporter.Install(server, mux)\n\t\thttpServer := &http.Server{Addr: fmt.Sprintf(\":%d\", port), Handler: mux}\n\n\t\tgo func() { httpServer.Serve(listener) }()\n\t}\n\n\t// Setup configuration.\n\tfor _, server := range servers {\n\t\t(servers)[0].Do(&DefaultJoinCommand{Name: server.Name(), ConnectionString: fmt.Sprintf(\"http://%s\", server.Name())})\n\t}\n\n\tc := make(chan bool)\n\n\t// Wait for configuration to propagate.\n\ttime.Sleep(testHeartbeatInterval * 2)\n\n\tb.ResetTimer()\n\tfor n := 0; n < b.N; n++ {\n\t\tfor i := 0; i < 1000; i++ {\n\t\t\tgo send(c, servers[0])\n\t\t}\n\n\t\tfor i := 0; i < 1000; i++ {\n\t\t\t<-c\n\t\t}\n\t}\n}\n\nfunc send(c chan bool, s Server) {\n\tfor i := 0; i < 20; i++ {\n\t\ts.Do(&NOPCommand{})\n\t}\n\tc <- true\n}\n"
        },
        {
          "name": "log.go",
          "type": "blob",
          "size": 16.92578125,
          "content": "package raft\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"sync\"\n\n\t\"github.com/goraft/raft/protobuf\"\n)\n\n//------------------------------------------------------------------------------\n//\n// Typedefs\n//\n//------------------------------------------------------------------------------\n\n// A log is a collection of log entries that are persisted to durable storage.\ntype Log struct {\n\tApplyFunc   func(*LogEntry, Command) (interface{}, error)\n\tfile        *os.File\n\tpath        string\n\tentries     []*LogEntry\n\tcommitIndex uint64\n\tmutex       sync.RWMutex\n\tstartIndex  uint64 // the index before the first entry in the Log entries\n\tstartTerm   uint64\n\tinitialized bool\n}\n\n// The results of the applying a log entry.\ntype logResult struct {\n\treturnValue interface{}\n\terr         error\n}\n\n//------------------------------------------------------------------------------\n//\n// Constructor\n//\n//------------------------------------------------------------------------------\n\n// Creates a new log.\nfunc newLog() *Log {\n\treturn &Log{\n\t\tentries: make([]*LogEntry, 0),\n\t}\n}\n\n//------------------------------------------------------------------------------\n//\n// Accessors\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Log Indices\n//--------------------------------------\n\n// The last committed index in the log.\nfunc (l *Log) CommitIndex() uint64 {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\treturn l.commitIndex\n}\n\n// The current index in the log.\nfunc (l *Log) currentIndex() uint64 {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\treturn l.internalCurrentIndex()\n}\n\n// The current index in the log without locking\nfunc (l *Log) internalCurrentIndex() uint64 {\n\tif len(l.entries) == 0 {\n\t\treturn l.startIndex\n\t}\n\treturn l.entries[len(l.entries)-1].Index()\n}\n\n// The next index in the log.\nfunc (l *Log) nextIndex() uint64 {\n\treturn l.currentIndex() + 1\n}\n\n// Determines if the log contains zero entries.\nfunc (l *Log) isEmpty() bool {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\treturn (len(l.entries) == 0) && (l.startIndex == 0)\n}\n\n// The name of the last command in the log.\nfunc (l *Log) lastCommandName() string {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\tif len(l.entries) > 0 {\n\t\tif entry := l.entries[len(l.entries)-1]; entry != nil {\n\t\t\treturn entry.CommandName()\n\t\t}\n\t}\n\treturn \"\"\n}\n\n//--------------------------------------\n// Log Terms\n//--------------------------------------\n\n// The current term in the log.\nfunc (l *Log) currentTerm() uint64 {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\n\tif len(l.entries) == 0 {\n\t\treturn l.startTerm\n\t}\n\treturn l.entries[len(l.entries)-1].Term()\n}\n\n//------------------------------------------------------------------------------\n//\n// Methods\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// State\n//--------------------------------------\n\n// Opens the log file and reads existing entries. The log can remain open and\n// continue to append entries to the end of the log.\nfunc (l *Log) open(path string) error {\n\t// Read all the entries from the log if one exists.\n\tvar readBytes int64\n\n\tvar err error\n\tdebugln(\"log.open.open \", path)\n\t// open log file\n\tl.file, err = os.OpenFile(path, os.O_RDWR, 0600)\n\tl.path = path\n\n\tif err != nil {\n\t\t// if the log file does not exist before\n\t\t// we create the log file and set commitIndex to 0\n\t\tif os.IsNotExist(err) {\n\t\t\tl.file, err = os.OpenFile(path, os.O_WRONLY|os.O_CREATE, 0600)\n\t\t\tdebugln(\"log.open.create \", path)\n\t\t\tif err == nil {\n\t\t\t\tl.initialized = true\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\treturn err\n\t}\n\tdebugln(\"log.open.exist \", path)\n\n\t// Read the file and decode entries.\n\tfor {\n\t\t// Instantiate log entry and decode into it.\n\t\tentry, _ := newLogEntry(l, nil, 0, 0, nil)\n\t\tentry.Position, _ = l.file.Seek(0, os.SEEK_CUR)\n\n\t\tn, err := entry.Decode(l.file)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tdebugln(\"open.log.append: finish \")\n\t\t\t} else {\n\t\t\t\tif err = os.Truncate(path, readBytes); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"raft.Log: Unable to recover: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tif entry.Index() > l.startIndex {\n\t\t\t// Append entry.\n\t\t\tl.entries = append(l.entries, entry)\n\t\t\tif entry.Index() <= l.commitIndex {\n\t\t\t\tcommand, err := newCommand(entry.CommandName(), entry.Command())\n\t\t\t\tif err != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tl.ApplyFunc(entry, command)\n\t\t\t}\n\t\t\tdebugln(\"open.log.append log index \", entry.Index())\n\t\t}\n\n\t\treadBytes += int64(n)\n\t}\n\tdebugln(\"open.log.recovery number of log \", len(l.entries))\n\tl.initialized = true\n\treturn nil\n}\n\n// Closes the log file.\nfunc (l *Log) close() {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tif l.file != nil {\n\t\tl.file.Close()\n\t\tl.file = nil\n\t}\n\tl.entries = make([]*LogEntry, 0)\n}\n\n// sync to disk\nfunc (l *Log) sync() error {\n\treturn l.file.Sync()\n}\n\n//--------------------------------------\n// Entries\n//--------------------------------------\n\n// Creates a log entry associated with this log.\nfunc (l *Log) createEntry(term uint64, command Command, e *ev) (*LogEntry, error) {\n\treturn newLogEntry(l, e, l.nextIndex(), term, command)\n}\n\n// Retrieves an entry from the log. If the entry has been eliminated because\n// of a snapshot then nil is returned.\nfunc (l *Log) getEntry(index uint64) *LogEntry {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\n\tif index <= l.startIndex || index > (l.startIndex+uint64(len(l.entries))) {\n\t\treturn nil\n\t}\n\treturn l.entries[index-l.startIndex-1]\n}\n\n// Checks if the log contains a given index/term combination.\nfunc (l *Log) containsEntry(index uint64, term uint64) bool {\n\tentry := l.getEntry(index)\n\treturn (entry != nil && entry.Term() == term)\n}\n\n// Retrieves a list of entries after a given index as well as the term of the\n// index provided. A nil list of entries is returned if the index no longer\n// exists because a snapshot was made.\nfunc (l *Log) getEntriesAfter(index uint64, maxLogEntriesPerRequest uint64) ([]*LogEntry, uint64) {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\n\t// Return nil if index is before the start of the log.\n\tif index < l.startIndex {\n\t\ttraceln(\"log.entriesAfter.before: \", index, \" \", l.startIndex)\n\t\treturn nil, 0\n\t}\n\n\t// Return an error if the index doesn't exist.\n\tif index > (uint64(len(l.entries)) + l.startIndex) {\n\t\tpanic(fmt.Sprintf(\"raft: Index is beyond end of log: %v %v\", len(l.entries), index))\n\t}\n\n\t// If we're going from the beginning of the log then return the whole log.\n\tif index == l.startIndex {\n\t\ttraceln(\"log.entriesAfter.beginning: \", index, \" \", l.startIndex)\n\t\treturn l.entries, l.startTerm\n\t}\n\n\ttraceln(\"log.entriesAfter.partial: \", index, \" \", l.entries[len(l.entries)-1].Index)\n\n\tentries := l.entries[index-l.startIndex:]\n\tlength := len(entries)\n\n\ttraceln(\"log.entriesAfter: startIndex:\", l.startIndex, \" length\", len(l.entries))\n\n\tif uint64(length) < maxLogEntriesPerRequest {\n\t\t// Determine the term at the given entry and return a subslice.\n\t\treturn entries, l.entries[index-1-l.startIndex].Term()\n\t} else {\n\t\treturn entries[:maxLogEntriesPerRequest], l.entries[index-1-l.startIndex].Term()\n\t}\n}\n\n//--------------------------------------\n// Commit\n//--------------------------------------\n\n// Retrieves the last index and term that has been committed to the log.\nfunc (l *Log) commitInfo() (index uint64, term uint64) {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\t// If we don't have any committed entries then just return zeros.\n\tif l.commitIndex == 0 {\n\t\treturn 0, 0\n\t}\n\n\t// No new commit log after snapshot\n\tif l.commitIndex == l.startIndex {\n\t\treturn l.startIndex, l.startTerm\n\t}\n\n\t// Return the last index & term from the last committed entry.\n\tdebugln(\"commitInfo.get.[\", l.commitIndex, \"/\", l.startIndex, \"]\")\n\tentry := l.entries[l.commitIndex-1-l.startIndex]\n\treturn entry.Index(), entry.Term()\n}\n\n// Retrieves the last index and term that has been appended to the log.\nfunc (l *Log) lastInfo() (index uint64, term uint64) {\n\tl.mutex.RLock()\n\tdefer l.mutex.RUnlock()\n\n\t// If we don't have any entries then just return zeros.\n\tif len(l.entries) == 0 {\n\t\treturn l.startIndex, l.startTerm\n\t}\n\n\t// Return the last index & term\n\tentry := l.entries[len(l.entries)-1]\n\treturn entry.Index(), entry.Term()\n}\n\n// Updates the commit index\nfunc (l *Log) updateCommitIndex(index uint64) {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\tif index > l.commitIndex {\n\t\tl.commitIndex = index\n\t}\n\tdebugln(\"update.commit.index \", index)\n}\n\n// Updates the commit index and writes entries after that index to the stable storage.\nfunc (l *Log) setCommitIndex(index uint64) error {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\t// this is not error any more after limited the number of sending entries\n\t// commit up to what we already have\n\tif index > l.startIndex+uint64(len(l.entries)) {\n\t\tdebugln(\"raft.Log: Commit index\", index, \"set back to \", len(l.entries))\n\t\tindex = l.startIndex + uint64(len(l.entries))\n\t}\n\n\t// Do not allow previous indices to be committed again.\n\n\t// This could happens, since the guarantee is that the new leader has up-to-dated\n\t// log entries rather than has most up-to-dated committed index\n\n\t// For example, Leader 1 send log 80 to follower 2 and follower 3\n\t// follower 2 and follow 3 all got the new entries and reply\n\t// leader 1 committed entry 80 and send reply to follower 2 and follower3\n\t// follower 2 receive the new committed index and update committed index to 80\n\t// leader 1 fail to send the committed index to follower 3\n\t// follower 3 promote to leader (server 1 and server 2 will vote, since leader 3\n\t// has up-to-dated the entries)\n\t// when new leader 3 send heartbeat with committed index = 0 to follower 2,\n\t// follower 2 should reply success and let leader 3 update the committed index to 80\n\n\tif index < l.commitIndex {\n\t\treturn nil\n\t}\n\n\t// Find all entries whose index is between the previous index and the current index.\n\tfor i := l.commitIndex + 1; i <= index; i++ {\n\t\tentryIndex := i - 1 - l.startIndex\n\t\tentry := l.entries[entryIndex]\n\n\t\t// Update commit index.\n\t\tl.commitIndex = entry.Index()\n\n\t\t// Decode the command.\n\t\tcommand, err := newCommand(entry.CommandName(), entry.Command())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Apply the changes to the state machine and store the error code.\n\t\treturnValue, err := l.ApplyFunc(entry, command)\n\n\t\tdebugf(\"setCommitIndex.set.result index: %v, entries index: %v\", i, entryIndex)\n\t\tif entry.event != nil {\n\t\t\tentry.event.returnValue = returnValue\n\t\t\tentry.event.c <- err\n\t\t}\n\n\t\t_, isJoinCommand := command.(JoinCommand)\n\n\t\t// we can only commit up to the most recent join command\n\t\t// if there is a join in this batch of commands.\n\t\t// after this commit, we need to recalculate the majority.\n\t\tif isJoinCommand {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn nil\n}\n\n// Set the commitIndex at the head of the log file to the current\n// commit Index. This should be called after obtained a log lock\nfunc (l *Log) flushCommitIndex() {\n\tl.file.Seek(0, os.SEEK_SET)\n\tfmt.Fprintf(l.file, \"%8x\\n\", l.commitIndex)\n\tl.file.Seek(0, os.SEEK_END)\n}\n\n//--------------------------------------\n// Truncation\n//--------------------------------------\n\n// Truncates the log to the given index and term. This only works if the log\n// at the index has not been committed.\nfunc (l *Log) truncate(index uint64, term uint64) error {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\tdebugln(\"log.truncate: \", index)\n\n\t// Do not allow committed entries to be truncated.\n\tif index < l.commitIndex {\n\t\tdebugln(\"log.truncate.before\")\n\t\treturn fmt.Errorf(\"raft.Log: Index is already committed (%v): (IDX=%v, TERM=%v)\", l.commitIndex, index, term)\n\t}\n\n\t// Do not truncate past end of entries.\n\tif index > l.startIndex+uint64(len(l.entries)) {\n\t\tdebugln(\"log.truncate.after\")\n\t\treturn fmt.Errorf(\"raft.Log: Entry index does not exist (MAX=%v): (IDX=%v, TERM=%v)\", len(l.entries), index, term)\n\t}\n\n\t// If we're truncating everything then just clear the entries.\n\tif index == l.startIndex {\n\t\tdebugln(\"log.truncate.clear\")\n\t\tl.file.Truncate(0)\n\t\tl.file.Seek(0, os.SEEK_SET)\n\n\t\t// notify clients if this node is the previous leader\n\t\tfor _, entry := range l.entries {\n\t\t\tif entry.event != nil {\n\t\t\t\tentry.event.c <- errors.New(\"command failed to be committed due to node failure\")\n\t\t\t}\n\t\t}\n\n\t\tl.entries = []*LogEntry{}\n\t} else {\n\t\t// Do not truncate if the entry at index does not have the matching term.\n\t\tentry := l.entries[index-l.startIndex-1]\n\t\tif len(l.entries) > 0 && entry.Term() != term {\n\t\t\tdebugln(\"log.truncate.termMismatch\")\n\t\t\treturn fmt.Errorf(\"raft.Log: Entry at index does not have matching term (%v): (IDX=%v, TERM=%v)\", entry.Term(), index, term)\n\t\t}\n\n\t\t// Otherwise truncate up to the desired entry.\n\t\tif index < l.startIndex+uint64(len(l.entries)) {\n\t\t\tdebugln(\"log.truncate.finish\")\n\t\t\tposition := l.entries[index-l.startIndex].Position\n\t\t\tl.file.Truncate(position)\n\t\t\tl.file.Seek(position, os.SEEK_SET)\n\n\t\t\t// notify clients if this node is the previous leader\n\t\t\tfor i := index - l.startIndex; i < uint64(len(l.entries)); i++ {\n\t\t\t\tentry := l.entries[i]\n\t\t\t\tif entry.event != nil {\n\t\t\t\t\tentry.event.c <- errors.New(\"command failed to be committed due to node failure\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tl.entries = l.entries[0 : index-l.startIndex]\n\t\t}\n\t}\n\n\treturn nil\n}\n\n//--------------------------------------\n// Append\n//--------------------------------------\n\n// Appends a series of entries to the log.\nfunc (l *Log) appendEntries(entries []*protobuf.LogEntry) error {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tstartPosition, _ := l.file.Seek(0, os.SEEK_CUR)\n\n\tw := bufio.NewWriter(l.file)\n\n\tvar size int64\n\tvar err error\n\t// Append each entry but exit if we hit an error.\n\tfor i := range entries {\n\t\tlogEntry := &LogEntry{\n\t\t\tlog:      l,\n\t\t\tPosition: startPosition,\n\t\t\tpb:       entries[i],\n\t\t}\n\n\t\tif size, err = l.writeEntry(logEntry, w); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tstartPosition += size\n\t}\n\tw.Flush()\n\terr = l.sync()\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn nil\n}\n\n// Writes a single log entry to the end of the log.\nfunc (l *Log) appendEntry(entry *LogEntry) error {\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tif l.file == nil {\n\t\treturn errors.New(\"raft.Log: Log is not open\")\n\t}\n\n\t// Make sure the term and index are greater than the previous.\n\tif len(l.entries) > 0 {\n\t\tlastEntry := l.entries[len(l.entries)-1]\n\t\tif entry.Term() < lastEntry.Term() {\n\t\t\treturn fmt.Errorf(\"raft.Log: Cannot append entry with earlier term (%x:%x <= %x:%x)\", entry.Term(), entry.Index(), lastEntry.Term(), lastEntry.Index())\n\t\t} else if entry.Term() == lastEntry.Term() && entry.Index() <= lastEntry.Index() {\n\t\t\treturn fmt.Errorf(\"raft.Log: Cannot append entry with earlier index in the same term (%x:%x <= %x:%x)\", entry.Term(), entry.Index(), lastEntry.Term(), lastEntry.Index())\n\t\t}\n\t}\n\n\tposition, _ := l.file.Seek(0, os.SEEK_CUR)\n\n\tentry.Position = position\n\n\t// Write to storage.\n\tif _, err := entry.Encode(l.file); err != nil {\n\t\treturn err\n\t}\n\n\t// Append to entries list if stored on disk.\n\tl.entries = append(l.entries, entry)\n\n\treturn nil\n}\n\n// appendEntry with Buffered io\nfunc (l *Log) writeEntry(entry *LogEntry, w io.Writer) (int64, error) {\n\tif l.file == nil {\n\t\treturn -1, errors.New(\"raft.Log: Log is not open\")\n\t}\n\n\t// Make sure the term and index are greater than the previous.\n\tif len(l.entries) > 0 {\n\t\tlastEntry := l.entries[len(l.entries)-1]\n\t\tif entry.Term() < lastEntry.Term() {\n\t\t\treturn -1, fmt.Errorf(\"raft.Log: Cannot append entry with earlier term (%x:%x <= %x:%x)\", entry.Term(), entry.Index(), lastEntry.Term(), lastEntry.Index())\n\t\t} else if entry.Term() == lastEntry.Term() && entry.Index() <= lastEntry.Index() {\n\t\t\treturn -1, fmt.Errorf(\"raft.Log: Cannot append entry with earlier index in the same term (%x:%x <= %x:%x)\", entry.Term(), entry.Index(), lastEntry.Term(), lastEntry.Index())\n\t\t}\n\t}\n\n\t// Write to storage.\n\tsize, err := entry.Encode(w)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\t// Append to entries list if stored on disk.\n\tl.entries = append(l.entries, entry)\n\n\treturn int64(size), nil\n}\n\n//--------------------------------------\n// Log compaction\n//--------------------------------------\n\n// compact the log before index (including index)\nfunc (l *Log) compact(index uint64, term uint64) error {\n\tvar entries []*LogEntry\n\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tif index == 0 {\n\t\treturn nil\n\t}\n\t// nothing to compaction\n\t// the index may be greater than the current index if\n\t// we just recovery from on snapshot\n\tif index >= l.internalCurrentIndex() {\n\t\tentries = make([]*LogEntry, 0)\n\t} else {\n\t\t// get all log entries after index\n\t\tentries = l.entries[index-l.startIndex:]\n\t}\n\n\t// create a new log file and add all the entries\n\tnew_file_path := l.path + \".new\"\n\tfile, err := os.OpenFile(new_file_path, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, entry := range entries {\n\t\tposition, _ := l.file.Seek(0, os.SEEK_CUR)\n\t\tentry.Position = position\n\n\t\tif _, err = entry.Encode(file); err != nil {\n\t\t\tfile.Close()\n\t\t\tos.Remove(new_file_path)\n\t\t\treturn err\n\t\t}\n\t}\n\tfile.Sync()\n\n\told_file := l.file\n\n\t// rename the new log file\n\terr = os.Rename(new_file_path, l.path)\n\tif err != nil {\n\t\tfile.Close()\n\t\tos.Remove(new_file_path)\n\t\treturn err\n\t}\n\tl.file = file\n\n\t// close the old log file\n\told_file.Close()\n\n\t// compaction the in memory log\n\tl.entries = entries\n\tl.startIndex = index\n\tl.startTerm = term\n\treturn nil\n}\n"
        },
        {
          "name": "log_entry.go",
          "type": "blob",
          "size": 2.076171875,
          "content": "package raft\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"code.google.com/p/gogoprotobuf/proto\"\n\t\"github.com/goraft/raft/protobuf\"\n)\n\n// A log entry stores a single item in the log.\ntype LogEntry struct {\n\tpb       *protobuf.LogEntry\n\tPosition int64 // position in the log file\n\tlog      *Log\n\tevent    *ev\n}\n\n// Creates a new log entry associated with a log.\nfunc newLogEntry(log *Log, event *ev, index uint64, term uint64, command Command) (*LogEntry, error) {\n\tvar buf bytes.Buffer\n\tvar commandName string\n\tif command != nil {\n\t\tcommandName = command.CommandName()\n\t\tif encoder, ok := command.(CommandEncoder); ok {\n\t\t\tif err := encoder.Encode(&buf); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := json.NewEncoder(&buf).Encode(command); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tpb := &protobuf.LogEntry{\n\t\tIndex:       proto.Uint64(index),\n\t\tTerm:        proto.Uint64(term),\n\t\tCommandName: proto.String(commandName),\n\t\tCommand:     buf.Bytes(),\n\t}\n\n\te := &LogEntry{\n\t\tpb:    pb,\n\t\tlog:   log,\n\t\tevent: event,\n\t}\n\n\treturn e, nil\n}\n\nfunc (e *LogEntry) Index() uint64 {\n\treturn e.pb.GetIndex()\n}\n\nfunc (e *LogEntry) Term() uint64 {\n\treturn e.pb.GetTerm()\n}\n\nfunc (e *LogEntry) CommandName() string {\n\treturn e.pb.GetCommandName()\n}\n\nfunc (e *LogEntry) Command() []byte {\n\treturn e.pb.GetCommand()\n}\n\n// Encodes the log entry to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (e *LogEntry) Encode(w io.Writer) (int, error) {\n\tb, err := proto.Marshal(e.pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\tif _, err = fmt.Fprintf(w, \"%8x\\n\", len(b)); err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(b)\n}\n\n// Decodes the log entry from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (e *LogEntry) Decode(r io.Reader) (int, error) {\n\n\tvar length int\n\t_, err := fmt.Fscanf(r, \"%8x\\n\", &length)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\tdata := make([]byte, length)\n\t_, err = io.ReadFull(r, data)\n\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\tif err = proto.Unmarshal(data, e.pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn length + 8 + 1, nil\n}\n"
        },
        {
          "name": "log_test.go",
          "type": "blob",
          "size": 7.453125,
          "content": "package raft\n\nimport (\n\t\"io/ioutil\"\n\t\"os\"\n\t\"reflect\"\n\t\"testing\"\n)\n\n//------------------------------------------------------------------------------\n//\n// Tests\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Append\n//--------------------------------------\n\n// Ensure that we can append to a new log.\nfunc TestLogNewLog(t *testing.T) {\n\tpath := getLogPath()\n\tlog := newLog()\n\tlog.ApplyFunc = func(e *LogEntry, c Command) (interface{}, error) {\n\t\treturn nil, nil\n\t}\n\tif err := log.open(path); err != nil {\n\t\tt.Fatalf(\"Unable to open log: %v\", err)\n\t}\n\tdefer log.close()\n\tdefer os.Remove(path)\n\n\te, _ := newLogEntry(log, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\tif err := log.appendEntry(e); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\te, _ = newLogEntry(log, nil, 2, 1, &testCommand2{X: 100})\n\tif err := log.appendEntry(e); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\te, _ = newLogEntry(log, nil, 3, 2, &testCommand1{Val: \"bar\", I: 0})\n\tif err := log.appendEntry(e); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\n\t// Partial commit.\n\tif err := log.setCommitIndex(2); err != nil {\n\t\tt.Fatalf(\"Unable to partially commit: %v\", err)\n\t}\n\tif index, term := log.commitInfo(); index != 2 || term != 1 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n\n\t// Full commit.\n\tif err := log.setCommitIndex(3); err != nil {\n\t\tt.Fatalf(\"Unable to commit: %v\", err)\n\t}\n\tif index, term := log.commitInfo(); index != 3 || term != 2 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n}\n\n// Ensure that we can decode and encode to an existing log.\nfunc TestLogExistingLog(t *testing.T) {\n\ttmpLog := newLog()\n\te0, _ := newLogEntry(tmpLog, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\te1, _ := newLogEntry(tmpLog, nil, 2, 1, &testCommand2{X: 100})\n\te2, _ := newLogEntry(tmpLog, nil, 3, 2, &testCommand1{Val: \"bar\", I: 0})\n\tlog, path := setupLog([]*LogEntry{e0, e1, e2})\n\tdefer log.close()\n\tdefer os.Remove(path)\n\n\t// Validate existing log entries.\n\tif len(log.entries) != 3 {\n\t\tt.Fatalf(\"Expected 3 entries, got %d\", len(log.entries))\n\t}\n\tif log.entries[0].Index() != 1 || log.entries[0].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[0]: %v\", log.entries[0])\n\t}\n\tif log.entries[1].Index() != 2 || log.entries[1].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[1]: %v\", log.entries[1])\n\t}\n\tif log.entries[2].Index() != 3 || log.entries[2].Term() != 2 {\n\t\tt.Fatalf(\"Unexpected entry[2]: %v\", log.entries[2])\n\t}\n}\n\n// Ensure that we can check the contents of the log by index/term.\nfunc TestLogContainsEntries(t *testing.T) {\n\ttmpLog := newLog()\n\te0, _ := newLogEntry(tmpLog, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\te1, _ := newLogEntry(tmpLog, nil, 2, 1, &testCommand2{X: 100})\n\te2, _ := newLogEntry(tmpLog, nil, 3, 2, &testCommand1{Val: \"bar\", I: 0})\n\tlog, path := setupLog([]*LogEntry{e0, e1, e2})\n\tdefer log.close()\n\tdefer os.Remove(path)\n\n\tif log.containsEntry(0, 0) {\n\t\tt.Fatalf(\"Zero-index entry should not exist in log.\")\n\t}\n\tif log.containsEntry(1, 0) {\n\t\tt.Fatalf(\"Entry with mismatched term should not exist\")\n\t}\n\tif log.containsEntry(4, 0) {\n\t\tt.Fatalf(\"Out-of-range entry should not exist\")\n\t}\n\tif !log.containsEntry(2, 1) {\n\t\tt.Fatalf(\"Entry 2/1 should exist\")\n\t}\n\tif !log.containsEntry(3, 2) {\n\t\tt.Fatalf(\"Entry 2/1 should exist\")\n\t}\n}\n\n// Ensure that we can recover from an incomplete/corrupt log and continue logging.\nfunc TestLogRecovery(t *testing.T) {\n\ttmpLog := newLog()\n\te0, _ := newLogEntry(tmpLog, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\te1, _ := newLogEntry(tmpLog, nil, 2, 1, &testCommand2{X: 100})\n\tf, _ := ioutil.TempFile(\"\", \"raft-log-\")\n\n\te0.Encode(f)\n\te1.Encode(f)\n\tf.WriteString(\"CORRUPT!\")\n\tf.Close()\n\n\tlog := newLog()\n\tlog.ApplyFunc = func(e *LogEntry, c Command) (interface{}, error) {\n\t\treturn nil, nil\n\t}\n\tif err := log.open(f.Name()); err != nil {\n\t\tt.Fatalf(\"Unable to open log: %v\", err)\n\t}\n\tdefer log.close()\n\tdefer os.Remove(f.Name())\n\n\te, _ := newLogEntry(log, nil, 3, 2, &testCommand1{Val: \"bat\", I: -5})\n\tif err := log.appendEntry(e); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\n\t// Validate existing log entries.\n\tif len(log.entries) != 3 {\n\t\tt.Fatalf(\"Expected 3 entries, got %d\", len(log.entries))\n\t}\n\tif log.entries[0].Index() != 1 || log.entries[0].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[0]: %v\", log.entries[0])\n\t}\n\tif log.entries[1].Index() != 2 || log.entries[1].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[1]: %v\", log.entries[1])\n\t}\n\tif log.entries[2].Index() != 3 || log.entries[2].Term() != 2 {\n\t\tt.Fatalf(\"Unexpected entry[2]: %v\", log.entries[2])\n\t}\n}\n\n//--------------------------------------\n// Append\n//--------------------------------------\n\n// Ensure that we can truncate uncommitted entries in the log.\nfunc TestLogTruncate(t *testing.T) {\n\tlog, path := setupLog(nil)\n\tif err := log.open(path); err != nil {\n\t\tt.Fatalf(\"Unable to open log: %v\", err)\n\t}\n\n\tdefer os.Remove(path)\n\n\tentry1, _ := newLogEntry(log, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\tif err := log.appendEntry(entry1); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\tentry2, _ := newLogEntry(log, nil, 2, 1, &testCommand2{X: 100})\n\tif err := log.appendEntry(entry2); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\tentry3, _ := newLogEntry(log, nil, 3, 2, &testCommand1{Val: \"bar\", I: 0})\n\tif err := log.appendEntry(entry3); err != nil {\n\t\tt.Fatalf(\"Unable to append: %v\", err)\n\t}\n\tif err := log.setCommitIndex(2); err != nil {\n\t\tt.Fatalf(\"Unable to partially commit: %v\", err)\n\t}\n\n\t// Truncate committed entry.\n\tif err := log.truncate(1, 1); err == nil || err.Error() != \"raft.Log: Index is already committed (2): (IDX=1, TERM=1)\" {\n\t\tt.Fatalf(\"Truncating committed entries shouldn't work: %v\", err)\n\t}\n\t// Truncate past end of log.\n\tif err := log.truncate(4, 2); err == nil || err.Error() != \"raft.Log: Entry index does not exist (MAX=3): (IDX=4, TERM=2)\" {\n\t\tt.Fatalf(\"Truncating past end-of-log shouldn't work: %v\", err)\n\t}\n\t// Truncate entry with mismatched term.\n\tif err := log.truncate(2, 2); err == nil || err.Error() != \"raft.Log: Entry at index does not have matching term (1): (IDX=2, TERM=2)\" {\n\t\tt.Fatalf(\"Truncating mismatched entries shouldn't work: %v\", err)\n\t}\n\t// Truncate end of log.\n\tif err := log.truncate(3, 2); !(err == nil && reflect.DeepEqual(log.entries, []*LogEntry{entry1, entry2, entry3})) {\n\t\tt.Fatalf(\"Truncating end of log should work: %v\\n\\nEntries:\\nActual: %v\\nExpected: %v\", err, log.entries, []*LogEntry{entry1, entry2, entry3})\n\t}\n\t// Truncate at last commit.\n\tif err := log.truncate(2, 1); !(err == nil && reflect.DeepEqual(log.entries, []*LogEntry{entry1, entry2})) {\n\t\tt.Fatalf(\"Truncating at last commit should work: %v\\n\\nEntries:\\nActual: %v\\nExpected: %v\", err, log.entries, []*LogEntry{entry1, entry2})\n\t}\n\n\t// Append after truncate\n\tif err := log.appendEntry(entry3); err != nil {\n\t\tt.Fatalf(\"Unable to append after truncate: %v\", err)\n\t}\n\n\tlog.close()\n\n\t// Recovery the truncated log\n\tlog = newLog()\n\tif err := log.open(path); err != nil {\n\t\tt.Fatalf(\"Unable to open log: %v\", err)\n\t}\n\t// Validate existing log entries.\n\tif len(log.entries) != 3 {\n\t\tt.Fatalf(\"Expected 3 entries, got %d\", len(log.entries))\n\t}\n\tif log.entries[0].Index() != 1 || log.entries[0].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[0]: %v\", log.entries[0])\n\t}\n\tif log.entries[1].Index() != 2 || log.entries[1].Term() != 1 {\n\t\tt.Fatalf(\"Unexpected entry[1]: %v\", log.entries[1])\n\t}\n\tif log.entries[2].Index() != 3 || log.entries[2].Term() != 2 {\n\t\tt.Fatalf(\"Unexpected entry[2]: %v\", log.entries[2])\n\t}\n}\n"
        },
        {
          "name": "peer.go",
          "type": "blob",
          "size": 8.66015625,
          "content": "package raft\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\n//------------------------------------------------------------------------------\n//\n// Typedefs\n//\n//------------------------------------------------------------------------------\n\n// A peer is a reference to another server involved in the consensus protocol.\ntype Peer struct {\n\tserver            *server\n\tName              string `json:\"name\"`\n\tConnectionString  string `json:\"connectionString\"`\n\tprevLogIndex      uint64\n\tstopChan          chan bool\n\theartbeatInterval time.Duration\n\tlastActivity      time.Time\n\tsync.RWMutex\n}\n\n//------------------------------------------------------------------------------\n//\n// Constructor\n//\n//------------------------------------------------------------------------------\n\n// Creates a new peer.\nfunc newPeer(server *server, name string, connectionString string, heartbeatInterval time.Duration) *Peer {\n\treturn &Peer{\n\t\tserver:            server,\n\t\tName:              name,\n\t\tConnectionString:  connectionString,\n\t\theartbeatInterval: heartbeatInterval,\n\t}\n}\n\n//------------------------------------------------------------------------------\n//\n// Accessors\n//\n//------------------------------------------------------------------------------\n\n// Sets the heartbeat timeout.\nfunc (p *Peer) setHeartbeatInterval(duration time.Duration) {\n\tp.heartbeatInterval = duration\n}\n\n//--------------------------------------\n// Prev log index\n//--------------------------------------\n\n// Retrieves the previous log index.\nfunc (p *Peer) getPrevLogIndex() uint64 {\n\tp.RLock()\n\tdefer p.RUnlock()\n\treturn p.prevLogIndex\n}\n\n// Sets the previous log index.\nfunc (p *Peer) setPrevLogIndex(value uint64) {\n\tp.Lock()\n\tdefer p.Unlock()\n\tp.prevLogIndex = value\n}\n\nfunc (p *Peer) setLastActivity(now time.Time) {\n\tp.Lock()\n\tdefer p.Unlock()\n\tp.lastActivity = now\n}\n\n//------------------------------------------------------------------------------\n//\n// Methods\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Heartbeat\n//--------------------------------------\n\n// Starts the peer heartbeat.\nfunc (p *Peer) startHeartbeat() {\n\tp.stopChan = make(chan bool)\n\tc := make(chan bool)\n\n\tp.setLastActivity(time.Now())\n\n\tp.server.routineGroup.Add(1)\n\tgo func() {\n\t\tdefer p.server.routineGroup.Done()\n\t\tp.heartbeat(c)\n\t}()\n\t<-c\n}\n\n// Stops the peer heartbeat.\nfunc (p *Peer) stopHeartbeat(flush bool) {\n\tp.setLastActivity(time.Time{})\n\n\tp.stopChan <- flush\n}\n\n// LastActivity returns the last time any response was received from the peer.\nfunc (p *Peer) LastActivity() time.Time {\n\tp.RLock()\n\tdefer p.RUnlock()\n\treturn p.lastActivity\n}\n\n//--------------------------------------\n// Copying\n//--------------------------------------\n\n// Clones the state of the peer. The clone is not attached to a server and\n// the heartbeat timer will not exist.\nfunc (p *Peer) clone() *Peer {\n\tp.Lock()\n\tdefer p.Unlock()\n\treturn &Peer{\n\t\tName:             p.Name,\n\t\tConnectionString: p.ConnectionString,\n\t\tprevLogIndex:     p.prevLogIndex,\n\t\tlastActivity:     p.lastActivity,\n\t}\n}\n\n//--------------------------------------\n// Heartbeat\n//--------------------------------------\n\n// Listens to the heartbeat timeout and flushes an AppendEntries RPC.\nfunc (p *Peer) heartbeat(c chan bool) {\n\tstopChan := p.stopChan\n\n\tc <- true\n\n\tticker := time.Tick(p.heartbeatInterval)\n\n\tdebugln(\"peer.heartbeat: \", p.Name, p.heartbeatInterval)\n\n\tfor {\n\t\tselect {\n\t\tcase flush := <-stopChan:\n\t\t\tif flush {\n\t\t\t\t// before we can safely remove a node\n\t\t\t\t// we must flush the remove command to the node first\n\t\t\t\tp.flush()\n\t\t\t\tdebugln(\"peer.heartbeat.stop.with.flush: \", p.Name)\n\t\t\t\treturn\n\t\t\t} else {\n\t\t\t\tdebugln(\"peer.heartbeat.stop: \", p.Name)\n\t\t\t\treturn\n\t\t\t}\n\n\t\tcase <-ticker:\n\t\t\tstart := time.Now()\n\t\t\tp.flush()\n\t\t\tduration := time.Now().Sub(start)\n\t\t\tp.server.DispatchEvent(newEvent(HeartbeatEventType, duration, nil))\n\t\t}\n\t}\n}\n\nfunc (p *Peer) flush() {\n\tdebugln(\"peer.heartbeat.flush: \", p.Name)\n\tprevLogIndex := p.getPrevLogIndex()\n\tterm := p.server.currentTerm\n\n\tentries, prevLogTerm := p.server.log.getEntriesAfter(prevLogIndex, p.server.maxLogEntriesPerRequest)\n\n\tif entries != nil {\n\t\tp.sendAppendEntriesRequest(newAppendEntriesRequest(term, prevLogIndex, prevLogTerm, p.server.log.CommitIndex(), p.server.name, entries))\n\t} else {\n\t\tp.sendSnapshotRequest(newSnapshotRequest(p.server.name, p.server.snapshot))\n\t}\n}\n\n//--------------------------------------\n// Append Entries\n//--------------------------------------\n\n// Sends an AppendEntries request to the peer through the transport.\nfunc (p *Peer) sendAppendEntriesRequest(req *AppendEntriesRequest) {\n\ttracef(\"peer.append.send: %s->%s [prevLog:%v length: %v]\\n\",\n\t\tp.server.Name(), p.Name, req.PrevLogIndex, len(req.Entries))\n\n\tresp := p.server.Transporter().SendAppendEntriesRequest(p.server, p, req)\n\tif resp == nil {\n\t\tp.server.DispatchEvent(newEvent(HeartbeatIntervalEventType, p, nil))\n\t\tdebugln(\"peer.append.timeout: \", p.server.Name(), \"->\", p.Name)\n\t\treturn\n\t}\n\ttraceln(\"peer.append.resp: \", p.server.Name(), \"<-\", p.Name)\n\n\tp.setLastActivity(time.Now())\n\t// If successful then update the previous log index.\n\tp.Lock()\n\tif resp.Success() {\n\t\tif len(req.Entries) > 0 {\n\t\t\tp.prevLogIndex = req.Entries[len(req.Entries)-1].GetIndex()\n\n\t\t\t// if peer append a log entry from the current term\n\t\t\t// we set append to true\n\t\t\tif req.Entries[len(req.Entries)-1].GetTerm() == p.server.currentTerm {\n\t\t\t\tresp.append = true\n\t\t\t}\n\t\t}\n\t\ttraceln(\"peer.append.resp.success: \", p.Name, \"; idx =\", p.prevLogIndex)\n\t\t// If it was unsuccessful then decrement the previous log index and\n\t\t// we'll try again next time.\n\t} else {\n\t\tif resp.Term() > p.server.Term() {\n\t\t\t// this happens when there is a new leader comes up that this *leader* has not\n\t\t\t// known yet.\n\t\t\t// this server can know until the new leader send a ae with higher term\n\t\t\t// or this server finish processing this response.\n\t\t\tdebugln(\"peer.append.resp.not.update: new.leader.found\")\n\t\t} else if resp.Term() == req.Term && resp.CommitIndex() >= p.prevLogIndex {\n\t\t\t// we may miss a response from peer\n\t\t\t// so maybe the peer has committed the logs we just sent\n\t\t\t// but we did not receive the successful reply and did not increase\n\t\t\t// the prevLogIndex\n\n\t\t\t// peer failed to truncate the log and sent a fail reply at this time\n\t\t\t// we just need to update peer's prevLog index to commitIndex\n\n\t\t\tp.prevLogIndex = resp.CommitIndex()\n\t\t\tdebugln(\"peer.append.resp.update: \", p.Name, \"; idx =\", p.prevLogIndex)\n\n\t\t} else if p.prevLogIndex > 0 {\n\t\t\t// Decrement the previous log index down until we find a match. Don't\n\t\t\t// let it go below where the peer's commit index is though. That's a\n\t\t\t// problem.\n\t\t\tp.prevLogIndex--\n\t\t\t// if it not enough, we directly decrease to the index of the\n\t\t\tif p.prevLogIndex > resp.Index() {\n\t\t\t\tp.prevLogIndex = resp.Index()\n\t\t\t}\n\n\t\t\tdebugln(\"peer.append.resp.decrement: \", p.Name, \"; idx =\", p.prevLogIndex)\n\t\t}\n\t}\n\tp.Unlock()\n\n\t// Attach the peer to resp, thus server can know where it comes from\n\tresp.peer = p.Name\n\t// Send response to server for processing.\n\tp.server.sendAsync(resp)\n}\n\n// Sends an Snapshot request to the peer through the transport.\nfunc (p *Peer) sendSnapshotRequest(req *SnapshotRequest) {\n\tdebugln(\"peer.snap.send: \", p.Name)\n\n\tresp := p.server.Transporter().SendSnapshotRequest(p.server, p, req)\n\tif resp == nil {\n\t\tdebugln(\"peer.snap.timeout: \", p.Name)\n\t\treturn\n\t}\n\n\tdebugln(\"peer.snap.recv: \", p.Name)\n\n\t// If successful, the peer should have been to snapshot state\n\t// Send it the snapshot!\n\tp.setLastActivity(time.Now())\n\n\tif resp.Success {\n\t\tp.sendSnapshotRecoveryRequest()\n\t} else {\n\t\tdebugln(\"peer.snap.failed: \", p.Name)\n\t\treturn\n\t}\n\n}\n\n// Sends an Snapshot Recovery request to the peer through the transport.\nfunc (p *Peer) sendSnapshotRecoveryRequest() {\n\treq := newSnapshotRecoveryRequest(p.server.name, p.server.snapshot)\n\tdebugln(\"peer.snap.recovery.send: \", p.Name)\n\tresp := p.server.Transporter().SendSnapshotRecoveryRequest(p.server, p, req)\n\n\tif resp == nil {\n\t\tdebugln(\"peer.snap.recovery.timeout: \", p.Name)\n\t\treturn\n\t}\n\n\tp.setLastActivity(time.Now())\n\tif resp.Success {\n\t\tp.prevLogIndex = req.LastIndex\n\t} else {\n\t\tdebugln(\"peer.snap.recovery.failed: \", p.Name)\n\t\treturn\n\t}\n\n\tp.server.sendAsync(resp)\n}\n\n//--------------------------------------\n// Vote Requests\n//--------------------------------------\n\n// send VoteRequest Request\nfunc (p *Peer) sendVoteRequest(req *RequestVoteRequest, c chan *RequestVoteResponse) {\n\tdebugln(\"peer.vote: \", p.server.Name(), \"->\", p.Name)\n\treq.peer = p\n\tif resp := p.server.Transporter().SendVoteRequest(p.server, p, req); resp != nil {\n\t\tdebugln(\"peer.vote.recv: \", p.server.Name(), \"<-\", p.Name)\n\t\tp.setLastActivity(time.Now())\n\t\tresp.peer = p\n\t\tc <- resp\n\t} else {\n\t\tdebugln(\"peer.vote.failed: \", p.server.Name(), \"<-\", p.Name)\n\t}\n}\n"
        },
        {
          "name": "protobuf",
          "type": "tree",
          "content": null
        },
        {
          "name": "request_vote.go",
          "type": "blob",
          "size": 2.9296875,
          "content": "package raft\n\nimport (\n\t\"io\"\n\t\"io/ioutil\"\n\n\t\"code.google.com/p/gogoprotobuf/proto\"\n\t\"github.com/goraft/raft/protobuf\"\n)\n\n// The request sent to a server to vote for a candidate to become a leader.\ntype RequestVoteRequest struct {\n\tpeer          *Peer\n\tTerm          uint64\n\tLastLogIndex  uint64\n\tLastLogTerm   uint64\n\tCandidateName string\n}\n\n// The response returned from a server after a vote for a candidate to become a leader.\ntype RequestVoteResponse struct {\n\tpeer        *Peer\n\tTerm        uint64\n\tVoteGranted bool\n}\n\n// Creates a new RequestVote request.\nfunc newRequestVoteRequest(term uint64, candidateName string, lastLogIndex uint64, lastLogTerm uint64) *RequestVoteRequest {\n\treturn &RequestVoteRequest{\n\t\tTerm:          term,\n\t\tLastLogIndex:  lastLogIndex,\n\t\tLastLogTerm:   lastLogTerm,\n\t\tCandidateName: candidateName,\n\t}\n}\n\n// Encodes the RequestVoteRequest to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (req *RequestVoteRequest) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.RequestVoteRequest{\n\t\tTerm:          proto.Uint64(req.Term),\n\t\tLastLogIndex:  proto.Uint64(req.LastLogIndex),\n\t\tLastLogTerm:   proto.Uint64(req.LastLogTerm),\n\t\tCandidateName: proto.String(req.CandidateName),\n\t}\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the RequestVoteRequest from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (req *RequestVoteRequest) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.RequestVoteRequest{}\n\tif err = proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treq.Term = pb.GetTerm()\n\treq.LastLogIndex = pb.GetLastLogIndex()\n\treq.LastLogTerm = pb.GetLastLogTerm()\n\treq.CandidateName = pb.GetCandidateName()\n\n\treturn totalBytes, nil\n}\n\n// Creates a new RequestVote response.\nfunc newRequestVoteResponse(term uint64, voteGranted bool) *RequestVoteResponse {\n\treturn &RequestVoteResponse{\n\t\tTerm:        term,\n\t\tVoteGranted: voteGranted,\n\t}\n}\n\n// Encodes the RequestVoteResponse to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (resp *RequestVoteResponse) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.RequestVoteResponse{\n\t\tTerm:        proto.Uint64(resp.Term),\n\t\tVoteGranted: proto.Bool(resp.VoteGranted),\n\t}\n\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the RequestVoteResponse from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (resp *RequestVoteResponse) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.RequestVoteResponse{}\n\tif err = proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\tresp.Term = pb.GetTerm()\n\tresp.VoteGranted = pb.GetVoteGranted()\n\n\treturn totalBytes, nil\n}\n"
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 39.0849609375,
          "content": "package raft\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n)\n\n//------------------------------------------------------------------------------\n//\n// Constants\n//\n//------------------------------------------------------------------------------\n\nconst (\n\tStopped      = \"stopped\"\n\tInitialized  = \"initialized\"\n\tFollower     = \"follower\"\n\tCandidate    = \"candidate\"\n\tLeader       = \"leader\"\n\tSnapshotting = \"snapshotting\"\n)\n\nconst (\n\tMaxLogEntriesPerRequest         = 2000\n\tNumberOfLogEntriesAfterSnapshot = 200\n)\n\nconst (\n\t// DefaultHeartbeatInterval is the interval that the leader will send\n\t// AppendEntriesRequests to followers to maintain leadership.\n\tDefaultHeartbeatInterval = 50 * time.Millisecond\n\n\tDefaultElectionTimeout = 150 * time.Millisecond\n)\n\n// ElectionTimeoutThresholdPercent specifies the threshold at which the server\n// will dispatch warning events that the heartbeat RTT is too close to the\n// election timeout.\nconst ElectionTimeoutThresholdPercent = 0.8\n\n//------------------------------------------------------------------------------\n//\n// Errors\n//\n//------------------------------------------------------------------------------\n\nvar NotLeaderError = errors.New(\"raft.Server: Not current leader\")\nvar DuplicatePeerError = errors.New(\"raft.Server: Duplicate peer\")\nvar CommandTimeoutError = errors.New(\"raft: Command timeout\")\nvar StopError = errors.New(\"raft: Has been stopped\")\n\n//------------------------------------------------------------------------------\n//\n// Typedefs\n//\n//------------------------------------------------------------------------------\n\n// A server is involved in the consensus protocol and can act as a follower,\n// candidate or a leader.\ntype Server interface {\n\tName() string\n\tContext() interface{}\n\tStateMachine() StateMachine\n\tLeader() string\n\tState() string\n\tPath() string\n\tLogPath() string\n\tSnapshotPath(lastIndex uint64, lastTerm uint64) string\n\tTerm() uint64\n\tCommitIndex() uint64\n\tVotedFor() string\n\tMemberCount() int\n\tQuorumSize() int\n\tIsLogEmpty() bool\n\tLogEntries() []*LogEntry\n\tLastCommandName() string\n\tGetState() string\n\tElectionTimeout() time.Duration\n\tSetElectionTimeout(duration time.Duration)\n\tHeartbeatInterval() time.Duration\n\tSetHeartbeatInterval(duration time.Duration)\n\tTransporter() Transporter\n\tSetTransporter(t Transporter)\n\tAppendEntries(req *AppendEntriesRequest) *AppendEntriesResponse\n\tRequestVote(req *RequestVoteRequest) *RequestVoteResponse\n\tRequestSnapshot(req *SnapshotRequest) *SnapshotResponse\n\tSnapshotRecoveryRequest(req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse\n\tAddPeer(name string, connectiongString string) error\n\tRemovePeer(name string) error\n\tPeers() map[string]*Peer\n\tInit() error\n\tStart() error\n\tStop()\n\tRunning() bool\n\tDo(command Command) (interface{}, error)\n\tTakeSnapshot() error\n\tLoadSnapshot() error\n\tAddEventListener(string, EventListener)\n\tFlushCommitIndex()\n}\n\ntype server struct {\n\t*eventDispatcher\n\n\tname        string\n\tpath        string\n\tstate       string\n\ttransporter Transporter\n\tcontext     interface{}\n\tcurrentTerm uint64\n\n\tvotedFor   string\n\tlog        *Log\n\tleader     string\n\tpeers      map[string]*Peer\n\tmutex      sync.RWMutex\n\tsyncedPeer map[string]bool\n\n\tstopped           chan bool\n\tc                 chan *ev\n\telectionTimeout   time.Duration\n\theartbeatInterval time.Duration\n\n\tsnapshot *Snapshot\n\n\t// PendingSnapshot is an unfinished snapshot.\n\t// After the pendingSnapshot is saved to disk,\n\t// it will be set to snapshot and also will be\n\t// set to nil.\n\tpendingSnapshot *Snapshot\n\n\tstateMachine            StateMachine\n\tmaxLogEntriesPerRequest uint64\n\n\tconnectionString string\n\n\troutineGroup sync.WaitGroup\n}\n\n// An internal event to be processed by the server's event loop.\ntype ev struct {\n\ttarget      interface{}\n\treturnValue interface{}\n\tc           chan error\n}\n\n//------------------------------------------------------------------------------\n//\n// Constructor\n//\n//------------------------------------------------------------------------------\n\n// Creates a new server with a log at the given path. transporter must\n// not be nil. stateMachine can be nil if snapshotting and log\n// compaction is to be disabled. context can be anything (including nil)\n// and is not used by the raft package except returned by\n// Server.Context(). connectionString can be anything.\nfunc NewServer(name string, path string, transporter Transporter, stateMachine StateMachine, ctx interface{}, connectionString string) (Server, error) {\n\tif name == \"\" {\n\t\treturn nil, errors.New(\"raft.Server: Name cannot be blank\")\n\t}\n\tif transporter == nil {\n\t\tpanic(\"raft: Transporter required\")\n\t}\n\n\ts := &server{\n\t\tname:                    name,\n\t\tpath:                    path,\n\t\ttransporter:             transporter,\n\t\tstateMachine:            stateMachine,\n\t\tcontext:                 ctx,\n\t\tstate:                   Stopped,\n\t\tpeers:                   make(map[string]*Peer),\n\t\tlog:                     newLog(),\n\t\tc:                       make(chan *ev, 256),\n\t\telectionTimeout:         DefaultElectionTimeout,\n\t\theartbeatInterval:       DefaultHeartbeatInterval,\n\t\tmaxLogEntriesPerRequest: MaxLogEntriesPerRequest,\n\t\tconnectionString:        connectionString,\n\t}\n\ts.eventDispatcher = newEventDispatcher(s)\n\n\t// Setup apply function.\n\ts.log.ApplyFunc = func(e *LogEntry, c Command) (interface{}, error) {\n\t\t// Dispatch commit event.\n\t\ts.DispatchEvent(newEvent(CommitEventType, e, nil))\n\n\t\t// Apply command to the state machine.\n\t\tswitch c := c.(type) {\n\t\tcase CommandApply:\n\t\t\treturn c.Apply(&context{\n\t\t\t\tserver:       s,\n\t\t\t\tcurrentTerm:  s.currentTerm,\n\t\t\t\tcurrentIndex: s.log.internalCurrentIndex(),\n\t\t\t\tcommitIndex:  s.log.commitIndex,\n\t\t\t})\n\t\tcase deprecatedCommandApply:\n\t\t\treturn c.Apply(s)\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"Command does not implement Apply()\")\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\n//------------------------------------------------------------------------------\n//\n// Accessors\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// General\n//--------------------------------------\n\n// Retrieves the name of the server.\nfunc (s *server) Name() string {\n\treturn s.name\n}\n\n// Retrieves the storage path for the server.\nfunc (s *server) Path() string {\n\treturn s.path\n}\n\n// The name of the current leader.\nfunc (s *server) Leader() string {\n\treturn s.leader\n}\n\n// Retrieves a copy of the peer data.\nfunc (s *server) Peers() map[string]*Peer {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\n\tpeers := make(map[string]*Peer)\n\tfor name, peer := range s.peers {\n\t\tpeers[name] = peer.clone()\n\t}\n\treturn peers\n}\n\n// Retrieves the object that transports requests.\nfunc (s *server) Transporter() Transporter {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn s.transporter\n}\n\nfunc (s *server) SetTransporter(t Transporter) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.transporter = t\n}\n\n// Retrieves the context passed into the constructor.\nfunc (s *server) Context() interface{} {\n\treturn s.context\n}\n\n// Retrieves the state machine passed into the constructor.\nfunc (s *server) StateMachine() StateMachine {\n\treturn s.stateMachine\n}\n\n// Retrieves the log path for the server.\nfunc (s *server) LogPath() string {\n\treturn path.Join(s.path, \"log\")\n}\n\n// Retrieves the current state of the server.\nfunc (s *server) State() string {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn s.state\n}\n\n// Sets the state of the server.\nfunc (s *server) setState(state string) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\n\t// Temporarily store previous values.\n\tprevState := s.state\n\tprevLeader := s.leader\n\n\t// Update state and leader.\n\ts.state = state\n\tif state == Leader {\n\t\ts.leader = s.Name()\n\t\ts.syncedPeer = make(map[string]bool)\n\t}\n\n\t// Dispatch state and leader change events.\n\ts.DispatchEvent(newEvent(StateChangeEventType, s.state, prevState))\n\n\tif prevLeader != s.leader {\n\t\ts.DispatchEvent(newEvent(LeaderChangeEventType, s.leader, prevLeader))\n\t}\n}\n\n// Retrieves the current term of the server.\nfunc (s *server) Term() uint64 {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn s.currentTerm\n}\n\n// Retrieves the current commit index of the server.\nfunc (s *server) CommitIndex() uint64 {\n\ts.log.mutex.RLock()\n\tdefer s.log.mutex.RUnlock()\n\treturn s.log.commitIndex\n}\n\n// Retrieves the name of the candidate this server voted for in this term.\nfunc (s *server) VotedFor() string {\n\treturn s.votedFor\n}\n\n// Retrieves whether the server's log has no entries.\nfunc (s *server) IsLogEmpty() bool {\n\treturn s.log.isEmpty()\n}\n\n// A list of all the log entries. This should only be used for debugging purposes.\nfunc (s *server) LogEntries() []*LogEntry {\n\ts.log.mutex.RLock()\n\tdefer s.log.mutex.RUnlock()\n\treturn s.log.entries\n}\n\n// A reference to the command name of the last entry.\nfunc (s *server) LastCommandName() string {\n\treturn s.log.lastCommandName()\n}\n\n// Get the state of the server for debugging\nfunc (s *server) GetState() string {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn fmt.Sprintf(\"Name: %s, State: %s, Term: %v, CommitedIndex: %v \", s.name, s.state, s.currentTerm, s.log.commitIndex)\n}\n\n// Check if the server is promotable\nfunc (s *server) promotable() bool {\n\treturn s.log.currentIndex() > 0\n}\n\n//--------------------------------------\n// Membership\n//--------------------------------------\n\n// Retrieves the number of member servers in the consensus.\nfunc (s *server) MemberCount() int {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn len(s.peers) + 1\n}\n\n// Retrieves the number of servers required to make a quorum.\nfunc (s *server) QuorumSize() int {\n\treturn (s.MemberCount() / 2) + 1\n}\n\n//--------------------------------------\n// Election timeout\n//--------------------------------------\n\n// Retrieves the election timeout.\nfunc (s *server) ElectionTimeout() time.Duration {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn s.electionTimeout\n}\n\n// Sets the election timeout.\nfunc (s *server) SetElectionTimeout(duration time.Duration) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.electionTimeout = duration\n}\n\n//--------------------------------------\n// Heartbeat timeout\n//--------------------------------------\n\n// Retrieves the heartbeat timeout.\nfunc (s *server) HeartbeatInterval() time.Duration {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn s.heartbeatInterval\n}\n\n// Sets the heartbeat timeout.\nfunc (s *server) SetHeartbeatInterval(duration time.Duration) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\n\ts.heartbeatInterval = duration\n\tfor _, peer := range s.peers {\n\t\tpeer.setHeartbeatInterval(duration)\n\t}\n}\n\n//------------------------------------------------------------------------------\n//\n// Methods\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Initialization\n//--------------------------------------\n\n// Reg the NOPCommand\nfunc init() {\n\tRegisterCommand(&NOPCommand{})\n\tRegisterCommand(&DefaultJoinCommand{})\n\tRegisterCommand(&DefaultLeaveCommand{})\n}\n\n// Start the raft server\n// If log entries exist then allow promotion to candidate if no AEs received.\n// If no log entries exist then wait for AEs from another node.\n// If no log entries exist and a self-join command is issued then\n// immediately become leader and commit entry.\nfunc (s *server) Start() error {\n\t// Exit if the server is already running.\n\tif s.Running() {\n\t\treturn fmt.Errorf(\"raft.Server: Server already running[%v]\", s.state)\n\t}\n\n\tif err := s.Init(); err != nil {\n\t\treturn err\n\t}\n\n\t// stopped needs to be allocated each time server starts\n\t// because it is closed at `Stop`.\n\ts.stopped = make(chan bool)\n\ts.setState(Follower)\n\n\t// If no log entries exist then\n\t// 1. wait for AEs from another node\n\t// 2. wait for self-join command\n\t// to set itself promotable\n\tif !s.promotable() {\n\t\ts.debugln(\"start as a new raft server\")\n\n\t\t// If log entries exist then allow promotion to candidate\n\t\t// if no AEs received.\n\t} else {\n\t\ts.debugln(\"start from previous saved state\")\n\t}\n\n\tdebugln(s.GetState())\n\n\ts.routineGroup.Add(1)\n\tgo func() {\n\t\tdefer s.routineGroup.Done()\n\t\ts.loop()\n\t}()\n\n\treturn nil\n}\n\n// Init initializes the raft server.\n// If there is no previous log file under the given path, Init() will create an empty log file.\n// Otherwise, Init() will load in the log entries from the log file.\nfunc (s *server) Init() error {\n\tif s.Running() {\n\t\treturn fmt.Errorf(\"raft.Server: Server already running[%v]\", s.state)\n\t}\n\n\t// Server has been initialized or server was stopped after initialized\n\t// If log has been initialized, we know that the server was stopped after\n\t// running.\n\tif s.state == Initialized || s.log.initialized {\n\t\ts.state = Initialized\n\t\treturn nil\n\t}\n\n\t// Create snapshot directory if it does not exist\n\terr := os.Mkdir(path.Join(s.path, \"snapshot\"), 0700)\n\tif err != nil && !os.IsExist(err) {\n\t\ts.debugln(\"raft: Snapshot dir error: \", err)\n\t\treturn fmt.Errorf(\"raft: Initialization error: %s\", err)\n\t}\n\n\tif err := s.readConf(); err != nil {\n\t\ts.debugln(\"raft: Conf file error: \", err)\n\t\treturn fmt.Errorf(\"raft: Initialization error: %s\", err)\n\t}\n\n\t// Initialize the log and load it up.\n\tif err := s.log.open(s.LogPath()); err != nil {\n\t\ts.debugln(\"raft: Log error: \", err)\n\t\treturn fmt.Errorf(\"raft: Initialization error: %s\", err)\n\t}\n\n\t// Update the term to the last term in the log.\n\t_, s.currentTerm = s.log.lastInfo()\n\n\ts.state = Initialized\n\treturn nil\n}\n\n// Shuts down the server.\nfunc (s *server) Stop() {\n\tif s.State() == Stopped {\n\t\treturn\n\t}\n\n\tclose(s.stopped)\n\n\t// make sure all goroutines have stopped before we close the log\n\ts.routineGroup.Wait()\n\n\ts.log.close()\n\ts.setState(Stopped)\n}\n\n// Checks if the server is currently running.\nfunc (s *server) Running() bool {\n\ts.mutex.RLock()\n\tdefer s.mutex.RUnlock()\n\treturn (s.state != Stopped && s.state != Initialized)\n}\n\n//--------------------------------------\n// Term\n//--------------------------------------\n\n// updates the current term for the server. This is only used when a larger\n// external term is found.\nfunc (s *server) updateCurrentTerm(term uint64, leaderName string) {\n\t_assert(term > s.currentTerm,\n\t\t\"upadteCurrentTerm: update is called when term is not larger than currentTerm\")\n\n\t// Store previous values temporarily.\n\tprevTerm := s.currentTerm\n\tprevLeader := s.leader\n\n\t// set currentTerm = T, convert to follower (§5.1)\n\t// stop heartbeats before step-down\n\tif s.state == Leader {\n\t\tfor _, peer := range s.peers {\n\t\t\tpeer.stopHeartbeat(false)\n\t\t}\n\t}\n\t// update the term and clear vote for\n\tif s.state != Follower {\n\t\ts.setState(Follower)\n\t}\n\n\ts.mutex.Lock()\n\ts.currentTerm = term\n\ts.leader = leaderName\n\ts.votedFor = \"\"\n\ts.mutex.Unlock()\n\n\t// Dispatch change events.\n\ts.DispatchEvent(newEvent(TermChangeEventType, s.currentTerm, prevTerm))\n\n\tif prevLeader != s.leader {\n\t\ts.DispatchEvent(newEvent(LeaderChangeEventType, s.leader, prevLeader))\n\t}\n}\n\n//--------------------------------------\n// Event Loop\n//--------------------------------------\n\n//               ________\n//            --|Snapshot|                 timeout\n//            |  --------                  ______\n// recover    |       ^                   |      |\n// snapshot / |       |snapshot           |      |\n// higher     |       |                   v      |     recv majority votes\n// term       |    --------    timeout    -----------                        -----------\n//            |-> |Follower| ----------> | Candidate |--------------------> |  Leader   |\n//                 --------               -----------                        -----------\n//                    ^          higher term/ |                         higher term |\n//                    |            new leader |                                     |\n//                    |_______________________|____________________________________ |\n// The main event loop for the server\nfunc (s *server) loop() {\n\tdefer s.debugln(\"server.loop.end\")\n\n\tstate := s.State()\n\n\tfor state != Stopped {\n\t\ts.debugln(\"server.loop.run \", state)\n\t\tswitch state {\n\t\tcase Follower:\n\t\t\ts.followerLoop()\n\t\tcase Candidate:\n\t\t\ts.candidateLoop()\n\t\tcase Leader:\n\t\t\ts.leaderLoop()\n\t\tcase Snapshotting:\n\t\t\ts.snapshotLoop()\n\t\t}\n\t\tstate = s.State()\n\t}\n}\n\n// Sends an event to the event loop to be processed. The function will wait\n// until the event is actually processed before returning.\nfunc (s *server) send(value interface{}) (interface{}, error) {\n\tif !s.Running() {\n\t\treturn nil, StopError\n\t}\n\n\tevent := &ev{target: value, c: make(chan error, 1)}\n\tselect {\n\tcase s.c <- event:\n\tcase <-s.stopped:\n\t\treturn nil, StopError\n\t}\n\tselect {\n\tcase <-s.stopped:\n\t\treturn nil, StopError\n\tcase err := <-event.c:\n\t\treturn event.returnValue, err\n\t}\n}\n\nfunc (s *server) sendAsync(value interface{}) {\n\tif !s.Running() {\n\t\treturn\n\t}\n\n\tevent := &ev{target: value, c: make(chan error, 1)}\n\t// try a non-blocking send first\n\t// in most cases, this should not be blocking\n\t// avoid create unnecessary go routines\n\tselect {\n\tcase s.c <- event:\n\t\treturn\n\tdefault:\n\t}\n\n\ts.routineGroup.Add(1)\n\tgo func() {\n\t\tdefer s.routineGroup.Done()\n\t\tselect {\n\t\tcase s.c <- event:\n\t\tcase <-s.stopped:\n\t\t}\n\t}()\n}\n\n// The event loop that is run when the server is in a Follower state.\n// Responds to RPCs from candidates and leaders.\n// Converts to candidate if election timeout elapses without either:\n//   1.Receiving valid AppendEntries RPC, or\n//   2.Granting vote to candidate\nfunc (s *server) followerLoop() {\n\tsince := time.Now()\n\telectionTimeout := s.ElectionTimeout()\n\ttimeoutChan := afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\n\n\tfor s.State() == Follower {\n\t\tvar err error\n\t\tupdate := false\n\t\tselect {\n\t\tcase <-s.stopped:\n\t\t\ts.setState(Stopped)\n\t\t\treturn\n\n\t\tcase e := <-s.c:\n\t\t\tswitch req := e.target.(type) {\n\t\t\tcase JoinCommand:\n\t\t\t\t//If no log entries exist and a self-join command is issued\n\t\t\t\t//then immediately become leader and commit entry.\n\t\t\t\tif s.log.currentIndex() == 0 && req.NodeName() == s.Name() {\n\t\t\t\t\ts.debugln(\"selfjoin and promote to leader\")\n\t\t\t\t\ts.setState(Leader)\n\t\t\t\t\ts.processCommand(req, e)\n\t\t\t\t} else {\n\t\t\t\t\terr = NotLeaderError\n\t\t\t\t}\n\t\t\tcase *AppendEntriesRequest:\n\t\t\t\t// If heartbeats get too close to the election timeout then send an event.\n\t\t\t\telapsedTime := time.Now().Sub(since)\n\t\t\t\tif elapsedTime > time.Duration(float64(electionTimeout)*ElectionTimeoutThresholdPercent) {\n\t\t\t\t\ts.DispatchEvent(newEvent(ElectionTimeoutThresholdEventType, elapsedTime, nil))\n\t\t\t\t}\n\t\t\t\te.returnValue, update = s.processAppendEntriesRequest(req)\n\t\t\tcase *RequestVoteRequest:\n\t\t\t\te.returnValue, update = s.processRequestVoteRequest(req)\n\t\t\tcase *SnapshotRequest:\n\t\t\t\te.returnValue = s.processSnapshotRequest(req)\n\t\t\tdefault:\n\t\t\t\terr = NotLeaderError\n\t\t\t}\n\t\t\t// Callback to event.\n\t\t\te.c <- err\n\n\t\tcase <-timeoutChan:\n\t\t\t// only allow synced follower to promote to candidate\n\t\t\tif s.promotable() {\n\t\t\t\ts.setState(Candidate)\n\t\t\t} else {\n\t\t\t\tupdate = true\n\t\t\t}\n\t\t}\n\n\t\t// Converts to candidate if election timeout elapses without either:\n\t\t//   1.Receiving valid AppendEntries RPC, or\n\t\t//   2.Granting vote to candidate\n\t\tif update {\n\t\t\tsince = time.Now()\n\t\t\ttimeoutChan = afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\n\t\t}\n\t}\n}\n\n// The event loop that is run when the server is in a Candidate state.\nfunc (s *server) candidateLoop() {\n\t// Clear leader value.\n\tprevLeader := s.leader\n\ts.leader = \"\"\n\tif prevLeader != s.leader {\n\t\ts.DispatchEvent(newEvent(LeaderChangeEventType, s.leader, prevLeader))\n\t}\n\n\tlastLogIndex, lastLogTerm := s.log.lastInfo()\n\tdoVote := true\n\tvotesGranted := 0\n\tvar timeoutChan <-chan time.Time\n\tvar respChan chan *RequestVoteResponse\n\n\tfor s.State() == Candidate {\n\t\tif doVote {\n\t\t\t// Increment current term, vote for self.\n\t\t\ts.currentTerm++\n\t\t\ts.votedFor = s.name\n\n\t\t\t// Send RequestVote RPCs to all other servers.\n\t\t\trespChan = make(chan *RequestVoteResponse, len(s.peers))\n\t\t\tfor _, peer := range s.peers {\n\t\t\t\ts.routineGroup.Add(1)\n\t\t\t\tgo func(peer *Peer) {\n\t\t\t\t\tdefer s.routineGroup.Done()\n\t\t\t\t\tpeer.sendVoteRequest(newRequestVoteRequest(s.currentTerm, s.name, lastLogIndex, lastLogTerm), respChan)\n\t\t\t\t}(peer)\n\t\t\t}\n\n\t\t\t// Wait for either:\n\t\t\t//   * Votes received from majority of servers: become leader\n\t\t\t//   * AppendEntries RPC received from new leader: step down.\n\t\t\t//   * Election timeout elapses without election resolution: increment term, start new election\n\t\t\t//   * Discover higher term: step down (§5.1)\n\t\t\tvotesGranted = 1\n\t\t\ttimeoutChan = afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\n\t\t\tdoVote = false\n\t\t}\n\n\t\t// If we received enough votes then stop waiting for more votes.\n\t\t// And return from the candidate loop\n\t\tif votesGranted == s.QuorumSize() {\n\t\t\ts.debugln(\"server.candidate.recv.enough.votes\")\n\t\t\ts.setState(Leader)\n\t\t\treturn\n\t\t}\n\n\t\t// Collect votes from peers.\n\t\tselect {\n\t\tcase <-s.stopped:\n\t\t\ts.setState(Stopped)\n\t\t\treturn\n\n\t\tcase resp := <-respChan:\n\t\t\tif success := s.processVoteResponse(resp); success {\n\t\t\t\ts.debugln(\"server.candidate.vote.granted: \", votesGranted)\n\t\t\t\tvotesGranted++\n\t\t\t}\n\n\t\tcase e := <-s.c:\n\t\t\tvar err error\n\t\t\tswitch req := e.target.(type) {\n\t\t\tcase Command:\n\t\t\t\terr = NotLeaderError\n\t\t\tcase *AppendEntriesRequest:\n\t\t\t\te.returnValue, _ = s.processAppendEntriesRequest(req)\n\t\t\tcase *RequestVoteRequest:\n\t\t\t\te.returnValue, _ = s.processRequestVoteRequest(req)\n\t\t\t}\n\n\t\t\t// Callback to event.\n\t\t\te.c <- err\n\n\t\tcase <-timeoutChan:\n\t\t\tdoVote = true\n\t\t}\n\t}\n}\n\n// The event loop that is run when the server is in a Leader state.\nfunc (s *server) leaderLoop() {\n\tlogIndex, _ := s.log.lastInfo()\n\n\t// Update the peers prevLogIndex to leader's lastLogIndex and start heartbeat.\n\ts.debugln(\"leaderLoop.set.PrevIndex to \", logIndex)\n\tfor _, peer := range s.peers {\n\t\tpeer.setPrevLogIndex(logIndex)\n\t\tpeer.startHeartbeat()\n\t}\n\n\t// Commit a NOP after the server becomes leader. From the Raft paper:\n\t// \"Upon election: send initial empty AppendEntries RPCs (heartbeat) to\n\t// each server; repeat during idle periods to prevent election timeouts\n\t// (§5.2)\". The heartbeats started above do the \"idle\" period work.\n\ts.routineGroup.Add(1)\n\tgo func() {\n\t\tdefer s.routineGroup.Done()\n\t\ts.Do(NOPCommand{})\n\t}()\n\n\t// Begin to collect response from followers\n\tfor s.State() == Leader {\n\t\tvar err error\n\t\tselect {\n\t\tcase <-s.stopped:\n\t\t\t// Stop all peers before stop\n\t\t\tfor _, peer := range s.peers {\n\t\t\t\tpeer.stopHeartbeat(false)\n\t\t\t}\n\t\t\ts.setState(Stopped)\n\t\t\treturn\n\n\t\tcase e := <-s.c:\n\t\t\tswitch req := e.target.(type) {\n\t\t\tcase Command:\n\t\t\t\ts.processCommand(req, e)\n\t\t\t\tcontinue\n\t\t\tcase *AppendEntriesRequest:\n\t\t\t\te.returnValue, _ = s.processAppendEntriesRequest(req)\n\t\t\tcase *AppendEntriesResponse:\n\t\t\t\ts.processAppendEntriesResponse(req)\n\t\t\tcase *RequestVoteRequest:\n\t\t\t\te.returnValue, _ = s.processRequestVoteRequest(req)\n\t\t\t}\n\n\t\t\t// Callback to event.\n\t\t\te.c <- err\n\t\t}\n\t}\n\n\ts.syncedPeer = nil\n}\n\nfunc (s *server) snapshotLoop() {\n\tfor s.State() == Snapshotting {\n\t\tvar err error\n\t\tselect {\n\t\tcase <-s.stopped:\n\t\t\ts.setState(Stopped)\n\t\t\treturn\n\n\t\tcase e := <-s.c:\n\t\t\tswitch req := e.target.(type) {\n\t\t\tcase Command:\n\t\t\t\terr = NotLeaderError\n\t\t\tcase *AppendEntriesRequest:\n\t\t\t\te.returnValue, _ = s.processAppendEntriesRequest(req)\n\t\t\tcase *RequestVoteRequest:\n\t\t\t\te.returnValue, _ = s.processRequestVoteRequest(req)\n\t\t\tcase *SnapshotRecoveryRequest:\n\t\t\t\te.returnValue = s.processSnapshotRecoveryRequest(req)\n\t\t\t}\n\t\t\t// Callback to event.\n\t\t\te.c <- err\n\t\t}\n\t}\n}\n\n//--------------------------------------\n// Commands\n//--------------------------------------\n\n// Attempts to execute a command and replicate it. The function will return\n// when the command has been successfully committed or an error has occurred.\n\nfunc (s *server) Do(command Command) (interface{}, error) {\n\treturn s.send(command)\n}\n\n// Processes a command.\nfunc (s *server) processCommand(command Command, e *ev) {\n\ts.debugln(\"server.command.process\")\n\n\t// Create an entry for the command in the log.\n\tentry, err := s.log.createEntry(s.currentTerm, command, e)\n\n\tif err != nil {\n\t\ts.debugln(\"server.command.log.entry.error:\", err)\n\t\te.c <- err\n\t\treturn\n\t}\n\n\tif err := s.log.appendEntry(entry); err != nil {\n\t\ts.debugln(\"server.command.log.error:\", err)\n\t\te.c <- err\n\t\treturn\n\t}\n\n\ts.syncedPeer[s.Name()] = true\n\tif len(s.peers) == 0 {\n\t\tcommitIndex := s.log.currentIndex()\n\t\ts.log.setCommitIndex(commitIndex)\n\t\ts.debugln(\"commit index \", commitIndex)\n\t}\n}\n\n//--------------------------------------\n// Append Entries\n//--------------------------------------\n\n// Appends zero or more log entry from the leader to this server.\nfunc (s *server) AppendEntries(req *AppendEntriesRequest) *AppendEntriesResponse {\n\tret, _ := s.send(req)\n\tresp, _ := ret.(*AppendEntriesResponse)\n\treturn resp\n}\n\n// Processes the \"append entries\" request.\nfunc (s *server) processAppendEntriesRequest(req *AppendEntriesRequest) (*AppendEntriesResponse, bool) {\n\ts.traceln(\"server.ae.process\")\n\n\tif req.Term < s.currentTerm {\n\t\ts.debugln(\"server.ae.error: stale term\")\n\t\treturn newAppendEntriesResponse(s.currentTerm, false, s.log.currentIndex(), s.log.CommitIndex()), false\n\t}\n\n\tif req.Term == s.currentTerm {\n\t\t_assert(s.State() != Leader, \"leader.elected.at.same.term.%d\\n\", s.currentTerm)\n\n\t\t// step-down to follower when it is a candidate\n\t\tif s.state == Candidate {\n\t\t\t// change state to follower\n\t\t\ts.setState(Follower)\n\t\t}\n\n\t\t// discover new leader when candidate\n\t\t// save leader name when follower\n\t\ts.leader = req.LeaderName\n\t} else {\n\t\t// Update term and leader.\n\t\ts.updateCurrentTerm(req.Term, req.LeaderName)\n\t}\n\n\t// Reject if log doesn't contain a matching previous entry.\n\tif err := s.log.truncate(req.PrevLogIndex, req.PrevLogTerm); err != nil {\n\t\ts.debugln(\"server.ae.truncate.error: \", err)\n\t\treturn newAppendEntriesResponse(s.currentTerm, false, s.log.currentIndex(), s.log.CommitIndex()), true\n\t}\n\n\t// Append entries to the log.\n\tif err := s.log.appendEntries(req.Entries); err != nil {\n\t\ts.debugln(\"server.ae.append.error: \", err)\n\t\treturn newAppendEntriesResponse(s.currentTerm, false, s.log.currentIndex(), s.log.CommitIndex()), true\n\t}\n\n\t// Commit up to the commit index.\n\tif err := s.log.setCommitIndex(req.CommitIndex); err != nil {\n\t\ts.debugln(\"server.ae.commit.error: \", err)\n\t\treturn newAppendEntriesResponse(s.currentTerm, false, s.log.currentIndex(), s.log.CommitIndex()), true\n\t}\n\n\t// once the server appended and committed all the log entries from the leader\n\n\treturn newAppendEntriesResponse(s.currentTerm, true, s.log.currentIndex(), s.log.CommitIndex()), true\n}\n\n// Processes the \"append entries\" response from the peer. This is only\n// processed when the server is a leader. Responses received during other\n// states are dropped.\nfunc (s *server) processAppendEntriesResponse(resp *AppendEntriesResponse) {\n\t// If we find a higher term then change to a follower and exit.\n\tif resp.Term() > s.Term() {\n\t\ts.updateCurrentTerm(resp.Term(), \"\")\n\t\treturn\n\t}\n\n\t// panic response if it's not successful.\n\tif !resp.Success() {\n\t\treturn\n\t}\n\n\t// if one peer successfully append a log from the leader term,\n\t// we add it to the synced list\n\tif resp.append == true {\n\t\ts.syncedPeer[resp.peer] = true\n\t}\n\n\t// Increment the commit count to make sure we have a quorum before committing.\n\tif len(s.syncedPeer) < s.QuorumSize() {\n\t\treturn\n\t}\n\n\t// Determine the committed index that a majority has.\n\tvar indices []uint64\n\tindices = append(indices, s.log.currentIndex())\n\tfor _, peer := range s.peers {\n\t\tindices = append(indices, peer.getPrevLogIndex())\n\t}\n\tsort.Sort(sort.Reverse(uint64Slice(indices)))\n\n\t// We can commit up to the index which the majority of the members have appended.\n\tcommitIndex := indices[s.QuorumSize()-1]\n\tcommittedIndex := s.log.commitIndex\n\n\tif commitIndex > committedIndex {\n\t\t// leader needs to do a fsync before committing log entries\n\t\ts.log.sync()\n\t\ts.log.setCommitIndex(commitIndex)\n\t\ts.debugln(\"commit index \", commitIndex)\n\t}\n}\n\n// processVoteReponse processes a vote request:\n// 1. if the vote is granted for the current term of the candidate, return true\n// 2. if the vote is denied due to smaller term, update the term of this server\n//    which will also cause the candidate to step-down, and return false.\n// 3. if the vote is for a smaller term, ignore it and return false.\nfunc (s *server) processVoteResponse(resp *RequestVoteResponse) bool {\n\tif resp.VoteGranted && resp.Term == s.currentTerm {\n\t\treturn true\n\t}\n\n\tif resp.Term > s.currentTerm {\n\t\ts.debugln(\"server.candidate.vote.failed\")\n\t\ts.updateCurrentTerm(resp.Term, \"\")\n\t} else {\n\t\ts.debugln(\"server.candidate.vote: denied\")\n\t}\n\treturn false\n}\n\n//--------------------------------------\n// Request Vote\n//--------------------------------------\n\n// Requests a vote from a server. A vote can be obtained if the vote's term is\n// at the server's current term and the server has not made a vote yet. A vote\n// can also be obtained if the term is greater than the server's current term.\nfunc (s *server) RequestVote(req *RequestVoteRequest) *RequestVoteResponse {\n\tret, _ := s.send(req)\n\tresp, _ := ret.(*RequestVoteResponse)\n\treturn resp\n}\n\n// Processes a \"request vote\" request.\nfunc (s *server) processRequestVoteRequest(req *RequestVoteRequest) (*RequestVoteResponse, bool) {\n\n\t// If the request is coming from an old term then reject it.\n\tif req.Term < s.Term() {\n\t\ts.debugln(\"server.rv.deny.vote: cause stale term\")\n\t\treturn newRequestVoteResponse(s.currentTerm, false), false\n\t}\n\n\t// If the term of the request peer is larger than this node, update the term\n\t// If the term is equal and we've already voted for a different candidate then\n\t// don't vote for this candidate.\n\tif req.Term > s.Term() {\n\t\ts.updateCurrentTerm(req.Term, \"\")\n\t} else if s.votedFor != \"\" && s.votedFor != req.CandidateName {\n\t\ts.debugln(\"server.deny.vote: cause duplicate vote: \", req.CandidateName,\n\t\t\t\" already vote for \", s.votedFor)\n\t\treturn newRequestVoteResponse(s.currentTerm, false), false\n\t}\n\n\t// If the candidate's log is not at least as up-to-date as our last log then don't vote.\n\tlastIndex, lastTerm := s.log.lastInfo()\n\tif lastIndex > req.LastLogIndex || lastTerm > req.LastLogTerm {\n\t\ts.debugln(\"server.deny.vote: cause out of date log: \", req.CandidateName,\n\t\t\t\"Index :[\", lastIndex, \"]\", \" [\", req.LastLogIndex, \"]\",\n\t\t\t\"Term :[\", lastTerm, \"]\", \" [\", req.LastLogTerm, \"]\")\n\t\treturn newRequestVoteResponse(s.currentTerm, false), false\n\t}\n\n\t// If we made it this far then cast a vote and reset our election time out.\n\ts.debugln(\"server.rv.vote: \", s.name, \" votes for\", req.CandidateName, \"at term\", req.Term)\n\ts.votedFor = req.CandidateName\n\n\treturn newRequestVoteResponse(s.currentTerm, true), true\n}\n\n//--------------------------------------\n// Membership\n//--------------------------------------\n\n// Adds a peer to the server.\nfunc (s *server) AddPeer(name string, connectiongString string) error {\n\ts.debugln(\"server.peer.add: \", name, len(s.peers))\n\n\t// Do not allow peers to be added twice.\n\tif s.peers[name] != nil {\n\t\treturn nil\n\t}\n\n\t// Skip the Peer if it has the same name as the Server\n\tif s.name != name {\n\t\tpeer := newPeer(s, name, connectiongString, s.heartbeatInterval)\n\n\t\tif s.State() == Leader {\n\t\t\tpeer.startHeartbeat()\n\t\t}\n\n\t\ts.peers[peer.Name] = peer\n\n\t\ts.DispatchEvent(newEvent(AddPeerEventType, name, nil))\n\t}\n\n\t// Write the configuration to file.\n\ts.writeConf()\n\n\treturn nil\n}\n\n// Removes a peer from the server.\nfunc (s *server) RemovePeer(name string) error {\n\ts.debugln(\"server.peer.remove: \", name, len(s.peers))\n\n\t// Skip the Peer if it has the same name as the Server\n\tif name != s.Name() {\n\t\t// Return error if peer doesn't exist.\n\t\tpeer := s.peers[name]\n\t\tif peer == nil {\n\t\t\treturn fmt.Errorf(\"raft: Peer not found: %s\", name)\n\t\t}\n\n\t\t// Stop peer and remove it.\n\t\tif s.State() == Leader {\n\t\t\t// We create a go routine here to avoid potential deadlock.\n\t\t\t// We are holding log write lock when reach this line of code.\n\t\t\t// Peer.stopHeartbeat can be blocked without go routine, if the\n\t\t\t// target go routine (which we want to stop) is calling\n\t\t\t// log.getEntriesAfter and waiting for log read lock.\n\t\t\t// So we might be holding log lock and waiting for log lock,\n\t\t\t// which lead to a deadlock.\n\t\t\t// TODO(xiangli) refactor log lock\n\t\t\ts.routineGroup.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer s.routineGroup.Done()\n\t\t\t\tpeer.stopHeartbeat(true)\n\t\t\t}()\n\t\t}\n\n\t\tdelete(s.peers, name)\n\n\t\ts.DispatchEvent(newEvent(RemovePeerEventType, name, nil))\n\t}\n\n\t// Write the configuration to file.\n\ts.writeConf()\n\n\treturn nil\n}\n\n//--------------------------------------\n// Log compaction\n//--------------------------------------\n\nfunc (s *server) TakeSnapshot() error {\n\tif s.stateMachine == nil {\n\t\treturn errors.New(\"Snapshot: Cannot create snapshot. Missing state machine.\")\n\t}\n\n\t// Shortcut without lock\n\t// Exit if the server is currently creating a snapshot.\n\tif s.pendingSnapshot != nil {\n\t\treturn errors.New(\"Snapshot: Last snapshot is not finished.\")\n\t}\n\n\t// TODO: acquire the lock and no more committed is allowed\n\t// This will be done after finishing refactoring heartbeat\n\ts.debugln(\"take.snapshot\")\n\n\tlastIndex, lastTerm := s.log.commitInfo()\n\n\t// check if there is log has been committed since the\n\t// last snapshot.\n\tif lastIndex == s.log.startIndex {\n\t\treturn nil\n\t}\n\n\tpath := s.SnapshotPath(lastIndex, lastTerm)\n\t// Attach snapshot to pending snapshot and save it to disk.\n\ts.pendingSnapshot = &Snapshot{lastIndex, lastTerm, nil, nil, path}\n\n\tstate, err := s.stateMachine.Save()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Clone the list of peers.\n\tpeers := make([]*Peer, 0, len(s.peers)+1)\n\tfor _, peer := range s.peers {\n\t\tpeers = append(peers, peer.clone())\n\t}\n\tpeers = append(peers, &Peer{Name: s.Name(), ConnectionString: s.connectionString})\n\n\t// Attach snapshot to pending snapshot and save it to disk.\n\ts.pendingSnapshot.Peers = peers\n\ts.pendingSnapshot.State = state\n\ts.saveSnapshot()\n\n\t// We keep some log entries after the snapshot.\n\t// We do not want to send the whole snapshot to the slightly slow machines\n\tif lastIndex-s.log.startIndex > NumberOfLogEntriesAfterSnapshot {\n\t\tcompactIndex := lastIndex - NumberOfLogEntriesAfterSnapshot\n\t\tcompactTerm := s.log.getEntry(compactIndex).Term()\n\t\ts.log.compact(compactIndex, compactTerm)\n\t}\n\n\treturn nil\n}\n\n// Retrieves the log path for the server.\nfunc (s *server) saveSnapshot() error {\n\tif s.pendingSnapshot == nil {\n\t\treturn errors.New(\"pendingSnapshot.is.nil\")\n\t}\n\n\t// Write snapshot to disk.\n\tif err := s.pendingSnapshot.save(); err != nil {\n\t\treturn err\n\t}\n\n\t// Swap the current and last snapshots.\n\ttmp := s.snapshot\n\ts.snapshot = s.pendingSnapshot\n\n\t// Delete the previous snapshot if there is any change\n\tif tmp != nil && !(tmp.LastIndex == s.snapshot.LastIndex && tmp.LastTerm == s.snapshot.LastTerm) {\n\t\ttmp.remove()\n\t}\n\ts.pendingSnapshot = nil\n\n\treturn nil\n}\n\n// Retrieves the log path for the server.\nfunc (s *server) SnapshotPath(lastIndex uint64, lastTerm uint64) string {\n\treturn path.Join(s.path, \"snapshot\", fmt.Sprintf(\"%v_%v.ss\", lastTerm, lastIndex))\n}\n\nfunc (s *server) RequestSnapshot(req *SnapshotRequest) *SnapshotResponse {\n\tret, _ := s.send(req)\n\tresp, _ := ret.(*SnapshotResponse)\n\treturn resp\n}\n\nfunc (s *server) processSnapshotRequest(req *SnapshotRequest) *SnapshotResponse {\n\t// If the follower’s log contains an entry at the snapshot’s last index with a term\n\t// that matches the snapshot’s last term, then the follower already has all the\n\t// information found in the snapshot and can reply false.\n\tentry := s.log.getEntry(req.LastIndex)\n\n\tif entry != nil && entry.Term() == req.LastTerm {\n\t\treturn newSnapshotResponse(false)\n\t}\n\n\t// Update state.\n\ts.setState(Snapshotting)\n\n\treturn newSnapshotResponse(true)\n}\n\nfunc (s *server) SnapshotRecoveryRequest(req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse {\n\tret, _ := s.send(req)\n\tresp, _ := ret.(*SnapshotRecoveryResponse)\n\treturn resp\n}\n\nfunc (s *server) processSnapshotRecoveryRequest(req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse {\n\t// Recover state sent from request.\n\tif err := s.stateMachine.Recovery(req.State); err != nil {\n\t\tpanic(\"cannot recover from previous state\")\n\t}\n\n\t// Recover the cluster configuration.\n\ts.peers = make(map[string]*Peer)\n\tfor _, peer := range req.Peers {\n\t\ts.AddPeer(peer.Name, peer.ConnectionString)\n\t}\n\n\t// Update log state.\n\ts.currentTerm = req.LastTerm\n\ts.log.updateCommitIndex(req.LastIndex)\n\n\t// Create local snapshot.\n\ts.pendingSnapshot = &Snapshot{req.LastIndex, req.LastTerm, req.Peers, req.State, s.SnapshotPath(req.LastIndex, req.LastTerm)}\n\ts.saveSnapshot()\n\n\t// Clear the previous log entries.\n\ts.log.compact(req.LastIndex, req.LastTerm)\n\n\treturn newSnapshotRecoveryResponse(req.LastTerm, true, req.LastIndex)\n}\n\n// Load a snapshot at restart\nfunc (s *server) LoadSnapshot() error {\n\t// Open snapshot/ directory.\n\tdir, err := os.OpenFile(path.Join(s.path, \"snapshot\"), os.O_RDONLY, 0)\n\tif err != nil {\n\t\ts.debugln(\"cannot.open.snapshot: \", err)\n\t\treturn err\n\t}\n\n\t// Retrieve a list of all snapshots.\n\tfilenames, err := dir.Readdirnames(-1)\n\tif err != nil {\n\t\tdir.Close()\n\t\tpanic(err)\n\t}\n\tdir.Close()\n\n\tif len(filenames) == 0 {\n\t\ts.debugln(\"no.snapshot.to.load\")\n\t\treturn nil\n\t}\n\n\t// Grab the latest snapshot.\n\tsort.Strings(filenames)\n\tsnapshotPath := path.Join(s.path, \"snapshot\", filenames[len(filenames)-1])\n\n\t// Read snapshot data.\n\tfile, err := os.OpenFile(snapshotPath, os.O_RDONLY, 0)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer file.Close()\n\n\t// Check checksum.\n\tvar checksum uint32\n\tn, err := fmt.Fscanf(file, \"%08x\\n\", &checksum)\n\tif err != nil {\n\t\treturn err\n\t} else if n != 1 {\n\t\treturn errors.New(\"checksum.err: bad.snapshot.file\")\n\t}\n\n\t// Load remaining snapshot contents.\n\tb, err := ioutil.ReadAll(file)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Generate checksum.\n\tbyteChecksum := crc32.ChecksumIEEE(b)\n\tif uint32(checksum) != byteChecksum {\n\t\ts.debugln(checksum, \" \", byteChecksum)\n\t\treturn errors.New(\"bad snapshot file\")\n\t}\n\n\t// Decode snapshot.\n\tif err = json.Unmarshal(b, &s.snapshot); err != nil {\n\t\ts.debugln(\"unmarshal.snapshot.error: \", err)\n\t\treturn err\n\t}\n\n\t// Recover snapshot into state machine.\n\tif err = s.stateMachine.Recovery(s.snapshot.State); err != nil {\n\t\ts.debugln(\"recovery.snapshot.error: \", err)\n\t\treturn err\n\t}\n\n\t// Recover cluster configuration.\n\tfor _, peer := range s.snapshot.Peers {\n\t\ts.AddPeer(peer.Name, peer.ConnectionString)\n\t}\n\n\t// Update log state.\n\ts.log.startTerm = s.snapshot.LastTerm\n\ts.log.startIndex = s.snapshot.LastIndex\n\ts.log.updateCommitIndex(s.snapshot.LastIndex)\n\n\treturn err\n}\n\n//--------------------------------------\n// Config File\n//--------------------------------------\n\n// Flushes commit index to the disk.\n// So when the raft server restarts, it will commit upto the flushed commitIndex.\nfunc (s *server) FlushCommitIndex() {\n\ts.debugln(\"server.conf.update\")\n\t// Write the configuration to file.\n\ts.writeConf()\n}\n\nfunc (s *server) writeConf() {\n\n\tpeers := make([]*Peer, len(s.peers))\n\n\ti := 0\n\tfor _, peer := range s.peers {\n\t\tpeers[i] = peer.clone()\n\t\ti++\n\t}\n\n\tr := &Config{\n\t\tCommitIndex: s.log.commitIndex,\n\t\tPeers:       peers,\n\t}\n\n\tb, _ := json.Marshal(r)\n\n\tconfPath := path.Join(s.path, \"conf\")\n\ttmpConfPath := path.Join(s.path, \"conf.tmp\")\n\n\terr := writeFileSynced(tmpConfPath, b, 0600)\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tos.Rename(tmpConfPath, confPath)\n}\n\n// Read the configuration for the server.\nfunc (s *server) readConf() error {\n\tconfPath := path.Join(s.path, \"conf\")\n\ts.debugln(\"readConf.open \", confPath)\n\n\t// open conf file\n\tb, err := ioutil.ReadFile(confPath)\n\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tconf := &Config{}\n\n\tif err = json.Unmarshal(b, conf); err != nil {\n\t\treturn err\n\t}\n\n\ts.log.updateCommitIndex(conf.CommitIndex)\n\n\treturn nil\n}\n\n//--------------------------------------\n// Debugging\n//--------------------------------------\n\nfunc (s *server) debugln(v ...interface{}) {\n\tif logLevel > Debug {\n\t\tdebugf(\"[%s Term:%d] %s\", s.name, s.Term(), fmt.Sprintln(v...))\n\t}\n}\n\nfunc (s *server) traceln(v ...interface{}) {\n\tif logLevel > Trace {\n\t\ttracef(\"[%s] %s\", s.name, fmt.Sprintln(v...))\n\t}\n}\n"
        },
        {
          "name": "server_test.go",
          "type": "blob",
          "size": 20.853515625,
          "content": "package raft\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n)\n\n//--------------------------------------\n// Request Vote\n//--------------------------------------\n\n// Ensure that we can request a vote from a server that has not voted.\nfunc TestServerRequestVote(t *testing.T) {\n\tserver := newTestServer(\"1\", &testTransporter{})\n\n\tserver.Start()\n\tif _, err := server.Do(&DefaultJoinCommand{Name: server.Name()}); err != nil {\n\t\tt.Fatalf(\"Server %s unable to join: %v\", server.Name(), err)\n\t}\n\n\tdefer server.Stop()\n\tresp := server.RequestVote(newRequestVoteRequest(1, \"foo\", 1, 0))\n\tif resp.Term != 1 || !resp.VoteGranted {\n\t\tt.Fatalf(\"Invalid request vote response: %v/%v\", resp.Term, resp.VoteGranted)\n\t}\n}\n\n// // Ensure that a vote request is denied if it comes from an old term.\nfunc TestServerRequestVoteDeniedForStaleTerm(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\n\ts.Start()\n\tif _, err := s.Do(&DefaultJoinCommand{Name: s.Name()}); err != nil {\n\t\tt.Fatalf(\"Server %s unable to join: %v\", s.Name(), err)\n\t}\n\n\ts.(*server).mutex.Lock()\n\ts.(*server).currentTerm = 2\n\ts.(*server).mutex.Unlock()\n\n\tdefer s.Stop()\n\tresp := s.RequestVote(newRequestVoteRequest(1, \"foo\", 1, 0))\n\tif resp.Term != 2 || resp.VoteGranted {\n\t\tt.Fatalf(\"Invalid request vote response: %v/%v\", resp.Term, resp.VoteGranted)\n\t}\n\tif s.Term() != 2 && s.State() != Follower {\n\t\tt.Fatalf(\"Server did not update term and demote: %v / %v\", s.Term(), s.State())\n\t}\n}\n\n// Ensure that a vote request is denied if we've already voted for a different candidate.\nfunc TestServerRequestVoteDeniedIfAlreadyVoted(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\n\ts.Start()\n\tif _, err := s.Do(&DefaultJoinCommand{Name: s.Name()}); err != nil {\n\t\tt.Fatalf(\"Server %s unable to join: %v\", s.Name(), err)\n\t}\n\n\ts.(*server).mutex.Lock()\n\ts.(*server).currentTerm = 2\n\ts.(*server).mutex.Unlock()\n\tdefer s.Stop()\n\tresp := s.RequestVote(newRequestVoteRequest(2, \"foo\", 1, 0))\n\tif resp.Term != 2 || !resp.VoteGranted {\n\t\tt.Fatalf(\"First vote should not have been denied\")\n\t}\n\tresp = s.RequestVote(newRequestVoteRequest(2, \"bar\", 1, 0))\n\tif resp.Term != 2 || resp.VoteGranted {\n\t\tt.Fatalf(\"Second vote should have been denied\")\n\t}\n}\n\n// Ensure that a vote request is approved if vote occurs in a new term.\nfunc TestServerRequestVoteApprovedIfAlreadyVotedInOlderTerm(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\n\ts.Start()\n\tif _, err := s.Do(&DefaultJoinCommand{Name: s.Name()}); err != nil {\n\t\tt.Fatalf(\"Server %s unable to join: %v\", s.Name(), err)\n\t}\n\n\ttime.Sleep(time.Millisecond * 100)\n\n\ts.(*server).mutex.Lock()\n\ts.(*server).currentTerm = 2\n\ts.(*server).mutex.Unlock()\n\tdefer s.Stop()\n\tresp := s.RequestVote(newRequestVoteRequest(2, \"foo\", 2, 1))\n\tif resp.Term != 2 || !resp.VoteGranted || s.VotedFor() != \"foo\" {\n\t\tt.Fatalf(\"First vote should not have been denied\")\n\t}\n\tresp = s.RequestVote(newRequestVoteRequest(3, \"bar\", 2, 1))\n\n\tif resp.Term != 3 || !resp.VoteGranted || s.VotedFor() != \"bar\" {\n\t\tt.Fatalf(\"Second vote should have been approved\")\n\t}\n}\n\n// Ensure that a vote request is denied if the log is out of date.\nfunc TestServerRequestVoteDenyIfCandidateLogIsBehind(t *testing.T) {\n\ttmpLog := newLog()\n\te0, _ := newLogEntry(tmpLog, nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\te1, _ := newLogEntry(tmpLog, nil, 2, 1, &testCommand2{X: 100})\n\te2, _ := newLogEntry(tmpLog, nil, 3, 2, &testCommand1{Val: \"bar\", I: 0})\n\ts := newTestServerWithLog(\"1\", &testTransporter{}, []*LogEntry{e0, e1, e2})\n\n\t// start as a follower with term 2 and index 3\n\ts.Start()\n\tdefer s.Stop()\n\n\t// request vote from term 3 with last log entry 2, 2\n\tresp := s.RequestVote(newRequestVoteRequest(3, \"foo\", 2, 2))\n\tif resp.Term != 3 || resp.VoteGranted {\n\t\tt.Fatalf(\"Stale index vote should have been denied [%v/%v]\", resp.Term, resp.VoteGranted)\n\t}\n\n\t// request vote from term 2 with last log entry 2, 3\n\tresp = s.RequestVote(newRequestVoteRequest(2, \"foo\", 3, 2))\n\tif resp.Term != 3 || resp.VoteGranted {\n\t\tt.Fatalf(\"Stale term vote should have been denied [%v/%v]\", resp.Term, resp.VoteGranted)\n\t}\n\n\t// request vote from term 3 with last log entry 2, 3\n\tresp = s.RequestVote(newRequestVoteRequest(3, \"foo\", 3, 2))\n\tif resp.Term != 3 || !resp.VoteGranted {\n\t\tt.Fatalf(\"Matching log vote should have been granted\")\n\t}\n\n\t// request vote from term 3 with last log entry 2, 4\n\tresp = s.RequestVote(newRequestVoteRequest(3, \"foo\", 4, 2))\n\tif resp.Term != 3 || !resp.VoteGranted {\n\t\tt.Fatalf(\"Ahead-of-log vote should have been granted\")\n\t}\n}\n\nfunc TestProcessVoteResponse(t *testing.T) {\n\t// server Term: 0, status: Leader\n\t// response Term : 1, granted\n\t// Expectation: not success\n\t// Server Term 1 status:Leader\n\tserver := &server{}\n\tserver.eventDispatcher = newEventDispatcher(server)\n\tserver.currentTerm = 0\n\tserver.state = Leader\n\tresponse := &RequestVoteResponse{\n\t\tVoteGranted: true,\n\t\tTerm:        1,\n\t}\n\tif success := server.processVoteResponse(response); success {\n\t\tt.Fatal(\"Process should fail if the resp's term is larger than server's\")\n\t}\n\tif server.state != Follower {\n\t\tt.Fatal(\"Server should stepdown\")\n\t}\n\n\t// server Term: 1, status: Follower\n\t// response Term: 2, granted\n\t// Expectation: not success\n\tresponse.Term = 2\n\tif success := server.processVoteResponse(response); success {\n\t\tt.Fatal(\"Process should fail if the resp's term is larger than server's\")\n\t}\n\tif server.state != Follower {\n\t\tt.Fatal(\"Server should still be Follower\")\n\t}\n\n\tserver.currentTerm = 2\n\t// server Term: 2, status: Follower\n\t// response Term: 2\n\t// Expectation: success\n\tif success := server.processVoteResponse(response); !success {\n\t\tt.Fatal(\"Process should success if the server's term is larger than resp's\")\n\t}\n\n}\n\n// //--------------------------------------\n// // Promotion\n// //--------------------------------------\n\n// // Ensure that we can self-promote a server to candidate, obtain votes and become a fearless leader.\nfunc TestServerPromoteSelf(t *testing.T) {\n\te0, _ := newLogEntry(newLog(), nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\ts := newTestServerWithLog(\"1\", &testTransporter{}, []*LogEntry{e0})\n\n\t// start as a follower\n\ts.Start()\n\tdefer s.Stop()\n\n\ttime.Sleep(2 * testElectionTimeout)\n\n\tif s.State() != Leader {\n\t\tt.Fatalf(\"Server self-promotion failed: %v\", s.State())\n\t}\n}\n\n//Ensure that we can promote a server within a cluster to a leader.\nfunc TestServerPromote(t *testing.T) {\n\tlookup := map[string]Server{}\n\ttransporter := &testTransporter{}\n\ttransporter.sendVoteRequestFunc = func(s Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\t\treturn lookup[peer.Name].RequestVote(req)\n\t}\n\ttransporter.sendAppendEntriesRequestFunc = func(s Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\t\treturn lookup[peer.Name].AppendEntries(req)\n\t}\n\tservers := newTestCluster([]string{\"1\", \"2\", \"3\"}, transporter, lookup)\n\n\tservers[0].Start()\n\tservers[1].Start()\n\tservers[2].Start()\n\n\ttime.Sleep(2 * testElectionTimeout)\n\n\tif servers[0].State() != Leader && servers[1].State() != Leader && servers[2].State() != Leader {\n\t\tt.Fatalf(\"No leader elected: (%s, %s, %s)\", servers[0].State(), servers[1].State(), servers[2].State())\n\t}\n\tfor _, s := range servers {\n\t\ts.Stop()\n\t}\n}\n\n//--------------------------------------\n// Append Entries\n//--------------------------------------\n\n// Ensure we can append entries to a server.\nfunc TestServerAppendEntries(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\n\ts.SetHeartbeatInterval(time.Second * 10)\n\ts.Start()\n\tdefer s.Stop()\n\n\t// Append single entry.\n\te, _ := newLogEntry(nil, nil, 1, 1, &testCommand1{Val: \"foo\", I: 10})\n\tentries := []*LogEntry{e}\n\tresp := s.AppendEntries(newAppendEntriesRequest(1, 0, 0, 0, \"ldr\", entries))\n\tif resp.Term() != 1 || !resp.Success() {\n\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\tif index, term := s.(*server).log.commitInfo(); index != 0 || term != 0 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n\n\t// Append multiple entries + commit the last one.\n\te1, _ := newLogEntry(nil, nil, 2, 1, &testCommand1{Val: \"bar\", I: 20})\n\te2, _ := newLogEntry(nil, nil, 3, 1, &testCommand1{Val: \"baz\", I: 30})\n\tentries = []*LogEntry{e1, e2}\n\tresp = s.AppendEntries(newAppendEntriesRequest(1, 1, 1, 1, \"ldr\", entries))\n\tif resp.Term() != 1 || !resp.Success() {\n\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\tif index, term := s.(*server).log.commitInfo(); index != 1 || term != 1 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n\n\t// Send zero entries and commit everything.\n\tresp = s.AppendEntries(newAppendEntriesRequest(2, 3, 1, 3, \"ldr\", []*LogEntry{}))\n\tif resp.Term() != 2 || !resp.Success() {\n\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\tif index, term := s.(*server).log.commitInfo(); index != 3 || term != 1 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n}\n\n//Ensure that entries with stale terms are rejected.\nfunc TestServerAppendEntriesWithStaleTermsAreRejected(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\n\ts.Start()\n\n\tdefer s.Stop()\n\ts.(*server).mutex.Lock()\n\ts.(*server).currentTerm = 2\n\ts.(*server).mutex.Unlock()\n\n\t// Append single entry.\n\te, _ := newLogEntry(nil, nil, 1, 1, &testCommand1{Val: \"foo\", I: 10})\n\tentries := []*LogEntry{e}\n\tresp := s.AppendEntries(newAppendEntriesRequest(1, 0, 0, 0, \"ldr\", entries))\n\tif resp.Term() != 2 || resp.Success() {\n\t\tt.Fatalf(\"AppendEntries should have failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\tif index, term := s.(*server).log.commitInfo(); index != 0 || term != 0 {\n\t\tt.Fatalf(\"Invalid commit info [IDX=%v, TERM=%v]\", index, term)\n\t}\n}\n\n// Ensure that we reject entries if the commit log is different.\nfunc TestServerAppendEntriesRejectedIfAlreadyCommitted(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\ts.Start()\n\tdefer s.Stop()\n\n\t// Append single entry + commit.\n\te1, _ := newLogEntry(nil, nil, 1, 1, &testCommand1{Val: \"foo\", I: 10})\n\te2, _ := newLogEntry(nil, nil, 2, 1, &testCommand1{Val: \"foo\", I: 15})\n\tentries := []*LogEntry{e1, e2}\n\tresp := s.AppendEntries(newAppendEntriesRequest(1, 0, 0, 2, \"ldr\", entries))\n\tif resp.Term() != 1 || !resp.Success() {\n\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\n\t// Append entry again (post-commit).\n\te, _ := newLogEntry(nil, nil, 2, 1, &testCommand1{Val: \"bar\", I: 20})\n\tentries = []*LogEntry{e}\n\tresp = s.AppendEntries(newAppendEntriesRequest(1, 2, 1, 1, \"ldr\", entries))\n\tif resp.Term() != 1 || resp.Success() {\n\t\tt.Fatalf(\"AppendEntries should have failed: %v/%v\", resp.Term, resp.Success)\n\t}\n}\n\n// Ensure that we uncommitted entries are rolled back if new entries overwrite them.\nfunc TestServerAppendEntriesOverwritesUncommittedEntries(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\ts.Start()\n\tdefer s.Stop()\n\n\tentry1, _ := newLogEntry(s.(*server).log, nil, 1, 1, &testCommand1{Val: \"foo\", I: 10})\n\tentry2, _ := newLogEntry(s.(*server).log, nil, 2, 1, &testCommand1{Val: \"foo\", I: 15})\n\tentry3, _ := newLogEntry(s.(*server).log, nil, 2, 2, &testCommand1{Val: \"bar\", I: 20})\n\n\t// Append single entry + commit.\n\tentries := []*LogEntry{entry1, entry2}\n\tresp := s.AppendEntries(newAppendEntriesRequest(1, 0, 0, 1, \"ldr\", entries))\n\tif resp.Term() != 1 || !resp.Success() || s.(*server).log.commitIndex != 1 {\n\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t}\n\n\tfor i, entry := range s.(*server).log.entries {\n\t\tif entry.Term() != entries[i].Term() || entry.Index() != entries[i].Index() || !bytes.Equal(entry.Command(), entries[i].Command()) {\n\t\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t\t}\n\t}\n\n\t// Append entry that overwrites the second (uncommitted) entry.\n\tentries = []*LogEntry{entry3}\n\tresp = s.AppendEntries(newAppendEntriesRequest(2, 1, 1, 2, \"ldr\", entries))\n\tif resp.Term() != 2 || !resp.Success() || s.(*server).log.commitIndex != 2 {\n\t\tt.Fatalf(\"AppendEntries should have succeeded: %v/%v\", resp.Term, resp.Success)\n\t}\n\n\tentries = []*LogEntry{entry1, entry3}\n\tfor i, entry := range s.(*server).log.entries {\n\t\tif entry.Term() != entries[i].Term() || entry.Index() != entries[i].Index() || !bytes.Equal(entry.Command(), entries[i].Command()) {\n\t\t\tt.Fatalf(\"AppendEntries failed: %v/%v\", resp.Term, resp.Success)\n\t\t}\n\t}\n}\n\n//--------------------------------------\n// Command Execution\n//--------------------------------------\n\n// Ensure that a follower cannot execute a command.\nfunc TestServerDenyCommandExecutionWhenFollower(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\ts.Start()\n\tdefer s.Stop()\n\tvar err error\n\tif _, err = s.Do(&testCommand1{Val: \"foo\", I: 10}); err != NotLeaderError {\n\t\tt.Fatalf(\"Expected error: %v, got: %v\", NotLeaderError, err)\n\t}\n}\n\n//--------------------------------------\n// Recovery\n//--------------------------------------\n\n// Ensure that a follower cannot execute a command.\nfunc TestServerRecoverFromPreviousLogAndConf(t *testing.T) {\n\t// Initialize the servers.\n\tvar mutex sync.RWMutex\n\tservers := map[string]Server{}\n\n\ttransporter := &testTransporter{}\n\ttransporter.sendVoteRequestFunc = func(s Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\t\tmutex.RLock()\n\t\ttarget := servers[peer.Name]\n\t\tmutex.RUnlock()\n\n\t\tb, _ := json.Marshal(req)\n\t\tclonedReq := &RequestVoteRequest{}\n\t\tjson.Unmarshal(b, clonedReq)\n\n\t\treturn target.RequestVote(clonedReq)\n\t}\n\ttransporter.sendAppendEntriesRequestFunc = func(s Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\t\tmutex.RLock()\n\t\ttarget := servers[peer.Name]\n\t\tmutex.RUnlock()\n\n\t\tb, _ := json.Marshal(req)\n\t\tclonedReq := &AppendEntriesRequest{}\n\t\tjson.Unmarshal(b, clonedReq)\n\n\t\treturn target.AppendEntries(clonedReq)\n\t}\n\n\tdisTransporter := &testTransporter{}\n\tdisTransporter.sendVoteRequestFunc = func(s Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\t\treturn nil\n\t}\n\tdisTransporter.sendAppendEntriesRequestFunc = func(s Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\t\treturn nil\n\t}\n\n\tvar names []string\n\tvar paths = make(map[string]string)\n\n\tn := 5\n\n\t// add n servers\n\tfor i := 1; i <= n; i++ {\n\t\tnames = append(names, strconv.Itoa(i))\n\t}\n\n\tvar leader Server\n\tfor _, name := range names {\n\t\ts := newTestServer(name, transporter)\n\n\t\tmutex.Lock()\n\t\tservers[name] = s\n\t\tmutex.Unlock()\n\t\tpaths[name] = s.Path()\n\n\t\tif name == \"1\" {\n\t\t\tleader = s\n\t\t\ts.SetHeartbeatInterval(testHeartbeatInterval)\n\t\t\ts.Start()\n\t\t\ttime.Sleep(testHeartbeatInterval)\n\t\t} else {\n\t\t\ts.SetElectionTimeout(testElectionTimeout)\n\t\t\ts.SetHeartbeatInterval(testHeartbeatInterval)\n\t\t\ts.Start()\n\t\t\ttime.Sleep(testHeartbeatInterval)\n\t\t}\n\t\tif _, err := leader.Do(&DefaultJoinCommand{Name: name}); err != nil {\n\t\t\tt.Fatalf(\"Unable to join server[%s]: %v\", name, err)\n\t\t}\n\n\t}\n\n\t// commit some commands\n\tfor i := 0; i < 10; i++ {\n\t\tif _, err := leader.Do(&testCommand2{X: 1}); err != nil {\n\t\t\tt.Fatalf(\"cannot commit command: %s\", err.Error())\n\t\t}\n\t}\n\n\ttime.Sleep(2 * testHeartbeatInterval)\n\n\tfor _, name := range names {\n\t\ts := servers[name]\n\t\tif s.CommitIndex() != 16 {\n\t\t\tt.Fatalf(\"%s commitIndex is invalid [%d/%d]\", name, s.CommitIndex(), 16)\n\t\t}\n\t\ts.Stop()\n\t}\n\n\tfor _, name := range names {\n\t\t// with old path and disable transportation\n\t\ts := newTestServerWithPath(name, disTransporter, paths[name])\n\t\tservers[name] = s\n\n\t\ts.Start()\n\n\t\t// should only commit to the last join command\n\t\tif s.CommitIndex() != 6 {\n\t\t\tt.Fatalf(\"%s recover phase 1 commitIndex is invalid [%d/%d]\", name, s.CommitIndex(), 6)\n\t\t}\n\n\t\t// peer conf should be recovered\n\t\tif len(s.Peers()) != 4 {\n\t\t\tt.Fatalf(\"%s recover phase 1 peer failed! [%d/%d]\", name, len(s.Peers()), 4)\n\t\t}\n\t}\n\n\t// let nodes talk to each other\n\tfor _, name := range names {\n\t\tservers[name].SetTransporter(transporter)\n\t}\n\n\ttime.Sleep(2 * testElectionTimeout)\n\n\t// should commit to the previous index + 1(nop command when new leader elected)\n\tfor _, name := range names {\n\t\ts := servers[name]\n\t\tif s.CommitIndex() != 17 {\n\t\t\tt.Fatalf(\"%s commitIndex is invalid [%d/%d]\", name, s.CommitIndex(), 17)\n\t\t}\n\t\ts.Stop()\n\t}\n}\n\n//--------------------------------------\n// Membership\n//--------------------------------------\n\n// Ensure that we can start a single server and append to its log.\nfunc TestServerSingleNode(t *testing.T) {\n\ts := newTestServer(\"1\", &testTransporter{})\n\tif s.State() != Stopped {\n\t\tt.Fatalf(\"Unexpected server state: %v\", s.State())\n\t}\n\n\ts.Start()\n\n\ttime.Sleep(testHeartbeatInterval)\n\n\t// Join the server to itself.\n\tif _, err := s.Do(&DefaultJoinCommand{Name: \"1\"}); err != nil {\n\t\tt.Fatalf(\"Unable to join: %v\", err)\n\t}\n\tdebugln(\"finish command\")\n\n\tif s.State() != Leader {\n\t\tt.Fatalf(\"Unexpected server state: %v\", s.State())\n\t}\n\n\ts.Stop()\n\n\tif s.State() != Stopped {\n\t\tt.Fatalf(\"Unexpected server state: %v\", s.State())\n\t}\n}\n\n// Ensure that we can start multiple servers and determine a leader.\nfunc TestServerMultiNode(t *testing.T) {\n\t// Initialize the servers.\n\tvar mutex sync.RWMutex\n\tservers := map[string]Server{}\n\n\ttransporter := &testTransporter{}\n\ttransporter.sendVoteRequestFunc = func(s Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\t\tmutex.RLock()\n\t\ttarget := servers[peer.Name]\n\t\tmutex.RUnlock()\n\n\t\tb, _ := json.Marshal(req)\n\t\tclonedReq := &RequestVoteRequest{}\n\t\tjson.Unmarshal(b, clonedReq)\n\n\t\tc := make(chan *RequestVoteResponse)\n\n\t\tgo func() {\n\t\t\tc <- target.RequestVote(clonedReq)\n\t\t}()\n\n\t\tselect {\n\t\tcase resp := <-c:\n\t\t\treturn resp\n\t\tcase <-time.After(time.Millisecond * 200):\n\t\t\treturn nil\n\t\t}\n\n\t}\n\ttransporter.sendAppendEntriesRequestFunc = func(s Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\t\tmutex.RLock()\n\t\ttarget := servers[peer.Name]\n\t\tmutex.RUnlock()\n\n\t\tb, _ := json.Marshal(req)\n\t\tclonedReq := &AppendEntriesRequest{}\n\t\tjson.Unmarshal(b, clonedReq)\n\n\t\tc := make(chan *AppendEntriesResponse)\n\n\t\tgo func() {\n\t\t\tc <- target.AppendEntries(clonedReq)\n\t\t}()\n\n\t\tselect {\n\t\tcase resp := <-c:\n\t\t\treturn resp\n\t\tcase <-time.After(time.Millisecond * 200):\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tdisTransporter := &testTransporter{}\n\tdisTransporter.sendVoteRequestFunc = func(s Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\t\treturn nil\n\t}\n\tdisTransporter.sendAppendEntriesRequestFunc = func(s Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\t\treturn nil\n\t}\n\n\tvar names []string\n\n\tn := 5\n\n\t// add n servers\n\tfor i := 1; i <= n; i++ {\n\t\tnames = append(names, strconv.Itoa(i))\n\t}\n\n\tvar leader Server\n\tfor _, name := range names {\n\t\ts := newTestServer(name, transporter)\n\t\tdefer s.Stop()\n\n\t\tmutex.Lock()\n\t\tservers[name] = s\n\t\tmutex.Unlock()\n\n\t\tif name == \"1\" {\n\t\t\tleader = s\n\t\t\ts.SetHeartbeatInterval(testHeartbeatInterval)\n\t\t\ts.Start()\n\t\t\ttime.Sleep(testHeartbeatInterval)\n\t\t} else {\n\t\t\ts.SetElectionTimeout(testElectionTimeout)\n\t\t\ts.SetHeartbeatInterval(testHeartbeatInterval)\n\t\t\ts.Start()\n\t\t\ttime.Sleep(testHeartbeatInterval)\n\t\t}\n\t\tif _, err := leader.Do(&DefaultJoinCommand{Name: name}); err != nil {\n\t\t\tt.Fatalf(\"Unable to join server[%s]: %v\", name, err)\n\t\t}\n\n\t}\n\ttime.Sleep(2 * testElectionTimeout)\n\n\t// Check that two peers exist on leader.\n\tmutex.RLock()\n\tif leader.MemberCount() != n {\n\t\tt.Fatalf(\"Expected member count to be %v, got %v\", n, leader.MemberCount())\n\t}\n\tif servers[\"2\"].State() == Leader || servers[\"3\"].State() == Leader {\n\t\tt.Fatalf(\"Expected leader should be 1: 2=%v, 3=%v\\n\", servers[\"2\"].State(), servers[\"3\"].State())\n\t}\n\tmutex.RUnlock()\n\n\tfor i := 0; i < 20; i++ {\n\t\tretry := 0\n\t\tfmt.Println(\"Round \", i)\n\n\t\tnum := strconv.Itoa(i%(len(servers)) + 1)\n\t\tnum_1 := strconv.Itoa((i+3)%(len(servers)) + 1)\n\t\ttoStop := servers[num]\n\t\ttoStop_1 := servers[num_1]\n\n\t\t// Stop the first server and wait for a re-election.\n\t\ttime.Sleep(2 * testElectionTimeout)\n\t\tdebugln(\"Disconnect \", toStop.Name())\n\t\tdebugln(\"disconnect \", num, \" \", num_1)\n\t\ttoStop.SetTransporter(disTransporter)\n\t\ttoStop_1.SetTransporter(disTransporter)\n\t\ttime.Sleep(2 * testElectionTimeout)\n\t\t// Check that either server 2 or 3 is the leader now.\n\t\t//mutex.Lock()\n\n\t\tleader := 0\n\n\t\tfor key, value := range servers {\n\t\t\tdebugln(\"Play begin\")\n\t\t\tif key != num && key != num_1 {\n\t\t\t\tif value.State() == Leader {\n\t\t\t\t\tdebugln(\"Found leader\")\n\t\t\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\t\t\tdebugln(\"[Test] do \", value.Name())\n\t\t\t\t\t\tif _, err := value.Do(&testCommand2{X: 1}); err != nil {\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\tdebugln(\"[Test] Done\")\n\t\t\t\t\t}\n\t\t\t\t\tdebugln(\"Leader is \", value.Name(), \" Index \", value.(*server).log.commitIndex)\n\t\t\t\t}\n\t\t\t\tdebugln(\"Not Found leader\")\n\t\t\t}\n\t\t}\n\t\tfor {\n\t\t\tfor key, value := range servers {\n\t\t\t\tif key != num && key != num_1 {\n\t\t\t\t\tif value.State() == Leader {\n\t\t\t\t\t\tleader++\n\t\t\t\t\t}\n\t\t\t\t\tdebugln(value.Name(), \" \", value.(*server).Term(), \" \", value.State())\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif leader > 1 {\n\t\t\t\tif retry < 300 {\n\t\t\t\t\tdebugln(\"retry\")\n\t\t\t\t\tretry++\n\t\t\t\t\tleader = 0\n\t\t\t\t\ttime.Sleep(2 * testElectionTimeout)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tt.Fatalf(\"wrong leader number %v\", leader)\n\t\t\t}\n\t\t\tif leader == 0 {\n\t\t\t\tif retry < 300 {\n\t\t\t\t\tretry++\n\t\t\t\t\tfmt.Println(\"retry 0\")\n\t\t\t\t\tleader = 0\n\t\t\t\t\ttime.Sleep(2 * testElectionTimeout)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tt.Fatalf(\"wrong leader number %v\", leader)\n\t\t\t}\n\t\t\tif leader == 1 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t//mutex.Unlock()\n\n\t\ttoStop.SetTransporter(transporter)\n\t\ttoStop_1.SetTransporter(transporter)\n\t}\n\n}\n"
        },
        {
          "name": "snapshot.go",
          "type": "blob",
          "size": 6.9599609375,
          "content": "package raft\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\n\t\"code.google.com/p/gogoprotobuf/proto\"\n\t\"github.com/goraft/raft/protobuf\"\n)\n\n// Snapshot represents an in-memory representation of the current state of the system.\ntype Snapshot struct {\n\tLastIndex uint64 `json:\"lastIndex\"`\n\tLastTerm  uint64 `json:\"lastTerm\"`\n\n\t// Cluster configuration.\n\tPeers []*Peer `json:\"peers\"`\n\tState []byte  `json:\"state\"`\n\tPath  string  `json:\"path\"`\n}\n\n// The request sent to a server to start from the snapshot.\ntype SnapshotRecoveryRequest struct {\n\tLeaderName string\n\tLastIndex  uint64\n\tLastTerm   uint64\n\tPeers      []*Peer\n\tState      []byte\n}\n\n// The response returned from a server appending entries to the log.\ntype SnapshotRecoveryResponse struct {\n\tTerm        uint64\n\tSuccess     bool\n\tCommitIndex uint64\n}\n\n// The request sent to a server to start from the snapshot.\ntype SnapshotRequest struct {\n\tLeaderName string\n\tLastIndex  uint64\n\tLastTerm   uint64\n}\n\n// The response returned if the follower entered snapshot state\ntype SnapshotResponse struct {\n\tSuccess bool `json:\"success\"`\n}\n\n// save writes the snapshot to file.\nfunc (ss *Snapshot) save() error {\n\t// Open the file for writing.\n\tfile, err := os.OpenFile(ss.Path, os.O_CREATE|os.O_WRONLY, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer file.Close()\n\n\t// Serialize to JSON.\n\tb, err := json.Marshal(ss)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Generate checksum and write it to disk.\n\tchecksum := crc32.ChecksumIEEE(b)\n\tif _, err = fmt.Fprintf(file, \"%08x\\n\", checksum); err != nil {\n\t\treturn err\n\t}\n\n\t// Write the snapshot to disk.\n\tif _, err = file.Write(b); err != nil {\n\t\treturn err\n\t}\n\n\t// Ensure that the snapshot has been flushed to disk before continuing.\n\tif err := file.Sync(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// remove deletes the snapshot file.\nfunc (ss *Snapshot) remove() error {\n\tif err := os.Remove(ss.Path); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Creates a new Snapshot request.\nfunc newSnapshotRecoveryRequest(leaderName string, snapshot *Snapshot) *SnapshotRecoveryRequest {\n\treturn &SnapshotRecoveryRequest{\n\t\tLeaderName: leaderName,\n\t\tLastIndex:  snapshot.LastIndex,\n\t\tLastTerm:   snapshot.LastTerm,\n\t\tPeers:      snapshot.Peers,\n\t\tState:      snapshot.State,\n\t}\n}\n\n// Encodes the SnapshotRecoveryRequest to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (req *SnapshotRecoveryRequest) Encode(w io.Writer) (int, error) {\n\n\tprotoPeers := make([]*protobuf.SnapshotRecoveryRequest_Peer, len(req.Peers))\n\n\tfor i, peer := range req.Peers {\n\t\tprotoPeers[i] = &protobuf.SnapshotRecoveryRequest_Peer{\n\t\t\tName:             proto.String(peer.Name),\n\t\t\tConnectionString: proto.String(peer.ConnectionString),\n\t\t}\n\t}\n\n\tpb := &protobuf.SnapshotRecoveryRequest{\n\t\tLeaderName: proto.String(req.LeaderName),\n\t\tLastIndex:  proto.Uint64(req.LastIndex),\n\t\tLastTerm:   proto.Uint64(req.LastTerm),\n\t\tPeers:      protoPeers,\n\t\tState:      req.State,\n\t}\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the SnapshotRecoveryRequest from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (req *SnapshotRecoveryRequest) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.SnapshotRecoveryRequest{}\n\tif err = proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treq.LeaderName = pb.GetLeaderName()\n\treq.LastIndex = pb.GetLastIndex()\n\treq.LastTerm = pb.GetLastTerm()\n\treq.State = pb.GetState()\n\n\treq.Peers = make([]*Peer, len(pb.Peers))\n\n\tfor i, peer := range pb.Peers {\n\t\treq.Peers[i] = &Peer{\n\t\t\tName:             peer.GetName(),\n\t\t\tConnectionString: peer.GetConnectionString(),\n\t\t}\n\t}\n\n\treturn totalBytes, nil\n}\n\n// Creates a new Snapshot response.\nfunc newSnapshotRecoveryResponse(term uint64, success bool, commitIndex uint64) *SnapshotRecoveryResponse {\n\treturn &SnapshotRecoveryResponse{\n\t\tTerm:        term,\n\t\tSuccess:     success,\n\t\tCommitIndex: commitIndex,\n\t}\n}\n\n// Encode writes the response to a writer.\n// Returns the number of bytes written and any error that occurs.\nfunc (req *SnapshotRecoveryResponse) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.SnapshotRecoveryResponse{\n\t\tTerm:        proto.Uint64(req.Term),\n\t\tSuccess:     proto.Bool(req.Success),\n\t\tCommitIndex: proto.Uint64(req.CommitIndex),\n\t}\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the SnapshotRecoveryResponse from a buffer.\nfunc (req *SnapshotRecoveryResponse) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.SnapshotRecoveryResponse{}\n\tif err := proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treq.Term = pb.GetTerm()\n\treq.Success = pb.GetSuccess()\n\treq.CommitIndex = pb.GetCommitIndex()\n\n\treturn totalBytes, nil\n}\n\n// Creates a new Snapshot request.\nfunc newSnapshotRequest(leaderName string, snapshot *Snapshot) *SnapshotRequest {\n\treturn &SnapshotRequest{\n\t\tLeaderName: leaderName,\n\t\tLastIndex:  snapshot.LastIndex,\n\t\tLastTerm:   snapshot.LastTerm,\n\t}\n}\n\n// Encodes the SnapshotRequest to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (req *SnapshotRequest) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.SnapshotRequest{\n\t\tLeaderName: proto.String(req.LeaderName),\n\t\tLastIndex:  proto.Uint64(req.LastIndex),\n\t\tLastTerm:   proto.Uint64(req.LastTerm),\n\t}\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the SnapshotRequest from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (req *SnapshotRequest) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.SnapshotRequest{}\n\n\tif err := proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\treq.LeaderName = pb.GetLeaderName()\n\treq.LastIndex = pb.GetLastIndex()\n\treq.LastTerm = pb.GetLastTerm()\n\n\treturn totalBytes, nil\n}\n\n// Creates a new Snapshot response.\nfunc newSnapshotResponse(success bool) *SnapshotResponse {\n\treturn &SnapshotResponse{\n\t\tSuccess: success,\n\t}\n}\n\n// Encodes the SnapshotResponse to a buffer. Returns the number of bytes\n// written and any error that may have occurred.\nfunc (resp *SnapshotResponse) Encode(w io.Writer) (int, error) {\n\tpb := &protobuf.SnapshotResponse{\n\t\tSuccess: proto.Bool(resp.Success),\n\t}\n\tp, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\n\treturn w.Write(p)\n}\n\n// Decodes the SnapshotResponse from a buffer. Returns the number of bytes read and\n// any error that occurs.\nfunc (resp *SnapshotResponse) Decode(r io.Reader) (int, error) {\n\tdata, err := ioutil.ReadAll(r)\n\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttotalBytes := len(data)\n\n\tpb := &protobuf.SnapshotResponse{}\n\tif err := proto.Unmarshal(data, pb); err != nil {\n\t\treturn -1, err\n\t}\n\n\tresp.Success = pb.GetSuccess()\n\n\treturn totalBytes, nil\n}\n"
        },
        {
          "name": "snapshot_test.go",
          "type": "blob",
          "size": 2.828125,
          "content": "package raft\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/mock\"\n)\n\n// Ensure that a snapshot occurs when there are existing logs.\nfunc TestSnapshot(t *testing.T) {\n\trunServerWithMockStateMachine(Leader, func(s Server, m *mock.Mock) {\n\t\tm.On(\"Save\").Return([]byte(\"foo\"), nil)\n\t\tm.On(\"Recovery\", []byte(\"foo\")).Return(nil)\n\n\t\ts.Do(&testCommand1{})\n\t\terr := s.TakeSnapshot()\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, s.(*server).snapshot.LastIndex, uint64(2))\n\n\t\t// Repeat to make sure new snapshot gets created.\n\t\ts.Do(&testCommand1{})\n\t\terr = s.TakeSnapshot()\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, s.(*server).snapshot.LastIndex, uint64(4))\n\n\t\t// Restart server.\n\t\ts.Stop()\n\t\t// Recover from snapshot.\n\t\terr = s.LoadSnapshot()\n\t\tassert.NoError(t, err)\n\t\ts.Start()\n\t})\n}\n\n// Ensure that a new server can recover from previous snapshot with log\nfunc TestSnapshotRecovery(t *testing.T) {\n\trunServerWithMockStateMachine(Leader, func(s Server, m *mock.Mock) {\n\t\tm.On(\"Save\").Return([]byte(\"foo\"), nil)\n\t\tm.On(\"Recovery\", []byte(\"foo\")).Return(nil)\n\n\t\ts.Do(&testCommand1{})\n\t\terr := s.TakeSnapshot()\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, s.(*server).snapshot.LastIndex, uint64(2))\n\n\t\t// Repeat to make sure new snapshot gets created.\n\t\ts.Do(&testCommand1{})\n\n\t\t// Stop the old server\n\t\ts.Stop()\n\n\t\t// create a new server with previous log and snapshot\n\t\tnewS, err := NewServer(\"1\", s.Path(), &testTransporter{}, s.StateMachine(), nil, \"\")\n\t\t// Recover from snapshot.\n\t\terr = newS.LoadSnapshot()\n\t\tassert.NoError(t, err)\n\n\t\tnewS.Start()\n\t\tdefer newS.Stop()\n\n\t\t// wait for it to become leader\n\t\ttime.Sleep(time.Second)\n\t\t// ensure server load the previous log\n\t\tassert.Equal(t, len(newS.LogEntries()), 3, \"\")\n\t})\n}\n\n// Ensure that a snapshot request can be sent and received.\nfunc TestSnapshotRequest(t *testing.T) {\n\trunServerWithMockStateMachine(Follower, func(s Server, m *mock.Mock) {\n\t\tm.On(\"Recovery\", []byte(\"bar\")).Return(nil)\n\n\t\t// Send snapshot request.\n\t\tresp := s.RequestSnapshot(&SnapshotRequest{LastIndex: 5, LastTerm: 1})\n\t\tassert.Equal(t, resp.Success, true)\n\t\tassert.Equal(t, s.State(), Snapshotting)\n\n\t\t// Send recovery request.\n\t\tresp2 := s.SnapshotRecoveryRequest(&SnapshotRecoveryRequest{\n\t\t\tLeaderName: \"1\",\n\t\t\tLastIndex:  5,\n\t\t\tLastTerm:   2,\n\t\t\tPeers:      make([]*Peer, 0),\n\t\t\tState:      []byte(\"bar\"),\n\t\t})\n\t\tassert.Equal(t, resp2.Success, true)\n\t})\n}\n\nfunc runServerWithMockStateMachine(state string, fn func(s Server, m *mock.Mock)) {\n\tvar m mockStateMachine\n\ts := newTestServer(\"1\", &testTransporter{})\n\ts.(*server).stateMachine = &m\n\tif err := s.Start(); err != nil {\n\t\tpanic(\"server start error: \" + err.Error())\n\t}\n\tif state == Leader {\n\t\tif _, err := s.Do(&DefaultJoinCommand{Name: s.Name()}); err != nil {\n\t\t\tpanic(\"unable to join server to self: \" + err.Error())\n\t\t}\n\t}\n\tdefer s.Stop()\n\tfn(s, &m.Mock)\n}\n"
        },
        {
          "name": "statemachine.go",
          "type": "blob",
          "size": 0.2626953125,
          "content": "package raft\n\n// StateMachine is the interface for allowing the host application to save and\n// recovery the state machine. This makes it possible to make snapshots\n// and compact the log.\ntype StateMachine interface {\n\tSave() ([]byte, error)\n\tRecovery([]byte) error\n}\n"
        },
        {
          "name": "statemachine_test.go",
          "type": "blob",
          "size": 0.3173828125,
          "content": "package raft\n\nimport (\n\t\"github.com/stretchr/testify/mock\"\n)\n\ntype mockStateMachine struct {\n\tmock.Mock\n}\n\nfunc (m *mockStateMachine) Save() ([]byte, error) {\n\targs := m.Called()\n\treturn args.Get(0).([]byte), args.Error(1)\n}\n\nfunc (m *mockStateMachine) Recovery(b []byte) error {\n\targs := m.Called(b)\n\treturn args.Error(0)\n}\n"
        },
        {
          "name": "test.go",
          "type": "blob",
          "size": 4.896484375,
          "content": "package raft\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"time\"\n)\n\nconst (\n\ttestHeartbeatInterval = 50 * time.Millisecond\n\ttestElectionTimeout   = 200 * time.Millisecond\n)\n\nconst (\n\ttestListenerLoggerEnabled = false\n)\n\nfunc init() {\n\tRegisterCommand(&testCommand1{})\n\tRegisterCommand(&testCommand2{})\n}\n\n//------------------------------------------------------------------------------\n//\n// Helpers\n//\n//------------------------------------------------------------------------------\n\n//--------------------------------------\n// Logs\n//--------------------------------------\n\nfunc getLogPath() string {\n\tf, _ := ioutil.TempFile(\"\", \"raft-log-\")\n\tf.Close()\n\tos.Remove(f.Name())\n\treturn f.Name()\n}\n\nfunc setupLog(entries []*LogEntry) (*Log, string) {\n\tf, _ := ioutil.TempFile(\"\", \"raft-log-\")\n\n\tfor _, entry := range entries {\n\t\tentry.Encode(f)\n\t}\n\terr := f.Close()\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tlog := newLog()\n\tlog.ApplyFunc = func(e *LogEntry, c Command) (interface{}, error) {\n\t\treturn nil, nil\n\t}\n\tif err := log.open(f.Name()); err != nil {\n\t\tpanic(err)\n\t}\n\treturn log, f.Name()\n}\n\n//--------------------------------------\n// Servers\n//--------------------------------------\n\nfunc newTestServer(name string, transporter Transporter) Server {\n\tp, _ := ioutil.TempDir(\"\", \"raft-server-\")\n\tif err := os.MkdirAll(p, 0644); err != nil {\n\t\tpanic(err.Error())\n\t}\n\tserver, _ := NewServer(name, p, transporter, nil, nil, \"\")\n\tif testListenerLoggerEnabled {\n\t\tfn := func(e Event) {\n\t\t\tserver := e.Source().(Server)\n\t\t\twarnf(\"[%s] %s %v -> %v\\n\", server.Name(), e.Type(), e.PrevValue(), e.Value())\n\t\t}\n\t\tserver.AddEventListener(StateChangeEventType, fn)\n\t\tserver.AddEventListener(LeaderChangeEventType, fn)\n\t\tserver.AddEventListener(TermChangeEventType, fn)\n\t}\n\treturn server\n}\n\nfunc newTestServerWithPath(name string, transporter Transporter, p string) Server {\n\tserver, _ := NewServer(name, p, transporter, nil, nil, \"\")\n\treturn server\n}\n\nfunc newTestServerWithLog(name string, transporter Transporter, entries []*LogEntry) Server {\n\tserver := newTestServer(name, transporter)\n\tf, err := os.Create(server.LogPath())\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, entry := range entries {\n\t\tentry.Encode(f)\n\t}\n\tf.Close()\n\treturn server\n}\n\nfunc newTestCluster(names []string, transporter Transporter, lookup map[string]Server) []Server {\n\tservers := []Server{}\n\te0, _ := newLogEntry(newLog(), nil, 1, 1, &testCommand1{Val: \"foo\", I: 20})\n\n\tfor _, name := range names {\n\t\tif lookup[name] != nil {\n\t\t\tpanic(fmt.Sprintf(\"raft: Duplicate server in test cluster! %v\", name))\n\t\t}\n\t\tserver := newTestServerWithLog(\"1\", transporter, []*LogEntry{e0})\n\t\tserver.SetElectionTimeout(testElectionTimeout)\n\t\tservers = append(servers, server)\n\t\tlookup[name] = server\n\t}\n\tfor _, server := range servers {\n\t\tserver.SetHeartbeatInterval(testHeartbeatInterval)\n\t\tserver.Start()\n\t\tfor _, peer := range servers {\n\t\t\tserver.AddPeer(peer.Name(), \"\")\n\t\t}\n\t}\n\treturn servers\n}\n\n//--------------------------------------\n// Transporter\n//--------------------------------------\n\ntype testTransporter struct {\n\tsendVoteRequestFunc          func(server Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse\n\tsendAppendEntriesRequestFunc func(server Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse\n\tsendSnapshotRequestFunc      func(server Server, peer *Peer, req *SnapshotRequest) *SnapshotResponse\n}\n\nfunc (t *testTransporter) SendVoteRequest(server Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse {\n\treturn t.sendVoteRequestFunc(server, peer, req)\n}\n\nfunc (t *testTransporter) SendAppendEntriesRequest(server Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse {\n\treturn t.sendAppendEntriesRequestFunc(server, peer, req)\n}\n\nfunc (t *testTransporter) SendSnapshotRequest(server Server, peer *Peer, req *SnapshotRequest) *SnapshotResponse {\n\treturn t.sendSnapshotRequestFunc(server, peer, req)\n}\n\nfunc (t *testTransporter) SendSnapshotRecoveryRequest(server Server, peer *Peer, req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse {\n\treturn t.SendSnapshotRecoveryRequest(server, peer, req)\n}\n\ntype testStateMachine struct {\n\tsaveFunc     func() ([]byte, error)\n\trecoveryFunc func([]byte) error\n}\n\nfunc (sm *testStateMachine) Save() ([]byte, error) {\n\treturn sm.saveFunc()\n}\n\nfunc (sm *testStateMachine) Recovery(state []byte) error {\n\treturn sm.recoveryFunc(state)\n}\n\n//--------------------------------------\n// Command1\n//--------------------------------------\n\ntype testCommand1 struct {\n\tVal string `json:\"val\"`\n\tI   int    `json:\"i\"`\n}\n\nfunc (c *testCommand1) CommandName() string {\n\treturn \"cmd_1\"\n}\n\nfunc (c *testCommand1) Apply(server Server) (interface{}, error) {\n\treturn nil, nil\n}\n\n//--------------------------------------\n// Command2\n//--------------------------------------\n\ntype testCommand2 struct {\n\tX int `json:\"x\"`\n}\n\nfunc (c *testCommand2) CommandName() string {\n\treturn \"cmd_2\"\n}\n\nfunc (c *testCommand2) Apply(server Server) (interface{}, error) {\n\treturn nil, nil\n}\n"
        },
        {
          "name": "transporter.go",
          "type": "blob",
          "size": 0.708984375,
          "content": "package raft\n\n//------------------------------------------------------------------------------\n//\n// Typedefs\n//\n//------------------------------------------------------------------------------\n\n// Transporter is the interface for allowing the host application to transport\n// requests to other nodes.\ntype Transporter interface {\n\tSendVoteRequest(server Server, peer *Peer, req *RequestVoteRequest) *RequestVoteResponse\n\tSendAppendEntriesRequest(server Server, peer *Peer, req *AppendEntriesRequest) *AppendEntriesResponse\n\tSendSnapshotRequest(server Server, peer *Peer, req *SnapshotRequest) *SnapshotResponse\n\tSendSnapshotRecoveryRequest(server Server, peer *Peer, req *SnapshotRecoveryRequest) *SnapshotRecoveryResponse\n}\n"
        },
        {
          "name": "util.go",
          "type": "blob",
          "size": 1.638671875,
          "content": "package raft\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"os\"\n\t\"time\"\n)\n\n// uint64Slice implements sort interface\ntype uint64Slice []uint64\n\nfunc (p uint64Slice) Len() int           { return len(p) }\nfunc (p uint64Slice) Less(i, j int) bool { return p[i] < p[j] }\nfunc (p uint64Slice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\n\n// WriteFile writes data to a file named by filename.\n// If the file does not exist, WriteFile creates it with permissions perm;\n// otherwise WriteFile truncates it before writing.\n// This is copied from ioutil.WriteFile with the addition of a Sync call to\n// ensure the data reaches the disk.\nfunc writeFileSynced(filename string, data []byte, perm os.FileMode) error {\n\tf, err := os.OpenFile(filename, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, perm)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close() // Idempotent\n\n\tn, err := f.Write(data)\n\tif err == nil && n < len(data) {\n\t\treturn io.ErrShortWrite\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\tif err = f.Sync(); err != nil {\n\t\treturn err\n\t}\n\n\treturn f.Close()\n}\n\n// Waits for a random time between two durations and sends the current time on\n// the returned channel.\nfunc afterBetween(min time.Duration, max time.Duration) <-chan time.Time {\n\trand := rand.New(rand.NewSource(time.Now().UnixNano()))\n\td, delta := min, (max - min)\n\tif delta > 0 {\n\t\td += time.Duration(rand.Int63n(int64(delta)))\n\t}\n\treturn time.After(d)\n}\n\n// TODO(xiangli): Remove assertions when we reach version 1.0\n\n// _assert will panic with a given formatted message if the given condition is false.\nfunc _assert(condition bool, msg string, v ...interface{}) {\n\tif !condition {\n\t\tpanic(fmt.Sprintf(\"assertion failed: \"+msg, v...))\n\t}\n}\n"
        },
        {
          "name": "z_test.go",
          "type": "blob",
          "size": 0.13671875,
          "content": "package raft\n\n/*\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestGC(t *testing.T) {\n\t<-time.After(500 * time.Millisecond)\n\tpanic(\"Oh god no!\")\n}\n*/\n"
        }
      ]
    }
  ]
}