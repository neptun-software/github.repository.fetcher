{
  "metadata": {
    "timestamp": 1736566486207,
    "page": 32,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "klauspost/compress",
      "stars": 4867,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.025390625,
          "content": "* -text\n*.bin -text -diff\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3515625,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n/s2/cmd/_s2sx/sfx-exe\n\n# Linux perf files\nperf.data\nperf.data.old\n\n# gdb history\n.gdb_history\n"
        },
        {
          "name": ".goreleaser.yml",
          "type": "blob",
          "size": 1.9462890625,
          "content": "version: 2\n\nbefore:\n  hooks:\n    - ./gen.sh\n\nbuilds:\n  -\n    id: \"s2c\"\n    binary: s2c\n    main: ./s2/cmd/s2c/main.go\n    flags:\n      - -trimpath\n    env:\n      - CGO_ENABLED=0\n    goos:\n      - aix\n      - linux\n      - freebsd\n      - netbsd\n      - windows\n      - darwin\n    goarch:\n      - 386\n      - amd64\n      - arm\n      - arm64\n      - ppc64\n      - ppc64le\n      - mips64\n      - mips64le\n    goarm:\n      - 7\n  -\n    id: \"s2d\"\n    binary: s2d\n    main: ./s2/cmd/s2d/main.go\n    flags:\n      - -trimpath\n    env:\n      - CGO_ENABLED=0\n    goos:\n      - aix\n      - linux\n      - freebsd\n      - netbsd\n      - windows\n      - darwin\n    goarch:\n      - 386\n      - amd64\n      - arm\n      - arm64\n      - ppc64\n      - ppc64le\n      - mips64\n      - mips64le\n    goarm:\n      - 7\n  -\n    id: \"s2sx\"\n    binary: s2sx\n    main: ./s2/cmd/_s2sx/main.go\n    flags:\n      - -modfile=s2sx.mod\n      - -trimpath\n    env:\n      - CGO_ENABLED=0\n    goos:\n      - aix\n      - linux\n      - freebsd\n      - netbsd\n      - windows\n      - darwin\n    goarch:\n      - 386\n      - amd64\n      - arm\n      - arm64\n      - ppc64\n      - ppc64le\n      - mips64\n      - mips64le\n    goarm:\n      - 7\n\narchives:\n  -\n    id: s2-binaries\n    name_template: \"s2-{{ .Os }}_{{ .Arch }}{{ if .Arm }}v{{ .Arm }}{{ end }}\"\n    format_overrides:\n      - goos: windows\n        format: zip\n    files:\n      - unpack/*\n      - s2/LICENSE\n      - s2/README.md\nchecksum:\n  name_template: 'checksums.txt'\nsnapshot:\n  version_template: \"{{ .Tag }}-next\"\nchangelog:\n  sort: asc\n  filters:\n    exclude:\n    - '^doc:'\n    - '^docs:'\n    - '^test:'\n    - '^tests:'\n    - '^Update\\sREADME.md'\n\nnfpms:\n  -\n    file_name_template: \"s2_package__{{ .Os }}_{{ .Arch }}{{ if .Arm }}v{{ .Arm }}{{ end }}\"\n    vendor: Klaus Post\n    homepage: https://github.com/klauspost/compress\n    maintainer: Klaus Post <klauspost@gmail.com>\n    description: S2 Compression Tool\n    license: BSD 3-Clause\n    formats:\n      - deb\n      - rpm\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 16.3408203125,
          "content": "Copyright (c) 2012 The Go Authors. All rights reserved.\nCopyright (c) 2019 Klaus Post. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n------------------\n\nFiles: gzhttp/*\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2016-2017 The New York Times Company\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------------\n\nFiles: s2/cmd/internal/readahead/*\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 Klaus Post\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---------------------\nFiles: snappy/*\nFiles: internal/snapref/*\n\nCopyright (c) 2011 The Snappy-Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n-----------------\n\nFiles: s2/cmd/internal/filepathx/*\n\nCopyright 2016 The filepathx Authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 56.8681640625,
          "content": "# compress\r\n\r\nThis package provides various compression algorithms.\r\n\r\n* [zstandard](https://github.com/klauspost/compress/tree/master/zstd#zstd) compression and decompression in pure Go.\r\n* [S2](https://github.com/klauspost/compress/tree/master/s2#s2-compression) is a high performance replacement for Snappy.\r\n* Optimized [deflate](https://godoc.org/github.com/klauspost/compress/flate) packages which can be used as a dropin replacement for [gzip](https://godoc.org/github.com/klauspost/compress/gzip), [zip](https://godoc.org/github.com/klauspost/compress/zip) and [zlib](https://godoc.org/github.com/klauspost/compress/zlib).\r\n* [snappy](https://github.com/klauspost/compress/tree/master/snappy) is a drop-in replacement for `github.com/golang/snappy` offering better compression and concurrent streams.\r\n* [huff0](https://github.com/klauspost/compress/tree/master/huff0) and [FSE](https://github.com/klauspost/compress/tree/master/fse) implementations for raw entropy encoding.\r\n* [gzhttp](https://github.com/klauspost/compress/tree/master/gzhttp) Provides client and server wrappers for handling gzipped requests efficiently.\r\n* [pgzip](https://github.com/klauspost/pgzip) is a separate package that provides a very fast parallel gzip implementation.\r\n\r\n[![Go Reference](https://pkg.go.dev/badge/klauspost/compress.svg)](https://pkg.go.dev/github.com/klauspost/compress?tab=subdirectories)\r\n[![Go](https://github.com/klauspost/compress/actions/workflows/go.yml/badge.svg)](https://github.com/klauspost/compress/actions/workflows/go.yml)\r\n[![Sourcegraph Badge](https://sourcegraph.com/github.com/klauspost/compress/-/badge.svg)](https://sourcegraph.com/github.com/klauspost/compress?badge)\r\n\r\n# changelog\r\n\r\n* Sep 23rd, 2024 - [1.17.10](https://github.com/klauspost/compress/releases/tag/v1.17.10)\r\n\t* gzhttp: Add TransportAlwaysDecompress option. https://github.com/klauspost/compress/pull/978\r\n\t* gzhttp: Add supported decompress request body by @mirecl in https://github.com/klauspost/compress/pull/1002\r\n\t* s2: Add EncodeBuffer buffer recycling callback https://github.com/klauspost/compress/pull/982\r\n\t* zstd: Improve memory usage on small streaming encodes https://github.com/klauspost/compress/pull/1007\r\n\t* flate: read data written with partial flush by @vajexal in https://github.com/klauspost/compress/pull/996\r\n\r\n* Jun 12th, 2024 - [1.17.9](https://github.com/klauspost/compress/releases/tag/v1.17.9)\r\n\t* s2: Reduce ReadFrom temporary allocations https://github.com/klauspost/compress/pull/949\r\n\t* flate, zstd: Shave some bytes off amd64 matchLen by @greatroar in https://github.com/klauspost/compress/pull/963\r\n\t* Upgrade zip/zlib to 1.22.4 upstream https://github.com/klauspost/compress/pull/970 https://github.com/klauspost/compress/pull/971\r\n\t* zstd: BuildDict fails with RLE table https://github.com/klauspost/compress/pull/951\r\n\r\n* Apr 9th, 2024 - [1.17.8](https://github.com/klauspost/compress/releases/tag/v1.17.8)\r\n\t* zstd: Reject blocks where reserved values are not 0 https://github.com/klauspost/compress/pull/885\r\n\t* zstd: Add RLE detection+encoding https://github.com/klauspost/compress/pull/938\r\n\r\n* Feb 21st, 2024 - [1.17.7](https://github.com/klauspost/compress/releases/tag/v1.17.7)\r\n\t* s2: Add AsyncFlush method: Complete the block without flushing by @Jille in https://github.com/klauspost/compress/pull/927\r\n\t* s2: Fix literal+repeat exceeds dst crash https://github.com/klauspost/compress/pull/930\r\n  \r\n* Feb 5th, 2024 - [1.17.6](https://github.com/klauspost/compress/releases/tag/v1.17.6)\r\n\t* zstd: Fix incorrect repeat coding in best mode https://github.com/klauspost/compress/pull/923\r\n\t* s2: Fix DecodeConcurrent deadlock on errors https://github.com/klauspost/compress/pull/925\r\n  \r\n* Jan 26th, 2024 - [v1.17.5](https://github.com/klauspost/compress/releases/tag/v1.17.5)\r\n\t* flate: Fix reset with dictionary on custom window encodes https://github.com/klauspost/compress/pull/912\r\n\t* zstd: Add Frame header encoding and stripping https://github.com/klauspost/compress/pull/908\r\n\t* zstd: Limit better/best default window to 8MB https://github.com/klauspost/compress/pull/913\r\n\t* zstd: Speed improvements by @greatroar in https://github.com/klauspost/compress/pull/896 https://github.com/klauspost/compress/pull/910\r\n\t* s2: Fix callbacks for skippable blocks and disallow 0xfe (Padding) by @Jille in https://github.com/klauspost/compress/pull/916 https://github.com/klauspost/compress/pull/917\r\nhttps://github.com/klauspost/compress/pull/919 https://github.com/klauspost/compress/pull/918\r\n\r\n* Dec 1st, 2023 - [v1.17.4](https://github.com/klauspost/compress/releases/tag/v1.17.4)\r\n\t* huff0: Speed up symbol counting by @greatroar in https://github.com/klauspost/compress/pull/887\r\n\t* huff0: Remove byteReader by @greatroar in https://github.com/klauspost/compress/pull/886\r\n\t* gzhttp: Allow overriding decompression on transport https://github.com/klauspost/compress/pull/892\r\n\t* gzhttp: Clamp compression level https://github.com/klauspost/compress/pull/890\r\n\t* gzip: Error out if reserved bits are set https://github.com/klauspost/compress/pull/891\r\n\r\n* Nov 15th, 2023 - [v1.17.3](https://github.com/klauspost/compress/releases/tag/v1.17.3)\r\n\t* fse: Fix max header size https://github.com/klauspost/compress/pull/881\r\n\t* zstd: Improve better/best compression https://github.com/klauspost/compress/pull/877\r\n\t* gzhttp: Fix missing content type on Close https://github.com/klauspost/compress/pull/883\r\n\r\n* Oct 22nd, 2023 - [v1.17.2](https://github.com/klauspost/compress/releases/tag/v1.17.2)\r\n\t* zstd: Fix rare *CORRUPTION* output in \"best\" mode. See https://github.com/klauspost/compress/pull/876\r\n\r\n* Oct 14th, 2023 - [v1.17.1](https://github.com/klauspost/compress/releases/tag/v1.17.1)\r\n\t* s2: Fix S2 \"best\" dictionary wrong encoding by @klauspost in https://github.com/klauspost/compress/pull/871\r\n\t* flate: Reduce allocations in decompressor and minor code improvements by @fakefloordiv in https://github.com/klauspost/compress/pull/869\r\n\t* s2: Fix EstimateBlockSize on 6&7 length input by @klauspost in https://github.com/klauspost/compress/pull/867\r\n\r\n* Sept 19th, 2023 - [v1.17.0](https://github.com/klauspost/compress/releases/tag/v1.17.0)\r\n\t* Add experimental dictionary builder  https://github.com/klauspost/compress/pull/853\r\n\t* Add xerial snappy read/writer https://github.com/klauspost/compress/pull/838\r\n\t* flate: Add limited window compression https://github.com/klauspost/compress/pull/843\r\n\t* s2: Do 2 overlapping match checks https://github.com/klauspost/compress/pull/839\r\n\t* flate: Add amd64 assembly matchlen https://github.com/klauspost/compress/pull/837\r\n\t* gzip: Copy bufio.Reader on Reset by @thatguystone in https://github.com/klauspost/compress/pull/860\r\n\r\n<details>\r\n\t<summary>See changes to v1.16.x</summary>\r\n\r\n   \r\n* July 1st, 2023 - [v1.16.7](https://github.com/klauspost/compress/releases/tag/v1.16.7)\r\n\t* zstd: Fix default level first dictionary encode https://github.com/klauspost/compress/pull/829\r\n\t* s2: add GetBufferCapacity() method by @GiedriusS in https://github.com/klauspost/compress/pull/832\r\n\r\n* June 13, 2023 - [v1.16.6](https://github.com/klauspost/compress/releases/tag/v1.16.6)\r\n\t* zstd: correctly ignore WithEncoderPadding(1) by @ianlancetaylor in https://github.com/klauspost/compress/pull/806\r\n\t* zstd: Add amd64 match length assembly https://github.com/klauspost/compress/pull/824\r\n\t* gzhttp: Handle informational headers by @rtribotte in https://github.com/klauspost/compress/pull/815\r\n\t* s2: Improve Better compression slightly https://github.com/klauspost/compress/pull/663\r\n\r\n* Apr 16, 2023 - [v1.16.5](https://github.com/klauspost/compress/releases/tag/v1.16.5)\r\n\t* zstd: readByte needs to use io.ReadFull by @jnoxon in https://github.com/klauspost/compress/pull/802\r\n\t* gzip: Fix WriterTo after initial read https://github.com/klauspost/compress/pull/804\r\n\r\n* Apr 5, 2023 - [v1.16.4](https://github.com/klauspost/compress/releases/tag/v1.16.4)\r\n\t* zstd: Improve zstd best efficiency by @greatroar and @klauspost in https://github.com/klauspost/compress/pull/784\r\n\t* zstd: Respect WithAllLitEntropyCompression https://github.com/klauspost/compress/pull/792\r\n\t* zstd: Fix amd64 not always detecting corrupt data https://github.com/klauspost/compress/pull/785\r\n\t* zstd: Various minor improvements by @greatroar in https://github.com/klauspost/compress/pull/788 https://github.com/klauspost/compress/pull/794 https://github.com/klauspost/compress/pull/795\r\n\t* s2: Fix huge block overflow https://github.com/klauspost/compress/pull/779\r\n\t* s2: Allow CustomEncoder fallback https://github.com/klauspost/compress/pull/780\r\n\t* gzhttp: Support ResponseWriter Unwrap() in gzhttp handler by @jgimenez in https://github.com/klauspost/compress/pull/799\r\n\r\n* Mar 13, 2023 - [v1.16.1](https://github.com/klauspost/compress/releases/tag/v1.16.1)\r\n\t* zstd: Speed up + improve best encoder by @greatroar in https://github.com/klauspost/compress/pull/776\r\n\t* gzhttp: Add optional [BREACH mitigation](https://github.com/klauspost/compress/tree/master/gzhttp#breach-mitigation). https://github.com/klauspost/compress/pull/762 https://github.com/klauspost/compress/pull/768 https://github.com/klauspost/compress/pull/769 https://github.com/klauspost/compress/pull/770 https://github.com/klauspost/compress/pull/767\r\n\t* s2: Add Intel LZ4s converter https://github.com/klauspost/compress/pull/766\r\n\t* zstd: Minor bug fixes https://github.com/klauspost/compress/pull/771 https://github.com/klauspost/compress/pull/772 https://github.com/klauspost/compress/pull/773\r\n\t* huff0: Speed up compress1xDo by @greatroar in https://github.com/klauspost/compress/pull/774\r\n\r\n* Feb 26, 2023 - [v1.16.0](https://github.com/klauspost/compress/releases/tag/v1.16.0)\r\n\t* s2: Add [Dictionary](https://github.com/klauspost/compress/tree/master/s2#dictionaries) support.  https://github.com/klauspost/compress/pull/685\r\n\t* s2: Add Compression Size Estimate.  https://github.com/klauspost/compress/pull/752\r\n\t* s2: Add support for custom stream encoder. https://github.com/klauspost/compress/pull/755\r\n\t* s2: Add LZ4 block converter. https://github.com/klauspost/compress/pull/748\r\n\t* s2: Support io.ReaderAt in ReadSeeker. https://github.com/klauspost/compress/pull/747\r\n\t* s2c/s2sx: Use concurrent decoding. https://github.com/klauspost/compress/pull/746\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes to v1.15.x</summary>\r\n\t\r\n* Jan 21st, 2023 (v1.15.15)\r\n\t* deflate: Improve level 7-9 by @klauspost in https://github.com/klauspost/compress/pull/739\r\n\t* zstd: Add delta encoding support by @greatroar in https://github.com/klauspost/compress/pull/728\r\n\t* zstd: Various speed improvements by @greatroar https://github.com/klauspost/compress/pull/741 https://github.com/klauspost/compress/pull/734 https://github.com/klauspost/compress/pull/736 https://github.com/klauspost/compress/pull/744 https://github.com/klauspost/compress/pull/743 https://github.com/klauspost/compress/pull/745\r\n\t* gzhttp: Add SuffixETag() and DropETag() options to prevent ETag collisions on compressed responses by @willbicks in https://github.com/klauspost/compress/pull/740\r\n\r\n* Jan 3rd, 2023 (v1.15.14)\r\n\r\n\t* flate: Improve speed in big stateless blocks https://github.com/klauspost/compress/pull/718\r\n\t* zstd: Minor speed tweaks by @greatroar in https://github.com/klauspost/compress/pull/716 https://github.com/klauspost/compress/pull/720\r\n\t* export NoGzipResponseWriter for custom ResponseWriter wrappers by @harshavardhana in https://github.com/klauspost/compress/pull/722\r\n\t* s2: Add example for indexing and existing stream https://github.com/klauspost/compress/pull/723\r\n\r\n* Dec 11, 2022 (v1.15.13)\r\n\t* zstd: Add [MaxEncodedSize](https://pkg.go.dev/github.com/klauspost/compress@v1.15.13/zstd#Encoder.MaxEncodedSize) to encoder  https://github.com/klauspost/compress/pull/691\r\n\t* zstd: Various tweaks and improvements https://github.com/klauspost/compress/pull/693 https://github.com/klauspost/compress/pull/695 https://github.com/klauspost/compress/pull/696 https://github.com/klauspost/compress/pull/701 https://github.com/klauspost/compress/pull/702 https://github.com/klauspost/compress/pull/703 https://github.com/klauspost/compress/pull/704 https://github.com/klauspost/compress/pull/705 https://github.com/klauspost/compress/pull/706 https://github.com/klauspost/compress/pull/707 https://github.com/klauspost/compress/pull/708\r\n\r\n* Oct 26, 2022 (v1.15.12)\r\n\r\n\t* zstd: Tweak decoder allocs. https://github.com/klauspost/compress/pull/680\r\n\t* gzhttp: Always delete `HeaderNoCompression` https://github.com/klauspost/compress/pull/683\r\n\r\n* Sept 26, 2022 (v1.15.11)\r\n\r\n\t* flate: Improve level 1-3 compression  https://github.com/klauspost/compress/pull/678\r\n\t* zstd: Improve \"best\" compression by @nightwolfz in https://github.com/klauspost/compress/pull/677\r\n\t* zstd: Fix+reduce decompression allocations https://github.com/klauspost/compress/pull/668\r\n\t* zstd: Fix non-effective noescape tag https://github.com/klauspost/compress/pull/667\r\n\r\n* Sept 16, 2022 (v1.15.10)\r\n\r\n\t* zstd: Add [WithDecodeAllCapLimit](https://pkg.go.dev/github.com/klauspost/compress@v1.15.10/zstd#WithDecodeAllCapLimit) https://github.com/klauspost/compress/pull/649\r\n\t* Add Go 1.19 - deprecate Go 1.16  https://github.com/klauspost/compress/pull/651\r\n\t* flate: Improve level 5+6 compression https://github.com/klauspost/compress/pull/656\r\n\t* zstd: Improve \"better\" compression  https://github.com/klauspost/compress/pull/657\r\n\t* s2: Improve \"best\" compression https://github.com/klauspost/compress/pull/658\r\n\t* s2: Improve \"better\" compression. https://github.com/klauspost/compress/pull/635\r\n\t* s2: Slightly faster non-assembly decompression https://github.com/klauspost/compress/pull/646\r\n\t* Use arrays for constant size copies https://github.com/klauspost/compress/pull/659\r\n\r\n* July 21, 2022 (v1.15.9)\r\n\r\n\t* zstd: Fix decoder crash on amd64 (no BMI) on invalid input https://github.com/klauspost/compress/pull/645\r\n\t* zstd: Disable decoder extended memory copies (amd64) due to possible crashes https://github.com/klauspost/compress/pull/644\r\n\t* zstd: Allow single segments up to \"max decoded size\" by @klauspost in https://github.com/klauspost/compress/pull/643\r\n\r\n* July 13, 2022 (v1.15.8)\r\n\r\n\t* gzip: fix stack exhaustion bug in Reader.Read https://github.com/klauspost/compress/pull/641\r\n\t* s2: Add Index header trim/restore https://github.com/klauspost/compress/pull/638\r\n\t* zstd: Optimize seqdeq amd64 asm by @greatroar in https://github.com/klauspost/compress/pull/636\r\n\t* zstd: Improve decoder memcopy https://github.com/klauspost/compress/pull/637\r\n\t* huff0: Pass a single bitReader pointer to asm by @greatroar in https://github.com/klauspost/compress/pull/634\r\n\t* zstd: Branchless getBits for amd64 w/o BMI2 by @greatroar in https://github.com/klauspost/compress/pull/640\r\n\t* gzhttp: Remove header before writing https://github.com/klauspost/compress/pull/639\r\n\r\n* June 29, 2022 (v1.15.7)\r\n\r\n\t* s2: Fix absolute forward seeks  https://github.com/klauspost/compress/pull/633\r\n\t* zip: Merge upstream  https://github.com/klauspost/compress/pull/631\r\n\t* zip: Re-add zip64 fix https://github.com/klauspost/compress/pull/624\r\n\t* zstd: translate fseDecoder.buildDtable into asm by @WojciechMula in https://github.com/klauspost/compress/pull/598\r\n\t* flate: Faster histograms  https://github.com/klauspost/compress/pull/620\r\n\t* deflate: Use compound hcode  https://github.com/klauspost/compress/pull/622\r\n\r\n* June 3, 2022 (v1.15.6)\r\n\t* s2: Improve coding for long, close matches https://github.com/klauspost/compress/pull/613\r\n\t* s2c: Add Snappy/S2 stream recompression https://github.com/klauspost/compress/pull/611\r\n\t* zstd: Always use configured block size https://github.com/klauspost/compress/pull/605\r\n\t* zstd: Fix incorrect hash table placement for dict encoding in default https://github.com/klauspost/compress/pull/606\r\n\t* zstd: Apply default config to ZipDecompressor without options https://github.com/klauspost/compress/pull/608\r\n\t* gzhttp: Exclude more common archive formats https://github.com/klauspost/compress/pull/612\r\n\t* s2: Add ReaderIgnoreCRC https://github.com/klauspost/compress/pull/609\r\n\t* s2: Remove sanity load on index creation https://github.com/klauspost/compress/pull/607\r\n\t* snappy: Use dedicated function for scoring https://github.com/klauspost/compress/pull/614\r\n\t* s2c+s2d: Use official snappy framed extension https://github.com/klauspost/compress/pull/610\r\n\r\n* May 25, 2022 (v1.15.5)\r\n\t* s2: Add concurrent stream decompression https://github.com/klauspost/compress/pull/602\r\n\t* s2: Fix final emit oob read crash on amd64 https://github.com/klauspost/compress/pull/601\r\n\t* huff0: asm implementation of Decompress1X by @WojciechMula https://github.com/klauspost/compress/pull/596\r\n\t* zstd: Use 1 less goroutine for stream decoding https://github.com/klauspost/compress/pull/588\r\n\t* zstd: Copy literal in 16 byte blocks when possible https://github.com/klauspost/compress/pull/592\r\n\t* zstd: Speed up when WithDecoderLowmem(false) https://github.com/klauspost/compress/pull/599\r\n\t* zstd: faster next state update in BMI2 version of decode by @WojciechMula in https://github.com/klauspost/compress/pull/593\r\n\t* huff0: Do not check max size when reading table. https://github.com/klauspost/compress/pull/586\r\n\t* flate: Inplace hashing for level 7-9 by @klauspost in https://github.com/klauspost/compress/pull/590\r\n\r\n\r\n* May 11, 2022 (v1.15.4)\r\n\t* huff0: decompress directly into output by @WojciechMula in [#577](https://github.com/klauspost/compress/pull/577)\r\n\t* inflate: Keep dict on stack [#581](https://github.com/klauspost/compress/pull/581)\r\n\t* zstd: Faster decoding memcopy in asm [#583](https://github.com/klauspost/compress/pull/583)\r\n\t* zstd: Fix ignored crc [#580](https://github.com/klauspost/compress/pull/580)\r\n\r\n* May 5, 2022 (v1.15.3)\r\n\t* zstd: Allow to ignore checksum checking by @WojciechMula [#572](https://github.com/klauspost/compress/pull/572)\r\n\t* s2: Fix incorrect seek for io.SeekEnd in [#575](https://github.com/klauspost/compress/pull/575)\r\n\r\n* Apr 26, 2022 (v1.15.2)\r\n\t* zstd: Add x86-64 assembly for decompression on streams and blocks. Contributed by [@WojciechMula](https://github.com/WojciechMula). Typically 2x faster.  [#528](https://github.com/klauspost/compress/pull/528) [#531](https://github.com/klauspost/compress/pull/531) [#545](https://github.com/klauspost/compress/pull/545) [#537](https://github.com/klauspost/compress/pull/537)\r\n\t* zstd: Add options to ZipDecompressor and fixes [#539](https://github.com/klauspost/compress/pull/539)\r\n\t* s2: Use sorted search for index [#555](https://github.com/klauspost/compress/pull/555)\r\n\t* Minimum version is Go 1.16, added CI test on 1.18.\r\n\r\n* Mar 11, 2022 (v1.15.1)\r\n\t* huff0: Add x86 assembly of Decode4X by @WojciechMula in [#512](https://github.com/klauspost/compress/pull/512)\r\n\t* zstd: Reuse zip decoders in [#514](https://github.com/klauspost/compress/pull/514)\r\n\t* zstd: Detect extra block data and report as corrupted in [#520](https://github.com/klauspost/compress/pull/520)\r\n\t* zstd: Handle zero sized frame content size stricter in [#521](https://github.com/klauspost/compress/pull/521)\r\n\t* zstd: Add stricter block size checks in [#523](https://github.com/klauspost/compress/pull/523)\r\n\r\n* Mar 3, 2022 (v1.15.0)\r\n\t* zstd: Refactor decoder by @klauspost in [#498](https://github.com/klauspost/compress/pull/498)\r\n\t* zstd: Add stream encoding without goroutines by @klauspost in [#505](https://github.com/klauspost/compress/pull/505)\r\n\t* huff0: Prevent single blocks exceeding 16 bits by @klauspost in[#507](https://github.com/klauspost/compress/pull/507)\r\n\t* flate: Inline literal emission by @klauspost in [#509](https://github.com/klauspost/compress/pull/509)\r\n\t* gzhttp: Add zstd to transport by @klauspost in [#400](https://github.com/klauspost/compress/pull/400)\r\n\t* gzhttp: Make content-type optional by @klauspost in [#510](https://github.com/klauspost/compress/pull/510)\r\n\r\nBoth compression and decompression now supports \"synchronous\" stream operations. This means that whenever \"concurrency\" is set to 1, they will operate without spawning goroutines.\r\n\r\nStream decompression is now faster on asynchronous, since the goroutine allocation much more effectively splits the workload. On typical streams this will typically use 2 cores fully for decompression. When a stream has finished decoding no goroutines will be left over, so decoders can now safely be pooled and still be garbage collected.\r\n\r\nWhile the release has been extensively tested, it is recommended to testing when upgrading.\r\n\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes to v1.14.x</summary>\r\n\t\r\n* Feb 22, 2022 (v1.14.4)\r\n\t* flate: Fix rare huffman only (-2) corruption. [#503](https://github.com/klauspost/compress/pull/503)\r\n\t* zip: Update deprecated CreateHeaderRaw to correctly call CreateRaw by @saracen in [#502](https://github.com/klauspost/compress/pull/502)\r\n\t* zip: don't read data descriptor early by @saracen in [#501](https://github.com/klauspost/compress/pull/501)  #501\r\n\t* huff0: Use static decompression buffer up to 30% faster by @klauspost in [#499](https://github.com/klauspost/compress/pull/499) [#500](https://github.com/klauspost/compress/pull/500)\r\n\r\n* Feb 17, 2022 (v1.14.3)\r\n\t* flate: Improve fastest levels compression speed ~10% more throughput. [#482](https://github.com/klauspost/compress/pull/482) [#489](https://github.com/klauspost/compress/pull/489) [#490](https://github.com/klauspost/compress/pull/490) [#491](https://github.com/klauspost/compress/pull/491) [#494](https://github.com/klauspost/compress/pull/494)  [#478](https://github.com/klauspost/compress/pull/478)\r\n\t* flate: Faster decompression speed, ~5-10%. [#483](https://github.com/klauspost/compress/pull/483)\r\n\t* s2: Faster compression with Go v1.18 and amd64 microarch level 3+. [#484](https://github.com/klauspost/compress/pull/484) [#486](https://github.com/klauspost/compress/pull/486)\r\n\r\n* Jan 25, 2022 (v1.14.2)\r\n\t* zstd: improve header decoder by @dsnet  [#476](https://github.com/klauspost/compress/pull/476)\r\n\t* zstd: Add bigger default blocks  [#469](https://github.com/klauspost/compress/pull/469)\r\n\t* zstd: Remove unused decompression buffer [#470](https://github.com/klauspost/compress/pull/470)\r\n\t* zstd: Fix logically dead code by @ningmingxiao [#472](https://github.com/klauspost/compress/pull/472)\r\n\t* flate: Improve level 7-9 [#471](https://github.com/klauspost/compress/pull/471) [#473](https://github.com/klauspost/compress/pull/473)\r\n\t* zstd: Add noasm tag for xxhash [#475](https://github.com/klauspost/compress/pull/475)\r\n\r\n* Jan 11, 2022 (v1.14.1)\r\n\t* s2: Add stream index in [#462](https://github.com/klauspost/compress/pull/462)\r\n\t* flate: Speed and efficiency improvements in [#439](https://github.com/klauspost/compress/pull/439) [#461](https://github.com/klauspost/compress/pull/461) [#455](https://github.com/klauspost/compress/pull/455) [#452](https://github.com/klauspost/compress/pull/452) [#458](https://github.com/klauspost/compress/pull/458)\r\n\t* zstd: Performance improvement in [#420]( https://github.com/klauspost/compress/pull/420) [#456](https://github.com/klauspost/compress/pull/456) [#437](https://github.com/klauspost/compress/pull/437) [#467](https://github.com/klauspost/compress/pull/467) [#468](https://github.com/klauspost/compress/pull/468)\r\n\t* zstd: add arm64 xxhash assembly in [#464](https://github.com/klauspost/compress/pull/464)\r\n\t* Add garbled for binaries for s2 in [#445](https://github.com/klauspost/compress/pull/445)\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes to v1.13.x</summary>\r\n\t\r\n* Aug 30, 2021 (v1.13.5)\r\n\t* gz/zlib/flate: Alias stdlib errors [#425](https://github.com/klauspost/compress/pull/425)\r\n\t* s2: Add block support to commandline tools [#413](https://github.com/klauspost/compress/pull/413)\r\n\t* zstd: pooledZipWriter should return Writers to the same pool [#426](https://github.com/klauspost/compress/pull/426)\r\n\t* Removed golang/snappy as external dependency for tests [#421](https://github.com/klauspost/compress/pull/421)\r\n\r\n* Aug 12, 2021 (v1.13.4)\r\n\t* Add [snappy replacement package](https://github.com/klauspost/compress/tree/master/snappy).\r\n\t* zstd: Fix incorrect encoding in \"best\" mode [#415](https://github.com/klauspost/compress/pull/415)\r\n\r\n* Aug 3, 2021 (v1.13.3) \r\n\t* zstd: Improve Best compression [#404](https://github.com/klauspost/compress/pull/404)\r\n\t* zstd: Fix WriteTo error forwarding [#411](https://github.com/klauspost/compress/pull/411)\r\n\t* gzhttp: Return http.HandlerFunc instead of http.Handler. Unlikely breaking change. [#406](https://github.com/klauspost/compress/pull/406)\r\n\t* s2sx: Fix max size error [#399](https://github.com/klauspost/compress/pull/399)\r\n\t* zstd: Add optional stream content size on reset [#401](https://github.com/klauspost/compress/pull/401)\r\n\t* zstd: use SpeedBestCompression for level >= 10 [#410](https://github.com/klauspost/compress/pull/410)\r\n\r\n* Jun 14, 2021 (v1.13.1)\r\n\t* s2: Add full Snappy output support  [#396](https://github.com/klauspost/compress/pull/396)\r\n\t* zstd: Add configurable [Decoder window](https://pkg.go.dev/github.com/klauspost/compress/zstd#WithDecoderMaxWindow) size [#394](https://github.com/klauspost/compress/pull/394)\r\n\t* gzhttp: Add header to skip compression  [#389](https://github.com/klauspost/compress/pull/389)\r\n\t* s2: Improve speed with bigger output margin  [#395](https://github.com/klauspost/compress/pull/395)\r\n\r\n* Jun 3, 2021 (v1.13.0)\r\n\t* Added [gzhttp](https://github.com/klauspost/compress/tree/master/gzhttp#gzip-handler) which allows wrapping HTTP servers and clients with GZIP compressors.\r\n\t* zstd: Detect short invalid signatures [#382](https://github.com/klauspost/compress/pull/382)\r\n\t* zstd: Spawn decoder goroutine only if needed. [#380](https://github.com/klauspost/compress/pull/380)\r\n</details>\r\n\r\n\r\n<details>\r\n\t<summary>See changes to v1.12.x</summary>\r\n\t\r\n* May 25, 2021 (v1.12.3)\r\n\t* deflate: Better/faster Huffman encoding [#374](https://github.com/klauspost/compress/pull/374)\r\n\t* deflate: Allocate less for history. [#375](https://github.com/klauspost/compress/pull/375)\r\n\t* zstd: Forward read errors [#373](https://github.com/klauspost/compress/pull/373) \r\n\r\n* Apr 27, 2021 (v1.12.2)\r\n\t* zstd: Improve better/best compression [#360](https://github.com/klauspost/compress/pull/360) [#364](https://github.com/klauspost/compress/pull/364) [#365](https://github.com/klauspost/compress/pull/365)\r\n\t* zstd: Add helpers to compress/decompress zstd inside zip files [#363](https://github.com/klauspost/compress/pull/363)\r\n\t* deflate: Improve level 5+6 compression [#367](https://github.com/klauspost/compress/pull/367)\r\n\t* s2: Improve better/best compression [#358](https://github.com/klauspost/compress/pull/358) [#359](https://github.com/klauspost/compress/pull/358)\r\n\t* s2: Load after checking src limit on amd64. [#362](https://github.com/klauspost/compress/pull/362)\r\n\t* s2sx: Limit max executable size [#368](https://github.com/klauspost/compress/pull/368) \r\n\r\n* Apr 14, 2021 (v1.12.1)\r\n\t* snappy package removed. Upstream added as dependency.\r\n\t* s2: Better compression in \"best\" mode [#353](https://github.com/klauspost/compress/pull/353)\r\n\t* s2sx: Add stdin input and detect pre-compressed from signature [#352](https://github.com/klauspost/compress/pull/352)\r\n\t* s2c/s2d: Add http as possible input [#348](https://github.com/klauspost/compress/pull/348)\r\n\t* s2c/s2d/s2sx: Always truncate when writing files [#352](https://github.com/klauspost/compress/pull/352)\r\n\t* zstd: Reduce memory usage further when using [WithLowerEncoderMem](https://pkg.go.dev/github.com/klauspost/compress/zstd#WithLowerEncoderMem) [#346](https://github.com/klauspost/compress/pull/346)\r\n\t* s2: Fix potential problem with amd64 assembly and profilers [#349](https://github.com/klauspost/compress/pull/349)\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes to v1.11.x</summary>\r\n\t\r\n* Mar 26, 2021 (v1.11.13)\r\n\t* zstd: Big speedup on small dictionary encodes [#344](https://github.com/klauspost/compress/pull/344) [#345](https://github.com/klauspost/compress/pull/345)\r\n\t* zstd: Add [WithLowerEncoderMem](https://pkg.go.dev/github.com/klauspost/compress/zstd#WithLowerEncoderMem) encoder option [#336](https://github.com/klauspost/compress/pull/336)\r\n\t* deflate: Improve entropy compression [#338](https://github.com/klauspost/compress/pull/338)\r\n\t* s2: Clean up and minor performance improvement in best [#341](https://github.com/klauspost/compress/pull/341)\r\n\r\n* Mar 5, 2021 (v1.11.12)\r\n\t* s2: Add `s2sx` binary that creates [self extracting archives](https://github.com/klauspost/compress/tree/master/s2#s2sx-self-extracting-archives).\r\n\t* s2: Speed up decompression on non-assembly platforms [#328](https://github.com/klauspost/compress/pull/328)\r\n\r\n* Mar 1, 2021 (v1.11.9)\r\n\t* s2: Add ARM64 decompression assembly. Around 2x output speed. [#324](https://github.com/klauspost/compress/pull/324)\r\n\t* s2: Improve \"better\" speed and efficiency. [#325](https://github.com/klauspost/compress/pull/325)\r\n\t* s2: Fix binaries.\r\n\r\n* Feb 25, 2021 (v1.11.8)\r\n\t* s2: Fixed occasional out-of-bounds write on amd64. Upgrade recommended.\r\n\t* s2: Add AMD64 assembly for better mode. 25-50% faster. [#315](https://github.com/klauspost/compress/pull/315)\r\n\t* s2: Less upfront decoder allocation. [#322](https://github.com/klauspost/compress/pull/322)\r\n\t* zstd: Faster \"compression\" of incompressible data. [#314](https://github.com/klauspost/compress/pull/314)\r\n\t* zip: Fix zip64 headers. [#313](https://github.com/klauspost/compress/pull/313)\r\n  \r\n* Jan 14, 2021 (v1.11.7)\r\n\t* Use Bytes() interface to get bytes across packages. [#309](https://github.com/klauspost/compress/pull/309)\r\n\t* s2: Add 'best' compression option.  [#310](https://github.com/klauspost/compress/pull/310)\r\n\t* s2: Add ReaderMaxBlockSize, changes `s2.NewReader` signature to include varargs. [#311](https://github.com/klauspost/compress/pull/311)\r\n\t* s2: Fix crash on small better buffers. [#308](https://github.com/klauspost/compress/pull/308)\r\n\t* s2: Clean up decoder. [#312](https://github.com/klauspost/compress/pull/312)\r\n\r\n* Jan 7, 2021 (v1.11.6)\r\n\t* zstd: Make decoder allocations smaller [#306](https://github.com/klauspost/compress/pull/306)\r\n\t* zstd: Free Decoder resources when Reset is called with a nil io.Reader  [#305](https://github.com/klauspost/compress/pull/305)\r\n\r\n* Dec 20, 2020 (v1.11.4)\r\n\t* zstd: Add Best compression mode [#304](https://github.com/klauspost/compress/pull/304)\r\n\t* Add header decoder [#299](https://github.com/klauspost/compress/pull/299)\r\n\t* s2: Add uncompressed stream option [#297](https://github.com/klauspost/compress/pull/297)\r\n\t* Simplify/speed up small blocks with known max size. [#300](https://github.com/klauspost/compress/pull/300)\r\n\t* zstd: Always reset literal dict encoder [#303](https://github.com/klauspost/compress/pull/303)\r\n\r\n* Nov 15, 2020 (v1.11.3)\r\n\t* inflate: 10-15% faster decompression  [#293](https://github.com/klauspost/compress/pull/293)\r\n\t* zstd: Tweak DecodeAll default allocation [#295](https://github.com/klauspost/compress/pull/295)\r\n\r\n* Oct 11, 2020 (v1.11.2)\r\n\t* s2: Fix out of bounds read in \"better\" block compression [#291](https://github.com/klauspost/compress/pull/291)\r\n\r\n* Oct 1, 2020 (v1.11.1)\r\n\t* zstd: Set allLitEntropy true in default configuration [#286](https://github.com/klauspost/compress/pull/286)\r\n\r\n* Sept 8, 2020 (v1.11.0)\r\n\t* zstd: Add experimental compression [dictionaries](https://github.com/klauspost/compress/tree/master/zstd#dictionaries) [#281](https://github.com/klauspost/compress/pull/281)\r\n\t* zstd: Fix mixed Write and ReadFrom calls [#282](https://github.com/klauspost/compress/pull/282)\r\n\t* inflate/gz: Limit variable shifts, ~5% faster decompression [#274](https://github.com/klauspost/compress/pull/274)\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes to v1.10.x</summary>\r\n \r\n* July 8, 2020 (v1.10.11) \r\n\t* zstd: Fix extra block when compressing with ReadFrom. [#278](https://github.com/klauspost/compress/pull/278)\r\n\t* huff0: Also populate compression table when reading decoding table. [#275](https://github.com/klauspost/compress/pull/275)\r\n\t\r\n* June 23, 2020 (v1.10.10) \r\n\t* zstd: Skip entropy compression in fastest mode when no matches. [#270](https://github.com/klauspost/compress/pull/270)\r\n\t\r\n* June 16, 2020 (v1.10.9): \r\n\t* zstd: API change for specifying dictionaries. See [#268](https://github.com/klauspost/compress/pull/268)\r\n\t* zip: update CreateHeaderRaw to handle zip64 fields. [#266](https://github.com/klauspost/compress/pull/266)\r\n\t* Fuzzit tests removed. The service has been purchased and is no longer available.\r\n\t\r\n* June 5, 2020 (v1.10.8): \r\n\t* 1.15x faster zstd block decompression. [#265](https://github.com/klauspost/compress/pull/265)\r\n\t\r\n* June 1, 2020 (v1.10.7): \r\n\t* Added zstd decompression [dictionary support](https://github.com/klauspost/compress/tree/master/zstd#dictionaries)\r\n\t* Increase zstd decompression speed up to 1.19x.  [#259](https://github.com/klauspost/compress/pull/259)\r\n\t* Remove internal reset call in zstd compression and reduce allocations. [#263](https://github.com/klauspost/compress/pull/263)\r\n\t\r\n* May 21, 2020: (v1.10.6) \r\n\t* zstd: Reduce allocations while decoding. [#258](https://github.com/klauspost/compress/pull/258), [#252](https://github.com/klauspost/compress/pull/252)\r\n\t* zstd: Stricter decompression checks.\r\n\t\r\n* April 12, 2020: (v1.10.5)\r\n\t* s2-commands: Flush output when receiving SIGINT. [#239](https://github.com/klauspost/compress/pull/239)\r\n\t\r\n* Apr 8, 2020: (v1.10.4) \r\n\t* zstd: Minor/special case optimizations. [#251](https://github.com/klauspost/compress/pull/251),  [#250](https://github.com/klauspost/compress/pull/250),  [#249](https://github.com/klauspost/compress/pull/249),  [#247](https://github.com/klauspost/compress/pull/247)\r\n* Mar 11, 2020: (v1.10.3) \r\n\t* s2: Use S2 encoder in pure Go mode for Snappy output as well. [#245](https://github.com/klauspost/compress/pull/245)\r\n\t* s2: Fix pure Go block encoder. [#244](https://github.com/klauspost/compress/pull/244)\r\n\t* zstd: Added \"better compression\" mode. [#240](https://github.com/klauspost/compress/pull/240)\r\n\t* zstd: Improve speed of fastest compression mode by 5-10% [#241](https://github.com/klauspost/compress/pull/241)\r\n\t* zstd: Skip creating encoders when not needed. [#238](https://github.com/klauspost/compress/pull/238)\r\n\t\r\n* Feb 27, 2020: (v1.10.2) \r\n\t* Close to 50% speedup in inflate (gzip/zip decompression). [#236](https://github.com/klauspost/compress/pull/236) [#234](https://github.com/klauspost/compress/pull/234) [#232](https://github.com/klauspost/compress/pull/232)\r\n\t* Reduce deflate level 1-6 memory usage up to 59%. [#227](https://github.com/klauspost/compress/pull/227)\r\n\t\r\n* Feb 18, 2020: (v1.10.1)\r\n\t* Fix zstd crash when resetting multiple times without sending data. [#226](https://github.com/klauspost/compress/pull/226)\r\n\t* deflate: Fix dictionary use on level 1-6. [#224](https://github.com/klauspost/compress/pull/224)\r\n\t* Remove deflate writer reference when closing. [#224](https://github.com/klauspost/compress/pull/224)\r\n\t\r\n* Feb 4, 2020: (v1.10.0) \r\n\t* Add optional dictionary to [stateless deflate](https://pkg.go.dev/github.com/klauspost/compress/flate?tab=doc#StatelessDeflate). Breaking change, send `nil` for previous behaviour. [#216](https://github.com/klauspost/compress/pull/216)\r\n\t* Fix buffer overflow on repeated small block deflate.  [#218](https://github.com/klauspost/compress/pull/218)\r\n\t* Allow copying content from an existing ZIP file without decompressing+compressing. [#214](https://github.com/klauspost/compress/pull/214)\r\n\t* Added [S2](https://github.com/klauspost/compress/tree/master/s2#s2-compression) AMD64 assembler and various optimizations. Stream speed >10GB/s.  [#186](https://github.com/klauspost/compress/pull/186)\r\n\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes prior to v1.10.0</summary>\r\n\r\n* Jan 20,2020 (v1.9.8) Optimize gzip/deflate with better size estimates and faster table generation. [#207](https://github.com/klauspost/compress/pull/207) by [luyu6056](https://github.com/luyu6056),  [#206](https://github.com/klauspost/compress/pull/206).\r\n* Jan 11, 2020: S2 Encode/Decode will use provided buffer if capacity is big enough. [#204](https://github.com/klauspost/compress/pull/204) \r\n* Jan 5, 2020: (v1.9.7) Fix another zstd regression in v1.9.5 - v1.9.6 removed.\r\n* Jan 4, 2020: (v1.9.6) Regression in v1.9.5 fixed causing corrupt zstd encodes in rare cases.\r\n* Jan 4, 2020: Faster IO in [s2c + s2d commandline tools](https://github.com/klauspost/compress/tree/master/s2#commandline-tools) compression/decompression. [#192](https://github.com/klauspost/compress/pull/192)\r\n* Dec 29, 2019: Removed v1.9.5 since fuzz tests showed a compatibility problem with the reference zstandard decoder.\r\n* Dec 29, 2019: (v1.9.5) zstd: 10-20% faster block compression. [#199](https://github.com/klauspost/compress/pull/199)\r\n* Dec 29, 2019: [zip](https://godoc.org/github.com/klauspost/compress/zip) package updated with latest Go features\r\n* Dec 29, 2019: zstd: Single segment flag condintions tweaked. [#197](https://github.com/klauspost/compress/pull/197)\r\n* Dec 18, 2019: s2: Faster compression when ReadFrom is used. [#198](https://github.com/klauspost/compress/pull/198)\r\n* Dec 10, 2019: s2: Fix repeat length output when just above at 16MB limit.\r\n* Dec 10, 2019: zstd: Add function to get decoder as io.ReadCloser. [#191](https://github.com/klauspost/compress/pull/191)\r\n* Dec 3, 2019: (v1.9.4) S2: limit max repeat length. [#188](https://github.com/klauspost/compress/pull/188)\r\n* Dec 3, 2019: Add [WithNoEntropyCompression](https://godoc.org/github.com/klauspost/compress/zstd#WithNoEntropyCompression) to zstd [#187](https://github.com/klauspost/compress/pull/187)\r\n* Dec 3, 2019: Reduce memory use for tests. Check for leaked goroutines.\r\n* Nov 28, 2019 (v1.9.3) Less allocations in stateless deflate.\r\n* Nov 28, 2019: 5-20% Faster huff0 decode. Impacts zstd as well. [#184](https://github.com/klauspost/compress/pull/184)\r\n* Nov 12, 2019 (v1.9.2) Added [Stateless Compression](#stateless-compression) for gzip/deflate.\r\n* Nov 12, 2019: Fixed zstd decompression of large single blocks. [#180](https://github.com/klauspost/compress/pull/180)\r\n* Nov 11, 2019: Set default  [s2c](https://github.com/klauspost/compress/tree/master/s2#commandline-tools) block size to 4MB.\r\n* Nov 11, 2019: Reduce inflate memory use by 1KB.\r\n* Nov 10, 2019: Less allocations in deflate bit writer.\r\n* Nov 10, 2019: Fix inconsistent error returned by zstd decoder.\r\n* Oct 28, 2019 (v1.9.1) ztsd: Fix crash when compressing blocks. [#174](https://github.com/klauspost/compress/pull/174)\r\n* Oct 24, 2019 (v1.9.0) zstd: Fix rare data corruption [#173](https://github.com/klauspost/compress/pull/173)\r\n* Oct 24, 2019 zstd: Fix huff0 out of buffer write [#171](https://github.com/klauspost/compress/pull/171) and always return errors [#172](https://github.com/klauspost/compress/pull/172) \r\n* Oct 10, 2019: Big deflate rewrite, 30-40% faster with better compression [#105](https://github.com/klauspost/compress/pull/105)\r\n\r\n</details>\r\n\r\n<details>\r\n\t<summary>See changes prior to v1.9.0</summary>\r\n\r\n* Oct 10, 2019: (v1.8.6) zstd: Allow partial reads to get flushed data. [#169](https://github.com/klauspost/compress/pull/169)\r\n* Oct 3, 2019: Fix inconsistent results on broken zstd streams.\r\n* Sep 25, 2019: Added `-rm` (remove source files) and `-q` (no output except errors) to `s2c` and `s2d` [commands](https://github.com/klauspost/compress/tree/master/s2#commandline-tools)\r\n* Sep 16, 2019: (v1.8.4) Add `s2c` and `s2d` [commandline tools](https://github.com/klauspost/compress/tree/master/s2#commandline-tools).\r\n* Sep 10, 2019: (v1.8.3) Fix s2 decoder [Skip](https://godoc.org/github.com/klauspost/compress/s2#Reader.Skip).\r\n* Sep 7, 2019: zstd: Added [WithWindowSize](https://godoc.org/github.com/klauspost/compress/zstd#WithWindowSize), contributed by [ianwilkes](https://github.com/ianwilkes).\r\n* Sep 5, 2019: (v1.8.2) Add [WithZeroFrames](https://godoc.org/github.com/klauspost/compress/zstd#WithZeroFrames) which adds full zero payload block encoding option.\r\n* Sep 5, 2019: Lazy initialization of zstandard predefined en/decoder tables.\r\n* Aug 26, 2019: (v1.8.1) S2: 1-2% compression increase in \"better\" compression mode.\r\n* Aug 26, 2019: zstd: Check maximum size of Huffman 1X compressed literals while decoding.\r\n* Aug 24, 2019: (v1.8.0) Added [S2 compression](https://github.com/klauspost/compress/tree/master/s2#s2-compression), a high performance replacement for Snappy. \r\n* Aug 21, 2019: (v1.7.6) Fixed minor issues found by fuzzer. One could lead to zstd not decompressing.\r\n* Aug 18, 2019: Add [fuzzit](https://fuzzit.dev/) continuous fuzzing.\r\n* Aug 14, 2019: zstd: Skip incompressible data 2x faster.  [#147](https://github.com/klauspost/compress/pull/147)\r\n* Aug 4, 2019 (v1.7.5): Better literal compression. [#146](https://github.com/klauspost/compress/pull/146)\r\n* Aug 4, 2019: Faster zstd compression. [#143](https://github.com/klauspost/compress/pull/143) [#144](https://github.com/klauspost/compress/pull/144)\r\n* Aug 4, 2019: Faster zstd decompression. [#145](https://github.com/klauspost/compress/pull/145) [#143](https://github.com/klauspost/compress/pull/143) [#142](https://github.com/klauspost/compress/pull/142)\r\n* July 15, 2019 (v1.7.4): Fix double EOF block in rare cases on zstd encoder.\r\n* July 15, 2019 (v1.7.3): Minor speedup/compression increase in default zstd encoder.\r\n* July 14, 2019: zstd decoder: Fix decompression error on multiple uses with mixed content.\r\n* July 7, 2019 (v1.7.2): Snappy update, zstd decoder potential race fix.\r\n* June 17, 2019: zstd decompression bugfix.\r\n* June 17, 2019: fix 32 bit builds.\r\n* June 17, 2019: Easier use in modules (less dependencies).\r\n* June 9, 2019: New stronger \"default\" [zstd](https://github.com/klauspost/compress/tree/master/zstd#zstd) compression mode. Matches zstd default compression ratio.\r\n* June 5, 2019: 20-40% throughput in [zstandard](https://github.com/klauspost/compress/tree/master/zstd#zstd) compression and better compression.\r\n* June 5, 2019: deflate/gzip compression: Reduce memory usage of lower compression levels.\r\n* June 2, 2019: Added [zstandard](https://github.com/klauspost/compress/tree/master/zstd#zstd) compression!\r\n* May 25, 2019: deflate/gzip: 10% faster bit writer, mostly visible in lower levels.\r\n* Apr 22, 2019: [zstd](https://github.com/klauspost/compress/tree/master/zstd#zstd) decompression added.\r\n* Aug 1, 2018: Added [huff0 README](https://github.com/klauspost/compress/tree/master/huff0#huff0-entropy-compression).\r\n* Jul 8, 2018: Added [Performance Update 2018](#performance-update-2018) below.\r\n* Jun 23, 2018: Merged [Go 1.11 inflate optimizations](https://go-review.googlesource.com/c/go/+/102235). Go 1.9 is now required. Backwards compatible version tagged with [v1.3.0](https://github.com/klauspost/compress/releases/tag/v1.3.0).\r\n* Apr 2, 2018: Added [huff0](https://godoc.org/github.com/klauspost/compress/huff0) en/decoder. Experimental for now, API may change.\r\n* Mar 4, 2018: Added [FSE Entropy](https://godoc.org/github.com/klauspost/compress/fse) en/decoder. Experimental for now, API may change.\r\n* Nov 3, 2017: Add compression [Estimate](https://godoc.org/github.com/klauspost/compress#Estimate) function.\r\n* May 28, 2017: Reduce allocations when resetting decoder.\r\n* Apr 02, 2017: Change back to official crc32, since changes were merged in Go 1.7.\r\n* Jan 14, 2017: Reduce stack pressure due to array copies. See [Issue #18625](https://github.com/golang/go/issues/18625).\r\n* Oct 25, 2016: Level 2-4 have been rewritten and now offers significantly better performance than before.\r\n* Oct 20, 2016: Port zlib changes from Go 1.7 to fix zlib writer issue. Please update.\r\n* Oct 16, 2016: Go 1.7 changes merged. Apples to apples this package is a few percent faster, but has a significantly better balance between speed and compression per level. \r\n* Mar 24, 2016: Always attempt Huffman encoding on level 4-7. This improves base 64 encoded data compression.\r\n* Mar 24, 2016: Small speedup for level 1-3.\r\n* Feb 19, 2016: Faster bit writer, level -2 is 15% faster, level 1 is 4% faster.\r\n* Feb 19, 2016: Handle small payloads faster in level 1-3.\r\n* Feb 19, 2016: Added faster level 2 + 3 compression modes.\r\n* Feb 19, 2016: [Rebalanced compression levels](https://blog.klauspost.com/rebalancing-deflate-compression-levels/), so there is a more even progression in terms of compression. New default level is 5.\r\n* Feb 14, 2016: Snappy: Merge upstream changes. \r\n* Feb 14, 2016: Snappy: Fix aggressive skipping.\r\n* Feb 14, 2016: Snappy: Update benchmark.\r\n* Feb 13, 2016: Deflate: Fixed assembler problem that could lead to sub-optimal compression.\r\n* Feb 12, 2016: Snappy: Added AMD64 SSE 4.2 optimizations to matching, which makes easy to compress material run faster. Typical speedup is around 25%.\r\n* Feb 9, 2016: Added Snappy package fork. This version is 5-7% faster, much more on hard to compress content.\r\n* Jan 30, 2016: Optimize level 1 to 3 by not considering static dictionary or storing uncompressed. ~4-5% speedup.\r\n* Jan 16, 2016: Optimization on deflate level 1,2,3 compression.\r\n* Jan 8 2016: Merge [CL 18317](https://go-review.googlesource.com/#/c/18317): fix reading, writing of zip64 archives.\r\n* Dec 8 2015: Make level 1 and -2 deterministic even if write size differs.\r\n* Dec 8 2015: Split encoding functions, so hashing and matching can potentially be inlined. 1-3% faster on AMD64. 5% faster on other platforms.\r\n* Dec 8 2015: Fixed rare [one byte out-of bounds read](https://github.com/klauspost/compress/issues/20). Please update!\r\n* Nov 23 2015: Optimization on token writer. ~2-4% faster. Contributed by [@dsnet](https://github.com/dsnet).\r\n* Nov 20 2015: Small optimization to bit writer on 64 bit systems.\r\n* Nov 17 2015: Fixed out-of-bound errors if the underlying Writer returned an error. See [#15](https://github.com/klauspost/compress/issues/15).\r\n* Nov 12 2015: Added [io.WriterTo](https://golang.org/pkg/io/#WriterTo) support to gzip/inflate.\r\n* Nov 11 2015: Merged [CL 16669](https://go-review.googlesource.com/#/c/16669/4): archive/zip: enable overriding (de)compressors per file\r\n* Oct 15 2015: Added skipping on uncompressible data. Random data speed up >5x.\r\n\r\n</details>\r\n\r\n# deflate usage\r\n\r\nThe packages are drop-in replacements for standard libraries. Simply replace the import path to use them:\r\n\r\n| old import         | new import                              | Documentation\r\n|--------------------|-----------------------------------------|--------------------|\r\n| `compress/gzip`    | `github.com/klauspost/compress/gzip`    | [gzip](https://pkg.go.dev/github.com/klauspost/compress/gzip?tab=doc)\r\n| `compress/zlib`    | `github.com/klauspost/compress/zlib`    | [zlib](https://pkg.go.dev/github.com/klauspost/compress/zlib?tab=doc)\r\n| `archive/zip`      | `github.com/klauspost/compress/zip`     | [zip](https://pkg.go.dev/github.com/klauspost/compress/zip?tab=doc)\r\n| `compress/flate`   | `github.com/klauspost/compress/flate`   | [flate](https://pkg.go.dev/github.com/klauspost/compress/flate?tab=doc)\r\n\r\n* Optimized [deflate](https://godoc.org/github.com/klauspost/compress/flate) packages which can be used as a dropin replacement for [gzip](https://godoc.org/github.com/klauspost/compress/gzip), [zip](https://godoc.org/github.com/klauspost/compress/zip) and [zlib](https://godoc.org/github.com/klauspost/compress/zlib).\r\n\r\nYou may also be interested in [pgzip](https://github.com/klauspost/pgzip), which is a drop in replacement for gzip, which support multithreaded compression on big files and the optimized [crc32](https://github.com/klauspost/crc32) package used by these packages.\r\n\r\nThe packages contains the same as the standard library, so you can use the godoc for that: [gzip](http://golang.org/pkg/compress/gzip/), [zip](http://golang.org/pkg/archive/zip/),  [zlib](http://golang.org/pkg/compress/zlib/), [flate](http://golang.org/pkg/compress/flate/).\r\n\r\nCurrently there is only minor speedup on decompression (mostly CRC32 calculation).\r\n\r\nMemory usage is typically 1MB for a Writer. stdlib is in the same range. \r\nIf you expect to have a lot of concurrently allocated Writers consider using \r\nthe stateless compress described below.\r\n\r\nFor compression performance, see: [this spreadsheet](https://docs.google.com/spreadsheets/d/1nuNE2nPfuINCZJRMt6wFWhKpToF95I47XjSsc-1rbPQ/edit?usp=sharing).\r\n\r\nTo disable all assembly add `-tags=noasm`. This works across all packages.\r\n\r\n# Stateless compression\r\n\r\nThis package offers stateless compression as a special option for gzip/deflate. \r\nIt will do compression but without maintaining any state between Write calls.\r\n\r\nThis means there will be no memory kept between Write calls, but compression and speed will be suboptimal.\r\n\r\nThis is only relevant in cases where you expect to run many thousands of compressors concurrently, \r\nbut with very little activity. This is *not* intended for regular web servers serving individual requests.  \r\n\r\nBecause of this, the size of actual Write calls will affect output size.\r\n\r\nIn gzip, specify level `-3` / `gzip.StatelessCompression` to enable.\r\n\r\nFor direct deflate use, NewStatelessWriter and StatelessDeflate are available. See [documentation](https://godoc.org/github.com/klauspost/compress/flate#NewStatelessWriter)\r\n\r\nA `bufio.Writer` can of course be used to control write sizes. For example, to use a 4KB buffer:\r\n\r\n```go\r\n\t// replace 'ioutil.Discard' with your output.\r\n\tgzw, err := gzip.NewWriterLevel(ioutil.Discard, gzip.StatelessCompression)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tdefer gzw.Close()\r\n\r\n\tw := bufio.NewWriterSize(gzw, 4096)\r\n\tdefer w.Flush()\r\n\t\r\n\t// Write to 'w' \r\n```\r\n\r\nThis will only use up to 4KB in memory when the writer is idle. \r\n\r\nCompression is almost always worse than the fastest compression level \r\nand each write will allocate (a little) memory. \r\n\r\n# Performance Update 2018\r\n\r\nIt has been a while since we have been looking at the speed of this package compared to the standard library, so I thought I would re-do my tests and give some overall recommendations based on the current state. All benchmarks have been performed with Go 1.10 on my Desktop Intel(R) Core(TM) i7-2600 CPU @3.40GHz. Since I last ran the tests, I have gotten more RAM, which means tests with big files are no longer limited by my SSD.\r\n\r\nThe raw results are in my [updated spreadsheet](https://docs.google.com/spreadsheets/d/1nuNE2nPfuINCZJRMt6wFWhKpToF95I47XjSsc-1rbPQ/edit?usp=sharing). Due to cgo changes and upstream updates i could not get the cgo version of gzip to compile. Instead I included the [zstd](https://github.com/datadog/zstd) cgo implementation. If I get cgo gzip to work again, I might replace the results in the sheet.\r\n\r\nThe columns to take note of are: *MB/s* - the throughput. *Reduction* - the data size reduction in percent of the original. *Rel Speed* relative speed compared to the standard library at the same level. *Smaller* - how many percent smaller is the compressed output compared to stdlib. Negative means the output was bigger. *Loss* means the loss (or gain) in compression as a percentage difference of the input.\r\n\r\nThe `gzstd` (standard library gzip) and `gzkp` (this package gzip) only uses one CPU core. [`pgzip`](https://github.com/klauspost/pgzip), [`bgzf`](https://github.com/biogo/hts/tree/master/bgzf) uses all 4 cores. [`zstd`](https://github.com/DataDog/zstd) uses one core, and is a beast (but not Go, yet).\r\n\r\n\r\n## Overall differences.\r\n\r\nThere appears to be a roughly 5-10% speed advantage over the standard library when comparing at similar compression levels.\r\n\r\nThe biggest difference you will see is the result of [re-balancing](https://blog.klauspost.com/rebalancing-deflate-compression-levels/) the compression levels. I wanted by library to give a smoother transition between the compression levels than the standard library.\r\n\r\nThis package attempts to provide a more smooth transition, where \"1\" is taking a lot of shortcuts, \"5\" is the reasonable trade-off and \"9\" is the \"give me the best compression\", and the values in between gives something reasonable in between. The standard library has big differences in levels 1-4, but levels 5-9 having no significant gains - often spending a lot more time than can be justified by the achieved compression.\r\n\r\nThere are links to all the test data in the [spreadsheet](https://docs.google.com/spreadsheets/d/1nuNE2nPfuINCZJRMt6wFWhKpToF95I47XjSsc-1rbPQ/edit?usp=sharing) in the top left field on each tab.\r\n\r\n## Web Content\r\n\r\nThis test set aims to emulate typical use in a web server. The test-set is 4GB data in 53k files, and is a mixture of (mostly) HTML, JS, CSS.\r\n\r\nSince level 1 and 9 are close to being the same code, they are quite close. But looking at the levels in-between the differences are quite big.\r\n\r\nLooking at level 6, this package is 88% faster, but will output about 6% more data. For a web server, this means you can serve 88% more data, but have to pay for 6% more bandwidth. You can draw your own conclusions on what would be the most expensive for your case.\r\n\r\n## Object files\r\n\r\nThis test is for typical data files stored on a server. In this case it is a collection of Go precompiled objects. They are very compressible.\r\n\r\nThe picture is similar to the web content, but with small differences since this is very compressible. Levels 2-3 offer good speed, but is sacrificing quite a bit of compression. \r\n\r\nThe standard library seems suboptimal on level 3 and 4 - offering both worse compression and speed than level 6 & 7 of this package respectively.\r\n\r\n## Highly Compressible File\r\n\r\nThis is a JSON file with very high redundancy. The reduction starts at 95% on level 1, so in real life terms we are dealing with something like a highly redundant stream of data, etc.\r\n\r\nIt is definitely visible that we are dealing with specialized content here, so the results are very scattered. This package does not do very well at levels 1-4, but picks up significantly at level 5 and levels 7 and 8 offering great speed for the achieved compression.\r\n\r\nSo if you know you content is extremely compressible you might want to go slightly higher than the defaults. The standard library has a huge gap between levels 3 and 4 in terms of speed (2.75x slowdown), so it offers little \"middle ground\".\r\n\r\n## Medium-High Compressible\r\n\r\nThis is a pretty common test corpus: [enwik9](http://mattmahoney.net/dc/textdata.html). It contains the first 10^9 bytes of the English Wikipedia dump on Mar. 3, 2006. This is a very good test of typical text based compression and more data heavy streams.\r\n\r\nWe see a similar picture here as in \"Web Content\". On equal levels some compression is sacrificed for more speed. Level 5 seems to be the best trade-off between speed and size, beating stdlib level 3 in both.\r\n\r\n## Medium Compressible\r\n\r\nI will combine two test sets, one [10GB file set](http://mattmahoney.net/dc/10gb.html) and a VM disk image (~8GB). Both contain different data types and represent a typical backup scenario.\r\n\r\nThe most notable thing is how quickly the standard library drops to very low compression speeds around level 5-6 without any big gains in compression. Since this type of data is fairly common, this does not seem like good behavior.\r\n\r\n\r\n## Un-compressible Content\r\n\r\nThis is mainly a test of how good the algorithms are at detecting un-compressible input. The standard library only offers this feature with very conservative settings at level 1. Obviously there is no reason for the algorithms to try to compress input that cannot be compressed.  The only downside is that it might skip some compressible data on false detections.\r\n\r\n\r\n## Huffman only compression\r\n\r\nThis compression library adds a special compression level, named `HuffmanOnly`, which allows near linear time compression. This is done by completely disabling matching of previous data, and only reduce the number of bits to represent each character. \r\n\r\nThis means that often used characters, like 'e' and ' ' (space) in text use the fewest bits to represent, and rare characters like '' takes more bits to represent. For more information see [wikipedia](https://en.wikipedia.org/wiki/Huffman_coding) or this nice [video](https://youtu.be/ZdooBTdW5bM).\r\n\r\nSince this type of compression has much less variance, the compression speed is mostly unaffected by the input data, and is usually more than *180MB/s* for a single core.\r\n\r\nThe downside is that the compression ratio is usually considerably worse than even the fastest conventional compression. The compression ratio can never be better than 8:1 (12.5%). \r\n\r\nThe linear time compression can be used as a \"better than nothing\" mode, where you cannot risk the encoder to slow down on some content. For comparison, the size of the \"Twain\" text is *233460 bytes* (+29% vs. level 1) and encode speed is 144MB/s (4.5x level 1). So in this case you trade a 30% size increase for a 4 times speedup.\r\n\r\nFor more information see my blog post on [Fast Linear Time Compression](http://blog.klauspost.com/constant-time-gzipzip-compression/).\r\n\r\nThis is implemented on Go 1.7 as \"Huffman Only\" mode, though not exposed for gzip.\r\n\r\n# Other packages\r\n\r\nHere are other packages of good quality and pure Go (no cgo wrappers or autoconverted code):\r\n\r\n* [github.com/pierrec/lz4](https://github.com/pierrec/lz4) - strong multithreaded LZ4 compression.\r\n* [github.com/cosnicolaou/pbzip2](https://github.com/cosnicolaou/pbzip2) - multithreaded bzip2 decompression.\r\n* [github.com/dsnet/compress](https://github.com/dsnet/compress) - brotli decompression, bzip2 writer.\r\n* [github.com/ronanh/intcomp](https://github.com/ronanh/intcomp) - Integer compression.\r\n* [github.com/spenczar/fpc](https://github.com/spenczar/fpc) - Float compression.\r\n* [github.com/minio/zipindex](https://github.com/minio/zipindex) - External ZIP directory index.\r\n* [github.com/ybirader/pzip](https://github.com/ybirader/pzip) - Fast concurrent zip archiver and extractor.\r\n\r\n# license\r\n\r\nThis code is licensed under the same conditions as the original Go code. See LICENSE file.\r\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 1.8154296875,
          "content": "# Security Policy\n\n## Supported Versions\n\nSecurity updates are applied only to the latest release.\n\n## Vulnerability Definition\n\nA security vulnerability is a bug that with certain input triggers a crash or an infinite loop. Most calls will have varying execution time and only in rare cases will slow operation be considered a security vulnerability.\n\nCorrupted output generally is not considered a security vulnerability, unless independent operations are able to affect each other. Note that not all functionality is re-entrant and safe to use concurrently.\n\nOut-of-memory crashes only applies if the en/decoder uses an abnormal amount of memory, with appropriate options applied, to limit maximum window size, concurrency, etc. However, if you are in doubt you are welcome to file a security issue.\n\nIt is assumed that all callers are trusted, meaning internal data exposed through reflection or inspection of returned data structures is not considered a vulnerability.\n\nVulnerabilities resulting from compiler/assembler errors should be reported upstream. Depending on the severity this package may or may not implement a workaround.\n\n## Reporting a Vulnerability\n\nIf you have discovered a security vulnerability in this project, please report it privately. **Do not disclose it as a public issue.** This gives us time to work with you to fix the issue before public exposure, reducing the chance that the exploit will be used before a patch is released.\n\nPlease disclose it at [security advisory](https://github.com/klauspost/compress/security/advisories/new). If possible please provide a minimal reproducer. If the issue only applies to a single platform, it would be helpful to provide access to that.\n\nThis project is maintained by a team of volunteers on a reasonable-effort basis. As such, vulnerabilities will be disclosed in a best effort base.\n"
        },
        {
          "name": "compressible.go",
          "type": "blob",
          "size": 1.8232421875,
          "content": "package compress\n\nimport \"math\"\n\n// Estimate returns a normalized compressibility estimate of block b.\n// Values close to zero are likely uncompressible.\n// Values above 0.1 are likely to be compressible.\n// Values above 0.5 are very compressible.\n// Very small lengths will return 0.\nfunc Estimate(b []byte) float64 {\n\tif len(b) < 16 {\n\t\treturn 0\n\t}\n\n\t// Correctly predicted order 1\n\thits := 0\n\tlastMatch := false\n\tvar o1 [256]byte\n\tvar hist [256]int\n\tc1 := byte(0)\n\tfor _, c := range b {\n\t\tif c == o1[c1] {\n\t\t\t// We only count a hit if there was two correct predictions in a row.\n\t\t\tif lastMatch {\n\t\t\t\thits++\n\t\t\t}\n\t\t\tlastMatch = true\n\t\t} else {\n\t\t\tlastMatch = false\n\t\t}\n\t\to1[c1] = c\n\t\tc1 = c\n\t\thist[c]++\n\t}\n\n\t// Use x^0.6 to give better spread\n\tprediction := math.Pow(float64(hits)/float64(len(b)), 0.6)\n\n\t// Calculate histogram distribution\n\tvariance := float64(0)\n\tavg := float64(len(b)) / 256\n\n\tfor _, v := range hist {\n\t\t := float64(v) - avg\n\t\tvariance +=  * \n\t}\n\n\tstddev := math.Sqrt(float64(variance)) / float64(len(b))\n\texp := math.Sqrt(1 / float64(len(b)))\n\n\t// Subtract expected stddev\n\tstddev -= exp\n\tif stddev < 0 {\n\t\tstddev = 0\n\t}\n\tstddev *= 1 + exp\n\n\t// Use x^0.4 to give better spread\n\tentropy := math.Pow(stddev, 0.4)\n\n\t// 50/50 weight between prediction and histogram distribution\n\treturn math.Pow((prediction+entropy)/2, 0.9)\n}\n\n// ShannonEntropyBits returns the number of bits minimum required to represent\n// an entropy encoding of the input bytes.\n// https://en.wiktionary.org/wiki/Shannon_entropy\nfunc ShannonEntropyBits(b []byte) int {\n\tif len(b) == 0 {\n\t\treturn 0\n\t}\n\tvar hist [256]int\n\tfor _, c := range b {\n\t\thist[c]++\n\t}\n\tshannon := float64(0)\n\tinvTotal := 1.0 / float64(len(b))\n\tfor _, v := range hist[:] {\n\t\tif v > 0 {\n\t\t\tn := float64(v)\n\t\t\tshannon += math.Ceil(-math.Log2(n*invTotal) * n)\n\t\t}\n\t}\n\treturn int(math.Ceil(shannon))\n}\n"
        },
        {
          "name": "compressible_test.go",
          "type": "blob",
          "size": 8.78125,
          "content": "package compress\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/base32\"\n\t\"testing\"\n)\n\nfunc BenchmarkEstimate(b *testing.B) {\n\tb.ReportAllocs()\n\t// (predictable, low entropy distribution)\n\tb.Run(\"zeroes-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (predictable, high entropy distribution)\n\tb.Run(\"predictable-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\tfor i := range testData {\n\t\t\ttestData[i] = byte(float64(i) / float64(len(testData)) * 256)\n\t\t}\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-500b\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 500)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-50k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 50000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-500k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 500000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\n\t// (not predictable, medium entropy distribution)\n\tb.Run(\"base-32-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\trand.Read(testData)\n\t\ts := base32.StdEncoding.EncodeToString(testData)\n\t\ttestData = []byte(s)\n\t\ttestData = testData[:5000]\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n\t// (medium predictable, medium entropy distribution)\n\tb.Run(\"text\", func(b *testing.B) {\n\t\tvar testData = []byte(`If compression is done per-chunk, care should be taken that it doesn't leave restic backups open to watermarking/fingerprinting attacks.\nThis is essentially the same problem we discussed related to fingerprinting the CDC deduplication process:\nWith \"naive\" CDC, a \"known plaintext\" file can be verified to exist within the backup if the size of individual blocks can be observed by an attacker, by using CDC on the file in parallel and comparing the resulting amount of chunks and individual chunk lengths.\nAs discussed earlier, this can be somewhat mitigated by salting the CDC algorithm with a secret value, as done in attic.\nWith salted CDC, I assume compression would happen on each individual chunk, after splitting the problematic file into chunks. Restic chunks are in the range of 512 KB to 8 MB (but not evenly distributed - right?).\nAttacker knows that the CDC algorithm uses a secret salt, so the attacker generates a range of chunks consisting of the first 512 KB to 8 MB of the file, one for each valid chunk length. The attacker is also able to determine the lengths of compressed chunks.\nThe attacker then compresses that chunk using the compression algorithm.\nThe attacker compares the lengths of the resulting chunks to the first chunk in the restic backup sets.\nIF a matching block length is found, the attacker repeats the exercise with the next chunk, and the next chunk, and the next chunk, ... and the next chunk.\nIt is my belief that with sufficiently large files, and considering the fact that the CDC algorithm is \"biased\" (in lack of better of words) towards generating blocks of about 1 MB, this would be sufficient to ascertain whether or not a certain large file exists in the backup.\nAS always, a paranoid and highly unscientific stream of consciousness.\nThoughts?`)\n\t\ttestData = append(testData, testData...)\n\t\ttestData = append(testData, testData...)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tEstimate(testData)\n\t\t}\n\t\tb.Log(Estimate(testData))\n\t})\n}\n\nfunc BenchmarkSnannonEntropyBits(b *testing.B) {\n\tb.ReportAllocs()\n\t// (predictable, low entropy distribution)\n\tb.Run(\"zeroes-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (predictable, high entropy distribution)\n\tb.Run(\"predictable-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\tfor i := range testData {\n\t\t\ttestData[i] = byte(float64(i) / float64(len(testData)) * 256)\n\t\t}\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-500b\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 500)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-50k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 50000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (not predictable, high entropy distribution)\n\tb.Run(\"random-500k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 500000)\n\t\trand.Read(testData)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\n\t// (not predictable, medium entropy distribution)\n\tb.Run(\"base-32-5k\", func(b *testing.B) {\n\t\tvar testData = make([]byte, 5000)\n\t\trand.Read(testData)\n\t\ts := base32.StdEncoding.EncodeToString(testData)\n\t\ttestData = []byte(s)\n\t\ttestData = testData[:5000]\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n\t// (medium predictable, medium entropy distribution)\n\tb.Run(\"text\", func(b *testing.B) {\n\t\tvar testData = []byte(`If compression is done per-chunk, care should be taken that it doesn't leave restic backups open to watermarking/fingerprinting attacks.\nThis is essentially the same problem we discussed related to fingerprinting the CDC deduplication process:\nWith \"naive\" CDC, a \"known plaintext\" file can be verified to exist within the backup if the size of individual blocks can be observed by an attacker, by using CDC on the file in parallel and comparing the resulting amount of chunks and individual chunk lengths.\nAs discussed earlier, this can be somewhat mitigated by salting the CDC algorithm with a secret value, as done in attic.\nWith salted CDC, I assume compression would happen on each individual chunk, after splitting the problematic file into chunks. Restic chunks are in the range of 512 KB to 8 MB (but not evenly distributed - right?).\nAttacker knows that the CDC algorithm uses a secret salt, so the attacker generates a range of chunks consisting of the first 512 KB to 8 MB of the file, one for each valid chunk length. The attacker is also able to determine the lengths of compressed chunks.\nThe attacker then compresses that chunk using the compression algorithm.\nThe attacker compares the lengths of the resulting chunks to the first chunk in the restic backup sets.\nIF a matching block length is found, the attacker repeats the exercise with the next chunk, and the next chunk, and the next chunk, ... and the next chunk.\nIt is my belief that with sufficiently large files, and considering the fact that the CDC algorithm is \"biased\" (in lack of better of words) towards generating blocks of about 1 MB, this would be sufficient to ascertain whether or not a certain large file exists in the backup.\nAS always, a paranoid and highly unscientific stream of consciousness.\nThoughts?`)\n\t\ttestData = append(testData, testData...)\n\t\ttestData = append(testData, testData...)\n\t\tb.SetBytes(int64(len(testData)))\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tShannonEntropyBits(testData)\n\t\t}\n\t\tb.Log(ShannonEntropyBits(testData))\n\t})\n}\n"
        },
        {
          "name": "dict",
          "type": "tree",
          "content": null
        },
        {
          "name": "flate",
          "type": "tree",
          "content": null
        },
        {
          "name": "fse",
          "type": "tree",
          "content": null
        },
        {
          "name": "gen.sh",
          "type": "blob",
          "size": 0.05078125,
          "content": "#!/bin/sh\n\ncd s2/cmd/_s2sx/ || exit 1\ngo generate .\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.1337890625,
          "content": "module github.com/klauspost/compress\n\ngo 1.21\n\nretract (\n\t// https://github.com/klauspost/compress/pull/503\n\tv1.14.3\n\tv1.14.2\n\tv1.14.1\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "gzhttp",
          "type": "tree",
          "content": null
        },
        {
          "name": "gzip",
          "type": "tree",
          "content": null
        },
        {
          "name": "huff0",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "ossfuzz",
          "type": "tree",
          "content": null
        },
        {
          "name": "s2",
          "type": "tree",
          "content": null
        },
        {
          "name": "s2sx.mod",
          "type": "blob",
          "size": 0.0458984375,
          "content": "module github.com/klauspost/compress\n\ngo 1.19\n\n"
        },
        {
          "name": "s2sx.sum",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "snappy",
          "type": "tree",
          "content": null
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "zip",
          "type": "tree",
          "content": null
        },
        {
          "name": "zlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "zstd",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}