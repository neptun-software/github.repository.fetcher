{
  "metadata": {
    "timestamp": 1736567134299,
    "page": 730,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjczMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/cassandra-gocql-driver",
      "stars": 2596,
      "defaultBranch": "trunk",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.2265625,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nnotifications:\n  commits:      commits@cassandra.apache.org\n  issues:       commits@cassandra.apache.org\n  pullrequests: pr@cassandra.apache.org\n  jira_options: link worklog\n\ngithub:\n  description: \"GoCQL Driver for Apache CassandraÂ®\"\n  homepage: https://cassandra.apache.org/\n  enabled_merge_buttons:\n    squash:  false\n    merge:   false\n    rebase:  true\n  features:\n    wiki: false\n    issues: true\n    projects: false\n  autolink_jira:\n    - CASSANDRA\n    - CASSGO\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.048828125,
          "content": "gocql-fuzz\nfuzz-corpus\nfuzz-work\ngocql.test\n.idea\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 6.8916015625,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n\n### Changed\n\n- Don't restrict server authenticator unless PasswordAuthentictor.AllowedAuthenticators is provided (CASSGO-19)\n\n- Remove global NewBatch function (CASSGO-15)\n\n- Detailed description for NumConns (CASSGO-3)\n\n- Change Batch API to be consistent with Query() (CASSGO-7)\n\n### Fixed\n\n- Retry policy now takes into account query idempotency (CASSGO-27)\n- Don't return error to caller with RetryType Ignore (CASSGO-28)\n\n## [1.7.0] - 2024-09-23\n\nThis release is the first after the donation of gocql to the Apache Software Foundation (ASF)\n\n### Changed\n- Update DRIVER_NAME parameter in STARTUP messages to a different value intended to clearly identify this\n  driver as an ASF driver.  This should clearly distinguish this release (and future cassandra-gocql-driver\n  releases) from prior versions. (#1824)\n- Supported Go versions updated to 1.23 and 1.22 to conform to gocql's sunset model. (#1825)\n\n## [1.6.0] - 2023-08-28\n\n### Added\n- Added the InstaclustrPasswordAuthenticator to the list of default approved authenticators. (#1711)\n- Added the `com.scylladb.auth.SaslauthdAuthenticator` and `com.scylladb.auth.TransitionalAuthenticator`\n  to the list of default approved authenticators. (#1712)\n- Added transferring Keyspace and Table names to the Query from the prepared response and updating\n  information about that every time this information is received. (#1714)\n\n### Changed\n- Tracer created with NewTraceWriter now includes the thread information from trace events in the output. (#1716)\n- Increased default timeouts so that they are higher than Cassandra default timeouts.\n  This should help prevent issues where a default configuration overloads a server using default timeouts\n  during retries. (#1701, #1719)\n\n## [1.5.2] - 2023-06-12\n\nSame as 1.5.0. GitHub does not like gpg signed text in the tag message (even with prefixed armor),\nso pushing a new tag.\n\n## [1.5.1] - 2023-06-12\n\nSame as 1.5.0. GitHub does not like gpg signed text in the tag message,\nso pushing a new tag.\n\n## [1.5.0] - 2023-06-12\n\n### Added\n\n- gocql now advertises the driver name and version in the STARTUP message to the server.\n  The values are taken from the Go module's path and version\n  (or from the replacement module, if used). (#1702)\n  That allows the server to track which fork of the driver is being used.\n- Query.Values() to retrieve the values bound to the Query.\n  This makes writing wrappers around Query easier. (#1700)\n\n### Fixed\n- Potential panic on deserialization (#1695)\n- Unmarshalling of dates outside of `[1677-09-22, 2262-04-11]` range. (#1692)\n\n## [1.4.0] - 2023-04-26\n\n### Added\n\n### Changed\n\n- gocql now refreshes the entire ring when it receives a topology change event and\n  when control connection is re-connected.\n  This simplifies code managing ring state. (#1680)\n- Supported versions of Cassandra that we test against are now 4.0.x and 4.1.x. (#1685)\n- Default HostDialer now uses already-resolved connect address instead of hostname when establishing TCP connections (#1683).\n\n### Fixed\n\n- Deadlock in Session.Close(). (#1688)\n- Race between Query.Release() and speculative executions (#1684)\n- Missed ring update during control connection reconnection (#1680)\n\n## [1.3.2] - 2023-03-27\n\n### Changed\n\n- Supported versions of Go that we test against are now Go 1.19 and Go 1.20.\n\n### Fixed\n\n- Node event handling now processes topology events before status events.\n  This fixes some cases where new nodes were missed. (#1682)\n- Learning a new IP address for an existing node (identified by host ID) now triggers replacement of that host.\n  This fixes some Kubernetes reconnection failures. (#1682)\n- Refresh ring when processing a node UP event for an unknown host.\n  This fixes some cases where new nodes were missed. (#1669)\n\n## [1.3.1] - 2022-12-13\n\n### Fixed\n\n- Panic in RackAwareRoundRobinPolicy caused by wrong alignment on 32-bit platforms. (#1666)\n\n## [1.3.0] - 2022-11-29\n\n### Added\n\n- Added a RackAwareRoundRobinPolicy that attempts to keep client->server traffic in the same rack when possible.\n\n### Changed\n\n- Supported versions of Go that we test against are now Go 1.18 and Go 1.19.\n\n## [1.2.1] - 2022-09-02\n\n### Changed\n\n- GetCustomPayload now returns nil instead of panicking in case of query error. (#1385)\n\n### Fixed\n\n- Nil pointer dereference in events.go when handling node removal. (#1652)\n- Reading peers from DataStax Enterprise clusters. This was a regression in 1.2.0. (#1646)\n- Unmarshaling maps did not pre-allocate the map. (#1642)\n\n## [1.2.0] - 2022-07-07\n\nThis release improves support for connecting through proxies and some improvements when using Cassandra 4.0 or later.\n\n### Added\n- HostDialer interface now allows customizing connection including TLS setup per host. (#1629)\n\n### Changed\n- The driver now uses `host_id` instead of connect address to identify nodes. (#1632)\n- gocql reads `system.peers_v2` instead of `system.peers` when connected to Cassandra 4.0 or later and\n  populates `HostInfo.Port` using the native port. (#1635)\n\n### Fixed\n- Data race in `HostInfo.HostnameAndPort()`. (#1631)\n- Handling of nils when marshaling/unmarshaling lists and maps. (#1630)\n- Silent data corruption in case a map was serialized into UDT and some fields in the UDT were not present in the map.\n  The driver now correctly writes nulls instead of shifting fields. (#1626, #1639)\n\n## [1.1.0] - 2022-04-29\n\n### Added\n- Changelog.\n- StreamObserver and StreamObserverContext interfaces to allow observing CQL streams.\n- ClusterConfig.WriteTimeout option now allows to specify a write-timeout different from read-timeout.\n- TypeInfo.NewWithError method.\n\n### Changed\n- Supported versions of Go that we test against are now Go 1.17 and Go 1.18.\n- The driver now returns an error if SetWriteDeadline fails. If you need to run gocql on\n  a platform that does not support SetWriteDeadline, set WriteTimeout to zero to disable the timeout.\n- Creating streams on a connection that is closing now fails early.\n- HostFilter now also applies to control connections.\n- TokenAwareHostPolicy now panics immediately during initialization instead of at random point later\n  if you reuse the TokenAwareHostPolicy between multiple sessions. Reusing TokenAwareHostPolicy between\n  sessions was never supported.\n\n### Fixed\n- The driver no longer resets the network connection if a write fails with non-network-related error.\n- Blocked network write to a network could block other goroutines, this is now fixed.\n- Fixed panic in unmarshalUDT when trying to unmarshal a user-defined-type to a non-pointer Go type.\n- Fixed panic when trying to unmarshal unknown/custom CQL type.\n\n## Deprecated\n- TypeInfo.New, please use TypeInfo.NewWithError instead.\n\n## [1.0.0] - 2022-03-04\n### Changed\n- Started tagging versions with semantic version tags\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.8896484375,
          "content": "# Contributing to the Apache Cassandra GoCQL Driver\n\n**TL;DR** This manifesto sets out the bare minimum requirements for submitting a patch to gocql. It also offers some suggestions to speed up the review, approve and merge process.\n\nThis guide outlines the process of landing patches in gocql and the general approach to maintaining the code base.\n\n## Background\n\nThe goal of the gocql project is to provide a stable and robust CQL driver for Go. This is a community driven project that is coordinated by a small team of developers in and around the Apache Cassandra project. For security, governance and administration issues please refer to the Cassandra Project Management Committee.\n\n## Engage with the community early\n\nIf you are interested in contributing a particular feature or bug fix we heavily encourage you to start a discussion with the community or at the very least announce your interest in working on it before jumping right into writing code. It helps reduce the likelihood of multiple people working on the same issue in parallel when they could be collaborating instead. Getting feedback early in the contribution process will also greatly speed up the review, approval and merge process. \n\nCommon ways to engage with the GoCQL community are: \n\n- [CASSGO project on ASF JIRA](https://issues.apache.org/jira/projects/CASSGO/issues/)\n- Apache Cassandra dev mailing list - [dev@cassandra.apache.org](mailto:dev@cassandra.apache.org)\n  - [Subscribe](mailto:dev-subscribe@cassandra.apache.org)\n  - [Archives](https://lists.apache.org/list.html?dev@cassandra.apache.org)\n- [#cassandra-drivers channel on ASF Slack](https://the-asf.slack.com/archives/C05LPRVNZV1)\n\n## Minimum Requirement Checklist\n\nThe following is a check list of requirements that need to be satisfied in order for us to merge your patch:\n\n* A JIRA issue exists in the [CASSGO Project](https://issues.apache.org/jira/projects/CASSGO/issues/) for the proposed changes\n  * If the proposed changes are significant then ideally a discussion about the implementation approach should happen before a PR is even opened (prototyping is fine though)\n* Pull request raised to apache/cassandra-gocql-driver on Github\n* The pull request has a title that clearly summarizes the purpose of the patch and references the relevant CASSGO JIRA issue if there is one\n* The motivation behind the patch is clearly defined in the pull request summary\n* JIRA issue is added to the \"UNRELEASED\" section of CHANGELOG.md\n  * If there's no JIRA issue yet then the author is encouraged to create it\n  * If the author is not able to create the JIRA issue then the committer that merges the PR will take care of this (creating the JIRA and adding it to CHANGELOG.md)\n  * Updating CHANGELOG.md is not required if the change is not \"releasable\" (e.g. changes to documentation, CI, etc.)\n* You agree that your contribution is donated to the Apache Software Foundation (appropriate copyright is on all new files)\n* The patch will merge cleanly\n* The test coverage does not fall\n* The merge commit passes the regression test suite on GitHub Actions\n* `go fmt` has been applied to the submitted code\n* Functional changes are documented in godoc\n* A correctly formatted commit message, see below\n\nIf there are any requirements that can't be reasonably satisfied, please state this either on the pull request or as part of discussion on the mailing list, JIRA or slack. Where appropriate, the core team may apply discretion and make an exception to these requirements.\n\n## Commit Message\n\nThe commit message format should be:\n\n```\n<short description>\n\n<reason why the change is needed>\n\nPatch by <authors>; reviewed by <Reviewers> for CASSGO-#####\n```\n\nShort description should:\n* Be a short sentence.\n* Start with a capital letter.\n* Be written in the present tense.\n* Summarize what is changed, not why it is changed.\n\nShort description should not:\n* End with a period.\n* Use the word Fixes . Most commits fix something.\n\nLong description / Reason:\n* Should describe why the change is needed. What is fixed by the change? Why it it was broken before? What use case does the new feature solve?\n* Consider adding details of other options that you considered when implementing the change and why you made the design decisions you made.\n\nThe `patch by â¦; reviewed by` line is important. It is parsed to build the [project contribulyse statistics](https://nightlies.apache.org/cassandra/devbranch/misc/contribulyze/html/).\n\nSome tips from the Apache Cassandra Project's \"How to Commit\" documentation: https://cassandra.apache.org/_/development/how_to_commit.html#tips\n\n#### Example commit message:\n\n```\nIncrease default timeouts to 11s\n\nClient timeouts need to be higher than server timeouts,\nso that work does not accumulate on the server with retries.\nIf the client timeout is shorter than a server timeout,\nthe client can start a retry while the original request\nis still running on the server.\n\nThe default gocql default timeout was lower\nthan the Cassandra default timeout.\n\nCassandra has multiple server timeout options,\nmost of them are less or equal to 10s by default as of Cassandra 4.1:\n\nread_request_timeout           5s\nrange_request_timeout          10s\nwrite_request_timeout          2s\ncounter_write_request_timeout  5s\ncas_contention_timeout         1s\ntruncate_request_timeout       60s\nrequest_timeout                10s\n\nTruncate is an uncommon operation, so we can use 11s as the default\ntimeout.\n\npatch by John Doe, Jane Doe; reviewed by Bob Smith, Jane Smith for CASSGO-#####\n```\n\n### Signing commits\n\nSigning commits with a pgp or ssh key is heavily encouraged although not required.\n\n## Beyond The Checklist\n\nIn addition to stating the hard requirements, there are a bunch of things that we consider when assessing changes to the library. These soft requirements are helpful pointers of how to get a patch landed quicker and with less fuss.\n\n### General QA Approach\n\nThe Cassandra project needs to consider the ongoing maintainability of the library at all times. Patches that look like they will introduce maintenance issues for the team will not be accepted.\n\nYour patch will get merged quicker if you have decent test cases that provide test coverage for the new behavior you wish to introduce.\n\nUnit tests are good, integration tests are even better. An example of a unit test is `marshal_test.go` - this tests the serialization code in isolation. `cassandra_test.go` is an integration test suite that is executed against every version of Cassandra that gocql supports as part of the CI process on Travis.\n\nThat said, the point of writing tests is to provide a safety net to catch regressions, so there is no need to go overboard with tests. Remember that the more tests you write, the more code we will have to maintain. So there's a balance to strike there.\n\n### Sign Off Procedure\n\nA Pull Request needs +1s from two committers before it can be merged (or one +1 if the author is a committer).\n\nAs stated earlier, suitable test coverage will increase the likelihood that a PR will be approved and merged. If your change has no test coverage, or looks like it may have wider implications for the health and stability of the library, the reviewers may elect to refer the change to other members of the community to achieve consensus before proceeding. Therefore, the tighter and cleaner your patch is, the quicker it will go through the review process.\n\n### Supported Features\n\ngocql is a low level wire driver for Cassandra CQL. By and large, we would like to keep the functional scope of the library as narrow as possible. We think that gocql should be tight and focused, and we will be naturally skeptical of things that could just as easily be implemented in a higher layer. \n\nInevitably you will come across something that could be implemented in a higher layer, save for a minor change to the core API. In this instance, please strike up a conversation in the Cassandra community. \n\nChances are we will understand what you are trying to achieve and will try to accommodate this in a maintainable way.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0927734375,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 7.630859375,
          "content": "Apache Cassandra GoCQL Driver\nCopyright 2024 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nThis product originates, before git sha\n34fdeebefcbf183ed7f916f931aa0586fdaa1b40, from software from the\nGocql Authors, with copyright and license as follows:\n\nCopyright (c) 2016, The Gocql authors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWhere The Gocql Authors for copyright purposes are below. Those marked with\nasterisk have agreed to donate (copyright assign) their contributions to the\nApache Software Foundation, signing CLAs when appropriate.\n\nChristoph Hack <christoph@tux21b.org>\nJonathan Rudenberg <jonathan@titanous.com> *\nThorsten von Eicken <tve@rightscale.com> *\nMatt Robenolt <mattr@disqus.com>\nPhillip Couto <phillip.couto@stemstudios.com> *\nNiklas Korz <korz.niklask@gmail.com>\nNimi Wariboko Jr <nimi@channelmeter.com>\nGhais Issa <ghais.issa@gmail.com> *\nSasha Klizhentas <klizhentas@gmail.com>\nKonstantin Cherkasov <k.cherkasoff@gmail.com>\nBen Hood <0x6e6562@gmail.com>\nPete Hopkins <phopkins@gmail.com>\nChris Bannister <c.bannister@gmail.com> *\nMaxim Bublis <b@codemonkey.ru>\nAlex Zorin <git@zor.io>\nKasper Middelboe Petersen <me@phant.dk>\nHarpreet Sawhney <harpreet.sawhney@gmail.com>\nCharlie Andrews <charlieandrews.cwa@gmail.com> *\nStanislavs Koikovs <stanislavs.koikovs@gmail.com>\nDan Forest <bonjour@dan.tf>\nMiguel Serrano <miguelvps@gmail.com> *\nStefan Radomski <gibheer@zero-knowledge.org>\nJosh Wright <jshwright@gmail.com>\nJacob Rhoden <jacob.rhoden@gmail.com>\nBen Frye <benfrye@gmail.com>\nFred McCann <fred@sharpnoodles.com> *\nDan Simmons <dan@simmons.io> *\nMuir Manders <muir@retailnext.net>\nSankar P <sankar.curiosity@gmail.com> *\nJulien Da Silva <julien.dasilva@gmail.com>\nDan Kennedy <daniel@firstcs.co.uk> *\nNick Dhupia<nick.dhupia@gmail.com>\nYasuharu Goto <matope.ono@gmail.com> *\nJeremy Schlatter <jeremy.schlatter@gmail.com> *\nMatthias Kadenbach <matthias.kadenbach@gmail.com>\nDean Elbaz <elbaz.dean@gmail.com>\nMike Berman <evencode@gmail.com>\nDmitriy Fedorenko <c0va23@gmail.com> *\nZach Marcantel <zmarcantel@gmail.com>\nJames Maloney <jamessagan@gmail.com>\nAshwin Purohit <purohit@gmail.com> *\nDan Kinder <dkinder.is.me@gmail.com> *\nOliver Beattie <oliver@obeattie.com> *\nJustin Corpron <jncorpron@gmail.com> *\nMiles Delahunty <miles.delahunty@gmail.com>\nZach Badgett <zach.badgett@gmail.com>\nMaciek Sakrejda <maciek@heroku.com> *\nJeff Mitchell <jeffrey.mitchell@gmail.com>\nBaptiste Fontaine <b@ptistefontaine.fr> *\nMatt Heath <matt@mattheath.com> *\nJamie Cuthill <jamie.cuthill@gmail.com>\nAdrian Casajus <adriancasajus@gmail.com> *\nJohn Weldon <johnweldon4@gmail.com> *\nAdrien Bustany <adrien@bustany.org> *\nAndrey Smirnov <smirnov.andrey@gmail.com> *\nAdam Weiner <adamsweiner@gmail.com> *\nDaniel Cannon <daniel@danielcannon.co.uk>\nJohnny BergstrÃ¶m <johnny@joonix.se>\nAdriano Orioli <orioli.adriano@gmail.com> *\nClaudiu Raveica <claudiu.raveica@gmail.com> *\nArtem Chernyshev <artem.0xD2@gmail.com> *\nFerence Fu <fym201@msn.com>\nLOVOO <opensource@lovoo.com>\nnikandfor <nikandfor@gmail.com> *\nAnthony Woods <awoods@raintank.io> *\nAlexander Inozemtsev <alexander.inozemtsev@gmail.com> *\nRob McColl <rob@robmccoll.com>; <rmccoll@ionicsecurity.com> *\nViktor TÃ¶nkÃ¶l <viktor.toenkoel@motionlogic.de> *\nIan Lozinski <ian.lozinski@gmail.com>\nMichael Highstead <highstead@gmail.com> *\nSarah Brown <esbie.is@gmail.com> *\nCaleb Doxsey <caleb@datadoghq.com> *\nFrederic Hemery <frederic.hemery@datadoghq.com> *\nPekka Enberg <penberg@scylladb.com> *\nMark M <m.mim95@gmail.com>\nBartosz Burclaf <burclaf@gmail.com> *\nMarcus King <marcusking01@gmail.com> *\nAndrew de Andrade <andrew@deandrade.com.br>\nRobert Nix <robert@nicerobot.org>\nNathan Youngman <git@nathany.com> *\nCharles Law <charles.law@gmail.com>; <claw@conduce.com> *\nNathan Davies <nathanjamesdavies@gmail.com> *\nBo Blanton <bo.blanton@gmail.com>\nVincent Rischmann <me@vrischmann.me> *\nJesse Claven <jesse.claven@gmail.com> *\nDerrick Wippler <thrawn01@gmail.com>\nLeigh McCulloch <leigh@leighmcculloch.com>\nRon Kuris <swcafe@gmail.com>\nRaphael Gavache <raphael.gavache@gmail.com> *\nYasser Abdolmaleki <yasser@yasser.ca>\nKrishnanand Thommandra <devtkrishna@gmail.com>\nBlake Atkinson <me@blakeatkinson.com>\nDharmendra Parsaila <d4dharmu@gmail.com>\nNayef Ghattas <nayef.ghattas@datadoghq.com> *\nMichaÅ Matczuk <mmatczuk@gmail.com> *\nBen Krebsbach <ben.krebsbach@gmail.com> *\nVivian Mathews <vivian.mathews.3@gmail.com> *\nSascha Steinbiss <satta@debian.org> *\nSeth Rosenblum <seth.t.rosenblum@gmail.com> *\nJavier Zunzunegui <javier.zunzunegui.b@gmail.com>\nLuke Hines <lukehines@protonmail.com> *\nZhixin Wen <john.wenzhixin@hotmail.com> *\nChang Liu <changliu.it@gmail.com>\nIngo Oeser <nightlyone@gmail.com> *\nLuke Hines <lukehines@protonmail.com>\nJacob Greenleaf <jacob@jacobgreenleaf.com>\nAlex Lourie <alex@instaclustr.com>; <djay.il@gmail.com> *\nMarco Cadetg <cadetg@gmail.com> *\nKarl Matthias <karl@matthias.org> *\nThomas Meson <zllak@hycik.org> *\nMartin Sucha <martin.sucha@kiwi.com>; <git@mm.ms47.eu> *\nPavel Buchinchik <p.buchinchik@gmail.com>\nRintaro Okamura <rintaro.okamura@gmail.com> *\nYura Sokolov <y.sokolov@joom.com>; <funny.falcon@gmail.com>\nJorge Bay <jorgebg@apache.org> *\nDmitriy Kozlov <hummerd@mail.ru> *\nAlexey Romanovsky <alexus1024+gocql@gmail.com>\nJaume Marhuenda Beltran <jaumemarhuenda@gmail.com>\nPiotr Dulikowski <piodul@scylladb.com>\nÃrni Dagur <arni@dagur.eu> *\nTushar Das <tushar.das5@gmail.com> *\nMaxim Vladimirskiy <horkhe@gmail.com> *\nBogdan-Ciprian Rusu <bogdanciprian.rusu@crowdstrike.com> *\nYuto Doi <yutodoi.seattle@gmail.com> *\nKrishna Vadali <tejavadali@gmail.com>\nJens-W. Schicke-Uffmann <drahflow@gmx.de> *\nOndrej PolakoviÄ <ondrej.polakovic@kiwi.com> *\nSergei Karetnikov <sergei.karetnikov@gmail.com> *\nStefan Miklosovic <smiklosovic@apache.org> *\nAdam Burk <amburk@gmail.com> *\nValerii Ponomarov <kiparis.kh@gmail.com> *\nNeal Turett <neal.turett@datadoghq.com> *\nDoug Schaapveld <djschaap@gmail.com> *\nSteven Seidman <steven.seidman@datadoghq.com>\nWojciech PrzytuÅa <wojciech.przytula@scylladb.com> *\nJoÃ£o Reis <joao.reis@datastax.com> *\nLauro Ramos Venancio <lauro.venancio@incognia.com>\nDmitry Kropachev <dmitry.kropachev@gmail.com>\nOliver Boyle <pleasedontspamme4321+gocql@gmail.com> *\nJackson Fleming <jackson.fleming@instaclustr.com> *\nSylwia Szunejko <sylwia.szunejko@scylladb.com> *\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.0771484375,
          "content": "Apache Cassandra GoCQL Driver\n=====\n\n[!Join the chat at https://the-asf.slack.com/archives/C05LPRVNZV1](https://the-asf.slack.com/archives/C05LPRVNZV1)\n![go build](https://github.com/apache/cassandra-gocql-driver/actions/workflows/main.yml/badge.svg)\n[![GoDoc](https://godoc.org/github.com/gocql/gocql?status.svg)](https://godoc.org/github.com/gocql/gocql)\n\nPackage gocql implements a fast and robust Cassandra client for the\nGo programming language.\n\nProject Website: https://cassandra.apache.org<br>\nAPI documentation: https://godoc.org/github.com/gocql/gocql<br>\nDiscussions: https://cassandra.apache.org/_/community.html#discussions\n\nSupported Versions\n------------------\n\nThe following matrix shows the versions of Go and Cassandra that are tested with the integration test suite as part of the CI build:\n\n| Go/Cassandra | 4.0.x | 4.1.x | \n|--------------|-------|-------|\n| 1.22         | yes   | yes   |\n| 1.23         | yes   | yes   |\n\nGocql has been tested in production against many versions of Cassandra. Due to limits in our CI setup we only\ntest against the latest 2 GA releases.\n\nSunsetting Model\n----------------\n\nIn general, the Cassandra community will focus on supporting the current and previous versions of Go. gocql may still work with older versions of Go, but official support for these versions will have been sunset.\n\nInstallation\n------------\n\n    go get github.com/gocql/gocql\n\n\nFeatures\n--------\n\n* Modern Cassandra client using the native transport\n* Automatic type conversions between Cassandra and Go\n  * Support for all common types including sets, lists and maps\n  * Custom types can implement a `Marshaler` and `Unmarshaler` interface\n  * Strict type conversions without any loss of precision\n  * Built-In support for UUIDs (version 1 and 4)\n* Support for logged, unlogged and counter batches\n* Cluster management\n  * Automatic reconnect on connection failures with exponential falloff\n  * Round robin distribution of queries to different hosts\n  * Round robin distribution of queries to different connections on a host\n  * Each connection can execute up to n concurrent queries (whereby n is the limit set by the protocol version the client chooses to use)\n  * Optional automatic discovery of nodes\n  * Policy based connection pool with token aware and round-robin policy implementations\n* Support for password authentication\n* Iteration over paged results with configurable page size\n* Support for TLS/SSL\n* Optional frame compression (using snappy)\n* Automatic query preparation\n* Support for query tracing\n* Support for Cassandra 2.1+ [binary protocol version 3](https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v3.spec)\n  * Support for up to 32768 streams\n  * Support for tuple types\n  * Support for client side timestamps by default\n  * Support for UDTs via a custom marshaller or struct tags\n* Support for Cassandra 3.0+ [binary protocol version 4](https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec)\n* An API to access the schema metadata of a given keyspace\n\nPerformance\n-----------\nWhile the driver strives to be highly performant, there are cases where it is difficult to test and verify. The driver is built\nwith maintainability and code readability in mind first and then performance and features, as such every now and then performance\nmay degrade, if this occurs please report and issue and it will be looked at and remedied. The only time the driver copies data from\nits read buffer is when it Unmarshal's data into supplied types.\n\nSome tips for getting more performance from the driver:\n* Use the TokenAware policy\n* Use many goroutines when doing inserts, the driver is asynchronous but provides a synchronous API, it can execute many queries concurrently\n* Tune query page size\n* Reading data from the network to unmarshal will incur a large amount of allocations, this can adversely affect the garbage collector, tune `GOGC`\n* Close iterators after use to recycle byte buffers\n\nImportant Default Keyspace Changes\n----------------------------------\ngocql no longer supports executing \"use <keyspace>\" statements to simplify the library. The user still has the\nability to define the default keyspace for connections but now the keyspace can only be defined before a\nsession is created. Queries can still access keyspaces by indicating the keyspace in the query:\n```sql\nSELECT * FROM example2.table;\n```\n\nExample of correct usage:\n```go\n\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n\tcluster.Keyspace = \"example\"\n\t...\n\tsession, err := cluster.CreateSession()\n\n```\nExample of incorrect usage:\n```go\n\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n\tcluster.Keyspace = \"example\"\n\t...\n\tsession, err := cluster.CreateSession()\n\n\tif err = session.Query(\"use example2\").Exec(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n```\nThis will result in an err being returned from the session.Query line as the user is trying to execute a \"use\"\nstatement.\n\nExample\n-------\n\nSee [package documentation](https://pkg.go.dev/github.com/gocql/gocql#pkg-examples).\n\nData Binding\n------------\n\nThere are various ways to bind application level data structures to CQL statements:\n\n* You can write the data binding by hand, as outlined in the Tweet example. This provides you with the greatest flexibility, but it does mean that you need to keep your application code in sync with your Cassandra schema.\n* You can dynamically marshal an entire query result into an `[]map[string]interface{}` using the `SliceMap()` API. This returns a slice of row maps keyed by CQL column names. This method requires no special interaction with the gocql API, but it does require your application to be able to deal with a key value view of your data.\n* As a refinement on the `SliceMap()` API you can also call `MapScan()` which returns `map[string]interface{}` instances in a row by row fashion.\n* The `Bind()` API provides a client app with a low level mechanism to introspect query meta data and extract appropriate field values from application level data structures.\n* The [gocqlx](https://github.com/scylladb/gocqlx) package is an idiomatic extension to gocql that provides usability features. With gocqlx you can bind the query parameters from maps and structs, use named query parameters (:identifier) and scan the query results into structs and slices. It comes with a fluent and flexible CQL query builder that supports full CQL spec, including BATCH statements and custom functions.\n* Building on top of the gocql driver, [cqlr](https://github.com/relops/cqlr) adds the ability to auto-bind a CQL iterator to a struct or to bind a struct to an INSERT statement.\n* Another external project that layers on top of gocql is [cqlc](http://relops.com/cqlc) which generates gocql compliant code from your Cassandra schema so that you can write type safe CQL statements in Go with a natural query syntax.\n* [gocassa](https://github.com/hailocab/gocassa) is an external project that layers on top of gocql to provide convenient query building and data binding.\n* [gocqltable](https://github.com/kristoiv/gocqltable) provides an ORM-style convenience layer to make CRUD operations with gocql easier.\n\nEcosystem\n---------\n\nThe following community maintained tools are known to integrate with gocql:\n\n* [gocqlx](https://github.com/scylladb/gocqlx) is a gocql extension that automates data binding, adds named queries support, provides flexible query builders and plays well with gocql.\n* [journey](https://github.com/db-journey/journey) is a migration tool with Cassandra support.\n* [negronicql](https://github.com/mikebthun/negronicql) is gocql middleware for Negroni.\n* [cqlr](https://github.com/relops/cqlr) adds the ability to auto-bind a CQL iterator to a struct or to bind a struct to an INSERT statement.\n* [cqlc](http://relops.com/cqlc) generates gocql compliant code from your Cassandra schema so that you can write type safe CQL statements in Go with a natural query syntax.\n* [gocassa](https://github.com/hailocab/gocassa) provides query building, adds data binding, and provides easy-to-use \"recipe\" tables for common query use-cases.\n* [gocqltable](https://github.com/kristoiv/gocqltable) is a wrapper around gocql that aims to simplify common operations.\n* [gockle](https://github.com/willfaught/gockle) provides simple, mockable interfaces that wrap gocql types\n* [scylladb](https://github.com/scylladb/scylla) is a fast Apache Cassandra-compatible NoSQL database\n* [go-cql-driver](https://github.com/MichaelS11/go-cql-driver) is an CQL driver conforming to the built-in database/sql interface. It is good for simple use cases where the database/sql interface is wanted. The CQL driver is a wrapper around this project.\n\nOther Projects\n--------------\n\n* [gocqldriver](https://github.com/tux21b/gocqldriver) is the predecessor of gocql based on Go's `database/sql` package. This project isn't maintained anymore, because Cassandra wasn't a good fit for the traditional `database/sql` API. Use this package instead.\n\nSEO\n---\n\nFor some reason, when you Google `golang cassandra`, this project doesn't feature very highly in the result list. But if you Google `go cassandra`, then we're a bit higher up the list. So this is note to try to convince Google that golang is an alias for Go.\n"
        },
        {
          "name": "address_translators.go",
          "type": "blob",
          "size": 1.953125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport \"net\"\n\n// AddressTranslator provides a way to translate node addresses (and ports) that are\n// discovered or received as a node event. This can be useful in an ec2 environment,\n// for instance, to translate public IPs to private IPs.\ntype AddressTranslator interface {\n\t// Translate will translate the provided address and/or port to another\n\t// address and/or port. If no translation is possible, Translate will return the\n\t// address and port provided to it.\n\tTranslate(addr net.IP, port int) (net.IP, int)\n}\n\ntype AddressTranslatorFunc func(addr net.IP, port int) (net.IP, int)\n\nfunc (fn AddressTranslatorFunc) Translate(addr net.IP, port int) (net.IP, int) {\n\treturn fn(addr, port)\n}\n\n// IdentityTranslator will do nothing but return what it was provided. It is essentially a no-op.\nfunc IdentityTranslator() AddressTranslator {\n\treturn AddressTranslatorFunc(func(addr net.IP, port int) (net.IP, int) {\n\t\treturn addr, port\n\t})\n}\n"
        },
        {
          "name": "address_translators_test.go",
          "type": "blob",
          "size": 1.8984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestIdentityAddressTranslator_NilAddrAndZeroPort(t *testing.T) {\n\tvar tr AddressTranslator = IdentityTranslator()\n\thostIP := net.ParseIP(\"\")\n\tif hostIP != nil {\n\t\tt.Errorf(\"expected host ip to be (nil) but was (%+v) instead\", hostIP)\n\t}\n\n\taddr, port := tr.Translate(hostIP, 0)\n\tif addr != nil {\n\t\tt.Errorf(\"expected translated host to be (nil) but was (%+v) instead\", addr)\n\t}\n\tassertEqual(t, \"translated port\", 0, port)\n}\n\nfunc TestIdentityAddressTranslator_HostProvided(t *testing.T) {\n\tvar tr AddressTranslator = IdentityTranslator()\n\thostIP := net.ParseIP(\"10.1.2.3\")\n\tif hostIP == nil {\n\t\tt.Error(\"expected host ip not to be (nil)\")\n\t}\n\n\taddr, port := tr.Translate(hostIP, 9042)\n\tif !hostIP.Equal(addr) {\n\t\tt.Errorf(\"expected translated addr to be (%+v) but was (%+v) instead\", hostIP, addr)\n\t}\n\tassertEqual(t, \"translated port\", 9042, port)\n}\n"
        },
        {
          "name": "batch_test.go",
          "type": "blob",
          "size": 2.5830078125,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestBatch_Errors(t *testing.T) {\n\tif *flagProto == 1 {\n\t}\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion2 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_errors (id int primary key, val inet)`); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tb := session.Batch(LoggedBatch)\n\tb = b.Query(\"SELECT * FROM gocql_test.batch_errors WHERE id=2 AND val=?\", nil)\n\tif err := b.Exec(); err == nil {\n\t\tt.Fatal(\"expected to get error for invalid query in batch\")\n\t}\n}\n\nfunc TestBatch_WithTimestamp(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"Batch timestamps are only available on protocol >= 3\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_ts (id int primary key, val text)`); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmicros := time.Now().UnixNano()/1e3 - 1000\n\n\tb := session.Batch(LoggedBatch)\n\tb.WithTimestamp(micros)\n\tb = b.Query(\"INSERT INTO gocql_test.batch_ts (id, val) VALUES (?, ?)\", 1, \"val\")\n\tb = b.Query(\"INSERT INTO gocql_test.batch_ts (id, val) VALUES (?, ?)\", 2, \"val\")\n\n\tif err := b.Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar storedTs int64\n\tif err := session.Query(`SELECT writetime(val) FROM gocql_test.batch_ts WHERE id = ?`, 1).Scan(&storedTs); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif storedTs != micros {\n\t\tt.Errorf(\"got ts %d, expected %d\", storedTs, micros)\n\t}\n}\n"
        },
        {
          "name": "cass1batch_test.go",
          "type": "blob",
          "size": 2.638671875,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestProto1BatchInsert(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.large (id int primary key)\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tbegin := \"BEGIN BATCH\"\n\tend := \"APPLY BATCH\"\n\tquery := \"INSERT INTO large (id) VALUES (?)\"\n\tfullQuery := strings.Join([]string{begin, query, end}, \"\\n\")\n\targs := []interface{}{5}\n\tif err := session.Query(fullQuery, args...).Consistency(Quorum).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestShouldPrepareFunction(t *testing.T) {\n\tvar shouldPrepareTests = []struct {\n\t\tStmt   string\n\t\tResult bool\n\t}{\n\t\t{`\n      BEGIN BATCH\n        INSERT INTO users (userID, password)\n        VALUES ('smith', 'secret')\n      APPLY BATCH\n    ;\n      `, true},\n\t\t{`INSERT INTO users (userID, password, name) VALUES ('user2', 'ch@ngem3b', 'second user')`, true},\n\t\t{`BEGIN COUNTER BATCH UPDATE stats SET views = views + 1 WHERE pageid = 1 APPLY BATCH`, true},\n\t\t{`delete name from users where userID = 'smith';`, true},\n\t\t{`  UPDATE users SET password = 'secret' WHERE userID = 'smith'   `, true},\n\t\t{`CREATE TABLE users (\n        user_name varchar PRIMARY KEY,\n        password varchar,\n        gender varchar,\n        session_token varchar,\n        state varchar,\n        birth_year bigint\n      );`, false},\n\t}\n\n\tfor _, test := range shouldPrepareTests {\n\t\tq := &Query{stmt: test.Stmt, routingInfo: &queryRoutingInfo{}}\n\t\tif got := q.shouldPrepare(); got != test.Result {\n\t\t\tt.Fatalf(\"%q: got %v, expected %v\\n\", test.Stmt, got, test.Result)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "cassandra_test.go",
          "type": "blob",
          "size": 104.345703125,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"github.com/stretchr/testify/require\"\n\t\"io\"\n\t\"math\"\n\t\"math/big\"\n\t\"net\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"gopkg.in/inf.v0\"\n)\n\nfunc TestEmptyHosts(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.Hosts = nil\n\tif session, err := cluster.CreateSession(); err == nil {\n\t\tsession.Close()\n\t\tt.Error(\"expected err, got nil\")\n\t}\n}\n\nfunc TestInvalidPeerEntry(t *testing.T) {\n\tt.Skip(\"dont mutate system tables, rewrite this to test what we mean to test\")\n\tsession := createSession(t)\n\n\t// rack, release_version, schema_version, tokens are all null\n\tquery := session.Query(\"INSERT into system.peers (peer, data_center, host_id, rpc_address) VALUES (?, ?, ?, ?)\",\n\t\t\"169.254.235.45\",\n\t\t\"datacenter1\",\n\t\t\"35c0ec48-5109-40fd-9281-9e9d4add2f1e\",\n\t\t\"169.254.235.45\",\n\t)\n\n\tif err := query.Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsession.Close()\n\n\tcluster := createCluster()\n\tcluster.PoolConfig.HostSelectionPolicy = TokenAwareHostPolicy(RoundRobinHostPolicy())\n\tsession = createSessionFromCluster(cluster, t)\n\tdefer func() {\n\t\tsession.Query(\"DELETE from system.peers where peer = ?\", \"169.254.235.45\").Exec()\n\t\tsession.Close()\n\t}()\n\n\t// check we can perform a query\n\titer := session.Query(\"select peer from system.peers\").Iter()\n\tvar peer string\n\tfor iter.Scan(&peer) {\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// TestUseStatementError checks to make sure the correct error is returned when the user tries to execute a use statement.\nfunc TestUseStatementError(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := session.Query(\"USE gocql_test\").Exec(); err != nil {\n\t\tif err != ErrUseStmt {\n\t\t\tt.Fatalf(\"expected ErrUseStmt, got \" + err.Error())\n\t\t}\n\t} else {\n\t\tt.Fatal(\"expected err, got nil.\")\n\t}\n}\n\n// TestInvalidKeyspace checks that an invalid keyspace will return promptly and without a flood of connections\nfunc TestInvalidKeyspace(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.Keyspace = \"invalidKeyspace\"\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tif err != ErrNoConnectionsStarted {\n\t\t\tt.Fatalf(\"Expected ErrNoConnections but got %v\", err)\n\t\t}\n\t} else {\n\t\tsession.Close() //Clean up the session\n\t\tt.Fatal(\"expected err, got nil.\")\n\t}\n}\n\nfunc TestTracing(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.trace (id int primary key)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tbuf := &bytes.Buffer{}\n\ttrace := &traceWriter{session: session, w: buf}\n\tif err := session.Query(`INSERT INTO trace (id) VALUES (?)`, 42).Trace(trace).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if buf.Len() == 0 {\n\t\tt.Fatal(\"insert: failed to obtain any tracing\")\n\t}\n\ttrace.mu.Lock()\n\tbuf.Reset()\n\ttrace.mu.Unlock()\n\n\tvar value int\n\tif err := session.Query(`SELECT id FROM trace WHERE id = ?`, 42).Trace(trace).Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t} else if value != 42 {\n\t\tt.Fatalf(\"value: expected %d, got %d\", 42, value)\n\t} else if buf.Len() == 0 {\n\t\tt.Fatal(\"select: failed to obtain any tracing\")\n\t}\n\n\t// also works from session tracer\n\tsession.SetTrace(trace)\n\ttrace.mu.Lock()\n\tbuf.Reset()\n\ttrace.mu.Unlock()\n\tif err := session.Query(`SELECT id FROM trace WHERE id = ?`, 42).Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t}\n\tif buf.Len() == 0 {\n\t\tt.Fatal(\"select: failed to obtain any tracing\")\n\t}\n}\n\nfunc TestObserve(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.observe (id int primary key)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tvar (\n\t\tobservedErr      error\n\t\tobservedKeyspace string\n\t\tobservedStmt     string\n\t)\n\n\tconst keyspace = \"gocql_test\"\n\n\tresetObserved := func() {\n\t\tobservedErr = errors.New(\"placeholder only\") // used to distinguish err=nil cases\n\t\tobservedKeyspace = \"\"\n\t\tobservedStmt = \"\"\n\t}\n\n\tobserver := funcQueryObserver(func(ctx context.Context, o ObservedQuery) {\n\t\tobservedKeyspace = o.Keyspace\n\t\tobservedStmt = o.Statement\n\t\tobservedErr = o.Err\n\t})\n\n\t// select before inserted, will error but the reporting is err=nil as the query is valid\n\tresetObserved()\n\tvar value int\n\tif err := session.Query(`SELECT id FROM observe WHERE id = ?`, 43).Observer(observer).Scan(&value); err == nil {\n\t\tt.Fatal(\"select: expected error\")\n\t} else if observedErr != nil {\n\t\tt.Fatalf(\"select: observed error expected nil, got %q\", observedErr)\n\t} else if observedKeyspace != keyspace {\n\t\tt.Fatal(\"select: unexpected observed keyspace\", observedKeyspace)\n\t} else if observedStmt != `SELECT id FROM observe WHERE id = ?` {\n\t\tt.Fatal(\"select: unexpected observed stmt\", observedStmt)\n\t}\n\n\tresetObserved()\n\tif err := session.Query(`INSERT INTO observe (id) VALUES (?)`, 42).Observer(observer).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if observedErr != nil {\n\t\tt.Fatal(\"insert:\", observedErr)\n\t} else if observedKeyspace != keyspace {\n\t\tt.Fatal(\"insert: unexpected observed keyspace\", observedKeyspace)\n\t} else if observedStmt != `INSERT INTO observe (id) VALUES (?)` {\n\t\tt.Fatal(\"insert: unexpected observed stmt\", observedStmt)\n\t}\n\n\tresetObserved()\n\tvalue = 0\n\tif err := session.Query(`SELECT id FROM observe WHERE id = ?`, 42).Observer(observer).Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t} else if value != 42 {\n\t\tt.Fatalf(\"value: expected %d, got %d\", 42, value)\n\t} else if observedErr != nil {\n\t\tt.Fatal(\"select:\", observedErr)\n\t} else if observedKeyspace != keyspace {\n\t\tt.Fatal(\"select: unexpected observed keyspace\", observedKeyspace)\n\t} else if observedStmt != `SELECT id FROM observe WHERE id = ?` {\n\t\tt.Fatal(\"select: unexpected observed stmt\", observedStmt)\n\t}\n\n\t// also works from session observer\n\tresetObserved()\n\toSession := createSession(t, func(config *ClusterConfig) { config.QueryObserver = observer })\n\tif err := oSession.Query(`SELECT id FROM observe WHERE id = ?`, 42).Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t} else if observedErr != nil {\n\t\tt.Fatal(\"select:\", err)\n\t} else if observedKeyspace != keyspace {\n\t\tt.Fatal(\"select: unexpected observed keyspace\", observedKeyspace)\n\t} else if observedStmt != `SELECT id FROM observe WHERE id = ?` {\n\t\tt.Fatal(\"select: unexpected observed stmt\", observedStmt)\n\t}\n\n\t// reports errors when the query is poorly formed\n\tresetObserved()\n\tvalue = 0\n\tif err := session.Query(`SELECT id FROM unknown_table WHERE id = ?`, 42).Observer(observer).Scan(&value); err == nil {\n\t\tt.Fatal(\"select: expecting error\")\n\t} else if observedErr == nil {\n\t\tt.Fatal(\"select: expecting observed error\")\n\t} else if observedKeyspace != keyspace {\n\t\tt.Fatal(\"select: unexpected observed keyspace\", observedKeyspace)\n\t} else if observedStmt != `SELECT id FROM unknown_table WHERE id = ?` {\n\t\tt.Fatal(\"select: unexpected observed stmt\", observedStmt)\n\t}\n}\n\nfunc TestObserve_Pagination(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.observe2 (id int, PRIMARY KEY (id))`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tvar observedRows int\n\n\tresetObserved := func() {\n\t\tobservedRows = -1\n\t}\n\n\tobserver := funcQueryObserver(func(ctx context.Context, o ObservedQuery) {\n\t\tobservedRows = o.Rows\n\t})\n\n\t// insert 100 entries, relevant for pagination\n\tfor i := 0; i < 50; i++ {\n\t\tif err := session.Query(`INSERT INTO observe2 (id) VALUES (?)`, i).Exec(); err != nil {\n\t\t\tt.Fatal(\"insert:\", err)\n\t\t}\n\t}\n\n\tresetObserved()\n\n\t// read the 100 entries in paginated entries of size 10. Expecting 5 observations, each with 10 rows\n\tscanner := session.Query(`SELECT id FROM observe2 LIMIT 100`).\n\t\tObserver(observer).\n\t\tPageSize(10).\n\t\tIter().Scanner()\n\tfor i := 0; i < 50; i++ {\n\t\tif !scanner.Next() {\n\t\t\tt.Fatalf(\"next: should still be true: %d: %v\", i, scanner.Err())\n\t\t}\n\t\tif i%10 == 0 {\n\t\t\tif observedRows != 10 {\n\t\t\t\tt.Fatalf(\"next: expecting a paginated query with 10 entries, got: %d (%d)\", observedRows, i)\n\t\t\t}\n\t\t} else if observedRows != -1 {\n\t\t\tt.Fatalf(\"next: not expecting paginated query (-1 entries), got: %d\", observedRows)\n\t\t}\n\n\t\tresetObserved()\n\t}\n\n\tif scanner.Next() {\n\t\tt.Fatal(\"next: no more entries where expected\")\n\t}\n}\n\nfunc TestPaging(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"Paging not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.paging (id int primary key)\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tfor i := 0; i < 100; i++ {\n\t\tif err := session.Query(\"INSERT INTO paging (id) VALUES (?)\", i).Exec(); err != nil {\n\t\t\tt.Fatal(\"insert:\", err)\n\t\t}\n\t}\n\n\titer := session.Query(\"SELECT id FROM paging\").PageSize(10).Iter()\n\tvar id int\n\tcount := 0\n\tfor iter.Scan(&id) {\n\t\tcount++\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(\"close:\", err)\n\t}\n\tif count != 100 {\n\t\tt.Fatalf(\"expected %d, got %d\", 100, count)\n\t}\n}\n\nfunc TestPagingWithBind(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"Paging not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.paging_bind (id int, val int, primary key(id,val))\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tfor i := 0; i < 100; i++ {\n\t\tif err := session.Query(\"INSERT INTO paging_bind (id,val) VALUES (?,?)\", 1, i).Exec(); err != nil {\n\t\t\tt.Fatal(\"insert:\", err)\n\t\t}\n\t}\n\n\tq := session.Query(\"SELECT val FROM paging_bind WHERE id = ? AND val < ?\", 1, 50).PageSize(10)\n\titer := q.Iter()\n\tvar id int\n\tcount := 0\n\tfor iter.Scan(&id) {\n\t\tcount++\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(\"close:\", err)\n\t}\n\tif count != 50 {\n\t\tt.Fatalf(\"expected %d, got %d\", 50, count)\n\t}\n\n\titer = q.Bind(1, 20).Iter()\n\tcount = 0\n\tfor iter.Scan(&id) {\n\t\tcount++\n\t}\n\tif count != 20 {\n\t\tt.Fatalf(\"expected %d, got %d\", 20, count)\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(\"close:\", err)\n\t}\n}\n\nfunc TestCAS(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.SerialConsistency = LocalSerial\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"lightweight transactions not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.cas_table (\n\t\t\ttitle         varchar,\n\t\t\trevid   \t  timeuuid,\n\t\t\tlast_modified timestamp,\n\t\t\tPRIMARY KEY (title, revid)\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\ttitle, revid, modified := \"baz\", TimeUUID(), time.Now()\n\tvar titleCAS string\n\tvar revidCAS UUID\n\tvar modifiedCAS time.Time\n\n\tif applied, err := session.Query(`INSERT INTO cas_table (title, revid, last_modified)\n\t\tVALUES (?, ?, ?) IF NOT EXISTS`,\n\t\ttitle, revid, modified).ScanCAS(&titleCAS, &revidCAS, &modifiedCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if !applied {\n\t\tt.Fatal(\"insert should have been applied\")\n\t}\n\n\tif applied, err := session.Query(`INSERT INTO cas_table (title, revid, last_modified)\n\t\tVALUES (?, ?, ?) IF NOT EXISTS`,\n\t\ttitle, revid, modified).ScanCAS(&titleCAS, &revidCAS, &modifiedCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatal(\"insert should not have been applied\")\n\t} else if title != titleCAS || revid != revidCAS {\n\t\tt.Fatalf(\"expected %s/%v/%v but got %s/%v/%v\", title, revid, modified, titleCAS, revidCAS, modifiedCAS)\n\t}\n\n\ttenSecondsLater := modified.Add(10 * time.Second)\n\n\tif applied, err := session.Query(`DELETE FROM cas_table WHERE title = ? and revid = ? IF last_modified = ?`,\n\t\ttitle, revid, tenSecondsLater).ScanCAS(&modifiedCAS); err != nil {\n\t\tt.Fatal(\"delete:\", err)\n\t} else if applied {\n\t\tt.Fatal(\"delete should have not been applied\")\n\t}\n\n\tif modifiedCAS.Unix() != tenSecondsLater.Add(-10*time.Second).Unix() {\n\t\tt.Fatalf(\"Was expecting modified CAS to be %v; but was one second later\", modifiedCAS.UTC())\n\t}\n\n\tif _, err := session.Query(`DELETE FROM cas_table WHERE title = ? and revid = ? IF last_modified = ?`,\n\t\ttitle, revid, tenSecondsLater).ScanCAS(); !strings.HasPrefix(err.Error(), \"gocql: not enough columns to scan into\") {\n\t\tt.Fatalf(\"delete: was expecting count mismatch error but got: %q\", err.Error())\n\t}\n\n\tif applied, err := session.Query(`DELETE FROM cas_table WHERE title = ? and revid = ? IF last_modified = ?`,\n\t\ttitle, revid, modified).ScanCAS(&modifiedCAS); err != nil {\n\t\tt.Fatal(\"delete:\", err)\n\t} else if !applied {\n\t\tt.Fatal(\"delete should have been applied\")\n\t}\n\n\tif err := session.Query(`TRUNCATE cas_table`).Exec(); err != nil {\n\t\tt.Fatal(\"truncate:\", err)\n\t}\n\n\tsuccessBatch := session.Batch(LoggedBatch)\n\tsuccessBatch.Query(\"INSERT INTO cas_table (title, revid, last_modified) VALUES (?, ?, ?) IF NOT EXISTS\", title, revid, modified)\n\tif applied, _, err := session.ExecuteBatchCAS(successBatch, &titleCAS, &revidCAS, &modifiedCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if !applied {\n\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", titleCAS, revidCAS, modifiedCAS)\n\t}\n\n\tsuccessBatch = session.Batch(LoggedBatch)\n\tsuccessBatch.Query(\"INSERT INTO cas_table (title, revid, last_modified) VALUES (?, ?, ?) IF NOT EXISTS\", title+\"_foo\", revid, modified)\n\tcasMap := make(map[string]interface{})\n\tif applied, _, err := session.MapExecuteBatchCAS(successBatch, casMap); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if !applied {\n\t\tt.Fatal(\"insert should have been applied\")\n\t}\n\n\tfailBatch := session.Batch(LoggedBatch)\n\tfailBatch.Query(\"INSERT INTO cas_table (title, revid, last_modified) VALUES (?, ?, ?) IF NOT EXISTS\", title, revid, modified)\n\tif applied, _, err := session.ExecuteBatchCAS(successBatch, &titleCAS, &revidCAS, &modifiedCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", titleCAS, revidCAS, modifiedCAS)\n\t}\n\n\tinsertBatch := session.Batch(LoggedBatch)\n\tinsertBatch.Query(\"INSERT INTO cas_table (title, revid, last_modified) VALUES ('_foo', 2c3af400-73a4-11e5-9381-29463d90c3f0, DATEOF(NOW()))\")\n\tinsertBatch.Query(\"INSERT INTO cas_table (title, revid, last_modified) VALUES ('_foo', 3e4ad2f1-73a4-11e5-9381-29463d90c3f0, DATEOF(NOW()))\")\n\tif err := session.ExecuteBatch(insertBatch); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\n\tfailBatch = session.Batch(LoggedBatch)\n\tfailBatch.Query(\"UPDATE cas_table SET last_modified = DATEOF(NOW()) WHERE title='_foo' AND revid=2c3af400-73a4-11e5-9381-29463d90c3f0 IF last_modified=DATEOF(NOW());\")\n\tfailBatch.Query(\"UPDATE cas_table SET last_modified = DATEOF(NOW()) WHERE title='_foo' AND revid=3e4ad2f1-73a4-11e5-9381-29463d90c3f0 IF last_modified=DATEOF(NOW());\")\n\tif applied, iter, err := session.ExecuteBatchCAS(failBatch, &titleCAS, &revidCAS, &modifiedCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", titleCAS, revidCAS, modifiedCAS)\n\t} else {\n\t\tif scan := iter.Scan(&applied, &titleCAS, &revidCAS, &modifiedCAS); scan && applied {\n\t\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", titleCAS, revidCAS, modifiedCAS)\n\t\t} else if !scan {\n\t\t\tt.Fatal(\"should have scanned another row\")\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tt.Fatal(\"scan:\", err)\n\t\t}\n\t}\n}\n\nfunc TestDurationType(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < 5 {\n\t\tt.Skip(\"Duration type is not supported. Please use protocol version >= 4 and cassandra version >= 3.11\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.duration_table (\n\t\tk int primary key, v duration\n\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tdurations := []Duration{\n\t\tDuration{\n\t\t\tMonths:      250,\n\t\t\tDays:        500,\n\t\t\tNanoseconds: 300010001,\n\t\t},\n\t\tDuration{\n\t\t\tMonths:      -250,\n\t\t\tDays:        -500,\n\t\t\tNanoseconds: -300010001,\n\t\t},\n\t\tDuration{\n\t\t\tMonths:      0,\n\t\t\tDays:        128,\n\t\t\tNanoseconds: 127,\n\t\t},\n\t\tDuration{\n\t\t\tMonths:      0x7FFFFFFF,\n\t\t\tDays:        0x7FFFFFFF,\n\t\t\tNanoseconds: 0x7FFFFFFFFFFFFFFF,\n\t\t},\n\t}\n\tfor _, durationSend := range durations {\n\t\tif err := session.Query(`INSERT INTO gocql_test.duration_table (k, v) VALUES (1, ?)`, durationSend).Exec(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tvar id int\n\t\tvar duration Duration\n\t\tif err := session.Query(`SELECT k, v FROM gocql_test.duration_table`).Scan(&id, &duration); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif duration.Months != durationSend.Months || duration.Days != durationSend.Days || duration.Nanoseconds != durationSend.Nanoseconds {\n\t\t\tt.Fatalf(\"Unexpeted value returned, expected=%v, received=%v\", durationSend, duration)\n\t\t}\n\t}\n}\n\nfunc TestMapScanCAS(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"lightweight transactions not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.cas_table2 (\n\t\t\ttitle         varchar,\n\t\t\trevid   \t  timeuuid,\n\t\t\tlast_modified timestamp,\n\t\t\tdeleted boolean,\n\t\t\tPRIMARY KEY (title, revid)\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\ttitle, revid, modified, deleted := \"baz\", TimeUUID(), time.Now(), false\n\tmapCAS := map[string]interface{}{}\n\n\tif applied, err := session.Query(`INSERT INTO cas_table2 (title, revid, last_modified, deleted)\n\t\tVALUES (?, ?, ?, ?) IF NOT EXISTS`,\n\t\ttitle, revid, modified, deleted).MapScanCAS(mapCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if !applied {\n\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", title, revid, modified)\n\t}\n\n\tmapCAS = map[string]interface{}{}\n\tif applied, err := session.Query(`INSERT INTO cas_table2 (title, revid, last_modified, deleted)\n\t\tVALUES (?, ?, ?, ?) IF NOT EXISTS`,\n\t\ttitle, revid, modified, deleted).MapScanCAS(mapCAS); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatalf(\"insert should have been applied: title=%v revID=%v modified=%v\", title, revid, modified)\n\t} else if title != mapCAS[\"title\"] || revid != mapCAS[\"revid\"] || deleted != mapCAS[\"deleted\"] {\n\t\tt.Fatalf(\"expected %s/%v/%v/%v but got %s/%v/%v%v\", title, revid, modified, false, mapCAS[\"title\"], mapCAS[\"revid\"], mapCAS[\"last_modified\"], mapCAS[\"deleted\"])\n\t}\n\n}\n\nfunc TestBatch(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_table (id int primary key)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tbatch := session.Batch(LoggedBatch)\n\tfor i := 0; i < 100; i++ {\n\t\tbatch.Query(`INSERT INTO batch_table (id) VALUES (?)`, i)\n\t}\n\n\tif err := session.ExecuteBatch(batch); err != nil {\n\t\tt.Fatal(\"execute batch:\", err)\n\t}\n\n\tcount := 0\n\tif err := session.Query(`SELECT COUNT(*) FROM batch_table`).Scan(&count); err != nil {\n\t\tt.Fatal(\"select count:\", err)\n\t} else if count != 100 {\n\t\tt.Fatalf(\"count: expected %d, got %d\\n\", 100, count)\n\t}\n}\n\nfunc TestUnpreparedBatch(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_unprepared (id int primary key, c counter)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tvar batch *Batch\n\tif session.cfg.ProtoVersion == 2 {\n\t\tbatch = session.Batch(CounterBatch)\n\t} else {\n\t\tbatch = session.Batch(UnloggedBatch)\n\t}\n\n\tfor i := 0; i < 100; i++ {\n\t\tbatch.Query(`UPDATE batch_unprepared SET c = c + 1 WHERE id = 1`)\n\t}\n\n\tif err := session.ExecuteBatch(batch); err != nil {\n\t\tt.Fatal(\"execute batch:\", err)\n\t}\n\n\tcount := 0\n\tif err := session.Query(`SELECT COUNT(*) FROM batch_unprepared`).Scan(&count); err != nil {\n\t\tt.Fatal(\"select count:\", err)\n\t} else if count != 1 {\n\t\tt.Fatalf(\"count: expected %d, got %d\\n\", 100, count)\n\t}\n\n\tif err := session.Query(`SELECT c FROM batch_unprepared`).Scan(&count); err != nil {\n\t\tt.Fatal(\"select count:\", err)\n\t} else if count != 100 {\n\t\tt.Fatalf(\"count: expected %d, got %d\\n\", 100, count)\n\t}\n}\n\n// TestBatchLimit tests gocql to make sure batch operations larger than the maximum\n// statement limit are not submitted to a cassandra node.\nfunc TestBatchLimit(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_table2 (id int primary key)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tbatch := session.Batch(LoggedBatch)\n\tfor i := 0; i < 65537; i++ {\n\t\tbatch.Query(`INSERT INTO batch_table2 (id) VALUES (?)`, i)\n\t}\n\tif err := session.ExecuteBatch(batch); err != ErrTooManyStmts {\n\t\tt.Fatal(\"gocql attempted to execute a batch larger than the support limit of statements.\")\n\t}\n\n}\n\nfunc TestWhereIn(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.where_in_table (id int, cluster int, primary key (id,cluster))`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tif err := session.Query(\"INSERT INTO where_in_table (id, cluster) VALUES (?,?)\", 100, 200).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\n\titer := session.Query(\"SELECT * FROM where_in_table WHERE id = ? AND cluster IN (?)\", 100, 200).Iter()\n\tvar id, cluster int\n\tcount := 0\n\tfor iter.Scan(&id, &cluster) {\n\t\tcount++\n\t}\n\n\tif id != 100 || cluster != 200 {\n\t\tt.Fatalf(\"Was expecting id and cluster to be (100,200) but were (%d,%d)\", id, cluster)\n\t}\n}\n\n// TestTooManyQueryArgs tests to make sure the library correctly handles the application level bug\n// whereby too many query arguments are passed to a query\nfunc TestTooManyQueryArgs(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.too_many_query_args (id int primary key, value int)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\t_, err := session.Query(`SELECT * FROM too_many_query_args WHERE id = ?`, 1, 2).Iter().SliceMap()\n\n\tif err == nil {\n\t\tt.Fatal(\"'`SELECT * FROM too_many_query_args WHERE id = ?`, 1, 2' should return an error\")\n\t}\n\n\tbatch := session.Batch(UnloggedBatch)\n\tbatch.Query(\"INSERT INTO too_many_query_args (id, value) VALUES (?, ?)\", 1, 2, 3)\n\terr = session.ExecuteBatch(batch)\n\n\tif err == nil {\n\t\tt.Fatal(\"'`INSERT INTO too_many_query_args (id, value) VALUES (?, ?)`, 1, 2, 3' should return an error\")\n\t}\n\n\t// TODO: should indicate via an error code that it is an invalid arg?\n\n}\n\n// TestNotEnoughQueryArgs tests to make sure the library correctly handles the application level bug\n// whereby not enough query arguments are passed to a query\nfunc TestNotEnoughQueryArgs(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.not_enough_query_args (id int, cluster int, value int, primary key (id, cluster))`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\t_, err := session.Query(`SELECT * FROM not_enough_query_args WHERE id = ? and cluster = ?`, 1).Iter().SliceMap()\n\n\tif err == nil {\n\t\tt.Fatal(\"'`SELECT * FROM not_enough_query_args WHERE id = ? and cluster = ?`, 1' should return an error\")\n\t}\n\n\tbatch := session.Batch(UnloggedBatch)\n\tbatch.Query(\"INSERT INTO not_enough_query_args (id, cluster, value) VALUES (?, ?, ?)\", 1, 2)\n\terr = session.ExecuteBatch(batch)\n\n\tif err == nil {\n\t\tt.Fatal(\"'`INSERT INTO not_enough_query_args (id, cluster, value) VALUES (?, ?, ?)`, 1, 2' should return an error\")\n\t}\n}\n\n// TestCreateSessionTimeout tests to make sure the CreateSession function timeouts out correctly\n// and prevents an infinite loop of connection retries.\nfunc TestCreateSessionTimeout(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tgo func() {\n\t\tselect {\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tt.Error(\"no startup timeout\")\n\t\tcase <-ctx.Done():\n\t\t}\n\t}()\n\n\tcluster := createCluster()\n\tcluster.Hosts = []string{\"127.0.0.1:1\"}\n\tsession, err := cluster.CreateSession()\n\tif err == nil {\n\t\tsession.Close()\n\t\tt.Fatal(\"expected ErrNoConnectionsStarted, but no error was returned.\")\n\t}\n}\n\nfunc TestReconnection(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.ReconnectInterval = 1 * time.Second\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\th := session.ring.allHosts()[0]\n\tsession.handleNodeDown(h.ConnectAddress(), h.Port())\n\n\tif h.State() != NodeDown {\n\t\tt.Fatal(\"Host should be NodeDown but not.\")\n\t}\n\n\ttime.Sleep(cluster.ReconnectInterval + h.Version().nodeUpDelay() + 1*time.Second)\n\n\tif h.State() != NodeUp {\n\t\tt.Fatal(\"Host should be NodeUp but not. Failed to reconnect.\")\n\t}\n}\n\ntype FullName struct {\n\tFirstName string\n\tLastName  string\n}\n\nfunc (n FullName) MarshalCQL(info TypeInfo) ([]byte, error) {\n\treturn []byte(n.FirstName + \" \" + n.LastName), nil\n}\n\nfunc (n *FullName) UnmarshalCQL(info TypeInfo, data []byte) error {\n\tt := strings.SplitN(string(data), \" \", 2)\n\tn.FirstName, n.LastName = t[0], t[1]\n\treturn nil\n}\n\nfunc TestMapScanWithRefMap(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif err := createTable(session, `CREATE TABLE gocql_test.scan_map_ref_table (\n\t\t\ttesttext       text PRIMARY KEY,\n\t\t\ttestfullname   text,\n\t\t\ttestint        int,\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tm := make(map[string]interface{})\n\tm[\"testtext\"] = \"testtext\"\n\tm[\"testfullname\"] = FullName{\"John\", \"Doe\"}\n\tm[\"testint\"] = 100\n\n\tif err := session.Query(`INSERT INTO scan_map_ref_table (testtext, testfullname, testint) values (?,?,?)`,\n\t\tm[\"testtext\"], m[\"testfullname\"], m[\"testint\"]).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\n\tvar testText string\n\tvar testFullName FullName\n\tret := map[string]interface{}{\n\t\t\"testtext\":     &testText,\n\t\t\"testfullname\": &testFullName,\n\t\t// testint is not set here.\n\t}\n\titer := session.Query(`SELECT * FROM scan_map_ref_table`).Iter()\n\tif ok := iter.MapScan(ret); !ok {\n\t\tt.Fatal(\"select:\", iter.Close())\n\t} else {\n\t\tif ret[\"testtext\"] != \"testtext\" {\n\t\t\tt.Fatal(\"returned testtext did not match\")\n\t\t}\n\t\tf := ret[\"testfullname\"].(FullName)\n\t\tif f.FirstName != \"John\" || f.LastName != \"Doe\" {\n\t\t\tt.Fatal(\"returned testfullname did not match\")\n\t\t}\n\t\tif ret[\"testint\"] != 100 {\n\t\t\tt.Fatal(\"returned testinit did not match\")\n\t\t}\n\t}\n\tif testText != \"testtext\" {\n\t\tt.Fatal(\"returned testtext did not match\")\n\t}\n\tif testFullName.FirstName != \"John\" || testFullName.LastName != \"Doe\" {\n\t\tt.Fatal(\"returned testfullname did not match\")\n\t}\n\n\t// using MapScan to read a nil int value\n\tintp := new(int64)\n\tret = map[string]interface{}{\n\t\t\"testint\": &intp,\n\t}\n\tif err := session.Query(\"INSERT INTO scan_map_ref_table(testtext, testint) VALUES(?, ?)\", \"null-int\", nil).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr := session.Query(`SELECT testint FROM scan_map_ref_table WHERE testtext = ?`, \"null-int\").MapScan(ret)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if v := ret[\"testint\"].(*int64); v != nil {\n\t\tt.Fatalf(\"testint should be nil got %+#v\", v)\n\t}\n\n}\n\nfunc TestMapScan(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif err := createTable(session, `CREATE TABLE gocql_test.scan_map_table (\n\t\t\tfullname       text PRIMARY KEY,\n\t\t\tage            int,\n\t\t\taddress        inet,\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tif err := session.Query(`INSERT INTO scan_map_table (fullname, age, address) values (?,?,?)`,\n\t\t\"Grace Hopper\", 31, net.ParseIP(\"10.0.0.1\")).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\tif err := session.Query(`INSERT INTO scan_map_table (fullname, age, address) values (?,?,?)`,\n\t\t\"Ada Lovelace\", 30, net.ParseIP(\"10.0.0.2\")).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\n\titer := session.Query(`SELECT * FROM scan_map_table`).Iter()\n\n\t// First iteration\n\trow := make(map[string]interface{})\n\tif !iter.MapScan(row) {\n\t\tt.Fatal(\"select:\", iter.Close())\n\t}\n\tassertEqual(t, \"fullname\", \"Ada Lovelace\", row[\"fullname\"])\n\tassertEqual(t, \"age\", 30, row[\"age\"])\n\tassertEqual(t, \"address\", \"10.0.0.2\", row[\"address\"])\n\n\t// Second iteration using a new map\n\trow = make(map[string]interface{})\n\tif !iter.MapScan(row) {\n\t\tt.Fatal(\"select:\", iter.Close())\n\t}\n\tassertEqual(t, \"fullname\", \"Grace Hopper\", row[\"fullname\"])\n\tassertEqual(t, \"age\", 31, row[\"age\"])\n\tassertEqual(t, \"address\", \"10.0.0.1\", row[\"address\"])\n}\n\nfunc TestSliceMap(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif err := createTable(session, `CREATE TABLE gocql_test.slice_map_table (\n\t\t\ttestuuid       timeuuid PRIMARY KEY,\n\t\t\ttesttimestamp  timestamp,\n\t\t\ttestvarchar    varchar,\n\t\t\ttestbigint     bigint,\n\t\t\ttestblob       blob,\n\t\t\ttestbool       boolean,\n\t\t\ttestfloat      float,\n\t\t\ttestdouble     double,\n\t\t\ttestint        int,\n\t\t\ttestdecimal    decimal,\n\t\t\ttestlist       list<text>,\n\t\t\ttestset        set<int>,\n\t\t\ttestmap        map<varchar, varchar>,\n\t\t\ttestvarint     varint,\n\t\t\ttestinet\t\t\t inet\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tm := make(map[string]interface{})\n\n\tbigInt := new(big.Int)\n\tif _, ok := bigInt.SetString(\"830169365738487321165427203929228\", 10); !ok {\n\t\tt.Fatal(\"Failed setting bigint by string\")\n\t}\n\n\tm[\"testuuid\"] = TimeUUID()\n\tm[\"testvarchar\"] = \"Test VarChar\"\n\tm[\"testbigint\"] = time.Now().Unix()\n\tm[\"testtimestamp\"] = time.Now().Truncate(time.Millisecond).UTC()\n\tm[\"testblob\"] = []byte(\"test blob\")\n\tm[\"testbool\"] = true\n\tm[\"testfloat\"] = float32(4.564)\n\tm[\"testdouble\"] = float64(4.815162342)\n\tm[\"testint\"] = 2343\n\tm[\"testdecimal\"] = inf.NewDec(100, 0)\n\tm[\"testlist\"] = []string{\"quux\", \"foo\", \"bar\", \"baz\", \"quux\"}\n\tm[\"testset\"] = []int{1, 2, 3, 4, 5, 6, 7, 8, 9}\n\tm[\"testmap\"] = map[string]string{\"field1\": \"val1\", \"field2\": \"val2\", \"field3\": \"val3\"}\n\tm[\"testvarint\"] = bigInt\n\tm[\"testinet\"] = \"213.212.2.19\"\n\tsliceMap := []map[string]interface{}{m}\n\tif err := session.Query(`INSERT INTO slice_map_table (testuuid, testtimestamp, testvarchar, testbigint, testblob, testbool, testfloat, testdouble, testint, testdecimal, testlist, testset, testmap, testvarint, testinet) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,\n\t\tm[\"testuuid\"], m[\"testtimestamp\"], m[\"testvarchar\"], m[\"testbigint\"], m[\"testblob\"], m[\"testbool\"], m[\"testfloat\"], m[\"testdouble\"], m[\"testint\"], m[\"testdecimal\"], m[\"testlist\"], m[\"testset\"], m[\"testmap\"], m[\"testvarint\"], m[\"testinet\"]).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\tif returned, retErr := session.Query(`SELECT * FROM slice_map_table`).Iter().SliceMap(); retErr != nil {\n\t\tt.Fatal(\"select:\", retErr)\n\t} else {\n\t\tmatchSliceMap(t, sliceMap, returned[0])\n\t}\n\n\t// Test for Iter.MapScan()\n\t{\n\t\ttestMap := make(map[string]interface{})\n\t\tif !session.Query(`SELECT * FROM slice_map_table`).Iter().MapScan(testMap) {\n\t\t\tt.Fatal(\"MapScan failed to work with one row\")\n\t\t}\n\t\tmatchSliceMap(t, sliceMap, testMap)\n\t}\n\n\t// Test for Query.MapScan()\n\t{\n\t\ttestMap := make(map[string]interface{})\n\t\tif session.Query(`SELECT * FROM slice_map_table`).MapScan(testMap) != nil {\n\t\t\tt.Fatal(\"MapScan failed to work with one row\")\n\t\t}\n\t\tmatchSliceMap(t, sliceMap, testMap)\n\t}\n}\nfunc matchSliceMap(t *testing.T, sliceMap []map[string]interface{}, testMap map[string]interface{}) {\n\tif sliceMap[0][\"testuuid\"] != testMap[\"testuuid\"] {\n\t\tt.Fatal(\"returned testuuid did not match\")\n\t}\n\tif sliceMap[0][\"testtimestamp\"] != testMap[\"testtimestamp\"] {\n\t\tt.Fatal(\"returned testtimestamp did not match\")\n\t}\n\tif sliceMap[0][\"testvarchar\"] != testMap[\"testvarchar\"] {\n\t\tt.Fatal(\"returned testvarchar did not match\")\n\t}\n\tif sliceMap[0][\"testbigint\"] != testMap[\"testbigint\"] {\n\t\tt.Fatal(\"returned testbigint did not match\")\n\t}\n\tif !reflect.DeepEqual(sliceMap[0][\"testblob\"], testMap[\"testblob\"]) {\n\t\tt.Fatal(\"returned testblob did not match\")\n\t}\n\tif sliceMap[0][\"testbool\"] != testMap[\"testbool\"] {\n\t\tt.Fatal(\"returned testbool did not match\")\n\t}\n\tif sliceMap[0][\"testfloat\"] != testMap[\"testfloat\"] {\n\t\tt.Fatal(\"returned testfloat did not match\")\n\t}\n\tif sliceMap[0][\"testdouble\"] != testMap[\"testdouble\"] {\n\t\tt.Fatal(\"returned testdouble did not match\")\n\t}\n\tif sliceMap[0][\"testinet\"] != testMap[\"testinet\"] {\n\t\tt.Fatal(\"returned testinet did not match\")\n\t}\n\n\texpectedDecimal := sliceMap[0][\"testdecimal\"].(*inf.Dec)\n\treturnedDecimal := testMap[\"testdecimal\"].(*inf.Dec)\n\n\tif expectedDecimal.Cmp(returnedDecimal) != 0 {\n\t\tt.Fatal(\"returned testdecimal did not match\")\n\t}\n\n\tif !reflect.DeepEqual(sliceMap[0][\"testlist\"], testMap[\"testlist\"]) {\n\t\tt.Fatal(\"returned testlist did not match\")\n\t}\n\tif !reflect.DeepEqual(sliceMap[0][\"testset\"], testMap[\"testset\"]) {\n\t\tt.Fatal(\"returned testset did not match\")\n\t}\n\tif !reflect.DeepEqual(sliceMap[0][\"testmap\"], testMap[\"testmap\"]) {\n\t\tt.Fatal(\"returned testmap did not match\")\n\t}\n\tif sliceMap[0][\"testint\"] != testMap[\"testint\"] {\n\t\tt.Fatal(\"returned testint did not match\")\n\t}\n}\n\ntype MyRetryPolicy struct {\n}\n\nfunc (*MyRetryPolicy) Attempt(q RetryableQuery) bool {\n\tif q.Attempts() > 5 {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (*MyRetryPolicy) GetRetryType(error) RetryType {\n\treturn Retry\n}\n\nfunc Test_RetryPolicyIdempotence(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\ttestCases := []struct {\n\t\tname                  string\n\t\tidempotency           bool\n\t\texpectedNumberOfTries int\n\t}{\n\t\t{\n\t\t\tname:                  \"with retry\",\n\t\t\tidempotency:           true,\n\t\t\texpectedNumberOfTries: 6,\n\t\t},\n\t\t{\n\t\t\tname:                  \"without retry\",\n\t\t\tidempotency:           false,\n\t\t\texpectedNumberOfTries: 1,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tq := session.Query(\"INSERT INTO  gocql_test.not_existing_table(event_id, time, args) VALUES (?,?,?)\", 4, UUIDFromTime(time.Now()), \"test\")\n\n\t\t\tq.Idempotent(tc.idempotency)\n\t\t\tq.RetryPolicy(&MyRetryPolicy{})\n\t\t\tq.Consistency(All)\n\n\t\t\t_ = q.Exec()\n\t\t\trequire.Equal(t, tc.expectedNumberOfTries, q.Attempts())\n\t\t})\n\t}\n}\n\nfunc TestSmallInt(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion4 {\n\t\tt.Skip(\"smallint is only supported in cassandra 2.2+\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.smallint_table (\n\t\t\ttestsmallint  smallint PRIMARY KEY,\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tm := make(map[string]interface{})\n\tm[\"testsmallint\"] = int16(2)\n\tsliceMap := []map[string]interface{}{m}\n\tif err := session.Query(`INSERT INTO smallint_table (testsmallint) VALUES (?)`,\n\t\tm[\"testsmallint\"]).Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\tif returned, retErr := session.Query(`SELECT * FROM smallint_table`).Iter().SliceMap(); retErr != nil {\n\t\tt.Fatal(\"select:\", retErr)\n\t} else {\n\t\tif sliceMap[0][\"testsmallint\"] != returned[0][\"testsmallint\"] {\n\t\t\tt.Fatal(\"returned testsmallint did not match\")\n\t\t}\n\t}\n}\n\nfunc TestScanWithNilArguments(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.scan_with_nil_arguments (\n\t\t\tfoo   varchar,\n\t\t\tbar   int,\n\t\t\tPRIMARY KEY (foo, bar)\n\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\tfor i := 1; i <= 20; i++ {\n\t\tif err := session.Query(\"INSERT INTO scan_with_nil_arguments (foo, bar) VALUES (?, ?)\",\n\t\t\t\"squares\", i*i).Exec(); err != nil {\n\t\t\tt.Fatal(\"insert:\", err)\n\t\t}\n\t}\n\n\titer := session.Query(\"SELECT * FROM scan_with_nil_arguments WHERE foo = ?\", \"squares\").Iter()\n\tvar n int\n\tcount := 0\n\tfor iter.Scan(nil, &n) {\n\t\tcount += n\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(\"close:\", err)\n\t}\n\tif count != 2870 {\n\t\tt.Fatalf(\"expected %d, got %d\", 2870, count)\n\t}\n}\n\nfunc TestScanCASWithNilArguments(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"lightweight transactions not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.scan_cas_with_nil_arguments (\n\t\tfoo   varchar,\n\t\tbar   varchar,\n\t\tPRIMARY KEY (foo, bar)\n\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tfoo := \"baz\"\n\tvar cas string\n\n\tif applied, err := session.Query(`INSERT INTO scan_cas_with_nil_arguments (foo, bar)\n\t\tVALUES (?, ?) IF NOT EXISTS`,\n\t\tfoo, foo).ScanCAS(nil, nil); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if !applied {\n\t\tt.Fatal(\"insert should have been applied\")\n\t}\n\n\tif applied, err := session.Query(`INSERT INTO scan_cas_with_nil_arguments (foo, bar)\n\t\tVALUES (?, ?) IF NOT EXISTS`,\n\t\tfoo, foo).ScanCAS(&cas, nil); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatal(\"insert should not have been applied\")\n\t} else if foo != cas {\n\t\tt.Fatalf(\"expected %v but got %v\", foo, cas)\n\t}\n\n\tif applied, err := session.Query(`INSERT INTO scan_cas_with_nil_arguments (foo, bar)\n\t\tVALUES (?, ?) IF NOT EXISTS`,\n\t\tfoo, foo).ScanCAS(nil, &cas); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t} else if applied {\n\t\tt.Fatal(\"insert should not have been applied\")\n\t} else if foo != cas {\n\t\tt.Fatalf(\"expected %v but got %v\", foo, cas)\n\t}\n}\n\nfunc TestRebindQueryInfo(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.rebind_query (id int, value text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tif err := session.Query(\"INSERT INTO rebind_query (id, value) VALUES (?, ?)\", 23, \"quux\").Exec(); err != nil {\n\t\tt.Fatalf(\"insert into rebind_query failed, err '%v'\", err)\n\t}\n\n\tif err := session.Query(\"INSERT INTO rebind_query (id, value) VALUES (?, ?)\", 24, \"w00t\").Exec(); err != nil {\n\t\tt.Fatalf(\"insert into rebind_query failed, err '%v'\", err)\n\t}\n\n\tq := session.Query(\"SELECT value FROM rebind_query WHERE ID = ?\")\n\tq.Bind(23)\n\n\titer := q.Iter()\n\tvar value string\n\tfor iter.Scan(&value) {\n\t}\n\n\tif value != \"quux\" {\n\t\tt.Fatalf(\"expected %v but got %v\", \"quux\", value)\n\t}\n\n\tq.Bind(24)\n\titer = q.Iter()\n\n\tfor iter.Scan(&value) {\n\t}\n\n\tif value != \"w00t\" {\n\t\tt.Fatalf(\"expected %v but got %v\", \"w00t\", value)\n\t}\n}\n\n// TestStaticQueryInfo makes sure that the application can manually bind query parameters using the simplest possible static binding strategy\nfunc TestStaticQueryInfo(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.static_query_info (id int, value text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tif err := session.Query(\"INSERT INTO static_query_info (id, value) VALUES (?, ?)\", 113, \"foo\").Exec(); err != nil {\n\t\tt.Fatalf(\"insert into static_query_info failed, err '%v'\", err)\n\t}\n\n\tautobinder := func(q *QueryInfo) ([]interface{}, error) {\n\t\tvalues := make([]interface{}, 1)\n\t\tvalues[0] = 113\n\t\treturn values, nil\n\t}\n\n\tqry := session.Bind(\"SELECT id, value FROM static_query_info WHERE id = ?\", autobinder)\n\n\tif err := qry.Exec(); err != nil {\n\t\tt.Fatalf(\"expose query info failed, error '%v'\", err)\n\t}\n\n\titer := qry.Iter()\n\n\tvar id int\n\tvar value string\n\n\titer.Scan(&id, &value)\n\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatalf(\"query with exposed info failed, err '%v'\", err)\n\t}\n\n\tif value != \"foo\" {\n\t\tt.Fatalf(\"Expected value %s, but got %s\", \"foo\", value)\n\t}\n\n}\n\ntype ClusteredKeyValue struct {\n\tId      int\n\tCluster int\n\tValue   string\n}\n\nfunc (kv *ClusteredKeyValue) Bind(q *QueryInfo) ([]interface{}, error) {\n\tvalues := make([]interface{}, len(q.Args))\n\n\tfor i, info := range q.Args {\n\t\tfieldName := upcaseInitial(info.Name)\n\t\tvalue := reflect.ValueOf(kv)\n\t\tfield := reflect.Indirect(value).FieldByName(fieldName)\n\t\tvalues[i] = field.Addr().Interface()\n\t}\n\n\treturn values, nil\n}\n\nfunc upcaseInitial(str string) string {\n\tfor i, v := range str {\n\t\treturn string(unicode.ToUpper(v)) + str[i+1:]\n\t}\n\treturn \"\"\n}\n\n// TestBoundQueryInfo makes sure that the application can manually bind query parameters using the query meta data supplied at runtime\nfunc TestBoundQueryInfo(t *testing.T) {\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.clustered_query_info (id int, cluster int, value text, PRIMARY KEY (id, cluster))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\twrite := &ClusteredKeyValue{Id: 200, Cluster: 300, Value: \"baz\"}\n\n\tinsert := session.Bind(\"INSERT INTO clustered_query_info (id, cluster, value) VALUES (?, ?,?)\", write.Bind)\n\n\tif err := insert.Exec(); err != nil {\n\t\tt.Fatalf(\"insert into clustered_query_info failed, err '%v'\", err)\n\t}\n\n\tread := &ClusteredKeyValue{Id: 200, Cluster: 300}\n\n\tqry := session.Bind(\"SELECT id, cluster, value FROM clustered_query_info WHERE id = ? and cluster = ?\", read.Bind)\n\n\titer := qry.Iter()\n\n\tvar id, cluster int\n\tvar value string\n\n\titer.Scan(&id, &cluster, &value)\n\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatalf(\"query with clustered_query_info info failed, err '%v'\", err)\n\t}\n\n\tif value != \"baz\" {\n\t\tt.Fatalf(\"Expected value %s, but got %s\", \"baz\", value)\n\t}\n\n}\n\n// TestBatchQueryInfo makes sure that the application can manually bind query parameters when executing in a batch\nfunc TestBatchQueryInfo(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.batch_query_info (id int, cluster int, value text, PRIMARY KEY (id, cluster))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\twrite := func(q *QueryInfo) ([]interface{}, error) {\n\t\tvalues := make([]interface{}, 3)\n\t\tvalues[0] = 4000\n\t\tvalues[1] = 5000\n\t\tvalues[2] = \"bar\"\n\t\treturn values, nil\n\t}\n\n\tbatch := session.Batch(LoggedBatch)\n\tbatch.Bind(\"INSERT INTO batch_query_info (id, cluster, value) VALUES (?, ?,?)\", write)\n\n\tif err := session.ExecuteBatch(batch); err != nil {\n\t\tt.Fatalf(\"batch insert into batch_query_info failed, err '%v'\", err)\n\t}\n\n\tread := func(q *QueryInfo) ([]interface{}, error) {\n\t\tvalues := make([]interface{}, 2)\n\t\tvalues[0] = 4000\n\t\tvalues[1] = 5000\n\t\treturn values, nil\n\t}\n\n\tqry := session.Bind(\"SELECT id, cluster, value FROM batch_query_info WHERE id = ? and cluster = ?\", read)\n\n\titer := qry.Iter()\n\n\tvar id, cluster int\n\tvar value string\n\n\titer.Scan(&id, &cluster, &value)\n\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatalf(\"query with batch_query_info info failed, err '%v'\", err)\n\t}\n\n\tif value != \"bar\" {\n\t\tt.Fatalf(\"Expected value %s, but got %s\", \"bar\", value)\n\t}\n}\n\nfunc getRandomConn(t *testing.T, session *Session) *Conn {\n\tconn := session.getConn()\n\tif conn == nil {\n\t\tt.Fatal(\"unable to get a connection\")\n\t}\n\treturn conn\n}\n\nfunc injectInvalidPreparedStatement(t *testing.T, session *Session, table string) (string, *Conn) {\n\tif err := createTable(session, `CREATE TABLE gocql_test.`+table+` (\n\t\t\tfoo   varchar,\n\t\t\tbar   int,\n\t\t\tPRIMARY KEY (foo, bar)\n\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tstmt := \"INSERT INTO \" + table + \" (foo, bar) VALUES (?, 7)\"\n\n\tconn := getRandomConn(t, session)\n\n\tflight := new(inflightPrepare)\n\tkey := session.stmtsLRU.keyFor(conn.host.HostID(), \"\", stmt)\n\tsession.stmtsLRU.add(key, flight)\n\n\tflight.preparedStatment = &preparedStatment{\n\t\tid: []byte{'f', 'o', 'o', 'b', 'a', 'r'},\n\t\trequest: preparedMetadata{\n\t\t\tresultMetadata: resultMetadata{\n\t\t\t\tcolCount:       1,\n\t\t\t\tactualColCount: 1,\n\t\t\t\tcolumns: []ColumnInfo{\n\t\t\t\t\t{\n\t\t\t\t\t\tKeyspace: \"gocql_test\",\n\t\t\t\t\t\tTable:    table,\n\t\t\t\t\t\tName:     \"foo\",\n\t\t\t\t\t\tTypeInfo: NativeType{\n\t\t\t\t\t\t\ttyp: TypeVarchar,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\treturn stmt, conn\n}\n\nfunc TestPrepare_MissingSchemaPrepare(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\ts := createSession(t)\n\tconn := getRandomConn(t, s)\n\tdefer s.Close()\n\n\tinsertQry := s.Query(\"INSERT INTO invalidschemaprep (val) VALUES (?)\", 5)\n\tif err := conn.executeQuery(ctx, insertQry).err; err == nil {\n\t\tt.Fatal(\"expected error, but got nil.\")\n\t}\n\n\tif err := createTable(s, \"CREATE TABLE gocql_test.invalidschemaprep (val int, PRIMARY KEY (val))\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\tif err := conn.executeQuery(ctx, insertQry).err; err != nil {\n\t\tt.Fatal(err) // unconfigured columnfamily\n\t}\n}\n\nfunc TestPrepare_ReprepareStatement(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tstmt, conn := injectInvalidPreparedStatement(t, session, \"test_reprepare_statement\")\n\tquery := session.Query(stmt, \"bar\")\n\tif err := conn.executeQuery(ctx, query).Close(); err != nil {\n\t\tt.Fatalf(\"Failed to execute query for reprepare statement: %v\", err)\n\t}\n}\n\nfunc TestPrepare_ReprepareBatch(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tstmt, conn := injectInvalidPreparedStatement(t, session, \"test_reprepare_statement_batch\")\n\tbatch := session.Batch(UnloggedBatch)\n\tbatch.Query(stmt, \"bar\")\n\tif err := conn.executeBatch(ctx, batch).Close(); err != nil {\n\t\tt.Fatalf(\"Failed to execute query for reprepare statement: %v\", err)\n\t}\n}\n\nfunc TestQueryInfo(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tconn := getRandomConn(t, session)\n\tinfo, err := conn.prepareStatement(context.Background(), \"SELECT release_version, host_id FROM system.local WHERE key = ?\", nil)\n\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to execute query for preparing statement: %v\", err)\n\t}\n\n\tif x := len(info.request.columns); x != 1 {\n\t\tt.Fatalf(\"Was not expecting meta data for %d query arguments, but got %d\\n\", 1, x)\n\t}\n\n\tif session.cfg.ProtoVersion > 1 {\n\t\tif x := len(info.response.columns); x != 2 {\n\t\t\tt.Fatalf(\"Was not expecting meta data for %d result columns, but got %d\\n\", 2, x)\n\t\t}\n\t}\n}\n\n// TestPreparedCacheEviction will make sure that the cache size is maintained\nfunc TestPrepare_PreparedCacheEviction(t *testing.T) {\n\tconst maxPrepared = 4\n\n\tclusterHosts := getClusterHosts()\n\thost := clusterHosts[0]\n\tcluster := createCluster()\n\tcluster.MaxPreparedStmts = maxPrepared\n\tcluster.Events.DisableSchemaEvents = true\n\tcluster.Hosts = []string{host}\n\n\tcluster.HostFilter = WhiteListHostFilter(host)\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.prepcachetest (id int,mod int,PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\t// clear the cache\n\tsession.stmtsLRU.clear()\n\n\t//Fill the table\n\tfor i := 0; i < 2; i++ {\n\t\tif err := session.Query(\"INSERT INTO prepcachetest (id,mod) VALUES (?, ?)\", i, 10000%(i+1)).Exec(); err != nil {\n\t\t\tt.Fatalf(\"insert into prepcachetest failed, err '%v'\", err)\n\t\t}\n\t}\n\t//Populate the prepared statement cache with select statements\n\tvar id, mod int\n\tfor i := 0; i < 2; i++ {\n\t\terr := session.Query(\"SELECT id,mod FROM prepcachetest WHERE id = \"+strconv.FormatInt(int64(i), 10)).Scan(&id, &mod)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"select from prepcachetest failed, error '%v'\", err)\n\t\t}\n\t}\n\n\t//generate an update statement to test they are prepared\n\terr := session.Query(\"UPDATE prepcachetest SET mod = ? WHERE id = ?\", 1, 11).Exec()\n\tif err != nil {\n\t\tt.Fatalf(\"update prepcachetest failed, error '%v'\", err)\n\t}\n\n\t//generate a delete statement to test they are prepared\n\terr = session.Query(\"DELETE FROM prepcachetest WHERE id = ?\", 1).Exec()\n\tif err != nil {\n\t\tt.Fatalf(\"delete from prepcachetest failed, error '%v'\", err)\n\t}\n\n\t//generate an insert statement to test they are prepared\n\terr = session.Query(\"INSERT INTO prepcachetest (id,mod) VALUES (?, ?)\", 3, 11).Exec()\n\tif err != nil {\n\t\tt.Fatalf(\"insert into prepcachetest failed, error '%v'\", err)\n\t}\n\n\tsession.stmtsLRU.mu.Lock()\n\tdefer session.stmtsLRU.mu.Unlock()\n\n\t//Make sure the cache size is maintained\n\tif session.stmtsLRU.lru.Len() != session.stmtsLRU.lru.MaxEntries {\n\t\tt.Fatalf(\"expected cache size of %v, got %v\", session.stmtsLRU.lru.MaxEntries, session.stmtsLRU.lru.Len())\n\t}\n\n\t// Walk through all the configured hosts and test cache retention and eviction\n\tfor _, host := range session.ring.hosts {\n\t\t_, ok := session.stmtsLRU.lru.Get(session.stmtsLRU.keyFor(host.HostID(), session.cfg.Keyspace, \"SELECT id,mod FROM prepcachetest WHERE id = 0\"))\n\t\tif ok {\n\t\t\tt.Errorf(\"expected first select to be purged but was in cache for host=%q\", host)\n\t\t}\n\n\t\t_, ok = session.stmtsLRU.lru.Get(session.stmtsLRU.keyFor(host.HostID(), session.cfg.Keyspace, \"SELECT id,mod FROM prepcachetest WHERE id = 1\"))\n\t\tif !ok {\n\t\t\tt.Errorf(\"exepected second select to be in cache for host=%q\", host)\n\t\t}\n\n\t\t_, ok = session.stmtsLRU.lru.Get(session.stmtsLRU.keyFor(host.HostID(), session.cfg.Keyspace, \"INSERT INTO prepcachetest (id,mod) VALUES (?, ?)\"))\n\t\tif !ok {\n\t\t\tt.Errorf(\"expected insert to be in cache for host=%q\", host)\n\t\t}\n\n\t\t_, ok = session.stmtsLRU.lru.Get(session.stmtsLRU.keyFor(host.HostID(), session.cfg.Keyspace, \"UPDATE prepcachetest SET mod = ? WHERE id = ?\"))\n\t\tif !ok {\n\t\t\tt.Errorf(\"expected update to be in cached for host=%q\", host)\n\t\t}\n\n\t\t_, ok = session.stmtsLRU.lru.Get(session.stmtsLRU.keyFor(host.HostID(), session.cfg.Keyspace, \"DELETE FROM prepcachetest WHERE id = ?\"))\n\t\tif !ok {\n\t\t\tt.Errorf(\"expected delete to be cached for host=%q\", host)\n\t\t}\n\t}\n}\n\nfunc TestPrepare_PreparedCacheKey(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\t// create a second keyspace\n\tcluster2 := createCluster()\n\tcreateKeyspace(t, cluster2, \"gocql_test2\")\n\tcluster2.Keyspace = \"gocql_test2\"\n\tsession2, err := cluster2.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(\"create session:\", err)\n\t}\n\tdefer session2.Close()\n\n\t// both keyspaces have a table named \"test_stmt_cache_key\"\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_stmt_cache_key (id varchar primary key, field varchar)\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tif err := createTable(session2, \"CREATE TABLE gocql_test2.test_stmt_cache_key (id varchar primary key, field varchar)\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\t// both tables have a single row with the same partition key but different column value\n\tif err = session.Query(`INSERT INTO test_stmt_cache_key (id, field) VALUES (?, ?)`, \"key\", \"one\").Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\tif err = session2.Query(`INSERT INTO test_stmt_cache_key (id, field) VALUES (?, ?)`, \"key\", \"two\").Exec(); err != nil {\n\t\tt.Fatal(\"insert:\", err)\n\t}\n\n\t// should be able to see different values in each keyspace\n\tvar value string\n\tif err = session.Query(\"SELECT field FROM test_stmt_cache_key WHERE id = ?\", \"key\").Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t}\n\tif value != \"one\" {\n\t\tt.Errorf(\"Expected one, got %s\", value)\n\t}\n\n\tif err = session2.Query(\"SELECT field FROM test_stmt_cache_key WHERE id = ?\", \"key\").Scan(&value); err != nil {\n\t\tt.Fatal(\"select:\", err)\n\t}\n\tif value != \"two\" {\n\t\tt.Errorf(\"Expected two, got %s\", value)\n\t}\n}\n\n// TestMarshalFloat64Ptr tests to see that a pointer to a float64 is marshalled correctly.\nfunc TestMarshalFloat64Ptr(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.float_test (id double, test double, primary key (id))\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\ttestNum := float64(7500)\n\tif err := session.Query(`INSERT INTO float_test (id,test) VALUES (?,?)`, float64(7500.00), &testNum).Exec(); err != nil {\n\t\tt.Fatal(\"insert float64:\", err)\n\t}\n}\n\n// TestMarshalInet tests to see that a pointer to a float64 is marshalled correctly.\nfunc TestMarshalInet(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.inet_test (ip inet, name text, primary key (ip))\"); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\tstringIp := \"123.34.45.56\"\n\tif err := session.Query(`INSERT INTO inet_test (ip,name) VALUES (?,?)`, stringIp, \"Test IP 1\").Exec(); err != nil {\n\t\tt.Fatal(\"insert string inet:\", err)\n\t}\n\tvar stringResult string\n\tif err := session.Query(\"SELECT ip FROM inet_test\").Scan(&stringResult); err != nil {\n\t\tt.Fatalf(\"select for string from inet_test 1 failed: %v\", err)\n\t}\n\tif stringResult != stringIp {\n\t\tt.Errorf(\"Expected %s, was %s\", stringIp, stringResult)\n\t}\n\n\tvar ipResult net.IP\n\tif err := session.Query(\"SELECT ip FROM inet_test\").Scan(&ipResult); err != nil {\n\t\tt.Fatalf(\"select for net.IP from inet_test 1 failed: %v\", err)\n\t}\n\tif ipResult.String() != stringIp {\n\t\tt.Errorf(\"Expected %s, was %s\", stringIp, ipResult.String())\n\t}\n\n\tif err := session.Query(`DELETE FROM inet_test WHERE ip = ?`, stringIp).Exec(); err != nil {\n\t\tt.Fatal(\"delete inet table:\", err)\n\t}\n\n\tnetIp := net.ParseIP(\"222.43.54.65\")\n\tif err := session.Query(`INSERT INTO inet_test (ip,name) VALUES (?,?)`, netIp, \"Test IP 2\").Exec(); err != nil {\n\t\tt.Fatal(\"insert netIp inet:\", err)\n\t}\n\n\tif err := session.Query(\"SELECT ip FROM inet_test\").Scan(&stringResult); err != nil {\n\t\tt.Fatalf(\"select for string from inet_test 2 failed: %v\", err)\n\t}\n\tif stringResult != netIp.String() {\n\t\tt.Errorf(\"Expected %s, was %s\", netIp.String(), stringResult)\n\t}\n\tif err := session.Query(\"SELECT ip FROM inet_test\").Scan(&ipResult); err != nil {\n\t\tt.Fatalf(\"select for net.IP from inet_test 2 failed: %v\", err)\n\t}\n\tif ipResult.String() != netIp.String() {\n\t\tt.Errorf(\"Expected %s, was %s\", netIp.String(), ipResult.String())\n\t}\n\n}\n\nfunc TestVarint(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.varint_test (id varchar, test varint, test2 varint, primary key (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tif err := session.Query(`INSERT INTO varint_test (id, test) VALUES (?, ?)`, \"id\", 0).Exec(); err != nil {\n\t\tt.Fatalf(\"insert varint: %v\", err)\n\t}\n\n\tvar result int\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(&result); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif result != 0 {\n\t\tt.Errorf(\"Expected 0, was %d\", result)\n\t}\n\n\tif err := session.Query(`INSERT INTO varint_test (id, test) VALUES (?, ?)`, \"id\", -1).Exec(); err != nil {\n\t\tt.Fatalf(\"insert varint: %v\", err)\n\t}\n\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(&result); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif result != -1 {\n\t\tt.Errorf(\"Expected -1, was %d\", result)\n\t}\n\n\tif err := session.Query(`INSERT INTO varint_test (id, test) VALUES (?, ?)`, \"id\", nil).Exec(); err != nil {\n\t\tt.Fatalf(\"insert varint: %v\", err)\n\t}\n\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(&result); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif result != 0 {\n\t\tt.Errorf(\"Expected 0, was %d\", result)\n\t}\n\n\tvar nullableResult *int\n\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(&nullableResult); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif nullableResult != nil {\n\t\tt.Errorf(\"Expected nil, was %d\", nullableResult)\n\t}\n\n\tif err := session.Query(`INSERT INTO varint_test (id, test) VALUES (?, ?)`, \"id\", int64(math.MaxInt32)+1).Exec(); err != nil {\n\t\tt.Fatalf(\"insert varint: %v\", err)\n\t}\n\n\tvar result64 int64\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(&result64); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif result64 != int64(math.MaxInt32)+1 {\n\t\tt.Errorf(\"Expected %d, was %d\", int64(math.MaxInt32)+1, result64)\n\t}\n\n\tbiggie := new(big.Int)\n\tbiggie.SetString(\"36893488147419103232\", 10) // > 2**64\n\tif err := session.Query(`INSERT INTO varint_test (id, test) VALUES (?, ?)`, \"id\", biggie).Exec(); err != nil {\n\t\tt.Fatalf(\"insert varint: %v\", err)\n\t}\n\n\tresultBig := new(big.Int)\n\tif err := session.Query(\"SELECT test FROM varint_test\").Scan(resultBig); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif resultBig.String() != biggie.String() {\n\t\tt.Errorf(\"Expected %s, was %s\", biggie.String(), resultBig.String())\n\t}\n\n\terr := session.Query(\"SELECT test FROM varint_test\").Scan(&result64)\n\tif err == nil || strings.Index(err.Error(), \"out of range\") == -1 {\n\t\tt.Errorf(\"expected out of range error since value is too big for int64\")\n\t}\n\n\t// value not set in cassandra, leave bind variable empty\n\tresultBig = new(big.Int)\n\tif err := session.Query(\"SELECT test2 FROM varint_test\").Scan(resultBig); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif resultBig.Int64() != 0 {\n\t\tt.Errorf(\"Expected %s, was %s\", biggie.String(), resultBig.String())\n\t}\n\n\t// can use double pointer to explicitly detect value is not set in cassandra\n\tif err := session.Query(\"SELECT test2 FROM varint_test\").Scan(&resultBig); err != nil {\n\t\tt.Fatalf(\"select from varint_test failed: %v\", err)\n\t}\n\n\tif resultBig != nil {\n\t\tt.Errorf(\"Expected %v, was %v\", nil, *resultBig)\n\t}\n}\n\n// TestQueryStats confirms that the stats are returning valid data. Accuracy may be questionable.\nfunc TestQueryStats(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tqry := session.Query(\"SELECT * FROM system.peers\")\n\tif err := qry.Exec(); err != nil {\n\t\tt.Fatalf(\"query failed. %v\", err)\n\t} else {\n\t\tif qry.Attempts() < 1 {\n\t\t\tt.Fatal(\"expected at least 1 attempt, but got 0\")\n\t\t}\n\t\tif qry.Latency() <= 0 {\n\t\t\tt.Fatalf(\"expected latency to be greater than 0, but got %v instead.\", qry.Latency())\n\t\t}\n\t}\n}\n\n// TestIterHosts confirms that host is added to Iter when the query succeeds.\nfunc TestIterHost(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\titer := session.Query(\"SELECT * FROM system.peers\").Iter()\n\n\t// check if Host method works\n\tif iter.Host() == nil {\n\t\tt.Error(\"No host in iter\")\n\t}\n}\n\n// TestBatchStats confirms that the stats are returning valid data. Accuracy may be questionable.\nfunc TestBatchStats(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.batchStats (id int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tb := session.Batch(LoggedBatch)\n\tb.Query(\"INSERT INTO batchStats (id) VALUES (?)\", 1)\n\tb.Query(\"INSERT INTO batchStats (id) VALUES (?)\", 2)\n\n\tif err := session.ExecuteBatch(b); err != nil {\n\t\tt.Fatalf(\"query failed. %v\", err)\n\t} else {\n\t\tif b.Attempts() < 1 {\n\t\t\tt.Fatal(\"expected at least 1 attempt, but got 0\")\n\t\t}\n\t\tif b.Latency() <= 0 {\n\t\t\tt.Fatalf(\"expected latency to be greater than 0, but got %v instead.\", b.Latency())\n\t\t}\n\t}\n}\n\ntype funcBatchObserver func(context.Context, ObservedBatch)\n\nfunc (f funcBatchObserver) ObserveBatch(ctx context.Context, o ObservedBatch) {\n\tf(ctx, o)\n}\n\nfunc TestBatchObserve(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\tt.Skip(\"atomic batches not supported. Please use Cassandra >= 2.0\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.batch_observe_table (id int, other int, PRIMARY KEY (id))`); err != nil {\n\t\tt.Fatal(\"create table:\", err)\n\t}\n\n\ttype observation struct {\n\t\tobservedErr      error\n\t\tobservedKeyspace string\n\t\tobservedStmts    []string\n\t\tobservedValues   [][]interface{}\n\t}\n\n\tvar observedBatch *observation\n\n\tbatch := session.Batch(LoggedBatch)\n\tbatch.Observer(funcBatchObserver(func(ctx context.Context, o ObservedBatch) {\n\t\tif observedBatch != nil {\n\t\t\tt.Fatal(\"batch observe called more than once\")\n\t\t}\n\n\t\tobservedBatch = &observation{\n\t\t\tobservedKeyspace: o.Keyspace,\n\t\t\tobservedStmts:    o.Statements,\n\t\t\tobservedErr:      o.Err,\n\t\t\tobservedValues:   o.Values,\n\t\t}\n\t}))\n\tfor i := 0; i < 100; i++ {\n\t\t// hard coding 'i' into one of the values for better  testing of observation\n\t\tbatch.Query(fmt.Sprintf(`INSERT INTO batch_observe_table (id,other) VALUES (?,%d)`, i), i)\n\t}\n\n\tif err := session.ExecuteBatch(batch); err != nil {\n\t\tt.Fatal(\"execute batch:\", err)\n\t}\n\tif observedBatch == nil {\n\t\tt.Fatal(\"batch observation has not been called\")\n\t}\n\tif len(observedBatch.observedStmts) != 100 {\n\t\tt.Fatal(\"expecting 100 observed statements, got\", len(observedBatch.observedStmts))\n\t}\n\tif observedBatch.observedErr != nil {\n\t\tt.Fatal(\"not expecting to observe an error\", observedBatch.observedErr)\n\t}\n\tif observedBatch.observedKeyspace != \"gocql_test\" {\n\t\tt.Fatalf(\"expecting keyspace 'gocql_test', got %q\", observedBatch.observedKeyspace)\n\t}\n\tfor i, stmt := range observedBatch.observedStmts {\n\t\tif stmt != fmt.Sprintf(`INSERT INTO batch_observe_table (id,other) VALUES (?,%d)`, i) {\n\t\t\tt.Fatal(\"unexpected query\", stmt)\n\t\t}\n\n\t\tassertDeepEqual(t, \"observed value\", []interface{}{i}, observedBatch.observedValues[i])\n\t}\n}\n\n// TestNilInQuery tests to see that a nil value passed to a query is handled by Cassandra\n// TODO validate the nil value by reading back the nil. Need to fix Unmarshalling.\nfunc TestNilInQuery(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.testNilInsert (id int, count int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\tif err := session.Query(\"INSERT INTO testNilInsert (id,count) VALUES (?,?)\", 1, nil).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to insert with err: %v\", err)\n\t}\n\n\tvar id int\n\n\tif err := session.Query(\"SELECT id FROM testNilInsert\").Scan(&id); err != nil {\n\t\tt.Fatalf(\"failed to select with err: %v\", err)\n\t} else if id != 1 {\n\t\tt.Fatalf(\"expected id to be 1, got %v\", id)\n\t}\n}\n\n// Don't initialize time.Time bind variable if cassandra timestamp column is empty\nfunc TestEmptyTimestamp(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_empty_timestamp (id int, time timestamp, num int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tif err := session.Query(\"INSERT INTO test_empty_timestamp (id, num) VALUES (?,?)\", 1, 561).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to insert with err: %v\", err)\n\t}\n\n\tvar timeVal time.Time\n\n\tif err := session.Query(\"SELECT time FROM test_empty_timestamp where id = ?\", 1).Scan(&timeVal); err != nil {\n\t\tt.Fatalf(\"failed to select with err: %v\", err)\n\t}\n\n\tif !timeVal.IsZero() {\n\t\tt.Errorf(\"time.Time bind variable should still be empty (was %s)\", timeVal)\n\t}\n}\n\n// Integration test of just querying for data from the system.schema_keyspace table where the keyspace DOES exist.\nfunc TestGetKeyspaceMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tkeyspaceMetadata, err := getKeyspaceMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query the keyspace metadata with err: %v\", err)\n\t}\n\tif keyspaceMetadata == nil {\n\t\tt.Fatal(\"failed to query the keyspace metadata, nil returned\")\n\t}\n\tif keyspaceMetadata.Name != \"gocql_test\" {\n\t\tt.Errorf(\"Expected keyspace name to be 'gocql' but was '%s'\", keyspaceMetadata.Name)\n\t}\n\tif keyspaceMetadata.StrategyClass != \"org.apache.cassandra.locator.SimpleStrategy\" {\n\t\tt.Errorf(\"Expected replication strategy class to be 'org.apache.cassandra.locator.SimpleStrategy' but was '%s'\", keyspaceMetadata.StrategyClass)\n\t}\n\tif keyspaceMetadata.StrategyOptions == nil {\n\t\tt.Error(\"Expected replication strategy options map but was nil\")\n\t}\n\trfStr, ok := keyspaceMetadata.StrategyOptions[\"replication_factor\"]\n\tif !ok {\n\t\tt.Fatalf(\"Expected strategy option 'replication_factor' but was not found in %v\", keyspaceMetadata.StrategyOptions)\n\t}\n\trfInt, err := strconv.Atoi(rfStr.(string))\n\tif err != nil {\n\t\tt.Fatalf(\"Error converting string to int with err: %v\", err)\n\t}\n\tif rfInt != *flagRF {\n\t\tt.Errorf(\"Expected replication factor to be %d but was %d\", *flagRF, rfInt)\n\t}\n}\n\n// Integration test of just querying for data from the system.schema_keyspace table where the keyspace DOES NOT exist.\nfunc TestGetKeyspaceMetadataFails(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\t_, err := getKeyspaceMetadata(session, \"gocql_keyspace_does_not_exist\")\n\n\tif err != ErrKeyspaceDoesNotExist || err == nil {\n\t\tt.Fatalf(\"Expected error of type ErrKeySpaceDoesNotExist. Instead, error was %v\", err)\n\t}\n}\n\n// Integration test of just querying for data from the system.schema_columnfamilies table\nfunc TestGetTableMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_table_metadata (first_id int, second_id int, third_id int, PRIMARY KEY (first_id, second_id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\ttables, err := getTableMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query the table metadata with err: %v\", err)\n\t}\n\tif tables == nil {\n\t\tt.Fatal(\"failed to query the table metadata, nil returned\")\n\t}\n\n\tvar testTable *TableMetadata\n\n\t// verify all tables have minimum expected data\n\tfor i := range tables {\n\t\ttable := &tables[i]\n\n\t\tif table.Name == \"\" {\n\t\t\tt.Errorf(\"Expected table name to be set, but it was empty: index=%d metadata=%+v\", i, table)\n\t\t}\n\t\tif table.Keyspace != \"gocql_test\" {\n\t\t\tt.Errorf(\"Expected keyspace for '%s' table metadata to be 'gocql_test' but was '%s'\", table.Name, table.Keyspace)\n\t\t}\n\t\tif session.cfg.ProtoVersion < 4 {\n\t\t\t// TODO(zariel): there has to be a better way to detect what metadata version\n\t\t\t// we are in, and a better way to structure the code so that it is abstracted away\n\t\t\t// from us here\n\t\t\tif table.KeyValidator == \"\" {\n\t\t\t\tt.Errorf(\"Expected key validator to be set for table %s\", table.Name)\n\t\t\t}\n\t\t\tif table.Comparator == \"\" {\n\t\t\t\tt.Errorf(\"Expected comparator to be set for table %s\", table.Name)\n\t\t\t}\n\t\t\tif table.DefaultValidator == \"\" {\n\t\t\t\tt.Errorf(\"Expected default validator to be set for table %s\", table.Name)\n\t\t\t}\n\t\t}\n\n\t\t// these fields are not set until the metadata is compiled\n\t\tif table.PartitionKey != nil {\n\t\t\tt.Errorf(\"Did not expect partition key for table %s\", table.Name)\n\t\t}\n\t\tif table.ClusteringColumns != nil {\n\t\t\tt.Errorf(\"Did not expect clustering columns for table %s\", table.Name)\n\t\t}\n\t\tif table.Columns != nil {\n\t\t\tt.Errorf(\"Did not expect columns for table %s\", table.Name)\n\t\t}\n\n\t\t// for the next part of the test after this loop, find the metadata for the test table\n\t\tif table.Name == \"test_table_metadata\" {\n\t\t\ttestTable = table\n\t\t}\n\t}\n\n\t// verify actual values on the test tables\n\tif testTable == nil {\n\t\tt.Fatal(\"Expected table metadata for name 'test_table_metadata'\")\n\t}\n\tif session.cfg.ProtoVersion == protoVersion1 {\n\t\tif testTable.KeyValidator != \"org.apache.cassandra.db.marshal.Int32Type\" {\n\t\t\tt.Errorf(\"Expected test_table_metadata key validator to be 'org.apache.cassandra.db.marshal.Int32Type' but was '%s'\", testTable.KeyValidator)\n\t\t}\n\t\tif testTable.Comparator != \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.UTF8Type)\" {\n\t\t\tt.Errorf(\"Expected test_table_metadata key validator to be 'org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.UTF8Type)' but was '%s'\", testTable.Comparator)\n\t\t}\n\t\tif testTable.DefaultValidator != \"org.apache.cassandra.db.marshal.BytesType\" {\n\t\t\tt.Errorf(\"Expected test_table_metadata key validator to be 'org.apache.cassandra.db.marshal.BytesType' but was '%s'\", testTable.DefaultValidator)\n\t\t}\n\t\texpectedKeyAliases := []string{\"first_id\"}\n\t\tif !reflect.DeepEqual(testTable.KeyAliases, expectedKeyAliases) {\n\t\t\tt.Errorf(\"Expected key aliases %v but was %v\", expectedKeyAliases, testTable.KeyAliases)\n\t\t}\n\t\texpectedColumnAliases := []string{\"second_id\"}\n\t\tif !reflect.DeepEqual(testTable.ColumnAliases, expectedColumnAliases) {\n\t\t\tt.Errorf(\"Expected key aliases %v but was %v\", expectedColumnAliases, testTable.ColumnAliases)\n\t\t}\n\t}\n\tif testTable.ValueAlias != \"\" {\n\t\tt.Errorf(\"Expected value alias '' but was '%s'\", testTable.ValueAlias)\n\t}\n}\n\n// Integration test of just querying for data from the system.schema_columns table\nfunc TestGetColumnMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_column_metadata (first_id int, second_id int, third_id int, PRIMARY KEY (first_id, second_id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tif err := session.Query(\"CREATE INDEX index_column_metadata ON test_column_metadata ( third_id )\").Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create index with err: %v\", err)\n\t}\n\n\tcolumns, err := getColumnMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query column metadata with err: %v\", err)\n\t}\n\tif columns == nil {\n\t\tt.Fatal(\"failed to query column metadata, nil returned\")\n\t}\n\n\ttestColumns := map[string]*ColumnMetadata{}\n\n\t// verify actual values on the test columns\n\tfor i := range columns {\n\t\tcolumn := &columns[i]\n\n\t\tif column.Name == \"\" {\n\t\t\tt.Errorf(\"Expected column name to be set, but it was empty: index=%d metadata=%+v\", i, column)\n\t\t}\n\t\tif column.Table == \"\" {\n\t\t\tt.Errorf(\"Expected column %s table name to be set, but it was empty\", column.Name)\n\t\t}\n\t\tif column.Keyspace != \"gocql_test\" {\n\t\t\tt.Errorf(\"Expected column %s keyspace name to be 'gocql_test', but it was '%s'\", column.Name, column.Keyspace)\n\t\t}\n\t\tif column.Kind == ColumnUnkownKind {\n\t\t\tt.Errorf(\"Expected column %s kind to be set, but it was empty\", column.Name)\n\t\t}\n\t\tif session.cfg.ProtoVersion == 1 && column.Kind != ColumnRegular {\n\t\t\tt.Errorf(\"Expected column %s kind to be set to 'regular' for proto V1 but it was '%s'\", column.Name, column.Kind)\n\t\t}\n\t\tif column.Validator == \"\" {\n\t\t\tt.Errorf(\"Expected column %s validator to be set, but it was empty\", column.Name)\n\t\t}\n\n\t\t// find the test table columns for the next step after this loop\n\t\tif column.Table == \"test_column_metadata\" {\n\t\t\ttestColumns[column.Name] = column\n\t\t}\n\t}\n\n\tif session.cfg.ProtoVersion == 1 {\n\t\t// V1 proto only returns \"regular columns\"\n\t\tif len(testColumns) != 1 {\n\t\t\tt.Errorf(\"Expected 1 test columns but there were %d\", len(testColumns))\n\t\t}\n\t\tthirdID, found := testColumns[\"third_id\"]\n\t\tif !found {\n\t\t\tt.Fatalf(\"Expected to find column 'third_id' metadata but there was only %v\", testColumns)\n\t\t}\n\n\t\tif thirdID.Kind != ColumnRegular {\n\t\t\tt.Errorf(\"Expected %s column kind to be '%s' but it was '%s'\", thirdID.Name, ColumnRegular, thirdID.Kind)\n\t\t}\n\n\t\tif thirdID.Index.Name != \"index_column_metadata\" {\n\t\t\tt.Errorf(\"Expected %s column index name to be 'index_column_metadata' but it was '%s'\", thirdID.Name, thirdID.Index.Name)\n\t\t}\n\t} else {\n\t\tif len(testColumns) != 3 {\n\t\t\tt.Errorf(\"Expected 3 test columns but there were %d\", len(testColumns))\n\t\t}\n\t\tfirstID, found := testColumns[\"first_id\"]\n\t\tif !found {\n\t\t\tt.Fatalf(\"Expected to find column 'first_id' metadata but there was only %v\", testColumns)\n\t\t}\n\t\tsecondID, found := testColumns[\"second_id\"]\n\t\tif !found {\n\t\t\tt.Fatalf(\"Expected to find column 'second_id' metadata but there was only %v\", testColumns)\n\t\t}\n\t\tthirdID, found := testColumns[\"third_id\"]\n\t\tif !found {\n\t\t\tt.Fatalf(\"Expected to find column 'third_id' metadata but there was only %v\", testColumns)\n\t\t}\n\n\t\tif firstID.Kind != ColumnPartitionKey {\n\t\t\tt.Errorf(\"Expected %s column kind to be '%s' but it was '%s'\", firstID.Name, ColumnPartitionKey, firstID.Kind)\n\t\t}\n\t\tif secondID.Kind != ColumnClusteringKey {\n\t\t\tt.Errorf(\"Expected %s column kind to be '%s' but it was '%s'\", secondID.Name, ColumnClusteringKey, secondID.Kind)\n\t\t}\n\t\tif thirdID.Kind != ColumnRegular {\n\t\t\tt.Errorf(\"Expected %s column kind to be '%s' but it was '%s'\", thirdID.Name, ColumnRegular, thirdID.Kind)\n\t\t}\n\n\t\tif !session.useSystemSchema && thirdID.Index.Name != \"index_column_metadata\" {\n\t\t\t// TODO(zariel): update metadata to scan index from system_schema\n\t\t\tt.Errorf(\"Expected %s column index name to be 'index_column_metadata' but it was '%s'\", thirdID.Name, thirdID.Index.Name)\n\t\t}\n\t}\n}\n\nfunc TestViewMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tcreateViews(t, session)\n\n\tviews, err := getViewsMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query view metadata with err: %v\", err)\n\t}\n\tif views == nil {\n\t\tt.Fatal(\"failed to query view metadata, nil returned\")\n\t}\n\n\tif len(views) != 1 {\n\t\tt.Fatal(\"expected one view\")\n\t}\n\n\ttextType := TypeText\n\tif flagCassVersion.Before(3, 0, 0) {\n\t\ttextType = TypeVarchar\n\t}\n\n\texpectedView := ViewMetadata{\n\t\tKeyspace:   \"gocql_test\",\n\t\tName:       \"basicview\",\n\t\tFieldNames: []string{\"birthday\", \"nationality\", \"weight\", \"height\"},\n\t\tFieldTypes: []TypeInfo{\n\t\t\tNativeType{typ: TypeTimestamp},\n\t\t\tNativeType{typ: textType},\n\t\t\tNativeType{typ: textType},\n\t\t\tNativeType{typ: textType},\n\t\t},\n\t}\n\n\tif !reflect.DeepEqual(views[0], expectedView) {\n\t\tt.Fatalf(\"view is %+v, but expected %+v\", views[0], expectedView)\n\t}\n}\n\nfunc TestMaterializedViewMetadata(t *testing.T) {\n\tif flagCassVersion.Before(3, 0, 0) {\n\t\treturn\n\t}\n\tsession := createSession(t)\n\tdefer session.Close()\n\tcreateMaterializedViews(t, session)\n\n\tmaterializedViews, err := getMaterializedViewsMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query view metadata with err: %v\", err)\n\t}\n\tif materializedViews == nil {\n\t\tt.Fatal(\"failed to query view metadata, nil returned\")\n\t}\n\tif len(materializedViews) != 2 {\n\t\tt.Fatal(\"expected two views\")\n\t}\n\texpectedChunkLengthInKB := \"16\"\n\texpectedDCLocalReadRepairChance := float64(0)\n\texpectedSpeculativeRetry := \"99p\"\n\tif flagCassVersion.Before(4, 0, 0) {\n\t\texpectedChunkLengthInKB = \"64\"\n\t\texpectedDCLocalReadRepairChance = 0.1\n\t\texpectedSpeculativeRetry = \"99PERCENTILE\"\n\t}\n\texpectedView1 := MaterializedViewMetadata{\n\t\tKeyspace:                \"gocql_test\",\n\t\tName:                    \"view_view\",\n\t\tbaseTableName:           \"view_table\",\n\t\tBloomFilterFpChance:     0.01,\n\t\tCaching:                 map[string]string{\"keys\": \"ALL\", \"rows_per_partition\": \"NONE\"},\n\t\tComment:                 \"\",\n\t\tCompaction:              map[string]string{\"class\": \"org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy\", \"max_threshold\": \"32\", \"min_threshold\": \"4\"},\n\t\tCompression:             map[string]string{\"chunk_length_in_kb\": expectedChunkLengthInKB, \"class\": \"org.apache.cassandra.io.compress.LZ4Compressor\"},\n\t\tCrcCheckChance:          1,\n\t\tDcLocalReadRepairChance: expectedDCLocalReadRepairChance,\n\t\tDefaultTimeToLive:       0,\n\t\tExtensions:              map[string]string{},\n\t\tGcGraceSeconds:          864000,\n\t\tIncludeAllColumns:       false, MaxIndexInterval: 2048, MemtableFlushPeriodInMs: 0, MinIndexInterval: 128, ReadRepairChance: 0,\n\t\tSpeculativeRetry: expectedSpeculativeRetry,\n\t}\n\texpectedView2 := MaterializedViewMetadata{\n\t\tKeyspace:                \"gocql_test\",\n\t\tName:                    \"view_view2\",\n\t\tbaseTableName:           \"view_table2\",\n\t\tBloomFilterFpChance:     0.01,\n\t\tCaching:                 map[string]string{\"keys\": \"ALL\", \"rows_per_partition\": \"NONE\"},\n\t\tComment:                 \"\",\n\t\tCompaction:              map[string]string{\"class\": \"org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy\", \"max_threshold\": \"32\", \"min_threshold\": \"4\"},\n\t\tCompression:             map[string]string{\"chunk_length_in_kb\": expectedChunkLengthInKB, \"class\": \"org.apache.cassandra.io.compress.LZ4Compressor\"},\n\t\tCrcCheckChance:          1,\n\t\tDcLocalReadRepairChance: expectedDCLocalReadRepairChance,\n\t\tDefaultTimeToLive:       0,\n\t\tExtensions:              map[string]string{},\n\t\tGcGraceSeconds:          864000,\n\t\tIncludeAllColumns:       false, MaxIndexInterval: 2048, MemtableFlushPeriodInMs: 0, MinIndexInterval: 128, ReadRepairChance: 0,\n\t\tSpeculativeRetry: expectedSpeculativeRetry,\n\t}\n\n\texpectedView1.BaseTableId = materializedViews[0].BaseTableId\n\texpectedView1.Id = materializedViews[0].Id\n\tif !reflect.DeepEqual(materializedViews[0], expectedView1) {\n\t\tt.Fatalf(\"materialized view is %+v, but expected %+v\", materializedViews[0], expectedView1)\n\t}\n\texpectedView2.BaseTableId = materializedViews[1].BaseTableId\n\texpectedView2.Id = materializedViews[1].Id\n\tif !reflect.DeepEqual(materializedViews[1], expectedView2) {\n\t\tt.Fatalf(\"materialized view is %+v, but expected %+v\", materializedViews[1], expectedView2)\n\t}\n}\n\nfunc TestAggregateMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tcreateAggregate(t, session)\n\n\taggregates, err := getAggregatesMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query aggregate metadata with err: %v\", err)\n\t}\n\tif aggregates == nil {\n\t\tt.Fatal(\"failed to query aggregate metadata, nil returned\")\n\t}\n\tif len(aggregates) != 2 {\n\t\tt.Fatal(\"expected two aggregates\")\n\t}\n\n\texpectedAggregrate := AggregateMetadata{\n\t\tKeyspace:      \"gocql_test\",\n\t\tName:          \"average\",\n\t\tArgumentTypes: []TypeInfo{NativeType{typ: TypeInt}},\n\t\tInitCond:      \"(0, 0)\",\n\t\tReturnType:    NativeType{typ: TypeDouble},\n\t\tStateType: TupleTypeInfo{\n\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\tElems: []TypeInfo{\n\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\tNativeType{typ: TypeBigInt},\n\t\t\t},\n\t\t},\n\t\tstateFunc: \"avgstate\",\n\t\tfinalFunc: \"avgfinal\",\n\t}\n\n\t// In this case cassandra is returning a blob\n\tif flagCassVersion.Before(3, 0, 0) {\n\t\texpectedAggregrate.InitCond = string([]byte{0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0})\n\t}\n\n\tif !reflect.DeepEqual(aggregates[0], expectedAggregrate) {\n\t\tt.Fatalf(\"aggregate 'average' is %+v, but expected %+v\", aggregates[0], expectedAggregrate)\n\t}\n\texpectedAggregrate.Name = \"average2\"\n\tif !reflect.DeepEqual(aggregates[1], expectedAggregrate) {\n\t\tt.Fatalf(\"aggregate 'average2' is %+v, but expected %+v\", aggregates[1], expectedAggregrate)\n\t}\n}\n\nfunc TestFunctionMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tcreateFunctions(t, session)\n\n\tfunctions, err := getFunctionsMetadata(session, \"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query function metadata with err: %v\", err)\n\t}\n\tif functions == nil {\n\t\tt.Fatal(\"failed to query function metadata, nil returned\")\n\t}\n\tif len(functions) != 2 {\n\t\tt.Fatal(\"expected two functions\")\n\t}\n\tavgState := functions[1]\n\tavgFinal := functions[0]\n\n\tavgStateBody := \"if (val !=null) {state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue());}return state;\"\n\texpectedAvgState := FunctionMetadata{\n\t\tKeyspace: \"gocql_test\",\n\t\tName:     \"avgstate\",\n\t\tArgumentTypes: []TypeInfo{\n\t\t\tTupleTypeInfo{\n\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\tNativeType{typ: TypeBigInt},\n\t\t\t\t},\n\t\t\t},\n\t\t\tNativeType{typ: TypeInt},\n\t\t},\n\t\tArgumentNames: []string{\"state\", \"val\"},\n\t\tReturnType: TupleTypeInfo{\n\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\tElems: []TypeInfo{\n\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\tNativeType{typ: TypeBigInt},\n\t\t\t},\n\t\t},\n\t\tCalledOnNullInput: true,\n\t\tLanguage:          \"java\",\n\t\tBody:              avgStateBody,\n\t}\n\tif !reflect.DeepEqual(avgState, expectedAvgState) {\n\t\tt.Fatalf(\"function is %+v, but expected %+v\", avgState, expectedAvgState)\n\t}\n\n\tfinalStateBody := \"double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);\"\n\texpectedAvgFinal := FunctionMetadata{\n\t\tKeyspace: \"gocql_test\",\n\t\tName:     \"avgfinal\",\n\t\tArgumentTypes: []TypeInfo{\n\t\t\tTupleTypeInfo{\n\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\tNativeType{typ: TypeBigInt},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tArgumentNames:     []string{\"state\"},\n\t\tReturnType:        NativeType{typ: TypeDouble},\n\t\tCalledOnNullInput: true,\n\t\tLanguage:          \"java\",\n\t\tBody:              finalStateBody,\n\t}\n\tif !reflect.DeepEqual(avgFinal, expectedAvgFinal) {\n\t\tt.Fatalf(\"function is %+v, but expected %+v\", avgFinal, expectedAvgFinal)\n\t}\n}\n\n// Integration test of querying and composition the keyspace metadata\nfunc TestKeyspaceMetadata(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_metadata (first_id int, second_id int, third_id int, PRIMARY KEY (first_id, second_id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\tcreateAggregate(t, session)\n\tcreateViews(t, session)\n\tcreateMaterializedViews(t, session)\n\n\tif err := session.Query(\"CREATE INDEX index_metadata ON test_metadata ( third_id )\").Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create index with err: %v\", err)\n\t}\n\n\tkeyspaceMetadata, err := session.KeyspaceMetadata(\"gocql_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to query keyspace metadata with err: %v\", err)\n\t}\n\tif keyspaceMetadata == nil {\n\t\tt.Fatal(\"expected the keyspace metadata to not be nil, but it was nil\")\n\t}\n\tif keyspaceMetadata.Name != session.cfg.Keyspace {\n\t\tt.Fatalf(\"Expected the keyspace name to be %s but was %s\", session.cfg.Keyspace, keyspaceMetadata.Name)\n\t}\n\tif len(keyspaceMetadata.Tables) == 0 {\n\t\tt.Errorf(\"Expected tables but there were none\")\n\t}\n\n\ttableMetadata, found := keyspaceMetadata.Tables[\"test_metadata\"]\n\tif !found {\n\t\tt.Fatalf(\"failed to find the test_metadata table metadata\")\n\t}\n\n\tif len(tableMetadata.PartitionKey) != 1 {\n\t\tt.Errorf(\"expected partition key length of 1, but was %d\", len(tableMetadata.PartitionKey))\n\t}\n\tfor i, column := range tableMetadata.PartitionKey {\n\t\tif column == nil {\n\t\t\tt.Errorf(\"partition key column metadata at index %d was nil\", i)\n\t\t}\n\t}\n\tif tableMetadata.PartitionKey[0].Name != \"first_id\" {\n\t\tt.Errorf(\"Expected the first partition key column to be 'first_id' but was '%s'\", tableMetadata.PartitionKey[0].Name)\n\t}\n\tif len(tableMetadata.ClusteringColumns) != 1 {\n\t\tt.Fatalf(\"expected clustering columns length of 1, but was %d\", len(tableMetadata.ClusteringColumns))\n\t}\n\tfor i, column := range tableMetadata.ClusteringColumns {\n\t\tif column == nil {\n\t\t\tt.Fatalf(\"clustering column metadata at index %d was nil\", i)\n\t\t}\n\t}\n\tif tableMetadata.ClusteringColumns[0].Name != \"second_id\" {\n\t\tt.Errorf(\"Expected the first clustering column to be 'second_id' but was '%s'\", tableMetadata.ClusteringColumns[0].Name)\n\t}\n\tthirdColumn, found := tableMetadata.Columns[\"third_id\"]\n\tif !found {\n\t\tt.Fatalf(\"Expected a column definition for 'third_id'\")\n\t}\n\tif !session.useSystemSchema && thirdColumn.Index.Name != \"index_metadata\" {\n\t\t// TODO(zariel): scan index info from system_schema\n\t\tt.Errorf(\"Expected column index named 'index_metadata' but was '%s'\", thirdColumn.Index.Name)\n\t}\n\n\taggregate, found := keyspaceMetadata.Aggregates[\"average\"]\n\tif !found {\n\t\tt.Fatal(\"failed to find the aggregate 'average' in metadata\")\n\t}\n\tif aggregate.FinalFunc.Name != \"avgfinal\" {\n\t\tt.Fatalf(\"expected final function %s, but got %s\", \"avgFinal\", aggregate.FinalFunc.Name)\n\t}\n\tif aggregate.StateFunc.Name != \"avgstate\" {\n\t\tt.Fatalf(\"expected state function %s, but got %s\", \"avgstate\", aggregate.StateFunc.Name)\n\t}\n\taggregate, found = keyspaceMetadata.Aggregates[\"average2\"]\n\tif !found {\n\t\tt.Fatal(\"failed to find the aggregate 'average2' in metadata\")\n\t}\n\tif aggregate.FinalFunc.Name != \"avgfinal\" {\n\t\tt.Fatalf(\"expected final function %s, but got %s\", \"avgFinal\", aggregate.FinalFunc.Name)\n\t}\n\tif aggregate.StateFunc.Name != \"avgstate\" {\n\t\tt.Fatalf(\"expected state function %s, but got %s\", \"avgstate\", aggregate.StateFunc.Name)\n\t}\n\n\t_, found = keyspaceMetadata.Views[\"basicview\"]\n\tif !found {\n\t\tt.Fatal(\"failed to find the view in metadata\")\n\t}\n\t_, found = keyspaceMetadata.UserTypes[\"basicview\"]\n\tif !found {\n\t\tt.Fatal(\"failed to find the types in metadata\")\n\t}\n\ttextType := TypeText\n\tif flagCassVersion.Before(3, 0, 0) {\n\t\ttextType = TypeVarchar\n\t}\n\texpectedType := UserTypeMetadata{\n\t\tKeyspace:   \"gocql_test\",\n\t\tName:       \"basicview\",\n\t\tFieldNames: []string{\"birthday\", \"nationality\", \"weight\", \"height\"},\n\t\tFieldTypes: []TypeInfo{\n\t\t\tNativeType{typ: TypeTimestamp},\n\t\t\tNativeType{typ: textType},\n\t\t\tNativeType{typ: textType},\n\t\t\tNativeType{typ: textType},\n\t\t},\n\t}\n\tif !reflect.DeepEqual(*keyspaceMetadata.UserTypes[\"basicview\"], expectedType) {\n\t\tt.Fatalf(\"type is %+v, but expected %+v\", keyspaceMetadata.UserTypes[\"basicview\"], expectedType)\n\t}\n\tif flagCassVersion.Major >= 3 {\n\t\tmaterializedView, found := keyspaceMetadata.MaterializedViews[\"view_view\"]\n\t\tif !found {\n\t\t\tt.Fatal(\"failed to find materialized view view_view in metadata\")\n\t\t}\n\t\tif materializedView.BaseTable.Name != \"view_table\" {\n\t\t\tt.Fatalf(\"expected name: %s, materialized view base table name: %s\", \"view_table\", materializedView.BaseTable.Name)\n\t\t}\n\t\tmaterializedView, found = keyspaceMetadata.MaterializedViews[\"view_view2\"]\n\t\tif !found {\n\t\t\tt.Fatal(\"failed to find materialized view view_view2 in metadata\")\n\t\t}\n\t\tif materializedView.BaseTable.Name != \"view_table2\" {\n\t\t\tt.Fatalf(\"expected name: %s, materialized view base table name: %s\", \"view_table2\", materializedView.BaseTable.Name)\n\t\t}\n\t}\n}\n\n// Integration test of the routing key calculation\nfunc TestRoutingKey(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_single_routing_key (first_id int, second_id int, PRIMARY KEY (first_id, second_id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_composite_routing_key (first_id int, second_id int, PRIMARY KEY ((first_id, second_id)))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\troutingKeyInfo, err := session.routingKeyInfo(context.Background(), \"SELECT * FROM test_single_routing_key WHERE second_id=? AND first_id=?\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to get routing key info due to error: %v\", err)\n\t}\n\tif routingKeyInfo == nil {\n\t\tt.Fatal(\"Expected routing key info, but was nil\")\n\t}\n\tif len(routingKeyInfo.indexes) != 1 {\n\t\tt.Fatalf(\"Expected routing key indexes length to be 1 but was %d\", len(routingKeyInfo.indexes))\n\t}\n\tif routingKeyInfo.indexes[0] != 1 {\n\t\tt.Errorf(\"Expected routing key index[0] to be 1 but was %d\", routingKeyInfo.indexes[0])\n\t}\n\tif len(routingKeyInfo.types) != 1 {\n\t\tt.Fatalf(\"Expected routing key types length to be 1 but was %d\", len(routingKeyInfo.types))\n\t}\n\tif routingKeyInfo.types[0] == nil {\n\t\tt.Fatal(\"Expected routing key types[0] to be non-nil\")\n\t}\n\tif routingKeyInfo.types[0].Type() != TypeInt {\n\t\tt.Fatalf(\"Expected routing key types[0].Type to be %v but was %v\", TypeInt, routingKeyInfo.types[0].Type())\n\t}\n\n\t// verify the cache is working\n\troutingKeyInfo, err = session.routingKeyInfo(context.Background(), \"SELECT * FROM test_single_routing_key WHERE second_id=? AND first_id=?\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to get routing key info due to error: %v\", err)\n\t}\n\tif len(routingKeyInfo.indexes) != 1 {\n\t\tt.Fatalf(\"Expected routing key indexes length to be 1 but was %d\", len(routingKeyInfo.indexes))\n\t}\n\tif routingKeyInfo.indexes[0] != 1 {\n\t\tt.Errorf(\"Expected routing key index[0] to be 1 but was %d\", routingKeyInfo.indexes[0])\n\t}\n\tif len(routingKeyInfo.types) != 1 {\n\t\tt.Fatalf(\"Expected routing key types length to be 1 but was %d\", len(routingKeyInfo.types))\n\t}\n\tif routingKeyInfo.types[0] == nil {\n\t\tt.Fatal(\"Expected routing key types[0] to be non-nil\")\n\t}\n\tif routingKeyInfo.types[0].Type() != TypeInt {\n\t\tt.Fatalf(\"Expected routing key types[0] to be %v but was %v\", TypeInt, routingKeyInfo.types[0].Type())\n\t}\n\tcacheSize := session.routingKeyInfoCache.lru.Len()\n\tif cacheSize != 1 {\n\t\tt.Errorf(\"Expected cache size to be 1 but was %d\", cacheSize)\n\t}\n\n\tquery := session.Query(\"SELECT * FROM test_single_routing_key WHERE second_id=? AND first_id=?\", 1, 2)\n\troutingKey, err := query.GetRoutingKey()\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to get routing key due to error: %v\", err)\n\t}\n\texpectedRoutingKey := []byte{0, 0, 0, 2}\n\tif !reflect.DeepEqual(expectedRoutingKey, routingKey) {\n\t\tt.Errorf(\"Expected routing key %v but was %v\", expectedRoutingKey, routingKey)\n\t}\n\n\troutingKeyInfo, err = session.routingKeyInfo(context.Background(), \"SELECT * FROM test_composite_routing_key WHERE second_id=? AND first_id=?\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to get routing key info due to error: %v\", err)\n\t}\n\tif routingKeyInfo == nil {\n\t\tt.Fatal(\"Expected routing key info, but was nil\")\n\t}\n\tif len(routingKeyInfo.indexes) != 2 {\n\t\tt.Fatalf(\"Expected routing key indexes length to be 2 but was %d\", len(routingKeyInfo.indexes))\n\t}\n\tif routingKeyInfo.indexes[0] != 1 {\n\t\tt.Errorf(\"Expected routing key index[0] to be 1 but was %d\", routingKeyInfo.indexes[0])\n\t}\n\tif routingKeyInfo.indexes[1] != 0 {\n\t\tt.Errorf(\"Expected routing key index[1] to be 0 but was %d\", routingKeyInfo.indexes[1])\n\t}\n\tif len(routingKeyInfo.types) != 2 {\n\t\tt.Fatalf(\"Expected routing key types length to be 1 but was %d\", len(routingKeyInfo.types))\n\t}\n\tif routingKeyInfo.types[0] == nil {\n\t\tt.Fatal(\"Expected routing key types[0] to be non-nil\")\n\t}\n\tif routingKeyInfo.types[0].Type() != TypeInt {\n\t\tt.Fatalf(\"Expected routing key types[0] to be %v but was %v\", TypeInt, routingKeyInfo.types[0].Type())\n\t}\n\tif routingKeyInfo.types[1] == nil {\n\t\tt.Fatal(\"Expected routing key types[1] to be non-nil\")\n\t}\n\tif routingKeyInfo.types[1].Type() != TypeInt {\n\t\tt.Fatalf(\"Expected routing key types[0] to be %v but was %v\", TypeInt, routingKeyInfo.types[1].Type())\n\t}\n\n\tquery = session.Query(\"SELECT * FROM test_composite_routing_key WHERE second_id=? AND first_id=?\", 1, 2)\n\troutingKey, err = query.GetRoutingKey()\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to get routing key due to error: %v\", err)\n\t}\n\texpectedRoutingKey = []byte{0, 4, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 1, 0}\n\tif !reflect.DeepEqual(expectedRoutingKey, routingKey) {\n\t\tt.Errorf(\"Expected routing key %v but was %v\", expectedRoutingKey, routingKey)\n\t}\n\n\t// verify the cache is working\n\tcacheSize = session.routingKeyInfoCache.lru.Len()\n\tif cacheSize != 2 {\n\t\tt.Errorf(\"Expected cache size to be 2 but was %d\", cacheSize)\n\t}\n}\n\n// Integration test of the token-aware policy-based connection pool\nfunc TestTokenAwareConnPool(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.PoolConfig.HostSelectionPolicy = TokenAwareHostPolicy(RoundRobinHostPolicy())\n\n\t// force metadata query to page\n\tcluster.PageSize = 1\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\texpectedPoolSize := cluster.NumConns * len(session.ring.allHosts())\n\n\t// wait for pool to fill\n\tfor i := 0; i < 10; i++ {\n\t\tif session.pool.Size() == expectedPoolSize {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tif expectedPoolSize != session.pool.Size() {\n\t\tt.Errorf(\"Expected pool size %d but was %d\", expectedPoolSize, session.pool.Size())\n\t}\n\n\t// add another cf so there are two pages when fetching table metadata from our keyspace\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_token_aware_other_cf (id int, data text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create test_token_aware table with err: %v\", err)\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.test_token_aware (id int, data text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create test_token_aware table with err: %v\", err)\n\t}\n\tquery := session.Query(\"INSERT INTO test_token_aware (id, data) VALUES (?,?)\", 42, \"8 * 6 =\")\n\tif err := query.Exec(); err != nil {\n\t\tt.Fatalf(\"failed to insert with err: %v\", err)\n\t}\n\n\tquery = session.Query(\"SELECT data FROM test_token_aware where id = ?\", 42).Consistency(One)\n\tvar data string\n\tif err := query.Scan(&data); err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// TODO add verification that the query went to the correct host\n}\n\nfunc TestNegativeStream(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tconn := getRandomConn(t, session)\n\n\tconst stream = -50\n\twriter := frameWriterFunc(func(f *framer, streamID int) error {\n\t\tf.writeHeader(0, opOptions, stream)\n\t\treturn f.finish()\n\t})\n\n\tframe, err := conn.exec(context.Background(), writer, nil)\n\tif err == nil {\n\t\tt.Fatalf(\"expected to get an error on stream %d\", stream)\n\t} else if frame != nil {\n\t\tt.Fatalf(\"expected to get nil frame got %+v\", frame)\n\t}\n}\n\nfunc TestManualQueryPaging(t *testing.T) {\n\tconst rowsToInsert = 5\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.testManualPaging (id int, count int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfor i := 0; i < rowsToInsert; i++ {\n\t\terr := session.Query(\"INSERT INTO testManualPaging(id, count) VALUES(?, ?)\", i, i*i).Exec()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// disable auto paging, 1 page per iteration\n\tquery := session.Query(\"SELECT id, count FROM testManualPaging\").PageState(nil).PageSize(2)\n\tvar id, count, fetched int\n\n\titer := query.Iter()\n\t// NOTE: this isnt very indicative of how it should be used, the idea is that\n\t// the page state is returned to some client who will send it back to manually\n\t// page through the results.\n\tfor {\n\t\tfor iter.Scan(&id, &count) {\n\t\t\tif count != (id * id) {\n\t\t\t\tt.Fatalf(\"got wrong value from iteration: got %d expected %d\", count, id*id)\n\t\t\t}\n\n\t\t\tfetched++\n\t\t}\n\n\t\tif len(iter.PageState()) > 0 {\n\t\t\t// more pages\n\t\t\titer = query.PageState(iter.PageState()).Iter()\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif err := iter.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif fetched != rowsToInsert {\n\t\tt.Fatalf(\"expected to fetch %d rows got %d\", rowsToInsert, fetched)\n\t}\n}\n\nfunc TestLexicalUUIDType(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.test_lexical_uuid (\n\t\t\tkey     varchar,\n\t\t\tcolumn1 'org.apache.cassandra.db.marshal.LexicalUUIDType',\n\t\t\tvalue   int,\n\t\t\tPRIMARY KEY (key, column1)\n\t\t)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tkey := TimeUUID().String()\n\tcolumn1 := TimeUUID()\n\n\terr := session.Query(\"INSERT INTO test_lexical_uuid(key, column1, value) VALUES(?, ?, ?)\", key, column1, 55).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar gotUUID UUID\n\tif err := session.Query(\"SELECT column1 from test_lexical_uuid where key = ? AND column1 = ?\", key, column1).Scan(&gotUUID); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif gotUUID != column1 {\n\t\tt.Errorf(\"got %s, expected %s\", gotUUID, column1)\n\t}\n}\n\n// Issue 475\nfunc TestSessionBindRoutingKey(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.PoolConfig.HostSelectionPolicy = TokenAwareHostPolicy(RoundRobinHostPolicy())\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.test_bind_routing_key (\n\t\t\tkey     varchar,\n\t\t\tvalue   int,\n\t\t\tPRIMARY KEY (key)\n\t\t)`); err != nil {\n\n\t\tt.Fatal(err)\n\t}\n\n\tconst (\n\t\tkey   = \"routing-key\"\n\t\tvalue = 5\n\t)\n\n\tfn := func(info *QueryInfo) ([]interface{}, error) {\n\t\treturn []interface{}{key, value}, nil\n\t}\n\n\tq := session.Bind(\"INSERT INTO test_bind_routing_key(key, value) VALUES(?, ?)\", fn)\n\tif err := q.Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestJSONSupport(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < 4 {\n\t\tt.Skip(\"skipping JSON support on proto < 4\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.test_json (\n\t\t    id text PRIMARY KEY,\n\t\t    age int,\n\t\t    state text\n\t\t)`); err != nil {\n\n\t\tt.Fatal(err)\n\t}\n\n\terr := session.Query(\"INSERT INTO test_json JSON ?\", `{\"id\": \"user123\", \"age\": 42, \"state\": \"TX\"}`).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tid    string\n\t\tage   int\n\t\tstate string\n\t)\n\n\terr = session.Query(\"SELECT id, age, state FROM test_json WHERE id = ?\", \"user123\").Scan(&id, &age, &state)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif id != \"user123\" {\n\t\tt.Errorf(\"got id %q expected %q\", id, \"user123\")\n\t}\n\tif age != 42 {\n\t\tt.Errorf(\"got age %d expected %d\", age, 42)\n\t}\n\tif state != \"TX\" {\n\t\tt.Errorf(\"got state %q expected %q\", state, \"TX\")\n\t}\n}\n\nfunc TestDiscoverViaProxy(t *testing.T) {\n\t// This (complicated) test tests that when the driver is given an initial host\n\t// that is infact a proxy it discovers the rest of the ring behind the proxy\n\t// and does not store the proxies address as a host in its connection pool.\n\t// See https://github.com/apache/cassandra-gocql-driver/issues/481\n\tclusterHosts := getClusterHosts()\n\tproxy, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"unable to create proxy listener: %v\", err)\n\t}\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tvar (\n\t\tmu         sync.Mutex\n\t\tproxyConns []net.Conn\n\t\tclosed     bool\n\t)\n\n\tgo func() {\n\t\tcassandraAddr := JoinHostPort(clusterHosts[0], 9042)\n\n\t\tcassandra := func() (net.Conn, error) {\n\t\t\treturn net.Dial(\"tcp\", cassandraAddr)\n\t\t}\n\n\t\tproxyFn := func(errs chan error, from, to net.Conn) {\n\t\t\t_, err := io.Copy(to, from)\n\t\t\tif err != nil {\n\t\t\t\terrs <- err\n\t\t\t}\n\t\t}\n\n\t\t// handle dials cassandra and then proxies requests and reponsess. It waits\n\t\t// for both the read and write side of the TCP connection to close before\n\t\t// returning.\n\t\thandle := func(conn net.Conn) error {\n\t\t\tcass, err := cassandra()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdefer cass.Close()\n\n\t\t\terrs := make(chan error, 2)\n\t\t\tgo proxyFn(errs, conn, cass)\n\t\t\tgo proxyFn(errs, cass, conn)\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tcase err := <-errs:\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tfor {\n\t\t\t// proxy just accepts connections and then proxies them to cassandra,\n\t\t\t// it runs until it is closed.\n\t\t\tconn, err := proxy.Accept()\n\t\t\tif err != nil {\n\t\t\t\tmu.Lock()\n\t\t\t\tif !closed {\n\t\t\t\t\tt.Error(err)\n\t\t\t\t}\n\t\t\t\tmu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tmu.Lock()\n\t\t\tproxyConns = append(proxyConns, conn)\n\t\t\tmu.Unlock()\n\n\t\t\tgo func(conn net.Conn) {\n\t\t\t\tdefer conn.Close()\n\n\t\t\t\tif err := handle(conn); err != nil {\n\t\t\t\t\tmu.Lock()\n\t\t\t\t\tif !closed {\n\t\t\t\t\t\tt.Error(err)\n\t\t\t\t\t}\n\t\t\t\t\tmu.Unlock()\n\t\t\t\t}\n\t\t\t}(conn)\n\t\t}\n\t}()\n\n\tproxyAddr := proxy.Addr().String()\n\n\tcluster := createCluster()\n\tcluster.NumConns = 1\n\t// initial host is the proxy address\n\tcluster.Hosts = []string{proxyAddr}\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\t// we shouldnt need this but to be safe\n\ttime.Sleep(1 * time.Second)\n\n\tsession.pool.mu.RLock()\n\tfor _, host := range clusterHosts {\n\t\tfound := false\n\t\tfor _, hi := range session.pool.hostConnPools {\n\t\t\tif hi.host.ConnectAddress().String() == host {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif !found {\n\t\t\tt.Errorf(\"missing host in pool after discovery: %q\", host)\n\t\t}\n\t}\n\tsession.pool.mu.RUnlock()\n\n\tmu.Lock()\n\tclosed = true\n\tif err := proxy.Close(); err != nil {\n\t\tt.Log(err)\n\t}\n\n\tfor _, conn := range proxyConns {\n\t\tif err := conn.Close(); err != nil {\n\t\t\tt.Log(err)\n\t\t}\n\t}\n\tmu.Unlock()\n}\n\nfunc TestUnmarshallNestedTypes(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"can not have frozen types in cassandra < 2.1.3\")\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.test_557 (\n\t\t    id text PRIMARY KEY,\n\t\t    val list<frozen<map<text, text> > >\n\t\t)`); err != nil {\n\n\t\tt.Fatal(err)\n\t}\n\n\tm := []map[string]string{\n\t\t{\"key1\": \"val1\"},\n\t\t{\"key2\": \"val2\"},\n\t}\n\n\tconst id = \"key\"\n\terr := session.Query(\"INSERT INTO test_557(id, val) VALUES(?, ?)\", id, m).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar data []map[string]string\n\tif err := session.Query(\"SELECT val FROM test_557 WHERE id = ?\", id).Scan(&data); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !reflect.DeepEqual(data, m) {\n\t\tt.Fatalf(\"%+#v != %+#v\", data, m)\n\t}\n}\n\nfunc TestSchemaReset(t *testing.T) {\n\tif flagCassVersion.Major == 0 || flagCassVersion.Before(2, 1, 3) {\n\t\tt.Skipf(\"skipping TestSchemaReset due to CASSANDRA-7910 in Cassandra <2.1.3 version=%v\", flagCassVersion)\n\t}\n\n\tcluster := createCluster()\n\tcluster.NumConns = 1\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.test_schema_reset (\n\t\tid text PRIMARY KEY)`); err != nil {\n\n\t\tt.Fatal(err)\n\t}\n\n\tconst key = \"test\"\n\n\terr := session.Query(\"INSERT INTO test_schema_reset(id) VALUES(?)\", key).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar id string\n\terr = session.Query(\"SELECT * FROM test_schema_reset WHERE id=?\", key).Scan(&id)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if id != key {\n\t\tt.Fatalf(\"expected to get id=%q got=%q\", key, id)\n\t}\n\n\tif err := createTable(session, `ALTER TABLE gocql_test.test_schema_reset ADD val text`); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconst expVal = \"test-val\"\n\terr = session.Query(\"INSERT INTO test_schema_reset(id, val) VALUES(?, ?)\", key, expVal).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar val string\n\terr = session.Query(\"SELECT * FROM test_schema_reset WHERE id=?\", key).Scan(&id, &val)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif id != key {\n\t\tt.Errorf(\"expected to get id=%q got=%q\", key, id)\n\t}\n\tif val != expVal {\n\t\tt.Errorf(\"expected to get val=%q got=%q\", expVal, val)\n\t}\n}\n\nfunc TestCreateSession_DontSwallowError(t *testing.T) {\n\tt.Skip(\"This test is bad, and the resultant error from cassandra changes between versions\")\n\tcluster := createCluster()\n\tcluster.ProtoVersion = 0x100\n\tsession, err := cluster.CreateSession()\n\tif err == nil {\n\t\tsession.Close()\n\n\t\tt.Fatal(\"expected to get an error for unsupported protocol\")\n\t}\n\n\tif flagCassVersion.Major < 3 {\n\t\t// TODO: we should get a distinct error type here which include the underlying\n\t\t// cassandra error about the protocol version, for now check this here.\n\t\tif !strings.Contains(err.Error(), \"Invalid or unsupported protocol version\") {\n\t\t\tt.Fatalf(`expcted to get error \"unsupported protocol version\" got: %q`, err)\n\t\t}\n\t} else {\n\t\tif !strings.Contains(err.Error(), \"unsupported response version\") {\n\t\t\tt.Fatalf(`expcted to get error \"unsupported response version\" got: %q`, err)\n\t\t}\n\t}\n}\n\nfunc TestControl_DiscoverProtocol(t *testing.T) {\n\tcluster := createCluster()\n\tcluster.ProtoVersion = 0\n\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion == 0 {\n\t\tt.Fatal(\"did not discovery protocol\")\n\t}\n}\n\n// TestUnsetCol verify unset column will not replace an existing column\nfunc TestUnsetCol(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < 4 {\n\t\tt.Skip(\"Unset Values are not supported in protocol < 4\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.testUnsetInsert (id int, my_int int, my_text text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\tif err := session.Query(\"INSERT INTO testUnSetInsert (id,my_int,my_text) VALUES (?,?,?)\", 1, 2, \"3\").Exec(); err != nil {\n\t\tt.Fatalf(\"failed to insert with err: %v\", err)\n\t}\n\tif err := session.Query(\"INSERT INTO testUnSetInsert (id,my_int,my_text) VALUES (?,?,?)\", 1, UnsetValue, UnsetValue).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to insert with err: %v\", err)\n\t}\n\n\tvar id, mInt int\n\tvar mText string\n\n\tif err := session.Query(\"SELECT id, my_int ,my_text FROM testUnsetInsert\").Scan(&id, &mInt, &mText); err != nil {\n\t\tt.Fatalf(\"failed to select with err: %v\", err)\n\t} else if id != 1 || mInt != 2 || mText != \"3\" {\n\t\tt.Fatalf(\"Expected results: 1, 2, \\\"3\\\", got %v, %v, %v\", id, mInt, mText)\n\t}\n}\n\n// TestUnsetColBatch verify unset column will not replace a column in batch\nfunc TestUnsetColBatch(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < 4 {\n\t\tt.Skip(\"Unset Values are not supported in protocol < 4\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.batchUnsetInsert (id int, my_int int, my_text text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\n\tb := session.Batch(LoggedBatch)\n\tb.Query(\"INSERT INTO gocql_test.batchUnsetInsert(id, my_int, my_text) VALUES (?,?,?)\", 1, 1, UnsetValue)\n\tb.Query(\"INSERT INTO gocql_test.batchUnsetInsert(id, my_int, my_text) VALUES (?,?,?)\", 1, UnsetValue, \"\")\n\tb.Query(\"INSERT INTO gocql_test.batchUnsetInsert(id, my_int, my_text) VALUES (?,?,?)\", 2, 2, UnsetValue)\n\n\tif err := session.ExecuteBatch(b); err != nil {\n\t\tt.Fatalf(\"query failed. %v\", err)\n\t} else {\n\t\tif b.Attempts() < 1 {\n\t\t\tt.Fatal(\"expected at least 1 attempt, but got 0\")\n\t\t}\n\t\tif b.Latency() <= 0 {\n\t\t\tt.Fatalf(\"expected latency to be greater than 0, but got %v instead.\", b.Latency())\n\t\t}\n\t}\n\tvar id, mInt, count int\n\tvar mText string\n\n\tif err := session.Query(\"SELECT count(*) FROM gocql_test.batchUnsetInsert;\").Scan(&count); err != nil {\n\t\tt.Fatalf(\"Failed to select with err: %v\", err)\n\t} else if count != 2 {\n\t\tt.Fatalf(\"Expected Batch Insert count 2, got %v\", count)\n\t}\n\n\tif err := session.Query(\"SELECT id, my_int ,my_text FROM gocql_test.batchUnsetInsert where id=1;\").Scan(&id, &mInt, &mText); err != nil {\n\t\tt.Fatalf(\"failed to select with err: %v\", err)\n\t} else if id != mInt {\n\t\tt.Fatalf(\"expected id, my_int to be 1, got %v and %v\", id, mInt)\n\t}\n}\n\nfunc TestQuery_NamedValues(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < 3 {\n\t\tt.Skip(\"named Values are not supported in protocol < 3\")\n\t}\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.named_query(id int, value text, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr := session.Query(\"INSERT INTO gocql_test.named_query(id, value) VALUES(:id, :value)\", NamedValue(\"id\", 1), NamedValue(\"value\", \"i am a value\")).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tvar value string\n\tif err := session.Query(\"SELECT VALUE from gocql_test.named_query WHERE id = :id\", NamedValue(\"id\", 1)).Scan(&value); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "cluster.go",
          "type": "blob",
          "size": 13.8603515625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"net\"\n\t\"time\"\n)\n\n// PoolConfig configures the connection pool used by the driver, it defaults to\n// using a round-robin host selection policy and a round-robin connection selection\n// policy for each host.\ntype PoolConfig struct {\n\t// HostSelectionPolicy sets the policy for selecting which host to use for a\n\t// given query (default: RoundRobinHostPolicy())\n\t// It is not supported to use a single HostSelectionPolicy in multiple sessions\n\t// (even if you close the old session before using in a new session).\n\tHostSelectionPolicy HostSelectionPolicy\n}\n\nfunc (p PoolConfig) buildPool(session *Session) *policyConnPool {\n\treturn newPolicyConnPool(session)\n}\n\n// ClusterConfig is a struct to configure the default cluster implementation\n// of gocql. It has a variety of attributes that can be used to modify the\n// behavior to fit the most common use cases. Applications that require a\n// different setup must implement their own cluster.\ntype ClusterConfig struct {\n\t// addresses for the initial connections. It is recommended to use the value set in\n\t// the Cassandra config for broadcast_address or listen_address, an IP address not\n\t// a domain name. This is because events from Cassandra will use the configured IP\n\t// address, which is used to index connected hosts. If the domain name specified\n\t// resolves to more than 1 IP address then the driver may connect multiple times to\n\t// the same host, and will not mark the node being down or up from events.\n\tHosts []string\n\n\t// CQL version (default: 3.0.0)\n\tCQLVersion string\n\n\t// ProtoVersion sets the version of the native protocol to use, this will\n\t// enable features in the driver for specific protocol versions, generally this\n\t// should be set to a known version (2,3,4) for the cluster being connected to.\n\t//\n\t// If it is 0 or unset (the default) then the driver will attempt to discover the\n\t// highest supported protocol for the cluster. In clusters with nodes of different\n\t// versions the protocol selected is not defined (ie, it can be any of the supported in the cluster)\n\tProtoVersion int\n\n\t// Timeout limits the time spent on the client side while executing a query.\n\t// Specifically, query or batch execution will return an error if the client does not receive a response\n\t// from the server within the Timeout period.\n\t// Timeout is also used to configure the read timeout on the underlying network connection.\n\t// Client Timeout should always be higher than the request timeouts configured on the server,\n\t// so that retries don't overload the server.\n\t// Timeout has a default value of 11 seconds, which is higher than default server timeout for most query types.\n\t// Timeout is not applied to requests during initial connection setup, see ConnectTimeout.\n\tTimeout time.Duration\n\n\t// ConnectTimeout limits the time spent during connection setup.\n\t// During initial connection setup, internal queries, AUTH requests will return an error if the client\n\t// does not receive a response within the ConnectTimeout period.\n\t// ConnectTimeout is applied to the connection setup queries independently.\n\t// ConnectTimeout also limits the duration of dialing a new TCP connection\n\t// in case there is no Dialer nor HostDialer configured.\n\t// ConnectTimeout has a default value of 11 seconds.\n\tConnectTimeout time.Duration\n\n\t// WriteTimeout limits the time the driver waits to write a request to a network connection.\n\t// WriteTimeout should be lower than or equal to Timeout.\n\t// WriteTimeout defaults to the value of Timeout.\n\tWriteTimeout time.Duration\n\n\t// Port used when dialing.\n\t// Default: 9042\n\tPort int\n\n\t// Initial keyspace. Optional.\n\tKeyspace string\n\n\t// The size of the connection pool for each host.\n\t// The pool filling runs in separate gourutine during the session initialization phase.\n\t// gocql will always try to get 1 connection on each host pool\n\t// during session initialization AND it will attempt\n\t// to fill each pool afterward asynchronously if NumConns > 1.\n\t// Notice: There is no guarantee that pool filling will be finished in the initialization phase.\n\t// Also, it describes a maximum number of connections at the same time.\n\t// Default: 2\n\tNumConns int\n\n\t// Default consistency level.\n\t// Default: Quorum\n\tConsistency Consistency\n\n\t// Compression algorithm.\n\t// Default: nil\n\tCompressor Compressor\n\n\t// Default: nil\n\tAuthenticator Authenticator\n\n\t// An Authenticator factory. Can be used to create alternative authenticators.\n\t// Default: nil\n\tAuthProvider func(h *HostInfo) (Authenticator, error)\n\n\t// Default retry policy to use for queries.\n\t// Default: no retries.\n\tRetryPolicy RetryPolicy\n\n\t// ConvictionPolicy decides whether to mark host as down based on the error and host info.\n\t// Default: SimpleConvictionPolicy\n\tConvictionPolicy ConvictionPolicy\n\n\t// Default reconnection policy to use for reconnecting before trying to mark host as down.\n\tReconnectionPolicy ReconnectionPolicy\n\n\t// The keepalive period to use, enabled if > 0 (default: 0)\n\t// SocketKeepalive is used to set up the default dialer and is ignored if Dialer or HostDialer is provided.\n\tSocketKeepalive time.Duration\n\n\t// Maximum cache size for prepared statements globally for gocql.\n\t// Default: 1000\n\tMaxPreparedStmts int\n\n\t// Maximum cache size for query info about statements for each session.\n\t// Default: 1000\n\tMaxRoutingKeyInfo int\n\n\t// Default page size to use for created sessions.\n\t// Default: 5000\n\tPageSize int\n\n\t// Consistency for the serial part of queries, values can be either SERIAL or LOCAL_SERIAL.\n\t// Default: unset\n\tSerialConsistency SerialConsistency\n\n\t// SslOpts configures TLS use when HostDialer is not set.\n\t// SslOpts is ignored if HostDialer is set.\n\tSslOpts *SslOptions\n\n\t// Sends a client side timestamp for all requests which overrides the timestamp at which it arrives at the server.\n\t// Default: true, only enabled for protocol 3 and above.\n\tDefaultTimestamp bool\n\n\t// PoolConfig configures the underlying connection pool, allowing the\n\t// configuration of host selection and connection selection policies.\n\tPoolConfig PoolConfig\n\n\t// If not zero, gocql attempt to reconnect known DOWN nodes in every ReconnectInterval.\n\tReconnectInterval time.Duration\n\n\t// The maximum amount of time to wait for schema agreement in a cluster after\n\t// receiving a schema change frame. (default: 60s)\n\tMaxWaitSchemaAgreement time.Duration\n\n\t// HostFilter will filter all incoming events for host, any which don't pass\n\t// the filter will be ignored. If set will take precedence over any options set\n\t// via Discovery\n\tHostFilter HostFilter\n\n\t// AddressTranslator will translate addresses found on peer discovery and/or\n\t// node change events.\n\tAddressTranslator AddressTranslator\n\n\t// If IgnorePeerAddr is true and the address in system.peers does not match\n\t// the supplied host by either initial hosts or discovered via events then the\n\t// host will be replaced with the supplied address.\n\t//\n\t// For example if an event comes in with host=10.0.0.1 but when looking up that\n\t// address in system.local or system.peers returns 127.0.0.1, the peer will be\n\t// set to 10.0.0.1 which is what will be used to connect to.\n\tIgnorePeerAddr bool\n\n\t// If DisableInitialHostLookup then the driver will not attempt to get host info\n\t// from the system.peers table, this will mean that the driver will connect to\n\t// hosts supplied and will not attempt to lookup the hosts information, this will\n\t// mean that data_centre, rack and token information will not be available and as\n\t// such host filtering and token aware query routing will not be available.\n\tDisableInitialHostLookup bool\n\n\t// Configure events the driver will register for\n\tEvents struct {\n\t\t// disable registering for status events (node up/down)\n\t\tDisableNodeStatusEvents bool\n\t\t// disable registering for topology events (node added/removed/moved)\n\t\tDisableTopologyEvents bool\n\t\t// disable registering for schema events (keyspace/table/function removed/created/updated)\n\t\tDisableSchemaEvents bool\n\t}\n\n\t// DisableSkipMetadata will override the internal result metadata cache so that the driver does not\n\t// send skip_metadata for queries, this means that the result will always contain\n\t// the metadata to parse the rows and will not reuse the metadata from the prepared\n\t// statement.\n\t//\n\t// See https://issues.apache.org/jira/browse/CASSANDRA-10786\n\tDisableSkipMetadata bool\n\n\t// QueryObserver will set the provided query observer on all queries created from this session.\n\t// Use it to collect metrics / stats from queries by providing an implementation of QueryObserver.\n\tQueryObserver QueryObserver\n\n\t// BatchObserver will set the provided batch observer on all queries created from this session.\n\t// Use it to collect metrics / stats from batch queries by providing an implementation of BatchObserver.\n\tBatchObserver BatchObserver\n\n\t// ConnectObserver will set the provided connect observer on all queries\n\t// created from this session.\n\tConnectObserver ConnectObserver\n\n\t// FrameHeaderObserver will set the provided frame header observer on all frames' headers created from this session.\n\t// Use it to collect metrics / stats from frames by providing an implementation of FrameHeaderObserver.\n\tFrameHeaderObserver FrameHeaderObserver\n\n\t// StreamObserver will be notified of stream state changes.\n\t// This can be used to track in-flight protocol requests and responses.\n\tStreamObserver StreamObserver\n\n\t// Default idempotence for queries\n\tDefaultIdempotence bool\n\n\t// The time to wait for frames before flushing the frames connection to Cassandra.\n\t// Can help reduce syscall overhead by making less calls to write. Set to 0 to\n\t// disable.\n\t//\n\t// (default: 200 microseconds)\n\tWriteCoalesceWaitTime time.Duration\n\n\t// Dialer will be used to establish all connections created for this Cluster.\n\t// If not provided, a default dialer configured with ConnectTimeout will be used.\n\t// Dialer is ignored if HostDialer is provided.\n\tDialer Dialer\n\n\t// HostDialer will be used to establish all connections for this Cluster.\n\t// If not provided, Dialer will be used instead.\n\tHostDialer HostDialer\n\n\t// Logger for this ClusterConfig.\n\t// If not specified, defaults to the global gocql.Logger.\n\tLogger StdLogger\n\n\t// internal config for testing\n\tdisableControlConn bool\n}\n\ntype Dialer interface {\n\tDialContext(ctx context.Context, network, addr string) (net.Conn, error)\n}\n\n// NewCluster generates a new config for the default cluster implementation.\n//\n// The supplied hosts are used to initially connect to the cluster then the rest of\n// the ring will be automatically discovered. It is recommended to use the value set in\n// the Cassandra config for broadcast_address or listen_address, an IP address not\n// a domain name. This is because events from Cassandra will use the configured IP\n// address, which is used to index connected hosts. If the domain name specified\n// resolves to more than 1 IP address then the driver may connect multiple times to\n// the same host, and will not mark the node being down or up from events.\nfunc NewCluster(hosts ...string) *ClusterConfig {\n\tcfg := &ClusterConfig{\n\t\tHosts:                  hosts,\n\t\tCQLVersion:             \"3.0.0\",\n\t\tTimeout:                11 * time.Second,\n\t\tConnectTimeout:         11 * time.Second,\n\t\tPort:                   9042,\n\t\tNumConns:               2,\n\t\tConsistency:            Quorum,\n\t\tMaxPreparedStmts:       defaultMaxPreparedStmts,\n\t\tMaxRoutingKeyInfo:      1000,\n\t\tPageSize:               5000,\n\t\tDefaultTimestamp:       true,\n\t\tMaxWaitSchemaAgreement: 60 * time.Second,\n\t\tReconnectInterval:      60 * time.Second,\n\t\tConvictionPolicy:       &SimpleConvictionPolicy{},\n\t\tReconnectionPolicy:     &ConstantReconnectionPolicy{MaxRetries: 3, Interval: 1 * time.Second},\n\t\tWriteCoalesceWaitTime:  200 * time.Microsecond,\n\t}\n\treturn cfg\n}\n\nfunc (cfg *ClusterConfig) logger() StdLogger {\n\tif cfg.Logger == nil {\n\t\treturn Logger\n\t}\n\treturn cfg.Logger\n}\n\n// CreateSession initializes the cluster based on this config and returns a\n// session object that can be used to interact with the database.\nfunc (cfg *ClusterConfig) CreateSession() (*Session, error) {\n\treturn NewSession(*cfg)\n}\n\n// translateAddressPort is a helper method that will use the given AddressTranslator\n// if defined, to translate the given address and port into a possibly new address\n// and port, If no AddressTranslator or if an error occurs, the given address and\n// port will be returned.\nfunc (cfg *ClusterConfig) translateAddressPort(addr net.IP, port int) (net.IP, int) {\n\tif cfg.AddressTranslator == nil || len(addr) == 0 {\n\t\treturn addr, port\n\t}\n\tnewAddr, newPort := cfg.AddressTranslator.Translate(addr, port)\n\tif gocqlDebug {\n\t\tcfg.logger().Printf(\"gocql: translating address '%v:%d' to '%v:%d'\", addr, port, newAddr, newPort)\n\t}\n\treturn newAddr, newPort\n}\n\nfunc (cfg *ClusterConfig) filterHost(host *HostInfo) bool {\n\treturn !(cfg.HostFilter == nil || cfg.HostFilter.Accept(host))\n}\n\nvar (\n\tErrNoHosts              = errors.New(\"no hosts provided\")\n\tErrNoConnectionsStarted = errors.New(\"no connections were made when creating the session\")\n\tErrHostQueryFailed      = errors.New(\"unable to populate Hosts\")\n)\n"
        },
        {
          "name": "cluster_test.go",
          "type": "blob",
          "size": 3.6611328125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestNewCluster_Defaults(t *testing.T) {\n\tcfg := NewCluster()\n\tassertEqual(t, \"cluster config cql version\", \"3.0.0\", cfg.CQLVersion)\n\tassertEqual(t, \"cluster config timeout\", 11*time.Second, cfg.Timeout)\n\tassertEqual(t, \"cluster config port\", 9042, cfg.Port)\n\tassertEqual(t, \"cluster config num-conns\", 2, cfg.NumConns)\n\tassertEqual(t, \"cluster config consistency\", Quorum, cfg.Consistency)\n\tassertEqual(t, \"cluster config max prepared statements\", defaultMaxPreparedStmts, cfg.MaxPreparedStmts)\n\tassertEqual(t, \"cluster config max routing key info\", 1000, cfg.MaxRoutingKeyInfo)\n\tassertEqual(t, \"cluster config page-size\", 5000, cfg.PageSize)\n\tassertEqual(t, \"cluster config default timestamp\", true, cfg.DefaultTimestamp)\n\tassertEqual(t, \"cluster config max wait schema agreement\", 60*time.Second, cfg.MaxWaitSchemaAgreement)\n\tassertEqual(t, \"cluster config reconnect interval\", 60*time.Second, cfg.ReconnectInterval)\n\tassertTrue(t, \"cluster config conviction policy\",\n\t\treflect.DeepEqual(&SimpleConvictionPolicy{}, cfg.ConvictionPolicy))\n\tassertTrue(t, \"cluster config reconnection policy\",\n\t\treflect.DeepEqual(&ConstantReconnectionPolicy{MaxRetries: 3, Interval: 1 * time.Second}, cfg.ReconnectionPolicy))\n}\n\nfunc TestNewCluster_WithHosts(t *testing.T) {\n\tcfg := NewCluster(\"addr1\", \"addr2\")\n\tassertEqual(t, \"cluster config hosts length\", 2, len(cfg.Hosts))\n\tassertEqual(t, \"cluster config host 0\", \"addr1\", cfg.Hosts[0])\n\tassertEqual(t, \"cluster config host 1\", \"addr2\", cfg.Hosts[1])\n}\n\nfunc TestClusterConfig_translateAddressAndPort_NilTranslator(t *testing.T) {\n\tcfg := NewCluster()\n\tassertNil(t, \"cluster config address translator\", cfg.AddressTranslator)\n\tnewAddr, newPort := cfg.translateAddressPort(net.ParseIP(\"10.0.0.1\"), 1234)\n\tassertTrue(t, \"same address as provided\", net.ParseIP(\"10.0.0.1\").Equal(newAddr))\n\tassertEqual(t, \"translated host and port\", 1234, newPort)\n}\n\nfunc TestClusterConfig_translateAddressAndPort_EmptyAddr(t *testing.T) {\n\tcfg := NewCluster()\n\tcfg.AddressTranslator = staticAddressTranslator(net.ParseIP(\"10.10.10.10\"), 5432)\n\tnewAddr, newPort := cfg.translateAddressPort(net.IP([]byte{}), 0)\n\tassertTrue(t, \"translated address is still empty\", len(newAddr) == 0)\n\tassertEqual(t, \"translated port\", 0, newPort)\n}\n\nfunc TestClusterConfig_translateAddressAndPort_Success(t *testing.T) {\n\tcfg := NewCluster()\n\tcfg.AddressTranslator = staticAddressTranslator(net.ParseIP(\"10.10.10.10\"), 5432)\n\tnewAddr, newPort := cfg.translateAddressPort(net.ParseIP(\"10.0.0.1\"), 2345)\n\tassertTrue(t, \"translated address\", net.ParseIP(\"10.10.10.10\").Equal(newAddr))\n\tassertEqual(t, \"translated port\", 5432, newPort)\n}\n"
        },
        {
          "name": "common_test.go",
          "type": "blob",
          "size": 9.166015625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n)\n\nvar (\n\tflagCluster      = flag.String(\"cluster\", \"127.0.0.1\", \"a comma-separated list of host:port tuples\")\n\tflagProto        = flag.Int(\"proto\", 0, \"protcol version\")\n\tflagCQL          = flag.String(\"cql\", \"3.0.0\", \"CQL version\")\n\tflagRF           = flag.Int(\"rf\", 1, \"replication factor for test keyspace\")\n\tclusterSize      = flag.Int(\"clusterSize\", 1, \"the expected size of the cluster\")\n\tflagRetry        = flag.Int(\"retries\", 5, \"number of times to retry queries\")\n\tflagAutoWait     = flag.Duration(\"autowait\", 1000*time.Millisecond, \"time to wait for autodiscovery to fill the hosts poll\")\n\tflagRunSslTest   = flag.Bool(\"runssl\", false, \"Set to true to run ssl test\")\n\tflagRunAuthTest  = flag.Bool(\"runauth\", false, \"Set to true to run authentication test\")\n\tflagCompressTest = flag.String(\"compressor\", \"\", \"compressor to use\")\n\tflagTimeout      = flag.Duration(\"gocql.timeout\", 5*time.Second, \"sets the connection `timeout` for all operations\")\n\n\tflagCassVersion cassVersion\n)\n\nfunc init() {\n\tflag.Var(&flagCassVersion, \"gocql.cversion\", \"the cassandra version being tested against\")\n\n\tlog.SetFlags(log.Lshortfile | log.LstdFlags)\n}\n\nfunc getClusterHosts() []string {\n\treturn strings.Split(*flagCluster, \",\")\n}\n\nfunc addSslOptions(cluster *ClusterConfig) *ClusterConfig {\n\tif *flagRunSslTest {\n\t\tcluster.SslOpts = &SslOptions{\n\t\t\tCertPath:               \"testdata/pki/gocql.crt\",\n\t\t\tKeyPath:                \"testdata/pki/gocql.key\",\n\t\t\tCaPath:                 \"testdata/pki/ca.crt\",\n\t\t\tEnableHostVerification: false,\n\t\t}\n\t}\n\treturn cluster\n}\n\nvar initOnce sync.Once\n\nfunc createTable(s *Session, table string) error {\n\t// lets just be really sure\n\tif err := s.control.awaitSchemaAgreement(); err != nil {\n\t\tlog.Printf(\"error waiting for schema agreement pre create table=%q err=%v\\n\", table, err)\n\t\treturn err\n\t}\n\n\tif err := s.Query(table).RetryPolicy(&SimpleRetryPolicy{}).Exec(); err != nil {\n\t\tlog.Printf(\"error creating table table=%q err=%v\\n\", table, err)\n\t\treturn err\n\t}\n\n\tif err := s.control.awaitSchemaAgreement(); err != nil {\n\t\tlog.Printf(\"error waiting for schema agreement post create table=%q err=%v\\n\", table, err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc createCluster(opts ...func(*ClusterConfig)) *ClusterConfig {\n\tclusterHosts := getClusterHosts()\n\tcluster := NewCluster(clusterHosts...)\n\tcluster.ProtoVersion = *flagProto\n\tcluster.CQLVersion = *flagCQL\n\tcluster.Timeout = *flagTimeout\n\tcluster.Consistency = Quorum\n\tcluster.MaxWaitSchemaAgreement = 2 * time.Minute // travis might be slow\n\tif *flagRetry > 0 {\n\t\tcluster.RetryPolicy = &SimpleRetryPolicy{NumRetries: *flagRetry}\n\t}\n\n\tswitch *flagCompressTest {\n\tcase \"snappy\":\n\t\tcluster.Compressor = &SnappyCompressor{}\n\tcase \"\":\n\tdefault:\n\t\tpanic(\"invalid compressor: \" + *flagCompressTest)\n\t}\n\n\tcluster = addSslOptions(cluster)\n\n\tfor _, opt := range opts {\n\t\topt(cluster)\n\t}\n\n\treturn cluster\n}\n\nfunc createKeyspace(tb testing.TB, cluster *ClusterConfig, keyspace string) {\n\t// TODO: tb.Helper()\n\tc := *cluster\n\tc.Keyspace = \"system\"\n\tc.Timeout = 30 * time.Second\n\tsession, err := c.CreateSession()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer session.Close()\n\n\terr = createTable(session, `DROP KEYSPACE IF EXISTS `+keyspace)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"unable to drop keyspace: %v\", err))\n\t}\n\n\terr = createTable(session, fmt.Sprintf(`CREATE KEYSPACE %s\n\tWITH replication = {\n\t\t'class' : 'SimpleStrategy',\n\t\t'replication_factor' : %d\n\t}`, keyspace, *flagRF))\n\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"unable to create keyspace: %v\", err))\n\t}\n}\n\nfunc createSessionFromCluster(cluster *ClusterConfig, tb testing.TB) *Session {\n\t// Drop and re-create the keyspace once. Different tests should use their own\n\t// individual tables, but can assume that the table does not exist before.\n\tinitOnce.Do(func() {\n\t\tcreateKeyspace(tb, cluster, \"gocql_test\")\n\t})\n\n\tcluster.Keyspace = \"gocql_test\"\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\ttb.Fatal(\"createSession:\", err)\n\t}\n\n\tif err := session.control.awaitSchemaAgreement(); err != nil {\n\t\ttb.Fatal(err)\n\t}\n\n\treturn session\n}\n\nfunc createSession(tb testing.TB, opts ...func(config *ClusterConfig)) *Session {\n\tcluster := createCluster(opts...)\n\treturn createSessionFromCluster(cluster, tb)\n}\n\nfunc createViews(t *testing.T, session *Session) {\n\tif err := session.Query(`\n\t\tCREATE TYPE IF NOT EXISTS gocql_test.basicView (\n\t\tbirthday timestamp,\n\t\tnationality text,\n\t\tweight text,\n\t\theight text);\t`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create view with err: %v\", err)\n\t}\n}\n\nfunc createMaterializedViews(t *testing.T, session *Session) {\n\tif flagCassVersion.Before(3, 0, 0) {\n\t\treturn\n\t}\n\tif err := session.Query(`CREATE TABLE IF NOT EXISTS gocql_test.view_table (\n\t\t    userid text,\n\t\t    year int,\n\t\t    month int,\n    \t\t    PRIMARY KEY (userid));`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create materialized view with err: %v\", err)\n\t}\n\tif err := session.Query(`CREATE TABLE IF NOT EXISTS gocql_test.view_table2 (\n\t\t    userid text,\n\t\t    year int,\n\t\t    month int,\n    \t\t    PRIMARY KEY (userid));`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create materialized view with err: %v\", err)\n\t}\n\tif err := session.Query(`CREATE MATERIALIZED VIEW IF NOT EXISTS gocql_test.view_view AS\n\t\t   SELECT year, month, userid\n\t\t   FROM gocql_test.view_table\n\t\t   WHERE year IS NOT NULL AND month IS NOT NULL AND userid IS NOT NULL\n\t\t   PRIMARY KEY (userid, year);`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create materialized view with err: %v\", err)\n\t}\n\tif err := session.Query(`CREATE MATERIALIZED VIEW IF NOT EXISTS gocql_test.view_view2 AS\n\t\t   SELECT year, month, userid\n\t\t   FROM gocql_test.view_table2\n\t\t   WHERE year IS NOT NULL AND month IS NOT NULL AND userid IS NOT NULL\n\t\t   PRIMARY KEY (userid, year);`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create materialized view with err: %v\", err)\n\t}\n}\n\nfunc createFunctions(t *testing.T, session *Session) {\n\tif err := session.Query(`\n\t\tCREATE OR REPLACE FUNCTION gocql_test.avgState ( state tuple<int,bigint>, val int )\n\t\tCALLED ON NULL INPUT\n\t\tRETURNS tuple<int,bigint>\n\t\tLANGUAGE java AS\n\t\t$$if (val !=null) {state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue());}return state;$$;\t`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create function with err: %v\", err)\n\t}\n\tif err := session.Query(`\n\t\tCREATE OR REPLACE FUNCTION gocql_test.avgFinal ( state tuple<int,bigint> )\n\t\tCALLED ON NULL INPUT\n\t\tRETURNS double\n\t\tLANGUAGE java AS\n\t\t$$double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);$$ \n\t`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create function with err: %v\", err)\n\t}\n}\n\nfunc createAggregate(t *testing.T, session *Session) {\n\tcreateFunctions(t, session)\n\tif err := session.Query(`\n\t\tCREATE OR REPLACE AGGREGATE gocql_test.average(int)\n\t\tSFUNC avgState\n\t\tSTYPE tuple<int,bigint>\n\t\tFINALFUNC avgFinal\n\t\tINITCOND (0,0);\n\t`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create aggregate with err: %v\", err)\n\t}\n\tif err := session.Query(`\n\t\tCREATE OR REPLACE AGGREGATE gocql_test.average2(int)\n\t\tSFUNC avgState\n\t\tSTYPE tuple<int,bigint>\n\t\tFINALFUNC avgFinal\n\t\tINITCOND (0,0);\n\t`).Exec(); err != nil {\n\t\tt.Fatalf(\"failed to create aggregate with err: %v\", err)\n\t}\n}\n\nfunc staticAddressTranslator(newAddr net.IP, newPort int) AddressTranslator {\n\treturn AddressTranslatorFunc(func(addr net.IP, port int) (net.IP, int) {\n\t\treturn newAddr, newPort\n\t})\n}\n\nfunc assertTrue(t *testing.T, description string, value bool) {\n\tt.Helper()\n\tif !value {\n\t\tt.Fatalf(\"expected %s to be true\", description)\n\t}\n}\n\nfunc assertEqual(t *testing.T, description string, expected, actual interface{}) {\n\tt.Helper()\n\tif expected != actual {\n\t\tt.Fatalf(\"expected %s to be (%+v) but was (%+v) instead\", description, expected, actual)\n\t}\n}\n\nfunc assertDeepEqual(t *testing.T, description string, expected, actual interface{}) {\n\tt.Helper()\n\tif !reflect.DeepEqual(expected, actual) {\n\t\tt.Fatalf(\"expected %s to be (%+v) but was (%+v) instead\", description, expected, actual)\n\t}\n}\n\nfunc assertNil(t *testing.T, description string, actual interface{}) {\n\tt.Helper()\n\tif actual != nil {\n\t\tt.Fatalf(\"expected %s to be (nil) but was (%+v) instead\", description, actual)\n\t}\n}\n"
        },
        {
          "name": "compressor.go",
          "type": "blob",
          "size": 1.6845703125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"github.com/golang/snappy\"\n)\n\ntype Compressor interface {\n\tName() string\n\tEncode(data []byte) ([]byte, error)\n\tDecode(data []byte) ([]byte, error)\n}\n\n// SnappyCompressor implements the Compressor interface and can be used to\n// compress incoming and outgoing frames. The snappy compression algorithm\n// aims for very high speeds and reasonable compression.\ntype SnappyCompressor struct{}\n\nfunc (s SnappyCompressor) Name() string {\n\treturn \"snappy\"\n}\n\nfunc (s SnappyCompressor) Encode(data []byte) ([]byte, error) {\n\treturn snappy.Encode(nil, data), nil\n}\n\nfunc (s SnappyCompressor) Decode(data []byte) ([]byte, error) {\n\treturn snappy.Decode(nil, data)\n}\n"
        },
        {
          "name": "compressor_test.go",
          "type": "blob",
          "size": 2.0712890625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n\n\t\"github.com/golang/snappy\"\n)\n\nfunc TestSnappyCompressor(t *testing.T) {\n\tc := SnappyCompressor{}\n\tif c.Name() != \"snappy\" {\n\t\tt.Fatalf(\"expected name to be 'snappy', got %v\", c.Name())\n\t}\n\n\tstr := \"My Test String\"\n\t//Test Encoding\n\texpected := snappy.Encode(nil, []byte(str))\n\tif res, err := c.Encode([]byte(str)); err != nil {\n\t\tt.Fatalf(\"failed to encode '%v' with error %v\", str, err)\n\t} else if bytes.Compare(expected, res) != 0 {\n\t\tt.Fatal(\"failed to match the expected encoded value with the result encoded value.\")\n\t}\n\n\tval, err := c.Encode([]byte(str))\n\tif err != nil {\n\t\tt.Fatalf(\"failed to encode '%v' with error '%v'\", str, err)\n\t}\n\n\t//Test Decoding\n\tif expected, err := snappy.Decode(nil, val); err != nil {\n\t\tt.Fatalf(\"failed to decode '%v' with error %v\", val, err)\n\t} else if res, err := c.Decode(val); err != nil {\n\t\tt.Fatalf(\"failed to decode '%v' with error %v\", val, err)\n\t} else if bytes.Compare(expected, res) != 0 {\n\t\tt.Fatal(\"failed to match the expected decoded value with the result decoded value.\")\n\t}\n}\n"
        },
        {
          "name": "conn.go",
          "type": "blob",
          "size": 44.9248046875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/gocql/gocql/internal/lru\"\n\t\"github.com/gocql/gocql/internal/streams\"\n)\n\n// approve the authenticator with the list of allowed authenticators. If the provided list is empty,\n// the given authenticator is allowed.\nfunc approve(authenticator string, approvedAuthenticators []string) bool {\n\tif len(approvedAuthenticators) == 0 {\n\t\treturn true\n\t}\n\tfor _, s := range approvedAuthenticators {\n\t\tif authenticator == s {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// JoinHostPort is a utility to return an address string that can be used\n// by `gocql.Conn` to form a connection with a host.\nfunc JoinHostPort(addr string, port int) string {\n\taddr = strings.TrimSpace(addr)\n\tif _, _, err := net.SplitHostPort(addr); err != nil {\n\t\taddr = net.JoinHostPort(addr, strconv.Itoa(port))\n\t}\n\treturn addr\n}\n\ntype Authenticator interface {\n\tChallenge(req []byte) (resp []byte, auth Authenticator, err error)\n\tSuccess(data []byte) error\n}\n\n// PasswordAuthenticator specifies credentials to be used when authenticating.\n// It can be configured with an \"allow list\" of authenticator class names to avoid\n// attempting to authenticate with Cassandra if it doesn't provide an expected authenticator.\ntype PasswordAuthenticator struct {\n\tUsername string\n\tPassword string\n\t// Setting this to nil or empty will allow authenticating with any authenticator\n\t// provided by the server.  This is the default behavior of most other driver\n\t// implementations.\n\tAllowedAuthenticators []string\n}\n\nfunc (p PasswordAuthenticator) Challenge(req []byte) ([]byte, Authenticator, error) {\n\tif !approve(string(req), p.AllowedAuthenticators) {\n\t\treturn nil, nil, fmt.Errorf(\"unexpected authenticator %q\", req)\n\t}\n\tresp := make([]byte, 2+len(p.Username)+len(p.Password))\n\tresp[0] = 0\n\tcopy(resp[1:], p.Username)\n\tresp[len(p.Username)+1] = 0\n\tcopy(resp[2+len(p.Username):], p.Password)\n\treturn resp, nil, nil\n}\n\nfunc (p PasswordAuthenticator) Success(data []byte) error {\n\treturn nil\n}\n\n// SslOptions configures TLS use.\n//\n// Warning: Due to historical reasons, the SslOptions is insecure by default, so you need to set EnableHostVerification\n// to true if no Config is set. Most users should set SslOptions.Config to a *tls.Config.\n// SslOptions and Config.InsecureSkipVerify interact as follows:\n//\n//\tConfig.InsecureSkipVerify | EnableHostVerification | Result\n//\tConfig is nil             | false                  | do not verify host\n//\tConfig is nil             | true                   | verify host\n//\tfalse                     | false                  | verify host\n//\ttrue                      | false                  | do not verify host\n//\tfalse                     | true                   | verify host\n//\ttrue                      | true                   | verify host\ntype SslOptions struct {\n\t*tls.Config\n\n\t// CertPath and KeyPath are optional depending on server\n\t// config, but both fields must be omitted to avoid using a\n\t// client certificate\n\tCertPath string\n\tKeyPath  string\n\tCaPath   string //optional depending on server config\n\t// If you want to verify the hostname and server cert (like a wildcard for cass cluster) then you should turn this\n\t// on.\n\t// This option is basically the inverse of tls.Config.InsecureSkipVerify.\n\t// See InsecureSkipVerify in http://golang.org/pkg/crypto/tls/ for more info.\n\t//\n\t// See SslOptions documentation to see how EnableHostVerification interacts with the provided tls.Config.\n\tEnableHostVerification bool\n}\n\ntype ConnConfig struct {\n\tProtoVersion   int\n\tCQLVersion     string\n\tTimeout        time.Duration\n\tWriteTimeout   time.Duration\n\tConnectTimeout time.Duration\n\tDialer         Dialer\n\tHostDialer     HostDialer\n\tCompressor     Compressor\n\tAuthenticator  Authenticator\n\tAuthProvider   func(h *HostInfo) (Authenticator, error)\n\tKeepalive      time.Duration\n\tLogger         StdLogger\n\n\ttlsConfig       *tls.Config\n\tdisableCoalesce bool\n}\n\nfunc (c *ConnConfig) logger() StdLogger {\n\tif c.Logger == nil {\n\t\treturn Logger\n\t}\n\treturn c.Logger\n}\n\ntype ConnErrorHandler interface {\n\tHandleError(conn *Conn, err error, closed bool)\n}\n\ntype connErrorHandlerFn func(conn *Conn, err error, closed bool)\n\nfunc (fn connErrorHandlerFn) HandleError(conn *Conn, err error, closed bool) {\n\tfn(conn, err, closed)\n}\n\n// If not zero, how many timeouts we will allow to occur before the connection is closed\n// and restarted. This is to prevent a single query timeout from killing a connection\n// which may be serving more queries just fine.\n// Default is 0, should not be changed concurrently with queries.\n//\n// Deprecated.\nvar TimeoutLimit int64 = 0\n\n// Conn is a single connection to a Cassandra node. It can be used to execute\n// queries, but users are usually advised to use a more reliable, higher\n// level API.\ntype Conn struct {\n\tconn net.Conn\n\tr    *bufio.Reader\n\tw    contextWriter\n\n\ttimeout        time.Duration\n\twriteTimeout   time.Duration\n\tcfg            *ConnConfig\n\tframeObserver  FrameHeaderObserver\n\tstreamObserver StreamObserver\n\n\theaderBuf [maxFrameHeaderSize]byte\n\n\tstreams *streams.IDGenerator\n\tmu      sync.Mutex\n\t// calls stores a map from stream ID to callReq.\n\t// This map is protected by mu.\n\t// calls should not be used when closed is true, calls is set to nil when closed=true.\n\tcalls map[int]*callReq\n\n\terrorHandler ConnErrorHandler\n\tcompressor   Compressor\n\tauth         Authenticator\n\taddr         string\n\n\tversion         uint8\n\tcurrentKeyspace string\n\thost            *HostInfo\n\tisSchemaV2      bool\n\n\tsession *Session\n\n\t// true if connection close process for the connection started.\n\t// closed is protected by mu.\n\tclosed bool\n\tctx    context.Context\n\tcancel context.CancelFunc\n\n\ttimeouts int64\n\n\tlogger StdLogger\n}\n\n// connect establishes a connection to a Cassandra node using session's connection config.\nfunc (s *Session) connect(ctx context.Context, host *HostInfo, errorHandler ConnErrorHandler) (*Conn, error) {\n\treturn s.dial(ctx, host, s.connCfg, errorHandler)\n}\n\n// dial establishes a connection to a Cassandra node and notifies the session's connectObserver.\nfunc (s *Session) dial(ctx context.Context, host *HostInfo, connConfig *ConnConfig, errorHandler ConnErrorHandler) (*Conn, error) {\n\tvar obs ObservedConnect\n\tif s.connectObserver != nil {\n\t\tobs.Host = host\n\t\tobs.Start = time.Now()\n\t}\n\n\tconn, err := s.dialWithoutObserver(ctx, host, connConfig, errorHandler)\n\n\tif s.connectObserver != nil {\n\t\tobs.End = time.Now()\n\t\tobs.Err = err\n\t\ts.connectObserver.ObserveConnect(obs)\n\t}\n\n\treturn conn, err\n}\n\n// dialWithoutObserver establishes connection to a Cassandra node.\n//\n// dialWithoutObserver does not notify the connection observer, so you most probably want to call dial() instead.\nfunc (s *Session) dialWithoutObserver(ctx context.Context, host *HostInfo, cfg *ConnConfig, errorHandler ConnErrorHandler) (*Conn, error) {\n\tdialedHost, err := cfg.HostDialer.DialHost(ctx, host)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\twriteTimeout := cfg.Timeout\n\tif cfg.WriteTimeout > 0 {\n\t\twriteTimeout = cfg.WriteTimeout\n\t}\n\n\tctx, cancel := context.WithCancel(ctx)\n\tc := &Conn{\n\t\tconn:          dialedHost.Conn,\n\t\tr:             bufio.NewReader(dialedHost.Conn),\n\t\tcfg:           cfg,\n\t\tcalls:         make(map[int]*callReq),\n\t\tversion:       uint8(cfg.ProtoVersion),\n\t\taddr:          dialedHost.Conn.RemoteAddr().String(),\n\t\terrorHandler:  errorHandler,\n\t\tcompressor:    cfg.Compressor,\n\t\tsession:       s,\n\t\tstreams:       streams.New(cfg.ProtoVersion),\n\t\thost:          host,\n\t\tisSchemaV2:    true, // Try using \"system.peers_v2\" until proven otherwise\n\t\tframeObserver: s.frameObserver,\n\t\tw: &deadlineContextWriter{\n\t\t\tw:         dialedHost.Conn,\n\t\t\ttimeout:   writeTimeout,\n\t\t\tsemaphore: make(chan struct{}, 1),\n\t\t\tquit:      make(chan struct{}),\n\t\t},\n\t\tctx:            ctx,\n\t\tcancel:         cancel,\n\t\tlogger:         cfg.logger(),\n\t\tstreamObserver: s.streamObserver,\n\t\twriteTimeout:   writeTimeout,\n\t}\n\n\tif err := c.init(ctx, dialedHost); err != nil {\n\t\tcancel()\n\t\tc.Close()\n\t\treturn nil, err\n\t}\n\n\treturn c, nil\n}\n\nfunc (c *Conn) init(ctx context.Context, dialedHost *DialedHost) error {\n\tif c.session.cfg.AuthProvider != nil {\n\t\tvar err error\n\t\tc.auth, err = c.cfg.AuthProvider(c.host)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tc.auth = c.cfg.Authenticator\n\t}\n\n\tstartup := &startupCoordinator{\n\t\tframeTicker: make(chan struct{}),\n\t\tconn:        c,\n\t}\n\n\tc.timeout = c.cfg.ConnectTimeout\n\tif err := startup.setupConn(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tc.timeout = c.cfg.Timeout\n\n\t// dont coalesce startup frames\n\tif c.session.cfg.WriteCoalesceWaitTime > 0 && !c.cfg.disableCoalesce && !dialedHost.DisableCoalesce {\n\t\tc.w = newWriteCoalescer(c.conn, c.writeTimeout, c.session.cfg.WriteCoalesceWaitTime, ctx.Done())\n\t}\n\n\tgo c.serve(ctx)\n\tgo c.heartBeat(ctx)\n\n\treturn nil\n}\n\nfunc (c *Conn) Write(p []byte) (n int, err error) {\n\treturn c.w.writeContext(context.Background(), p)\n}\n\nfunc (c *Conn) Read(p []byte) (n int, err error) {\n\tconst maxAttempts = 5\n\n\tfor i := 0; i < maxAttempts; i++ {\n\t\tvar nn int\n\t\tif c.timeout > 0 {\n\t\t\tc.conn.SetReadDeadline(time.Now().Add(c.timeout))\n\t\t}\n\n\t\tnn, err = io.ReadFull(c.r, p[n:])\n\t\tn += nn\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\n\t\tif verr, ok := err.(net.Error); !ok || !verr.Temporary() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn\n}\n\ntype startupCoordinator struct {\n\tconn        *Conn\n\tframeTicker chan struct{}\n}\n\nfunc (s *startupCoordinator) setupConn(ctx context.Context) error {\n\tvar cancel context.CancelFunc\n\tif s.conn.timeout > 0 {\n\t\tctx, cancel = context.WithTimeout(ctx, s.conn.timeout)\n\t} else {\n\t\tctx, cancel = context.WithCancel(ctx)\n\t}\n\tdefer cancel()\n\n\tstartupErr := make(chan error)\n\tgo func() {\n\t\tfor range s.frameTicker {\n\t\t\terr := s.conn.recv(ctx)\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase startupErr <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t}\n\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\tgo func() {\n\t\tdefer close(s.frameTicker)\n\t\terr := s.options(ctx)\n\t\tselect {\n\t\tcase startupErr <- err:\n\t\tcase <-ctx.Done():\n\t\t}\n\t}()\n\n\tselect {\n\tcase err := <-startupErr:\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\tcase <-ctx.Done():\n\t\treturn errors.New(\"gocql: no response to connection startup within timeout\")\n\t}\n\n\treturn nil\n}\n\nfunc (s *startupCoordinator) write(ctx context.Context, frame frameBuilder) (frame, error) {\n\tselect {\n\tcase s.frameTicker <- struct{}{}:\n\tcase <-ctx.Done():\n\t\treturn nil, ctx.Err()\n\t}\n\n\tframer, err := s.conn.exec(ctx, frame, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn framer.parseFrame()\n}\n\nfunc (s *startupCoordinator) options(ctx context.Context) error {\n\tframe, err := s.write(ctx, &writeOptionsFrame{})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsupported, ok := frame.(*supportedFrame)\n\tif !ok {\n\t\treturn NewErrProtocol(\"Unknown type of response to startup frame: %T\", frame)\n\t}\n\n\treturn s.startup(ctx, supported.supported)\n}\n\nfunc (s *startupCoordinator) startup(ctx context.Context, supported map[string][]string) error {\n\tm := map[string]string{\n\t\t\"CQL_VERSION\":    s.conn.cfg.CQLVersion,\n\t\t\"DRIVER_NAME\":    driverName,\n\t\t\"DRIVER_VERSION\": driverVersion,\n\t}\n\n\tif s.conn.compressor != nil {\n\t\tcomp := supported[\"COMPRESSION\"]\n\t\tname := s.conn.compressor.Name()\n\t\tfor _, compressor := range comp {\n\t\t\tif compressor == name {\n\t\t\t\tm[\"COMPRESSION\"] = compressor\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif _, ok := m[\"COMPRESSION\"]; !ok {\n\t\t\ts.conn.compressor = nil\n\t\t}\n\t}\n\n\tframe, err := s.write(ctx, &writeStartupFrame{opts: m})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tswitch v := frame.(type) {\n\tcase error:\n\t\treturn v\n\tcase *readyFrame:\n\t\treturn nil\n\tcase *authenticateFrame:\n\t\treturn s.authenticateHandshake(ctx, v)\n\tdefault:\n\t\treturn NewErrProtocol(\"Unknown type of response to startup frame: %s\", v)\n\t}\n}\n\nfunc (s *startupCoordinator) authenticateHandshake(ctx context.Context, authFrame *authenticateFrame) error {\n\tif s.conn.auth == nil {\n\t\treturn fmt.Errorf(\"authentication required (using %q)\", authFrame.class)\n\t}\n\n\tresp, challenger, err := s.conn.auth.Challenge([]byte(authFrame.class))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq := &writeAuthResponseFrame{data: resp}\n\tfor {\n\t\tframe, err := s.write(ctx, req)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tswitch v := frame.(type) {\n\t\tcase error:\n\t\t\treturn v\n\t\tcase *authSuccessFrame:\n\t\t\tif challenger != nil {\n\t\t\t\treturn challenger.Success(v.data)\n\t\t\t}\n\t\t\treturn nil\n\t\tcase *authChallengeFrame:\n\t\t\tresp, challenger, err = challenger.Challenge(v.data)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treq = &writeAuthResponseFrame{\n\t\t\t\tdata: resp,\n\t\t\t}\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown frame response during authentication: %v\", v)\n\t\t}\n\t}\n}\n\nfunc (c *Conn) closeWithError(err error) {\n\tif c == nil {\n\t\treturn\n\t}\n\n\tc.mu.Lock()\n\tif c.closed {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tc.closed = true\n\n\tvar callsToClose map[int]*callReq\n\n\t// We should attempt to deliver the error back to the caller if it\n\t// exists. However, don't block c.mu while we are delivering the\n\t// error to outstanding calls.\n\tif err != nil {\n\t\tcallsToClose = c.calls\n\t\t// It is safe to change c.calls to nil. Nobody should use it after c.closed is set to true.\n\t\tc.calls = nil\n\t}\n\tc.mu.Unlock()\n\n\tfor _, req := range callsToClose {\n\t\t// we need to send the error to all waiting queries.\n\t\tselect {\n\t\tcase req.resp <- callResp{err: err}:\n\t\tcase <-req.timeout:\n\t\t}\n\t\tif req.streamObserverContext != nil {\n\t\t\treq.streamObserverEndOnce.Do(func() {\n\t\t\t\treq.streamObserverContext.StreamAbandoned(ObservedStream{\n\t\t\t\t\tHost: c.host,\n\t\t\t\t})\n\t\t\t})\n\t\t}\n\t}\n\n\t// if error was nil then unblock the quit channel\n\tc.cancel()\n\tcerr := c.close()\n\n\tif err != nil {\n\t\tc.errorHandler.HandleError(c, err, true)\n\t} else if cerr != nil {\n\t\t// TODO(zariel): is it a good idea to do this?\n\t\tc.errorHandler.HandleError(c, cerr, true)\n\t}\n}\n\nfunc (c *Conn) close() error {\n\treturn c.conn.Close()\n}\n\nfunc (c *Conn) Close() {\n\tc.closeWithError(nil)\n}\n\n// Serve starts the stream multiplexer for this connection, which is required\n// to execute any queries. This method runs as long as the connection is\n// open and is therefore usually called in a separate goroutine.\nfunc (c *Conn) serve(ctx context.Context) {\n\tvar err error\n\tfor err == nil {\n\t\terr = c.recv(ctx)\n\t}\n\n\tc.closeWithError(err)\n}\n\nfunc (c *Conn) discardFrame(head frameHeader) error {\n\t_, err := io.CopyN(ioutil.Discard, c, int64(head.length))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\ntype protocolError struct {\n\tframe frame\n}\n\nfunc (p *protocolError) Error() string {\n\tif err, ok := p.frame.(error); ok {\n\t\treturn err.Error()\n\t}\n\treturn fmt.Sprintf(\"gocql: received unexpected frame on stream %d: %v\", p.frame.Header().stream, p.frame)\n}\n\nfunc (c *Conn) heartBeat(ctx context.Context) {\n\tsleepTime := 1 * time.Second\n\ttimer := time.NewTimer(sleepTime)\n\tdefer timer.Stop()\n\n\tvar failures int\n\n\tfor {\n\t\tif failures > 5 {\n\t\t\tc.closeWithError(fmt.Errorf(\"gocql: heartbeat failed\"))\n\t\t\treturn\n\t\t}\n\n\t\ttimer.Reset(sleepTime)\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t}\n\n\t\tframer, err := c.exec(context.Background(), &writeOptionsFrame{}, nil)\n\t\tif err != nil {\n\t\t\tfailures++\n\t\t\tcontinue\n\t\t}\n\n\t\tresp, err := framer.parseFrame()\n\t\tif err != nil {\n\t\t\t// invalid frame\n\t\t\tfailures++\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch resp.(type) {\n\t\tcase *supportedFrame:\n\t\t\t// Everything ok\n\t\t\tsleepTime = 5 * time.Second\n\t\t\tfailures = 0\n\t\tcase error:\n\t\t\t// TODO: should we do something here?\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"gocql: unknown frame in response to options: %T\", resp))\n\t\t}\n\t}\n}\n\nfunc (c *Conn) recv(ctx context.Context) error {\n\t// not safe for concurrent reads\n\n\t// read a full header, ignore timeouts, as this is being ran in a loop\n\t// TODO: TCP level deadlines? or just query level deadlines?\n\tif c.timeout > 0 {\n\t\tc.conn.SetReadDeadline(time.Time{})\n\t}\n\n\theadStartTime := time.Now()\n\t// were just reading headers over and over and copy bodies\n\thead, err := readHeader(c.r, c.headerBuf[:])\n\theadEndTime := time.Now()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif c.frameObserver != nil {\n\t\tc.frameObserver.ObserveFrameHeader(context.Background(), ObservedFrameHeader{\n\t\t\tVersion: protoVersion(head.version),\n\t\t\tFlags:   head.flags,\n\t\t\tStream:  int16(head.stream),\n\t\t\tOpcode:  frameOp(head.op),\n\t\t\tLength:  int32(head.length),\n\t\t\tStart:   headStartTime,\n\t\t\tEnd:     headEndTime,\n\t\t\tHost:    c.host,\n\t\t})\n\t}\n\n\tif head.stream > c.streams.NumStreams {\n\t\treturn fmt.Errorf(\"gocql: frame header stream is beyond call expected bounds: %d\", head.stream)\n\t} else if head.stream == -1 {\n\t\t// TODO: handle cassandra event frames, we shouldnt get any currently\n\t\tframer := newFramer(c.compressor, c.version)\n\t\tif err := framer.readFrame(c, &head); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgo c.session.handleEvent(framer)\n\t\treturn nil\n\t} else if head.stream <= 0 {\n\t\t// reserved stream that we dont use, probably due to a protocol error\n\t\t// or a bug in Cassandra, this should be an error, parse it and return.\n\t\tframer := newFramer(c.compressor, c.version)\n\t\tif err := framer.readFrame(c, &head); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tframe, err := framer.parseFrame()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn &protocolError{\n\t\t\tframe: frame,\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tif c.closed {\n\t\tc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\tcall, ok := c.calls[head.stream]\n\tdelete(c.calls, head.stream)\n\tc.mu.Unlock()\n\tif call == nil || !ok {\n\t\tc.logger.Printf(\"gocql: received response for stream which has no handler: header=%v\\n\", head)\n\t\treturn c.discardFrame(head)\n\t} else if head.stream != call.streamID {\n\t\tpanic(fmt.Sprintf(\"call has incorrect streamID: got %d expected %d\", call.streamID, head.stream))\n\t}\n\n\tframer := newFramer(c.compressor, c.version)\n\n\terr = framer.readFrame(c, &head)\n\tif err != nil {\n\t\t// only net errors should cause the connection to be closed. Though\n\t\t// cassandra returning corrupt frames will be returned here as well.\n\t\tif _, ok := err.(net.Error); ok {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// we either, return a response to the caller, the caller timedout, or the\n\t// connection has closed. Either way we should never block indefinatly here\n\tselect {\n\tcase call.resp <- callResp{framer: framer, err: err}:\n\tcase <-call.timeout:\n\t\tc.releaseStream(call)\n\tcase <-ctx.Done():\n\t}\n\n\treturn nil\n}\n\nfunc (c *Conn) releaseStream(call *callReq) {\n\tif call.timer != nil {\n\t\tcall.timer.Stop()\n\t}\n\n\tc.streams.Clear(call.streamID)\n\n\tif call.streamObserverContext != nil {\n\t\tcall.streamObserverEndOnce.Do(func() {\n\t\t\tcall.streamObserverContext.StreamFinished(ObservedStream{\n\t\t\t\tHost: c.host,\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc (c *Conn) handleTimeout() {\n\tif TimeoutLimit > 0 && atomic.AddInt64(&c.timeouts, 1) > TimeoutLimit {\n\t\tc.closeWithError(ErrTooManyTimeouts)\n\t}\n}\n\ntype callReq struct {\n\t// resp will receive the frame that was sent as a response to this stream.\n\tresp     chan callResp\n\ttimeout  chan struct{} // indicates to recv() that a call has timed out\n\tstreamID int           // current stream in use\n\n\ttimer *time.Timer\n\n\t// streamObserverContext is notified about events regarding this stream\n\tstreamObserverContext StreamObserverContext\n\n\t// streamObserverEndOnce ensures that either StreamAbandoned or StreamFinished is called,\n\t// but not both.\n\tstreamObserverEndOnce sync.Once\n}\n\ntype callResp struct {\n\t// framer is the response frame.\n\t// May be nil if err is not nil.\n\tframer *framer\n\t// err is error encountered, if any.\n\terr error\n}\n\n// contextWriter is like io.Writer, but takes context as well.\ntype contextWriter interface {\n\t// writeContext writes p to the connection.\n\t//\n\t// If ctx is canceled before we start writing p (e.g. during waiting while another write is currently in progress),\n\t// p is not written and ctx.Err() is returned. Context is ignored after we start writing p (i.e. we don't interrupt\n\t// blocked writes that are in progress) so that we always either write the full frame or not write it at all.\n\t//\n\t// It returns the number of bytes written from p (0 <= n <= len(p)) and any error that caused the write to stop\n\t// early. writeContext must return a non-nil error if it returns n < len(p). writeContext must not modify the\n\t// data in p, even temporarily.\n\twriteContext(ctx context.Context, p []byte) (n int, err error)\n}\n\ntype deadlineWriter interface {\n\tSetWriteDeadline(time.Time) error\n\tio.Writer\n}\n\ntype deadlineContextWriter struct {\n\tw       deadlineWriter\n\ttimeout time.Duration\n\t// semaphore protects critical section for SetWriteDeadline/Write.\n\t// It is a channel with capacity 1.\n\tsemaphore chan struct{}\n\n\t// quit closed once the connection is closed.\n\tquit chan struct{}\n}\n\n// writeContext implements contextWriter.\nfunc (c *deadlineContextWriter) writeContext(ctx context.Context, p []byte) (int, error) {\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn 0, ctx.Err()\n\tcase <-c.quit:\n\t\treturn 0, ErrConnectionClosed\n\tcase c.semaphore <- struct{}{}:\n\t\t// acquired\n\t}\n\n\tdefer func() {\n\t\t// release\n\t\t<-c.semaphore\n\t}()\n\n\tif c.timeout > 0 {\n\t\terr := c.w.SetWriteDeadline(time.Now().Add(c.timeout))\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\treturn c.w.Write(p)\n}\n\nfunc newWriteCoalescer(conn deadlineWriter, writeTimeout, coalesceDuration time.Duration,\n\tquit <-chan struct{}) *writeCoalescer {\n\twc := &writeCoalescer{\n\t\twriteCh: make(chan writeRequest),\n\t\tc:       conn,\n\t\tquit:    quit,\n\t\ttimeout: writeTimeout,\n\t}\n\tgo wc.writeFlusher(coalesceDuration)\n\treturn wc\n}\n\ntype writeCoalescer struct {\n\tc deadlineWriter\n\n\tmu sync.Mutex\n\n\tquit    <-chan struct{}\n\twriteCh chan writeRequest\n\n\ttimeout time.Duration\n\n\ttestEnqueuedHook func()\n\ttestFlushedHook  func()\n}\n\ntype writeRequest struct {\n\t// resultChan is a channel (with buffer size 1) where to send results of the write.\n\tresultChan chan<- writeResult\n\t// data to write.\n\tdata []byte\n}\n\ntype writeResult struct {\n\tn   int\n\terr error\n}\n\n// writeContext implements contextWriter.\nfunc (w *writeCoalescer) writeContext(ctx context.Context, p []byte) (int, error) {\n\tresultChan := make(chan writeResult, 1)\n\twr := writeRequest{\n\t\tresultChan: resultChan,\n\t\tdata:       p,\n\t}\n\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn 0, ctx.Err()\n\tcase <-w.quit:\n\t\treturn 0, io.EOF // TODO: better error here?\n\tcase w.writeCh <- wr:\n\t\t// enqueued for writing\n\t}\n\n\tif w.testEnqueuedHook != nil {\n\t\tw.testEnqueuedHook()\n\t}\n\n\tresult := <-resultChan\n\treturn result.n, result.err\n}\n\nfunc (w *writeCoalescer) writeFlusher(interval time.Duration) {\n\ttimer := time.NewTimer(interval)\n\tdefer timer.Stop()\n\n\tif !timer.Stop() {\n\t\t<-timer.C\n\t}\n\n\tw.writeFlusherImpl(timer.C, func() { timer.Reset(interval) })\n}\n\nfunc (w *writeCoalescer) writeFlusherImpl(timerC <-chan time.Time, resetTimer func()) {\n\trunning := false\n\n\tvar buffers net.Buffers\n\tvar resultChans []chan<- writeResult\n\n\tfor {\n\t\tselect {\n\t\tcase req := <-w.writeCh:\n\t\t\tbuffers = append(buffers, req.data)\n\t\t\tresultChans = append(resultChans, req.resultChan)\n\t\t\tif !running {\n\t\t\t\t// Start timer on first write.\n\t\t\t\tresetTimer()\n\t\t\t\trunning = true\n\t\t\t}\n\t\tcase <-w.quit:\n\t\t\tresult := writeResult{\n\t\t\t\tn:   0,\n\t\t\t\terr: io.EOF, // TODO: better error here?\n\t\t\t}\n\t\t\t// Unblock whoever was waiting.\n\t\t\tfor _, resultChan := range resultChans {\n\t\t\t\t// resultChan has capacity 1, so it does not block.\n\t\t\t\tresultChan <- result\n\t\t\t}\n\t\t\treturn\n\t\tcase <-timerC:\n\t\t\trunning = false\n\t\t\tw.flush(resultChans, buffers)\n\t\t\tbuffers = nil\n\t\t\tresultChans = nil\n\t\t\tif w.testFlushedHook != nil {\n\t\t\t\tw.testFlushedHook()\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (w *writeCoalescer) flush(resultChans []chan<- writeResult, buffers net.Buffers) {\n\t// Flush everything we have so far.\n\tif w.timeout > 0 {\n\t\terr := w.c.SetWriteDeadline(time.Now().Add(w.timeout))\n\t\tif err != nil {\n\t\t\tfor i := range resultChans {\n\t\t\t\tresultChans[i] <- writeResult{\n\t\t\t\t\tn:   0,\n\t\t\t\t\terr: err,\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\t// Copy buffers because WriteTo modifies buffers in-place.\n\tbuffers2 := make(net.Buffers, len(buffers))\n\tcopy(buffers2, buffers)\n\tn, err := buffers2.WriteTo(w.c)\n\t// Writes of bytes before n succeeded, writes of bytes starting from n failed with err.\n\t// Use n as remaining byte counter.\n\tfor i := range buffers {\n\t\tif int64(len(buffers[i])) <= n {\n\t\t\t// this buffer was fully written.\n\t\t\tresultChans[i] <- writeResult{\n\t\t\t\tn:   len(buffers[i]),\n\t\t\t\terr: nil,\n\t\t\t}\n\t\t\tn -= int64(len(buffers[i]))\n\t\t} else {\n\t\t\t// this buffer was not (fully) written.\n\t\t\tresultChans[i] <- writeResult{\n\t\t\t\tn:   int(n),\n\t\t\t\terr: err,\n\t\t\t}\n\t\t\tn = 0\n\t\t}\n\t}\n}\n\n// addCall attempts to add a call to c.calls.\n// It fails with error if the connection already started closing or if a call for the given stream\n// already exists.\nfunc (c *Conn) addCall(call *callReq) error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif c.closed {\n\t\treturn ErrConnectionClosed\n\t}\n\texistingCall := c.calls[call.streamID]\n\tif existingCall != nil {\n\t\treturn fmt.Errorf(\"attempting to use stream already in use: %d -> %d\", call.streamID,\n\t\t\texistingCall.streamID)\n\t}\n\tc.calls[call.streamID] = call\n\treturn nil\n}\n\nfunc (c *Conn) exec(ctx context.Context, req frameBuilder, tracer Tracer) (*framer, error) {\n\tif ctxErr := ctx.Err(); ctxErr != nil {\n\t\treturn nil, ctxErr\n\t}\n\n\t// TODO: move tracer onto conn\n\tstream, ok := c.streams.GetStream()\n\tif !ok {\n\t\treturn nil, ErrNoStreams\n\t}\n\n\t// resp is basically a waiting semaphore protecting the framer\n\tframer := newFramer(c.compressor, c.version)\n\n\tcall := &callReq{\n\t\ttimeout:  make(chan struct{}),\n\t\tstreamID: stream,\n\t\tresp:     make(chan callResp),\n\t}\n\n\tif c.streamObserver != nil {\n\t\tcall.streamObserverContext = c.streamObserver.StreamContext(ctx)\n\t}\n\n\tif err := c.addCall(call); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// After this point, we need to either read from call.resp or close(call.timeout)\n\t// since closeWithError can try to write a connection close error to call.resp.\n\t// If we don't close(call.timeout) or read from call.resp, closeWithError can deadlock.\n\n\tif tracer != nil {\n\t\tframer.trace()\n\t}\n\n\tif call.streamObserverContext != nil {\n\t\tcall.streamObserverContext.StreamStarted(ObservedStream{\n\t\t\tHost: c.host,\n\t\t})\n\t}\n\n\terr := req.buildFrame(framer, stream)\n\tif err != nil {\n\t\t// closeWithError will block waiting for this stream to either receive a response\n\t\t// or for us to timeout.\n\t\tclose(call.timeout)\n\t\t// We failed to serialize the frame into a buffer.\n\t\t// This should not affect the connection as we didn't write anything. We just free the current call.\n\t\tc.mu.Lock()\n\t\tif !c.closed {\n\t\t\tdelete(c.calls, call.streamID)\n\t\t}\n\t\tc.mu.Unlock()\n\t\t// We need to release the stream after we remove the call from c.calls, otherwise the existingCall != nil\n\t\t// check above could fail.\n\t\tc.releaseStream(call)\n\t\treturn nil, err\n\t}\n\n\tn, err := c.w.writeContext(ctx, framer.buf)\n\tif err != nil {\n\t\t// closeWithError will block waiting for this stream to either receive a response\n\t\t// or for us to timeout, close the timeout chan here. Im not entirely sure\n\t\t// but we should not get a response after an error on the write side.\n\t\tclose(call.timeout)\n\t\tif (errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)) && n == 0 {\n\t\t\t// We have not started to write this frame.\n\t\t\t// Release the stream as no response can come from the server on the stream.\n\t\t\tc.mu.Lock()\n\t\t\tif !c.closed {\n\t\t\t\tdelete(c.calls, call.streamID)\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\t// We need to release the stream after we remove the call from c.calls, otherwise the existingCall != nil\n\t\t\t// check above could fail.\n\t\t\tc.releaseStream(call)\n\t\t} else {\n\t\t\t// I think this is the correct thing to do, im not entirely sure. It is not\n\t\t\t// ideal as readers might still get some data, but they probably wont.\n\t\t\t// Here we need to be careful as the stream is not available and if all\n\t\t\t// writes just timeout or fail then the pool might use this connection to\n\t\t\t// send a frame on, with all the streams used up and not returned.\n\t\t\tc.closeWithError(err)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tvar timeoutCh <-chan time.Time\n\tif c.timeout > 0 {\n\t\tif call.timer == nil {\n\t\t\tcall.timer = time.NewTimer(0)\n\t\t\t<-call.timer.C\n\t\t} else {\n\t\t\tif !call.timer.Stop() {\n\t\t\t\tselect {\n\t\t\t\tcase <-call.timer.C:\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcall.timer.Reset(c.timeout)\n\t\ttimeoutCh = call.timer.C\n\t}\n\n\tvar ctxDone <-chan struct{}\n\tif ctx != nil {\n\t\tctxDone = ctx.Done()\n\t}\n\n\tselect {\n\tcase resp := <-call.resp:\n\t\tclose(call.timeout)\n\t\tif resp.err != nil {\n\t\t\tif !c.Closed() {\n\t\t\t\t// if the connection is closed then we cant release the stream,\n\t\t\t\t// this is because the request is still outstanding and we have\n\t\t\t\t// been handed another error from another stream which caused the\n\t\t\t\t// connection to close.\n\t\t\t\tc.releaseStream(call)\n\t\t\t}\n\t\t\treturn nil, resp.err\n\t\t}\n\t\t// dont release the stream if detect a timeout as another request can reuse\n\t\t// that stream and get a response for the old request, which we have no\n\t\t// easy way of detecting.\n\t\t//\n\t\t// Ensure that the stream is not released if there are potentially outstanding\n\t\t// requests on the stream to prevent nil pointer dereferences in recv().\n\t\tdefer c.releaseStream(call)\n\n\t\tif v := resp.framer.header.version.version(); v != c.version {\n\t\t\treturn nil, NewErrProtocol(\"unexpected protocol version in response: got %d expected %d\", v, c.version)\n\t\t}\n\n\t\treturn resp.framer, nil\n\tcase <-timeoutCh:\n\t\tclose(call.timeout)\n\t\tc.handleTimeout()\n\t\treturn nil, ErrTimeoutNoResponse\n\tcase <-ctxDone:\n\t\tclose(call.timeout)\n\t\treturn nil, ctx.Err()\n\tcase <-c.ctx.Done():\n\t\tclose(call.timeout)\n\t\treturn nil, ErrConnectionClosed\n\t}\n}\n\n// ObservedStream observes a single request/response stream.\ntype ObservedStream struct {\n\t// Host of the connection used to send the stream.\n\tHost *HostInfo\n}\n\n// StreamObserver is notified about request/response pairs.\n// Streams are created for executing queries/batches or\n// internal requests to the database and might live longer than\n// execution of the query - the stream is still tracked until\n// response arrives so that stream IDs are not reused.\ntype StreamObserver interface {\n\t// StreamContext is called before creating a new stream.\n\t// ctx is context passed to Session.Query / Session.Batch,\n\t// but might also be an internal context (for example\n\t// for internal requests that use control connection).\n\t// StreamContext might return nil if it is not interested\n\t// in the details of this stream.\n\t// StreamContext is called before the stream is created\n\t// and the returned StreamObserverContext might be discarded\n\t// without any methods called on the StreamObserverContext if\n\t// creation of the stream fails.\n\t// Note that if you don't need to track per-stream data,\n\t// you can always return the same StreamObserverContext.\n\tStreamContext(ctx context.Context) StreamObserverContext\n}\n\n// StreamObserverContext is notified about state of a stream.\n// A stream is started every time a request is written to the server\n// and is finished when a response is received.\n// It is abandoned when the underlying network connection is closed\n// before receiving a response.\ntype StreamObserverContext interface {\n\t// StreamStarted is called when the stream is started.\n\t// This happens just before a request is written to the wire.\n\tStreamStarted(observedStream ObservedStream)\n\n\t// StreamAbandoned is called when we stop waiting for response.\n\t// This happens when the underlying network connection is closed.\n\t// StreamFinished won't be called if StreamAbandoned is.\n\tStreamAbandoned(observedStream ObservedStream)\n\n\t// StreamFinished is called when we receive a response for the stream.\n\tStreamFinished(observedStream ObservedStream)\n}\n\ntype preparedStatment struct {\n\tid       []byte\n\trequest  preparedMetadata\n\tresponse resultMetadata\n}\n\ntype inflightPrepare struct {\n\tdone chan struct{}\n\terr  error\n\n\tpreparedStatment *preparedStatment\n}\n\nfunc (c *Conn) prepareStatement(ctx context.Context, stmt string, tracer Tracer) (*preparedStatment, error) {\n\tstmtCacheKey := c.session.stmtsLRU.keyFor(c.host.HostID(), c.currentKeyspace, stmt)\n\tflight, ok := c.session.stmtsLRU.execIfMissing(stmtCacheKey, func(lru *lru.Cache) *inflightPrepare {\n\t\tflight := &inflightPrepare{\n\t\t\tdone: make(chan struct{}),\n\t\t}\n\t\tlru.Add(stmtCacheKey, flight)\n\t\treturn flight\n\t})\n\n\tif !ok {\n\t\tgo func() {\n\t\t\tdefer close(flight.done)\n\n\t\t\tprep := &writePrepareFrame{\n\t\t\t\tstatement: stmt,\n\t\t\t}\n\t\t\tif c.version > protoVersion4 {\n\t\t\t\tprep.keyspace = c.currentKeyspace\n\t\t\t}\n\n\t\t\t// we won the race to do the load, if our context is canceled we shouldnt\n\t\t\t// stop the load as other callers are waiting for it but this caller should get\n\t\t\t// their context cancelled error.\n\t\t\tframer, err := c.exec(c.ctx, prep, tracer)\n\t\t\tif err != nil {\n\t\t\t\tflight.err = err\n\t\t\t\tc.session.stmtsLRU.remove(stmtCacheKey)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tframe, err := framer.parseFrame()\n\t\t\tif err != nil {\n\t\t\t\tflight.err = err\n\t\t\t\tc.session.stmtsLRU.remove(stmtCacheKey)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// TODO(zariel): tidy this up, simplify handling of frame parsing so its not duplicated\n\t\t\t// everytime we need to parse a frame.\n\t\t\tif len(framer.traceID) > 0 && tracer != nil {\n\t\t\t\ttracer.Trace(framer.traceID)\n\t\t\t}\n\n\t\t\tswitch x := frame.(type) {\n\t\t\tcase *resultPreparedFrame:\n\t\t\t\tflight.preparedStatment = &preparedStatment{\n\t\t\t\t\t// defensively copy as we will recycle the underlying buffer after we\n\t\t\t\t\t// return.\n\t\t\t\t\tid: copyBytes(x.preparedID),\n\t\t\t\t\t// the type info's should _not_ have a reference to the framers read buffer,\n\t\t\t\t\t// therefore we can just copy them directly.\n\t\t\t\t\trequest:  x.reqMeta,\n\t\t\t\t\tresponse: x.respMeta,\n\t\t\t\t}\n\t\t\tcase error:\n\t\t\t\tflight.err = x\n\t\t\tdefault:\n\t\t\t\tflight.err = NewErrProtocol(\"Unknown type in response to prepare frame: %s\", x)\n\t\t\t}\n\n\t\t\tif flight.err != nil {\n\t\t\t\tc.session.stmtsLRU.remove(stmtCacheKey)\n\t\t\t}\n\t\t}()\n\t}\n\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn nil, ctx.Err()\n\tcase <-flight.done:\n\t\treturn flight.preparedStatment, flight.err\n\t}\n}\n\nfunc marshalQueryValue(typ TypeInfo, value interface{}, dst *queryValues) error {\n\tif named, ok := value.(*namedValue); ok {\n\t\tdst.name = named.name\n\t\tvalue = named.value\n\t}\n\n\tif _, ok := value.(unsetColumn); !ok {\n\t\tval, err := Marshal(typ, value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdst.value = val\n\t} else {\n\t\tdst.isUnset = true\n\t}\n\n\treturn nil\n}\n\nfunc (c *Conn) executeQuery(ctx context.Context, qry *Query) *Iter {\n\tparams := queryParams{\n\t\tconsistency: qry.cons,\n\t}\n\n\t// frame checks that it is not 0\n\tparams.serialConsistency = qry.serialCons\n\tparams.defaultTimestamp = qry.defaultTimestamp\n\tparams.defaultTimestampValue = qry.defaultTimestampValue\n\n\tif len(qry.pageState) > 0 {\n\t\tparams.pagingState = qry.pageState\n\t}\n\tif qry.pageSize > 0 {\n\t\tparams.pageSize = qry.pageSize\n\t}\n\tif c.version > protoVersion4 {\n\t\tparams.keyspace = c.currentKeyspace\n\t}\n\n\tvar (\n\t\tframe frameBuilder\n\t\tinfo  *preparedStatment\n\t)\n\n\tif !qry.skipPrepare && qry.shouldPrepare() {\n\t\t// Prepare all DML queries. Other queries can not be prepared.\n\t\tvar err error\n\t\tinfo, err = c.prepareStatement(ctx, qry.stmt, qry.trace)\n\t\tif err != nil {\n\t\t\treturn &Iter{err: err}\n\t\t}\n\n\t\tvalues := qry.values\n\t\tif qry.binding != nil {\n\t\t\tvalues, err = qry.binding(&QueryInfo{\n\t\t\t\tId:          info.id,\n\t\t\t\tArgs:        info.request.columns,\n\t\t\t\tRval:        info.response.columns,\n\t\t\t\tPKeyColumns: info.request.pkeyColumns,\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\treturn &Iter{err: err}\n\t\t\t}\n\t\t}\n\n\t\tif len(values) != info.request.actualColCount {\n\t\t\treturn &Iter{err: fmt.Errorf(\"gocql: expected %d values send got %d\", info.request.actualColCount, len(values))}\n\t\t}\n\n\t\tparams.values = make([]queryValues, len(values))\n\t\tfor i := 0; i < len(values); i++ {\n\t\t\tv := &params.values[i]\n\t\t\tvalue := values[i]\n\t\t\ttyp := info.request.columns[i].TypeInfo\n\t\t\tif err := marshalQueryValue(typ, value, v); err != nil {\n\t\t\t\treturn &Iter{err: err}\n\t\t\t}\n\t\t}\n\n\t\tparams.skipMeta = !(c.session.cfg.DisableSkipMetadata || qry.disableSkipMetadata)\n\n\t\tframe = &writeExecuteFrame{\n\t\t\tpreparedID:    info.id,\n\t\t\tparams:        params,\n\t\t\tcustomPayload: qry.customPayload,\n\t\t}\n\n\t\t// Set \"keyspace\" and \"table\" property in the query if it is present in preparedMetadata\n\t\tqry.routingInfo.mu.Lock()\n\t\tqry.routingInfo.keyspace = info.request.keyspace\n\t\tqry.routingInfo.table = info.request.table\n\t\tqry.routingInfo.mu.Unlock()\n\t} else {\n\t\tframe = &writeQueryFrame{\n\t\t\tstatement:     qry.stmt,\n\t\t\tparams:        params,\n\t\t\tcustomPayload: qry.customPayload,\n\t\t}\n\t}\n\n\tframer, err := c.exec(ctx, frame, qry.trace)\n\tif err != nil {\n\t\treturn &Iter{err: err}\n\t}\n\n\tresp, err := framer.parseFrame()\n\tif err != nil {\n\t\treturn &Iter{err: err}\n\t}\n\n\tif len(framer.traceID) > 0 && qry.trace != nil {\n\t\tqry.trace.Trace(framer.traceID)\n\t}\n\n\tswitch x := resp.(type) {\n\tcase *resultVoidFrame:\n\t\treturn &Iter{framer: framer}\n\tcase *resultRowsFrame:\n\t\titer := &Iter{\n\t\t\tmeta:    x.meta,\n\t\t\tframer:  framer,\n\t\t\tnumRows: x.numRows,\n\t\t}\n\n\t\tif params.skipMeta {\n\t\t\tif info != nil {\n\t\t\t\titer.meta = info.response\n\t\t\t\titer.meta.pagingState = copyBytes(x.meta.pagingState)\n\t\t\t} else {\n\t\t\t\treturn &Iter{framer: framer, err: errors.New(\"gocql: did not receive metadata but prepared info is nil\")}\n\t\t\t}\n\t\t} else {\n\t\t\titer.meta = x.meta\n\t\t}\n\n\t\tif x.meta.morePages() && !qry.disableAutoPage {\n\t\t\tnewQry := new(Query)\n\t\t\t*newQry = *qry\n\t\t\tnewQry.pageState = copyBytes(x.meta.pagingState)\n\t\t\tnewQry.metrics = &queryMetrics{m: make(map[string]*hostMetrics)}\n\n\t\t\titer.next = &nextIter{\n\t\t\t\tqry: newQry,\n\t\t\t\tpos: int((1 - qry.prefetch) * float64(x.numRows)),\n\t\t\t}\n\n\t\t\tif iter.next.pos < 1 {\n\t\t\t\titer.next.pos = 1\n\t\t\t}\n\t\t}\n\n\t\treturn iter\n\tcase *resultKeyspaceFrame:\n\t\treturn &Iter{framer: framer}\n\tcase *schemaChangeKeyspace, *schemaChangeTable, *schemaChangeFunction, *schemaChangeAggregate, *schemaChangeType:\n\t\titer := &Iter{framer: framer}\n\t\tif err := c.awaitSchemaAgreement(ctx); err != nil {\n\t\t\t// TODO: should have this behind a flag\n\t\t\tc.logger.Println(err)\n\t\t}\n\t\t// dont return an error from this, might be a good idea to give a warning\n\t\t// though. The impact of this returning an error would be that the cluster\n\t\t// is not consistent with regards to its schema.\n\t\treturn iter\n\tcase *RequestErrUnprepared:\n\t\tstmtCacheKey := c.session.stmtsLRU.keyFor(c.host.HostID(), c.currentKeyspace, qry.stmt)\n\t\tc.session.stmtsLRU.evictPreparedID(stmtCacheKey, x.StatementId)\n\t\treturn c.executeQuery(ctx, qry)\n\tcase error:\n\t\treturn &Iter{err: x, framer: framer}\n\tdefault:\n\t\treturn &Iter{\n\t\t\terr:    NewErrProtocol(\"Unknown type in response to execute query (%T): %s\", x, x),\n\t\t\tframer: framer,\n\t\t}\n\t}\n}\n\nfunc (c *Conn) Pick(qry *Query) *Conn {\n\tif c.Closed() {\n\t\treturn nil\n\t}\n\treturn c\n}\n\nfunc (c *Conn) Closed() bool {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\treturn c.closed\n}\n\nfunc (c *Conn) Address() string {\n\treturn c.addr\n}\n\nfunc (c *Conn) AvailableStreams() int {\n\treturn c.streams.Available()\n}\n\nfunc (c *Conn) UseKeyspace(keyspace string) error {\n\tq := &writeQueryFrame{statement: `USE \"` + keyspace + `\"`}\n\tq.params.consistency = c.session.cons\n\n\tframer, err := c.exec(c.ctx, q, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tresp, err := framer.parseFrame()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tswitch x := resp.(type) {\n\tcase *resultKeyspaceFrame:\n\tcase error:\n\t\treturn x\n\tdefault:\n\t\treturn NewErrProtocol(\"unknown frame in response to USE: %v\", x)\n\t}\n\n\tc.currentKeyspace = keyspace\n\n\treturn nil\n}\n\nfunc (c *Conn) executeBatch(ctx context.Context, batch *Batch) *Iter {\n\tif c.version == protoVersion1 {\n\t\treturn &Iter{err: ErrUnsupported}\n\t}\n\n\tn := len(batch.Entries)\n\treq := &writeBatchFrame{\n\t\ttyp:                   batch.Type,\n\t\tstatements:            make([]batchStatment, n),\n\t\tconsistency:           batch.Cons,\n\t\tserialConsistency:     batch.serialCons,\n\t\tdefaultTimestamp:      batch.defaultTimestamp,\n\t\tdefaultTimestampValue: batch.defaultTimestampValue,\n\t\tcustomPayload:         batch.CustomPayload,\n\t}\n\n\tstmts := make(map[string]string, len(batch.Entries))\n\n\tfor i := 0; i < n; i++ {\n\t\tentry := &batch.Entries[i]\n\t\tb := &req.statements[i]\n\n\t\tif len(entry.Args) > 0 || entry.binding != nil {\n\t\t\tinfo, err := c.prepareStatement(batch.Context(), entry.Stmt, batch.trace)\n\t\t\tif err != nil {\n\t\t\t\treturn &Iter{err: err}\n\t\t\t}\n\n\t\t\tvar values []interface{}\n\t\t\tif entry.binding == nil {\n\t\t\t\tvalues = entry.Args\n\t\t\t} else {\n\t\t\t\tvalues, err = entry.binding(&QueryInfo{\n\t\t\t\t\tId:          info.id,\n\t\t\t\t\tArgs:        info.request.columns,\n\t\t\t\t\tRval:        info.response.columns,\n\t\t\t\t\tPKeyColumns: info.request.pkeyColumns,\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn &Iter{err: err}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif len(values) != info.request.actualColCount {\n\t\t\t\treturn &Iter{err: fmt.Errorf(\"gocql: batch statement %d expected %d values send got %d\", i, info.request.actualColCount, len(values))}\n\t\t\t}\n\n\t\t\tb.preparedID = info.id\n\t\t\tstmts[string(info.id)] = entry.Stmt\n\n\t\t\tb.values = make([]queryValues, info.request.actualColCount)\n\n\t\t\tfor j := 0; j < info.request.actualColCount; j++ {\n\t\t\t\tv := &b.values[j]\n\t\t\t\tvalue := values[j]\n\t\t\t\ttyp := info.request.columns[j].TypeInfo\n\t\t\t\tif err := marshalQueryValue(typ, value, v); err != nil {\n\t\t\t\t\treturn &Iter{err: err}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tb.statement = entry.Stmt\n\t\t}\n\t}\n\n\tframer, err := c.exec(batch.Context(), req, batch.trace)\n\tif err != nil {\n\t\treturn &Iter{err: err}\n\t}\n\n\tresp, err := framer.parseFrame()\n\tif err != nil {\n\t\treturn &Iter{err: err, framer: framer}\n\t}\n\n\tif len(framer.traceID) > 0 && batch.trace != nil {\n\t\tbatch.trace.Trace(framer.traceID)\n\t}\n\n\tswitch x := resp.(type) {\n\tcase *resultVoidFrame:\n\t\treturn &Iter{}\n\tcase *RequestErrUnprepared:\n\t\tstmt, found := stmts[string(x.StatementId)]\n\t\tif found {\n\t\t\tkey := c.session.stmtsLRU.keyFor(c.host.HostID(), c.currentKeyspace, stmt)\n\t\t\tc.session.stmtsLRU.evictPreparedID(key, x.StatementId)\n\t\t}\n\t\treturn c.executeBatch(ctx, batch)\n\tcase *resultRowsFrame:\n\t\titer := &Iter{\n\t\t\tmeta:    x.meta,\n\t\t\tframer:  framer,\n\t\t\tnumRows: x.numRows,\n\t\t}\n\n\t\treturn iter\n\tcase error:\n\t\treturn &Iter{err: x, framer: framer}\n\tdefault:\n\t\treturn &Iter{err: NewErrProtocol(\"Unknown type in response to batch statement: %s\", x), framer: framer}\n\t}\n}\n\nfunc (c *Conn) query(ctx context.Context, statement string, values ...interface{}) (iter *Iter) {\n\tq := c.session.Query(statement, values...).Consistency(One).Trace(nil)\n\tq.skipPrepare = true\n\tq.disableSkipMetadata = true\n\t// we want to keep the query on this connection\n\tq.conn = c\n\treturn c.executeQuery(ctx, q)\n}\n\nfunc (c *Conn) querySystemPeers(ctx context.Context, version cassVersion) *Iter {\n\tconst (\n\t\tpeerSchema    = \"SELECT * FROM system.peers\"\n\t\tpeerV2Schemas = \"SELECT * FROM system.peers_v2\"\n\t)\n\n\tc.mu.Lock()\n\tisSchemaV2 := c.isSchemaV2\n\tc.mu.Unlock()\n\n\tif version.AtLeast(4, 0, 0) && isSchemaV2 {\n\t\t// Try \"system.peers_v2\" and fallback to \"system.peers\" if it's not found\n\t\titer := c.query(ctx, peerV2Schemas)\n\n\t\terr := iter.checkErrAndNotFound()\n\t\tif err != nil {\n\t\t\tif errFrame, ok := err.(errorFrame); ok && errFrame.code == ErrCodeInvalid { // system.peers_v2 not found, try system.peers\n\t\t\t\tc.mu.Lock()\n\t\t\t\tc.isSchemaV2 = false\n\t\t\t\tc.mu.Unlock()\n\t\t\t\treturn c.query(ctx, peerSchema)\n\t\t\t} else {\n\t\t\t\treturn iter\n\t\t\t}\n\t\t}\n\t\treturn iter\n\t} else {\n\t\treturn c.query(ctx, peerSchema)\n\t}\n}\n\nfunc (c *Conn) querySystemLocal(ctx context.Context) *Iter {\n\treturn c.query(ctx, \"SELECT * FROM system.local WHERE key='local'\")\n}\n\nfunc (c *Conn) awaitSchemaAgreement(ctx context.Context) (err error) {\n\tconst localSchemas = \"SELECT schema_version FROM system.local WHERE key='local'\"\n\n\tvar versions map[string]struct{}\n\tvar schemaVersion string\n\n\tendDeadline := time.Now().Add(c.session.cfg.MaxWaitSchemaAgreement)\n\n\tfor time.Now().Before(endDeadline) {\n\t\titer := c.querySystemPeers(ctx, c.host.version)\n\n\t\tversions = make(map[string]struct{})\n\n\t\trows, err := iter.SliceMap()\n\t\tif err != nil {\n\t\t\tgoto cont\n\t\t}\n\n\t\tfor _, row := range rows {\n\t\t\thost, err := c.session.hostInfoFromMap(row, &HostInfo{connectAddress: c.host.ConnectAddress(), port: c.session.cfg.Port})\n\t\t\tif err != nil {\n\t\t\t\tgoto cont\n\t\t\t}\n\t\t\tif !isValidPeer(host) || host.schemaVersion == \"\" {\n\t\t\t\tc.logger.Printf(\"invalid peer or peer with empty schema_version: peer=%q\", host)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tversions[host.schemaVersion] = struct{}{}\n\t\t}\n\n\t\tif err = iter.Close(); err != nil {\n\t\t\tgoto cont\n\t\t}\n\n\t\titer = c.query(ctx, localSchemas)\n\t\tfor iter.Scan(&schemaVersion) {\n\t\t\tversions[schemaVersion] = struct{}{}\n\t\t\tschemaVersion = \"\"\n\t\t}\n\n\t\tif err = iter.Close(); err != nil {\n\t\t\tgoto cont\n\t\t}\n\n\t\tif len(versions) <= 1 {\n\t\t\treturn nil\n\t\t}\n\n\tcont:\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-time.After(200 * time.Millisecond):\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tschemas := make([]string, 0, len(versions))\n\tfor schema := range versions {\n\t\tschemas = append(schemas, schema)\n\t}\n\n\t// not exported\n\treturn fmt.Errorf(\"gocql: cluster schema versions not consistent: %+v\", schemas)\n}\n\nvar (\n\tErrQueryArgLength    = errors.New(\"gocql: query argument length mismatch\")\n\tErrTimeoutNoResponse = errors.New(\"gocql: no response received from cassandra within timeout period\")\n\tErrTooManyTimeouts   = errors.New(\"gocql: too many query timeouts on the connection\")\n\tErrConnectionClosed  = errors.New(\"gocql: connection closed waiting for response\")\n\tErrNoStreams         = errors.New(\"gocql: no streams available on connection\")\n)\n"
        },
        {
          "name": "conn_test.go",
          "type": "blob",
          "size": 33.8740234375,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gocql/gocql/internal/streams\"\n)\n\nconst (\n\tdefaultProto = protoVersion2\n)\n\nfunc TestApprove(t *testing.T) {\n\ttests := map[bool]bool{\n\t\tapprove(\"org.apache.cassandra.auth.PasswordAuthenticator\", []string{}):                                             true,\n\t\tapprove(\"org.apache.cassandra.auth.MutualTlsWithPasswordFallbackAuthenticator\", []string{}):                        true,\n\t\tapprove(\"org.apache.cassandra.auth.MutualTlsAuthenticator\", []string{}):                                            true,\n\t\tapprove(\"com.instaclustr.cassandra.auth.SharedSecretAuthenticator\", []string{}):                                    true,\n\t\tapprove(\"com.datastax.bdp.cassandra.auth.DseAuthenticator\", []string{}):                                            true,\n\t\tapprove(\"io.aiven.cassandra.auth.AivenAuthenticator\", []string{}):                                                  true,\n\t\tapprove(\"com.amazon.helenus.auth.HelenusAuthenticator\", []string{}):                                                true,\n\t\tapprove(\"com.ericsson.bss.cassandra.ecaudit.auth.AuditAuthenticator\", []string{}):                                  true,\n\t\tapprove(\"com.scylladb.auth.SaslauthdAuthenticator\", []string{}):                                                    true,\n\t\tapprove(\"com.scylladb.auth.TransitionalAuthenticator\", []string{}):                                                 true,\n\t\tapprove(\"com.instaclustr.cassandra.auth.InstaclustrPasswordAuthenticator\", []string{}):                             true,\n\t\tapprove(\"com.apache.cassandra.auth.FakeAuthenticator\", []string{}):                                                 true,\n\t\tapprove(\"com.apache.cassandra.auth.FakeAuthenticator\", nil):                                                        true,\n\t\tapprove(\"com.apache.cassandra.auth.FakeAuthenticator\", []string{\"com.apache.cassandra.auth.FakeAuthenticator\"}):    true,\n\t\tapprove(\"com.apache.cassandra.auth.FakeAuthenticator\", []string{\"com.apache.cassandra.auth.NotFakeAuthenticator\"}): false,\n\t}\n\tfor k, v := range tests {\n\t\tif k != v {\n\t\t\tt.Fatalf(\"expected '%v', got '%v'\", k, v)\n\t\t}\n\t}\n}\n\nfunc TestJoinHostPort(t *testing.T) {\n\ttests := map[string]string{\n\t\t\"127.0.0.1:0\": JoinHostPort(\"127.0.0.1\", 0),\n\t\t\"127.0.0.1:1\": JoinHostPort(\"127.0.0.1:1\", 9142),\n\t\t\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:0\": JoinHostPort(\"2001:0db8:85a3:0000:0000:8a2e:0370:7334\", 0),\n\t\t\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:1\": JoinHostPort(\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:1\", 9142),\n\t}\n\tfor k, v := range tests {\n\t\tif k != v {\n\t\t\tt.Fatalf(\"expected '%v', got '%v'\", k, v)\n\t\t}\n\t}\n}\n\nfunc testCluster(proto protoVersion, addresses ...string) *ClusterConfig {\n\tcluster := NewCluster(addresses...)\n\tcluster.ProtoVersion = int(proto)\n\tcluster.disableControlConn = true\n\treturn cluster\n}\n\nfunc TestSimple(t *testing.T) {\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"0x%x: NewCluster: %v\", defaultProto, err)\n\t}\n\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatalf(\"0x%x: %v\", defaultProto, err)\n\t}\n}\n\nfunc TestSSLSimple(t *testing.T) {\n\tsrv := NewSSLTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tdb, err := createTestSslCluster(srv.Address, defaultProto, true).CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"0x%x: NewCluster: %v\", defaultProto, err)\n\t}\n\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatalf(\"0x%x: %v\", defaultProto, err)\n\t}\n}\n\nfunc TestSSLSimpleNoClientCert(t *testing.T) {\n\tsrv := NewSSLTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tdb, err := createTestSslCluster(srv.Address, defaultProto, false).CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"0x%x: NewCluster: %v\", defaultProto, err)\n\t}\n\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatalf(\"0x%x: %v\", defaultProto, err)\n\t}\n}\n\nfunc createTestSslCluster(addr string, proto protoVersion, useClientCert bool) *ClusterConfig {\n\tcluster := testCluster(proto, addr)\n\tsslOpts := &SslOptions{\n\t\tCaPath:                 \"testdata/pki/ca.crt\",\n\t\tEnableHostVerification: false,\n\t}\n\n\tif useClientCert {\n\t\tsslOpts.CertPath = \"testdata/pki/gocql.crt\"\n\t\tsslOpts.KeyPath = \"testdata/pki/gocql.key\"\n\t}\n\n\tcluster.SslOpts = sslOpts\n\treturn cluster\n}\n\nfunc TestClosed(t *testing.T) {\n\tt.Skip(\"Skipping the execution of TestClosed for now to try to concentrate on more important test failures on Travis\")\n\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tsession, err := newTestSession(defaultProto, srv.Address)\n\tif err != nil {\n\t\tt.Fatalf(\"0x%x: NewCluster: %v\", defaultProto, err)\n\t}\n\n\tsession.Close()\n\n\tif err := session.Query(\"void\").Exec(); err != ErrSessionClosed {\n\t\tt.Fatalf(\"0x%x: expected %#v, got %#v\", defaultProto, ErrSessionClosed, err)\n\t}\n}\n\nfunc newTestSession(proto protoVersion, addresses ...string) (*Session, error) {\n\treturn testCluster(proto, addresses...).CreateSession()\n}\n\nfunc TestDNSLookupConnected(t *testing.T) {\n\tlog := &testLogger{}\n\n\t// Override the defaul DNS resolver and restore at the end\n\tfailDNS = true\n\tdefer func() { failDNS = false }()\n\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := NewCluster(\"cassandra1.invalid\", srv.Address, \"cassandra2.invalid\")\n\tcluster.Logger = log\n\tcluster.ProtoVersion = int(defaultProto)\n\tcluster.disableControlConn = true\n\n\t// CreateSession() should attempt to resolve the DNS name \"cassandraX.invalid\"\n\t// and fail, but continue to connect via srv.Address\n\t_, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(\"CreateSession() should have connected\")\n\t}\n\n\tif !strings.Contains(log.String(), \"gocql: dns error\") {\n\t\tt.Fatalf(\"Expected to receive dns error log message  - got '%s' instead\", log.String())\n\t}\n}\n\nfunc TestDNSLookupError(t *testing.T) {\n\tlog := &testLogger{}\n\n\t// Override the defaul DNS resolver and restore at the end\n\tfailDNS = true\n\tdefer func() { failDNS = false }()\n\n\tcluster := NewCluster(\"cassandra1.invalid\", \"cassandra2.invalid\")\n\tcluster.Logger = log\n\tcluster.ProtoVersion = int(defaultProto)\n\tcluster.disableControlConn = true\n\n\t// CreateSession() should attempt to resolve each DNS name \"cassandraX.invalid\"\n\t// and fail since it could not resolve any dns entries\n\t_, err := cluster.CreateSession()\n\tif err == nil {\n\t\tt.Fatal(\"CreateSession() should have returned an error\")\n\t}\n\n\tif !strings.Contains(log.String(), \"gocql: dns error\") {\n\t\tt.Fatalf(\"Expected to receive dns error log message  - got '%s' instead\", log.String())\n\t}\n\n\tif err.Error() != \"gocql: unable to create session: failed to resolve any of the provided hostnames\" {\n\t\tt.Fatalf(\"Expected CreateSession() to fail with message  - got '%s' instead\", err.Error())\n\t}\n}\n\nfunc TestStartupTimeout(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tlog := &testLogger{}\n\n\tsrv := NewTestServer(t, defaultProto, ctx)\n\tdefer srv.Stop()\n\n\t// Tell the server to never respond to Startup frame\n\tatomic.StoreInt32(&srv.TimeoutOnStartup, 1)\n\n\tstartTime := time.Now()\n\tcluster := NewCluster(srv.Address)\n\tcluster.Logger = log\n\tcluster.ProtoVersion = int(defaultProto)\n\tcluster.disableControlConn = true\n\t// Set very long query connection timeout\n\t// so we know CreateSession() is using the ConnectTimeout\n\tcluster.Timeout = time.Second * 5\n\tcluster.ConnectTimeout = 600 * time.Millisecond\n\n\t// Create session should timeout during connect attempt\n\t_, err := cluster.CreateSession()\n\tif err == nil {\n\t\tt.Fatal(\"CreateSession() should have returned a timeout error\")\n\t}\n\n\telapsed := time.Since(startTime)\n\tif elapsed > time.Second*5 {\n\t\tt.Fatal(\"ConnectTimeout is not respected\")\n\t}\n\n\tif !errors.Is(err, ErrNoConnectionsStarted) {\n\t\tt.Fatalf(\"Expected to receive no connections error - got '%s'\", err)\n\t}\n\n\tif !strings.Contains(log.String(), \"no response to connection startup within timeout\") && !strings.Contains(log.String(), \"no response received from cassandra within timeout period\") {\n\t\tt.Fatalf(\"Expected to receive timeout log message  - got '%s'\", log.String())\n\t}\n\n\tcancel()\n}\n\nfunc TestTimeout(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\n\tsrv := NewTestServer(t, defaultProto, ctx)\n\tdefer srv.Stop()\n\n\tdb, err := newTestSession(defaultProto, srv.Address)\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\tgo func() {\n\t\tdefer wg.Done()\n\n\t\tselect {\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Errorf(\"no timeout\")\n\t\tcase <-ctx.Done():\n\t\t}\n\t}()\n\n\tif err := db.Query(\"kill\").WithContext(ctx).Exec(); err == nil {\n\t\tt.Fatal(\"expected error got nil\")\n\t}\n\tcancel()\n\n\twg.Wait()\n}\n\nfunc TestCancel(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tsrv := NewTestServer(t, defaultProto, ctx)\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\tcluster.Timeout = 1 * time.Second\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tqry := db.Query(\"timeout\").WithContext(ctx)\n\n\t// Make sure we finish the query without leftovers\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\tgo func() {\n\t\tif err := qry.Exec(); err != context.Canceled {\n\t\t\tt.Fatalf(\"expected to get context cancel error: '%v', got '%v'\", context.Canceled, err)\n\t\t}\n\t\twg.Done()\n\t}()\n\n\t// The query will timeout after about 1 seconds, so cancel it after a short pause\n\ttime.AfterFunc(20*time.Millisecond, cancel)\n\twg.Wait()\n}\n\ntype testQueryObserver struct {\n\tmetrics map[string]*hostMetrics\n\tverbose bool\n\tlogger  StdLogger\n}\n\nfunc (o *testQueryObserver) ObserveQuery(ctx context.Context, q ObservedQuery) {\n\thost := q.Host.ConnectAddress().String()\n\to.metrics[host] = q.Metrics\n\tif o.verbose {\n\t\to.logger.Printf(\"Observed query %q. Returned %v rows, took %v on host %q with %v attempts and total latency %v. Error: %q\\n\",\n\t\t\tq.Statement, q.Rows, q.End.Sub(q.Start), host, q.Metrics.Attempts, q.Metrics.TotalLatency, q.Err)\n\t}\n}\n\nfunc (o *testQueryObserver) GetMetrics(host *HostInfo) *hostMetrics {\n\treturn o.metrics[host.ConnectAddress().String()]\n}\n\n// TestQueryRetry will test to make sure that gocql will execute\n// the exact amount of retry queries designated by the user.\nfunc TestQueryRetry(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tsrv := NewTestServer(t, defaultProto, ctx)\n\tdefer srv.Stop()\n\n\tdb, err := newTestSession(defaultProto, srv.Address)\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tgo func() {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Errorf(\"no timeout\")\n\t\t}\n\t}()\n\n\trt := &SimpleRetryPolicy{NumRetries: 1}\n\n\tqry := db.Query(\"kill\").RetryPolicy(rt)\n\tif err := qry.Exec(); err == nil {\n\t\tt.Fatalf(\"expected error\")\n\t}\n\n\trequests := atomic.LoadInt64(&srv.nKillReq)\n\tattempts := qry.Attempts()\n\tif requests != int64(attempts) {\n\t\tt.Fatalf(\"expected requests %v to match query attempts %v\", requests, attempts)\n\t}\n\n\t// the query will only be attempted once, but is being retried\n\tif requests != int64(rt.NumRetries) {\n\t\tt.Fatalf(\"failed to retry the query %v time(s). Query executed %v times\", rt.NumRetries, requests-1)\n\t}\n}\n\nfunc TestQueryMultinodeWithMetrics(t *testing.T) {\n\tlog := &testLogger{}\n\tdefer func() {\n\t\tos.Stdout.WriteString(log.String())\n\t}()\n\n\t// Build a 3 node cluster to test host metric mapping\n\tvar nodes []*TestServer\n\tvar addresses = []string{\n\t\t\"127.0.0.1\",\n\t\t\"127.0.0.2\",\n\t\t\"127.0.0.3\",\n\t}\n\t// Can do with 1 context for all servers\n\tctx := context.Background()\n\tfor _, ip := range addresses {\n\t\tsrv := NewTestServerWithAddress(ip+\":0\", t, defaultProto, ctx)\n\t\tdefer srv.Stop()\n\t\tnodes = append(nodes, srv)\n\t}\n\n\tdb, err := newTestSession(defaultProto, nodes[0].Address, nodes[1].Address, nodes[2].Address)\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// 1 retry per host\n\trt := &SimpleRetryPolicy{NumRetries: 3}\n\tobserver := &testQueryObserver{metrics: make(map[string]*hostMetrics), verbose: false, logger: log}\n\tqry := db.Query(\"kill\").RetryPolicy(rt).Observer(observer).Idempotent(true)\n\tif err := qry.Exec(); err == nil {\n\t\tt.Fatalf(\"expected error\")\n\t}\n\n\tfor i, ip := range addresses {\n\t\thost := &HostInfo{connectAddress: net.ParseIP(ip)}\n\t\tqueryMetric := qry.metrics.hostMetrics(host)\n\t\tobservedMetrics := observer.GetMetrics(host)\n\n\t\trequests := int(atomic.LoadInt64(&nodes[i].nKillReq))\n\t\thostAttempts := queryMetric.Attempts\n\t\tif requests != hostAttempts {\n\t\t\tt.Fatalf(\"expected requests %v to match query attempts %v\", requests, hostAttempts)\n\t\t}\n\n\t\tif hostAttempts != observedMetrics.Attempts {\n\t\t\tt.Fatalf(\"expected observed attempts %v to match query attempts %v on host %v\", observedMetrics.Attempts, hostAttempts, ip)\n\t\t}\n\n\t\thostLatency := queryMetric.TotalLatency\n\t\tobservedLatency := observedMetrics.TotalLatency\n\t\tif hostLatency != observedLatency {\n\t\t\tt.Fatalf(\"expected observed latency %v to match query latency %v on host %v\", observedLatency, hostLatency, ip)\n\t\t}\n\t}\n\t// the query will only be attempted once, but is being retried\n\tattempts := qry.Attempts()\n\tif attempts != rt.NumRetries {\n\t\tt.Fatalf(\"failed to retry the query %v time(s). Query executed %v times\", rt.NumRetries, attempts)\n\t}\n\n}\n\ntype testRetryPolicy struct {\n\tNumRetries int\n}\n\nfunc (t *testRetryPolicy) Attempt(qry RetryableQuery) bool {\n\treturn qry.Attempts() <= t.NumRetries\n}\nfunc (t *testRetryPolicy) GetRetryType(err error) RetryType {\n\treturn Retry\n}\n\nfunc TestSpeculativeExecution(t *testing.T) {\n\tlog := &testLogger{}\n\tdefer func() {\n\t\tos.Stdout.WriteString(log.String())\n\t}()\n\n\t// Build a 3 node cluster\n\tvar nodes []*TestServer\n\tvar addresses = []string{\n\t\t\"127.0.0.1\",\n\t\t\"127.0.0.2\",\n\t\t\"127.0.0.3\",\n\t}\n\t// Can do with 1 context for all servers\n\tctx := context.Background()\n\tfor _, ip := range addresses {\n\t\tsrv := NewTestServerWithAddress(ip+\":0\", t, defaultProto, ctx)\n\t\tdefer srv.Stop()\n\t\tnodes = append(nodes, srv)\n\t}\n\n\tdb, err := newTestSession(defaultProto, nodes[0].Address, nodes[1].Address, nodes[2].Address)\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// Create a test retry policy, 6 retries will cover 2 executions\n\trt := &testRetryPolicy{NumRetries: 8}\n\t// test Speculative policy with 1 additional execution\n\tsp := &SimpleSpeculativeExecution{NumAttempts: 1, TimeoutDelay: 200 * time.Millisecond}\n\n\t// Build the query\n\tqry := db.Query(\"speculative\").RetryPolicy(rt).SetSpeculativeExecutionPolicy(sp).Idempotent(true)\n\n\t// Execute the query and close, check that it doesn't error out\n\tif err := qry.Exec(); err != nil {\n\t\tt.Errorf(\"The query failed with '%v'!\\n\", err)\n\t}\n\trequests1 := atomic.LoadInt64(&nodes[0].nKillReq)\n\trequests2 := atomic.LoadInt64(&nodes[1].nKillReq)\n\trequests3 := atomic.LoadInt64(&nodes[2].nKillReq)\n\n\t// Spec Attempts == 1, so expecting to see only 1 regular + 1 speculative = 2 nodes attempted\n\tif requests1 != 0 && requests2 != 0 && requests3 != 0 {\n\t\tt.Error(\"error: all 3 nodes were attempted, should have been only 2\")\n\t}\n\n\t// Only the 4th request will generate results, so\n\tif requests1 != 4 && requests2 != 4 && requests3 != 4 {\n\t\tt.Error(\"error: none of 3 nodes was attempted 4 times!\")\n\t}\n\n\t// \"speculative\" query will succeed on one arbitrary node after 4 attempts, so\n\t// expecting to see 4 (on successful node) + not more than 2 (as cancelled on another node) == 6\n\tif requests1+requests2+requests3 > 6 {\n\t\tt.Errorf(\"error: expected to see 6 attempts, got %v\\n\", requests1+requests2+requests3)\n\t}\n}\n\n// This tests that the policy connection pool handles SSL correctly\nfunc TestPolicyConnPoolSSL(t *testing.T) {\n\tsrv := NewSSLTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := createTestSslCluster(srv.Address, defaultProto, true)\n\tcluster.PoolConfig.HostSelectionPolicy = RoundRobinHostPolicy()\n\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create new session: %v\", err)\n\t}\n\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatalf(\"query failed due to error: %v\", err)\n\t}\n\tdb.Close()\n\n\t// wait for the pool to drain\n\ttime.Sleep(100 * time.Millisecond)\n\tsize := db.pool.Size()\n\tif size != 0 {\n\t\tt.Fatalf(\"connection pool did not drain, still contains %d connections\", size)\n\t}\n}\n\nfunc TestQueryTimeout(t *testing.T) {\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\t// Set the timeout arbitrarily low so that the query hits the timeout in a\n\t// timely manner.\n\tcluster.Timeout = 1 * time.Millisecond\n\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tch := make(chan error, 1)\n\n\tgo func() {\n\t\terr := db.Query(\"timeout\").Exec()\n\t\tif err != nil {\n\t\t\tch <- err\n\t\t\treturn\n\t\t}\n\t\tt.Errorf(\"err was nil, expected to get a timeout after %v\", db.cfg.Timeout)\n\t}()\n\n\tselect {\n\tcase err := <-ch:\n\t\tif err != ErrTimeoutNoResponse {\n\t\t\tt.Fatalf(\"expected to get %v for timeout got %v\", ErrTimeoutNoResponse, err)\n\t\t}\n\tcase <-time.After(40*time.Millisecond + db.cfg.Timeout):\n\t\t// ensure that the query goroutines have been scheduled\n\t\tt.Fatalf(\"query did not timeout after %v\", db.cfg.Timeout)\n\t}\n}\n\nfunc BenchmarkSingleConn(b *testing.B) {\n\tsrv := NewTestServer(b, 3, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(3, srv.Address)\n\t// Set the timeout arbitrarily low so that the query hits the timeout in a\n\t// timely manner.\n\tcluster.Timeout = 500 * time.Millisecond\n\tcluster.NumConns = 1\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tb.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tb.ResetTimer()\n\tb.RunParallel(func(pb *testing.PB) {\n\t\tfor pb.Next() {\n\t\t\terr := db.Query(\"void\").Exec()\n\t\t\tif err != nil {\n\t\t\t\tb.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestQueryTimeoutReuseStream(t *testing.T) {\n\tt.Skip(\"no longer tests anything\")\n\t// TODO(zariel): move this to conn test, we really just want to check what\n\t// happens when a conn is\n\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\t// Set the timeout arbitrarily low so that the query hits the timeout in a\n\t// timely manner.\n\tcluster.Timeout = 1 * time.Millisecond\n\tcluster.NumConns = 1\n\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tdb.Query(\"slow\").Exec()\n\n\terr = db.Query(\"void\").Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestQueryTimeoutClose(t *testing.T) {\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\t// Set the timeout arbitrarily low so that the query hits the timeout in a\n\t// timely manner.\n\tcluster.Timeout = 1000 * time.Millisecond\n\tcluster.NumConns = 1\n\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\n\tch := make(chan error)\n\tgo func() {\n\t\terr := db.Query(\"timeout\").Exec()\n\t\tch <- err\n\t}()\n\t// ensure that the above goroutine gets sheduled\n\ttime.Sleep(50 * time.Millisecond)\n\n\tdb.Close()\n\tselect {\n\tcase err = <-ch:\n\tcase <-time.After(1 * time.Second):\n\t\tt.Fatal(\"timedout waiting to get a response once cluster is closed\")\n\t}\n\n\tif err != ErrConnectionClosed {\n\t\tt.Fatalf(\"expected to get %v got %v\", ErrConnectionClosed, err)\n\t}\n}\n\nfunc TestStream0(t *testing.T) {\n\t// TODO: replace this with type check\n\tconst expErr = \"gocql: received unexpected frame on stream 0\"\n\n\tvar buf bytes.Buffer\n\tf := newFramer(nil, protoVersion4)\n\tf.writeHeader(0, opResult, 0)\n\tf.writeInt(resultKindVoid)\n\tf.buf[0] |= 0x80\n\tif err := f.finish(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := f.writeTo(&buf); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconn := &Conn{\n\t\tr:       bufio.NewReader(&buf),\n\t\tstreams: streams.New(protoVersion4),\n\t\tlogger:  &defaultLogger{},\n\t}\n\n\terr := conn.recv(context.Background())\n\tif err == nil {\n\t\tt.Fatal(\"expected to get an error on stream 0\")\n\t} else if !strings.HasPrefix(err.Error(), expErr) {\n\t\tt.Fatalf(\"expected to get error prefix %q got %q\", expErr, err.Error())\n\t}\n}\n\nfunc TestContext_Timeout(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tsrv := NewTestServer(t, defaultProto, ctx)\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\tcluster.Timeout = 5 * time.Second\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer db.Close()\n\n\tctx, cancel = context.WithCancel(ctx)\n\tcancel()\n\n\terr = db.Query(\"timeout\").WithContext(ctx).Exec()\n\tif err != context.Canceled {\n\t\tt.Fatalf(\"expected to get context cancel error: %v got %v\", context.Canceled, err)\n\t}\n}\n\nfunc TestContext_CanceledBeforeExec(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tvar reqCount uint64\n\n\tsrv := newTestServerOpts{\n\t\taddr:     \"127.0.0.1:0\",\n\t\tprotocol: defaultProto,\n\t\trecvHook: func(f *framer) {\n\t\t\tif f.header.op == opStartup || f.header.op == opOptions {\n\t\t\t\t// ignore statup and heartbeat messages\n\t\t\t\treturn\n\t\t\t}\n\t\t\tatomic.AddUint64(&reqCount, 1)\n\t\t},\n\t}.newServer(t, ctx)\n\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\tcluster.Timeout = 5 * time.Second\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer db.Close()\n\n\tstartupRequestCount := atomic.LoadUint64(&reqCount)\n\n\tctx, cancel = context.WithCancel(ctx)\n\tcancel()\n\n\terr = db.Query(\"timeout\").WithContext(ctx).Exec()\n\tif err != context.Canceled {\n\t\tt.Fatalf(\"expected to get context cancel error: %v got %v\", context.Canceled, err)\n\t}\n\n\t// Queries are executed by separate goroutine and we don't have a synchronization point that would allow us to\n\t// check if a request was sent or not.\n\t// Fall back to waiting a little bit.\n\ttime.Sleep(100 * time.Millisecond)\n\n\tqueryRequestCount := atomic.LoadUint64(&reqCount) - startupRequestCount\n\tif queryRequestCount != 0 {\n\t\tt.Fatalf(\"expected that no request is sent to server, sent %d requests\", queryRequestCount)\n\t}\n}\n\n// tcpConnPair returns a matching set of a TCP client side and server side connection.\nfunc tcpConnPair() (s, c net.Conn, err error) {\n\tl, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\t// maybe ipv6 works, if ipv4 fails?\n\t\tl, err = net.Listen(\"tcp6\", \"[::1]:0\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\tdefer l.Close() // we only try to accept one connection, so will stop listening.\n\n\taddr := l.Addr()\n\tdone := make(chan struct{})\n\tvar errDial error\n\tgo func(done chan<- struct{}) {\n\t\tc, errDial = net.Dial(addr.Network(), addr.String())\n\t\tclose(done)\n\t}(done)\n\n\ts, err = l.Accept()\n\t<-done\n\n\tif err == nil {\n\t\terr = errDial\n\t}\n\n\tif err != nil {\n\t\tif s != nil {\n\t\t\ts.Close()\n\t\t}\n\t\tif c != nil {\n\t\t\tc.Close()\n\t\t}\n\t}\n\n\treturn s, c, err\n}\n\nfunc TestWriteCoalescing(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tserver, client, err := tcpConnPair()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdone := make(chan struct{}, 1)\n\tvar (\n\t\tbuf      bytes.Buffer\n\t\tbufMutex sync.Mutex\n\t)\n\tgo func() {\n\t\tdefer close(done)\n\t\tdefer server.Close()\n\t\tvar err error\n\t\tb := make([]byte, 256)\n\t\tvar n int\n\t\tfor {\n\t\t\tif n, err = server.Read(b); err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tbufMutex.Lock()\n\t\t\tbuf.Write(b[:n])\n\t\t\tbufMutex.Unlock()\n\t\t}\n\t\tif err != io.EOF {\n\t\t\tt.Errorf(\"unexpected read error: %v\", err)\n\t\t}\n\t}()\n\tenqueued := make(chan struct{})\n\tresetTimer := make(chan struct{})\n\tw := &writeCoalescer{\n\t\twriteCh: make(chan writeRequest),\n\t\tc:       client,\n\t\tquit:    ctx.Done(),\n\t\ttimeout: 500 * time.Millisecond,\n\t\ttestEnqueuedHook: func() {\n\t\t\tenqueued <- struct{}{}\n\t\t},\n\t\ttestFlushedHook: func() {\n\t\t\tclient.Close()\n\t\t},\n\t}\n\ttimerC := make(chan time.Time, 1)\n\tgo func() {\n\t\tw.writeFlusherImpl(timerC, func() { resetTimer <- struct{}{} })\n\t}()\n\n\tgo func() {\n\t\tif _, err := w.writeContext(context.Background(), []byte(\"one\")); err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t}()\n\n\tgo func() {\n\t\tif _, err := w.writeContext(context.Background(), []byte(\"two\")); err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t}()\n\n\t<-enqueued\n\t<-resetTimer\n\t<-enqueued\n\n\t// flush\n\ttimerC <- time.Now()\n\n\t<-done\n\n\tif got := buf.String(); got != \"onetwo\" && got != \"twoone\" {\n\t\tt.Fatalf(\"expected to get %q got %q\", \"onetwo or twoone\", got)\n\t}\n}\n\nfunc TestWriteCoalescing_WriteAfterClose(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tvar buf bytes.Buffer\n\tdefer cancel()\n\tserver, client, err := tcpConnPair()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdone := make(chan struct{}, 1)\n\tgo func() {\n\t\tio.Copy(&buf, server)\n\t\tserver.Close()\n\t\tclose(done)\n\t}()\n\tw := newWriteCoalescer(client, 0, 5*time.Millisecond, ctx.Done())\n\n\t// ensure 1 write works\n\tif _, err := w.writeContext(context.Background(), []byte(\"one\")); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tclient.Close()\n\t<-done\n\tif v := buf.String(); v != \"one\" {\n\t\tt.Fatalf(\"expected buffer to be %q got %q\", \"one\", v)\n\t}\n\n\t// now close and do a write, we should error\n\tcancel()\n\tclient.Close() // close client conn too, since server won't see the answer anyway.\n\n\tif _, err := w.writeContext(context.Background(), []byte(\"two\")); err == nil {\n\t\tt.Fatal(\"expected to get error for write after closing\")\n\t} else if err != io.EOF {\n\t\tt.Fatalf(\"expected to get EOF got %v\", err)\n\t}\n}\n\ntype recordingFrameHeaderObserver struct {\n\tt      *testing.T\n\tmu     sync.Mutex\n\tframes []ObservedFrameHeader\n}\n\nfunc (r *recordingFrameHeaderObserver) ObserveFrameHeader(ctx context.Context, frm ObservedFrameHeader) {\n\tr.mu.Lock()\n\tr.frames = append(r.frames, frm)\n\tr.mu.Unlock()\n}\n\nfunc (r *recordingFrameHeaderObserver) getFrames() []ObservedFrameHeader {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\treturn r.frames\n}\n\nfunc TestFrameHeaderObserver(t *testing.T) {\n\tsrv := NewTestServer(t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\tcluster := testCluster(defaultProto, srv.Address)\n\tcluster.NumConns = 1\n\tobserver := &recordingFrameHeaderObserver{t: t}\n\tcluster.FrameHeaderObserver = observer\n\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tframes := observer.getFrames()\n\texpFrames := []frameOp{opSupported, opReady, opResult}\n\tif len(frames) != len(expFrames) {\n\t\tt.Fatalf(\"Expected to receive %d frames, instead received %d\", len(expFrames), len(frames))\n\t}\n\n\tfor i, op := range expFrames {\n\t\tif op != frames[i].Opcode {\n\t\t\tt.Fatalf(\"expected frame %d to be %v got %v\", i, op, frames[i])\n\t\t}\n\t}\n\tvoidResultFrame := frames[2]\n\tif voidResultFrame.Length != int32(4) {\n\t\tt.Fatalf(\"Expected to receive frame with body length 4, instead received body length %d\", voidResultFrame.Length)\n\t}\n}\n\nfunc NewTestServerWithAddress(addr string, t testing.TB, protocol uint8, ctx context.Context) *TestServer {\n\treturn newTestServerOpts{\n\t\taddr:     addr,\n\t\tprotocol: protocol,\n\t}.newServer(t, ctx)\n}\n\ntype newTestServerOpts struct {\n\taddr     string\n\tprotocol uint8\n\trecvHook func(*framer)\n}\n\nfunc (nts newTestServerOpts) newServer(t testing.TB, ctx context.Context) *TestServer {\n\tladdr, err := net.ResolveTCPAddr(\"tcp\", nts.addr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tlisten, err := net.ListenTCP(\"tcp\", laddr)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\theaderSize := 8\n\tif nts.protocol > protoVersion2 {\n\t\theaderSize = 9\n\t}\n\n\tctx, cancel := context.WithCancel(ctx)\n\tsrv := &TestServer{\n\t\tAddress:    listen.Addr().String(),\n\t\tlisten:     listen,\n\t\tt:          t,\n\t\tprotocol:   nts.protocol,\n\t\theaderSize: headerSize,\n\t\tctx:        ctx,\n\t\tcancel:     cancel,\n\n\t\tonRecv: nts.recvHook,\n\t}\n\n\tgo srv.closeWatch()\n\tgo srv.serve()\n\n\treturn srv\n}\n\nfunc NewTestServer(t testing.TB, protocol uint8, ctx context.Context) *TestServer {\n\treturn NewTestServerWithAddress(\"127.0.0.1:0\", t, protocol, ctx)\n}\n\nfunc NewSSLTestServer(t testing.TB, protocol uint8, ctx context.Context) *TestServer {\n\tpem, err := ioutil.ReadFile(\"testdata/pki/ca.crt\")\n\tcertPool := x509.NewCertPool()\n\tif !certPool.AppendCertsFromPEM(pem) {\n\t\tt.Fatalf(\"Failed parsing or appending certs\")\n\t}\n\tmycert, err := tls.LoadX509KeyPair(\"testdata/pki/cassandra.crt\", \"testdata/pki/cassandra.key\")\n\tif err != nil {\n\t\tt.Fatalf(\"could not load cert\")\n\t}\n\tconfig := &tls.Config{\n\t\tCertificates: []tls.Certificate{mycert},\n\t\tRootCAs:      certPool,\n\t}\n\tlisten, err := tls.Listen(\"tcp\", \"127.0.0.1:0\", config)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\theaderSize := 8\n\tif protocol > protoVersion2 {\n\t\theaderSize = 9\n\t}\n\n\tctx, cancel := context.WithCancel(ctx)\n\tsrv := &TestServer{\n\t\tAddress:    listen.Addr().String(),\n\t\tlisten:     listen,\n\t\tt:          t,\n\t\tprotocol:   protocol,\n\t\theaderSize: headerSize,\n\t\tctx:        ctx,\n\t\tcancel:     cancel,\n\t}\n\n\tgo srv.closeWatch()\n\tgo srv.serve()\n\treturn srv\n}\n\ntype TestServer struct {\n\tAddress          string\n\tTimeoutOnStartup int32\n\tt                testing.TB\n\tlisten           net.Listener\n\tnKillReq         int64\n\n\tprotocol   byte\n\theaderSize int\n\tctx        context.Context\n\tcancel     context.CancelFunc\n\n\tmu     sync.Mutex\n\tclosed bool\n\n\t// onRecv is a hook point for tests, called in receive loop.\n\tonRecv func(*framer)\n}\n\nfunc (srv *TestServer) closeWatch() {\n\t<-srv.ctx.Done()\n\n\tsrv.mu.Lock()\n\tdefer srv.mu.Unlock()\n\n\tsrv.closeLocked()\n}\n\nfunc (srv *TestServer) serve() {\n\tdefer srv.listen.Close()\n\tfor !srv.isClosed() {\n\t\tconn, err := srv.listen.Accept()\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\tgo func(conn net.Conn) {\n\t\t\tdefer conn.Close()\n\t\t\tfor !srv.isClosed() {\n\t\t\t\tframer, err := srv.readFrame(conn)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif err == io.EOF {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tsrv.errorLocked(err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif srv.onRecv != nil {\n\t\t\t\t\tsrv.onRecv(framer)\n\t\t\t\t}\n\n\t\t\t\tgo srv.process(conn, framer)\n\t\t\t}\n\t\t}(conn)\n\t}\n}\n\nfunc (srv *TestServer) isClosed() bool {\n\tsrv.mu.Lock()\n\tdefer srv.mu.Unlock()\n\treturn srv.closed\n}\n\nfunc (srv *TestServer) closeLocked() {\n\tif srv.closed {\n\t\treturn\n\t}\n\n\tsrv.closed = true\n\n\tsrv.listen.Close()\n\tsrv.cancel()\n}\n\nfunc (srv *TestServer) Stop() {\n\tsrv.mu.Lock()\n\tdefer srv.mu.Unlock()\n\tsrv.closeLocked()\n}\n\nfunc (srv *TestServer) errorLocked(err interface{}) {\n\tsrv.mu.Lock()\n\tdefer srv.mu.Unlock()\n\tif srv.closed {\n\t\treturn\n\t}\n\tsrv.t.Error(err)\n}\n\nfunc (srv *TestServer) process(conn net.Conn, reqFrame *framer) {\n\thead := reqFrame.header\n\tif head == nil {\n\t\tsrv.errorLocked(\"process frame with a nil header\")\n\t\treturn\n\t}\n\trespFrame := newFramer(nil, reqFrame.proto)\n\n\tswitch head.op {\n\tcase opStartup:\n\t\tif atomic.LoadInt32(&srv.TimeoutOnStartup) > 0 {\n\t\t\t// Do not respond to startup command\n\t\t\t// wait until we get a cancel signal\n\t\t\tselect {\n\t\t\tcase <-srv.ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\trespFrame.writeHeader(0, opReady, head.stream)\n\tcase opOptions:\n\t\trespFrame.writeHeader(0, opSupported, head.stream)\n\t\trespFrame.writeShort(0)\n\tcase opQuery:\n\t\tquery := reqFrame.readLongString()\n\t\tfirst := query\n\t\tif n := strings.Index(query, \" \"); n > 0 {\n\t\t\tfirst = first[:n]\n\t\t}\n\t\tswitch strings.ToLower(first) {\n\t\tcase \"kill\":\n\t\t\tatomic.AddInt64(&srv.nKillReq, 1)\n\t\t\trespFrame.writeHeader(0, opError, head.stream)\n\t\t\trespFrame.writeInt(0x1001)\n\t\t\trespFrame.writeString(\"query killed\")\n\t\tcase \"use\":\n\t\t\trespFrame.writeInt(resultKindKeyspace)\n\t\t\trespFrame.writeString(strings.TrimSpace(query[3:]))\n\t\tcase \"void\":\n\t\t\trespFrame.writeHeader(0, opResult, head.stream)\n\t\t\trespFrame.writeInt(resultKindVoid)\n\t\tcase \"timeout\":\n\t\t\t<-srv.ctx.Done()\n\t\t\treturn\n\t\tcase \"slow\":\n\t\t\tgo func() {\n\t\t\t\trespFrame.writeHeader(0, opResult, head.stream)\n\t\t\t\trespFrame.writeInt(resultKindVoid)\n\t\t\t\trespFrame.buf[0] = srv.protocol | 0x80\n\t\t\t\tselect {\n\t\t\t\tcase <-srv.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\t\t\trespFrame.finish()\n\t\t\t\t\trespFrame.writeTo(conn)\n\t\t\t\t}\n\t\t\t}()\n\t\t\treturn\n\t\tcase \"speculative\":\n\t\t\tatomic.AddInt64(&srv.nKillReq, 1)\n\t\t\tif atomic.LoadInt64(&srv.nKillReq) > 3 {\n\t\t\t\trespFrame.writeHeader(0, opResult, head.stream)\n\t\t\t\trespFrame.writeInt(resultKindVoid)\n\t\t\t\trespFrame.writeString(\"speculative query success on the node \" + srv.Address)\n\t\t\t} else {\n\t\t\t\trespFrame.writeHeader(0, opError, head.stream)\n\t\t\t\trespFrame.writeInt(0x1001)\n\t\t\t\trespFrame.writeString(\"speculative error\")\n\t\t\t\trand.Seed(time.Now().UnixNano())\n\t\t\t\t<-time.After(time.Millisecond * 120)\n\t\t\t}\n\t\tdefault:\n\t\t\trespFrame.writeHeader(0, opResult, head.stream)\n\t\t\trespFrame.writeInt(resultKindVoid)\n\t\t}\n\tcase opError:\n\t\trespFrame.writeHeader(0, opError, head.stream)\n\t\trespFrame.buf = append(respFrame.buf, reqFrame.buf...)\n\tdefault:\n\t\trespFrame.writeHeader(0, opError, head.stream)\n\t\trespFrame.writeInt(0)\n\t\trespFrame.writeString(\"not supported\")\n\t}\n\n\trespFrame.buf[0] = srv.protocol | 0x80\n\n\tif err := respFrame.finish(); err != nil {\n\t\tsrv.errorLocked(err)\n\t}\n\n\tif err := respFrame.writeTo(conn); err != nil {\n\t\tsrv.errorLocked(err)\n\t}\n}\n\nfunc (srv *TestServer) readFrame(conn net.Conn) (*framer, error) {\n\tbuf := make([]byte, srv.headerSize)\n\thead, err := readHeader(conn, buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tframer := newFramer(nil, srv.protocol)\n\n\terr = framer.readFrame(conn, &head)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// should be a request frame\n\tif head.version.response() {\n\t\treturn nil, fmt.Errorf(\"expected to read a request frame got version: %v\", head.version)\n\t} else if head.version.version() != srv.protocol {\n\t\treturn nil, fmt.Errorf(\"expected to read protocol version 0x%x got 0x%x\", srv.protocol, head.version.version())\n\t}\n\n\treturn framer, nil\n}\n"
        },
        {
          "name": "connectionpool.go",
          "type": "blob",
          "size": 14.818359375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// interface to implement to receive the host information\ntype SetHosts interface {\n\tSetHosts(hosts []*HostInfo)\n}\n\n// interface to implement to receive the partitioner value\ntype SetPartitioner interface {\n\tSetPartitioner(partitioner string)\n}\n\nfunc setupTLSConfig(sslOpts *SslOptions) (*tls.Config, error) {\n\t//  Config.InsecureSkipVerify | EnableHostVerification | Result\n\t//  Config is nil             | true                   | verify host\n\t//  Config is nil             | false                  | do not verify host\n\t//  false                     | false                  | verify host\n\t//  true                      | false                  | do not verify host\n\t//  false                     | true                   | verify host\n\t//  true                      | true                   | verify host\n\tvar tlsConfig *tls.Config\n\tif sslOpts.Config == nil {\n\t\ttlsConfig = &tls.Config{\n\t\t\tInsecureSkipVerify: !sslOpts.EnableHostVerification,\n\t\t}\n\t} else {\n\t\t// use clone to avoid race.\n\t\ttlsConfig = sslOpts.Config.Clone()\n\t}\n\n\tif tlsConfig.InsecureSkipVerify && sslOpts.EnableHostVerification {\n\t\ttlsConfig.InsecureSkipVerify = false\n\t}\n\n\t// ca cert is optional\n\tif sslOpts.CaPath != \"\" {\n\t\tif tlsConfig.RootCAs == nil {\n\t\t\ttlsConfig.RootCAs = x509.NewCertPool()\n\t\t}\n\n\t\tpem, err := ioutil.ReadFile(sslOpts.CaPath)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"connectionpool: unable to open CA certs: %v\", err)\n\t\t}\n\n\t\tif !tlsConfig.RootCAs.AppendCertsFromPEM(pem) {\n\t\t\treturn nil, errors.New(\"connectionpool: failed parsing or CA certs\")\n\t\t}\n\t}\n\n\tif sslOpts.CertPath != \"\" || sslOpts.KeyPath != \"\" {\n\t\tmycert, err := tls.LoadX509KeyPair(sslOpts.CertPath, sslOpts.KeyPath)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"connectionpool: unable to load X509 key pair: %v\", err)\n\t\t}\n\t\ttlsConfig.Certificates = append(tlsConfig.Certificates, mycert)\n\t}\n\n\treturn tlsConfig, nil\n}\n\ntype policyConnPool struct {\n\tsession *Session\n\n\tport     int\n\tnumConns int\n\tkeyspace string\n\n\tmu            sync.RWMutex\n\thostConnPools map[string]*hostConnPool\n}\n\nfunc connConfig(cfg *ClusterConfig) (*ConnConfig, error) {\n\tvar (\n\t\terr        error\n\t\thostDialer HostDialer\n\t)\n\n\thostDialer = cfg.HostDialer\n\tif hostDialer == nil {\n\t\tvar tlsConfig *tls.Config\n\n\t\t// TODO(zariel): move tls config setup into session init.\n\t\tif cfg.SslOpts != nil {\n\t\t\ttlsConfig, err = setupTLSConfig(cfg.SslOpts)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tdialer := cfg.Dialer\n\t\tif dialer == nil {\n\t\t\td := &net.Dialer{\n\t\t\t\tTimeout: cfg.ConnectTimeout,\n\t\t\t}\n\t\t\tif cfg.SocketKeepalive > 0 {\n\t\t\t\td.KeepAlive = cfg.SocketKeepalive\n\t\t\t}\n\t\t\tdialer = d\n\t\t}\n\n\t\thostDialer = &defaultHostDialer{\n\t\t\tdialer:    dialer,\n\t\t\ttlsConfig: tlsConfig,\n\t\t}\n\t}\n\n\treturn &ConnConfig{\n\t\tProtoVersion:   cfg.ProtoVersion,\n\t\tCQLVersion:     cfg.CQLVersion,\n\t\tTimeout:        cfg.Timeout,\n\t\tWriteTimeout:   cfg.WriteTimeout,\n\t\tConnectTimeout: cfg.ConnectTimeout,\n\t\tDialer:         cfg.Dialer,\n\t\tHostDialer:     hostDialer,\n\t\tCompressor:     cfg.Compressor,\n\t\tAuthenticator:  cfg.Authenticator,\n\t\tAuthProvider:   cfg.AuthProvider,\n\t\tKeepalive:      cfg.SocketKeepalive,\n\t\tLogger:         cfg.logger(),\n\t}, nil\n}\n\nfunc newPolicyConnPool(session *Session) *policyConnPool {\n\t// create the pool\n\tpool := &policyConnPool{\n\t\tsession:       session,\n\t\tport:          session.cfg.Port,\n\t\tnumConns:      session.cfg.NumConns,\n\t\tkeyspace:      session.cfg.Keyspace,\n\t\thostConnPools: map[string]*hostConnPool{},\n\t}\n\n\treturn pool\n}\n\nfunc (p *policyConnPool) SetHosts(hosts []*HostInfo) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\ttoRemove := make(map[string]struct{})\n\tfor hostID := range p.hostConnPools {\n\t\ttoRemove[hostID] = struct{}{}\n\t}\n\n\tpools := make(chan *hostConnPool)\n\tcreateCount := 0\n\tfor _, host := range hosts {\n\t\tif !host.IsUp() {\n\t\t\t// don't create a connection pool for a down host\n\t\t\tcontinue\n\t\t}\n\t\thostID := host.HostID()\n\t\tif _, exists := p.hostConnPools[hostID]; exists {\n\t\t\t// still have this host, so don't remove it\n\t\t\tdelete(toRemove, hostID)\n\t\t\tcontinue\n\t\t}\n\n\t\tcreateCount++\n\t\tgo func(host *HostInfo) {\n\t\t\t// create a connection pool for the host\n\t\t\tpools <- newHostConnPool(\n\t\t\t\tp.session,\n\t\t\t\thost,\n\t\t\t\tp.port,\n\t\t\t\tp.numConns,\n\t\t\t\tp.keyspace,\n\t\t\t)\n\t\t}(host)\n\t}\n\n\t// add created pools\n\tfor createCount > 0 {\n\t\tpool := <-pools\n\t\tcreateCount--\n\t\tif pool.Size() > 0 {\n\t\t\t// add pool only if there a connections available\n\t\t\tp.hostConnPools[pool.host.HostID()] = pool\n\t\t}\n\t}\n\n\tfor addr := range toRemove {\n\t\tpool := p.hostConnPools[addr]\n\t\tdelete(p.hostConnPools, addr)\n\t\tgo pool.Close()\n\t}\n}\n\nfunc (p *policyConnPool) Size() int {\n\tp.mu.RLock()\n\tcount := 0\n\tfor _, pool := range p.hostConnPools {\n\t\tcount += pool.Size()\n\t}\n\tp.mu.RUnlock()\n\n\treturn count\n}\n\nfunc (p *policyConnPool) getPool(host *HostInfo) (pool *hostConnPool, ok bool) {\n\thostID := host.HostID()\n\tp.mu.RLock()\n\tpool, ok = p.hostConnPools[hostID]\n\tp.mu.RUnlock()\n\treturn\n}\n\nfunc (p *policyConnPool) Close() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\t// close the pools\n\tfor addr, pool := range p.hostConnPools {\n\t\tdelete(p.hostConnPools, addr)\n\t\tpool.Close()\n\t}\n}\n\nfunc (p *policyConnPool) addHost(host *HostInfo) {\n\thostID := host.HostID()\n\tp.mu.Lock()\n\tpool, ok := p.hostConnPools[hostID]\n\tif !ok {\n\t\tpool = newHostConnPool(\n\t\t\tp.session,\n\t\t\thost,\n\t\t\thost.Port(), // TODO: if port == 0 use pool.port?\n\t\t\tp.numConns,\n\t\t\tp.keyspace,\n\t\t)\n\n\t\tp.hostConnPools[hostID] = pool\n\t}\n\tp.mu.Unlock()\n\n\tpool.fill()\n}\n\nfunc (p *policyConnPool) removeHost(hostID string) {\n\tp.mu.Lock()\n\tpool, ok := p.hostConnPools[hostID]\n\tif !ok {\n\t\tp.mu.Unlock()\n\t\treturn\n\t}\n\n\tdelete(p.hostConnPools, hostID)\n\tp.mu.Unlock()\n\n\tgo pool.Close()\n}\n\n// hostConnPool is a connection pool for a single host.\n// Connection selection is based on a provided ConnSelectionPolicy\ntype hostConnPool struct {\n\tsession  *Session\n\thost     *HostInfo\n\tport     int\n\tsize     int\n\tkeyspace string\n\t// protection for conns, closed, filling\n\tmu      sync.RWMutex\n\tconns   []*Conn\n\tclosed  bool\n\tfilling bool\n\n\tpos    uint32\n\tlogger StdLogger\n}\n\nfunc (h *hostConnPool) String() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn fmt.Sprintf(\"[filling=%v closed=%v conns=%v size=%v host=%v]\",\n\t\th.filling, h.closed, len(h.conns), h.size, h.host)\n}\n\nfunc newHostConnPool(session *Session, host *HostInfo, port, size int,\n\tkeyspace string) *hostConnPool {\n\n\tpool := &hostConnPool{\n\t\tsession:  session,\n\t\thost:     host,\n\t\tport:     port,\n\t\tsize:     size,\n\t\tkeyspace: keyspace,\n\t\tconns:    make([]*Conn, 0, size),\n\t\tfilling:  false,\n\t\tclosed:   false,\n\t\tlogger:   session.logger,\n\t}\n\n\t// the pool is not filled or connected\n\treturn pool\n}\n\n// Pick a connection from this connection pool for the given query.\nfunc (pool *hostConnPool) Pick() *Conn {\n\tpool.mu.RLock()\n\tdefer pool.mu.RUnlock()\n\n\tif pool.closed {\n\t\treturn nil\n\t}\n\n\tsize := len(pool.conns)\n\tif size < pool.size {\n\t\t// try to fill the pool\n\t\tgo pool.fill()\n\n\t\tif size == 0 {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tpos := int(atomic.AddUint32(&pool.pos, 1) - 1)\n\n\tvar (\n\t\tleastBusyConn    *Conn\n\t\tstreamsAvailable int\n\t)\n\n\t// find the conn which has the most available streams, this is racy\n\tfor i := 0; i < size; i++ {\n\t\tconn := pool.conns[(pos+i)%size]\n\t\tif streams := conn.AvailableStreams(); streams > streamsAvailable {\n\t\t\tleastBusyConn = conn\n\t\t\tstreamsAvailable = streams\n\t\t}\n\t}\n\n\treturn leastBusyConn\n}\n\n// Size returns the number of connections currently active in the pool\nfunc (pool *hostConnPool) Size() int {\n\tpool.mu.RLock()\n\tdefer pool.mu.RUnlock()\n\n\treturn len(pool.conns)\n}\n\n// Close the connection pool\nfunc (pool *hostConnPool) Close() {\n\tpool.mu.Lock()\n\n\tif pool.closed {\n\t\tpool.mu.Unlock()\n\t\treturn\n\t}\n\tpool.closed = true\n\n\t// ensure we dont try to reacquire the lock in handleError\n\t// TODO: improve this as the following can happen\n\t// 1) we have locked pool.mu write lock\n\t// 2) conn.Close calls conn.closeWithError(nil)\n\t// 3) conn.closeWithError calls conn.Close() which returns an error\n\t// 4) conn.closeWithError calls pool.HandleError with the error from conn.Close\n\t// 5) pool.HandleError tries to lock pool.mu\n\t// deadlock\n\n\t// empty the pool\n\tconns := pool.conns\n\tpool.conns = nil\n\n\tpool.mu.Unlock()\n\n\t// close the connections\n\tfor _, conn := range conns {\n\t\tconn.Close()\n\t}\n}\n\n// Fill the connection pool\nfunc (pool *hostConnPool) fill() {\n\tpool.mu.RLock()\n\t// avoid filling a closed pool, or concurrent filling\n\tif pool.closed || pool.filling {\n\t\tpool.mu.RUnlock()\n\t\treturn\n\t}\n\n\t// determine the filling work to be done\n\tstartCount := len(pool.conns)\n\tfillCount := pool.size - startCount\n\n\t// avoid filling a full (or overfull) pool\n\tif fillCount <= 0 {\n\t\tpool.mu.RUnlock()\n\t\treturn\n\t}\n\n\t// switch from read to write lock\n\tpool.mu.RUnlock()\n\tpool.mu.Lock()\n\n\t// double check everything since the lock was released\n\tstartCount = len(pool.conns)\n\tfillCount = pool.size - startCount\n\tif pool.closed || pool.filling || fillCount <= 0 {\n\t\t// looks like another goroutine already beat this\n\t\t// goroutine to the filling\n\t\tpool.mu.Unlock()\n\t\treturn\n\t}\n\n\t// ok fill the pool\n\tpool.filling = true\n\n\t// allow others to access the pool while filling\n\tpool.mu.Unlock()\n\t// only this goroutine should make calls to fill/empty the pool at this\n\t// point until after this routine or its subordinates calls\n\t// fillingStopped\n\n\t// fill only the first connection synchronously\n\tif startCount == 0 {\n\t\terr := pool.connect()\n\t\tpool.logConnectErr(err)\n\n\t\tif err != nil {\n\t\t\t// probably unreachable host\n\t\t\tpool.fillingStopped(err)\n\t\t\treturn\n\t\t}\n\t\t// notify the session that this node is connected\n\t\tgo pool.session.handleNodeConnected(pool.host)\n\n\t\t// filled one\n\t\tfillCount--\n\t}\n\n\t// fill the rest of the pool asynchronously\n\tgo func() {\n\t\terr := pool.connectMany(fillCount)\n\n\t\t// mark the end of filling\n\t\tpool.fillingStopped(err)\n\n\t\tif err == nil && startCount > 0 {\n\t\t\t// notify the session that this node is connected again\n\t\t\tgo pool.session.handleNodeConnected(pool.host)\n\t\t}\n\t}()\n}\n\nfunc (pool *hostConnPool) logConnectErr(err error) {\n\tif opErr, ok := err.(*net.OpError); ok && (opErr.Op == \"dial\" || opErr.Op == \"read\") {\n\t\t// connection refused\n\t\t// these are typical during a node outage so avoid log spam.\n\t\tif gocqlDebug {\n\t\t\tpool.logger.Printf(\"gocql: unable to dial %q: %v\\n\", pool.host, err)\n\t\t}\n\t} else if err != nil {\n\t\t// unexpected error\n\t\tpool.logger.Printf(\"error: failed to connect to %q due to error: %v\", pool.host, err)\n\t}\n}\n\n// transition back to a not-filling state.\nfunc (pool *hostConnPool) fillingStopped(err error) {\n\tif err != nil {\n\t\tif gocqlDebug {\n\t\t\tpool.logger.Printf(\"gocql: filling stopped %q: %v\\n\", pool.host.ConnectAddress(), err)\n\t\t}\n\t\t// wait for some time to avoid back-to-back filling\n\t\t// this provides some time between failed attempts\n\t\t// to fill the pool for the host to recover\n\t\ttime.Sleep(time.Duration(rand.Int31n(100)+31) * time.Millisecond)\n\t}\n\n\tpool.mu.Lock()\n\tpool.filling = false\n\tcount := len(pool.conns)\n\thost := pool.host\n\tport := pool.port\n\tpool.mu.Unlock()\n\n\t// if we errored and the size is now zero, make sure the host is marked as down\n\t// see https://github.com/apache/cassandra-gocql-driver/issues/1614\n\tif gocqlDebug {\n\t\tpool.logger.Printf(\"gocql: conns of pool after stopped %q: %v\\n\", host.ConnectAddress(), count)\n\t}\n\tif err != nil && count == 0 {\n\t\tif pool.session.cfg.ConvictionPolicy.AddFailure(err, host) {\n\t\t\tpool.session.handleNodeDown(host.ConnectAddress(), port)\n\t\t}\n\t}\n}\n\n// connectMany creates new connections concurrent.\nfunc (pool *hostConnPool) connectMany(count int) error {\n\tif count == 0 {\n\t\treturn nil\n\t}\n\tvar (\n\t\twg         sync.WaitGroup\n\t\tmu         sync.Mutex\n\t\tconnectErr error\n\t)\n\twg.Add(count)\n\tfor i := 0; i < count; i++ {\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\terr := pool.connect()\n\t\t\tpool.logConnectErr(err)\n\t\t\tif err != nil {\n\t\t\t\tmu.Lock()\n\t\t\t\tconnectErr = err\n\t\t\t\tmu.Unlock()\n\t\t\t}\n\t\t}()\n\t}\n\t// wait for all connections are done\n\twg.Wait()\n\n\treturn connectErr\n}\n\n// create a new connection to the host and add it to the pool\nfunc (pool *hostConnPool) connect() (err error) {\n\t// TODO: provide a more robust connection retry mechanism, we should also\n\t// be able to detect hosts that come up by trying to connect to downed ones.\n\t// try to connect\n\tvar conn *Conn\n\treconnectionPolicy := pool.session.cfg.ReconnectionPolicy\n\tfor i := 0; i < reconnectionPolicy.GetMaxRetries(); i++ {\n\t\tconn, err = pool.session.connect(pool.session.ctx, pool.host, pool)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif opErr, isOpErr := err.(*net.OpError); isOpErr {\n\t\t\t// if the error is not a temporary error (ex: network unreachable) don't\n\t\t\t//  retry\n\t\t\tif !opErr.Temporary() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif gocqlDebug {\n\t\t\tpool.logger.Printf(\"gocql: connection failed %q: %v, reconnecting with %T\\n\",\n\t\t\t\tpool.host.ConnectAddress(), err, reconnectionPolicy)\n\t\t}\n\t\ttime.Sleep(reconnectionPolicy.GetInterval(i))\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif pool.keyspace != \"\" {\n\t\t// set the keyspace\n\t\tif err = conn.UseKeyspace(pool.keyspace); err != nil {\n\t\t\tconn.Close()\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// add the Conn to the pool\n\tpool.mu.Lock()\n\tdefer pool.mu.Unlock()\n\n\tif pool.closed {\n\t\tconn.Close()\n\t\treturn nil\n\t}\n\n\tpool.conns = append(pool.conns, conn)\n\n\treturn nil\n}\n\n// handle any error from a Conn\nfunc (pool *hostConnPool) HandleError(conn *Conn, err error, closed bool) {\n\tif !closed {\n\t\t// still an open connection, so continue using it\n\t\treturn\n\t}\n\n\t// TODO: track the number of errors per host and detect when a host is dead,\n\t// then also have something which can detect when a host comes back.\n\tpool.mu.Lock()\n\tdefer pool.mu.Unlock()\n\n\tif pool.closed {\n\t\t// pool closed\n\t\treturn\n\t}\n\n\tif gocqlDebug {\n\t\tpool.logger.Printf(\"gocql: pool connection error %q: %v\\n\", conn.addr, err)\n\t}\n\n\t// find the connection index\n\tfor i, candidate := range pool.conns {\n\t\tif candidate == conn {\n\t\t\t// remove the connection, not preserving order\n\t\t\tpool.conns[i], pool.conns = pool.conns[len(pool.conns)-1], pool.conns[:len(pool.conns)-1]\n\n\t\t\t// lost a connection, so fill the pool\n\t\t\tgo pool.fill()\n\t\t\tbreak\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "connectionpool_test.go",
          "type": "blob",
          "size": 2.9541015625,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"crypto/tls\"\n\t\"testing\"\n)\n\nfunc TestSetupTLSConfig(t *testing.T) {\n\ttests := []struct {\n\t\tname                       string\n\t\topts                       *SslOptions\n\t\texpectedInsecureSkipVerify bool\n\t}{\n\t\t{\n\t\t\tname: \"Config nil, EnableHostVerification false\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: false,\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: true,\n\t\t},\n\t\t{\n\t\t\tname: \"Config nil, EnableHostVerification true\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: true,\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: false,\n\t\t},\n\t\t{\n\t\t\tname: \"Config.InsecureSkipVerify false, EnableHostVerification false\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: false,\n\t\t\t\tConfig: &tls.Config{\n\t\t\t\t\tInsecureSkipVerify: false,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: false,\n\t\t},\n\t\t{\n\t\t\tname: \"Config.InsecureSkipVerify true, EnableHostVerification false\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: false,\n\t\t\t\tConfig: &tls.Config{\n\t\t\t\t\tInsecureSkipVerify: true,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: true,\n\t\t},\n\t\t{\n\t\t\tname: \"Config.InsecureSkipVerify false, EnableHostVerification true\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: true,\n\t\t\t\tConfig: &tls.Config{\n\t\t\t\t\tInsecureSkipVerify: false,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: false,\n\t\t},\n\t\t{\n\t\t\tname: \"Config.InsecureSkipVerify true, EnableHostVerification true\",\n\t\t\topts: &SslOptions{\n\t\t\t\tEnableHostVerification: true,\n\t\t\t\tConfig: &tls.Config{\n\t\t\t\t\tInsecureSkipVerify: true,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedInsecureSkipVerify: false,\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\ttest := test\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\ttlsConfig, err := setupTLSConfig(test.opts)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error %q\", err.Error())\n\t\t\t}\n\t\t\tif tlsConfig.InsecureSkipVerify != test.expectedInsecureSkipVerify {\n\t\t\t\tt.Fatalf(\"got %v, but expected %v\", tlsConfig.InsecureSkipVerify,\n\t\t\t\t\ttest.expectedInsecureSkipVerify)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "control.go",
          "type": "blob",
          "size": 12.8359375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\tcrand \"crypto/rand\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nvar (\n\trandr    *rand.Rand\n\tmutRandr sync.Mutex\n)\n\nfunc init() {\n\tb := make([]byte, 4)\n\tif _, err := crand.Read(b); err != nil {\n\t\tpanic(fmt.Sprintf(\"unable to seed random number generator: %v\", err))\n\t}\n\n\trandr = rand.New(rand.NewSource(int64(readInt(b))))\n}\n\nconst (\n\tcontrolConnStarting = 0\n\tcontrolConnStarted  = 1\n\tcontrolConnClosing  = -1\n)\n\n// Ensure that the atomic variable is aligned to a 64bit boundary\n// so that atomic operations can be applied on 32bit architectures.\ntype controlConn struct {\n\tstate        int32\n\treconnecting int32\n\n\tsession *Session\n\tconn    atomic.Value\n\n\tretry RetryPolicy\n\n\tquit chan struct{}\n}\n\nfunc createControlConn(session *Session) *controlConn {\n\tcontrol := &controlConn{\n\t\tsession: session,\n\t\tquit:    make(chan struct{}),\n\t\tretry:   &SimpleRetryPolicy{NumRetries: 3},\n\t}\n\n\tcontrol.conn.Store((*connHost)(nil))\n\n\treturn control\n}\n\nfunc (c *controlConn) heartBeat() {\n\tif !atomic.CompareAndSwapInt32(&c.state, controlConnStarting, controlConnStarted) {\n\t\treturn\n\t}\n\n\tsleepTime := 1 * time.Second\n\ttimer := time.NewTimer(sleepTime)\n\tdefer timer.Stop()\n\n\tfor {\n\t\ttimer.Reset(sleepTime)\n\n\t\tselect {\n\t\tcase <-c.quit:\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t}\n\n\t\tresp, err := c.writeFrame(&writeOptionsFrame{})\n\t\tif err != nil {\n\t\t\tgoto reconn\n\t\t}\n\n\t\tswitch resp.(type) {\n\t\tcase *supportedFrame:\n\t\t\t// Everything ok\n\t\t\tsleepTime = 5 * time.Second\n\t\t\tcontinue\n\t\tcase error:\n\t\t\tgoto reconn\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"gocql: unknown frame in response to options: %T\", resp))\n\t\t}\n\n\treconn:\n\t\t// try to connect a bit faster\n\t\tsleepTime = 1 * time.Second\n\t\tc.reconnect()\n\t\tcontinue\n\t}\n}\n\nvar hostLookupPreferV4 = os.Getenv(\"GOCQL_HOST_LOOKUP_PREFER_V4\") == \"true\"\n\nfunc hostInfo(addr string, defaultPort int) ([]*HostInfo, error) {\n\tvar port int\n\thost, portStr, err := net.SplitHostPort(addr)\n\tif err != nil {\n\t\thost = addr\n\t\tport = defaultPort\n\t} else {\n\t\tport, err = strconv.Atoi(portStr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar hosts []*HostInfo\n\n\t// Check if host is a literal IP address\n\tif ip := net.ParseIP(host); ip != nil {\n\t\thosts = append(hosts, &HostInfo{hostname: host, connectAddress: ip, port: port})\n\t\treturn hosts, nil\n\t}\n\n\t// Look up host in DNS\n\tips, err := LookupIP(host)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if len(ips) == 0 {\n\t\treturn nil, fmt.Errorf(\"no IP's returned from DNS lookup for %q\", addr)\n\t}\n\n\t// Filter to v4 addresses if any present\n\tif hostLookupPreferV4 {\n\t\tvar preferredIPs []net.IP\n\t\tfor _, v := range ips {\n\t\t\tif v4 := v.To4(); v4 != nil {\n\t\t\t\tpreferredIPs = append(preferredIPs, v4)\n\t\t\t}\n\t\t}\n\t\tif len(preferredIPs) != 0 {\n\t\t\tips = preferredIPs\n\t\t}\n\t}\n\n\tfor _, ip := range ips {\n\t\thosts = append(hosts, &HostInfo{hostname: host, connectAddress: ip, port: port})\n\t}\n\n\treturn hosts, nil\n}\n\nfunc shuffleHosts(hosts []*HostInfo) []*HostInfo {\n\tshuffled := make([]*HostInfo, len(hosts))\n\tcopy(shuffled, hosts)\n\n\tmutRandr.Lock()\n\trandr.Shuffle(len(hosts), func(i, j int) {\n\t\tshuffled[i], shuffled[j] = shuffled[j], shuffled[i]\n\t})\n\tmutRandr.Unlock()\n\n\treturn shuffled\n}\n\n// this is going to be version dependant and a nightmare to maintain :(\nvar protocolSupportRe = regexp.MustCompile(`the lowest supported version is \\d+ and the greatest is (\\d+)$`)\n\nfunc parseProtocolFromError(err error) int {\n\t// I really wish this had the actual info in the error frame...\n\tmatches := protocolSupportRe.FindAllStringSubmatch(err.Error(), -1)\n\tif len(matches) != 1 || len(matches[0]) != 2 {\n\t\tif verr, ok := err.(*protocolError); ok {\n\t\t\treturn int(verr.frame.Header().version.version())\n\t\t}\n\t\treturn 0\n\t}\n\n\tmax, err := strconv.Atoi(matches[0][1])\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\treturn max\n}\n\nfunc (c *controlConn) discoverProtocol(hosts []*HostInfo) (int, error) {\n\thosts = shuffleHosts(hosts)\n\n\tconnCfg := *c.session.connCfg\n\tconnCfg.ProtoVersion = 4 // TODO: define maxProtocol\n\n\thandler := connErrorHandlerFn(func(c *Conn, err error, closed bool) {\n\t\t// we should never get here, but if we do it means we connected to a\n\t\t// host successfully which means our attempted protocol version worked\n\t\tif !closed {\n\t\t\tc.Close()\n\t\t}\n\t})\n\n\tvar err error\n\tfor _, host := range hosts {\n\t\tvar conn *Conn\n\t\tconn, err = c.session.dial(c.session.ctx, host, &connCfg, handler)\n\t\tif conn != nil {\n\t\t\tconn.Close()\n\t\t}\n\n\t\tif err == nil {\n\t\t\treturn connCfg.ProtoVersion, nil\n\t\t}\n\n\t\tif proto := parseProtocolFromError(err); proto > 0 {\n\t\t\treturn proto, nil\n\t\t}\n\t}\n\n\treturn 0, err\n}\n\nfunc (c *controlConn) connect(hosts []*HostInfo) error {\n\tif len(hosts) == 0 {\n\t\treturn errors.New(\"control: no endpoints specified\")\n\t}\n\n\t// shuffle endpoints so not all drivers will connect to the same initial\n\t// node.\n\thosts = shuffleHosts(hosts)\n\n\tcfg := *c.session.connCfg\n\tcfg.disableCoalesce = true\n\n\tvar conn *Conn\n\tvar err error\n\tfor _, host := range hosts {\n\t\tconn, err = c.session.dial(c.session.ctx, host, &cfg, c)\n\t\tif err != nil {\n\t\t\tc.session.logger.Printf(\"gocql: unable to dial control conn %v:%v: %v\\n\", host.ConnectAddress(), host.Port(), err)\n\t\t\tcontinue\n\t\t}\n\t\terr = c.setupConn(conn)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tc.session.logger.Printf(\"gocql: unable setup control conn %v:%v: %v\\n\", host.ConnectAddress(), host.Port(), err)\n\t\tconn.Close()\n\t\tconn = nil\n\t}\n\tif conn == nil {\n\t\treturn fmt.Errorf(\"unable to connect to initial hosts: %v\", err)\n\t}\n\n\t// we could fetch the initial ring here and update initial host data. So that\n\t// when we return from here we have a ring topology ready to go.\n\n\tgo c.heartBeat()\n\n\treturn nil\n}\n\ntype connHost struct {\n\tconn *Conn\n\thost *HostInfo\n}\n\nfunc (c *controlConn) setupConn(conn *Conn) error {\n\t// we need up-to-date host info for the filterHost call below\n\titer := conn.querySystemLocal(context.TODO())\n\thost, err := c.session.hostInfoFromIter(iter, conn.host.connectAddress, conn.conn.RemoteAddr().(*net.TCPAddr).Port)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\thost = c.session.ring.addOrUpdate(host)\n\n\tif c.session.cfg.filterHost(host) {\n\t\treturn fmt.Errorf(\"host was filtered: %v\", host.ConnectAddress())\n\t}\n\n\tif err := c.registerEvents(conn); err != nil {\n\t\treturn fmt.Errorf(\"register events: %v\", err)\n\t}\n\n\tch := &connHost{\n\t\tconn: conn,\n\t\thost: host,\n\t}\n\n\tc.conn.Store(ch)\n\tif c.session.initialized() {\n\t\t// We connected to control conn, so add the connect the host in pool as well.\n\t\t// Notify session we can start trying to connect to the node.\n\t\t// We can't start the fill before the session is initialized, otherwise the fill would interfere\n\t\t// with the fill called by Session.init. Session.init needs to wait for its fill to finish and that\n\t\t// would return immediately if we started the fill here.\n\t\t// TODO(martin-sucha): Trigger pool refill for all hosts, like in reconnectDownedHosts?\n\t\tgo c.session.startPoolFill(host)\n\t}\n\treturn nil\n}\n\nfunc (c *controlConn) registerEvents(conn *Conn) error {\n\tvar events []string\n\n\tif !c.session.cfg.Events.DisableTopologyEvents {\n\t\tevents = append(events, \"TOPOLOGY_CHANGE\")\n\t}\n\tif !c.session.cfg.Events.DisableNodeStatusEvents {\n\t\tevents = append(events, \"STATUS_CHANGE\")\n\t}\n\tif !c.session.cfg.Events.DisableSchemaEvents {\n\t\tevents = append(events, \"SCHEMA_CHANGE\")\n\t}\n\n\tif len(events) == 0 {\n\t\treturn nil\n\t}\n\n\tframer, err := conn.exec(context.Background(),\n\t\t&writeRegisterFrame{\n\t\t\tevents: events,\n\t\t}, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tframe, err := framer.parseFrame()\n\tif err != nil {\n\t\treturn err\n\t} else if _, ok := frame.(*readyFrame); !ok {\n\t\treturn fmt.Errorf(\"unexpected frame in response to register: got %T: %v\\n\", frame, frame)\n\t}\n\n\treturn nil\n}\n\nfunc (c *controlConn) reconnect() {\n\tif atomic.LoadInt32(&c.state) == controlConnClosing {\n\t\treturn\n\t}\n\tif !atomic.CompareAndSwapInt32(&c.reconnecting, 0, 1) {\n\t\treturn\n\t}\n\tdefer atomic.StoreInt32(&c.reconnecting, 0)\n\n\tconn, err := c.attemptReconnect()\n\n\tif conn == nil {\n\t\tc.session.logger.Printf(\"gocql: unable to reconnect control connection: %v\\n\", err)\n\t\treturn\n\t}\n\n\terr = c.session.refreshRing()\n\tif err != nil {\n\t\tc.session.logger.Printf(\"gocql: unable to refresh ring: %v\\n\", err)\n\t}\n}\n\nfunc (c *controlConn) attemptReconnect() (*Conn, error) {\n\thosts := c.session.ring.allHosts()\n\thosts = shuffleHosts(hosts)\n\n\t// keep the old behavior of connecting to the old host first by moving it to\n\t// the front of the slice\n\tch := c.getConn()\n\tif ch != nil {\n\t\tfor i := range hosts {\n\t\t\tif hosts[i].Equal(ch.host) {\n\t\t\t\thosts[0], hosts[i] = hosts[i], hosts[0]\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tch.conn.Close()\n\t}\n\n\tconn, err := c.attemptReconnectToAnyOfHosts(hosts)\n\n\tif conn != nil {\n\t\treturn conn, err\n\t}\n\n\tc.session.logger.Printf(\"gocql: unable to connect to any ring node: %v\\n\", err)\n\tc.session.logger.Printf(\"gocql: control falling back to initial contact points.\\n\")\n\t// Fallback to initial contact points, as it may be the case that all known initialHosts\n\t// changed their IPs while keeping the same hostname(s).\n\tinitialHosts, resolvErr := addrsToHosts(c.session.cfg.Hosts, c.session.cfg.Port, c.session.logger)\n\tif resolvErr != nil {\n\t\treturn nil, fmt.Errorf(\"resolve contact points' hostnames: %v\", resolvErr)\n\t}\n\n\treturn c.attemptReconnectToAnyOfHosts(initialHosts)\n}\n\nfunc (c *controlConn) attemptReconnectToAnyOfHosts(hosts []*HostInfo) (*Conn, error) {\n\tvar conn *Conn\n\tvar err error\n\tfor _, host := range hosts {\n\t\tconn, err = c.session.connect(c.session.ctx, host, c)\n\t\tif err != nil {\n\t\t\tc.session.logger.Printf(\"gocql: unable to dial control conn %v:%v: %v\\n\", host.ConnectAddress(), host.Port(), err)\n\t\t\tcontinue\n\t\t}\n\t\terr = c.setupConn(conn)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tc.session.logger.Printf(\"gocql: unable setup control conn %v:%v: %v\\n\", host.ConnectAddress(), host.Port(), err)\n\t\tconn.Close()\n\t\tconn = nil\n\t}\n\treturn conn, err\n}\n\nfunc (c *controlConn) HandleError(conn *Conn, err error, closed bool) {\n\tif !closed {\n\t\treturn\n\t}\n\n\toldConn := c.getConn()\n\n\t// If connection has long gone, and not been attempted for awhile,\n\t// it's possible to have oldConn as nil here (#1297).\n\tif oldConn != nil && oldConn.conn != conn {\n\t\treturn\n\t}\n\n\tc.reconnect()\n}\n\nfunc (c *controlConn) getConn() *connHost {\n\treturn c.conn.Load().(*connHost)\n}\n\nfunc (c *controlConn) writeFrame(w frameBuilder) (frame, error) {\n\tch := c.getConn()\n\tif ch == nil {\n\t\treturn nil, errNoControl\n\t}\n\n\tframer, err := ch.conn.exec(context.Background(), w, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn framer.parseFrame()\n}\n\nfunc (c *controlConn) withConnHost(fn func(*connHost) *Iter) *Iter {\n\tconst maxConnectAttempts = 5\n\tconnectAttempts := 0\n\n\tfor i := 0; i < maxConnectAttempts; i++ {\n\t\tch := c.getConn()\n\t\tif ch == nil {\n\t\t\tif connectAttempts > maxConnectAttempts {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tconnectAttempts++\n\n\t\t\tc.reconnect()\n\t\t\tcontinue\n\t\t}\n\n\t\treturn fn(ch)\n\t}\n\n\treturn &Iter{err: errNoControl}\n}\n\nfunc (c *controlConn) withConn(fn func(*Conn) *Iter) *Iter {\n\treturn c.withConnHost(func(ch *connHost) *Iter {\n\t\treturn fn(ch.conn)\n\t})\n}\n\n// query will return nil if the connection is closed or nil\nfunc (c *controlConn) query(statement string, values ...interface{}) (iter *Iter) {\n\tq := c.session.Query(statement, values...).Consistency(One).RoutingKey([]byte{}).Trace(nil)\n\n\tfor {\n\t\titer = c.withConn(func(conn *Conn) *Iter {\n\t\t\t// we want to keep the query on the control connection\n\t\t\tq.conn = conn\n\t\t\treturn conn.executeQuery(context.TODO(), q)\n\t\t})\n\n\t\tif gocqlDebug && iter.err != nil {\n\t\t\tc.session.logger.Printf(\"control: error executing %q: %v\\n\", statement, iter.err)\n\t\t}\n\n\t\tq.AddAttempts(1, c.getConn().host)\n\t\tif iter.err == nil || !c.retry.Attempt(q) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (c *controlConn) awaitSchemaAgreement() error {\n\treturn c.withConn(func(conn *Conn) *Iter {\n\t\treturn &Iter{err: conn.awaitSchemaAgreement(context.TODO())}\n\t}).err\n}\n\nfunc (c *controlConn) close() {\n\tif atomic.CompareAndSwapInt32(&c.state, controlConnStarted, controlConnClosing) {\n\t\tc.quit <- struct{}{}\n\t}\n\n\tch := c.getConn()\n\tif ch != nil {\n\t\tch.conn.Close()\n\t}\n}\n\nvar errNoControl = errors.New(\"gocql: no control connection available\")\n"
        },
        {
          "name": "control_ccm_test.go",
          "type": "blob",
          "size": 4.880859375,
          "content": "//go:build ccm\n// +build ccm\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gocql/gocql/internal/ccm\"\n)\n\ntype TestHostFilter struct {\n\tmu           sync.Mutex\n\tallowedHosts map[string]ccm.Host\n}\n\nfunc (f *TestHostFilter) Accept(h *HostInfo) bool {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\t_, ok := f.allowedHosts[h.ConnectAddress().String()]\n\treturn ok\n}\n\nfunc (f *TestHostFilter) SetAllowedHosts(hosts map[string]ccm.Host) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tf.allowedHosts = hosts\n}\n\nfunc TestControlConn_ReconnectRefreshesRing(t *testing.T) {\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tallCcmHosts, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(allCcmHosts) < 2 {\n\t\tt.Skip(\"this test requires at least 2 nodes\")\n\t}\n\n\tallAllowedHosts := map[string]ccm.Host{}\n\tvar firstNode *ccm.Host\n\tfor _, node := range allCcmHosts {\n\t\tif firstNode == nil {\n\t\t\tfirstNode = &node\n\t\t}\n\t\tallAllowedHosts[node.Addr] = node\n\t}\n\n\tallowedHosts := map[string]ccm.Host{\n\t\tfirstNode.Addr: *firstNode,\n\t}\n\n\ttestFilter := &TestHostFilter{allowedHosts: allowedHosts}\n\n\tsession := createSession(t, func(config *ClusterConfig) {\n\t\tconfig.Hosts = []string{firstNode.Addr}\n\t\tconfig.Events.DisableTopologyEvents = true\n\t\tconfig.Events.DisableNodeStatusEvents = true\n\t\tconfig.HostFilter = testFilter\n\t})\n\tdefer session.Close()\n\n\tif session.control == nil || session.control.conn.Load() == nil {\n\t\tt.Fatal(\"control conn is nil\")\n\t}\n\n\tcontrolConnection := session.control.getConn()\n\tccHost := controlConnection.host\n\n\tvar ccHostName string\n\tfor _, node := range allCcmHosts {\n\t\tif node.Addr == ccHost.ConnectAddress().String() {\n\t\t\tccHostName = node.Name\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif ccHostName == \"\" {\n\t\tt.Fatal(\"could not find name of control host\")\n\t}\n\n\tif err := ccm.NodeDown(ccHostName); err != nil {\n\t\tt.Fatal()\n\t}\n\n\tdefer func() {\n\t\tccmStatus, err := ccm.Status()\n\t\tif err != nil {\n\t\t\tt.Logf(\"could not bring nodes back up after test: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tfor _, node := range ccmStatus {\n\t\t\tif node.State == ccm.NodeStateDown {\n\t\t\t\terr = ccm.NodeUp(node.Name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Logf(\"could not bring node %v back up after test: %v\", node.Name, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tassertNodeDown := func() error {\n\t\thosts := session.ring.currentHosts()\n\t\tif len(hosts) != 1 {\n\t\t\treturn fmt.Errorf(\"expected 1 host in ring but there were %v\", len(hosts))\n\t\t}\n\t\tfor _, host := range hosts {\n\t\t\tif host.IsUp() {\n\t\t\t\treturn fmt.Errorf(\"expected host to be DOWN but %v isn't\", host.String())\n\t\t\t}\n\t\t}\n\n\t\tsession.pool.mu.RLock()\n\t\tpoolsLen := len(session.pool.hostConnPools)\n\t\tsession.pool.mu.RUnlock()\n\t\tif poolsLen != 0 {\n\t\t\treturn fmt.Errorf(\"expected 0 connection pool but there were %v\", poolsLen)\n\t\t}\n\t\treturn nil\n\t}\n\n\tmaxAttempts := 5\n\tdelayPerAttempt := 1 * time.Second\n\tassertErr := assertNodeDown()\n\tfor i := 0; i < maxAttempts && assertErr != nil; i++ {\n\t\ttime.Sleep(delayPerAttempt)\n\t\tassertErr = assertNodeDown()\n\t}\n\n\tif assertErr != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttestFilter.SetAllowedHosts(allAllowedHosts)\n\n\tif err = ccm.NodeUp(ccHostName); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tassertNodeUp := func() error {\n\t\thosts := session.ring.currentHosts()\n\t\tif len(hosts) != len(allCcmHosts) {\n\t\t\treturn fmt.Errorf(\"expected %v hosts in ring but there were %v\", len(allCcmHosts), len(hosts))\n\t\t}\n\t\tfor _, host := range hosts {\n\t\t\tif !host.IsUp() {\n\t\t\t\treturn fmt.Errorf(\"expected all hosts to be UP but %v isn't\", host.String())\n\t\t\t}\n\t\t}\n\t\tsession.pool.mu.RLock()\n\t\tpoolsLen := len(session.pool.hostConnPools)\n\t\tsession.pool.mu.RUnlock()\n\t\tif poolsLen != len(allCcmHosts) {\n\t\t\treturn fmt.Errorf(\"expected %v connection pool but there were %v\", len(allCcmHosts), poolsLen)\n\t\t}\n\t\treturn nil\n\t}\n\n\tmaxAttempts = 30\n\tdelayPerAttempt = 1 * time.Second\n\tassertErr = assertNodeUp()\n\tfor i := 0; i < maxAttempts && assertErr != nil; i++ {\n\t\ttime.Sleep(delayPerAttempt)\n\t\tassertErr = assertNodeUp()\n\t}\n\n\tif assertErr != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "control_test.go",
          "type": "blob",
          "size": 2.341796875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestHostInfo_Lookup(t *testing.T) {\n\thostLookupPreferV4 = true\n\tdefer func() { hostLookupPreferV4 = false }()\n\n\ttests := [...]struct {\n\t\taddr string\n\t\tip   net.IP\n\t}{\n\t\t{\"127.0.0.1\", net.IPv4(127, 0, 0, 1)},\n\t\t{\"localhost\", net.IPv4(127, 0, 0, 1)}, // TODO: this may be host dependant\n\t}\n\n\tfor i, test := range tests {\n\t\thosts, err := hostInfo(test.addr, 1)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%d: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\thost := hosts[0]\n\t\tif !host.ConnectAddress().Equal(test.ip) {\n\t\t\tt.Errorf(\"expected ip %v got %v for addr %q\", test.ip, host.ConnectAddress(), test.addr)\n\t\t}\n\t}\n}\n\nfunc TestParseProtocol(t *testing.T) {\n\ttests := [...]struct {\n\t\terr   error\n\t\tproto int\n\t}{\n\t\t{\n\t\t\terr: &protocolError{\n\t\t\t\tframe: errorFrame{\n\t\t\t\t\tcode:    0x10,\n\t\t\t\t\tmessage: \"Invalid or unsupported protocol version (5); the lowest supported version is 3 and the greatest is 4\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tproto: 4,\n\t\t},\n\t\t{\n\t\t\terr: &protocolError{\n\t\t\t\tframe: errorFrame{\n\t\t\t\t\tframeHeader: frameHeader{\n\t\t\t\t\t\tversion: 0x83,\n\t\t\t\t\t},\n\t\t\t\t\tcode:    0x10,\n\t\t\t\t\tmessage: \"Invalid or unsupported protocol version: 5\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tproto: 3,\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tif proto := parseProtocolFromError(test.err); proto != test.proto {\n\t\t\tt.Errorf(\"%d: exepcted proto %d got %d\", i, test.proto, proto)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "cqltypes.go",
          "type": "blob",
          "size": 1.115234375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\ntype Duration struct {\n\tMonths      int32\n\tDays        int32\n\tNanoseconds int64\n}\n"
        },
        {
          "name": "debug_off.go",
          "type": "blob",
          "size": 1.1064453125,
          "content": "//go:build !gocql_debug\n// +build !gocql_debug\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nconst gocqlDebug = false\n"
        },
        {
          "name": "debug_on.go",
          "type": "blob",
          "size": 1.103515625,
          "content": "//go:build gocql_debug\n// +build gocql_debug\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nconst gocqlDebug = true\n"
        },
        {
          "name": "dial.go",
          "type": "blob",
          "size": 3.9482421875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n)\n\n// HostDialer allows customizing connection to cluster nodes.\ntype HostDialer interface {\n\t// DialHost establishes a connection to the host.\n\t// The returned connection must be directly usable for CQL protocol,\n\t// specifically DialHost is responsible also for setting up the TLS session if needed.\n\t// DialHost should disable write coalescing if the returned net.Conn does not support writev.\n\t// As of Go 1.18, only plain TCP connections support writev, TLS sessions should disable coalescing.\n\t// You can use WrapTLS helper function if you don't need to override the TLS setup.\n\tDialHost(ctx context.Context, host *HostInfo) (*DialedHost, error)\n}\n\n// DialedHost contains information about established connection to a host.\ntype DialedHost struct {\n\t// Conn used to communicate with the server.\n\tConn net.Conn\n\n\t// DisableCoalesce disables write coalescing for the Conn.\n\t// If true, the effect is the same as if WriteCoalesceWaitTime was configured to 0.\n\tDisableCoalesce bool\n}\n\n// defaultHostDialer dials host in a default way.\ntype defaultHostDialer struct {\n\tdialer    Dialer\n\ttlsConfig *tls.Config\n}\n\nfunc (hd *defaultHostDialer) DialHost(ctx context.Context, host *HostInfo) (*DialedHost, error) {\n\tip := host.ConnectAddress()\n\tport := host.Port()\n\n\tif !validIpAddr(ip) {\n\t\treturn nil, fmt.Errorf(\"host missing connect ip address: %v\", ip)\n\t} else if port == 0 {\n\t\treturn nil, fmt.Errorf(\"host missing port: %v\", port)\n\t}\n\n\tconnAddr := host.ConnectAddressAndPort()\n\tconn, err := hd.dialer.DialContext(ctx, \"tcp\", connAddr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\taddr := host.HostnameAndPort()\n\treturn WrapTLS(ctx, conn, addr, hd.tlsConfig)\n}\n\nfunc tlsConfigForAddr(tlsConfig *tls.Config, addr string) *tls.Config {\n\t// the TLS config is safe to be reused by connections but it must not\n\t// be modified after being used.\n\tif !tlsConfig.InsecureSkipVerify && tlsConfig.ServerName == \"\" {\n\t\tcolonPos := strings.LastIndex(addr, \":\")\n\t\tif colonPos == -1 {\n\t\t\tcolonPos = len(addr)\n\t\t}\n\t\thostname := addr[:colonPos]\n\t\t// clone config to avoid modifying the shared one.\n\t\ttlsConfig = tlsConfig.Clone()\n\t\ttlsConfig.ServerName = hostname\n\t}\n\treturn tlsConfig\n}\n\n// WrapTLS optionally wraps a net.Conn connected to addr with the given tlsConfig.\n// If the tlsConfig is nil, conn is not wrapped into a TLS session, so is insecure.\n// If the tlsConfig does not have server name set, it is updated based on the default gocql rules.\nfunc WrapTLS(ctx context.Context, conn net.Conn, addr string, tlsConfig *tls.Config) (*DialedHost, error) {\n\tif tlsConfig != nil {\n\t\ttlsConfig := tlsConfigForAddr(tlsConfig, addr)\n\t\ttconn := tls.Client(conn, tlsConfig)\n\t\tif err := tconn.HandshakeContext(ctx); err != nil {\n\t\t\tconn.Close()\n\t\t\treturn nil, err\n\t\t}\n\t\tconn = tconn\n\t}\n\n\treturn &DialedHost{\n\t\tConn:            conn,\n\t\tDisableCoalesce: tlsConfig != nil, // write coalescing can't use writev when the connection is wrapped.\n\t}, nil\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 18.048828125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\n// Package gocql implements a fast and robust Cassandra driver for the\n// Go programming language.\n//\n// # Connecting to the cluster\n//\n// Pass a list of initial node IP addresses to NewCluster to create a new cluster configuration:\n//\n//\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n//\n// Port can be specified as part of the address, the above is equivalent to:\n//\n//\tcluster := gocql.NewCluster(\"192.168.1.1:9042\", \"192.168.1.2:9042\", \"192.168.1.3:9042\")\n//\n// It is recommended to use the value set in the Cassandra config for broadcast_address or listen_address,\n// an IP address not a domain name. This is because events from Cassandra will use the configured IP\n// address, which is used to index connected hosts. If the domain name specified resolves to more than 1 IP address\n// then the driver may connect multiple times to the same host, and will not mark the node being down or up from events.\n//\n// Then you can customize more options (see ClusterConfig):\n//\n//\tcluster.Keyspace = \"example\"\n//\tcluster.Consistency = gocql.Quorum\n//\tcluster.ProtoVersion = 4\n//\n// The driver tries to automatically detect the protocol version to use if not set, but you might want to set the\n// protocol version explicitly, as it's not defined which version will be used in certain situations (for example\n// during upgrade of the cluster when some of the nodes support different set of protocol versions than other nodes).\n//\n// The driver advertises the module name and version in the STARTUP message, so servers are able to detect the version.\n// If you use replace directive in go.mod, the driver will send information about the replacement module instead.\n//\n// When ready, create a session from the configuration. Don't forget to Close the session once you are done with it:\n//\n//\tsession, err := cluster.CreateSession()\n//\tif err != nil {\n//\t\treturn err\n//\t}\n//\tdefer session.Close()\n//\n// # Authentication\n//\n// CQL protocol uses a SASL-based authentication mechanism and so consists of an exchange of server challenges and\n// client response pairs. The details of the exchanged messages depend on the authenticator used.\n//\n// To use authentication, set ClusterConfig.Authenticator or ClusterConfig.AuthProvider.\n//\n// PasswordAuthenticator is provided to use for username/password authentication:\n//\n//\t cluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n//\t cluster.Authenticator = gocql.PasswordAuthenticator{\n//\t\t\tUsername: \"user\",\n//\t\t\tPassword: \"password\"\n//\t }\n//\t session, err := cluster.CreateSession()\n//\t if err != nil {\n//\t \treturn err\n//\t }\n//\t defer session.Close()\n//\n// By default, PasswordAuthenticator will attempt to authenticate regardless of what implementation the server returns\n// in its AUTHENTICATE message as its authenticator, (e.g. org.apache.cassandra.auth.PasswordAuthenticator).  If you\n// wish to restrict this you may use PasswordAuthenticator.AllowedAuthenticators:\n//\n//\t cluster.Authenticator = gocql.PasswordAuthenticator {\n//\t\t\tUsername:              \"user\",\n//\t\t\tPassword:              \"password\"\n//\t\t\tAllowedAuthenticators: []string{\"org.apache.cassandra.auth.PasswordAuthenticator\"},\n//\t }\n//\n// # Transport layer security\n//\n// It is possible to secure traffic between the client and server with TLS.\n//\n// To use TLS, set the ClusterConfig.SslOpts field. SslOptions embeds *tls.Config so you can set that directly.\n// There are also helpers to load keys/certificates from files.\n//\n// Warning: Due to historical reasons, the SslOptions is insecure by default, so you need to set EnableHostVerification\n// to true if no Config is set. Most users should set SslOptions.Config to a *tls.Config.\n// SslOptions and Config.InsecureSkipVerify interact as follows:\n//\n//\tConfig.InsecureSkipVerify | EnableHostVerification | Result\n//\tConfig is nil             | false                  | do not verify host\n//\tConfig is nil             | true                   | verify host\n//\tfalse                     | false                  | verify host\n//\ttrue                      | false                  | do not verify host\n//\tfalse                     | true                   | verify host\n//\ttrue                      | true                   | verify host\n//\n// For example:\n//\n//\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n//\tcluster.SslOpts = &gocql.SslOptions{\n//\t\tEnableHostVerification: true,\n//\t}\n//\tsession, err := cluster.CreateSession()\n//\tif err != nil {\n//\t\treturn err\n//\t}\n//\tdefer session.Close()\n//\n// # Data-center awareness and query routing\n//\n// To route queries to local DC first, use DCAwareRoundRobinPolicy. For example, if the datacenter you\n// want to primarily connect is called dc1 (as configured in the database):\n//\n//\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n//\tcluster.PoolConfig.HostSelectionPolicy = gocql.DCAwareRoundRobinPolicy(\"dc1\")\n//\n// The driver can route queries to nodes that hold data replicas based on partition key (preferring local DC).\n//\n//\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n//\tcluster.PoolConfig.HostSelectionPolicy = gocql.TokenAwareHostPolicy(gocql.DCAwareRoundRobinPolicy(\"dc1\"))\n//\n// Note that TokenAwareHostPolicy can take options such as gocql.ShuffleReplicas and gocql.NonLocalReplicasFallback.\n//\n// We recommend running with a token aware host policy in production for maximum performance.\n//\n// The driver can only use token-aware routing for queries where all partition key columns are query parameters.\n// For example, instead of\n//\n//\tsession.Query(\"select value from mytable where pk1 = 'abc' AND pk2 = ?\", \"def\")\n//\n// use\n//\n//\tsession.Query(\"select value from mytable where pk1 = ? AND pk2 = ?\", \"abc\", \"def\")\n//\n// # Rack-level awareness\n//\n// The DCAwareRoundRobinPolicy can be replaced with RackAwareRoundRobinPolicy, which takes two parameters, datacenter and rack.\n//\n// Instead of dividing hosts with two tiers (local datacenter and remote datacenters) it divides hosts into three\n// (the local rack, the rest of the local datacenter, and everything else).\n//\n// RackAwareRoundRobinPolicy can be combined with TokenAwareHostPolicy in the same way as DCAwareRoundRobinPolicy.\n//\n// # Executing queries\n//\n// Create queries with Session.Query. Query values must not be reused between different executions and must not be\n// modified after starting execution of the query.\n//\n// To execute a query without reading results, use Query.Exec:\n//\n//\t err := session.Query(`INSERT INTO tweet (timeline, id, text) VALUES (?, ?, ?)`,\n//\t\t\t\"me\", gocql.TimeUUID(), \"hello world\").WithContext(ctx).Exec()\n//\n// Single row can be read by calling Query.Scan:\n//\n//\t err := session.Query(`SELECT id, text FROM tweet WHERE timeline = ? LIMIT 1`,\n//\t\t\t\"me\").WithContext(ctx).Consistency(gocql.One).Scan(&id, &text)\n//\n// Multiple rows can be read using Iter.Scanner:\n//\n//\t scanner := session.Query(`SELECT id, text FROM tweet WHERE timeline = ?`,\n//\t \t\"me\").WithContext(ctx).Iter().Scanner()\n//\t for scanner.Next() {\n//\t \tvar (\n//\t \t\tid gocql.UUID\n//\t\t\ttext string\n//\t \t)\n//\t \terr = scanner.Scan(&id, &text)\n//\t \tif err != nil {\n//\t \t\tlog.Fatal(err)\n//\t \t}\n//\t \tfmt.Println(\"Tweet:\", id, text)\n//\t }\n//\t // scanner.Err() closes the iterator, so scanner nor iter should be used afterwards.\n//\t if err := scanner.Err(); err != nil {\n//\t \tlog.Fatal(err)\n//\t }\n//\n// See Example for complete example.\n//\n// # Prepared statements\n//\n// The driver automatically prepares DML queries (SELECT/INSERT/UPDATE/DELETE/BATCH statements) and maintains a cache\n// of prepared statements.\n// CQL protocol does not support preparing other query types.\n//\n// When using CQL protocol >= 4, it is possible to use gocql.UnsetValue as the bound value of a column.\n// This will cause the database to ignore writing the column.\n// The main advantage is the ability to keep the same prepared statement even when you don't\n// want to update some fields, where before you needed to make another prepared statement.\n//\n// # Executing multiple queries concurrently\n//\n// Session is safe to use from multiple goroutines, so to execute multiple concurrent queries, just execute them\n// from several worker goroutines. Gocql provides synchronously-looking API (as recommended for Go APIs) and the queries\n// are executed asynchronously at the protocol level.\n//\n//\tresults := make(chan error, 2)\n//\tgo func() {\n//\t\tresults <- session.Query(`INSERT INTO tweet (timeline, id, text) VALUES (?, ?, ?)`,\n//\t\t\t\"me\", gocql.TimeUUID(), \"hello world 1\").Exec()\n//\t}()\n//\tgo func() {\n//\t\tresults <- session.Query(`INSERT INTO tweet (timeline, id, text) VALUES (?, ?, ?)`,\n//\t\t\t\"me\", gocql.TimeUUID(), \"hello world 2\").Exec()\n//\t}()\n//\n// # Nulls\n//\n// Null values are are unmarshalled as zero value of the type. If you need to distinguish for example between text\n// column being null and empty string, you can unmarshal into *string variable instead of string.\n//\n//\tvar text *string\n//\terr := scanner.Scan(&text)\n//\tif err != nil {\n//\t\t// handle error\n//\t}\n//\tif text != nil {\n//\t\t// not null\n//\t}\n//\telse {\n//\t\t// null\n//\t}\n//\n// See Example_nulls for full example.\n//\n// # Reusing slices\n//\n// The driver reuses backing memory of slices when unmarshalling. This is an optimization so that a buffer does not\n// need to be allocated for every processed row. However, you need to be careful when storing the slices to other\n// memory structures.\n//\n//\tscanner := session.Query(`SELECT myints FROM table WHERE pk = ?`, \"key\").WithContext(ctx).Iter().Scanner()\n//\tvar myInts []int\n//\tfor scanner.Next() {\n//\t\t// This scan reuses backing store of myInts for each row.\n//\t\terr = scanner.Scan(&myInts)\n//\t\tif err != nil {\n//\t\t\tlog.Fatal(err)\n//\t\t}\n//\t}\n//\n// When you want to save the data for later use, pass a new slice every time. A common pattern is to declare the\n// slice variable within the scanner loop:\n//\n//\tscanner := session.Query(`SELECT myints FROM table WHERE pk = ?`, \"key\").WithContext(ctx).Iter().Scanner()\n//\tfor scanner.Next() {\n//\t\tvar myInts []int\n//\t\t// This scan always gets pointer to fresh myInts slice, so does not reuse memory.\n//\t\terr = scanner.Scan(&myInts)\n//\t\tif err != nil {\n//\t\t\tlog.Fatal(err)\n//\t\t}\n//\t}\n//\n// # Paging\n//\n// The driver supports paging of results with automatic prefetch, see ClusterConfig.PageSize, Session.SetPrefetch,\n// Query.PageSize, and Query.Prefetch.\n//\n// It is also possible to control the paging manually with Query.PageState (this disables automatic prefetch).\n// Manual paging is useful if you want to store the page state externally, for example in a URL to allow users\n// browse pages in a result. You might want to sign/encrypt the paging state when exposing it externally since\n// it contains data from primary keys.\n//\n// Paging state is specific to the CQL protocol version and the exact query used. It is meant as opaque state that\n// should not be modified. If you send paging state from different query or protocol version, then the behaviour\n// is not defined (you might get unexpected results or an error from the server). For example, do not send paging state\n// returned by node using protocol version 3 to a node using protocol version 4. Also, when using protocol version 4,\n// paging state between Cassandra 2.2 and 3.0 is incompatible (https://issues.apache.org/jira/browse/CASSANDRA-10880).\n//\n// The driver does not check whether the paging state is from the same protocol version/statement.\n// You might want to validate yourself as this could be a problem if you store paging state externally.\n// For example, if you store paging state in a URL, the URLs might become broken when you upgrade your cluster.\n//\n// Call Query.PageState(nil) to fetch just the first page of the query results. Pass the page state returned by\n// Iter.PageState to Query.PageState of a subsequent query to get the next page. If the length of slice returned\n// by Iter.PageState is zero, there are no more pages available (or an error occurred).\n//\n// Using too low values of PageSize will negatively affect performance, a value below 100 is probably too low.\n// While Cassandra returns exactly PageSize items (except for last page) in a page currently, the protocol authors\n// explicitly reserved the right to return smaller or larger amount of items in a page for performance reasons, so don't\n// rely on the page having the exact count of items.\n//\n// See Example_paging for an example of manual paging.\n//\n// # Dynamic list of columns\n//\n// There are certain situations when you don't know the list of columns in advance, mainly when the query is supplied\n// by the user. Iter.Columns, Iter.RowData, Iter.MapScan and Iter.SliceMap can be used to handle this case.\n//\n// See Example_dynamicColumns.\n//\n// # Batches\n//\n// The CQL protocol supports sending batches of DML statements (INSERT/UPDATE/DELETE) and so does gocql.\n// Use Session.Batch to create a new batch and then fill-in details of individual queries.\n// Then execute the batch with Session.ExecuteBatch.\n//\n// Logged batches ensure atomicity, either all or none of the operations in the batch will succeed, but they have\n// overhead to ensure this property.\n// Unlogged batches don't have the overhead of logged batches, but don't guarantee atomicity.\n// Updates of counters are handled specially by Cassandra so batches of counter updates have to use CounterBatch type.\n// A counter batch can only contain statements to update counters.\n//\n// For unlogged batches it is recommended to send only single-partition batches (i.e. all statements in the batch should\n// involve only a single partition).\n// Multi-partition batch needs to be split by the coordinator node and re-sent to\n// correct nodes.\n// With single-partition batches you can send the batch directly to the node for the partition without incurring the\n// additional network hop.\n//\n// It is also possible to pass entire BEGIN BATCH .. APPLY BATCH statement to Query.Exec.\n// There are differences how those are executed.\n// BEGIN BATCH statement passed to Query.Exec is prepared as a whole in a single statement.\n// Session.ExecuteBatch prepares individual statements in the batch.\n// If you have variable-length batches using the same statement, using Session.ExecuteBatch is more efficient.\n//\n// See Example_batch for an example.\n//\n// # Lightweight transactions\n//\n// Query.ScanCAS or Query.MapScanCAS can be used to execute a single-statement lightweight transaction (an\n// INSERT/UPDATE .. IF statement) and reading its result. See example for Query.MapScanCAS.\n//\n// Multiple-statement lightweight transactions can be executed as a logged batch that contains at least one conditional\n// statement. All the conditions must return true for the batch to be applied. You can use Session.ExecuteBatchCAS and\n// Session.MapExecuteBatchCAS when executing the batch to learn about the result of the LWT. See example for\n// Session.MapExecuteBatchCAS.\n//\n// # Retries and speculative execution\n//\n// Queries can be marked as idempotent. Marking the query as idempotent tells the driver that the query can be executed\n// multiple times without affecting its result. Non-idempotent queries are not eligible for retrying nor speculative\n// execution.\n//\n// Idempotent queries are retried in case of errors based on the configured RetryPolicy.\n//\n// Queries can be retried even before they fail by setting a SpeculativeExecutionPolicy. The policy can\n// cause the driver to retry on a different node if the query is taking longer than a specified delay even before the\n// driver receives an error or timeout from the server. When a query is speculatively executed, the original execution\n// is still executing. The two parallel executions of the query race to return a result, the first received result will\n// be returned.\n//\n// # User-defined types\n//\n// UDTs can be mapped (un)marshaled from/to map[string]interface{} a Go struct (or a type implementing\n// UDTUnmarshaler, UDTMarshaler, Unmarshaler or Marshaler interfaces).\n//\n// For structs, cql tag can be used to specify the CQL field name to be mapped to a struct field:\n//\n//\ttype MyUDT struct {\n//\t\tFieldA int32 `cql:\"a\"`\n//\t\tFieldB string `cql:\"b\"`\n//\t}\n//\n// See Example_userDefinedTypesMap, Example_userDefinedTypesStruct, ExampleUDTMarshaler, ExampleUDTUnmarshaler.\n//\n// # Metrics and tracing\n//\n// It is possible to provide observer implementations that could be used to gather metrics:\n//\n//   - QueryObserver for monitoring individual queries.\n//   - BatchObserver for monitoring batch queries.\n//   - ConnectObserver for monitoring new connections from the driver to the database.\n//   - FrameHeaderObserver for monitoring individual protocol frames.\n//\n// CQL protocol also supports tracing of queries. When enabled, the database will write information about\n// internal events that happened during execution of the query. You can use Query.Trace to request tracing and receive\n// the session ID that the database used to store the trace information in system_traces.sessions and\n// system_traces.events tables. NewTraceWriter returns an implementation of Tracer that writes the events to a writer.\n// Gathering trace information might be essential for debugging and optimizing queries, but writing traces has overhead,\n// so this feature should not be used on production systems with very high load unless you know what you are doing.\npackage gocql // import \"github.com/gocql/gocql\"\n"
        },
        {
          "name": "errors.go",
          "type": "blob",
          "size": 7.087890625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport \"fmt\"\n\n// See CQL Binary Protocol v5, section 8 for more details.\n// https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec\nconst (\n\t// ErrCodeServer indicates unexpected error on server-side.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1246-L1247\n\tErrCodeServer = 0x0000\n\t// ErrCodeProtocol indicates a protocol violation by some client message.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1248-L1250\n\tErrCodeProtocol = 0x000A\n\t// ErrCodeCredentials indicates missing required authentication.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1251-L1254\n\tErrCodeCredentials = 0x0100\n\t// ErrCodeUnavailable indicates unavailable error.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1255-L1265\n\tErrCodeUnavailable = 0x1000\n\t// ErrCodeOverloaded returned in case of request on overloaded node coordinator.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1266-L1267\n\tErrCodeOverloaded = 0x1001\n\t// ErrCodeBootstrapping returned from the coordinator node in bootstrapping phase.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1268-L1269\n\tErrCodeBootstrapping = 0x1002\n\t// ErrCodeTruncate indicates truncation exception.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1270\n\tErrCodeTruncate = 0x1003\n\t// ErrCodeWriteTimeout returned in case of timeout during the request write.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1271-L1304\n\tErrCodeWriteTimeout = 0x1100\n\t// ErrCodeReadTimeout returned in case of timeout during the request read.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1305-L1321\n\tErrCodeReadTimeout = 0x1200\n\t// ErrCodeReadFailure indicates request read error which is not covered by ErrCodeReadTimeout.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1322-L1340\n\tErrCodeReadFailure = 0x1300\n\t// ErrCodeFunctionFailure indicates an error in user-defined function.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1341-L1347\n\tErrCodeFunctionFailure = 0x1400\n\t// ErrCodeWriteFailure indicates request write error which is not covered by ErrCodeWriteTimeout.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1348-L1385\n\tErrCodeWriteFailure = 0x1500\n\t// ErrCodeCDCWriteFailure is defined, but not yet documented in CQLv5 protocol.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1386\n\tErrCodeCDCWriteFailure = 0x1600\n\t// ErrCodeCASWriteUnknown indicates only partially completed CAS operation.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1387-L1397\n\tErrCodeCASWriteUnknown = 0x1700\n\t// ErrCodeSyntax indicates the syntax error in the query.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1399\n\tErrCodeSyntax = 0x2000\n\t// ErrCodeUnauthorized indicates access rights violation by user on performed operation.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1400-L1401\n\tErrCodeUnauthorized = 0x2100\n\t// ErrCodeInvalid indicates invalid query error which is not covered by ErrCodeSyntax.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1402\n\tErrCodeInvalid = 0x2200\n\t// ErrCodeConfig indicates the configuration error.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1403\n\tErrCodeConfig = 0x2300\n\t// ErrCodeAlreadyExists is returned for the requests creating the existing keyspace/table.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1404-L1413\n\tErrCodeAlreadyExists = 0x2400\n\t// ErrCodeUnprepared returned from the host for prepared statement which is unknown.\n\t//\n\t// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1414-L1417\n\tErrCodeUnprepared = 0x2500\n)\n\ntype RequestError interface {\n\tCode() int\n\tMessage() string\n\tError() string\n}\n\ntype errorFrame struct {\n\tframeHeader\n\n\tcode    int\n\tmessage string\n}\n\nfunc (e errorFrame) Code() int {\n\treturn e.code\n}\n\nfunc (e errorFrame) Message() string {\n\treturn e.message\n}\n\nfunc (e errorFrame) Error() string {\n\treturn e.Message()\n}\n\nfunc (e errorFrame) String() string {\n\treturn fmt.Sprintf(\"[error code=%x message=%q]\", e.code, e.message)\n}\n\ntype RequestErrUnavailable struct {\n\terrorFrame\n\tConsistency Consistency\n\tRequired    int\n\tAlive       int\n}\n\nfunc (e *RequestErrUnavailable) String() string {\n\treturn fmt.Sprintf(\"[request_error_unavailable consistency=%s required=%d alive=%d]\", e.Consistency, e.Required, e.Alive)\n}\n\ntype ErrorMap map[string]uint16\n\ntype RequestErrWriteTimeout struct {\n\terrorFrame\n\tConsistency Consistency\n\tReceived    int\n\tBlockFor    int\n\tWriteType   string\n}\n\ntype RequestErrWriteFailure struct {\n\terrorFrame\n\tConsistency Consistency\n\tReceived    int\n\tBlockFor    int\n\tNumFailures int\n\tWriteType   string\n\tErrorMap    ErrorMap\n}\n\ntype RequestErrCDCWriteFailure struct {\n\terrorFrame\n}\n\ntype RequestErrReadTimeout struct {\n\terrorFrame\n\tConsistency Consistency\n\tReceived    int\n\tBlockFor    int\n\tDataPresent byte\n}\n\ntype RequestErrAlreadyExists struct {\n\terrorFrame\n\tKeyspace string\n\tTable    string\n}\n\ntype RequestErrUnprepared struct {\n\terrorFrame\n\tStatementId []byte\n}\n\ntype RequestErrReadFailure struct {\n\terrorFrame\n\tConsistency Consistency\n\tReceived    int\n\tBlockFor    int\n\tNumFailures int\n\tDataPresent bool\n\tErrorMap    ErrorMap\n}\n\ntype RequestErrFunctionFailure struct {\n\terrorFrame\n\tKeyspace string\n\tFunction string\n\tArgTypes []string\n}\n\n// RequestErrCASWriteUnknown is distinct error for ErrCodeCasWriteUnknown.\n//\n// See https://github.com/apache/cassandra/blob/7337fc0/doc/native_protocol_v5.spec#L1387-L1397\ntype RequestErrCASWriteUnknown struct {\n\terrorFrame\n\tConsistency Consistency\n\tReceived    int\n\tBlockFor    int\n}\n"
        },
        {
          "name": "errors_test.go",
          "type": "blob",
          "size": 1.783203125,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"testing\"\n)\n\nfunc TestErrorsParse(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.errors_parse (id int primary key)`); err != nil {\n\t\tt.Fatal(\"create:\", err)\n\t}\n\n\tif err := createTable(session, `CREATE TABLE gocql_test.errors_parse (id int primary key)`); err == nil {\n\t\tt.Fatal(\"Should have gotten already exists error from cassandra server.\")\n\t} else {\n\t\tswitch e := err.(type) {\n\t\tcase *RequestErrAlreadyExists:\n\t\t\tif e.Table != \"errors_parse\" {\n\t\t\t\tt.Fatalf(\"expected error table to be 'errors_parse' but was %q\", e.Table)\n\t\t\t}\n\t\tdefault:\n\t\t\tt.Fatalf(\"expected to get RequestErrAlreadyExists instead got %T\", e)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "events.go",
          "type": "blob",
          "size": 6.6728515625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype eventDebouncer struct {\n\tname   string\n\ttimer  *time.Timer\n\tmu     sync.Mutex\n\tevents []frame\n\n\tcallback func([]frame)\n\tquit     chan struct{}\n\n\tlogger StdLogger\n}\n\nfunc newEventDebouncer(name string, eventHandler func([]frame), logger StdLogger) *eventDebouncer {\n\te := &eventDebouncer{\n\t\tname:     name,\n\t\tquit:     make(chan struct{}),\n\t\ttimer:    time.NewTimer(eventDebounceTime),\n\t\tcallback: eventHandler,\n\t\tlogger:   logger,\n\t}\n\te.timer.Stop()\n\tgo e.flusher()\n\n\treturn e\n}\n\nfunc (e *eventDebouncer) stop() {\n\te.quit <- struct{}{} // sync with flusher\n\tclose(e.quit)\n}\n\nfunc (e *eventDebouncer) flusher() {\n\tfor {\n\t\tselect {\n\t\tcase <-e.timer.C:\n\t\t\te.mu.Lock()\n\t\t\te.flush()\n\t\t\te.mu.Unlock()\n\t\tcase <-e.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nconst (\n\teventBufferSize   = 1000\n\teventDebounceTime = 1 * time.Second\n)\n\n// flush must be called with mu locked\nfunc (e *eventDebouncer) flush() {\n\tif len(e.events) == 0 {\n\t\treturn\n\t}\n\n\t// if the flush interval is faster than the callback then we will end up calling\n\t// the callback multiple times, probably a bad idea. In this case we could drop\n\t// frames?\n\tgo e.callback(e.events)\n\te.events = make([]frame, 0, eventBufferSize)\n}\n\nfunc (e *eventDebouncer) debounce(frame frame) {\n\te.mu.Lock()\n\te.timer.Reset(eventDebounceTime)\n\n\t// TODO: probably need a warning to track if this threshold is too low\n\tif len(e.events) < eventBufferSize {\n\t\te.events = append(e.events, frame)\n\t} else {\n\t\te.logger.Printf(\"%s: buffer full, dropping event frame: %s\", e.name, frame)\n\t}\n\n\te.mu.Unlock()\n}\n\nfunc (s *Session) handleEvent(framer *framer) {\n\tframe, err := framer.parseFrame()\n\tif err != nil {\n\t\ts.logger.Printf(\"gocql: unable to parse event frame: %v\\n\", err)\n\t\treturn\n\t}\n\n\tif gocqlDebug {\n\t\ts.logger.Printf(\"gocql: handling frame: %v\\n\", frame)\n\t}\n\n\tswitch f := frame.(type) {\n\tcase *schemaChangeKeyspace, *schemaChangeFunction,\n\t\t*schemaChangeTable, *schemaChangeAggregate, *schemaChangeType:\n\n\t\ts.schemaEvents.debounce(frame)\n\tcase *topologyChangeEventFrame, *statusChangeEventFrame:\n\t\ts.nodeEvents.debounce(frame)\n\tdefault:\n\t\ts.logger.Printf(\"gocql: invalid event frame (%T): %v\\n\", f, f)\n\t}\n}\n\nfunc (s *Session) handleSchemaEvent(frames []frame) {\n\t// TODO: debounce events\n\tfor _, frame := range frames {\n\t\tswitch f := frame.(type) {\n\t\tcase *schemaChangeKeyspace:\n\t\t\ts.schemaDescriber.clearSchema(f.keyspace)\n\t\t\ts.handleKeyspaceChange(f.keyspace, f.change)\n\t\tcase *schemaChangeTable:\n\t\t\ts.schemaDescriber.clearSchema(f.keyspace)\n\t\tcase *schemaChangeAggregate:\n\t\t\ts.schemaDescriber.clearSchema(f.keyspace)\n\t\tcase *schemaChangeFunction:\n\t\t\ts.schemaDescriber.clearSchema(f.keyspace)\n\t\tcase *schemaChangeType:\n\t\t\ts.schemaDescriber.clearSchema(f.keyspace)\n\t\t}\n\t}\n}\n\nfunc (s *Session) handleKeyspaceChange(keyspace, change string) {\n\ts.control.awaitSchemaAgreement()\n\ts.policy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: keyspace, Change: change})\n}\n\n// handleNodeEvent handles inbound status and topology change events.\n//\n// Status events are debounced by host IP; only the latest event is processed.\n//\n// Topology events are debounced by performing a single full topology refresh\n// whenever any topology event comes in.\n//\n// Processing topology change events before status change events ensures\n// that a NEW_NODE event is not dropped in favor of a newer UP event (which\n// would itself be dropped/ignored, as the node is not yet known).\nfunc (s *Session) handleNodeEvent(frames []frame) {\n\ttype nodeEvent struct {\n\t\tchange string\n\t\thost   net.IP\n\t\tport   int\n\t}\n\n\ttopologyEventReceived := false\n\t// status change events\n\tsEvents := make(map[string]*nodeEvent)\n\n\tfor _, frame := range frames {\n\t\tswitch f := frame.(type) {\n\t\tcase *topologyChangeEventFrame:\n\t\t\ttopologyEventReceived = true\n\t\tcase *statusChangeEventFrame:\n\t\t\tevent, ok := sEvents[f.host.String()]\n\t\t\tif !ok {\n\t\t\t\tevent = &nodeEvent{change: f.change, host: f.host, port: f.port}\n\t\t\t\tsEvents[f.host.String()] = event\n\t\t\t}\n\t\t\tevent.change = f.change\n\t\t}\n\t}\n\n\tif topologyEventReceived && !s.cfg.Events.DisableTopologyEvents {\n\t\ts.debounceRingRefresh()\n\t}\n\n\tfor _, f := range sEvents {\n\t\tif gocqlDebug {\n\t\t\ts.logger.Printf(\"gocql: dispatching status change event: %+v\\n\", f)\n\t\t}\n\n\t\t// ignore events we received if they were disabled\n\t\t// see https://github.com/apache/cassandra-gocql-driver/issues/1591\n\t\tswitch f.change {\n\t\tcase \"UP\":\n\t\t\tif !s.cfg.Events.DisableNodeStatusEvents {\n\t\t\t\ts.handleNodeUp(f.host, f.port)\n\t\t\t}\n\t\tcase \"DOWN\":\n\t\t\tif !s.cfg.Events.DisableNodeStatusEvents {\n\t\t\t\ts.handleNodeDown(f.host, f.port)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (s *Session) handleNodeUp(eventIp net.IP, eventPort int) {\n\tif gocqlDebug {\n\t\ts.logger.Printf(\"gocql: Session.handleNodeUp: %s:%d\\n\", eventIp.String(), eventPort)\n\t}\n\n\thost, ok := s.ring.getHostByIP(eventIp.String())\n\tif !ok {\n\t\ts.debounceRingRefresh()\n\t\treturn\n\t}\n\n\tif s.cfg.filterHost(host) {\n\t\treturn\n\t}\n\n\tif d := host.Version().nodeUpDelay(); d > 0 {\n\t\ttime.Sleep(d)\n\t}\n\ts.startPoolFill(host)\n}\n\nfunc (s *Session) startPoolFill(host *HostInfo) {\n\t// we let the pool call handleNodeConnected to change the host state\n\ts.pool.addHost(host)\n\ts.policy.AddHost(host)\n}\n\nfunc (s *Session) handleNodeConnected(host *HostInfo) {\n\tif gocqlDebug {\n\t\ts.logger.Printf(\"gocql: Session.handleNodeConnected: %s:%d\\n\", host.ConnectAddress(), host.Port())\n\t}\n\n\thost.setState(NodeUp)\n\n\tif !s.cfg.filterHost(host) {\n\t\ts.policy.HostUp(host)\n\t}\n}\n\nfunc (s *Session) handleNodeDown(ip net.IP, port int) {\n\tif gocqlDebug {\n\t\ts.logger.Printf(\"gocql: Session.handleNodeDown: %s:%d\\n\", ip.String(), port)\n\t}\n\n\thost, ok := s.ring.getHostByIP(ip.String())\n\tif ok {\n\t\thost.setState(NodeDown)\n\t\tif s.cfg.filterHost(host) {\n\t\t\treturn\n\t\t}\n\n\t\ts.policy.HostDown(host)\n\t\thostID := host.HostID()\n\t\ts.pool.removeHost(hostID)\n\t}\n}\n"
        },
        {
          "name": "events_ccm_test.go",
          "type": "blob",
          "size": 7.32421875,
          "content": "//go:build (ccm && ignore) || ignore\n// +build ccm,ignore ignore\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"log\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/gocql/gocql/internal/ccm\"\n)\n\nfunc TestEventDiscovery(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Logf(\"status=%+v\\n\", status)\n\n\tsession.pool.mu.RLock()\n\tpoolHosts := session.pool.hostConnPools // TODO: replace with session.ring\n\tt.Logf(\"poolhosts=%+v\\n\", poolHosts)\n\t// check we discovered all the nodes in the ring\n\tfor _, host := range status {\n\t\tif _, ok := poolHosts[host.Addr]; !ok {\n\t\t\tt.Errorf(\"did not discover %q\", host.Addr)\n\t\t}\n\t}\n\tsession.pool.mu.RUnlock()\n\tif t.Failed() {\n\t\tt.FailNow()\n\t}\n}\n\nfunc TestEventNodeDownControl(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tconst targetNode = \"node1\"\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcluster := createCluster()\n\tcluster.Hosts = []string{status[targetNode].Addr}\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tt.Log(\"marking \" + targetNode + \" as down\")\n\tif err := ccm.NodeDown(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Logf(\"status=%+v\\n\", status)\n\tt.Logf(\"marking node %q down: %v\\n\", targetNode, status[targetNode])\n\n\ttime.Sleep(5 * time.Second)\n\n\tsession.pool.mu.RLock()\n\n\tpoolHosts := session.pool.hostConnPools\n\tnode := status[targetNode]\n\tt.Logf(\"poolhosts=%+v\\n\", poolHosts)\n\n\tif _, ok := poolHosts[node.Addr]; ok {\n\t\tsession.pool.mu.RUnlock()\n\t\tt.Fatal(\"node not removed after remove event\")\n\t}\n\tsession.pool.mu.RUnlock()\n\n\thost := session.ring.getHost(node.Addr)\n\tif host == nil {\n\t\tt.Fatal(\"node not in metadata ring\")\n\t} else if host.IsUp() {\n\t\tt.Fatalf(\"not not marked as down after event in metadata: %v\", host)\n\t}\n}\n\nfunc TestEventNodeDown(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tconst targetNode = \"node3\"\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := ccm.NodeDown(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Logf(\"status=%+v\\n\", status)\n\tt.Logf(\"marking node %q down: %v\\n\", targetNode, status[targetNode])\n\n\ttime.Sleep(5 * time.Second)\n\n\tsession.pool.mu.RLock()\n\tdefer session.pool.mu.RUnlock()\n\n\tpoolHosts := session.pool.hostConnPools\n\tnode := status[targetNode]\n\tt.Logf(\"poolhosts=%+v\\n\", poolHosts)\n\n\tif _, ok := poolHosts[node.Addr]; ok {\n\t\tt.Fatal(\"node not removed after remove event\")\n\t}\n\n\thost := session.ring.getHost(node.Addr)\n\tif host == nil {\n\t\tt.Fatal(\"node not in metadata ring\")\n\t} else if host.IsUp() {\n\t\tt.Fatalf(\"not not marked as down after event in metadata: %v\", host)\n\t}\n}\n\nfunc TestEventNodeUp(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tlog.Printf(\"status=%+v\\n\", status)\n\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tconst targetNode = \"node2\"\n\tnode := status[targetNode]\n\n\t_, ok := session.pool.getPool(node.Addr)\n\tif !ok {\n\t\tsession.pool.mu.RLock()\n\t\tt.Errorf(\"target pool not in connection pool: addr=%q pools=%v\", status[targetNode].Addr, session.pool.hostConnPools)\n\t\tsession.pool.mu.RUnlock()\n\t\tt.FailNow()\n\t}\n\n\tif err := ccm.NodeDown(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(5 * time.Second)\n\n\t_, ok = session.pool.getPool(node.Addr)\n\tif ok {\n\t\tt.Fatal(\"node not removed after remove event\")\n\t}\n\n\tif err := ccm.NodeUp(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// cassandra < 2.2 needs 10 seconds to start up the binary service\n\ttime.Sleep(15 * time.Second)\n\n\t_, ok = session.pool.getPool(node.Addr)\n\tif !ok {\n\t\tt.Fatal(\"node not added after node added event\")\n\t}\n\n\thost := session.ring.getHost(node.Addr)\n\tif host == nil {\n\t\tt.Fatal(\"node not in metadata ring\")\n\t} else if !host.IsUp() {\n\t\tt.Fatalf(\"not not marked as UP after event in metadata: addr=%q host=%p: %v\", node.Addr, host, host)\n\t}\n}\n\nfunc TestEventFilter(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tlog.Printf(\"status=%+v\\n\", status)\n\n\tcluster := createCluster()\n\tcluster.HostFilter = WhiteListHostFilter(status[\"node1\"].Addr)\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif _, ok := session.pool.getPool(status[\"node1\"].Addr); !ok {\n\t\tt.Errorf(\"should have %v in pool but dont\", \"node1\")\n\t}\n\n\tfor _, host := range [...]string{\"node2\", \"node3\"} {\n\t\t_, ok := session.pool.getPool(status[host].Addr)\n\t\tif ok {\n\t\t\tt.Errorf(\"should not have %v in pool\", host)\n\t\t}\n\t}\n\n\tif t.Failed() {\n\t\tt.FailNow()\n\t}\n\n\tif err := ccm.NodeDown(\"node2\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(5 * time.Second)\n\n\tif err := ccm.NodeUp(\"node2\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(15 * time.Second)\n\tfor _, host := range [...]string{\"node2\", \"node3\"} {\n\t\t_, ok := session.pool.getPool(status[host].Addr)\n\t\tif ok {\n\t\t\tt.Errorf(\"should not have %v in pool\", host)\n\t\t}\n\t}\n\n\tif t.Failed() {\n\t\tt.FailNow()\n\t}\n\n}\n\nfunc TestEventDownQueryable(t *testing.T) {\n\tt.Skip(\"FLAKE skipping\")\n\tif err := ccm.AllUp(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tstatus, err := ccm.Status()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tlog.Printf(\"status=%+v\\n\", status)\n\n\tconst targetNode = \"node1\"\n\n\taddr := status[targetNode].Addr\n\n\tcluster := createCluster()\n\tcluster.Hosts = []string{addr}\n\tcluster.HostFilter = WhiteListHostFilter(addr)\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif pool, ok := session.pool.getPool(addr); !ok {\n\t\tt.Fatalf(\"should have %v in pool but dont\", addr)\n\t} else if !pool.host.IsUp() {\n\t\tt.Fatalf(\"host is not up %v\", pool.host)\n\t}\n\n\tif err := ccm.NodeDown(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(5 * time.Second)\n\n\tif err := ccm.NodeUp(targetNode); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttime.Sleep(15 * time.Second)\n\n\tif pool, ok := session.pool.getPool(addr); !ok {\n\t\tt.Fatalf(\"should have %v in pool but dont\", addr)\n\t} else if !pool.host.IsUp() {\n\t\tt.Fatalf(\"host is not up %v\", pool.host)\n\t}\n\n\tvar rows int\n\tif err := session.Query(\"SELECT COUNT(*) FROM system.local\").Scan(&rows); err != nil {\n\t\tt.Fatal(err)\n\t} else if rows != 1 {\n\t\tt.Fatalf(\"expected to get 1 row got %d\", rows)\n\t}\n}\n"
        },
        {
          "name": "events_test.go",
          "type": "blob",
          "size": 1.6220703125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"sync\"\n\t\"testing\"\n)\n\nfunc TestEventDebounce(t *testing.T) {\n\tconst eventCount = 150\n\twg := &sync.WaitGroup{}\n\twg.Add(1)\n\n\teventsSeen := 0\n\tdebouncer := newEventDebouncer(\"testDebouncer\", func(events []frame) {\n\t\tdefer wg.Done()\n\t\teventsSeen += len(events)\n\t}, &defaultLogger{})\n\tdefer debouncer.stop()\n\n\tfor i := 0; i < eventCount; i++ {\n\t\tdebouncer.debounce(&statusChangeEventFrame{\n\t\t\tchange: \"UP\",\n\t\t\thost:   net.IPv4(127, 0, 0, 1),\n\t\t\tport:   9042,\n\t\t})\n\t}\n\n\twg.Wait()\n\tif eventCount != eventsSeen {\n\t\tt.Fatalf(\"expected to see %d events but got %d\", eventCount, eventsSeen)\n\t}\n}\n"
        },
        {
          "name": "example_batch_test.go",
          "type": "blob",
          "size": 2.751953125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/gocql/gocql\"\n)\n\n// Example_batch demonstrates how to execute a batch of statements.\nfunc Example_batch() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.batches(pk int, ck int, description text, PRIMARY KEY(pk, ck));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tb := session.Batch(gocql.UnloggedBatch).WithContext(ctx)\n\tb.Entries = append(b.Entries, gocql.BatchEntry{\n\t\tStmt:       \"INSERT INTO example.batches (pk, ck, description) VALUES (?, ?, ?)\",\n\t\tArgs:       []interface{}{1, 2, \"1.2\"},\n\t\tIdempotent: true,\n\t})\n\tb.Entries = append(b.Entries, gocql.BatchEntry{\n\t\tStmt:       \"INSERT INTO example.batches (pk, ck, description) VALUES (?, ?, ?)\",\n\t\tArgs:       []interface{}{1, 3, \"1.3\"},\n\t\tIdempotent: true,\n\t})\n\n\terr = session.ExecuteBatch(b)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\terr = b.Query(\"INSERT INTO example.batches (pk, ck, description) VALUES (?, ?, ?)\", 1, 4, \"1.4\").\n\t\tQuery(\"INSERT INTO example.batches (pk, ck, description) VALUES (?, ?, ?)\", 1, 5, \"1.5\").\n\t\tExec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tscanner := session.Query(\"SELECT pk, ck, description FROM example.batches\").Iter().Scanner()\n\tfor scanner.Next() {\n\t\tvar pk, ck int32\n\t\tvar description string\n\t\terr = scanner.Scan(&pk, &ck, &description)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tfmt.Println(pk, ck, description)\n\t}\n\t// 1 2 1.2\n\t// 1 3 1.3\n\t// 1 4 1.4\n\t// 1 5 1.5\n}\n"
        },
        {
          "name": "example_dynamic_columns_test.go",
          "type": "blob",
          "size": 3.7216796875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"reflect\"\n\t\"text/tabwriter\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// Example_dynamicColumns demonstrates how to handle dynamic column list.\nfunc Example_dynamicColumns() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.table1(pk text, ck int, value1 text, value2 int, PRIMARY KEY(pk, ck));\n\tinsert into example.table1 (pk, ck, value1, value2) values ('a', 1, 'b', 2);\n\tinsert into example.table1 (pk, ck, value1, value2) values ('c', 3, 'd', 4);\n\tinsert into example.table1 (pk, ck, value1, value2) values ('c', 5, null, null);\n\tcreate table example.table2(pk int, value1 timestamp, PRIMARY KEY(pk));\n\tinsert into example.table2 (pk, value1) values (1, '2020-01-02 03:04:05');\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tprintQuery := func(ctx context.Context, session *gocql.Session, stmt string, values ...interface{}) error {\n\t\titer := session.Query(stmt, values...).WithContext(ctx).Iter()\n\t\tfmt.Println(stmt)\n\t\tw := tabwriter.NewWriter(os.Stdout, 0, 0, 1, ' ',\n\t\t\t0)\n\t\tfor i, columnInfo := range iter.Columns() {\n\t\t\tif i > 0 {\n\t\t\t\tfmt.Fprint(w, \"\\t| \")\n\t\t\t}\n\t\t\tfmt.Fprintf(w, \"%s (%s)\", columnInfo.Name, columnInfo.TypeInfo)\n\t\t}\n\n\t\tfor {\n\t\t\trd, err := iter.RowData()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif !iter.Scan(rd.Values...) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Fprint(w, \"\\n\")\n\t\t\tfor i, val := range rd.Values {\n\t\t\t\tif i > 0 {\n\t\t\t\t\tfmt.Fprint(w, \"\\t| \")\n\t\t\t\t}\n\n\t\t\t\tfmt.Fprint(w, reflect.Indirect(reflect.ValueOf(val)).Interface())\n\t\t\t}\n\t\t}\n\n\t\tfmt.Fprint(w, \"\\n\")\n\t\tw.Flush()\n\t\tfmt.Println()\n\n\t\treturn iter.Close()\n\t}\n\n\tctx := context.Background()\n\n\terr = printQuery(ctx, session, \"SELECT * FROM table1\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\terr = printQuery(ctx, session, \"SELECT value2, pk, ck FROM table1\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\terr = printQuery(ctx, session, \"SELECT * FROM table2\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// SELECT * FROM table1\n\t// pk (varchar) | ck (int) | value1 (varchar) | value2 (int)\n\t// a            | 1        | b                | 2\n\t// c            | 3        | d                | 4\n\t// c            | 5        |                  | 0\n\t//\n\t// SELECT value2, pk, ck FROM table1\n\t// value2 (int) | pk (varchar) | ck (int)\n\t// 2            | a            | 1\n\t// 4            | c            | 3\n\t// 0            | c            | 5\n\t//\n\t// SELECT * FROM table2\n\t// pk (int) | value1 (timestamp)\n\t// 1        | 2020-01-02 03:04:05 +0000 UTC\n}\n"
        },
        {
          "name": "example_lwt_batch_test.go",
          "type": "blob",
          "size": 3.501953125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/gocql/gocql\"\n)\n\n// ExampleSession_MapExecuteBatchCAS demonstrates how to execute a batch lightweight transaction.\nfunc ExampleSession_MapExecuteBatchCAS() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.my_lwt_batch_table(pk text, ck text, version int, value text, PRIMARY KEY(pk, ck));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\terr = session.Query(\"INSERT INTO example.my_lwt_batch_table (pk, ck, version, value) VALUES (?, ?, ?, ?)\",\n\t\t\"pk1\", \"ck1\", 1, \"a\").WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\terr = session.Query(\"INSERT INTO example.my_lwt_batch_table (pk, ck, version, value) VALUES (?, ?, ?, ?)\",\n\t\t\"pk1\", \"ck2\", 1, \"A\").WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\texecuteBatch := func(ck2Version int) {\n\t\tb := session.Batch(gocql.LoggedBatch)\n\t\tb.Entries = append(b.Entries, gocql.BatchEntry{\n\t\t\tStmt: \"UPDATE my_lwt_batch_table SET value=? WHERE pk=? AND ck=? IF version=?\",\n\t\t\tArgs: []interface{}{\"b\", \"pk1\", \"ck1\", 1},\n\t\t})\n\t\tb.Entries = append(b.Entries, gocql.BatchEntry{\n\t\t\tStmt: \"UPDATE my_lwt_batch_table SET value=? WHERE pk=? AND ck=? IF version=?\",\n\t\t\tArgs: []interface{}{\"B\", \"pk1\", \"ck2\", ck2Version},\n\t\t})\n\t\tm := make(map[string]interface{})\n\t\tapplied, iter, err := session.MapExecuteBatchCAS(b.WithContext(ctx), m)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tfmt.Println(applied, m)\n\n\t\tm = make(map[string]interface{})\n\t\tfor iter.MapScan(m) {\n\t\t\tfmt.Println(m)\n\t\t\tm = make(map[string]interface{})\n\t\t}\n\n\t\tif err := iter.Close(); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n\n\tprintState := func() {\n\t\tscanner := session.Query(\"SELECT ck, value FROM example.my_lwt_batch_table WHERE pk = ?\", \"pk1\").\n\t\t\tWithContext(ctx).Iter().Scanner()\n\t\tfor scanner.Next() {\n\t\t\tvar ck, value string\n\t\t\terr = scanner.Scan(&ck, &value)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tfmt.Println(ck, value)\n\t\t}\n\t\tif err := scanner.Err(); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n\n\texecuteBatch(0)\n\tprintState()\n\texecuteBatch(1)\n\tprintState()\n\n\t// false map[ck:ck1 pk:pk1 version:1]\n\t// map[[applied]:false ck:ck2 pk:pk1 version:1]\n\t// ck1 a\n\t// ck2 A\n\t// true map[]\n\t// ck1 b\n\t// ck2 B\n}\n"
        },
        {
          "name": "example_lwt_test.go",
          "type": "blob",
          "size": 2.7919921875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// ExampleQuery_MapScanCAS demonstrates how to execute a single-statement lightweight transaction.\nfunc ExampleQuery_MapScanCAS() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.my_lwt_table(pk int, version int, value text, PRIMARY KEY(pk));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\terr = session.Query(\"INSERT INTO example.my_lwt_table (pk, version, value) VALUES (?, ?, ?)\",\n\t\t1, 1, \"a\").WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tm := make(map[string]interface{})\n\tapplied, err := session.Query(\"UPDATE example.my_lwt_table SET value = ? WHERE pk = ? IF version = ?\",\n\t\t\"b\", 1, 0).WithContext(ctx).MapScanCAS(m)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(applied, m)\n\n\tvar value string\n\terr = session.Query(\"SELECT value FROM example.my_lwt_table WHERE pk = ?\", 1).WithContext(ctx).\n\t\tScan(&value)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(value)\n\n\tm = make(map[string]interface{})\n\tapplied, err = session.Query(\"UPDATE example.my_lwt_table SET value = ? WHERE pk = ? IF version = ?\",\n\t\t\"b\", 1, 1).WithContext(ctx).MapScanCAS(m)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(applied, m)\n\n\tvar value2 string\n\terr = session.Query(\"SELECT value FROM example.my_lwt_table WHERE pk = ?\", 1).WithContext(ctx).\n\t\tScan(&value2)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(value2)\n\t// false map[version:1]\n\t// a\n\t// true map[]\n\t// b\n}\n"
        },
        {
          "name": "example_marshaler_test.go",
          "type": "blob",
          "size": 3.4326171875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"strconv\"\n\t\"strings\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// MyMarshaler implements Marshaler and Unmarshaler.\n// It represents a version number stored as string.\ntype MyMarshaler struct {\n\tmajor, minor, patch int\n}\n\nfunc (m MyMarshaler) MarshalCQL(info gocql.TypeInfo) ([]byte, error) {\n\treturn gocql.Marshal(info, fmt.Sprintf(\"%d.%d.%d\", m.major, m.minor, m.patch))\n}\n\nfunc (m *MyMarshaler) UnmarshalCQL(info gocql.TypeInfo, data []byte) error {\n\tvar s string\n\terr := gocql.Unmarshal(info, data, &s)\n\tif err != nil {\n\t\treturn err\n\t}\n\tparts := strings.SplitN(s, \".\", 3)\n\tif len(parts) != 3 {\n\t\treturn fmt.Errorf(\"parse version %q: %d parts instead of 3\", s, len(parts))\n\t}\n\tmajor, err := strconv.Atoi(parts[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parse version %q major number: %v\", s, err)\n\t}\n\tminor, err := strconv.Atoi(parts[1])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parse version %q minor number: %v\", s, err)\n\t}\n\tpatch, err := strconv.Atoi(parts[2])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parse version %q patch number: %v\", s, err)\n\t}\n\tm.major = major\n\tm.minor = minor\n\tm.patch = patch\n\treturn nil\n}\n\n// Example_marshalerUnmarshaler demonstrates how to implement a Marshaler and Unmarshaler.\nfunc Example_marshalerUnmarshaler() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.my_marshaler_table(pk int, value text, PRIMARY KEY(pk));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tvalue := MyMarshaler{\n\t\tmajor: 1,\n\t\tminor: 2,\n\t\tpatch: 3,\n\t}\n\terr = session.Query(\"INSERT INTO example.my_marshaler_table (pk, value) VALUES (?, ?)\",\n\t\t1, value).WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tvar stringValue string\n\terr = session.Query(\"SELECT value FROM example.my_marshaler_table WHERE pk = 1\").WithContext(ctx).\n\t\tScan(&stringValue)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(stringValue)\n\tvar unmarshaledValue MyMarshaler\n\terr = session.Query(\"SELECT value FROM example.my_marshaler_table WHERE pk = 1\").WithContext(ctx).\n\t\tScan(&unmarshaledValue)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(unmarshaledValue)\n\t// 1.2.3\n\t// {1 2 3}\n}\n"
        },
        {
          "name": "example_nulls_test.go",
          "type": "blob",
          "size": 2.431640625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// Example_nulls demonstrates how to distinguish between null and zero value when needed.\n//\n// Null values are unmarshalled as zero value of the type. If you need to distinguish for example between text\n// column being null and empty string, you can unmarshal into *string field.\nfunc Example_nulls() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.stringvals(id int, value text, PRIMARY KEY(id));\n\tinsert into example.stringvals (id, value) values (1, null);\n\tinsert into example.stringvals (id, value) values (2, '');\n\tinsert into example.stringvals (id, value) values (3, 'hello');\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\tscanner := session.Query(`SELECT id, value FROM stringvals`).Iter().Scanner()\n\tfor scanner.Next() {\n\t\tvar (\n\t\t\tid  int32\n\t\t\tval *string\n\t\t)\n\t\terr := scanner.Scan(&id, &val)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tif val != nil {\n\t\t\tfmt.Printf(\"Row %d is %q\\n\", id, *val)\n\t\t} else {\n\t\t\tfmt.Printf(\"Row %d is null\\n\", id)\n\t\t}\n\n\t}\n\terr = scanner.Err()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Row 1 is null\n\t// Row 2 is \"\"\n\t// Row 3 is \"hello\"\n}\n"
        },
        {
          "name": "example_paging_test.go",
          "type": "blob",
          "size": 3.017578125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// Example_paging demonstrates how to manually fetch pages and use page state.\n//\n// See also package documentation about paging.\nfunc Example_paging() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.itoa(id int, description text, PRIMARY KEY(id));\n\tinsert into example.itoa (id, description) values (1, 'one');\n\tinsert into example.itoa (id, description) values (2, 'two');\n\tinsert into example.itoa (id, description) values (3, 'three');\n\tinsert into example.itoa (id, description) values (4, 'four');\n\tinsert into example.itoa (id, description) values (5, 'five');\n\tinsert into example.itoa (id, description) values (6, 'six');\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tvar pageState []byte\n\tfor {\n\t\t// We use PageSize(2) for the sake of example, use larger values in production (default is 5000) for performance\n\t\t// reasons.\n\t\titer := session.Query(`SELECT id, description FROM itoa`).PageSize(2).PageState(pageState).Iter()\n\t\tnextPageState := iter.PageState()\n\t\tscanner := iter.Scanner()\n\t\tfor scanner.Next() {\n\t\t\tvar (\n\t\t\t\tid          int\n\t\t\t\tdescription string\n\t\t\t)\n\t\t\terr = scanner.Scan(&id, &description)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tfmt.Println(id, description)\n\t\t}\n\t\terr = scanner.Err()\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tfmt.Printf(\"next page state: %+v\\n\", nextPageState)\n\t\tif len(nextPageState) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tpageState = nextPageState\n\t}\n\t// 5 five\n\t// 1 one\n\t// next page state: [4 0 0 0 1 0 240 127 255 255 253 0]\n\t// 2 two\n\t// 4 four\n\t// next page state: [4 0 0 0 4 0 240 127 255 255 251 0]\n\t// 6 six\n\t// 3 three\n\t// next page state: [4 0 0 0 3 0 240 127 255 255 249 0]\n\t// next page state: []\n}\n"
        },
        {
          "name": "example_set_test.go",
          "type": "blob",
          "size": 2.6875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"sort\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// Example_set demonstrates how to use sets.\nfunc Example_set() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.sets(id int, value set<text>, PRIMARY KEY(id));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\terr = session.Query(`UPDATE sets SET value=? WHERE id=1`, []string{\"alpha\", \"beta\", \"gamma\"}).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\terr = session.Query(`UPDATE sets SET value=value+? WHERE id=1`, \"epsilon\").Exec()\n\tif err != nil {\n\t\t// This does not work because the ? expects a set, not a single item.\n\t\tfmt.Printf(\"expected error: %v\\n\", err)\n\t}\n\terr = session.Query(`UPDATE sets SET value=value+? WHERE id=1`, []string{\"delta\"}).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// map[x]struct{} is supported too.\n\ttoRemove := map[string]struct{}{\n\t\t\"alpha\": {},\n\t\t\"gamma\": {},\n\t}\n\terr = session.Query(`UPDATE sets SET value=value-? WHERE id=1`, toRemove).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tscanner := session.Query(`SELECT id, value FROM sets`).Iter().Scanner()\n\tfor scanner.Next() {\n\t\tvar (\n\t\t\tid  int32\n\t\t\tval []string\n\t\t)\n\t\terr := scanner.Scan(&id, &val)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tsort.Strings(val)\n\t\tfmt.Printf(\"Row %d is %v\\n\", id, val)\n\t}\n\terr = scanner.Err()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// expected error: can not marshal string into set(varchar)\n\t// Row 1 is [beta delta]\n}\n"
        },
        {
          "name": "example_test.go",
          "type": "blob",
          "size": 2.91015625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\nfunc Example() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate table example.tweet(timeline text, id UUID, text text, PRIMARY KEY(id));\n\tcreate index on example.tweet(timeline);\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.Consistency = gocql.Quorum\n\t// connect to the cluster\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\t// insert a tweet\n\tif err := session.Query(`INSERT INTO tweet (timeline, id, text) VALUES (?, ?, ?)`,\n\t\t\"me\", gocql.TimeUUID(), \"hello world\").WithContext(ctx).Exec(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tvar id gocql.UUID\n\tvar text string\n\n\t/* Search for a specific set of records whose 'timeline' column matches\n\t * the value 'me'. The secondary index that we created earlier will be\n\t * used for optimizing the search */\n\tif err := session.Query(`SELECT id, text FROM tweet WHERE timeline = ? LIMIT 1`,\n\t\t\"me\").WithContext(ctx).Consistency(gocql.One).Scan(&id, &text); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(\"Tweet:\", id, text)\n\tfmt.Println()\n\n\t// list all tweets\n\tscanner := session.Query(`SELECT id, text FROM tweet WHERE timeline = ?`,\n\t\t\"me\").WithContext(ctx).Iter().Scanner()\n\tfor scanner.Next() {\n\t\terr = scanner.Scan(&id, &text)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tfmt.Println(\"Tweet:\", id, text)\n\t}\n\t// scanner.Err() closes the iterator, so scanner nor iter should be used afterwards.\n\tif err := scanner.Err(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Tweet: cad53821-3731-11eb-971c-708bcdaada84 hello world\n\t//\n\t// Tweet: cad53821-3731-11eb-971c-708bcdaada84 hello world\n\t// Tweet: d577ab85-3731-11eb-81eb-708bcdaada84 hello world\n}\n"
        },
        {
          "name": "example_udt_map_test.go",
          "type": "blob",
          "size": 2.3935546875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// Example_userDefinedTypesMap demonstrates how to work with user-defined types as maps.\n// See also Example_userDefinedTypesStruct and examples for UDTMarshaler and UDTUnmarshaler if you want to map to structs.\nfunc Example_userDefinedTypesMap() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate type example.my_udt (field_a text, field_b int);\n\tcreate table example.my_udt_table(pk int, value frozen<my_udt>, PRIMARY KEY(pk));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tvalue := map[string]interface{}{\n\t\t\"field_a\": \"a value\",\n\t\t\"field_b\": 42,\n\t}\n\terr = session.Query(\"INSERT INTO example.my_udt_table (pk, value) VALUES (?, ?)\",\n\t\t1, value).WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tvar readValue map[string]interface{}\n\n\terr = session.Query(\"SELECT value FROM example.my_udt_table WHERE pk = 1\").WithContext(ctx).Scan(&readValue)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(readValue[\"field_a\"])\n\tfmt.Println(readValue[\"field_b\"])\n\t// a value\n\t// 42\n}\n"
        },
        {
          "name": "example_udt_marshaler_test.go",
          "type": "blob",
          "size": 2.6083984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// MyUDTMarshaler implements UDTMarshaler.\ntype MyUDTMarshaler struct {\n\tfieldA string\n\tfieldB int32\n}\n\n// MarshalUDT marshals the selected field to bytes.\nfunc (m MyUDTMarshaler) MarshalUDT(name string, info gocql.TypeInfo) ([]byte, error) {\n\tswitch name {\n\tcase \"field_a\":\n\t\treturn gocql.Marshal(info, m.fieldA)\n\tcase \"field_b\":\n\t\treturn gocql.Marshal(info, m.fieldB)\n\tdefault:\n\t\t// If you want to be strict and return error un unknown field, you can do so here instead.\n\t\t// Returning nil, nil will set the value of unknown fields to null, which might be handy if you want\n\t\t// to be forward-compatible when a new field is added to the UDT.\n\t\treturn nil, nil\n\t}\n}\n\n// ExampleUDTMarshaler demonstrates how to implement a UDTMarshaler.\nfunc ExampleUDTMarshaler() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate type example.my_udt (field_a text, field_b int);\n\tcreate table example.my_udt_table(pk int, value frozen<my_udt>, PRIMARY KEY(pk));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tvalue := MyUDTMarshaler{\n\t\tfieldA: \"a value\",\n\t\tfieldB: 42,\n\t}\n\terr = session.Query(\"INSERT INTO example.my_udt_table (pk, value) VALUES (?, ?)\",\n\t\t1, value).WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "example_udt_struct_test.go",
          "type": "blob",
          "size": 2.41796875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\ntype MyUDT struct {\n\tFieldA string `cql:\"field_a\"`\n\tFieldB int32  `cql:\"field_b\"`\n}\n\n// Example_userDefinedTypesStruct demonstrates how to work with user-defined types as structs.\n// See also examples for UDTMarshaler and UDTUnmarshaler if you need more control/better performance.\nfunc Example_userDefinedTypesStruct() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate type example.my_udt (field_a text, field_b int);\n\tcreate table example.my_udt_table(pk int, value frozen<my_udt>, PRIMARY KEY(pk));\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tvalue := MyUDT{\n\t\tFieldA: \"a value\",\n\t\tFieldB: 42,\n\t}\n\terr = session.Query(\"INSERT INTO example.my_udt_table (pk, value) VALUES (?, ?)\",\n\t\t1, value).WithContext(ctx).Exec()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tvar readValue MyUDT\n\n\terr = session.Query(\"SELECT value FROM example.my_udt_table WHERE pk = 1\").WithContext(ctx).Scan(&readValue)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(readValue.FieldA)\n\tfmt.Println(readValue.FieldB)\n\t// a value\n\t// 42\n}\n"
        },
        {
          "name": "example_udt_unmarshaler_test.go",
          "type": "blob",
          "size": 2.765625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\n// MyUDTUnmarshaler implements UDTUnmarshaler.\ntype MyUDTUnmarshaler struct {\n\tfieldA string\n\tfieldB int32\n}\n\n// UnmarshalUDT unmarshals the field identified by name into MyUDTUnmarshaler.\nfunc (m *MyUDTUnmarshaler) UnmarshalUDT(name string, info gocql.TypeInfo, data []byte) error {\n\tswitch name {\n\tcase \"field_a\":\n\t\treturn gocql.Unmarshal(info, data, &m.fieldA)\n\tcase \"field_b\":\n\t\treturn gocql.Unmarshal(info, data, &m.fieldB)\n\tdefault:\n\t\t// If you want to be strict and return error un unknown field, you can do so here instead.\n\t\t// Returning nil will ignore unknown fields, which might be handy if you want\n\t\t// to be forward-compatible when a new field is added to the UDT.\n\t\treturn nil\n\t}\n}\n\n// ExampleUDTUnmarshaler demonstrates how to implement a UDTUnmarshaler.\nfunc ExampleUDTUnmarshaler() {\n\t/* The example assumes the following CQL was used to setup the keyspace:\n\tcreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\tcreate type example.my_udt (field_a text, field_b int);\n\tcreate table example.my_udt_table(pk int, value frozen<my_udt>, PRIMARY KEY(pk));\n\tinsert into example.my_udt_table (pk, value) values (1, {field_a: 'a value', field_b: 42});\n\t*/\n\tcluster := gocql.NewCluster(\"localhost:9042\")\n\tcluster.Keyspace = \"example\"\n\tcluster.ProtoVersion = 4\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer session.Close()\n\n\tctx := context.Background()\n\n\tvar value MyUDTUnmarshaler\n\terr = session.Query(\"SELECT value FROM example.my_udt_table WHERE pk = 1\").WithContext(ctx).Scan(&value)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(value.fieldA)\n\tfmt.Println(value.fieldB)\n\t// a value\n\t// 42\n}\n"
        },
        {
          "name": "filters.go",
          "type": "blob",
          "size": 2.58984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport \"fmt\"\n\n// HostFilter interface is used when a host is discovered via server sent events.\ntype HostFilter interface {\n\t// Called when a new host is discovered, returning true will cause the host\n\t// to be added to the pools.\n\tAccept(host *HostInfo) bool\n}\n\n// HostFilterFunc converts a func(host HostInfo) bool into a HostFilter\ntype HostFilterFunc func(host *HostInfo) bool\n\nfunc (fn HostFilterFunc) Accept(host *HostInfo) bool {\n\treturn fn(host)\n}\n\n// AcceptAllFilter will accept all hosts\nfunc AcceptAllFilter() HostFilter {\n\treturn HostFilterFunc(func(host *HostInfo) bool {\n\t\treturn true\n\t})\n}\n\nfunc DenyAllFilter() HostFilter {\n\treturn HostFilterFunc(func(host *HostInfo) bool {\n\t\treturn false\n\t})\n}\n\n// DataCentreHostFilter filters all hosts such that they are in the same data centre\n// as the supplied data centre.\nfunc DataCentreHostFilter(dataCentre string) HostFilter {\n\treturn HostFilterFunc(func(host *HostInfo) bool {\n\t\treturn host.DataCenter() == dataCentre\n\t})\n}\n\n// WhiteListHostFilter filters incoming hosts by checking that their address is\n// in the initial hosts whitelist.\nfunc WhiteListHostFilter(hosts ...string) HostFilter {\n\thostInfos, err := addrsToHosts(hosts, 9042, nopLogger{})\n\tif err != nil {\n\t\t// dont want to panic here, but rather not break the API\n\t\tpanic(fmt.Errorf(\"unable to lookup host info from address: %v\", err))\n\t}\n\n\tm := make(map[string]bool, len(hostInfos))\n\tfor _, host := range hostInfos {\n\t\tm[host.ConnectAddress().String()] = true\n\t}\n\n\treturn HostFilterFunc(func(host *HostInfo) bool {\n\t\treturn m[host.ConnectAddress().String()]\n\t})\n}\n"
        },
        {
          "name": "filters_test.go",
          "type": "blob",
          "size": 2.984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestFilter_WhiteList(t *testing.T) {\n\tf := WhiteListHostFilter(\"127.0.0.1\", \"127.0.0.2\")\n\ttests := [...]struct {\n\t\taddr   net.IP\n\t\taccept bool\n\t}{\n\t\t{net.ParseIP(\"127.0.0.1\"), true},\n\t\t{net.ParseIP(\"127.0.0.2\"), true},\n\t\t{net.ParseIP(\"127.0.0.3\"), false},\n\t}\n\n\tfor i, test := range tests {\n\t\tif f.Accept(&HostInfo{connectAddress: test.addr}) {\n\t\t\tif !test.accept {\n\t\t\t\tt.Errorf(\"%d: should not have been accepted but was\", i)\n\t\t\t}\n\t\t} else if test.accept {\n\t\t\tt.Errorf(\"%d: should have been accepted but wasn't\", i)\n\t\t}\n\t}\n}\n\nfunc TestFilter_AllowAll(t *testing.T) {\n\tf := AcceptAllFilter()\n\ttests := [...]struct {\n\t\taddr   net.IP\n\t\taccept bool\n\t}{\n\t\t{net.ParseIP(\"127.0.0.1\"), true},\n\t\t{net.ParseIP(\"127.0.0.2\"), true},\n\t\t{net.ParseIP(\"127.0.0.3\"), true},\n\t}\n\n\tfor i, test := range tests {\n\t\tif f.Accept(&HostInfo{connectAddress: test.addr}) {\n\t\t\tif !test.accept {\n\t\t\t\tt.Errorf(\"%d: should not have been accepted but was\", i)\n\t\t\t}\n\t\t} else if test.accept {\n\t\t\tt.Errorf(\"%d: should have been accepted but wasn't\", i)\n\t\t}\n\t}\n}\n\nfunc TestFilter_DenyAll(t *testing.T) {\n\tf := DenyAllFilter()\n\ttests := [...]struct {\n\t\taddr   net.IP\n\t\taccept bool\n\t}{\n\t\t{net.ParseIP(\"127.0.0.1\"), false},\n\t\t{net.ParseIP(\"127.0.0.2\"), false},\n\t\t{net.ParseIP(\"127.0.0.3\"), false},\n\t}\n\n\tfor i, test := range tests {\n\t\tif f.Accept(&HostInfo{connectAddress: test.addr}) {\n\t\t\tif !test.accept {\n\t\t\t\tt.Errorf(\"%d: should not have been accepted but was\", i)\n\t\t\t}\n\t\t} else if test.accept {\n\t\t\tt.Errorf(\"%d: should have been accepted but wasn't\", i)\n\t\t}\n\t}\n}\n\nfunc TestFilter_DataCentre(t *testing.T) {\n\tf := DataCentreHostFilter(\"dc1\")\n\ttests := [...]struct {\n\t\tdc     string\n\t\taccept bool\n\t}{\n\t\t{\"dc1\", true},\n\t\t{\"dc2\", false},\n\t}\n\n\tfor i, test := range tests {\n\t\tif f.Accept(&HostInfo{dataCenter: test.dc}) {\n\t\t\tif !test.accept {\n\t\t\t\tt.Errorf(\"%d: should not have been accepted but was\", i)\n\t\t\t}\n\t\t} else if test.accept {\n\t\t\tt.Errorf(\"%d: should have been accepted but wasn't\", i)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "frame.go",
          "type": "blob",
          "size": 44.84375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"runtime\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype unsetColumn struct{}\n\n// UnsetValue represents a value used in a query binding that will be ignored by Cassandra.\n//\n// By setting a field to the unset value Cassandra will ignore the write completely.\n// The main advantage is the ability to keep the same prepared statement even when you don't\n// want to update some fields, where before you needed to make another prepared statement.\n//\n// UnsetValue is only available when using the version 4 of the protocol.\nvar UnsetValue = unsetColumn{}\n\ntype namedValue struct {\n\tname  string\n\tvalue interface{}\n}\n\n// NamedValue produce a value which will bind to the named parameter in a query\nfunc NamedValue(name string, value interface{}) interface{} {\n\treturn &namedValue{\n\t\tname:  name,\n\t\tvalue: value,\n\t}\n}\n\nconst (\n\tprotoDirectionMask = 0x80\n\tprotoVersionMask   = 0x7F\n\tprotoVersion1      = 0x01\n\tprotoVersion2      = 0x02\n\tprotoVersion3      = 0x03\n\tprotoVersion4      = 0x04\n\tprotoVersion5      = 0x05\n\n\tmaxFrameSize = 256 * 1024 * 1024\n)\n\ntype protoVersion byte\n\nfunc (p protoVersion) request() bool {\n\treturn p&protoDirectionMask == 0x00\n}\n\nfunc (p protoVersion) response() bool {\n\treturn p&protoDirectionMask == 0x80\n}\n\nfunc (p protoVersion) version() byte {\n\treturn byte(p) & protoVersionMask\n}\n\nfunc (p protoVersion) String() string {\n\tdir := \"REQ\"\n\tif p.response() {\n\t\tdir = \"RESP\"\n\t}\n\n\treturn fmt.Sprintf(\"[version=%d direction=%s]\", p.version(), dir)\n}\n\ntype frameOp byte\n\nconst (\n\t// header ops\n\topError         frameOp = 0x00\n\topStartup       frameOp = 0x01\n\topReady         frameOp = 0x02\n\topAuthenticate  frameOp = 0x03\n\topOptions       frameOp = 0x05\n\topSupported     frameOp = 0x06\n\topQuery         frameOp = 0x07\n\topResult        frameOp = 0x08\n\topPrepare       frameOp = 0x09\n\topExecute       frameOp = 0x0A\n\topRegister      frameOp = 0x0B\n\topEvent         frameOp = 0x0C\n\topBatch         frameOp = 0x0D\n\topAuthChallenge frameOp = 0x0E\n\topAuthResponse  frameOp = 0x0F\n\topAuthSuccess   frameOp = 0x10\n)\n\nfunc (f frameOp) String() string {\n\tswitch f {\n\tcase opError:\n\t\treturn \"ERROR\"\n\tcase opStartup:\n\t\treturn \"STARTUP\"\n\tcase opReady:\n\t\treturn \"READY\"\n\tcase opAuthenticate:\n\t\treturn \"AUTHENTICATE\"\n\tcase opOptions:\n\t\treturn \"OPTIONS\"\n\tcase opSupported:\n\t\treturn \"SUPPORTED\"\n\tcase opQuery:\n\t\treturn \"QUERY\"\n\tcase opResult:\n\t\treturn \"RESULT\"\n\tcase opPrepare:\n\t\treturn \"PREPARE\"\n\tcase opExecute:\n\t\treturn \"EXECUTE\"\n\tcase opRegister:\n\t\treturn \"REGISTER\"\n\tcase opEvent:\n\t\treturn \"EVENT\"\n\tcase opBatch:\n\t\treturn \"BATCH\"\n\tcase opAuthChallenge:\n\t\treturn \"AUTH_CHALLENGE\"\n\tcase opAuthResponse:\n\t\treturn \"AUTH_RESPONSE\"\n\tcase opAuthSuccess:\n\t\treturn \"AUTH_SUCCESS\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"UNKNOWN_OP_%d\", f)\n\t}\n}\n\nconst (\n\t// result kind\n\tresultKindVoid          = 1\n\tresultKindRows          = 2\n\tresultKindKeyspace      = 3\n\tresultKindPrepared      = 4\n\tresultKindSchemaChanged = 5\n\n\t// rows flags\n\tflagGlobalTableSpec int = 0x01\n\tflagHasMorePages    int = 0x02\n\tflagNoMetaData      int = 0x04\n\n\t// query flags\n\tflagValues                byte = 0x01\n\tflagSkipMetaData          byte = 0x02\n\tflagPageSize              byte = 0x04\n\tflagWithPagingState       byte = 0x08\n\tflagWithSerialConsistency byte = 0x10\n\tflagDefaultTimestamp      byte = 0x20\n\tflagWithNameValues        byte = 0x40\n\tflagWithKeyspace          byte = 0x80\n\n\t// prepare flags\n\tflagWithPreparedKeyspace uint32 = 0x01\n\n\t// header flags\n\tflagCompress      byte = 0x01\n\tflagTracing       byte = 0x02\n\tflagCustomPayload byte = 0x04\n\tflagWarning       byte = 0x08\n\tflagBetaProtocol  byte = 0x10\n)\n\ntype Consistency uint16\n\nconst (\n\tAny         Consistency = 0x00\n\tOne         Consistency = 0x01\n\tTwo         Consistency = 0x02\n\tThree       Consistency = 0x03\n\tQuorum      Consistency = 0x04\n\tAll         Consistency = 0x05\n\tLocalQuorum Consistency = 0x06\n\tEachQuorum  Consistency = 0x07\n\tLocalOne    Consistency = 0x0A\n)\n\nfunc (c Consistency) String() string {\n\tswitch c {\n\tcase Any:\n\t\treturn \"ANY\"\n\tcase One:\n\t\treturn \"ONE\"\n\tcase Two:\n\t\treturn \"TWO\"\n\tcase Three:\n\t\treturn \"THREE\"\n\tcase Quorum:\n\t\treturn \"QUORUM\"\n\tcase All:\n\t\treturn \"ALL\"\n\tcase LocalQuorum:\n\t\treturn \"LOCAL_QUORUM\"\n\tcase EachQuorum:\n\t\treturn \"EACH_QUORUM\"\n\tcase LocalOne:\n\t\treturn \"LOCAL_ONE\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"UNKNOWN_CONS_0x%x\", uint16(c))\n\t}\n}\n\nfunc (c Consistency) MarshalText() (text []byte, err error) {\n\treturn []byte(c.String()), nil\n}\n\nfunc (c *Consistency) UnmarshalText(text []byte) error {\n\tswitch string(text) {\n\tcase \"ANY\":\n\t\t*c = Any\n\tcase \"ONE\":\n\t\t*c = One\n\tcase \"TWO\":\n\t\t*c = Two\n\tcase \"THREE\":\n\t\t*c = Three\n\tcase \"QUORUM\":\n\t\t*c = Quorum\n\tcase \"ALL\":\n\t\t*c = All\n\tcase \"LOCAL_QUORUM\":\n\t\t*c = LocalQuorum\n\tcase \"EACH_QUORUM\":\n\t\t*c = EachQuorum\n\tcase \"LOCAL_ONE\":\n\t\t*c = LocalOne\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid consistency %q\", string(text))\n\t}\n\n\treturn nil\n}\n\nfunc ParseConsistency(s string) Consistency {\n\tvar c Consistency\n\tif err := c.UnmarshalText([]byte(strings.ToUpper(s))); err != nil {\n\t\tpanic(err)\n\t}\n\treturn c\n}\n\n// ParseConsistencyWrapper wraps gocql.ParseConsistency to provide an err\n// return instead of a panic\nfunc ParseConsistencyWrapper(s string) (consistency Consistency, err error) {\n\terr = consistency.UnmarshalText([]byte(strings.ToUpper(s)))\n\treturn\n}\n\n// MustParseConsistency is the same as ParseConsistency except it returns\n// an error (never). It is kept here since breaking changes are not good.\n// DEPRECATED: use ParseConsistency if you want a panic on parse error.\nfunc MustParseConsistency(s string) (Consistency, error) {\n\tc, err := ParseConsistencyWrapper(s)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn c, nil\n}\n\ntype SerialConsistency uint16\n\nconst (\n\tSerial      SerialConsistency = 0x08\n\tLocalSerial SerialConsistency = 0x09\n)\n\nfunc (s SerialConsistency) String() string {\n\tswitch s {\n\tcase Serial:\n\t\treturn \"SERIAL\"\n\tcase LocalSerial:\n\t\treturn \"LOCAL_SERIAL\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"UNKNOWN_SERIAL_CONS_0x%x\", uint16(s))\n\t}\n}\n\nfunc (s SerialConsistency) MarshalText() (text []byte, err error) {\n\treturn []byte(s.String()), nil\n}\n\nfunc (s *SerialConsistency) UnmarshalText(text []byte) error {\n\tswitch string(text) {\n\tcase \"SERIAL\":\n\t\t*s = Serial\n\tcase \"LOCAL_SERIAL\":\n\t\t*s = LocalSerial\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid consistency %q\", string(text))\n\t}\n\n\treturn nil\n}\n\nconst (\n\tapacheCassandraTypePrefix = \"org.apache.cassandra.db.marshal.\"\n)\n\nvar (\n\tErrFrameTooBig = errors.New(\"frame length is bigger than the maximum allowed\")\n)\n\nconst maxFrameHeaderSize = 9\n\nfunc readInt(p []byte) int32 {\n\treturn int32(p[0])<<24 | int32(p[1])<<16 | int32(p[2])<<8 | int32(p[3])\n}\n\ntype frameHeader struct {\n\tversion  protoVersion\n\tflags    byte\n\tstream   int\n\top       frameOp\n\tlength   int\n\twarnings []string\n}\n\nfunc (f frameHeader) String() string {\n\treturn fmt.Sprintf(\"[header version=%s flags=0x%x stream=%d op=%s length=%d]\", f.version, f.flags, f.stream, f.op, f.length)\n}\n\nfunc (f frameHeader) Header() frameHeader {\n\treturn f\n}\n\nconst defaultBufSize = 128\n\ntype ObservedFrameHeader struct {\n\tVersion protoVersion\n\tFlags   byte\n\tStream  int16\n\tOpcode  frameOp\n\tLength  int32\n\n\t// StartHeader is the time we started reading the frame header off the network connection.\n\tStart time.Time\n\t// EndHeader is the time we finished reading the frame header off the network connection.\n\tEnd time.Time\n\n\t// Host is Host of the connection the frame header was read from.\n\tHost *HostInfo\n}\n\nfunc (f ObservedFrameHeader) String() string {\n\treturn fmt.Sprintf(\"[observed header version=%s flags=0x%x stream=%d op=%s length=%d]\", f.Version, f.Flags, f.Stream, f.Opcode, f.Length)\n}\n\n// FrameHeaderObserver is the interface implemented by frame observers / stat collectors.\n//\n// Experimental, this interface and use may change\ntype FrameHeaderObserver interface {\n\t// ObserveFrameHeader gets called on every received frame header.\n\tObserveFrameHeader(context.Context, ObservedFrameHeader)\n}\n\n// a framer is responsible for reading, writing and parsing frames on a single stream\ntype framer struct {\n\tproto byte\n\t// flags are for outgoing flags, enabling compression and tracing etc\n\tflags    byte\n\tcompres  Compressor\n\theadSize int\n\t// if this frame was read then the header will be here\n\theader *frameHeader\n\n\t// if tracing flag is set this is not nil\n\ttraceID []byte\n\n\t// holds a ref to the whole byte slice for buf so that it can be reset to\n\t// 0 after a read.\n\treadBuffer []byte\n\n\tbuf []byte\n\n\tcustomPayload map[string][]byte\n}\n\nfunc newFramer(compressor Compressor, version byte) *framer {\n\tbuf := make([]byte, defaultBufSize)\n\tf := &framer{\n\t\tbuf:        buf[:0],\n\t\treadBuffer: buf,\n\t}\n\tvar flags byte\n\tif compressor != nil {\n\t\tflags |= flagCompress\n\t}\n\tif version == protoVersion5 {\n\t\tflags |= flagBetaProtocol\n\t}\n\n\tversion &= protoVersionMask\n\n\theadSize := 8\n\tif version > protoVersion2 {\n\t\theadSize = 9\n\t}\n\n\tf.compres = compressor\n\tf.proto = version\n\tf.flags = flags\n\tf.headSize = headSize\n\n\tf.header = nil\n\tf.traceID = nil\n\n\treturn f\n}\n\ntype frame interface {\n\tHeader() frameHeader\n}\n\nfunc readHeader(r io.Reader, p []byte) (head frameHeader, err error) {\n\t_, err = io.ReadFull(r, p[:1])\n\tif err != nil {\n\t\treturn frameHeader{}, err\n\t}\n\n\tversion := p[0] & protoVersionMask\n\n\tif version < protoVersion1 || version > protoVersion5 {\n\t\treturn frameHeader{}, fmt.Errorf(\"gocql: unsupported protocol response version: %d\", version)\n\t}\n\n\theadSize := 9\n\tif version < protoVersion3 {\n\t\theadSize = 8\n\t}\n\n\t_, err = io.ReadFull(r, p[1:headSize])\n\tif err != nil {\n\t\treturn frameHeader{}, err\n\t}\n\n\tp = p[:headSize]\n\n\thead.version = protoVersion(p[0])\n\thead.flags = p[1]\n\n\tif version > protoVersion2 {\n\t\tif len(p) != 9 {\n\t\t\treturn frameHeader{}, fmt.Errorf(\"not enough bytes to read header require 9 got: %d\", len(p))\n\t\t}\n\n\t\thead.stream = int(int16(p[2])<<8 | int16(p[3]))\n\t\thead.op = frameOp(p[4])\n\t\thead.length = int(readInt(p[5:]))\n\t} else {\n\t\tif len(p) != 8 {\n\t\t\treturn frameHeader{}, fmt.Errorf(\"not enough bytes to read header require 8 got: %d\", len(p))\n\t\t}\n\n\t\thead.stream = int(int8(p[2]))\n\t\thead.op = frameOp(p[3])\n\t\thead.length = int(readInt(p[4:]))\n\t}\n\n\treturn head, nil\n}\n\n// explicitly enables tracing for the framers outgoing requests\nfunc (f *framer) trace() {\n\tf.flags |= flagTracing\n}\n\n// explicitly enables the custom payload flag\nfunc (f *framer) payload() {\n\tf.flags |= flagCustomPayload\n}\n\n// reads a frame form the wire into the framers buffer\nfunc (f *framer) readFrame(r io.Reader, head *frameHeader) error {\n\tif head.length < 0 {\n\t\treturn fmt.Errorf(\"frame body length can not be less than 0: %d\", head.length)\n\t} else if head.length > maxFrameSize {\n\t\t// need to free up the connection to be used again\n\t\t_, err := io.CopyN(ioutil.Discard, r, int64(head.length))\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error whilst trying to discard frame with invalid length: %v\", err)\n\t\t}\n\t\treturn ErrFrameTooBig\n\t}\n\n\tif cap(f.readBuffer) >= head.length {\n\t\tf.buf = f.readBuffer[:head.length]\n\t} else {\n\t\tf.readBuffer = make([]byte, head.length)\n\t\tf.buf = f.readBuffer\n\t}\n\n\t// assume the underlying reader takes care of timeouts and retries\n\tn, err := io.ReadFull(r, f.buf)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to read frame body: read %d/%d bytes: %v\", n, head.length, err)\n\t}\n\n\tif head.flags&flagCompress == flagCompress {\n\t\tif f.compres == nil {\n\t\t\treturn NewErrProtocol(\"no compressor available with compressed frame body\")\n\t\t}\n\n\t\tf.buf, err = f.compres.Decode(f.buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tf.header = head\n\treturn nil\n}\n\nfunc (f *framer) parseFrame() (frame frame, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tif _, ok := r.(runtime.Error); ok {\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t\terr = r.(error)\n\t\t}\n\t}()\n\n\tif f.header.version.request() {\n\t\treturn nil, NewErrProtocol(\"got a request frame from server: %v\", f.header.version)\n\t}\n\n\tif f.header.flags&flagTracing == flagTracing {\n\t\tf.readTrace()\n\t}\n\n\tif f.header.flags&flagWarning == flagWarning {\n\t\tf.header.warnings = f.readStringList()\n\t}\n\n\tif f.header.flags&flagCustomPayload == flagCustomPayload {\n\t\tf.customPayload = f.readBytesMap()\n\t}\n\n\t// assumes that the frame body has been read into rbuf\n\tswitch f.header.op {\n\tcase opError:\n\t\tframe = f.parseErrorFrame()\n\tcase opReady:\n\t\tframe = f.parseReadyFrame()\n\tcase opResult:\n\t\tframe, err = f.parseResultFrame()\n\tcase opSupported:\n\t\tframe = f.parseSupportedFrame()\n\tcase opAuthenticate:\n\t\tframe = f.parseAuthenticateFrame()\n\tcase opAuthChallenge:\n\t\tframe = f.parseAuthChallengeFrame()\n\tcase opAuthSuccess:\n\t\tframe = f.parseAuthSuccessFrame()\n\tcase opEvent:\n\t\tframe = f.parseEventFrame()\n\tdefault:\n\t\treturn nil, NewErrProtocol(\"unknown op in frame header: %s\", f.header.op)\n\t}\n\n\treturn\n}\n\nfunc (f *framer) parseErrorFrame() frame {\n\tcode := f.readInt()\n\tmsg := f.readString()\n\n\terrD := errorFrame{\n\t\tframeHeader: *f.header,\n\t\tcode:        code,\n\t\tmessage:     msg,\n\t}\n\n\tswitch code {\n\tcase ErrCodeUnavailable:\n\t\tcl := f.readConsistency()\n\t\trequired := f.readInt()\n\t\talive := f.readInt()\n\t\treturn &RequestErrUnavailable{\n\t\t\terrorFrame:  errD,\n\t\t\tConsistency: cl,\n\t\t\tRequired:    required,\n\t\t\tAlive:       alive,\n\t\t}\n\tcase ErrCodeWriteTimeout:\n\t\tcl := f.readConsistency()\n\t\treceived := f.readInt()\n\t\tblockfor := f.readInt()\n\t\twriteType := f.readString()\n\t\treturn &RequestErrWriteTimeout{\n\t\t\terrorFrame:  errD,\n\t\t\tConsistency: cl,\n\t\t\tReceived:    received,\n\t\t\tBlockFor:    blockfor,\n\t\t\tWriteType:   writeType,\n\t\t}\n\tcase ErrCodeReadTimeout:\n\t\tcl := f.readConsistency()\n\t\treceived := f.readInt()\n\t\tblockfor := f.readInt()\n\t\tdataPresent := f.readByte()\n\t\treturn &RequestErrReadTimeout{\n\t\t\terrorFrame:  errD,\n\t\t\tConsistency: cl,\n\t\t\tReceived:    received,\n\t\t\tBlockFor:    blockfor,\n\t\t\tDataPresent: dataPresent,\n\t\t}\n\tcase ErrCodeAlreadyExists:\n\t\tks := f.readString()\n\t\ttable := f.readString()\n\t\treturn &RequestErrAlreadyExists{\n\t\t\terrorFrame: errD,\n\t\t\tKeyspace:   ks,\n\t\t\tTable:      table,\n\t\t}\n\tcase ErrCodeUnprepared:\n\t\tstmtId := f.readShortBytes()\n\t\treturn &RequestErrUnprepared{\n\t\t\terrorFrame:  errD,\n\t\t\tStatementId: copyBytes(stmtId), // defensively copy\n\t\t}\n\tcase ErrCodeReadFailure:\n\t\tres := &RequestErrReadFailure{\n\t\t\terrorFrame: errD,\n\t\t}\n\t\tres.Consistency = f.readConsistency()\n\t\tres.Received = f.readInt()\n\t\tres.BlockFor = f.readInt()\n\t\tif f.proto > protoVersion4 {\n\t\t\tres.ErrorMap = f.readErrorMap()\n\t\t\tres.NumFailures = len(res.ErrorMap)\n\t\t} else {\n\t\t\tres.NumFailures = f.readInt()\n\t\t}\n\t\tres.DataPresent = f.readByte() != 0\n\n\t\treturn res\n\tcase ErrCodeWriteFailure:\n\t\tres := &RequestErrWriteFailure{\n\t\t\terrorFrame: errD,\n\t\t}\n\t\tres.Consistency = f.readConsistency()\n\t\tres.Received = f.readInt()\n\t\tres.BlockFor = f.readInt()\n\t\tif f.proto > protoVersion4 {\n\t\t\tres.ErrorMap = f.readErrorMap()\n\t\t\tres.NumFailures = len(res.ErrorMap)\n\t\t} else {\n\t\t\tres.NumFailures = f.readInt()\n\t\t}\n\t\tres.WriteType = f.readString()\n\t\treturn res\n\tcase ErrCodeFunctionFailure:\n\t\tres := &RequestErrFunctionFailure{\n\t\t\terrorFrame: errD,\n\t\t}\n\t\tres.Keyspace = f.readString()\n\t\tres.Function = f.readString()\n\t\tres.ArgTypes = f.readStringList()\n\t\treturn res\n\n\tcase ErrCodeCDCWriteFailure:\n\t\tres := &RequestErrCDCWriteFailure{\n\t\t\terrorFrame: errD,\n\t\t}\n\t\treturn res\n\tcase ErrCodeCASWriteUnknown:\n\t\tres := &RequestErrCASWriteUnknown{\n\t\t\terrorFrame: errD,\n\t\t}\n\t\tres.Consistency = f.readConsistency()\n\t\tres.Received = f.readInt()\n\t\tres.BlockFor = f.readInt()\n\t\treturn res\n\tcase ErrCodeInvalid, ErrCodeBootstrapping, ErrCodeConfig, ErrCodeCredentials, ErrCodeOverloaded,\n\t\tErrCodeProtocol, ErrCodeServer, ErrCodeSyntax, ErrCodeTruncate, ErrCodeUnauthorized:\n\t\t// TODO(zariel): we should have some distinct types for these errors\n\t\treturn errD\n\tdefault:\n\t\tpanic(fmt.Errorf(\"unknown error code: 0x%x\", errD.code))\n\t}\n}\n\nfunc (f *framer) readErrorMap() (errMap ErrorMap) {\n\terrMap = make(ErrorMap)\n\tnumErrs := f.readInt()\n\tfor i := 0; i < numErrs; i++ {\n\t\tip := f.readInetAdressOnly().String()\n\t\terrMap[ip] = f.readShort()\n\t}\n\treturn\n}\n\nfunc (f *framer) writeHeader(flags byte, op frameOp, stream int) {\n\tf.buf = f.buf[:0]\n\tf.buf = append(f.buf,\n\t\tf.proto,\n\t\tflags,\n\t)\n\n\tif f.proto > protoVersion2 {\n\t\tf.buf = append(f.buf,\n\t\t\tbyte(stream>>8),\n\t\t\tbyte(stream),\n\t\t)\n\t} else {\n\t\tf.buf = append(f.buf,\n\t\t\tbyte(stream),\n\t\t)\n\t}\n\n\t// pad out length\n\tf.buf = append(f.buf,\n\t\tbyte(op),\n\t\t0,\n\t\t0,\n\t\t0,\n\t\t0,\n\t)\n}\n\nfunc (f *framer) setLength(length int) {\n\tp := 4\n\tif f.proto > protoVersion2 {\n\t\tp = 5\n\t}\n\n\tf.buf[p+0] = byte(length >> 24)\n\tf.buf[p+1] = byte(length >> 16)\n\tf.buf[p+2] = byte(length >> 8)\n\tf.buf[p+3] = byte(length)\n}\n\nfunc (f *framer) finish() error {\n\tif len(f.buf) > maxFrameSize {\n\t\t// huge app frame, lets remove it so it doesn't bloat the heap\n\t\tf.buf = make([]byte, defaultBufSize)\n\t\treturn ErrFrameTooBig\n\t}\n\n\tif f.buf[1]&flagCompress == flagCompress {\n\t\tif f.compres == nil {\n\t\t\tpanic(\"compress flag set with no compressor\")\n\t\t}\n\n\t\t// TODO: only compress frames which are big enough\n\t\tcompressed, err := f.compres.Encode(f.buf[f.headSize:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tf.buf = append(f.buf[:f.headSize], compressed...)\n\t}\n\tlength := len(f.buf) - f.headSize\n\tf.setLength(length)\n\n\treturn nil\n}\n\nfunc (f *framer) writeTo(w io.Writer) error {\n\t_, err := w.Write(f.buf)\n\treturn err\n}\n\nfunc (f *framer) readTrace() {\n\tf.traceID = f.readUUID().Bytes()\n}\n\ntype readyFrame struct {\n\tframeHeader\n}\n\nfunc (f *framer) parseReadyFrame() frame {\n\treturn &readyFrame{\n\t\tframeHeader: *f.header,\n\t}\n}\n\ntype supportedFrame struct {\n\tframeHeader\n\n\tsupported map[string][]string\n}\n\n// TODO: if we move the body buffer onto the frameHeader then we only need a single\n// framer, and can move the methods onto the header.\nfunc (f *framer) parseSupportedFrame() frame {\n\treturn &supportedFrame{\n\t\tframeHeader: *f.header,\n\n\t\tsupported: f.readStringMultiMap(),\n\t}\n}\n\ntype writeStartupFrame struct {\n\topts map[string]string\n}\n\nfunc (w writeStartupFrame) String() string {\n\treturn fmt.Sprintf(\"[startup opts=%+v]\", w.opts)\n}\n\nfunc (w *writeStartupFrame) buildFrame(f *framer, streamID int) error {\n\tf.writeHeader(f.flags&^flagCompress, opStartup, streamID)\n\tf.writeStringMap(w.opts)\n\n\treturn f.finish()\n}\n\ntype writePrepareFrame struct {\n\tstatement     string\n\tkeyspace      string\n\tcustomPayload map[string][]byte\n}\n\nfunc (w *writePrepareFrame) buildFrame(f *framer, streamID int) error {\n\tif len(w.customPayload) > 0 {\n\t\tf.payload()\n\t}\n\tf.writeHeader(f.flags, opPrepare, streamID)\n\tf.writeCustomPayload(&w.customPayload)\n\tf.writeLongString(w.statement)\n\n\tvar flags uint32 = 0\n\tif w.keyspace != \"\" {\n\t\tif f.proto > protoVersion4 {\n\t\t\tflags |= flagWithPreparedKeyspace\n\t\t} else {\n\t\t\tpanic(fmt.Errorf(\"the keyspace can only be set with protocol 5 or higher\"))\n\t\t}\n\t}\n\tif f.proto > protoVersion4 {\n\t\tf.writeUint(flags)\n\t}\n\tif w.keyspace != \"\" {\n\t\tf.writeString(w.keyspace)\n\t}\n\n\treturn f.finish()\n}\n\nfunc (f *framer) readTypeInfo() TypeInfo {\n\t// TODO: factor this out so the same code paths can be used to parse custom\n\t// types and other types, as much of the logic will be duplicated.\n\tid := f.readShort()\n\n\tsimple := NativeType{\n\t\tproto: f.proto,\n\t\ttyp:   Type(id),\n\t}\n\n\tif simple.typ == TypeCustom {\n\t\tsimple.custom = f.readString()\n\t\tif cassType := getApacheCassandraType(simple.custom); cassType != TypeCustom {\n\t\t\tsimple.typ = cassType\n\t\t}\n\t}\n\n\tswitch simple.typ {\n\tcase TypeTuple:\n\t\tn := f.readShort()\n\t\ttuple := TupleTypeInfo{\n\t\t\tNativeType: simple,\n\t\t\tElems:      make([]TypeInfo, n),\n\t\t}\n\n\t\tfor i := 0; i < int(n); i++ {\n\t\t\ttuple.Elems[i] = f.readTypeInfo()\n\t\t}\n\n\t\treturn tuple\n\n\tcase TypeUDT:\n\t\tudt := UDTTypeInfo{\n\t\t\tNativeType: simple,\n\t\t}\n\t\tudt.KeySpace = f.readString()\n\t\tudt.Name = f.readString()\n\n\t\tn := f.readShort()\n\t\tudt.Elements = make([]UDTField, n)\n\t\tfor i := 0; i < int(n); i++ {\n\t\t\tfield := &udt.Elements[i]\n\t\t\tfield.Name = f.readString()\n\t\t\tfield.Type = f.readTypeInfo()\n\t\t}\n\n\t\treturn udt\n\tcase TypeMap, TypeList, TypeSet:\n\t\tcollection := CollectionType{\n\t\t\tNativeType: simple,\n\t\t}\n\n\t\tif simple.typ == TypeMap {\n\t\t\tcollection.Key = f.readTypeInfo()\n\t\t}\n\n\t\tcollection.Elem = f.readTypeInfo()\n\n\t\treturn collection\n\t}\n\n\treturn simple\n}\n\ntype preparedMetadata struct {\n\tresultMetadata\n\n\t// proto v4+\n\tpkeyColumns []int\n\n\tkeyspace string\n\n\ttable string\n}\n\nfunc (r preparedMetadata) String() string {\n\treturn fmt.Sprintf(\"[prepared flags=0x%x pkey=%v paging_state=% X columns=%v col_count=%d actual_col_count=%d]\", r.flags, r.pkeyColumns, r.pagingState, r.columns, r.colCount, r.actualColCount)\n}\n\nfunc (f *framer) parsePreparedMetadata() preparedMetadata {\n\t// TODO: deduplicate this from parseMetadata\n\tmeta := preparedMetadata{}\n\n\tmeta.flags = f.readInt()\n\tmeta.colCount = f.readInt()\n\tif meta.colCount < 0 {\n\t\tpanic(fmt.Errorf(\"received negative column count: %d\", meta.colCount))\n\t}\n\tmeta.actualColCount = meta.colCount\n\n\tif f.proto >= protoVersion4 {\n\t\tpkeyCount := f.readInt()\n\t\tpkeys := make([]int, pkeyCount)\n\t\tfor i := 0; i < pkeyCount; i++ {\n\t\t\tpkeys[i] = int(f.readShort())\n\t\t}\n\t\tmeta.pkeyColumns = pkeys\n\t}\n\n\tif meta.flags&flagHasMorePages == flagHasMorePages {\n\t\tmeta.pagingState = copyBytes(f.readBytes())\n\t}\n\n\tif meta.flags&flagNoMetaData == flagNoMetaData {\n\t\treturn meta\n\t}\n\n\tglobalSpec := meta.flags&flagGlobalTableSpec == flagGlobalTableSpec\n\tif globalSpec {\n\t\tmeta.keyspace = f.readString()\n\t\tmeta.table = f.readString()\n\t}\n\n\tvar cols []ColumnInfo\n\tif meta.colCount < 1000 {\n\t\t// preallocate columninfo to avoid excess copying\n\t\tcols = make([]ColumnInfo, meta.colCount)\n\t\tfor i := 0; i < meta.colCount; i++ {\n\t\t\tf.readCol(&cols[i], &meta.resultMetadata, globalSpec, meta.keyspace, meta.table)\n\t\t}\n\t} else {\n\t\t// use append, huge number of columns usually indicates a corrupt frame or\n\t\t// just a huge row.\n\t\tfor i := 0; i < meta.colCount; i++ {\n\t\t\tvar col ColumnInfo\n\t\t\tf.readCol(&col, &meta.resultMetadata, globalSpec, meta.keyspace, meta.table)\n\t\t\tcols = append(cols, col)\n\t\t}\n\t}\n\n\tmeta.columns = cols\n\n\treturn meta\n}\n\ntype resultMetadata struct {\n\tflags int\n\n\t// only if flagPageState\n\tpagingState []byte\n\n\tcolumns  []ColumnInfo\n\tcolCount int\n\n\t// this is a count of the total number of columns which can be scanned,\n\t// it is at minimum len(columns) but may be larger, for instance when a column\n\t// is a UDT or tuple.\n\tactualColCount int\n}\n\nfunc (r *resultMetadata) morePages() bool {\n\treturn r.flags&flagHasMorePages == flagHasMorePages\n}\n\nfunc (r resultMetadata) String() string {\n\treturn fmt.Sprintf(\"[metadata flags=0x%x paging_state=% X columns=%v]\", r.flags, r.pagingState, r.columns)\n}\n\nfunc (f *framer) readCol(col *ColumnInfo, meta *resultMetadata, globalSpec bool, keyspace, table string) {\n\tif !globalSpec {\n\t\tcol.Keyspace = f.readString()\n\t\tcol.Table = f.readString()\n\t} else {\n\t\tcol.Keyspace = keyspace\n\t\tcol.Table = table\n\t}\n\n\tcol.Name = f.readString()\n\tcol.TypeInfo = f.readTypeInfo()\n\tswitch v := col.TypeInfo.(type) {\n\t// maybe also UDT\n\tcase TupleTypeInfo:\n\t\t// -1 because we already included the tuple column\n\t\tmeta.actualColCount += len(v.Elems) - 1\n\t}\n}\n\nfunc (f *framer) parseResultMetadata() resultMetadata {\n\tvar meta resultMetadata\n\n\tmeta.flags = f.readInt()\n\tmeta.colCount = f.readInt()\n\tif meta.colCount < 0 {\n\t\tpanic(fmt.Errorf(\"received negative column count: %d\", meta.colCount))\n\t}\n\tmeta.actualColCount = meta.colCount\n\n\tif meta.flags&flagHasMorePages == flagHasMorePages {\n\t\tmeta.pagingState = copyBytes(f.readBytes())\n\t}\n\n\tif meta.flags&flagNoMetaData == flagNoMetaData {\n\t\treturn meta\n\t}\n\n\tvar keyspace, table string\n\tglobalSpec := meta.flags&flagGlobalTableSpec == flagGlobalTableSpec\n\tif globalSpec {\n\t\tkeyspace = f.readString()\n\t\ttable = f.readString()\n\t}\n\n\tvar cols []ColumnInfo\n\tif meta.colCount < 1000 {\n\t\t// preallocate columninfo to avoid excess copying\n\t\tcols = make([]ColumnInfo, meta.colCount)\n\t\tfor i := 0; i < meta.colCount; i++ {\n\t\t\tf.readCol(&cols[i], &meta, globalSpec, keyspace, table)\n\t\t}\n\n\t} else {\n\t\t// use append, huge number of columns usually indicates a corrupt frame or\n\t\t// just a huge row.\n\t\tfor i := 0; i < meta.colCount; i++ {\n\t\t\tvar col ColumnInfo\n\t\t\tf.readCol(&col, &meta, globalSpec, keyspace, table)\n\t\t\tcols = append(cols, col)\n\t\t}\n\t}\n\n\tmeta.columns = cols\n\n\treturn meta\n}\n\ntype resultVoidFrame struct {\n\tframeHeader\n}\n\nfunc (f *resultVoidFrame) String() string {\n\treturn \"[result_void]\"\n}\n\nfunc (f *framer) parseResultFrame() (frame, error) {\n\tkind := f.readInt()\n\n\tswitch kind {\n\tcase resultKindVoid:\n\t\treturn &resultVoidFrame{frameHeader: *f.header}, nil\n\tcase resultKindRows:\n\t\treturn f.parseResultRows(), nil\n\tcase resultKindKeyspace:\n\t\treturn f.parseResultSetKeyspace(), nil\n\tcase resultKindPrepared:\n\t\treturn f.parseResultPrepared(), nil\n\tcase resultKindSchemaChanged:\n\t\treturn f.parseResultSchemaChange(), nil\n\t}\n\n\treturn nil, NewErrProtocol(\"unknown result kind: %x\", kind)\n}\n\ntype resultRowsFrame struct {\n\tframeHeader\n\n\tmeta resultMetadata\n\t// dont parse the rows here as we only need to do it once\n\tnumRows int\n}\n\nfunc (f *resultRowsFrame) String() string {\n\treturn fmt.Sprintf(\"[result_rows meta=%v]\", f.meta)\n}\n\nfunc (f *framer) parseResultRows() frame {\n\tresult := &resultRowsFrame{}\n\tresult.meta = f.parseResultMetadata()\n\n\tresult.numRows = f.readInt()\n\tif result.numRows < 0 {\n\t\tpanic(fmt.Errorf(\"invalid row_count in result frame: %d\", result.numRows))\n\t}\n\n\treturn result\n}\n\ntype resultKeyspaceFrame struct {\n\tframeHeader\n\tkeyspace string\n}\n\nfunc (r *resultKeyspaceFrame) String() string {\n\treturn fmt.Sprintf(\"[result_keyspace keyspace=%s]\", r.keyspace)\n}\n\nfunc (f *framer) parseResultSetKeyspace() frame {\n\treturn &resultKeyspaceFrame{\n\t\tframeHeader: *f.header,\n\t\tkeyspace:    f.readString(),\n\t}\n}\n\ntype resultPreparedFrame struct {\n\tframeHeader\n\n\tpreparedID []byte\n\treqMeta    preparedMetadata\n\trespMeta   resultMetadata\n}\n\nfunc (f *framer) parseResultPrepared() frame {\n\tframe := &resultPreparedFrame{\n\t\tframeHeader: *f.header,\n\t\tpreparedID:  f.readShortBytes(),\n\t\treqMeta:     f.parsePreparedMetadata(),\n\t}\n\n\tif f.proto < protoVersion2 {\n\t\treturn frame\n\t}\n\n\tframe.respMeta = f.parseResultMetadata()\n\n\treturn frame\n}\n\ntype schemaChangeKeyspace struct {\n\tframeHeader\n\n\tchange   string\n\tkeyspace string\n}\n\nfunc (f schemaChangeKeyspace) String() string {\n\treturn fmt.Sprintf(\"[event schema_change_keyspace change=%q keyspace=%q]\", f.change, f.keyspace)\n}\n\ntype schemaChangeTable struct {\n\tframeHeader\n\n\tchange   string\n\tkeyspace string\n\tobject   string\n}\n\nfunc (f schemaChangeTable) String() string {\n\treturn fmt.Sprintf(\"[event schema_change change=%q keyspace=%q object=%q]\", f.change, f.keyspace, f.object)\n}\n\ntype schemaChangeType struct {\n\tframeHeader\n\n\tchange   string\n\tkeyspace string\n\tobject   string\n}\n\ntype schemaChangeFunction struct {\n\tframeHeader\n\n\tchange   string\n\tkeyspace string\n\tname     string\n\targs     []string\n}\n\ntype schemaChangeAggregate struct {\n\tframeHeader\n\n\tchange   string\n\tkeyspace string\n\tname     string\n\targs     []string\n}\n\nfunc (f *framer) parseResultSchemaChange() frame {\n\tif f.proto <= protoVersion2 {\n\t\tchange := f.readString()\n\t\tkeyspace := f.readString()\n\t\ttable := f.readString()\n\n\t\tif table != \"\" {\n\t\t\treturn &schemaChangeTable{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t\tkeyspace:    keyspace,\n\t\t\t\tobject:      table,\n\t\t\t}\n\t\t} else {\n\t\t\treturn &schemaChangeKeyspace{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t\tkeyspace:    keyspace,\n\t\t\t}\n\t\t}\n\t} else {\n\t\tchange := f.readString()\n\t\ttarget := f.readString()\n\n\t\t// TODO: could just use a separate type for each target\n\t\tswitch target {\n\t\tcase \"KEYSPACE\":\n\t\t\tframe := &schemaChangeKeyspace{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t}\n\n\t\t\tframe.keyspace = f.readString()\n\n\t\t\treturn frame\n\t\tcase \"TABLE\":\n\t\t\tframe := &schemaChangeTable{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t}\n\n\t\t\tframe.keyspace = f.readString()\n\t\t\tframe.object = f.readString()\n\n\t\t\treturn frame\n\t\tcase \"TYPE\":\n\t\t\tframe := &schemaChangeType{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t}\n\n\t\t\tframe.keyspace = f.readString()\n\t\t\tframe.object = f.readString()\n\n\t\t\treturn frame\n\t\tcase \"FUNCTION\":\n\t\t\tframe := &schemaChangeFunction{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t}\n\n\t\t\tframe.keyspace = f.readString()\n\t\t\tframe.name = f.readString()\n\t\t\tframe.args = f.readStringList()\n\n\t\t\treturn frame\n\t\tcase \"AGGREGATE\":\n\t\t\tframe := &schemaChangeAggregate{\n\t\t\t\tframeHeader: *f.header,\n\t\t\t\tchange:      change,\n\t\t\t}\n\n\t\t\tframe.keyspace = f.readString()\n\t\t\tframe.name = f.readString()\n\t\t\tframe.args = f.readStringList()\n\n\t\t\treturn frame\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"gocql: unknown SCHEMA_CHANGE target: %q change: %q\", target, change))\n\t\t}\n\t}\n\n}\n\ntype authenticateFrame struct {\n\tframeHeader\n\n\tclass string\n}\n\nfunc (a *authenticateFrame) String() string {\n\treturn fmt.Sprintf(\"[authenticate class=%q]\", a.class)\n}\n\nfunc (f *framer) parseAuthenticateFrame() frame {\n\treturn &authenticateFrame{\n\t\tframeHeader: *f.header,\n\t\tclass:       f.readString(),\n\t}\n}\n\ntype authSuccessFrame struct {\n\tframeHeader\n\n\tdata []byte\n}\n\nfunc (a *authSuccessFrame) String() string {\n\treturn fmt.Sprintf(\"[auth_success data=%q]\", a.data)\n}\n\nfunc (f *framer) parseAuthSuccessFrame() frame {\n\treturn &authSuccessFrame{\n\t\tframeHeader: *f.header,\n\t\tdata:        f.readBytes(),\n\t}\n}\n\ntype authChallengeFrame struct {\n\tframeHeader\n\n\tdata []byte\n}\n\nfunc (a *authChallengeFrame) String() string {\n\treturn fmt.Sprintf(\"[auth_challenge data=%q]\", a.data)\n}\n\nfunc (f *framer) parseAuthChallengeFrame() frame {\n\treturn &authChallengeFrame{\n\t\tframeHeader: *f.header,\n\t\tdata:        f.readBytes(),\n\t}\n}\n\ntype statusChangeEventFrame struct {\n\tframeHeader\n\n\tchange string\n\thost   net.IP\n\tport   int\n}\n\nfunc (t statusChangeEventFrame) String() string {\n\treturn fmt.Sprintf(\"[status_change change=%s host=%v port=%v]\", t.change, t.host, t.port)\n}\n\n// essentially the same as statusChange\ntype topologyChangeEventFrame struct {\n\tframeHeader\n\n\tchange string\n\thost   net.IP\n\tport   int\n}\n\nfunc (t topologyChangeEventFrame) String() string {\n\treturn fmt.Sprintf(\"[topology_change change=%s host=%v port=%v]\", t.change, t.host, t.port)\n}\n\nfunc (f *framer) parseEventFrame() frame {\n\teventType := f.readString()\n\n\tswitch eventType {\n\tcase \"TOPOLOGY_CHANGE\":\n\t\tframe := &topologyChangeEventFrame{frameHeader: *f.header}\n\t\tframe.change = f.readString()\n\t\tframe.host, frame.port = f.readInet()\n\n\t\treturn frame\n\tcase \"STATUS_CHANGE\":\n\t\tframe := &statusChangeEventFrame{frameHeader: *f.header}\n\t\tframe.change = f.readString()\n\t\tframe.host, frame.port = f.readInet()\n\n\t\treturn frame\n\tcase \"SCHEMA_CHANGE\":\n\t\t// this should work for all versions\n\t\treturn f.parseResultSchemaChange()\n\tdefault:\n\t\tpanic(fmt.Errorf(\"gocql: unknown event type: %q\", eventType))\n\t}\n\n}\n\ntype writeAuthResponseFrame struct {\n\tdata []byte\n}\n\nfunc (a *writeAuthResponseFrame) String() string {\n\treturn fmt.Sprintf(\"[auth_response data=%q]\", a.data)\n}\n\nfunc (a *writeAuthResponseFrame) buildFrame(framer *framer, streamID int) error {\n\treturn framer.writeAuthResponseFrame(streamID, a.data)\n}\n\nfunc (f *framer) writeAuthResponseFrame(streamID int, data []byte) error {\n\tf.writeHeader(f.flags, opAuthResponse, streamID)\n\tf.writeBytes(data)\n\treturn f.finish()\n}\n\ntype queryValues struct {\n\tvalue []byte\n\n\t// optional name, will set With names for values flag\n\tname    string\n\tisUnset bool\n}\n\ntype queryParams struct {\n\tconsistency Consistency\n\t// v2+\n\tskipMeta          bool\n\tvalues            []queryValues\n\tpageSize          int\n\tpagingState       []byte\n\tserialConsistency SerialConsistency\n\t// v3+\n\tdefaultTimestamp      bool\n\tdefaultTimestampValue int64\n\t// v5+\n\tkeyspace string\n}\n\nfunc (q queryParams) String() string {\n\treturn fmt.Sprintf(\"[query_params consistency=%v skip_meta=%v page_size=%d paging_state=%q serial_consistency=%v default_timestamp=%v values=%v keyspace=%s]\",\n\t\tq.consistency, q.skipMeta, q.pageSize, q.pagingState, q.serialConsistency, q.defaultTimestamp, q.values, q.keyspace)\n}\n\nfunc (f *framer) writeQueryParams(opts *queryParams) {\n\tf.writeConsistency(opts.consistency)\n\n\tif f.proto == protoVersion1 {\n\t\treturn\n\t}\n\n\tvar flags byte\n\tif len(opts.values) > 0 {\n\t\tflags |= flagValues\n\t}\n\tif opts.skipMeta {\n\t\tflags |= flagSkipMetaData\n\t}\n\tif opts.pageSize > 0 {\n\t\tflags |= flagPageSize\n\t}\n\tif len(opts.pagingState) > 0 {\n\t\tflags |= flagWithPagingState\n\t}\n\tif opts.serialConsistency > 0 {\n\t\tflags |= flagWithSerialConsistency\n\t}\n\n\tnames := false\n\n\t// protoV3 specific things\n\tif f.proto > protoVersion2 {\n\t\tif opts.defaultTimestamp {\n\t\t\tflags |= flagDefaultTimestamp\n\t\t}\n\n\t\tif len(opts.values) > 0 && opts.values[0].name != \"\" {\n\t\t\tflags |= flagWithNameValues\n\t\t\tnames = true\n\t\t}\n\t}\n\n\tif opts.keyspace != \"\" {\n\t\tif f.proto > protoVersion4 {\n\t\t\tflags |= flagWithKeyspace\n\t\t} else {\n\t\t\tpanic(fmt.Errorf(\"the keyspace can only be set with protocol 5 or higher\"))\n\t\t}\n\t}\n\n\tif f.proto > protoVersion4 {\n\t\tf.writeUint(uint32(flags))\n\t} else {\n\t\tf.writeByte(flags)\n\t}\n\n\tif n := len(opts.values); n > 0 {\n\t\tf.writeShort(uint16(n))\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tif names {\n\t\t\t\tf.writeString(opts.values[i].name)\n\t\t\t}\n\t\t\tif opts.values[i].isUnset {\n\t\t\t\tf.writeUnset()\n\t\t\t} else {\n\t\t\t\tf.writeBytes(opts.values[i].value)\n\t\t\t}\n\t\t}\n\t}\n\n\tif opts.pageSize > 0 {\n\t\tf.writeInt(int32(opts.pageSize))\n\t}\n\n\tif len(opts.pagingState) > 0 {\n\t\tf.writeBytes(opts.pagingState)\n\t}\n\n\tif opts.serialConsistency > 0 {\n\t\tf.writeConsistency(Consistency(opts.serialConsistency))\n\t}\n\n\tif f.proto > protoVersion2 && opts.defaultTimestamp {\n\t\t// timestamp in microseconds\n\t\tvar ts int64\n\t\tif opts.defaultTimestampValue != 0 {\n\t\t\tts = opts.defaultTimestampValue\n\t\t} else {\n\t\t\tts = time.Now().UnixNano() / 1000\n\t\t}\n\t\tf.writeLong(ts)\n\t}\n\n\tif opts.keyspace != \"\" {\n\t\tf.writeString(opts.keyspace)\n\t}\n}\n\ntype writeQueryFrame struct {\n\tstatement string\n\tparams    queryParams\n\n\t// v4+\n\tcustomPayload map[string][]byte\n}\n\nfunc (w *writeQueryFrame) String() string {\n\treturn fmt.Sprintf(\"[query statement=%q params=%v]\", w.statement, w.params)\n}\n\nfunc (w *writeQueryFrame) buildFrame(framer *framer, streamID int) error {\n\treturn framer.writeQueryFrame(streamID, w.statement, &w.params, w.customPayload)\n}\n\nfunc (f *framer) writeQueryFrame(streamID int, statement string, params *queryParams, customPayload map[string][]byte) error {\n\tif len(customPayload) > 0 {\n\t\tf.payload()\n\t}\n\tf.writeHeader(f.flags, opQuery, streamID)\n\tf.writeCustomPayload(&customPayload)\n\tf.writeLongString(statement)\n\tf.writeQueryParams(params)\n\n\treturn f.finish()\n}\n\ntype frameBuilder interface {\n\tbuildFrame(framer *framer, streamID int) error\n}\n\ntype frameWriterFunc func(framer *framer, streamID int) error\n\nfunc (f frameWriterFunc) buildFrame(framer *framer, streamID int) error {\n\treturn f(framer, streamID)\n}\n\ntype writeExecuteFrame struct {\n\tpreparedID []byte\n\tparams     queryParams\n\n\t// v4+\n\tcustomPayload map[string][]byte\n}\n\nfunc (e *writeExecuteFrame) String() string {\n\treturn fmt.Sprintf(\"[execute id=% X params=%v]\", e.preparedID, &e.params)\n}\n\nfunc (e *writeExecuteFrame) buildFrame(fr *framer, streamID int) error {\n\treturn fr.writeExecuteFrame(streamID, e.preparedID, &e.params, &e.customPayload)\n}\n\nfunc (f *framer) writeExecuteFrame(streamID int, preparedID []byte, params *queryParams, customPayload *map[string][]byte) error {\n\tif len(*customPayload) > 0 {\n\t\tf.payload()\n\t}\n\tf.writeHeader(f.flags, opExecute, streamID)\n\tf.writeCustomPayload(customPayload)\n\tf.writeShortBytes(preparedID)\n\tif f.proto > protoVersion1 {\n\t\tf.writeQueryParams(params)\n\t} else {\n\t\tn := len(params.values)\n\t\tf.writeShort(uint16(n))\n\t\tfor i := 0; i < n; i++ {\n\t\t\tif params.values[i].isUnset {\n\t\t\t\tf.writeUnset()\n\t\t\t} else {\n\t\t\t\tf.writeBytes(params.values[i].value)\n\t\t\t}\n\t\t}\n\t\tf.writeConsistency(params.consistency)\n\t}\n\n\treturn f.finish()\n}\n\n// TODO: can we replace BatchStatemt with batchStatement? As they prety much\n// duplicate each other\ntype batchStatment struct {\n\tpreparedID []byte\n\tstatement  string\n\tvalues     []queryValues\n}\n\ntype writeBatchFrame struct {\n\ttyp         BatchType\n\tstatements  []batchStatment\n\tconsistency Consistency\n\n\t// v3+\n\tserialConsistency     SerialConsistency\n\tdefaultTimestamp      bool\n\tdefaultTimestampValue int64\n\n\t//v4+\n\tcustomPayload map[string][]byte\n}\n\nfunc (w *writeBatchFrame) buildFrame(framer *framer, streamID int) error {\n\treturn framer.writeBatchFrame(streamID, w, w.customPayload)\n}\n\nfunc (f *framer) writeBatchFrame(streamID int, w *writeBatchFrame, customPayload map[string][]byte) error {\n\tif len(customPayload) > 0 {\n\t\tf.payload()\n\t}\n\tf.writeHeader(f.flags, opBatch, streamID)\n\tf.writeCustomPayload(&customPayload)\n\tf.writeByte(byte(w.typ))\n\n\tn := len(w.statements)\n\tf.writeShort(uint16(n))\n\n\tvar flags byte\n\n\tfor i := 0; i < n; i++ {\n\t\tb := &w.statements[i]\n\t\tif len(b.preparedID) == 0 {\n\t\t\tf.writeByte(0)\n\t\t\tf.writeLongString(b.statement)\n\t\t} else {\n\t\t\tf.writeByte(1)\n\t\t\tf.writeShortBytes(b.preparedID)\n\t\t}\n\n\t\tf.writeShort(uint16(len(b.values)))\n\t\tfor j := range b.values {\n\t\t\tcol := b.values[j]\n\t\t\tif f.proto > protoVersion2 && col.name != \"\" {\n\t\t\t\t// TODO: move this check into the caller and set a flag on writeBatchFrame\n\t\t\t\t// to indicate using named values\n\t\t\t\tif f.proto <= protoVersion5 {\n\t\t\t\t\treturn fmt.Errorf(\"gocql: named query values are not supported in batches, please see https://issues.apache.org/jira/browse/CASSANDRA-10246\")\n\t\t\t\t}\n\t\t\t\tflags |= flagWithNameValues\n\t\t\t\tf.writeString(col.name)\n\t\t\t}\n\t\t\tif col.isUnset {\n\t\t\t\tf.writeUnset()\n\t\t\t} else {\n\t\t\t\tf.writeBytes(col.value)\n\t\t\t}\n\t\t}\n\t}\n\n\tf.writeConsistency(w.consistency)\n\n\tif f.proto > protoVersion2 {\n\t\tif w.serialConsistency > 0 {\n\t\t\tflags |= flagWithSerialConsistency\n\t\t}\n\t\tif w.defaultTimestamp {\n\t\t\tflags |= flagDefaultTimestamp\n\t\t}\n\n\t\tif f.proto > protoVersion4 {\n\t\t\tf.writeUint(uint32(flags))\n\t\t} else {\n\t\t\tf.writeByte(flags)\n\t\t}\n\n\t\tif w.serialConsistency > 0 {\n\t\t\tf.writeConsistency(Consistency(w.serialConsistency))\n\t\t}\n\n\t\tif w.defaultTimestamp {\n\t\t\tvar ts int64\n\t\t\tif w.defaultTimestampValue != 0 {\n\t\t\t\tts = w.defaultTimestampValue\n\t\t\t} else {\n\t\t\t\tts = time.Now().UnixNano() / 1000\n\t\t\t}\n\t\t\tf.writeLong(ts)\n\t\t}\n\t}\n\n\treturn f.finish()\n}\n\ntype writeOptionsFrame struct{}\n\nfunc (w *writeOptionsFrame) buildFrame(framer *framer, streamID int) error {\n\treturn framer.writeOptionsFrame(streamID, w)\n}\n\nfunc (f *framer) writeOptionsFrame(stream int, _ *writeOptionsFrame) error {\n\tf.writeHeader(f.flags&^flagCompress, opOptions, stream)\n\treturn f.finish()\n}\n\ntype writeRegisterFrame struct {\n\tevents []string\n}\n\nfunc (w *writeRegisterFrame) buildFrame(framer *framer, streamID int) error {\n\treturn framer.writeRegisterFrame(streamID, w)\n}\n\nfunc (f *framer) writeRegisterFrame(streamID int, w *writeRegisterFrame) error {\n\tf.writeHeader(f.flags, opRegister, streamID)\n\tf.writeStringList(w.events)\n\n\treturn f.finish()\n}\n\nfunc (f *framer) readByte() byte {\n\tif len(f.buf) < 1 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read byte require 1 got: %d\", len(f.buf)))\n\t}\n\n\tb := f.buf[0]\n\tf.buf = f.buf[1:]\n\treturn b\n}\n\nfunc (f *framer) readInt() (n int) {\n\tif len(f.buf) < 4 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read int require 4 got: %d\", len(f.buf)))\n\t}\n\n\tn = int(int32(f.buf[0])<<24 | int32(f.buf[1])<<16 | int32(f.buf[2])<<8 | int32(f.buf[3]))\n\tf.buf = f.buf[4:]\n\treturn\n}\n\nfunc (f *framer) readShort() (n uint16) {\n\tif len(f.buf) < 2 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read short require 2 got: %d\", len(f.buf)))\n\t}\n\tn = uint16(f.buf[0])<<8 | uint16(f.buf[1])\n\tf.buf = f.buf[2:]\n\treturn\n}\n\nfunc (f *framer) readString() (s string) {\n\tsize := f.readShort()\n\n\tif len(f.buf) < int(size) {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read string require %d got: %d\", size, len(f.buf)))\n\t}\n\n\ts = string(f.buf[:size])\n\tf.buf = f.buf[size:]\n\treturn\n}\n\nfunc (f *framer) readLongString() (s string) {\n\tsize := f.readInt()\n\n\tif len(f.buf) < size {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read long string require %d got: %d\", size, len(f.buf)))\n\t}\n\n\ts = string(f.buf[:size])\n\tf.buf = f.buf[size:]\n\treturn\n}\n\nfunc (f *framer) readUUID() *UUID {\n\tif len(f.buf) < 16 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read uuid require %d got: %d\", 16, len(f.buf)))\n\t}\n\n\t// TODO: how to handle this error, if it is a uuid, then sureley, problems?\n\tu, _ := UUIDFromBytes(f.buf[:16])\n\tf.buf = f.buf[16:]\n\treturn &u\n}\n\nfunc (f *framer) readStringList() []string {\n\tsize := f.readShort()\n\n\tl := make([]string, size)\n\tfor i := 0; i < int(size); i++ {\n\t\tl[i] = f.readString()\n\t}\n\n\treturn l\n}\n\nfunc (f *framer) readBytesInternal() ([]byte, error) {\n\tsize := f.readInt()\n\tif size < 0 {\n\t\treturn nil, nil\n\t}\n\n\tif len(f.buf) < size {\n\t\treturn nil, fmt.Errorf(\"not enough bytes in buffer to read bytes require %d got: %d\", size, len(f.buf))\n\t}\n\n\tl := f.buf[:size]\n\tf.buf = f.buf[size:]\n\n\treturn l, nil\n}\n\nfunc (f *framer) readBytes() []byte {\n\tl, err := f.readBytesInternal()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn l\n}\n\nfunc (f *framer) readShortBytes() []byte {\n\tsize := f.readShort()\n\tif len(f.buf) < int(size) {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read short bytes: require %d got %d\", size, len(f.buf)))\n\t}\n\n\tl := f.buf[:size]\n\tf.buf = f.buf[size:]\n\n\treturn l\n}\n\nfunc (f *framer) readInetAdressOnly() net.IP {\n\tif len(f.buf) < 1 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read inet size require %d got: %d\", 1, len(f.buf)))\n\t}\n\n\tsize := f.buf[0]\n\tf.buf = f.buf[1:]\n\n\tif !(size == 4 || size == 16) {\n\t\tpanic(fmt.Errorf(\"invalid IP size: %d\", size))\n\t}\n\n\tif len(f.buf) < 1 {\n\t\tpanic(fmt.Errorf(\"not enough bytes in buffer to read inet require %d got: %d\", size, len(f.buf)))\n\t}\n\n\tip := make([]byte, size)\n\tcopy(ip, f.buf[:size])\n\tf.buf = f.buf[size:]\n\treturn net.IP(ip)\n}\n\nfunc (f *framer) readInet() (net.IP, int) {\n\treturn f.readInetAdressOnly(), f.readInt()\n}\n\nfunc (f *framer) readConsistency() Consistency {\n\treturn Consistency(f.readShort())\n}\n\nfunc (f *framer) readBytesMap() map[string][]byte {\n\tsize := f.readShort()\n\tm := make(map[string][]byte, size)\n\n\tfor i := 0; i < int(size); i++ {\n\t\tk := f.readString()\n\t\tv := f.readBytes()\n\t\tm[k] = v\n\t}\n\n\treturn m\n}\n\nfunc (f *framer) readStringMultiMap() map[string][]string {\n\tsize := f.readShort()\n\tm := make(map[string][]string, size)\n\n\tfor i := 0; i < int(size); i++ {\n\t\tk := f.readString()\n\t\tv := f.readStringList()\n\t\tm[k] = v\n\t}\n\n\treturn m\n}\n\nfunc (f *framer) writeByte(b byte) {\n\tf.buf = append(f.buf, b)\n}\n\nfunc appendBytes(p []byte, d []byte) []byte {\n\tif d == nil {\n\t\treturn appendInt(p, -1)\n\t}\n\tp = appendInt(p, int32(len(d)))\n\tp = append(p, d...)\n\treturn p\n}\n\nfunc appendShort(p []byte, n uint16) []byte {\n\treturn append(p,\n\t\tbyte(n>>8),\n\t\tbyte(n),\n\t)\n}\n\nfunc appendInt(p []byte, n int32) []byte {\n\treturn append(p, byte(n>>24),\n\t\tbyte(n>>16),\n\t\tbyte(n>>8),\n\t\tbyte(n))\n}\n\nfunc appendUint(p []byte, n uint32) []byte {\n\treturn append(p, byte(n>>24),\n\t\tbyte(n>>16),\n\t\tbyte(n>>8),\n\t\tbyte(n))\n}\n\nfunc appendLong(p []byte, n int64) []byte {\n\treturn append(p,\n\t\tbyte(n>>56),\n\t\tbyte(n>>48),\n\t\tbyte(n>>40),\n\t\tbyte(n>>32),\n\t\tbyte(n>>24),\n\t\tbyte(n>>16),\n\t\tbyte(n>>8),\n\t\tbyte(n),\n\t)\n}\n\nfunc (f *framer) writeCustomPayload(customPayload *map[string][]byte) {\n\tif len(*customPayload) > 0 {\n\t\tif f.proto < protoVersion4 {\n\t\t\tpanic(\"Custom payload is not supported with version V3 or less\")\n\t\t}\n\t\tf.writeBytesMap(*customPayload)\n\t}\n}\n\n// these are protocol level binary types\nfunc (f *framer) writeInt(n int32) {\n\tf.buf = appendInt(f.buf, n)\n}\n\nfunc (f *framer) writeUint(n uint32) {\n\tf.buf = appendUint(f.buf, n)\n}\n\nfunc (f *framer) writeShort(n uint16) {\n\tf.buf = appendShort(f.buf, n)\n}\n\nfunc (f *framer) writeLong(n int64) {\n\tf.buf = appendLong(f.buf, n)\n}\n\nfunc (f *framer) writeString(s string) {\n\tf.writeShort(uint16(len(s)))\n\tf.buf = append(f.buf, s...)\n}\n\nfunc (f *framer) writeLongString(s string) {\n\tf.writeInt(int32(len(s)))\n\tf.buf = append(f.buf, s...)\n}\n\nfunc (f *framer) writeStringList(l []string) {\n\tf.writeShort(uint16(len(l)))\n\tfor _, s := range l {\n\t\tf.writeString(s)\n\t}\n}\n\nfunc (f *framer) writeUnset() {\n\t// Protocol version 4 specifies that bind variables do not require having a\n\t// value when executing a statement.   Bind variables without a value are\n\t// called 'unset'. The 'unset' bind variable is serialized as the int\n\t// value '-2' without following bytes.\n\tf.writeInt(-2)\n}\n\nfunc (f *framer) writeBytes(p []byte) {\n\t// TODO: handle null case correctly,\n\t//     [bytes]        A [int] n, followed by n bytes if n >= 0. If n < 0,\n\t//\t\t\t\t\t  no byte should follow and the value represented is `null`.\n\tif p == nil {\n\t\tf.writeInt(-1)\n\t} else {\n\t\tf.writeInt(int32(len(p)))\n\t\tf.buf = append(f.buf, p...)\n\t}\n}\n\nfunc (f *framer) writeShortBytes(p []byte) {\n\tf.writeShort(uint16(len(p)))\n\tf.buf = append(f.buf, p...)\n}\n\nfunc (f *framer) writeConsistency(cons Consistency) {\n\tf.writeShort(uint16(cons))\n}\n\nfunc (f *framer) writeStringMap(m map[string]string) {\n\tf.writeShort(uint16(len(m)))\n\tfor k, v := range m {\n\t\tf.writeString(k)\n\t\tf.writeString(v)\n\t}\n}\n\nfunc (f *framer) writeBytesMap(m map[string][]byte) {\n\tf.writeShort(uint16(len(m)))\n\tfor k, v := range m {\n\t\tf.writeString(k)\n\t\tf.writeBytes(v)\n\t}\n}\n"
        },
        {
          "name": "frame_test.go",
          "type": "blob",
          "size": 3.646484375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"os\"\n\t\"testing\"\n)\n\nfunc TestFuzzBugs(t *testing.T) {\n\t// these inputs are found using go-fuzz (https://github.com/dvyukov/go-fuzz)\n\t// and should cause a panic unless fixed.\n\ttests := [][]byte{\n\t\t[]byte(\"00000\\xa0000\"),\n\t\t[]byte(\"\\x8000\\x0e\\x00\\x00\\x00\\x000\"),\n\t\t[]byte(\"\\x8000\\x00\\x00\\x00\\x00\\t0000000000\"),\n\t\t[]byte(\"\\xa0\\xff\\x01\\xae\\xefqE\\xf2\\x1a\"),\n\t\t[]byte(\"\\x8200\\b\\x00\\x00\\x00c\\x00\\x00\\x00\\x02000\\x01\\x00\\x00\\x00\\x03\" +\n\t\t\t\"\\x00\\n0000000000\\x00\\x14000000\" +\n\t\t\t\"00000000000000\\x00\\x020000\" +\n\t\t\t\"\\x00\\a000000000\\x00\\x050000000\" +\n\t\t\t\"\\xff0000000000000000000\" +\n\t\t\t\"0000000\"),\n\t\t[]byte(\"\\x82\\xe600\\x00\\x00\\x00\\x000\"),\n\t\t[]byte(\"\\x8200\\b\\x00\\x00\\x00\\b0\\x00\\x00\\x00\\x040000\"),\n\t\t[]byte(\"\\x8200\\x00\\x00\\x00\\x00\\x100\\x00\\x00\\x12\\x00\\x00\\x0000000\" +\n\t\t\t\"00000\"),\n\t\t[]byte(\"\\x83000\\b\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x020000000\" +\n\t\t\t\"000000000\"),\n\t\t[]byte(\"\\x83000\\b\\x00\\x00\\x000\\x00\\x00\\x00\\x04\\x00\\x1000000\" +\n\t\t\t\"00000000000000e00000\" +\n\t\t\t\"000\\x800000000000000000\" +\n\t\t\t\"0000000000000\"),\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Logf(\"test %d input: %q\", i, test)\n\n\t\tr := bytes.NewReader(test)\n\t\thead, err := readHeader(r, make([]byte, 9))\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tframer := newFramer(nil, byte(head.version))\n\t\terr = framer.readFrame(r, &head)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tframe, err := framer.parseFrame()\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tt.Errorf(\"(%d) expected to fail for input % X\", i, test)\n\t\tt.Errorf(\"(%d) frame=%+#v\", i, frame)\n\t}\n}\n\nfunc TestFrameWriteTooLong(t *testing.T) {\n\tif os.Getenv(\"TRAVIS\") == \"true\" {\n\t\tt.Skip(\"skipping test in travis due to memory pressure with the race detecor\")\n\t}\n\n\tframer := newFramer(nil, 2)\n\n\tframer.writeHeader(0, opStartup, 1)\n\tframer.writeBytes(make([]byte, maxFrameSize+1))\n\terr := framer.finish()\n\tif err != ErrFrameTooBig {\n\t\tt.Fatalf(\"expected to get %v got %v\", ErrFrameTooBig, err)\n\t}\n}\n\nfunc TestFrameReadTooLong(t *testing.T) {\n\tif os.Getenv(\"TRAVIS\") == \"true\" {\n\t\tt.Skip(\"skipping test in travis due to memory pressure with the race detecor\")\n\t}\n\n\tr := &bytes.Buffer{}\n\tr.Write(make([]byte, maxFrameSize+1))\n\t// write a new header right after this frame to verify that we can read it\n\tr.Write([]byte{0x02, 0x00, 0x00, byte(opReady), 0x00, 0x00, 0x00, 0x00})\n\n\tframer := newFramer(nil, 2)\n\n\thead := frameHeader{\n\t\tversion: 2,\n\t\top:      opReady,\n\t\tlength:  r.Len() - 8,\n\t}\n\n\terr := framer.readFrame(r, &head)\n\tif err != ErrFrameTooBig {\n\t\tt.Fatalf(\"expected to get %v got %v\", ErrFrameTooBig, err)\n\t}\n\n\thead, err = readHeader(r, make([]byte, 8))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif head.op != opReady {\n\t\tt.Fatalf(\"expected to get header %v got %v\", opReady, head.op)\n\t}\n}\n"
        },
        {
          "name": "framer_bench_test.go",
          "type": "blob",
          "size": 1.7412109375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"compress/gzip\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"testing\"\n)\n\nfunc readGzipData(path string) ([]byte, error) {\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\tr, err := gzip.NewReader(f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer r.Close()\n\n\treturn ioutil.ReadAll(r)\n}\n\nfunc BenchmarkParseRowsFrame(b *testing.B) {\n\tdata, err := readGzipData(\"testdata/frames/bench_parse_result.gz\")\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tframer := &framer{\n\t\t\theader: &frameHeader{\n\t\t\t\tversion: protoVersion4 | 0x80,\n\t\t\t\top:      opResult,\n\t\t\t\tlength:  len(data),\n\t\t\t},\n\t\t\tbuf: data,\n\t\t}\n\n\t\t_, err = framer.parseFrame()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "fuzz.go",
          "type": "blob",
          "size": 1.4638671875,
          "content": "//go:build gofuzz\n// +build gofuzz\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport \"bytes\"\n\nfunc Fuzz(data []byte) int {\n\tvar bw bytes.Buffer\n\n\tr := bytes.NewReader(data)\n\n\thead, err := readHeader(r, make([]byte, 9))\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\tframer := newFramer(r, &bw, nil, byte(head.version))\n\terr = framer.readFrame(&head)\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\tframe, err := framer.parseFrame()\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\tif frame != nil {\n\t\treturn 1\n\t}\n\n\treturn 2\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.1953125,
          "content": "//\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\nmodule github.com/gocql/gocql\n\nrequire (\n\tgithub.com/bitly/go-hostpool v0.0.0-20171023180738-a3a6125de932 // indirect\n\tgithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869 // indirect\n\tgithub.com/golang/snappy v0.0.3\n\tgithub.com/hailocab/go-hostpool v0.0.0-20160125115350-e80d13ce29ed\n\tgithub.com/kr/pretty v0.1.0 // indirect\n\tgithub.com/stretchr/testify v1.3.0 // indirect\n\tgopkg.in/inf.v0 v0.9.1\n)\n\ngo 1.13\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 1.9677734375,
          "content": "github.com/bitly/go-hostpool v0.0.0-20171023180738-a3a6125de932 h1:mXoPYz/Ul5HYEDvkta6I8/rnYM5gSdSV2tJ6XbZuEtY=\ngithub.com/bitly/go-hostpool v0.0.0-20171023180738-a3a6125de932/go.mod h1:NOuUCSz6Q9T7+igc/hlvDOUdtWKryOrtFyIVABv/p7k=\ngithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869 h1:DDGfHa7BWjL4YnC6+E63dPcxHo2sUxDIu8g3QgEJdRY=\ngithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869/go.mod h1:Ekp36dRnpXw/yCqJaO+ZrUyxD+3VXMFFr56k5XYrpB4=\ngithub.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/golang/snappy v0.0.3 h1:fHPg5GQYlCeLIPB9BZqMVR5nR9A+IM5zcgeTdjMYmLA=\ngithub.com/golang/snappy v0.0.3/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/hailocab/go-hostpool v0.0.0-20160125115350-e80d13ce29ed h1:5upAirOpQc1Q53c0bnx2ufif5kANL7bfZWcc6VJWJd8=\ngithub.com/hailocab/go-hostpool v0.0.0-20160125115350-e80d13ce29ed/go.mod h1:tMWxXQ9wFIaZeTI9F+hmhFiGpFmhOHzyShyFUhRm0H4=\ngithub.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=\ngopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=\n"
        },
        {
          "name": "helpers.go",
          "type": "blob",
          "size": 11.7021484375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"math/big\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"gopkg.in/inf.v0\"\n)\n\ntype RowData struct {\n\tColumns []string\n\tValues  []interface{}\n}\n\nfunc goType(t TypeInfo) (reflect.Type, error) {\n\tswitch t.Type() {\n\tcase TypeVarchar, TypeAscii, TypeInet, TypeText:\n\t\treturn reflect.TypeOf(*new(string)), nil\n\tcase TypeBigInt, TypeCounter:\n\t\treturn reflect.TypeOf(*new(int64)), nil\n\tcase TypeTime:\n\t\treturn reflect.TypeOf(*new(time.Duration)), nil\n\tcase TypeTimestamp:\n\t\treturn reflect.TypeOf(*new(time.Time)), nil\n\tcase TypeBlob:\n\t\treturn reflect.TypeOf(*new([]byte)), nil\n\tcase TypeBoolean:\n\t\treturn reflect.TypeOf(*new(bool)), nil\n\tcase TypeFloat:\n\t\treturn reflect.TypeOf(*new(float32)), nil\n\tcase TypeDouble:\n\t\treturn reflect.TypeOf(*new(float64)), nil\n\tcase TypeInt:\n\t\treturn reflect.TypeOf(*new(int)), nil\n\tcase TypeSmallInt:\n\t\treturn reflect.TypeOf(*new(int16)), nil\n\tcase TypeTinyInt:\n\t\treturn reflect.TypeOf(*new(int8)), nil\n\tcase TypeDecimal:\n\t\treturn reflect.TypeOf(*new(*inf.Dec)), nil\n\tcase TypeUUID, TypeTimeUUID:\n\t\treturn reflect.TypeOf(*new(UUID)), nil\n\tcase TypeList, TypeSet:\n\t\telemType, err := goType(t.(CollectionType).Elem)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn reflect.SliceOf(elemType), nil\n\tcase TypeMap:\n\t\tkeyType, err := goType(t.(CollectionType).Key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalueType, err := goType(t.(CollectionType).Elem)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn reflect.MapOf(keyType, valueType), nil\n\tcase TypeVarint:\n\t\treturn reflect.TypeOf(*new(*big.Int)), nil\n\tcase TypeTuple:\n\t\t// what can we do here? all there is to do is to make a list of interface{}\n\t\ttuple := t.(TupleTypeInfo)\n\t\treturn reflect.TypeOf(make([]interface{}, len(tuple.Elems))), nil\n\tcase TypeUDT:\n\t\treturn reflect.TypeOf(make(map[string]interface{})), nil\n\tcase TypeDate:\n\t\treturn reflect.TypeOf(*new(time.Time)), nil\n\tcase TypeDuration:\n\t\treturn reflect.TypeOf(*new(Duration)), nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"cannot create Go type for unknown CQL type %s\", t)\n\t}\n}\n\nfunc dereference(i interface{}) interface{} {\n\treturn reflect.Indirect(reflect.ValueOf(i)).Interface()\n}\n\nfunc getCassandraBaseType(name string) Type {\n\tswitch name {\n\tcase \"ascii\":\n\t\treturn TypeAscii\n\tcase \"bigint\":\n\t\treturn TypeBigInt\n\tcase \"blob\":\n\t\treturn TypeBlob\n\tcase \"boolean\":\n\t\treturn TypeBoolean\n\tcase \"counter\":\n\t\treturn TypeCounter\n\tcase \"date\":\n\t\treturn TypeDate\n\tcase \"decimal\":\n\t\treturn TypeDecimal\n\tcase \"double\":\n\t\treturn TypeDouble\n\tcase \"duration\":\n\t\treturn TypeDuration\n\tcase \"float\":\n\t\treturn TypeFloat\n\tcase \"int\":\n\t\treturn TypeInt\n\tcase \"smallint\":\n\t\treturn TypeSmallInt\n\tcase \"tinyint\":\n\t\treturn TypeTinyInt\n\tcase \"time\":\n\t\treturn TypeTime\n\tcase \"timestamp\":\n\t\treturn TypeTimestamp\n\tcase \"uuid\":\n\t\treturn TypeUUID\n\tcase \"varchar\":\n\t\treturn TypeVarchar\n\tcase \"text\":\n\t\treturn TypeText\n\tcase \"varint\":\n\t\treturn TypeVarint\n\tcase \"timeuuid\":\n\t\treturn TypeTimeUUID\n\tcase \"inet\":\n\t\treturn TypeInet\n\tcase \"MapType\":\n\t\treturn TypeMap\n\tcase \"ListType\":\n\t\treturn TypeList\n\tcase \"SetType\":\n\t\treturn TypeSet\n\tcase \"TupleType\":\n\t\treturn TypeTuple\n\tdefault:\n\t\treturn TypeCustom\n\t}\n}\n\nfunc getCassandraType(name string, logger StdLogger) TypeInfo {\n\tif strings.HasPrefix(name, \"frozen<\") {\n\t\treturn getCassandraType(strings.TrimPrefix(name[:len(name)-1], \"frozen<\"), logger)\n\t} else if strings.HasPrefix(name, \"set<\") {\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{typ: TypeSet},\n\t\t\tElem:       getCassandraType(strings.TrimPrefix(name[:len(name)-1], \"set<\"), logger),\n\t\t}\n\t} else if strings.HasPrefix(name, \"list<\") {\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\tElem:       getCassandraType(strings.TrimPrefix(name[:len(name)-1], \"list<\"), logger),\n\t\t}\n\t} else if strings.HasPrefix(name, \"map<\") {\n\t\tnames := splitCompositeTypes(strings.TrimPrefix(name[:len(name)-1], \"map<\"))\n\t\tif len(names) != 2 {\n\t\t\tlogger.Printf(\"Error parsing map type, it has %d subelements, expecting 2\\n\", len(names))\n\t\t\treturn NativeType{\n\t\t\t\ttyp: TypeCustom,\n\t\t\t}\n\t\t}\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{typ: TypeMap},\n\t\t\tKey:        getCassandraType(names[0], logger),\n\t\t\tElem:       getCassandraType(names[1], logger),\n\t\t}\n\t} else if strings.HasPrefix(name, \"tuple<\") {\n\t\tnames := splitCompositeTypes(strings.TrimPrefix(name[:len(name)-1], \"tuple<\"))\n\t\ttypes := make([]TypeInfo, len(names))\n\n\t\tfor i, name := range names {\n\t\t\ttypes[i] = getCassandraType(name, logger)\n\t\t}\n\n\t\treturn TupleTypeInfo{\n\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\tElems:      types,\n\t\t}\n\t} else {\n\t\treturn NativeType{\n\t\t\ttyp: getCassandraBaseType(name),\n\t\t}\n\t}\n}\n\nfunc splitCompositeTypes(name string) []string {\n\tif !strings.Contains(name, \"<\") {\n\t\treturn strings.Split(name, \", \")\n\t}\n\tvar parts []string\n\tlessCount := 0\n\tsegment := \"\"\n\tfor _, char := range name {\n\t\tif char == ',' && lessCount == 0 {\n\t\t\tif segment != \"\" {\n\t\t\t\tparts = append(parts, strings.TrimSpace(segment))\n\t\t\t}\n\t\t\tsegment = \"\"\n\t\t\tcontinue\n\t\t}\n\t\tsegment += string(char)\n\t\tif char == '<' {\n\t\t\tlessCount++\n\t\t} else if char == '>' {\n\t\t\tlessCount--\n\t\t}\n\t}\n\tif segment != \"\" {\n\t\tparts = append(parts, strings.TrimSpace(segment))\n\t}\n\treturn parts\n}\n\nfunc apacheToCassandraType(t string) string {\n\tt = strings.Replace(t, apacheCassandraTypePrefix, \"\", -1)\n\tt = strings.Replace(t, \"(\", \"<\", -1)\n\tt = strings.Replace(t, \")\", \">\", -1)\n\ttypes := strings.FieldsFunc(t, func(r rune) bool {\n\t\treturn r == '<' || r == '>' || r == ','\n\t})\n\tfor _, typ := range types {\n\t\tt = strings.Replace(t, typ, getApacheCassandraType(typ).String(), -1)\n\t}\n\t// This is done so it exactly matches what Cassandra returns\n\treturn strings.Replace(t, \",\", \", \", -1)\n}\n\nfunc getApacheCassandraType(class string) Type {\n\tswitch strings.TrimPrefix(class, apacheCassandraTypePrefix) {\n\tcase \"AsciiType\":\n\t\treturn TypeAscii\n\tcase \"LongType\":\n\t\treturn TypeBigInt\n\tcase \"BytesType\":\n\t\treturn TypeBlob\n\tcase \"BooleanType\":\n\t\treturn TypeBoolean\n\tcase \"CounterColumnType\":\n\t\treturn TypeCounter\n\tcase \"DecimalType\":\n\t\treturn TypeDecimal\n\tcase \"DoubleType\":\n\t\treturn TypeDouble\n\tcase \"FloatType\":\n\t\treturn TypeFloat\n\tcase \"Int32Type\":\n\t\treturn TypeInt\n\tcase \"ShortType\":\n\t\treturn TypeSmallInt\n\tcase \"ByteType\":\n\t\treturn TypeTinyInt\n\tcase \"TimeType\":\n\t\treturn TypeTime\n\tcase \"DateType\", \"TimestampType\":\n\t\treturn TypeTimestamp\n\tcase \"UUIDType\", \"LexicalUUIDType\":\n\t\treturn TypeUUID\n\tcase \"UTF8Type\":\n\t\treturn TypeVarchar\n\tcase \"IntegerType\":\n\t\treturn TypeVarint\n\tcase \"TimeUUIDType\":\n\t\treturn TypeTimeUUID\n\tcase \"InetAddressType\":\n\t\treturn TypeInet\n\tcase \"MapType\":\n\t\treturn TypeMap\n\tcase \"ListType\":\n\t\treturn TypeList\n\tcase \"SetType\":\n\t\treturn TypeSet\n\tcase \"TupleType\":\n\t\treturn TypeTuple\n\tcase \"DurationType\":\n\t\treturn TypeDuration\n\tdefault:\n\t\treturn TypeCustom\n\t}\n}\n\nfunc (r *RowData) rowMap(m map[string]interface{}) {\n\tfor i, column := range r.Columns {\n\t\tval := dereference(r.Values[i])\n\t\tif valVal := reflect.ValueOf(val); valVal.Kind() == reflect.Slice {\n\t\t\tvalCopy := reflect.MakeSlice(valVal.Type(), valVal.Len(), valVal.Cap())\n\t\t\treflect.Copy(valCopy, valVal)\n\t\t\tm[column] = valCopy.Interface()\n\t\t} else {\n\t\t\tm[column] = val\n\t\t}\n\t}\n}\n\n// TupeColumnName will return the column name of a tuple value in a column named\n// c at index n. It should be used if a specific element within a tuple is needed\n// to be extracted from a map returned from SliceMap or MapScan.\nfunc TupleColumnName(c string, n int) string {\n\treturn fmt.Sprintf(\"%s[%d]\", c, n)\n}\n\nfunc (iter *Iter) RowData() (RowData, error) {\n\tif iter.err != nil {\n\t\treturn RowData{}, iter.err\n\t}\n\n\tcolumns := make([]string, 0, len(iter.Columns()))\n\tvalues := make([]interface{}, 0, len(iter.Columns()))\n\n\tfor _, column := range iter.Columns() {\n\t\tif c, ok := column.TypeInfo.(TupleTypeInfo); !ok {\n\t\t\tval, err := column.TypeInfo.NewWithError()\n\t\t\tif err != nil {\n\t\t\t\treturn RowData{}, err\n\t\t\t}\n\t\t\tcolumns = append(columns, column.Name)\n\t\t\tvalues = append(values, val)\n\t\t} else {\n\t\t\tfor i, elem := range c.Elems {\n\t\t\t\tcolumns = append(columns, TupleColumnName(column.Name, i))\n\t\t\t\tval, err := elem.NewWithError()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn RowData{}, err\n\t\t\t\t}\n\t\t\t\tvalues = append(values, val)\n\t\t\t}\n\t\t}\n\t}\n\n\trowData := RowData{\n\t\tColumns: columns,\n\t\tValues:  values,\n\t}\n\n\treturn rowData, nil\n}\n\n// TODO(zariel): is it worth exporting this?\nfunc (iter *Iter) rowMap() (map[string]interface{}, error) {\n\tif iter.err != nil {\n\t\treturn nil, iter.err\n\t}\n\n\trowData, _ := iter.RowData()\n\titer.Scan(rowData.Values...)\n\tm := make(map[string]interface{}, len(rowData.Columns))\n\trowData.rowMap(m)\n\treturn m, nil\n}\n\n// SliceMap is a helper function to make the API easier to use\n// returns the data from the query in the form of []map[string]interface{}\nfunc (iter *Iter) SliceMap() ([]map[string]interface{}, error) {\n\tif iter.err != nil {\n\t\treturn nil, iter.err\n\t}\n\n\t// Not checking for the error because we just did\n\trowData, _ := iter.RowData()\n\tdataToReturn := make([]map[string]interface{}, 0)\n\tfor iter.Scan(rowData.Values...) {\n\t\tm := make(map[string]interface{}, len(rowData.Columns))\n\t\trowData.rowMap(m)\n\t\tdataToReturn = append(dataToReturn, m)\n\t}\n\tif iter.err != nil {\n\t\treturn nil, iter.err\n\t}\n\treturn dataToReturn, nil\n}\n\n// MapScan takes a map[string]interface{} and populates it with a row\n// that is returned from cassandra.\n//\n// Each call to MapScan() must be called with a new map object.\n// During the call to MapScan() any pointers in the existing map\n// are replaced with non pointer types before the call returns\n//\n//\titer := session.Query(`SELECT * FROM mytable`).Iter()\n//\tfor {\n//\t\t// New map each iteration\n//\t\trow := make(map[string]interface{})\n//\t\tif !iter.MapScan(row) {\n//\t\t\tbreak\n//\t\t}\n//\t\t// Do things with row\n//\t\tif fullname, ok := row[\"fullname\"]; ok {\n//\t\t\tfmt.Printf(\"Full Name: %s\\n\", fullname)\n//\t\t}\n//\t}\n//\n// You can also pass pointers in the map before each call\n//\n//\tvar fullName FullName // Implements gocql.Unmarshaler and gocql.Marshaler interfaces\n//\tvar address net.IP\n//\tvar age int\n//\titer := session.Query(`SELECT * FROM scan_map_table`).Iter()\n//\tfor {\n//\t\t// New map each iteration\n//\t\trow := map[string]interface{}{\n//\t\t\t\"fullname\": &fullName,\n//\t\t\t\"age\":      &age,\n//\t\t\t\"address\":  &address,\n//\t\t}\n//\t\tif !iter.MapScan(row) {\n//\t\t\tbreak\n//\t\t}\n//\t\tfmt.Printf(\"First: %s Age: %d Address: %q\\n\", fullName.FirstName, age, address)\n//\t}\nfunc (iter *Iter) MapScan(m map[string]interface{}) bool {\n\tif iter.err != nil {\n\t\treturn false\n\t}\n\n\t// Not checking for the error because we just did\n\trowData, _ := iter.RowData()\n\n\tfor i, col := range rowData.Columns {\n\t\tif dest, ok := m[col]; ok {\n\t\t\trowData.Values[i] = dest\n\t\t}\n\t}\n\n\tif iter.Scan(rowData.Values...) {\n\t\trowData.rowMap(m)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc copyBytes(p []byte) []byte {\n\tb := make([]byte, len(p))\n\tcopy(b, p)\n\treturn b\n}\n\nvar failDNS = false\n\nfunc LookupIP(host string) ([]net.IP, error) {\n\tif failDNS {\n\t\treturn nil, &net.DNSError{}\n\t}\n\treturn net.LookupIP(host)\n\n}\n"
        },
        {
          "name": "helpers_test.go",
          "type": "blob",
          "size": 6.1123046875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestGetCassandraType_Set(t *testing.T) {\n\ttyp := getCassandraType(\"set<text>\", &defaultLogger{})\n\tset, ok := typ.(CollectionType)\n\tif !ok {\n\t\tt.Fatalf(\"expected CollectionType got %T\", typ)\n\t} else if set.typ != TypeSet {\n\t\tt.Fatalf(\"expected type %v got %v\", TypeSet, set.typ)\n\t}\n\n\tinner, ok := set.Elem.(NativeType)\n\tif !ok {\n\t\tt.Fatalf(\"expected to get NativeType got %T\", set.Elem)\n\t} else if inner.typ != TypeText {\n\t\tt.Fatalf(\"expected to get %v got %v for set value\", TypeText, set.typ)\n\t}\n}\n\nfunc TestGetCassandraType(t *testing.T) {\n\ttests := []struct {\n\t\tinput string\n\t\texp   TypeInfo\n\t}{\n\t\t{\n\t\t\t\"set<text>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeSet},\n\n\t\t\t\tElem: NativeType{typ: TypeText},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"map<text, varchar>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeMap},\n\n\t\t\t\tKey:  NativeType{typ: TypeText},\n\t\t\t\tElem: NativeType{typ: TypeVarchar},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"list<int>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\tElem:       NativeType{typ: TypeInt},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"tuple<int, int, text>\", TupleTypeInfo{\n\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\tNativeType{typ: TypeText},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"frozen<map<text, frozen<list<frozen<tuple<int, int>>>>>>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeMap},\n\n\t\t\t\tKey: NativeType{typ: TypeText},\n\t\t\t\tElem: CollectionType{\n\t\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\t\tElem: TupleTypeInfo{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"frozen<tuple<frozen<tuple<text, frozen<list<frozen<tuple<int, int>>>>>>, frozen<tuple<text, frozen<list<frozen<tuple<int, int>>>>>>,  frozen<map<text, frozen<list<frozen<tuple<int, int>>>>>>>>\",\n\t\t\tTupleTypeInfo{\n\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\tTupleTypeInfo{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\tNativeType{typ: TypeText},\n\t\t\t\t\t\t\tCollectionType{\n\t\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\t\t\t\t\tElem: TupleTypeInfo{\n\t\t\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tTupleTypeInfo{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\tNativeType{typ: TypeText},\n\t\t\t\t\t\t\tCollectionType{\n\t\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\t\t\t\t\tElem: TupleTypeInfo{\n\t\t\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tCollectionType{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeMap},\n\t\t\t\t\t\tKey:        NativeType{typ: TypeText},\n\t\t\t\t\t\tElem: CollectionType{\n\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\t\t\t\tElem: TupleTypeInfo{\n\t\t\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\t\t\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"frozen<tuple<frozen<tuple<int, int>>, int, frozen<tuple<int, int>>>>\", TupleTypeInfo{\n\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\tTupleTypeInfo{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\tTupleTypeInfo{\n\t\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"frozen<map<frozen<tuple<int, int>>, int>>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeMap},\n\n\t\t\t\tKey: TupleTypeInfo{\n\t\t\t\t\tNativeType: NativeType{typ: TypeTuple},\n\n\t\t\t\t\tElems: []TypeInfo{\n\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t\tNativeType{typ: TypeInt},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tElem: NativeType{typ: TypeInt},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"set<smallint>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeSet},\n\t\t\t\tElem:       NativeType{typ: TypeSmallInt},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"list<tinyint>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\tElem:       NativeType{typ: TypeTinyInt},\n\t\t\t},\n\t\t},\n\t\t{\"smallint\", NativeType{typ: TypeSmallInt}},\n\t\t{\"tinyint\", NativeType{typ: TypeTinyInt}},\n\t\t{\"duration\", NativeType{typ: TypeDuration}},\n\t\t{\"date\", NativeType{typ: TypeDate}},\n\t\t{\n\t\t\t\"list<date>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeList},\n\t\t\t\tElem:       NativeType{typ: TypeDate},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"set<duration>\", CollectionType{\n\t\t\t\tNativeType: NativeType{typ: TypeSet},\n\t\t\t\tElem:       NativeType{typ: TypeDuration},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.input, func(t *testing.T) {\n\t\t\tgot := getCassandraType(test.input, &defaultLogger{})\n\n\t\t\t// TODO(zariel): define an equal method on the types?\n\t\t\tif !reflect.DeepEqual(got, test.exp) {\n\t\t\t\tt.Fatalf(\"expected %v got %v\", test.exp, got)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "host_source.go",
          "type": "blob",
          "size": 21.3359375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n)\n\nvar ErrCannotFindHost = errors.New(\"cannot find host\")\nvar ErrHostAlreadyExists = errors.New(\"host already exists\")\n\ntype nodeState int32\n\nfunc (n nodeState) String() string {\n\tif n == NodeUp {\n\t\treturn \"UP\"\n\t} else if n == NodeDown {\n\t\treturn \"DOWN\"\n\t}\n\treturn fmt.Sprintf(\"UNKNOWN_%d\", n)\n}\n\nconst (\n\tNodeUp nodeState = iota\n\tNodeDown\n)\n\ntype cassVersion struct {\n\tMajor, Minor, Patch int\n}\n\nfunc (c *cassVersion) Set(v string) error {\n\tif v == \"\" {\n\t\treturn nil\n\t}\n\n\treturn c.UnmarshalCQL(nil, []byte(v))\n}\n\nfunc (c *cassVersion) UnmarshalCQL(info TypeInfo, data []byte) error {\n\treturn c.unmarshal(data)\n}\n\nfunc (c *cassVersion) unmarshal(data []byte) error {\n\tversion := strings.TrimSuffix(string(data), \"-SNAPSHOT\")\n\tversion = strings.TrimPrefix(version, \"v\")\n\tv := strings.Split(version, \".\")\n\n\tif len(v) < 2 {\n\t\treturn fmt.Errorf(\"invalid version string: %s\", data)\n\t}\n\n\tvar err error\n\tc.Major, err = strconv.Atoi(v[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid major version %v: %v\", v[0], err)\n\t}\n\n\tc.Minor, err = strconv.Atoi(v[1])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid minor version %v: %v\", v[1], err)\n\t}\n\n\tif len(v) > 2 {\n\t\tc.Patch, err = strconv.Atoi(v[2])\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"invalid patch version %v: %v\", v[2], err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c cassVersion) Before(major, minor, patch int) bool {\n\t// We're comparing us (cassVersion) with the provided version (major, minor, patch)\n\t// We return true if our version is lower (comes before) than the provided one.\n\tif c.Major < major {\n\t\treturn true\n\t} else if c.Major == major {\n\t\tif c.Minor < minor {\n\t\t\treturn true\n\t\t} else if c.Minor == minor && c.Patch < patch {\n\t\t\treturn true\n\t\t}\n\n\t}\n\treturn false\n}\n\nfunc (c cassVersion) AtLeast(major, minor, patch int) bool {\n\treturn !c.Before(major, minor, patch)\n}\n\nfunc (c cassVersion) String() string {\n\treturn fmt.Sprintf(\"v%d.%d.%d\", c.Major, c.Minor, c.Patch)\n}\n\nfunc (c cassVersion) nodeUpDelay() time.Duration {\n\tif c.Major >= 2 && c.Minor >= 2 {\n\t\t// CASSANDRA-8236\n\t\treturn 0\n\t}\n\n\treturn 10 * time.Second\n}\n\ntype HostInfo struct {\n\t// TODO(zariel): reduce locking maybe, not all values will change, but to ensure\n\t// that we are thread safe use a mutex to access all fields.\n\tmu               sync.RWMutex\n\thostname         string\n\tpeer             net.IP\n\tbroadcastAddress net.IP\n\tlistenAddress    net.IP\n\trpcAddress       net.IP\n\tpreferredIP      net.IP\n\tconnectAddress   net.IP\n\tport             int\n\tdataCenter       string\n\track             string\n\thostId           string\n\tworkload         string\n\tgraph            bool\n\tdseVersion       string\n\tpartitioner      string\n\tclusterName      string\n\tversion          cassVersion\n\tstate            nodeState\n\tschemaVersion    string\n\ttokens           []string\n}\n\nfunc (h *HostInfo) Equal(host *HostInfo) bool {\n\tif h == host {\n\t\t// prevent rlock reentry\n\t\treturn true\n\t}\n\n\treturn h.ConnectAddress().Equal(host.ConnectAddress())\n}\n\nfunc (h *HostInfo) Peer() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.peer\n}\n\nfunc (h *HostInfo) invalidConnectAddr() bool {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\taddr, _ := h.connectAddressLocked()\n\treturn !validIpAddr(addr)\n}\n\nfunc validIpAddr(addr net.IP) bool {\n\treturn addr != nil && !addr.IsUnspecified()\n}\n\nfunc (h *HostInfo) connectAddressLocked() (net.IP, string) {\n\tif validIpAddr(h.connectAddress) {\n\t\treturn h.connectAddress, \"connect_address\"\n\t} else if validIpAddr(h.rpcAddress) {\n\t\treturn h.rpcAddress, \"rpc_adress\"\n\t} else if validIpAddr(h.preferredIP) {\n\t\t// where does perferred_ip get set?\n\t\treturn h.preferredIP, \"preferred_ip\"\n\t} else if validIpAddr(h.broadcastAddress) {\n\t\treturn h.broadcastAddress, \"broadcast_address\"\n\t} else if validIpAddr(h.peer) {\n\t\treturn h.peer, \"peer\"\n\t}\n\treturn net.IPv4zero, \"invalid\"\n}\n\n// nodeToNodeAddress returns address broadcasted between node to nodes.\n// It's either `broadcast_address` if host info is read from system.local or `peer` if read from system.peers.\n// This IP address is also part of CQL Event emitted on topology/status changes,\n// but does not uniquely identify the node in case multiple nodes use the same IP address.\nfunc (h *HostInfo) nodeToNodeAddress() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\n\tif validIpAddr(h.broadcastAddress) {\n\t\treturn h.broadcastAddress\n\t} else if validIpAddr(h.peer) {\n\t\treturn h.peer\n\t}\n\treturn net.IPv4zero\n}\n\n// Returns the address that should be used to connect to the host.\n// If you wish to override this, use an AddressTranslator or\n// use a HostFilter to SetConnectAddress()\nfunc (h *HostInfo) ConnectAddress() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\n\tif addr, _ := h.connectAddressLocked(); validIpAddr(addr) {\n\t\treturn addr\n\t}\n\tpanic(fmt.Sprintf(\"no valid connect address for host: %v. Is your cluster configured correctly?\", h))\n}\n\nfunc (h *HostInfo) SetConnectAddress(address net.IP) *HostInfo {\n\t// TODO(zariel): should this not be exported?\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.connectAddress = address\n\treturn h\n}\n\nfunc (h *HostInfo) BroadcastAddress() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.broadcastAddress\n}\n\nfunc (h *HostInfo) ListenAddress() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.listenAddress\n}\n\nfunc (h *HostInfo) RPCAddress() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.rpcAddress\n}\n\nfunc (h *HostInfo) PreferredIP() net.IP {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.preferredIP\n}\n\nfunc (h *HostInfo) DataCenter() string {\n\th.mu.RLock()\n\tdc := h.dataCenter\n\th.mu.RUnlock()\n\treturn dc\n}\n\nfunc (h *HostInfo) Rack() string {\n\th.mu.RLock()\n\track := h.rack\n\th.mu.RUnlock()\n\treturn rack\n}\n\nfunc (h *HostInfo) HostID() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.hostId\n}\n\nfunc (h *HostInfo) SetHostID(hostID string) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.hostId = hostID\n}\n\nfunc (h *HostInfo) WorkLoad() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.workload\n}\n\nfunc (h *HostInfo) Graph() bool {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.graph\n}\n\nfunc (h *HostInfo) DSEVersion() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.dseVersion\n}\n\nfunc (h *HostInfo) Partitioner() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.partitioner\n}\n\nfunc (h *HostInfo) ClusterName() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.clusterName\n}\n\nfunc (h *HostInfo) Version() cassVersion {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.version\n}\n\nfunc (h *HostInfo) State() nodeState {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.state\n}\n\nfunc (h *HostInfo) setState(state nodeState) *HostInfo {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.state = state\n\treturn h\n}\n\nfunc (h *HostInfo) Tokens() []string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.tokens\n}\n\nfunc (h *HostInfo) Port() int {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.port\n}\n\nfunc (h *HostInfo) update(from *HostInfo) {\n\tif h == from {\n\t\treturn\n\t}\n\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tfrom.mu.RLock()\n\tdefer from.mu.RUnlock()\n\n\t// autogenerated do not update\n\tif h.peer == nil {\n\t\th.peer = from.peer\n\t}\n\tif h.broadcastAddress == nil {\n\t\th.broadcastAddress = from.broadcastAddress\n\t}\n\tif h.listenAddress == nil {\n\t\th.listenAddress = from.listenAddress\n\t}\n\tif h.rpcAddress == nil {\n\t\th.rpcAddress = from.rpcAddress\n\t}\n\tif h.preferredIP == nil {\n\t\th.preferredIP = from.preferredIP\n\t}\n\tif h.connectAddress == nil {\n\t\th.connectAddress = from.connectAddress\n\t}\n\tif h.port == 0 {\n\t\th.port = from.port\n\t}\n\tif h.dataCenter == \"\" {\n\t\th.dataCenter = from.dataCenter\n\t}\n\tif h.rack == \"\" {\n\t\th.rack = from.rack\n\t}\n\tif h.hostId == \"\" {\n\t\th.hostId = from.hostId\n\t}\n\tif h.workload == \"\" {\n\t\th.workload = from.workload\n\t}\n\tif h.dseVersion == \"\" {\n\t\th.dseVersion = from.dseVersion\n\t}\n\tif h.partitioner == \"\" {\n\t\th.partitioner = from.partitioner\n\t}\n\tif h.clusterName == \"\" {\n\t\th.clusterName = from.clusterName\n\t}\n\tif h.version == (cassVersion{}) {\n\t\th.version = from.version\n\t}\n\tif h.tokens == nil {\n\t\th.tokens = from.tokens\n\t}\n}\n\nfunc (h *HostInfo) IsUp() bool {\n\treturn h != nil && h.State() == NodeUp\n}\n\nfunc (h *HostInfo) HostnameAndPort() string {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\tif h.hostname == \"\" {\n\t\taddr, _ := h.connectAddressLocked()\n\t\th.hostname = addr.String()\n\t}\n\treturn net.JoinHostPort(h.hostname, strconv.Itoa(h.port))\n}\n\nfunc (h *HostInfo) ConnectAddressAndPort() string {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\taddr, _ := h.connectAddressLocked()\n\treturn net.JoinHostPort(addr.String(), strconv.Itoa(h.port))\n}\n\nfunc (h *HostInfo) String() string {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\n\tconnectAddr, source := h.connectAddressLocked()\n\treturn fmt.Sprintf(\"[HostInfo hostname=%q connectAddress=%q peer=%q rpc_address=%q broadcast_address=%q \"+\n\t\t\"preferred_ip=%q connect_addr=%q connect_addr_source=%q \"+\n\t\t\"port=%d data_centre=%q rack=%q host_id=%q version=%q state=%s num_tokens=%d]\",\n\t\th.hostname, h.connectAddress, h.peer, h.rpcAddress, h.broadcastAddress, h.preferredIP,\n\t\tconnectAddr, source,\n\t\th.port, h.dataCenter, h.rack, h.hostId, h.version, h.state, len(h.tokens))\n}\n\n// Polls system.peers at a specific interval to find new hosts\ntype ringDescriber struct {\n\tsession         *Session\n\tmu              sync.Mutex\n\tprevHosts       []*HostInfo\n\tprevPartitioner string\n}\n\n// Returns true if we are using system_schema.keyspaces instead of system.schema_keyspaces\nfunc checkSystemSchema(control *controlConn) (bool, error) {\n\titer := control.query(\"SELECT * FROM system_schema.keyspaces\")\n\tif err := iter.err; err != nil {\n\t\tif errf, ok := err.(*errorFrame); ok {\n\t\t\tif errf.code == ErrCodeSyntax {\n\t\t\t\treturn false, nil\n\t\t\t}\n\t\t}\n\n\t\treturn false, err\n\t}\n\n\treturn true, nil\n}\n\n// Given a map that represents a row from either system.local or system.peers\n// return as much information as we can in *HostInfo\nfunc (s *Session) hostInfoFromMap(row map[string]interface{}, host *HostInfo) (*HostInfo, error) {\n\tconst assertErrorMsg = \"Assertion failed for %s\"\n\tvar ok bool\n\n\t// Default to our connected port if the cluster doesn't have port information\n\tfor key, value := range row {\n\t\tswitch key {\n\t\tcase \"data_center\":\n\t\t\thost.dataCenter, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"data_center\")\n\t\t\t}\n\t\tcase \"rack\":\n\t\t\thost.rack, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"rack\")\n\t\t\t}\n\t\tcase \"host_id\":\n\t\t\thostId, ok := value.(UUID)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"host_id\")\n\t\t\t}\n\t\t\thost.hostId = hostId.String()\n\t\tcase \"release_version\":\n\t\t\tversion, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"release_version\")\n\t\t\t}\n\t\t\thost.version.Set(version)\n\t\tcase \"peer\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"peer\")\n\t\t\t}\n\t\t\thost.peer = net.ParseIP(ip)\n\t\tcase \"cluster_name\":\n\t\t\thost.clusterName, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"cluster_name\")\n\t\t\t}\n\t\tcase \"partitioner\":\n\t\t\thost.partitioner, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"partitioner\")\n\t\t\t}\n\t\tcase \"broadcast_address\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"broadcast_address\")\n\t\t\t}\n\t\t\thost.broadcastAddress = net.ParseIP(ip)\n\t\tcase \"preferred_ip\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"preferred_ip\")\n\t\t\t}\n\t\t\thost.preferredIP = net.ParseIP(ip)\n\t\tcase \"rpc_address\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"rpc_address\")\n\t\t\t}\n\t\t\thost.rpcAddress = net.ParseIP(ip)\n\t\tcase \"native_address\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"native_address\")\n\t\t\t}\n\t\t\thost.rpcAddress = net.ParseIP(ip)\n\t\tcase \"listen_address\":\n\t\t\tip, ok := value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"listen_address\")\n\t\t\t}\n\t\t\thost.listenAddress = net.ParseIP(ip)\n\t\tcase \"native_port\":\n\t\t\tnative_port, ok := value.(int)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"native_port\")\n\t\t\t}\n\t\t\thost.port = native_port\n\t\tcase \"workload\":\n\t\t\thost.workload, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"workload\")\n\t\t\t}\n\t\tcase \"graph\":\n\t\t\thost.graph, ok = value.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"graph\")\n\t\t\t}\n\t\tcase \"tokens\":\n\t\t\thost.tokens, ok = value.([]string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"tokens\")\n\t\t\t}\n\t\tcase \"dse_version\":\n\t\t\thost.dseVersion, ok = value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"dse_version\")\n\t\t\t}\n\t\tcase \"schema_version\":\n\t\t\tschemaVersion, ok := value.(UUID)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(assertErrorMsg, \"schema_version\")\n\t\t\t}\n\t\t\thost.schemaVersion = schemaVersion.String()\n\t\t}\n\t\t// TODO(thrawn01): Add 'port'? once CASSANDRA-7544 is complete\n\t\t// Not sure what the port field will be called until the JIRA issue is complete\n\t}\n\n\tip, port := s.cfg.translateAddressPort(host.ConnectAddress(), host.port)\n\thost.connectAddress = ip\n\thost.port = port\n\n\treturn host, nil\n}\n\nfunc (s *Session) hostInfoFromIter(iter *Iter, connectAddress net.IP, defaultPort int) (*HostInfo, error) {\n\trows, err := iter.SliceMap()\n\tif err != nil {\n\t\t// TODO(zariel): make typed error\n\t\treturn nil, err\n\t}\n\n\tif len(rows) == 0 {\n\t\treturn nil, errors.New(\"query returned 0 rows\")\n\t}\n\n\thost, err := s.hostInfoFromMap(rows[0], &HostInfo{connectAddress: connectAddress, port: defaultPort})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn host, nil\n}\n\n// Ask the control node for the local host info\nfunc (r *ringDescriber) getLocalHostInfo() (*HostInfo, error) {\n\tif r.session.control == nil {\n\t\treturn nil, errNoControl\n\t}\n\n\titer := r.session.control.withConnHost(func(ch *connHost) *Iter {\n\t\treturn ch.conn.querySystemLocal(context.TODO())\n\t})\n\n\tif iter == nil {\n\t\treturn nil, errNoControl\n\t}\n\n\thost, err := r.session.hostInfoFromIter(iter, nil, r.session.cfg.Port)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not retrieve local host info: %w\", err)\n\t}\n\treturn host, nil\n}\n\n// Ask the control node for host info on all it's known peers\nfunc (r *ringDescriber) getClusterPeerInfo(localHost *HostInfo) ([]*HostInfo, error) {\n\tif r.session.control == nil {\n\t\treturn nil, errNoControl\n\t}\n\n\tvar peers []*HostInfo\n\titer := r.session.control.withConnHost(func(ch *connHost) *Iter {\n\t\treturn ch.conn.querySystemPeers(context.TODO(), localHost.version)\n\t})\n\n\tif iter == nil {\n\t\treturn nil, errNoControl\n\t}\n\n\trows, err := iter.SliceMap()\n\tif err != nil {\n\t\t// TODO(zariel): make typed error\n\t\treturn nil, fmt.Errorf(\"unable to fetch peer host info: %s\", err)\n\t}\n\n\tfor _, row := range rows {\n\t\t// extract all available info about the peer\n\t\thost, err := r.session.hostInfoFromMap(row, &HostInfo{port: r.session.cfg.Port})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if !isValidPeer(host) {\n\t\t\t// If it's not a valid peer\n\t\t\tr.session.logger.Printf(\"Found invalid peer '%s' \"+\n\t\t\t\t\"Likely due to a gossip or snitch issue, this host will be ignored\", host)\n\t\t\tcontinue\n\t\t}\n\n\t\tpeers = append(peers, host)\n\t}\n\n\treturn peers, nil\n}\n\n// Return true if the host is a valid peer\nfunc isValidPeer(host *HostInfo) bool {\n\treturn !(len(host.RPCAddress()) == 0 ||\n\t\thost.hostId == \"\" ||\n\t\thost.dataCenter == \"\" ||\n\t\thost.rack == \"\" ||\n\t\tlen(host.tokens) == 0)\n}\n\n// GetHosts returns a list of hosts found via queries to system.local and system.peers\nfunc (r *ringDescriber) GetHosts() ([]*HostInfo, string, error) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tlocalHost, err := r.getLocalHostInfo()\n\tif err != nil {\n\t\treturn r.prevHosts, r.prevPartitioner, err\n\t}\n\n\tpeerHosts, err := r.getClusterPeerInfo(localHost)\n\tif err != nil {\n\t\treturn r.prevHosts, r.prevPartitioner, err\n\t}\n\n\thosts := append([]*HostInfo{localHost}, peerHosts...)\n\tvar partitioner string\n\tif len(hosts) > 0 {\n\t\tpartitioner = hosts[0].Partitioner()\n\t}\n\n\treturn hosts, partitioner, nil\n}\n\n// debounceRingRefresh submits a ring refresh request to the ring refresh debouncer.\nfunc (s *Session) debounceRingRefresh() {\n\ts.ringRefresher.debounce()\n}\n\n// refreshRing executes a ring refresh immediately and cancels pending debounce ring refresh requests.\nfunc (s *Session) refreshRing() error {\n\terr, ok := <-s.ringRefresher.refreshNow()\n\tif !ok {\n\t\treturn errors.New(\"could not refresh ring because stop was requested\")\n\t}\n\n\treturn err\n}\n\nfunc refreshRing(r *ringDescriber) error {\n\thosts, partitioner, err := r.GetHosts()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprevHosts := r.session.ring.currentHosts()\n\n\tfor _, h := range hosts {\n\t\tif r.session.cfg.filterHost(h) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif host, ok := r.session.ring.addHostIfMissing(h); !ok {\n\t\t\tr.session.startPoolFill(h)\n\t\t} else {\n\t\t\t// host (by hostID) already exists; determine if IP has changed\n\t\t\tnewHostID := h.HostID()\n\t\t\texisting, ok := prevHosts[newHostID]\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"get existing host=%s from prevHosts: %w\", h, ErrCannotFindHost)\n\t\t\t}\n\t\t\tif h.connectAddress.Equal(existing.connectAddress) && h.nodeToNodeAddress().Equal(existing.nodeToNodeAddress()) {\n\t\t\t\t// no host IP change\n\t\t\t\thost.update(h)\n\t\t\t} else {\n\t\t\t\t// host IP has changed\n\t\t\t\t// remove old HostInfo (w/old IP)\n\t\t\t\tr.session.removeHost(existing)\n\t\t\t\tif _, alreadyExists := r.session.ring.addHostIfMissing(h); alreadyExists {\n\t\t\t\t\treturn fmt.Errorf(\"add new host=%s after removal: %w\", h, ErrHostAlreadyExists)\n\t\t\t\t}\n\t\t\t\t// add new HostInfo (same hostID, new IP)\n\t\t\t\tr.session.startPoolFill(h)\n\t\t\t}\n\t\t}\n\t\tdelete(prevHosts, h.HostID())\n\t}\n\n\tfor _, host := range prevHosts {\n\t\tr.session.removeHost(host)\n\t}\n\n\tr.session.metadata.setPartitioner(partitioner)\n\tr.session.policy.SetPartitioner(partitioner)\n\treturn nil\n}\n\nconst (\n\tringRefreshDebounceTime = 1 * time.Second\n)\n\n// debounces requests to call a refresh function (currently used for ring refresh). It also supports triggering a refresh immediately.\ntype refreshDebouncer struct {\n\tmu           sync.Mutex\n\tstopped      bool\n\tbroadcaster  *errorBroadcaster\n\tinterval     time.Duration\n\ttimer        *time.Timer\n\trefreshNowCh chan struct{}\n\tquit         chan struct{}\n\trefreshFn    func() error\n}\n\nfunc newRefreshDebouncer(interval time.Duration, refreshFn func() error) *refreshDebouncer {\n\td := &refreshDebouncer{\n\t\tstopped:      false,\n\t\tbroadcaster:  nil,\n\t\trefreshNowCh: make(chan struct{}, 1),\n\t\tquit:         make(chan struct{}),\n\t\tinterval:     interval,\n\t\ttimer:        time.NewTimer(interval),\n\t\trefreshFn:    refreshFn,\n\t}\n\td.timer.Stop()\n\tgo d.flusher()\n\treturn d\n}\n\n// debounces a request to call the refresh function\nfunc (d *refreshDebouncer) debounce() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif d.stopped {\n\t\treturn\n\t}\n\td.timer.Reset(d.interval)\n}\n\n// requests an immediate refresh which will cancel pending refresh requests\nfunc (d *refreshDebouncer) refreshNow() <-chan error {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif d.broadcaster == nil {\n\t\td.broadcaster = newErrorBroadcaster()\n\t\tselect {\n\t\tcase d.refreshNowCh <- struct{}{}:\n\t\tdefault:\n\t\t\t// already a refresh pending\n\t\t}\n\t}\n\treturn d.broadcaster.newListener()\n}\n\nfunc (d *refreshDebouncer) flusher() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.refreshNowCh:\n\t\tcase <-d.timer.C:\n\t\tcase <-d.quit:\n\t\t}\n\t\td.mu.Lock()\n\t\tif d.stopped {\n\t\t\tif d.broadcaster != nil {\n\t\t\t\td.broadcaster.stop()\n\t\t\t\td.broadcaster = nil\n\t\t\t}\n\t\t\td.timer.Stop()\n\t\t\td.mu.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// make sure both request channels are cleared before we refresh\n\t\tselect {\n\t\tcase <-d.refreshNowCh:\n\t\tdefault:\n\t\t}\n\n\t\td.timer.Stop()\n\t\tselect {\n\t\tcase <-d.timer.C:\n\t\tdefault:\n\t\t}\n\n\t\tcurBroadcaster := d.broadcaster\n\t\td.broadcaster = nil\n\t\td.mu.Unlock()\n\n\t\terr := d.refreshFn()\n\t\tif curBroadcaster != nil {\n\t\t\tcurBroadcaster.broadcast(err)\n\t\t}\n\t}\n}\n\nfunc (d *refreshDebouncer) stop() {\n\td.mu.Lock()\n\tif d.stopped {\n\t\td.mu.Unlock()\n\t\treturn\n\t}\n\td.stopped = true\n\td.mu.Unlock()\n\td.quit <- struct{}{} // sync with flusher\n\tclose(d.quit)\n}\n\n// broadcasts an error to multiple channels (listeners)\ntype errorBroadcaster struct {\n\tlisteners []chan<- error\n\tmu        sync.Mutex\n}\n\nfunc newErrorBroadcaster() *errorBroadcaster {\n\treturn &errorBroadcaster{\n\t\tlisteners: nil,\n\t\tmu:        sync.Mutex{},\n\t}\n}\n\nfunc (b *errorBroadcaster) newListener() <-chan error {\n\tch := make(chan error, 1)\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tb.listeners = append(b.listeners, ch)\n\treturn ch\n}\n\nfunc (b *errorBroadcaster) broadcast(err error) {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tcurListeners := b.listeners\n\tif len(curListeners) > 0 {\n\t\tb.listeners = nil\n\t} else {\n\t\treturn\n\t}\n\n\tfor _, listener := range curListeners {\n\t\tlistener <- err\n\t\tclose(listener)\n\t}\n}\n\nfunc (b *errorBroadcaster) stop() {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif len(b.listeners) == 0 {\n\t\treturn\n\t}\n\tfor _, listener := range b.listeners {\n\t\tclose(listener)\n\t}\n\tb.listeners = nil\n}\n"
        },
        {
          "name": "host_source_gen.go",
          "type": "blob",
          "size": 1.8134765625,
          "content": "//go:build genhostinfo\n// +build genhostinfo\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage main\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"sync\"\n\n\tgocql \"github.com/gocql/gocql\"\n)\n\nfunc gen(clause, field string) {\n\tfmt.Printf(\"if h.%s == %s {\\n\", field, clause)\n\tfmt.Printf(\"\\th.%s = from.%s\\n\", field, field)\n\tfmt.Println(\"}\")\n}\n\nfunc main() {\n\tt := reflect.ValueOf(&gocql.HostInfo{}).Elem().Type()\n\tmu := reflect.TypeOf(sync.RWMutex{})\n\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tif f.Type == mu {\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch f.Type.Kind() {\n\t\tcase reflect.Slice:\n\t\t\tgen(\"nil\", f.Name)\n\t\tcase reflect.String:\n\t\t\tgen(`\"\"`, f.Name)\n\t\tcase reflect.Int:\n\t\t\tgen(\"0\", f.Name)\n\t\tcase reflect.Struct:\n\t\t\tgen(\"(\"+f.Type.Name()+\"{})\", f.Name)\n\t\tcase reflect.Bool, reflect.Int32:\n\t\t\tcontinue\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unknown field: %s\", f))\n\t\t}\n\t}\n\n}\n"
        },
        {
          "name": "host_source_test.go",
          "type": "blob",
          "size": 10.1796875,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"errors\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestUnmarshalCassVersion(t *testing.T) {\n\ttests := [...]struct {\n\t\tdata    string\n\t\tversion cassVersion\n\t}{\n\t\t{\"3.2\", cassVersion{3, 2, 0}},\n\t\t{\"2.10.1-SNAPSHOT\", cassVersion{2, 10, 1}},\n\t\t{\"1.2.3\", cassVersion{1, 2, 3}},\n\t}\n\n\tfor i, test := range tests {\n\t\tv := &cassVersion{}\n\t\tif err := v.UnmarshalCQL(nil, []byte(test.data)); err != nil {\n\t\t\tt.Errorf(\"%d: %v\", i, err)\n\t\t} else if *v != test.version {\n\t\t\tt.Errorf(\"%d: expected %#+v got %#+v\", i, test.version, *v)\n\t\t}\n\t}\n}\n\nfunc TestCassVersionBefore(t *testing.T) {\n\ttests := [...]struct {\n\t\tversion             cassVersion\n\t\tmajor, minor, patch int\n\t}{\n\t\t{cassVersion{1, 0, 0}, 0, 0, 0},\n\t\t{cassVersion{0, 1, 0}, 0, 0, 0},\n\t\t{cassVersion{0, 0, 1}, 0, 0, 0},\n\n\t\t{cassVersion{1, 0, 0}, 0, 1, 0},\n\t\t{cassVersion{0, 1, 0}, 0, 0, 1},\n\t\t{cassVersion{4, 1, 0}, 3, 1, 2},\n\t}\n\n\tfor i, test := range tests {\n\t\tif test.version.Before(test.major, test.minor, test.patch) {\n\t\t\tt.Errorf(\"%d: expected v%d.%d.%d to be before %v\", i, test.major, test.minor, test.patch, test.version)\n\t\t}\n\t}\n\n}\n\nfunc TestIsValidPeer(t *testing.T) {\n\thost := &HostInfo{\n\t\trpcAddress: net.ParseIP(\"0.0.0.0\"),\n\t\track:       \"myRack\",\n\t\thostId:     \"0\",\n\t\tdataCenter: \"datacenter\",\n\t\ttokens:     []string{\"0\", \"1\"},\n\t}\n\n\tif !isValidPeer(host) {\n\t\tt.Errorf(\"expected %+v to be a valid peer\", host)\n\t}\n\n\thost.rack = \"\"\n\tif isValidPeer(host) {\n\t\tt.Errorf(\"expected %+v to NOT be a valid peer\", host)\n\t}\n}\n\nfunc TestHostInfo_ConnectAddress(t *testing.T) {\n\tvar localhost = net.IPv4(127, 0, 0, 1)\n\ttests := []struct {\n\t\tname          string\n\t\tconnectAddr   net.IP\n\t\trpcAddr       net.IP\n\t\tbroadcastAddr net.IP\n\t\tpeer          net.IP\n\t}{\n\t\t{name: \"rpc_address\", rpcAddr: localhost},\n\t\t{name: \"connect_address\", connectAddr: localhost},\n\t\t{name: \"broadcast_address\", broadcastAddr: localhost},\n\t\t{name: \"peer\", peer: localhost},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\thost := &HostInfo{\n\t\t\t\tconnectAddress:   test.connectAddr,\n\t\t\t\trpcAddress:       test.rpcAddr,\n\t\t\t\tbroadcastAddress: test.broadcastAddr,\n\t\t\t\tpeer:             test.peer,\n\t\t\t}\n\n\t\t\tif addr := host.ConnectAddress(); !addr.Equal(localhost) {\n\t\t\t\tt.Fatalf(\"expected ConnectAddress to be %s got %s\", localhost, addr)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// This test sends debounce requests and waits until the refresh function is called (which should happen when the timer elapses).\nfunc TestRefreshDebouncer_MultipleEvents(t *testing.T) {\n\tconst numberOfEvents = 10\n\tchannel := make(chan int, numberOfEvents) // should never use more than 1 but allow for more to possibly detect bugs\n\tfn := func() error {\n\t\tchannel <- 0\n\t\treturn nil\n\t}\n\tbeforeEvents := time.Now()\n\twg := sync.WaitGroup{}\n\td := newRefreshDebouncer(2*time.Second, fn)\n\tdefer d.stop()\n\tfor i := 0; i < numberOfEvents; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\td.debounce()\n\t\t}()\n\t}\n\twg.Wait()\n\ttimeoutCh := time.After(2500 * time.Millisecond) // extra time to avoid flakiness\n\tselect {\n\tcase <-channel:\n\tcase <-timeoutCh:\n\t\tt.Fatalf(\"timeout elapsed without flush function being called\")\n\t}\n\tafterFunctionCall := time.Now()\n\n\t// use 1.5 seconds instead of 2 seconds to avoid timer precision issues\n\tif afterFunctionCall.Sub(beforeEvents) < 1500*time.Millisecond {\n\t\tt.Fatalf(\"function was called after %v ms instead of ~2 seconds\", afterFunctionCall.Sub(beforeEvents).Milliseconds())\n\t}\n\n\t// wait another 2 seconds and check if function was called again\n\ttime.Sleep(2500 * time.Millisecond)\n\tif len(channel) > 0 {\n\t\tt.Fatalf(\"function was called more than once\")\n\t}\n}\n\n// This test:\n//\n//\t1 - Sends debounce requests when test starts\n//\t2 - Calls refreshNow() before the timer elapsed (which stops the timer) about 1.5 seconds after test starts\n//\n// The end result should be 1 refresh function call when refreshNow() is called.\nfunc TestRefreshDebouncer_RefreshNow(t *testing.T) {\n\tconst numberOfEvents = 10\n\tchannel := make(chan int, numberOfEvents) // should never use more than 1 but allow for more to possibly detect bugs\n\tfn := func() error {\n\t\tchannel <- 0\n\t\treturn nil\n\t}\n\tbeforeEvents := time.Now()\n\teventsWg := sync.WaitGroup{}\n\td := newRefreshDebouncer(2*time.Second, fn)\n\tdefer d.stop()\n\tfor i := 0; i < numberOfEvents; i++ {\n\t\teventsWg.Add(1)\n\t\tgo func() {\n\t\t\tdefer eventsWg.Done()\n\t\t\td.debounce()\n\t\t}()\n\t}\n\n\trefreshNowWg := sync.WaitGroup{}\n\trefreshNowWg.Add(1)\n\tgo func() {\n\t\tdefer refreshNowWg.Done()\n\t\ttime.Sleep(1500 * time.Millisecond)\n\t\td.refreshNow()\n\t}()\n\n\teventsWg.Wait()\n\tselect {\n\tcase <-channel:\n\t\tt.Fatalf(\"function was called before the expected time\")\n\tdefault:\n\t}\n\n\trefreshNowWg.Wait()\n\n\ttimeoutCh := time.After(200 * time.Millisecond) // allow for 200ms of delay to prevent flakiness\n\tselect {\n\tcase <-channel:\n\tcase <-timeoutCh:\n\t\tt.Fatalf(\"timeout elapsed without flush function being called\")\n\t}\n\tafterFunctionCall := time.Now()\n\n\t// use 1 second instead of 1.5s to avoid timer precision issues\n\tif afterFunctionCall.Sub(beforeEvents) < 1000*time.Millisecond {\n\t\tt.Fatalf(\"function was called after %v ms instead of ~1.5 seconds\", afterFunctionCall.Sub(beforeEvents).Milliseconds())\n\t}\n\n\t// wait some time and check if function was called again\n\ttime.Sleep(2500 * time.Millisecond)\n\tif len(channel) > 0 {\n\t\tt.Fatalf(\"function was called more than once\")\n\t}\n}\n\n// This test:\n//\n//\t1 - Sends debounce requests when test starts\n//\t2 - Calls refreshNow() before the timer elapsed (which stops the timer) about 1 second after test starts\n//\t3 - Sends more debounce requests (which resets the timer with a 3-second interval) about 2 seconds after test starts\n//\n// The end result should be 2 refresh function calls:\n//\n//\t1 - When refreshNow() is called (1 second after the test starts)\n//\t2 - When the timer elapses after the second \"wave\" of debounce requests (5 seconds after the test starts)\nfunc TestRefreshDebouncer_EventsAfterRefreshNow(t *testing.T) {\n\tconst numberOfEvents = 10\n\tchannel := make(chan int, numberOfEvents) // should never use more than 2 but allow for more to possibly detect bugs\n\tfn := func() error {\n\t\tchannel <- 0\n\t\treturn nil\n\t}\n\tbeforeEvents := time.Now()\n\twg := sync.WaitGroup{}\n\td := newRefreshDebouncer(3*time.Second, fn)\n\tdefer d.stop()\n\tfor i := 0; i < numberOfEvents; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\td.debounce()\n\t\t\ttime.Sleep(2000 * time.Millisecond)\n\t\t\td.debounce()\n\t\t}()\n\t}\n\n\tgo func() {\n\t\ttime.Sleep(1 * time.Second)\n\t\td.refreshNow()\n\t}()\n\n\twg.Wait()\n\ttimeoutCh := time.After(1500 * time.Millisecond) // extra 500ms to prevent flakiness\n\tselect {\n\tcase <-channel:\n\tcase <-timeoutCh:\n\t\tt.Fatalf(\"timeout elapsed without flush function being called after refreshNow()\")\n\t}\n\tafterFunctionCall := time.Now()\n\n\t// use 500ms instead of 1s to avoid timer precision issues\n\tif afterFunctionCall.Sub(beforeEvents) < 500*time.Millisecond {\n\t\tt.Fatalf(\"function was called after %v ms instead of ~1 second\", afterFunctionCall.Sub(beforeEvents).Milliseconds())\n\t}\n\n\ttimeoutCh = time.After(4 * time.Second) // extra 1s to prevent flakiness\n\tselect {\n\tcase <-channel:\n\tcase <-timeoutCh:\n\t\tt.Fatalf(\"timeout elapsed without flush function being called after debounce requests\")\n\t}\n\tafterSecondFunctionCall := time.Now()\n\n\t// use 2.5s instead of 3s to avoid timer precision issues\n\tif afterSecondFunctionCall.Sub(afterFunctionCall) < 2500*time.Millisecond {\n\t\tt.Fatalf(\"function was called after %v ms instead of ~3 seconds\", afterSecondFunctionCall.Sub(afterFunctionCall).Milliseconds())\n\t}\n\n\tif len(channel) > 0 {\n\t\tt.Fatalf(\"function was called more than twice\")\n\t}\n}\n\nfunc TestErrorBroadcaster_MultipleListeners(t *testing.T) {\n\tb := newErrorBroadcaster()\n\tdefer b.stop()\n\tconst numberOfListeners = 10\n\tvar listeners []<-chan error\n\tfor i := 0; i < numberOfListeners; i++ {\n\t\tlisteners = append(listeners, b.newListener())\n\t}\n\n\terr := errors.New(\"expected error\")\n\twg := sync.WaitGroup{}\n\tresult := atomic.Value{}\n\tfor _, listener := range listeners {\n\t\tcurrentListener := listener\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\treceivedErr, ok := <-currentListener\n\t\t\tif !ok {\n\t\t\t\tresult.Store(errors.New(\"listener was closed\"))\n\t\t\t} else if receivedErr != err {\n\t\t\t\tresult.Store(errors.New(\"expected received error to be the same as the one that was broadcasted\"))\n\t\t\t}\n\t\t}()\n\t}\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tb.broadcast(err)\n\t\tb.stop()\n\t}()\n\twg.Wait()\n\tif loadedVal := result.Load(); loadedVal != nil {\n\t\tt.Errorf(loadedVal.(error).Error())\n\t}\n}\n\nfunc TestErrorBroadcaster_StopWithoutBroadcast(t *testing.T) {\n\tvar b = newErrorBroadcaster()\n\tdefer b.stop()\n\tconst numberOfListeners = 10\n\tvar listeners []<-chan error\n\tfor i := 0; i < numberOfListeners; i++ {\n\t\tlisteners = append(listeners, b.newListener())\n\t}\n\n\twg := sync.WaitGroup{}\n\tresult := atomic.Value{}\n\tfor _, listener := range listeners {\n\t\tcurrentListener := listener\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\t// broadcaster stopped, expect listener to be closed\n\t\t\t_, ok := <-currentListener\n\t\t\tif ok {\n\t\t\t\tresult.Store(errors.New(\"expected listener to be closed\"))\n\t\t\t}\n\t\t}()\n\t}\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\t// call stop without broadcasting anything to current listeners\n\t\tb.stop()\n\t}()\n\twg.Wait()\n\tif loadedVal := result.Load(); loadedVal != nil {\n\t\tt.Errorf(loadedVal.(error).Error())\n\t}\n}\n"
        },
        {
          "name": "install_test_deps.sh",
          "type": "blob",
          "size": 1.033203125,
          "content": "#!/usr/bin/env bash\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nset -x\n\n# This is not supposed to be an error-prone script; just a convenience.\n\n# Install CCM\npip install -i https://pypi.org/simple --user cql PyYAML six psutil\ngit clone https://github.com/pcmanus/ccm.git\npushd ccm\n./setup.py install --user\npopd\n"
        },
        {
          "name": "integration.sh",
          "type": "blob",
          "size": 3.31640625,
          "content": "#!/bin/bash\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nset -eux\n\nfunction run_tests() {\n\tlocal clusterSize=3\n\tlocal version=$1\n\tlocal auth=$2\n\tlocal compressor=$3\n\n\tif [ \"$auth\" = true ]; then\n\t\tclusterSize=1\n\tfi\n\n\tlocal keypath=\"$(pwd)/testdata/pki\"\n\n\tlocal conf=(\n\t\t\"client_encryption_options.enabled: true\"\n\t\t\"client_encryption_options.keystore: $keypath/.keystore\"\n\t\t\"client_encryption_options.keystore_password: cassandra\"\n\t\t\"client_encryption_options.require_client_auth: true\"\n\t\t\"client_encryption_options.truststore: $keypath/.truststore\"\n\t\t\"client_encryption_options.truststore_password: cassandra\"\n\t\t\"concurrent_reads: 2\"\n\t\t\"concurrent_writes: 2\"\n\t\t\"rpc_server_type: sync\"\n\t\t\"rpc_min_threads: 2\"\n\t\t\"rpc_max_threads: 2\"\n\t\t\"write_request_timeout_in_ms: 5000\"\n\t\t\"read_request_timeout_in_ms: 5000\"\n\t)\n\n\tccm remove test || true\n\n\tccm create test -v $version -n $clusterSize -d --vnodes --jvm_arg=\"-Xmx256m -XX:NewSize=100m\"\n\tccm updateconf \"${conf[@]}\"\n\n\tif [ \"$auth\" = true ]\n\tthen\n\t\tccm updateconf 'authenticator: PasswordAuthenticator' 'authorizer: CassandraAuthorizer'\n\t\trm -rf $HOME/.ccm/test/node1/data/system_auth\n\tfi\n\n\tlocal proto=2\n\tif [[ $version == 1.2.* ]]; then\n\t\tproto=1\n\telif [[ $version == 2.0.* ]]; then\n\t\tproto=2\n\telif [[ $version == 2.1.* ]]; then\n\t\tproto=3\n\telif [[ $version == 2.2.* || $version == 3.0.* ]]; then\n\t\tproto=4\n\t\tccm updateconf 'enable_user_defined_functions: true'\n\t\texport JVM_EXTRA_OPTS=\" -Dcassandra.test.fail_writes_ks=test -Dcassandra.custom_query_handler_class=org.apache.cassandra.cql3.CustomPayloadMirroringQueryHandler\"\n\telif [[ $version == 3.*.* ]]; then\n\t\tproto=5\n\t\tccm updateconf 'enable_user_defined_functions: true'\n\t\texport JVM_EXTRA_OPTS=\" -Dcassandra.test.fail_writes_ks=test -Dcassandra.custom_query_handler_class=org.apache.cassandra.cql3.CustomPayloadMirroringQueryHandler\"\n\tfi\n\n\tsleep 1s\n\n\tccm list\n\tccm start --wait-for-binary-proto\n\tccm status\n\tccm node1 nodetool status\n\n\tlocal args=\"-gocql.timeout=60s -runssl -proto=$proto -rf=3 -clusterSize=$clusterSize -autowait=2000ms -compressor=$compressor -gocql.cversion=$version -cluster=$(ccm liveset) ./...\"\n\n\tgo test -v -tags unit -race\n\n\tif [ \"$auth\" = true ]\n\tthen\n\t\tsleep 30s\n\t\tgo test -run=TestAuthentication -tags \"integration gocql_debug\" -timeout=15s -runauth $args\n\telse\n\t\tsleep 1s\n\t\tgo test -tags \"cassandra gocql_debug\" -timeout=5m -race $args\n\n\t\tccm clear\n\t\tccm start --wait-for-binary-proto\n\t\tsleep 1s\n\n\t\tgo test -tags \"integration gocql_debug\" -timeout=5m -race $args\n\n\t\tccm clear\n\t\tccm start --wait-for-binary-proto\n\t\tsleep 1s\n\n\t\tgo test -tags \"ccm gocql_debug\" -timeout=5m -race $args\n\tfi\n\n\tccm remove\n}\n\nrun_tests $1 $2 $3\n"
        },
        {
          "name": "integration_test.go",
          "type": "blob",
          "size": 8.62109375,
          "content": "//go:build all || integration\n// +build all integration\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\n// This file groups integration tests where Cassandra has to be set up with some special integration variables\nimport (\n\t\"context\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n)\n\n// TestAuthentication verifies that gocql will work with a host configured to only accept authenticated connections\nfunc TestAuthentication(t *testing.T) {\n\n\tif *flagProto < 2 {\n\t\tt.Skip(\"Authentication is not supported with protocol < 2\")\n\t}\n\n\tif !*flagRunAuthTest {\n\t\tt.Skip(\"Authentication is not configured in the target cluster\")\n\t}\n\n\tcluster := createCluster()\n\n\tcluster.Authenticator = PasswordAuthenticator{\n\t\tUsername: \"cassandra\",\n\t\tPassword: \"cassandra\",\n\t}\n\n\tsession, err := cluster.CreateSession()\n\n\tif err != nil {\n\t\tt.Fatalf(\"Authentication error: %s\", err)\n\t}\n\n\tsession.Close()\n}\n\nfunc TestGetHosts(t *testing.T) {\n\tclusterHosts := getClusterHosts()\n\tcluster := createCluster()\n\tsession := createSessionFromCluster(cluster, t)\n\n\thosts, partitioner, err := session.hostSource.GetHosts()\n\n\tassertTrue(t, \"err == nil\", err == nil)\n\tassertEqual(t, \"len(hosts)\", len(clusterHosts), len(hosts))\n\tassertTrue(t, \"len(partitioner) != 0\", len(partitioner) != 0)\n}\n\n// TestRingDiscovery makes sure that you can autodiscover other cluster members\n// when you seed a cluster config with just one node\nfunc TestRingDiscovery(t *testing.T) {\n\tclusterHosts := getClusterHosts()\n\tcluster := createCluster()\n\tcluster.Hosts = clusterHosts[:1]\n\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif *clusterSize > 1 {\n\t\t// wait for autodiscovery to update the pool with the list of known hosts\n\t\ttime.Sleep(*flagAutoWait)\n\t}\n\n\tsession.pool.mu.RLock()\n\tdefer session.pool.mu.RUnlock()\n\tsize := len(session.pool.hostConnPools)\n\n\tif *clusterSize != size {\n\t\tfor p, pool := range session.pool.hostConnPools {\n\t\t\tt.Logf(\"p=%q host=%v ips=%s\", p, pool.host, pool.host.ConnectAddress().String())\n\n\t\t}\n\t\tt.Errorf(\"Expected a cluster size of %d, but actual size was %d\", *clusterSize, size)\n\t}\n}\n\n// TestHostFilterDiscovery ensures that host filtering works even when we discover hosts\nfunc TestHostFilterDiscovery(t *testing.T) {\n\tclusterHosts := getClusterHosts()\n\tif len(clusterHosts) < 2 {\n\t\tt.Skip(\"skipping because we don't have 2 or more hosts\")\n\t}\n\tcluster := createCluster()\n\trr := RoundRobinHostPolicy().(*roundRobinHostPolicy)\n\tcluster.PoolConfig.HostSelectionPolicy = rr\n\t// we'll filter out the second host\n\tfiltered := clusterHosts[1]\n\tcluster.Hosts = clusterHosts[:1]\n\tcluster.HostFilter = HostFilterFunc(func(host *HostInfo) bool {\n\t\tif host.ConnectAddress().String() == filtered {\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t})\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tassertEqual(t, \"len(clusterHosts)-1 != len(rr.hosts.get())\", len(clusterHosts)-1, len(rr.hosts.get()))\n}\n\n// TestHostFilterInitial ensures that host filtering works for the initial\n// connection including the control connection\nfunc TestHostFilterInitial(t *testing.T) {\n\tclusterHosts := getClusterHosts()\n\tif len(clusterHosts) < 2 {\n\t\tt.Skip(\"skipping because we don't have 2 or more hosts\")\n\t}\n\tcluster := createCluster()\n\trr := RoundRobinHostPolicy().(*roundRobinHostPolicy)\n\tcluster.PoolConfig.HostSelectionPolicy = rr\n\t// we'll filter out the second host\n\tfiltered := clusterHosts[1]\n\tcluster.HostFilter = HostFilterFunc(func(host *HostInfo) bool {\n\t\tif host.ConnectAddress().String() == filtered {\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t})\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tassertEqual(t, \"len(clusterHosts)-1 != len(rr.hosts.get())\", len(clusterHosts)-1, len(rr.hosts.get()))\n}\n\nfunc TestWriteFailure(t *testing.T) {\n\tcluster := createCluster()\n\tcreateKeyspace(t, cluster, \"test\")\n\tcluster.Keyspace = \"test\"\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(\"create session:\", err)\n\t}\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE test.test (id int,value int,PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatalf(\"failed to create table with error '%v'\", err)\n\t}\n\tif err := session.Query(`INSERT INTO test.test (id, value) VALUES (1, 1)`).Exec(); err != nil {\n\t\terrWrite, ok := err.(*RequestErrWriteFailure)\n\t\tif ok {\n\t\t\tif session.cfg.ProtoVersion >= 5 {\n\t\t\t\t// ErrorMap should be filled with some hosts that should've errored\n\t\t\t\tif len(errWrite.ErrorMap) == 0 {\n\t\t\t\t\tt.Fatal(\"errWrite.ErrorMap should have some failed hosts but it didn't have any\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Map doesn't get filled for V4\n\t\t\t\tif len(errWrite.ErrorMap) != 0 {\n\t\t\t\t\tt.Fatal(\"errWrite.ErrorMap should have length 0, it's: \", len(errWrite.ErrorMap))\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tt.Fatal(\"error should be RequestErrWriteFailure, it's: \", errWrite)\n\t\t}\n\t} else {\n\t\tt.Fatal(\"a write fail error should have happened when querying test keyspace\")\n\t}\n\n\tif err = session.Query(\"DROP KEYSPACE test\").Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestCustomPayloadMessages(t *testing.T) {\n\tcluster := createCluster()\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.testCustomPayloadMessages (id int, value int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// QueryMessage\n\tvar customPayload = map[string][]byte{\"a\": []byte{10, 20}, \"b\": []byte{20, 30}}\n\tquery := session.Query(\"SELECT id FROM testCustomPayloadMessages where id = ?\", 42).Consistency(One).CustomPayload(customPayload)\n\titer := query.Iter()\n\trCustomPayload := iter.GetCustomPayload()\n\tif !reflect.DeepEqual(customPayload, rCustomPayload) {\n\t\tt.Fatal(\"The received custom payload should match the sent\")\n\t}\n\titer.Close()\n\n\t// Insert query\n\tquery = session.Query(\"INSERT INTO testCustomPayloadMessages(id,value) VALUES(1, 1)\").Consistency(One).CustomPayload(customPayload)\n\titer = query.Iter()\n\trCustomPayload = iter.GetCustomPayload()\n\tif !reflect.DeepEqual(customPayload, rCustomPayload) {\n\t\tt.Fatal(\"The received custom payload should match the sent\")\n\t}\n\titer.Close()\n\n\t// Batch Message\n\tb := session.Batch(LoggedBatch)\n\tb.CustomPayload = customPayload\n\tb.Query(\"INSERT INTO testCustomPayloadMessages(id,value) VALUES(1, 1)\")\n\tif err := session.ExecuteBatch(b); err != nil {\n\t\tt.Fatalf(\"query failed. %v\", err)\n\t}\n}\n\nfunc TestCustomPayloadValues(t *testing.T) {\n\tcluster := createCluster()\n\tsession := createSessionFromCluster(cluster, t)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE gocql_test.testCustomPayloadValues (id int, value int, PRIMARY KEY (id))\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvalues := []map[string][]byte{map[string][]byte{\"a\": []byte{10, 20}, \"b\": []byte{20, 30}}, nil, map[string][]byte{\"a\": []byte{10, 20}, \"b\": nil}}\n\n\tfor _, customPayload := range values {\n\t\tquery := session.Query(\"SELECT id FROM testCustomPayloadValues where id = ?\", 42).Consistency(One).CustomPayload(customPayload)\n\t\titer := query.Iter()\n\t\trCustomPayload := iter.GetCustomPayload()\n\t\tif !reflect.DeepEqual(customPayload, rCustomPayload) {\n\t\t\tt.Fatal(\"The received custom payload should match the sent\")\n\t\t}\n\t}\n}\n\nfunc TestSessionAwaitSchemaAgreement(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif err := session.AwaitSchemaAgreement(context.Background()); err != nil {\n\t\tt.Fatalf(\"expected session.AwaitSchemaAgreement to not return an error but got '%v'\", err)\n\t}\n}\n\nfunc TestUDF(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < 4 {\n\t\tt.Skip(\"skipping UDF support on proto < 4\")\n\t}\n\n\tconst query = `CREATE OR REPLACE FUNCTION uniq(state set<text>, val text)\n\t  CALLED ON NULL INPUT RETURNS set<text> LANGUAGE java\n\t  AS 'state.add(val); return state;'`\n\n\terr := session.Query(query).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "keyspace_table_test.go",
          "type": "blob",
          "size": 3.013671875,
          "content": "//go:build all || integration\n// +build all integration\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n)\n\n// Keyspace_table checks if Query.Keyspace() is updated based on prepared statement\nfunc TestKeyspaceTable(t *testing.T) {\n\tcluster := createCluster()\n\n\tfallback := RoundRobinHostPolicy()\n\tcluster.PoolConfig.HostSelectionPolicy = TokenAwareHostPolicy(fallback)\n\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatal(\"createSession:\", err)\n\t}\n\n\tcluster.Keyspace = \"wrong_keyspace\"\n\n\tkeyspace := \"test1\"\n\ttable := \"table1\"\n\n\terr = createTable(session, `DROP KEYSPACE IF EXISTS `+keyspace)\n\tif err != nil {\n\t\tt.Fatal(\"unable to drop keyspace:\", err)\n\t}\n\n\terr = createTable(session, fmt.Sprintf(`CREATE KEYSPACE %s\n\tWITH replication = {\n\t\t'class' : 'SimpleStrategy',\n\t\t'replication_factor' : 1\n\t}`, keyspace))\n\n\tif err != nil {\n\t\tt.Fatal(\"unable to create keyspace:\", err)\n\t}\n\n\tif err := session.control.awaitSchemaAgreement(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, fmt.Sprintf(`CREATE TABLE %s.%s (pk int, ck int, v int, PRIMARY KEY (pk, ck));\n\t`, keyspace, table))\n\n\tif err != nil {\n\t\tt.Fatal(\"unable to create table:\", err)\n\t}\n\n\tif err := session.control.awaitSchemaAgreement(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tctx := context.Background()\n\n\t// insert a row\n\tif err := session.Query(`INSERT INTO test1.table1(pk, ck, v) VALUES (?, ?, ?)`,\n\t\t1, 2, 3).WithContext(ctx).Consistency(One).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar pk int\n\n\t/* Search for a specific set of records whose 'pk' column matches\n\t * the value of inserted row. */\n\tqry := session.Query(`SELECT pk FROM test1.table1 WHERE pk = ? LIMIT 1`,\n\t\t1).WithContext(ctx).Consistency(One)\n\tif err := qry.Scan(&pk); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// cluster.Keyspace was set to \"wrong_keyspace\", but during prepering statement\n\t// Keyspace in Query should be changed to \"test\" and Table should be changed to table1\n\tassertEqual(t, \"qry.Keyspace()\", \"test1\", qry.Keyspace())\n\tassertEqual(t, \"qry.Table()\", \"table1\", qry.Table())\n}\n"
        },
        {
          "name": "logger.go",
          "type": "blob",
          "size": 2.193359375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"log\"\n)\n\ntype StdLogger interface {\n\tPrint(v ...interface{})\n\tPrintf(format string, v ...interface{})\n\tPrintln(v ...interface{})\n}\n\ntype nopLogger struct{}\n\nfunc (n nopLogger) Print(_ ...interface{}) {}\n\nfunc (n nopLogger) Printf(_ string, _ ...interface{}) {}\n\nfunc (n nopLogger) Println(_ ...interface{}) {}\n\ntype testLogger struct {\n\tcapture bytes.Buffer\n}\n\nfunc (l *testLogger) Print(v ...interface{})                 { fmt.Fprint(&l.capture, v...) }\nfunc (l *testLogger) Printf(format string, v ...interface{}) { fmt.Fprintf(&l.capture, format, v...) }\nfunc (l *testLogger) Println(v ...interface{})               { fmt.Fprintln(&l.capture, v...) }\nfunc (l *testLogger) String() string                         { return l.capture.String() }\n\ntype defaultLogger struct{}\n\nfunc (l *defaultLogger) Print(v ...interface{})                 { log.Print(v...) }\nfunc (l *defaultLogger) Printf(format string, v ...interface{}) { log.Printf(format, v...) }\nfunc (l *defaultLogger) Println(v ...interface{})               { log.Println(v...) }\n\n// Logger for logging messages.\n// Deprecated: Use ClusterConfig.Logger instead.\nvar Logger StdLogger = &defaultLogger{}\n"
        },
        {
          "name": "lz4",
          "type": "tree",
          "content": null
        },
        {
          "name": "marshal.go",
          "type": "blob",
          "size": 69.2392578125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/big\"\n\t\"math/bits\"\n\t\"net\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"gopkg.in/inf.v0\"\n)\n\nvar (\n\tbigOne     = big.NewInt(1)\n\temptyValue reflect.Value\n)\n\nvar (\n\tErrorUDTUnavailable = errors.New(\"UDT are not available on protocols less than 3, please update config\")\n)\n\n// Marshaler is the interface implemented by objects that can marshal\n// themselves into values understood by Cassandra.\ntype Marshaler interface {\n\tMarshalCQL(info TypeInfo) ([]byte, error)\n}\n\n// Unmarshaler is the interface implemented by objects that can unmarshal\n// a Cassandra specific description of themselves.\ntype Unmarshaler interface {\n\tUnmarshalCQL(info TypeInfo, data []byte) error\n}\n\n// Marshal returns the CQL encoding of the value for the Cassandra\n// internal type described by the info parameter.\n//\n// nil is serialized as CQL null.\n// If value implements Marshaler, its MarshalCQL method is called to marshal the data.\n// If value is a pointer, the pointed-to value is marshaled.\n//\n// Supported conversions are as follows, other type combinations may be added in the future:\n//\n//\tCQL type                    | Go type (value)    | Note\n//\tvarchar, ascii, blob, text  | string, []byte     |\n//\tboolean                     | bool               |\n//\ttinyint, smallint, int      | integer types      |\n//\ttinyint, smallint, int      | string             | formatted as base 10 number\n//\tbigint, counter             | integer types      |\n//\tbigint, counter             | big.Int            |\n//\tbigint, counter             | string             | formatted as base 10 number\n//\tfloat                       | float32            |\n//\tdouble                      | float64            |\n//\tdecimal                     | inf.Dec            |\n//\ttime                        | int64              | nanoseconds since start of day\n//\ttime                        | time.Duration      | duration since start of day\n//\ttimestamp                   | int64              | milliseconds since Unix epoch\n//\ttimestamp                   | time.Time          |\n//\tlist, set                   | slice, array       |\n//\tlist, set                   | map[X]struct{}     |\n//\tmap                         | map[X]Y            |\n//\tuuid, timeuuid              | gocql.UUID         |\n//\tuuid, timeuuid              | [16]byte           | raw UUID bytes\n//\tuuid, timeuuid              | []byte             | raw UUID bytes, length must be 16 bytes\n//\tuuid, timeuuid              | string             | hex representation, see ParseUUID\n//\tvarint                      | integer types      |\n//\tvarint                      | big.Int            |\n//\tvarint                      | string             | value of number in decimal notation\n//\tinet                        | net.IP             |\n//\tinet                        | string             | IPv4 or IPv6 address string\n//\ttuple                       | slice, array       |\n//\ttuple                       | struct             | fields are marshaled in order of declaration\n//\tuser-defined type           | gocql.UDTMarshaler | MarshalUDT is called\n//\tuser-defined type           | map[string]interface{} |\n//\tuser-defined type           | struct             | struct fields' cql tags are used for column names\n//\tdate                        | int64              | milliseconds since Unix epoch to start of day (in UTC)\n//\tdate                        | time.Time          | start of day (in UTC)\n//\tdate                        | string             | parsed using \"2006-01-02\" format\n//\tduration                    | int64              | duration in nanoseconds\n//\tduration                    | time.Duration      |\n//\tduration                    | gocql.Duration     |\n//\tduration                    | string             | parsed with time.ParseDuration\nfunc Marshal(info TypeInfo, value interface{}) ([]byte, error) {\n\tif info.Version() < protoVersion1 {\n\t\tpanic(\"protocol version not set\")\n\t}\n\n\tif valueRef := reflect.ValueOf(value); valueRef.Kind() == reflect.Ptr {\n\t\tif valueRef.IsNil() {\n\t\t\treturn nil, nil\n\t\t} else if v, ok := value.(Marshaler); ok {\n\t\t\treturn v.MarshalCQL(info)\n\t\t} else {\n\t\t\treturn Marshal(info, valueRef.Elem().Interface())\n\t\t}\n\t}\n\n\tif v, ok := value.(Marshaler); ok {\n\t\treturn v.MarshalCQL(info)\n\t}\n\n\tswitch info.Type() {\n\tcase TypeVarchar, TypeAscii, TypeBlob, TypeText:\n\t\treturn marshalVarchar(info, value)\n\tcase TypeBoolean:\n\t\treturn marshalBool(info, value)\n\tcase TypeTinyInt:\n\t\treturn marshalTinyInt(info, value)\n\tcase TypeSmallInt:\n\t\treturn marshalSmallInt(info, value)\n\tcase TypeInt:\n\t\treturn marshalInt(info, value)\n\tcase TypeBigInt, TypeCounter:\n\t\treturn marshalBigInt(info, value)\n\tcase TypeFloat:\n\t\treturn marshalFloat(info, value)\n\tcase TypeDouble:\n\t\treturn marshalDouble(info, value)\n\tcase TypeDecimal:\n\t\treturn marshalDecimal(info, value)\n\tcase TypeTime:\n\t\treturn marshalTime(info, value)\n\tcase TypeTimestamp:\n\t\treturn marshalTimestamp(info, value)\n\tcase TypeList, TypeSet:\n\t\treturn marshalList(info, value)\n\tcase TypeMap:\n\t\treturn marshalMap(info, value)\n\tcase TypeUUID, TypeTimeUUID:\n\t\treturn marshalUUID(info, value)\n\tcase TypeVarint:\n\t\treturn marshalVarint(info, value)\n\tcase TypeInet:\n\t\treturn marshalInet(info, value)\n\tcase TypeTuple:\n\t\treturn marshalTuple(info, value)\n\tcase TypeUDT:\n\t\treturn marshalUDT(info, value)\n\tcase TypeDate:\n\t\treturn marshalDate(info, value)\n\tcase TypeDuration:\n\t\treturn marshalDuration(info, value)\n\t}\n\n\t// detect protocol 2 UDT\n\tif strings.HasPrefix(info.Custom(), \"org.apache.cassandra.db.marshal.UserType\") && info.Version() < 3 {\n\t\treturn nil, ErrorUDTUnavailable\n\t}\n\n\t// TODO(tux21b): add the remaining types\n\treturn nil, fmt.Errorf(\"can not marshal %T into %s\", value, info)\n}\n\n// Unmarshal parses the CQL encoded data based on the info parameter that\n// describes the Cassandra internal data type and stores the result in the\n// value pointed by value.\n//\n// If value implements Unmarshaler, it's UnmarshalCQL method is called to\n// unmarshal the data.\n// If value is a pointer to pointer, it is set to nil if the CQL value is\n// null. Otherwise, nulls are unmarshalled as zero value.\n//\n// Supported conversions are as follows, other type combinations may be added in the future:\n//\n//\tCQL type                                | Go type (value)         | Note\n//\tvarchar, ascii, blob, text              | *string                 |\n//\tvarchar, ascii, blob, text              | *[]byte                 | non-nil buffer is reused\n//\tbool                                    | *bool                   |\n//\ttinyint, smallint, int, bigint, counter | *integer types          |\n//\ttinyint, smallint, int, bigint, counter | *big.Int                |\n//\ttinyint, smallint, int, bigint, counter | *string                 | formatted as base 10 number\n//\tfloat                                   | *float32                |\n//\tdouble                                  | *float64                |\n//\tdecimal                                 | *inf.Dec                |\n//\ttime                                    | *int64                  | nanoseconds since start of day\n//\ttime                                    | *time.Duration          |\n//\ttimestamp                               | *int64                  | milliseconds since Unix epoch\n//\ttimestamp                               | *time.Time              |\n//\tlist, set                               | *slice, *array          |\n//\tmap                                     | *map[X]Y                |\n//\tuuid, timeuuid                          | *string                 | see UUID.String\n//\tuuid, timeuuid                          | *[]byte                 | raw UUID bytes\n//\tuuid, timeuuid                          | *gocql.UUID             |\n//\ttimeuuid                                | *time.Time              | timestamp of the UUID\n//\tinet                                    | *net.IP                 |\n//\tinet                                    | *string                 | IPv4 or IPv6 address string\n//\ttuple                                   | *slice, *array          |\n//\ttuple                                   | *struct                 | struct fields are set in order of declaration\n//\tuser-defined types                      | gocql.UDTUnmarshaler    | UnmarshalUDT is called\n//\tuser-defined types                      | *map[string]interface{} |\n//\tuser-defined types                      | *struct                 | cql tag is used to determine field name\n//\tdate                                    | *time.Time              | time of beginning of the day (in UTC)\n//\tdate                                    | *string                 | formatted with 2006-01-02 format\n//\tduration                                | *gocql.Duration         |\nfunc Unmarshal(info TypeInfo, data []byte, value interface{}) error {\n\tif v, ok := value.(Unmarshaler); ok {\n\t\treturn v.UnmarshalCQL(info, data)\n\t}\n\n\tif isNullableValue(value) {\n\t\treturn unmarshalNullable(info, data, value)\n\t}\n\n\tswitch info.Type() {\n\tcase TypeVarchar, TypeAscii, TypeBlob, TypeText:\n\t\treturn unmarshalVarchar(info, data, value)\n\tcase TypeBoolean:\n\t\treturn unmarshalBool(info, data, value)\n\tcase TypeInt:\n\t\treturn unmarshalInt(info, data, value)\n\tcase TypeBigInt, TypeCounter:\n\t\treturn unmarshalBigInt(info, data, value)\n\tcase TypeVarint:\n\t\treturn unmarshalVarint(info, data, value)\n\tcase TypeSmallInt:\n\t\treturn unmarshalSmallInt(info, data, value)\n\tcase TypeTinyInt:\n\t\treturn unmarshalTinyInt(info, data, value)\n\tcase TypeFloat:\n\t\treturn unmarshalFloat(info, data, value)\n\tcase TypeDouble:\n\t\treturn unmarshalDouble(info, data, value)\n\tcase TypeDecimal:\n\t\treturn unmarshalDecimal(info, data, value)\n\tcase TypeTime:\n\t\treturn unmarshalTime(info, data, value)\n\tcase TypeTimestamp:\n\t\treturn unmarshalTimestamp(info, data, value)\n\tcase TypeList, TypeSet:\n\t\treturn unmarshalList(info, data, value)\n\tcase TypeMap:\n\t\treturn unmarshalMap(info, data, value)\n\tcase TypeTimeUUID:\n\t\treturn unmarshalTimeUUID(info, data, value)\n\tcase TypeUUID:\n\t\treturn unmarshalUUID(info, data, value)\n\tcase TypeInet:\n\t\treturn unmarshalInet(info, data, value)\n\tcase TypeTuple:\n\t\treturn unmarshalTuple(info, data, value)\n\tcase TypeUDT:\n\t\treturn unmarshalUDT(info, data, value)\n\tcase TypeDate:\n\t\treturn unmarshalDate(info, data, value)\n\tcase TypeDuration:\n\t\treturn unmarshalDuration(info, data, value)\n\t}\n\n\t// detect protocol 2 UDT\n\tif strings.HasPrefix(info.Custom(), \"org.apache.cassandra.db.marshal.UserType\") && info.Version() < 3 {\n\t\treturn ErrorUDTUnavailable\n\t}\n\n\t// TODO(tux21b): add the remaining types\n\treturn fmt.Errorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc isNullableValue(value interface{}) bool {\n\tv := reflect.ValueOf(value)\n\treturn v.Kind() == reflect.Ptr && v.Type().Elem().Kind() == reflect.Ptr\n}\n\nfunc isNullData(info TypeInfo, data []byte) bool {\n\treturn data == nil\n}\n\nfunc unmarshalNullable(info TypeInfo, data []byte, value interface{}) error {\n\tvalueRef := reflect.ValueOf(value)\n\n\tif isNullData(info, data) {\n\t\tnilValue := reflect.Zero(valueRef.Type().Elem())\n\t\tvalueRef.Elem().Set(nilValue)\n\t\treturn nil\n\t}\n\n\tnewValue := reflect.New(valueRef.Type().Elem().Elem())\n\tvalueRef.Elem().Set(newValue)\n\treturn Unmarshal(info, data, newValue.Interface())\n}\n\nfunc marshalVarchar(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase string:\n\t\treturn []byte(v), nil\n\tcase []byte:\n\t\treturn v, nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tt := rv.Type()\n\tk := t.Kind()\n\tswitch {\n\tcase k == reflect.String:\n\t\treturn []byte(rv.String()), nil\n\tcase k == reflect.Slice && t.Elem().Kind() == reflect.Uint8:\n\t\treturn rv.Bytes(), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalVarchar(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *string:\n\t\t*v = string(data)\n\t\treturn nil\n\tcase *[]byte:\n\t\tif data != nil {\n\t\t\t*v = append((*v)[:0], data...)\n\t\t} else {\n\t\t\t*v = nil\n\t\t}\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tt := rv.Type()\n\tk := t.Kind()\n\tswitch {\n\tcase k == reflect.String:\n\t\trv.SetString(string(data))\n\t\treturn nil\n\tcase k == reflect.Slice && t.Elem().Kind() == reflect.Uint8:\n\t\tvar dataCopy []byte\n\t\tif data != nil {\n\t\t\tdataCopy = make([]byte, len(data))\n\t\t\tcopy(dataCopy, data)\n\t\t}\n\t\trv.SetBytes(dataCopy)\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc marshalSmallInt(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int16:\n\t\treturn encShort(v), nil\n\tcase uint16:\n\t\treturn encShort(int16(v)), nil\n\tcase int8:\n\t\treturn encShort(int16(v)), nil\n\tcase uint8:\n\t\treturn encShort(int16(v)), nil\n\tcase int:\n\t\tif v > math.MaxInt16 || v < math.MinInt16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase int32:\n\t\tif v > math.MaxInt16 || v < math.MinInt16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase int64:\n\t\tif v > math.MaxInt16 || v < math.MinInt16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase uint:\n\t\tif v > math.MaxUint16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase uint32:\n\t\tif v > math.MaxUint16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase uint64:\n\t\tif v > math.MaxUint16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase string:\n\t\tn, err := strconv.ParseInt(v, 10, 16)\n\t\tif err != nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal %T into %s: %v\", value, info, err)\n\t\t}\n\t\treturn encShort(int16(n)), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tswitch rv := reflect.ValueOf(value); rv.Type().Kind() {\n\tcase reflect.Int, reflect.Int64, reflect.Int32, reflect.Int16, reflect.Int8:\n\t\tv := rv.Int()\n\t\tif v > math.MaxInt16 || v < math.MinInt16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase reflect.Uint, reflect.Uint64, reflect.Uint32, reflect.Uint16, reflect.Uint8:\n\t\tv := rv.Uint()\n\t\tif v > math.MaxUint16 {\n\t\t\treturn nil, marshalErrorf(\"marshal smallint: value %d out of range\", v)\n\t\t}\n\t\treturn encShort(int16(v)), nil\n\tcase reflect.Ptr:\n\t\tif rv.IsNil() {\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc marshalTinyInt(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int8:\n\t\treturn []byte{byte(v)}, nil\n\tcase uint8:\n\t\treturn []byte{byte(v)}, nil\n\tcase int16:\n\t\tif v > math.MaxInt8 || v < math.MinInt8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase uint16:\n\t\tif v > math.MaxUint8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase int:\n\t\tif v > math.MaxInt8 || v < math.MinInt8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase int32:\n\t\tif v > math.MaxInt8 || v < math.MinInt8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase int64:\n\t\tif v > math.MaxInt8 || v < math.MinInt8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase uint:\n\t\tif v > math.MaxUint8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase uint32:\n\t\tif v > math.MaxUint8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase uint64:\n\t\tif v > math.MaxUint8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase string:\n\t\tn, err := strconv.ParseInt(v, 10, 8)\n\t\tif err != nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal %T into %s: %v\", value, info, err)\n\t\t}\n\t\treturn []byte{byte(n)}, nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tswitch rv := reflect.ValueOf(value); rv.Type().Kind() {\n\tcase reflect.Int, reflect.Int64, reflect.Int32, reflect.Int16, reflect.Int8:\n\t\tv := rv.Int()\n\t\tif v > math.MaxInt8 || v < math.MinInt8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase reflect.Uint, reflect.Uint64, reflect.Uint32, reflect.Uint16, reflect.Uint8:\n\t\tv := rv.Uint()\n\t\tif v > math.MaxUint8 {\n\t\t\treturn nil, marshalErrorf(\"marshal tinyint: value %d out of range\", v)\n\t\t}\n\t\treturn []byte{byte(v)}, nil\n\tcase reflect.Ptr:\n\t\tif rv.IsNil() {\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc marshalInt(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int:\n\t\tif v > math.MaxInt32 || v < math.MinInt32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase uint:\n\t\tif v > math.MaxUint32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase int64:\n\t\tif v > math.MaxInt32 || v < math.MinInt32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase uint64:\n\t\tif v > math.MaxUint32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase int32:\n\t\treturn encInt(v), nil\n\tcase uint32:\n\t\treturn encInt(int32(v)), nil\n\tcase int16:\n\t\treturn encInt(int32(v)), nil\n\tcase uint16:\n\t\treturn encInt(int32(v)), nil\n\tcase int8:\n\t\treturn encInt(int32(v)), nil\n\tcase uint8:\n\t\treturn encInt(int32(v)), nil\n\tcase string:\n\t\ti, err := strconv.ParseInt(v, 10, 32)\n\t\tif err != nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal string to int: %s\", err)\n\t\t}\n\t\treturn encInt(int32(i)), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tswitch rv := reflect.ValueOf(value); rv.Type().Kind() {\n\tcase reflect.Int, reflect.Int64, reflect.Int32, reflect.Int16, reflect.Int8:\n\t\tv := rv.Int()\n\t\tif v > math.MaxInt32 || v < math.MinInt32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase reflect.Uint, reflect.Uint64, reflect.Uint32, reflect.Uint16, reflect.Uint8:\n\t\tv := rv.Uint()\n\t\tif v > math.MaxInt32 {\n\t\t\treturn nil, marshalErrorf(\"marshal int: value %d out of range\", v)\n\t\t}\n\t\treturn encInt(int32(v)), nil\n\tcase reflect.Ptr:\n\t\tif rv.IsNil() {\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc encInt(x int32) []byte {\n\treturn []byte{byte(x >> 24), byte(x >> 16), byte(x >> 8), byte(x)}\n}\n\nfunc decInt(x []byte) int32 {\n\tif len(x) != 4 {\n\t\treturn 0\n\t}\n\treturn int32(x[0])<<24 | int32(x[1])<<16 | int32(x[2])<<8 | int32(x[3])\n}\n\nfunc encShort(x int16) []byte {\n\tp := make([]byte, 2)\n\tp[0] = byte(x >> 8)\n\tp[1] = byte(x)\n\treturn p\n}\n\nfunc decShort(p []byte) int16 {\n\tif len(p) != 2 {\n\t\treturn 0\n\t}\n\treturn int16(p[0])<<8 | int16(p[1])\n}\n\nfunc decTiny(p []byte) int8 {\n\tif len(p) != 1 {\n\t\treturn 0\n\t}\n\treturn int8(p[0])\n}\n\nfunc marshalBigInt(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int:\n\t\treturn encBigInt(int64(v)), nil\n\tcase uint:\n\t\tif uint64(v) > math.MaxInt64 {\n\t\t\treturn nil, marshalErrorf(\"marshal bigint: value %d out of range\", v)\n\t\t}\n\t\treturn encBigInt(int64(v)), nil\n\tcase int64:\n\t\treturn encBigInt(v), nil\n\tcase uint64:\n\t\treturn encBigInt(int64(v)), nil\n\tcase int32:\n\t\treturn encBigInt(int64(v)), nil\n\tcase uint32:\n\t\treturn encBigInt(int64(v)), nil\n\tcase int16:\n\t\treturn encBigInt(int64(v)), nil\n\tcase uint16:\n\t\treturn encBigInt(int64(v)), nil\n\tcase int8:\n\t\treturn encBigInt(int64(v)), nil\n\tcase uint8:\n\t\treturn encBigInt(int64(v)), nil\n\tcase big.Int:\n\t\treturn encBigInt2C(&v), nil\n\tcase string:\n\t\ti, err := strconv.ParseInt(value.(string), 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal string to bigint: %s\", err)\n\t\t}\n\t\treturn encBigInt(i), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int, reflect.Int64, reflect.Int32, reflect.Int16, reflect.Int8:\n\t\tv := rv.Int()\n\t\treturn encBigInt(v), nil\n\tcase reflect.Uint, reflect.Uint64, reflect.Uint32, reflect.Uint16, reflect.Uint8:\n\t\tv := rv.Uint()\n\t\tif v > math.MaxInt64 {\n\t\t\treturn nil, marshalErrorf(\"marshal bigint: value %d out of range\", v)\n\t\t}\n\t\treturn encBigInt(int64(v)), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc encBigInt(x int64) []byte {\n\treturn []byte{byte(x >> 56), byte(x >> 48), byte(x >> 40), byte(x >> 32),\n\t\tbyte(x >> 24), byte(x >> 16), byte(x >> 8), byte(x)}\n}\n\nfunc bytesToInt64(data []byte) (ret int64) {\n\tfor i := range data {\n\t\tret |= int64(data[i]) << (8 * uint(len(data)-i-1))\n\t}\n\treturn ret\n}\n\nfunc bytesToUint64(data []byte) (ret uint64) {\n\tfor i := range data {\n\t\tret |= uint64(data[i]) << (8 * uint(len(data)-i-1))\n\t}\n\treturn ret\n}\n\nfunc unmarshalBigInt(info TypeInfo, data []byte, value interface{}) error {\n\treturn unmarshalIntlike(info, decBigInt(data), data, value)\n}\n\nfunc unmarshalInt(info TypeInfo, data []byte, value interface{}) error {\n\treturn unmarshalIntlike(info, int64(decInt(data)), data, value)\n}\n\nfunc unmarshalSmallInt(info TypeInfo, data []byte, value interface{}) error {\n\treturn unmarshalIntlike(info, int64(decShort(data)), data, value)\n}\n\nfunc unmarshalTinyInt(info TypeInfo, data []byte, value interface{}) error {\n\treturn unmarshalIntlike(info, int64(decTiny(data)), data, value)\n}\n\nfunc unmarshalVarint(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase *big.Int:\n\t\treturn unmarshalIntlike(info, 0, data, value)\n\tcase *uint64:\n\t\tif len(data) == 9 && data[0] == 0 {\n\t\t\t*v = bytesToUint64(data[1:])\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif len(data) > 8 {\n\t\treturn unmarshalErrorf(\"unmarshal int: varint value %v out of range for %T (use big.Int)\", data, value)\n\t}\n\n\tint64Val := bytesToInt64(data)\n\tif len(data) > 0 && len(data) < 8 && data[0]&0x80 > 0 {\n\t\tint64Val -= (1 << uint(len(data)*8))\n\t}\n\treturn unmarshalIntlike(info, int64Val, data, value)\n}\n\nfunc marshalVarint(info TypeInfo, value interface{}) ([]byte, error) {\n\tvar (\n\t\tretBytes []byte\n\t\terr      error\n\t)\n\n\tswitch v := value.(type) {\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase uint64:\n\t\tif v > uint64(math.MaxInt64) {\n\t\t\tretBytes = make([]byte, 9)\n\t\t\tbinary.BigEndian.PutUint64(retBytes[1:], v)\n\t\t} else {\n\t\t\tretBytes = make([]byte, 8)\n\t\t\tbinary.BigEndian.PutUint64(retBytes, v)\n\t\t}\n\tdefault:\n\t\tretBytes, err = marshalBigInt(info, value)\n\t}\n\n\tif err == nil {\n\t\t// trim down to most significant byte\n\t\ti := 0\n\t\tfor ; i < len(retBytes)-1; i++ {\n\t\t\tb0 := retBytes[i]\n\t\t\tif b0 != 0 && b0 != 0xFF {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tb1 := retBytes[i+1]\n\t\t\tif b0 == 0 && b1 != 0 {\n\t\t\t\tif b1&0x80 == 0 {\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tif b0 == 0xFF && b1 != 0xFF {\n\t\t\t\tif b1&0x80 > 0 {\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tretBytes = retBytes[i:]\n\t}\n\n\treturn retBytes, err\n}\n\nfunc unmarshalIntlike(info TypeInfo, int64Val int64, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase *int:\n\t\tif ^uint(0) == math.MaxUint32 && (int64Val < math.MinInt32 || int64Val > math.MaxInt32) {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t}\n\t\t*v = int(int64Val)\n\t\treturn nil\n\tcase *uint:\n\t\tunitVal := uint64(int64Val)\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\t*v = uint(unitVal) & 0xFFFFFFFF\n\t\tcase TypeSmallInt:\n\t\t\t*v = uint(unitVal) & 0xFFFF\n\t\tcase TypeTinyInt:\n\t\t\t*v = uint(unitVal) & 0xFF\n\t\tdefault:\n\t\t\tif ^uint(0) == math.MaxUint32 && (int64Val < 0 || int64Val > math.MaxUint32) {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", unitVal, *v)\n\t\t\t}\n\t\t\t*v = uint(unitVal)\n\t\t}\n\t\treturn nil\n\tcase *int64:\n\t\t*v = int64Val\n\t\treturn nil\n\tcase *uint64:\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\t*v = uint64(int64Val) & 0xFFFFFFFF\n\t\tcase TypeSmallInt:\n\t\t\t*v = uint64(int64Val) & 0xFFFF\n\t\tcase TypeTinyInt:\n\t\t\t*v = uint64(int64Val) & 0xFF\n\t\tdefault:\n\t\t\t*v = uint64(int64Val)\n\t\t}\n\t\treturn nil\n\tcase *int32:\n\t\tif int64Val < math.MinInt32 || int64Val > math.MaxInt32 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t}\n\t\t*v = int32(int64Val)\n\t\treturn nil\n\tcase *uint32:\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\t*v = uint32(int64Val) & 0xFFFFFFFF\n\t\tcase TypeSmallInt:\n\t\t\t*v = uint32(int64Val) & 0xFFFF\n\t\tcase TypeTinyInt:\n\t\t\t*v = uint32(int64Val) & 0xFF\n\t\tdefault:\n\t\t\tif int64Val < 0 || int64Val > math.MaxUint32 {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t\t}\n\t\t\t*v = uint32(int64Val) & 0xFFFFFFFF\n\t\t}\n\t\treturn nil\n\tcase *int16:\n\t\tif int64Val < math.MinInt16 || int64Val > math.MaxInt16 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t}\n\t\t*v = int16(int64Val)\n\t\treturn nil\n\tcase *uint16:\n\t\tswitch info.Type() {\n\t\tcase TypeSmallInt:\n\t\t\t*v = uint16(int64Val) & 0xFFFF\n\t\tcase TypeTinyInt:\n\t\t\t*v = uint16(int64Val) & 0xFF\n\t\tdefault:\n\t\t\tif int64Val < 0 || int64Val > math.MaxUint16 {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t\t}\n\t\t\t*v = uint16(int64Val) & 0xFFFF\n\t\t}\n\t\treturn nil\n\tcase *int8:\n\t\tif int64Val < math.MinInt8 || int64Val > math.MaxInt8 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t}\n\t\t*v = int8(int64Val)\n\t\treturn nil\n\tcase *uint8:\n\t\tif info.Type() != TypeTinyInt && (int64Val < 0 || int64Val > math.MaxUint8) {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %T\", int64Val, *v)\n\t\t}\n\t\t*v = uint8(int64Val) & 0xFF\n\t\treturn nil\n\tcase *big.Int:\n\t\tdecBigInt2C(data, v)\n\t\treturn nil\n\tcase *string:\n\t\t*v = strconv.FormatInt(int64Val, 10)\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int:\n\t\tif ^uint(0) == math.MaxUint32 && (int64Val < math.MinInt32 || int64Val > math.MaxInt32) {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range\", int64Val)\n\t\t}\n\t\trv.SetInt(int64Val)\n\t\treturn nil\n\tcase reflect.Int64:\n\t\trv.SetInt(int64Val)\n\t\treturn nil\n\tcase reflect.Int32:\n\t\tif int64Val < math.MinInt32 || int64Val > math.MaxInt32 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range\", int64Val)\n\t\t}\n\t\trv.SetInt(int64Val)\n\t\treturn nil\n\tcase reflect.Int16:\n\t\tif int64Val < math.MinInt16 || int64Val > math.MaxInt16 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range\", int64Val)\n\t\t}\n\t\trv.SetInt(int64Val)\n\t\treturn nil\n\tcase reflect.Int8:\n\t\tif int64Val < math.MinInt8 || int64Val > math.MaxInt8 {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range\", int64Val)\n\t\t}\n\t\trv.SetInt(int64Val)\n\t\treturn nil\n\tcase reflect.Uint:\n\t\tunitVal := uint64(int64Val)\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\trv.SetUint(unitVal & 0xFFFFFFFF)\n\t\tcase TypeSmallInt:\n\t\t\trv.SetUint(unitVal & 0xFFFF)\n\t\tcase TypeTinyInt:\n\t\t\trv.SetUint(unitVal & 0xFF)\n\t\tdefault:\n\t\t\tif ^uint(0) == math.MaxUint32 && (int64Val < 0 || int64Val > math.MaxUint32) {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %s\", unitVal, rv.Type())\n\t\t\t}\n\t\t\trv.SetUint(unitVal)\n\t\t}\n\t\treturn nil\n\tcase reflect.Uint64:\n\t\tunitVal := uint64(int64Val)\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\trv.SetUint(unitVal & 0xFFFFFFFF)\n\t\tcase TypeSmallInt:\n\t\t\trv.SetUint(unitVal & 0xFFFF)\n\t\tcase TypeTinyInt:\n\t\t\trv.SetUint(unitVal & 0xFF)\n\t\tdefault:\n\t\t\trv.SetUint(unitVal)\n\t\t}\n\t\treturn nil\n\tcase reflect.Uint32:\n\t\tunitVal := uint64(int64Val)\n\t\tswitch info.Type() {\n\t\tcase TypeInt:\n\t\t\trv.SetUint(unitVal & 0xFFFFFFFF)\n\t\tcase TypeSmallInt:\n\t\t\trv.SetUint(unitVal & 0xFFFF)\n\t\tcase TypeTinyInt:\n\t\t\trv.SetUint(unitVal & 0xFF)\n\t\tdefault:\n\t\t\tif int64Val < 0 || int64Val > math.MaxUint32 {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %s\", int64Val, rv.Type())\n\t\t\t}\n\t\t\trv.SetUint(unitVal & 0xFFFFFFFF)\n\t\t}\n\t\treturn nil\n\tcase reflect.Uint16:\n\t\tunitVal := uint64(int64Val)\n\t\tswitch info.Type() {\n\t\tcase TypeSmallInt:\n\t\t\trv.SetUint(unitVal & 0xFFFF)\n\t\tcase TypeTinyInt:\n\t\t\trv.SetUint(unitVal & 0xFF)\n\t\tdefault:\n\t\t\tif int64Val < 0 || int64Val > math.MaxUint16 {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %s\", int64Val, rv.Type())\n\t\t\t}\n\t\t\trv.SetUint(unitVal & 0xFFFF)\n\t\t}\n\t\treturn nil\n\tcase reflect.Uint8:\n\t\tif info.Type() != TypeTinyInt && (int64Val < 0 || int64Val > math.MaxUint8) {\n\t\t\treturn unmarshalErrorf(\"unmarshal int: value %d out of range for %s\", int64Val, rv.Type())\n\t\t}\n\t\trv.SetUint(uint64(int64Val) & 0xff)\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc decBigInt(data []byte) int64 {\n\tif len(data) != 8 {\n\t\treturn 0\n\t}\n\treturn int64(data[0])<<56 | int64(data[1])<<48 |\n\t\tint64(data[2])<<40 | int64(data[3])<<32 |\n\t\tint64(data[4])<<24 | int64(data[5])<<16 |\n\t\tint64(data[6])<<8 | int64(data[7])\n}\n\nfunc marshalBool(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase bool:\n\t\treturn encBool(v), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Bool:\n\t\treturn encBool(rv.Bool()), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc encBool(v bool) []byte {\n\tif v {\n\t\treturn []byte{1}\n\t}\n\treturn []byte{0}\n}\n\nfunc unmarshalBool(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *bool:\n\t\t*v = decBool(data)\n\t\treturn nil\n\t}\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tswitch rv.Type().Kind() {\n\tcase reflect.Bool:\n\t\trv.SetBool(decBool(data))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc decBool(v []byte) bool {\n\tif len(v) == 0 {\n\t\treturn false\n\t}\n\treturn v[0] != 0\n}\n\nfunc marshalFloat(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase float32:\n\t\treturn encInt(int32(math.Float32bits(v))), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Float32:\n\t\treturn encInt(int32(math.Float32bits(float32(rv.Float())))), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalFloat(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *float32:\n\t\t*v = math.Float32frombits(uint32(decInt(data)))\n\t\treturn nil\n\t}\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tswitch rv.Type().Kind() {\n\tcase reflect.Float32:\n\t\trv.SetFloat(float64(math.Float32frombits(uint32(decInt(data)))))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc marshalDouble(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase float64:\n\t\treturn encBigInt(int64(math.Float64bits(v))), nil\n\t}\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Float64:\n\t\treturn encBigInt(int64(math.Float64bits(rv.Float()))), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalDouble(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *float64:\n\t\t*v = math.Float64frombits(uint64(decBigInt(data)))\n\t\treturn nil\n\t}\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tswitch rv.Type().Kind() {\n\tcase reflect.Float64:\n\t\trv.SetFloat(math.Float64frombits(uint64(decBigInt(data))))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc marshalDecimal(info TypeInfo, value interface{}) ([]byte, error) {\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase inf.Dec:\n\t\tunscaled := encBigInt2C(v.UnscaledBig())\n\t\tif unscaled == nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n\t\t}\n\n\t\tbuf := make([]byte, 4+len(unscaled))\n\t\tcopy(buf[0:4], encInt(int32(v.Scale())))\n\t\tcopy(buf[4:], unscaled)\n\t\treturn buf, nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalDecimal(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *inf.Dec:\n\t\tif len(data) < 4 {\n\t\t\treturn unmarshalErrorf(\"inf.Dec needs at least 4 bytes, while value has only %d\", len(data))\n\t\t}\n\t\tscale := decInt(data[0:4])\n\t\tunscaled := decBigInt2C(data[4:], nil)\n\t\t*v = *inf.NewDecBig(unscaled, inf.Scale(scale))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\n// decBigInt2C sets the value of n to the big-endian two's complement\n// value stored in the given data. If data[0]&80 != 0, the number\n// is negative. If data is empty, the result will be 0.\nfunc decBigInt2C(data []byte, n *big.Int) *big.Int {\n\tif n == nil {\n\t\tn = new(big.Int)\n\t}\n\tn.SetBytes(data)\n\tif len(data) > 0 && data[0]&0x80 > 0 {\n\t\tn.Sub(n, new(big.Int).Lsh(bigOne, uint(len(data))*8))\n\t}\n\treturn n\n}\n\n// encBigInt2C returns the big-endian two's complement\n// form of n.\nfunc encBigInt2C(n *big.Int) []byte {\n\tswitch n.Sign() {\n\tcase 0:\n\t\treturn []byte{0}\n\tcase 1:\n\t\tb := n.Bytes()\n\t\tif b[0]&0x80 > 0 {\n\t\t\tb = append([]byte{0}, b...)\n\t\t}\n\t\treturn b\n\tcase -1:\n\t\tlength := uint(n.BitLen()/8+1) * 8\n\t\tb := new(big.Int).Add(n, new(big.Int).Lsh(bigOne, length)).Bytes()\n\t\t// When the most significant bit is on a byte\n\t\t// boundary, we can get some extra significant\n\t\t// bits, so strip them off when that happens.\n\t\tif len(b) >= 2 && b[0] == 0xff && b[1]&0x80 != 0 {\n\t\t\tb = b[1:]\n\t\t}\n\t\treturn b\n\t}\n\treturn nil\n}\n\nfunc marshalTime(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int64:\n\t\treturn encBigInt(v), nil\n\tcase time.Duration:\n\t\treturn encBigInt(v.Nanoseconds()), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int64:\n\t\treturn encBigInt(rv.Int()), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc marshalTimestamp(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int64:\n\t\treturn encBigInt(v), nil\n\tcase time.Time:\n\t\tif v.IsZero() {\n\t\t\treturn []byte{}, nil\n\t\t}\n\t\tx := int64(v.UTC().Unix()*1e3) + int64(v.UTC().Nanosecond()/1e6)\n\t\treturn encBigInt(x), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int64:\n\t\treturn encBigInt(rv.Int()), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalTime(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *int64:\n\t\t*v = decBigInt(data)\n\t\treturn nil\n\tcase *time.Duration:\n\t\t*v = time.Duration(decBigInt(data))\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int64:\n\t\trv.SetInt(decBigInt(data))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc unmarshalTimestamp(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *int64:\n\t\t*v = decBigInt(data)\n\t\treturn nil\n\tcase *time.Time:\n\t\tif len(data) == 0 {\n\t\t\t*v = time.Time{}\n\t\t\treturn nil\n\t\t}\n\t\tx := decBigInt(data)\n\t\tsec := x / 1000\n\t\tnsec := (x - sec*1000) * 1000000\n\t\t*v = time.Unix(sec, nsec).In(time.UTC)\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int64:\n\t\trv.SetInt(decBigInt(data))\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nconst millisecondsInADay int64 = 24 * 60 * 60 * 1000\n\nfunc marshalDate(info TypeInfo, value interface{}) ([]byte, error) {\n\tvar timestamp int64\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int64:\n\t\ttimestamp = v\n\t\tx := timestamp/millisecondsInADay + int64(1<<31)\n\t\treturn encInt(int32(x)), nil\n\tcase time.Time:\n\t\tif v.IsZero() {\n\t\t\treturn []byte{}, nil\n\t\t}\n\t\ttimestamp = int64(v.UTC().Unix()*1e3) + int64(v.UTC().Nanosecond()/1e6)\n\t\tx := timestamp/millisecondsInADay + int64(1<<31)\n\t\treturn encInt(int32(x)), nil\n\tcase *time.Time:\n\t\tif v.IsZero() {\n\t\t\treturn []byte{}, nil\n\t\t}\n\t\ttimestamp = int64(v.UTC().Unix()*1e3) + int64(v.UTC().Nanosecond()/1e6)\n\t\tx := timestamp/millisecondsInADay + int64(1<<31)\n\t\treturn encInt(int32(x)), nil\n\tcase string:\n\t\tif v == \"\" {\n\t\t\treturn []byte{}, nil\n\t\t}\n\t\tt, err := time.Parse(\"2006-01-02\", v)\n\t\tif err != nil {\n\t\t\treturn nil, marshalErrorf(\"can not marshal %T into %s, date layout must be '2006-01-02'\", value, info)\n\t\t}\n\t\ttimestamp = int64(t.UTC().Unix()*1e3) + int64(t.UTC().Nanosecond()/1e6)\n\t\tx := timestamp/millisecondsInADay + int64(1<<31)\n\t\treturn encInt(int32(x)), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalDate(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *time.Time:\n\t\tif len(data) == 0 {\n\t\t\t*v = time.Time{}\n\t\t\treturn nil\n\t\t}\n\t\tvar origin uint32 = 1 << 31\n\t\tvar current uint32 = binary.BigEndian.Uint32(data)\n\t\ttimestamp := (int64(current) - int64(origin)) * millisecondsInADay\n\t\t*v = time.UnixMilli(timestamp).In(time.UTC)\n\t\treturn nil\n\tcase *string:\n\t\tif len(data) == 0 {\n\t\t\t*v = \"\"\n\t\t\treturn nil\n\t\t}\n\t\tvar origin uint32 = 1 << 31\n\t\tvar current uint32 = binary.BigEndian.Uint32(data)\n\t\ttimestamp := (int64(current) - int64(origin)) * millisecondsInADay\n\t\t*v = time.UnixMilli(timestamp).In(time.UTC).Format(\"2006-01-02\")\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc marshalDuration(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase int64:\n\t\treturn encVints(0, 0, v), nil\n\tcase time.Duration:\n\t\treturn encVints(0, 0, v.Nanoseconds()), nil\n\tcase string:\n\t\td, err := time.ParseDuration(v)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn encVints(0, 0, d.Nanoseconds()), nil\n\tcase Duration:\n\t\treturn encVints(v.Months, v.Days, v.Nanoseconds), nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tswitch rv.Type().Kind() {\n\tcase reflect.Int64:\n\t\treturn encBigInt(rv.Int()), nil\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalDuration(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *Duration:\n\t\tif len(data) == 0 {\n\t\t\t*v = Duration{\n\t\t\t\tMonths:      0,\n\t\t\t\tDays:        0,\n\t\t\t\tNanoseconds: 0,\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\tmonths, days, nanos, err := decVints(data)\n\t\tif err != nil {\n\t\t\treturn unmarshalErrorf(\"failed to unmarshal %s into %T: %s\", info, value, err.Error())\n\t\t}\n\t\t*v = Duration{\n\t\t\tMonths:      months,\n\t\t\tDays:        days,\n\t\t\tNanoseconds: nanos,\n\t\t}\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc decVints(data []byte) (int32, int32, int64, error) {\n\tmonth, i, err := decVint(data, 0)\n\tif err != nil {\n\t\treturn 0, 0, 0, fmt.Errorf(\"failed to extract month: %s\", err.Error())\n\t}\n\tdays, i, err := decVint(data, i)\n\tif err != nil {\n\t\treturn 0, 0, 0, fmt.Errorf(\"failed to extract days: %s\", err.Error())\n\t}\n\tnanos, _, err := decVint(data, i)\n\tif err != nil {\n\t\treturn 0, 0, 0, fmt.Errorf(\"failed to extract nanoseconds: %s\", err.Error())\n\t}\n\treturn int32(month), int32(days), nanos, err\n}\n\nfunc decVint(data []byte, start int) (int64, int, error) {\n\tif len(data) <= start {\n\t\treturn 0, 0, errors.New(\"unexpected eof\")\n\t}\n\tfirstByte := data[start]\n\tif firstByte&0x80 == 0 {\n\t\treturn decIntZigZag(uint64(firstByte)), start + 1, nil\n\t}\n\tnumBytes := bits.LeadingZeros32(uint32(^firstByte)) - 24\n\tret := uint64(firstByte & (0xff >> uint(numBytes)))\n\tif len(data) < start+numBytes+1 {\n\t\treturn 0, 0, fmt.Errorf(\"data expect to have %d bytes, but it has only %d\", start+numBytes+1, len(data))\n\t}\n\tfor i := start; i < start+numBytes; i++ {\n\t\tret <<= 8\n\t\tret |= uint64(data[i+1] & 0xff)\n\t}\n\treturn decIntZigZag(ret), start + numBytes + 1, nil\n}\n\nfunc decIntZigZag(n uint64) int64 {\n\treturn int64((n >> 1) ^ -(n & 1))\n}\n\nfunc encIntZigZag(n int64) uint64 {\n\treturn uint64((n >> 63) ^ (n << 1))\n}\n\nfunc encVints(months int32, seconds int32, nanos int64) []byte {\n\tbuf := append(encVint(int64(months)), encVint(int64(seconds))...)\n\treturn append(buf, encVint(nanos)...)\n}\n\nfunc encVint(v int64) []byte {\n\tvEnc := encIntZigZag(v)\n\tlead0 := bits.LeadingZeros64(vEnc)\n\tnumBytes := (639 - lead0*9) >> 6\n\n\t// It can be 1 or 0 is v ==0\n\tif numBytes <= 1 {\n\t\treturn []byte{byte(vEnc)}\n\t}\n\textraBytes := numBytes - 1\n\tvar buf = make([]byte, numBytes)\n\tfor i := extraBytes; i >= 0; i-- {\n\t\tbuf[i] = byte(vEnc)\n\t\tvEnc >>= 8\n\t}\n\tbuf[0] |= byte(^(0xff >> uint(extraBytes)))\n\treturn buf\n}\n\nfunc writeCollectionSize(info CollectionType, n int, buf *bytes.Buffer) error {\n\tif info.proto > protoVersion2 {\n\t\tif n > math.MaxInt32 {\n\t\t\treturn marshalErrorf(\"marshal: collection too large\")\n\t\t}\n\n\t\tbuf.WriteByte(byte(n >> 24))\n\t\tbuf.WriteByte(byte(n >> 16))\n\t\tbuf.WriteByte(byte(n >> 8))\n\t\tbuf.WriteByte(byte(n))\n\t} else {\n\t\tif n > math.MaxUint16 {\n\t\t\treturn marshalErrorf(\"marshal: collection too large\")\n\t\t}\n\n\t\tbuf.WriteByte(byte(n >> 8))\n\t\tbuf.WriteByte(byte(n))\n\t}\n\n\treturn nil\n}\n\nfunc marshalList(info TypeInfo, value interface{}) ([]byte, error) {\n\tlistInfo, ok := info.(CollectionType)\n\tif !ok {\n\t\treturn nil, marshalErrorf(\"marshal: can not marshal non collection type into list\")\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t} else if _, ok := value.(unsetColumn); ok {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tt := rv.Type()\n\tk := t.Kind()\n\tif k == reflect.Slice && rv.IsNil() {\n\t\treturn nil, nil\n\t}\n\n\tswitch k {\n\tcase reflect.Slice, reflect.Array:\n\t\tbuf := &bytes.Buffer{}\n\t\tn := rv.Len()\n\n\t\tif err := writeCollectionSize(listInfo, n, buf); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\titem, err := Marshal(listInfo.Elem, rv.Index(i).Interface())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\titemLen := len(item)\n\t\t\t// Set the value to null for supported protocols\n\t\t\tif item == nil && listInfo.proto > protoVersion2 {\n\t\t\t\titemLen = -1\n\t\t\t}\n\t\t\tif err := writeCollectionSize(listInfo, itemLen, buf); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tbuf.Write(item)\n\t\t}\n\t\treturn buf.Bytes(), nil\n\tcase reflect.Map:\n\t\telem := t.Elem()\n\t\tif elem.Kind() == reflect.Struct && elem.NumField() == 0 {\n\t\t\trkeys := rv.MapKeys()\n\t\t\tkeys := make([]interface{}, len(rkeys))\n\t\t\tfor i := 0; i < len(keys); i++ {\n\t\t\t\tkeys[i] = rkeys[i].Interface()\n\t\t\t}\n\t\t\treturn marshalList(listInfo, keys)\n\t\t}\n\t}\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc readCollectionSize(info CollectionType, data []byte) (size, read int, err error) {\n\tif info.proto > protoVersion2 {\n\t\tif len(data) < 4 {\n\t\t\treturn 0, 0, unmarshalErrorf(\"unmarshal list: unexpected eof\")\n\t\t}\n\t\tsize = int(int32(data[0])<<24 | int32(data[1])<<16 | int32(data[2])<<8 | int32(data[3]))\n\t\tread = 4\n\t} else {\n\t\tif len(data) < 2 {\n\t\t\treturn 0, 0, unmarshalErrorf(\"unmarshal list: unexpected eof\")\n\t\t}\n\t\tsize = int(data[0])<<8 | int(data[1])\n\t\tread = 2\n\t}\n\treturn\n}\n\nfunc unmarshalList(info TypeInfo, data []byte, value interface{}) error {\n\tlistInfo, ok := info.(CollectionType)\n\tif !ok {\n\t\treturn unmarshalErrorf(\"unmarshal: can not unmarshal none collection type into list\")\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tt := rv.Type()\n\tk := t.Kind()\n\n\tswitch k {\n\tcase reflect.Slice, reflect.Array:\n\t\tif data == nil {\n\t\t\tif k == reflect.Array {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal list: can not store nil in array value\")\n\t\t\t}\n\t\t\tif rv.IsNil() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\trv.Set(reflect.Zero(t))\n\t\t\treturn nil\n\t\t}\n\t\tn, p, err := readCollectionSize(listInfo, data)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdata = data[p:]\n\t\tif k == reflect.Array {\n\t\t\tif rv.Len() != n {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal list: array with wrong size\")\n\t\t\t}\n\t\t} else {\n\t\t\trv.Set(reflect.MakeSlice(t, n, n))\n\t\t}\n\t\tfor i := 0; i < n; i++ {\n\t\t\tm, p, err := readCollectionSize(listInfo, data)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdata = data[p:]\n\t\t\t// In case m < 0, the value is null, and unmarshalData should be nil.\n\t\t\tvar unmarshalData []byte\n\t\t\tif m >= 0 {\n\t\t\t\tif len(data) < m {\n\t\t\t\t\treturn unmarshalErrorf(\"unmarshal list: unexpected eof\")\n\t\t\t\t}\n\t\t\t\tunmarshalData = data[:m]\n\t\t\t\tdata = data[m:]\n\t\t\t}\n\t\t\tif err := Unmarshal(listInfo.Elem, unmarshalData, rv.Index(i).Addr().Interface()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n}\n\nfunc marshalMap(info TypeInfo, value interface{}) ([]byte, error) {\n\tmapInfo, ok := info.(CollectionType)\n\tif !ok {\n\t\treturn nil, marshalErrorf(\"marshal: can not marshal none collection type into map\")\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t} else if _, ok := value.(unsetColumn); ok {\n\t\treturn nil, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\n\tt := rv.Type()\n\tif t.Kind() != reflect.Map {\n\t\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n\t}\n\n\tif rv.IsNil() {\n\t\treturn nil, nil\n\t}\n\n\tbuf := &bytes.Buffer{}\n\tn := rv.Len()\n\n\tif err := writeCollectionSize(mapInfo, n, buf); err != nil {\n\t\treturn nil, err\n\t}\n\n\tkeys := rv.MapKeys()\n\tfor _, key := range keys {\n\t\titem, err := Marshal(mapInfo.Key, key.Interface())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\titemLen := len(item)\n\t\t// Set the key to null for supported protocols\n\t\tif item == nil && mapInfo.proto > protoVersion2 {\n\t\t\titemLen = -1\n\t\t}\n\t\tif err := writeCollectionSize(mapInfo, itemLen, buf); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tbuf.Write(item)\n\n\t\titem, err = Marshal(mapInfo.Elem, rv.MapIndex(key).Interface())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\titemLen = len(item)\n\t\t// Set the value to null for supported protocols\n\t\tif item == nil && mapInfo.proto > protoVersion2 {\n\t\t\titemLen = -1\n\t\t}\n\t\tif err := writeCollectionSize(mapInfo, itemLen, buf); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tbuf.Write(item)\n\t}\n\treturn buf.Bytes(), nil\n}\n\nfunc unmarshalMap(info TypeInfo, data []byte, value interface{}) error {\n\tmapInfo, ok := info.(CollectionType)\n\tif !ok {\n\t\treturn unmarshalErrorf(\"unmarshal: can not unmarshal none collection type into map\")\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\trv = rv.Elem()\n\tt := rv.Type()\n\tif t.Kind() != reflect.Map {\n\t\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n\t}\n\tif data == nil {\n\t\trv.Set(reflect.Zero(t))\n\t\treturn nil\n\t}\n\tn, p, err := readCollectionSize(mapInfo, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif n < 0 {\n\t\treturn unmarshalErrorf(\"negative map size %d\", n)\n\t}\n\trv.Set(reflect.MakeMapWithSize(t, n))\n\tdata = data[p:]\n\tfor i := 0; i < n; i++ {\n\t\tm, p, err := readCollectionSize(mapInfo, data)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdata = data[p:]\n\t\tkey := reflect.New(t.Key())\n\t\t// In case m < 0, the key is null, and unmarshalData should be nil.\n\t\tvar unmarshalData []byte\n\t\tif m >= 0 {\n\t\t\tif len(data) < m {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal map: unexpected eof\")\n\t\t\t}\n\t\t\tunmarshalData = data[:m]\n\t\t\tdata = data[m:]\n\t\t}\n\t\tif err := Unmarshal(mapInfo.Key, unmarshalData, key.Interface()); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tm, p, err = readCollectionSize(mapInfo, data)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdata = data[p:]\n\t\tval := reflect.New(t.Elem())\n\n\t\t// In case m < 0, the value is null, and unmarshalData should be nil.\n\t\tunmarshalData = nil\n\t\tif m >= 0 {\n\t\t\tif len(data) < m {\n\t\t\t\treturn unmarshalErrorf(\"unmarshal map: unexpected eof\")\n\t\t\t}\n\t\t\tunmarshalData = data[:m]\n\t\t\tdata = data[m:]\n\t\t}\n\t\tif err := Unmarshal(mapInfo.Elem, unmarshalData, val.Interface()); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trv.SetMapIndex(key.Elem(), val.Elem())\n\t}\n\treturn nil\n}\n\nfunc marshalUUID(info TypeInfo, value interface{}) ([]byte, error) {\n\tswitch val := value.(type) {\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase UUID:\n\t\treturn val.Bytes(), nil\n\tcase [16]byte:\n\t\treturn val[:], nil\n\tcase []byte:\n\t\tif len(val) != 16 {\n\t\t\treturn nil, marshalErrorf(\"can not marshal []byte %d bytes long into %s, must be exactly 16 bytes long\", len(val), info)\n\t\t}\n\t\treturn val, nil\n\tcase string:\n\t\tb, err := ParseUUID(val)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b[:], nil\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\treturn nil, marshalErrorf(\"can not marshal %T into %s\", value, info)\n}\n\nfunc unmarshalUUID(info TypeInfo, data []byte, value interface{}) error {\n\tif len(data) == 0 {\n\t\tswitch v := value.(type) {\n\t\tcase *string:\n\t\t\t*v = \"\"\n\t\tcase *[]byte:\n\t\t\t*v = nil\n\t\tcase *UUID:\n\t\t\t*v = UUID{}\n\t\tdefault:\n\t\t\treturn unmarshalErrorf(\"can not unmarshal X %s into %T\", info, value)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tif len(data) != 16 {\n\t\treturn unmarshalErrorf(\"unable to parse UUID: UUIDs must be exactly 16 bytes long\")\n\t}\n\n\tswitch v := value.(type) {\n\tcase *[16]byte:\n\t\tcopy((*v)[:], data)\n\t\treturn nil\n\tcase *UUID:\n\t\tcopy((*v)[:], data)\n\t\treturn nil\n\t}\n\n\tu, err := UUIDFromBytes(data)\n\tif err != nil {\n\t\treturn unmarshalErrorf(\"unable to parse UUID: %s\", err)\n\t}\n\n\tswitch v := value.(type) {\n\tcase *string:\n\t\t*v = u.String()\n\t\treturn nil\n\tcase *[]byte:\n\t\t*v = u[:]\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"can not unmarshal X %s into %T\", info, value)\n}\n\nfunc unmarshalTimeUUID(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *time.Time:\n\t\tid, err := UUIDFromBytes(data)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t} else if id.Version() != 1 {\n\t\t\treturn unmarshalErrorf(\"invalid timeuuid\")\n\t\t}\n\t\t*v = id.Time()\n\t\treturn nil\n\tdefault:\n\t\treturn unmarshalUUID(info, data, value)\n\t}\n}\n\nfunc marshalInet(info TypeInfo, value interface{}) ([]byte, error) {\n\t// we return either the 4 or 16 byte representation of an\n\t// ip address here otherwise the db value will be prefixed\n\t// with the remaining byte values e.g. ::ffff:127.0.0.1 and not 127.0.0.1\n\tswitch val := value.(type) {\n\tcase unsetColumn:\n\t\treturn nil, nil\n\tcase net.IP:\n\t\tt := val.To4()\n\t\tif t == nil {\n\t\t\treturn val.To16(), nil\n\t\t}\n\t\treturn t, nil\n\tcase string:\n\t\tb := net.ParseIP(val)\n\t\tif b != nil {\n\t\t\tt := b.To4()\n\t\t\tif t == nil {\n\t\t\t\treturn b.To16(), nil\n\t\t\t}\n\t\t\treturn t, nil\n\t\t}\n\t\treturn nil, marshalErrorf(\"cannot marshal. invalid ip string %s\", val)\n\t}\n\n\tif value == nil {\n\t\treturn nil, nil\n\t}\n\n\treturn nil, marshalErrorf(\"cannot marshal %T into %s\", value, info)\n}\n\nfunc unmarshalInet(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase *net.IP:\n\t\tif x := len(data); !(x == 4 || x == 16) {\n\t\t\treturn unmarshalErrorf(\"cannot unmarshal %s into %T: invalid sized IP: got %d bytes not 4 or 16\", info, value, x)\n\t\t}\n\t\tbuf := copyBytes(data)\n\t\tip := net.IP(buf)\n\t\tif v4 := ip.To4(); v4 != nil {\n\t\t\t*v = v4\n\t\t\treturn nil\n\t\t}\n\t\t*v = ip\n\t\treturn nil\n\tcase *string:\n\t\tif len(data) == 0 {\n\t\t\t*v = \"\"\n\t\t\treturn nil\n\t\t}\n\t\tip := net.IP(data)\n\t\tif v4 := ip.To4(); v4 != nil {\n\t\t\t*v = v4.String()\n\t\t\treturn nil\n\t\t}\n\t\t*v = ip.String()\n\t\treturn nil\n\t}\n\treturn unmarshalErrorf(\"cannot unmarshal %s into %T\", info, value)\n}\n\nfunc marshalTuple(info TypeInfo, value interface{}) ([]byte, error) {\n\ttuple := info.(TupleTypeInfo)\n\tswitch v := value.(type) {\n\tcase unsetColumn:\n\t\treturn nil, unmarshalErrorf(\"Invalid request: UnsetValue is unsupported for tuples\")\n\tcase []interface{}:\n\t\tif len(v) != len(tuple.Elems) {\n\t\t\treturn nil, unmarshalErrorf(\"cannont marshal tuple: wrong number of elements\")\n\t\t}\n\n\t\tvar buf []byte\n\t\tfor i, elem := range v {\n\t\t\tif elem == nil {\n\t\t\t\tbuf = appendInt(buf, int32(-1))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdata, err := Marshal(tuple.Elems[i], elem)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tn := len(data)\n\t\t\tbuf = appendInt(buf, int32(n))\n\t\t\tbuf = append(buf, data...)\n\t\t}\n\n\t\treturn buf, nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tt := rv.Type()\n\tk := t.Kind()\n\n\tswitch k {\n\tcase reflect.Struct:\n\t\tif v := t.NumField(); v != len(tuple.Elems) {\n\t\t\treturn nil, marshalErrorf(\"can not marshal tuple into struct %v, not enough fields have %d need %d\", t, v, len(tuple.Elems))\n\t\t}\n\n\t\tvar buf []byte\n\t\tfor i, elem := range tuple.Elems {\n\t\t\tfield := rv.Field(i)\n\n\t\t\tif field.Kind() == reflect.Ptr && field.IsNil() {\n\t\t\t\tbuf = appendInt(buf, int32(-1))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdata, err := Marshal(elem, field.Interface())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tn := len(data)\n\t\t\tbuf = appendInt(buf, int32(n))\n\t\t\tbuf = append(buf, data...)\n\t\t}\n\n\t\treturn buf, nil\n\tcase reflect.Slice, reflect.Array:\n\t\tsize := rv.Len()\n\t\tif size != len(tuple.Elems) {\n\t\t\treturn nil, marshalErrorf(\"can not marshal tuple into %v of length %d need %d elements\", k, size, len(tuple.Elems))\n\t\t}\n\n\t\tvar buf []byte\n\t\tfor i, elem := range tuple.Elems {\n\t\t\titem := rv.Index(i)\n\n\t\t\tif item.Kind() == reflect.Ptr && item.IsNil() {\n\t\t\t\tbuf = appendInt(buf, int32(-1))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdata, err := Marshal(elem, item.Interface())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tn := len(data)\n\t\t\tbuf = appendInt(buf, int32(n))\n\t\t\tbuf = append(buf, data...)\n\t\t}\n\n\t\treturn buf, nil\n\t}\n\n\treturn nil, marshalErrorf(\"cannot marshal %T into %s\", value, tuple)\n}\n\nfunc readBytes(p []byte) ([]byte, []byte) {\n\t// TODO: really should use a framer\n\tsize := readInt(p)\n\tp = p[4:]\n\tif size < 0 {\n\t\treturn nil, p\n\t}\n\treturn p[:size], p[size:]\n}\n\n// currently only support unmarshal into a list of values, this makes it possible\n// to support tuples without changing the query API. In the future this can be extend\n// to allow unmarshalling into custom tuple types.\nfunc unmarshalTuple(info TypeInfo, data []byte, value interface{}) error {\n\tif v, ok := value.(Unmarshaler); ok {\n\t\treturn v.UnmarshalCQL(info, data)\n\t}\n\n\ttuple := info.(TupleTypeInfo)\n\tswitch v := value.(type) {\n\tcase []interface{}:\n\t\tfor i, elem := range tuple.Elems {\n\t\t\t// each element inside data is a [bytes]\n\t\t\tvar p []byte\n\t\t\tif len(data) >= 4 {\n\t\t\t\tp, data = readBytes(data)\n\t\t\t}\n\t\t\terr := Unmarshal(elem, p, v[i])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\n\trv = rv.Elem()\n\tt := rv.Type()\n\tk := t.Kind()\n\n\tswitch k {\n\tcase reflect.Struct:\n\t\tif v := t.NumField(); v != len(tuple.Elems) {\n\t\t\treturn unmarshalErrorf(\"can not unmarshal tuple into struct %v, not enough fields have %d need %d\", t, v, len(tuple.Elems))\n\t\t}\n\n\t\tfor i, elem := range tuple.Elems {\n\t\t\tvar p []byte\n\t\t\tif len(data) >= 4 {\n\t\t\t\tp, data = readBytes(data)\n\t\t\t}\n\n\t\t\tv, err := elem.NewWithError()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := Unmarshal(elem, p, v); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tswitch rv.Field(i).Kind() {\n\t\t\tcase reflect.Ptr:\n\t\t\t\tif p != nil {\n\t\t\t\t\trv.Field(i).Set(reflect.ValueOf(v))\n\t\t\t\t} else {\n\t\t\t\t\trv.Field(i).Set(reflect.Zero(reflect.TypeOf(v)))\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\trv.Field(i).Set(reflect.ValueOf(v).Elem())\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\tcase reflect.Slice, reflect.Array:\n\t\tif k == reflect.Array {\n\t\t\tsize := rv.Len()\n\t\t\tif size != len(tuple.Elems) {\n\t\t\t\treturn unmarshalErrorf(\"can not unmarshal tuple into array of length %d need %d elements\", size, len(tuple.Elems))\n\t\t\t}\n\t\t} else {\n\t\t\trv.Set(reflect.MakeSlice(t, len(tuple.Elems), len(tuple.Elems)))\n\t\t}\n\n\t\tfor i, elem := range tuple.Elems {\n\t\t\tvar p []byte\n\t\t\tif len(data) >= 4 {\n\t\t\t\tp, data = readBytes(data)\n\t\t\t}\n\n\t\t\tv, err := elem.NewWithError()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := Unmarshal(elem, p, v); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tswitch rv.Index(i).Kind() {\n\t\t\tcase reflect.Ptr:\n\t\t\t\tif p != nil {\n\t\t\t\t\trv.Index(i).Set(reflect.ValueOf(v))\n\t\t\t\t} else {\n\t\t\t\t\trv.Index(i).Set(reflect.Zero(reflect.TypeOf(v)))\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\trv.Index(i).Set(reflect.ValueOf(v).Elem())\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\treturn unmarshalErrorf(\"cannot unmarshal %s into %T\", info, value)\n}\n\n// UDTMarshaler is an interface which should be implemented by users wishing to\n// handle encoding UDT types to sent to Cassandra. Note: due to current implentations\n// methods defined for this interface must be value receivers not pointer receivers.\ntype UDTMarshaler interface {\n\t// MarshalUDT will be called for each field in the the UDT returned by Cassandra,\n\t// the implementor should marshal the type to return by for example calling\n\t// Marshal.\n\tMarshalUDT(name string, info TypeInfo) ([]byte, error)\n}\n\n// UDTUnmarshaler should be implemented by users wanting to implement custom\n// UDT unmarshaling.\ntype UDTUnmarshaler interface {\n\t// UnmarshalUDT will be called for each field in the UDT return by Cassandra,\n\t// the implementor should unmarshal the data into the value of their chosing,\n\t// for example by calling Unmarshal.\n\tUnmarshalUDT(name string, info TypeInfo, data []byte) error\n}\n\nfunc marshalUDT(info TypeInfo, value interface{}) ([]byte, error) {\n\tudt := info.(UDTTypeInfo)\n\n\tswitch v := value.(type) {\n\tcase Marshaler:\n\t\treturn v.MarshalCQL(info)\n\tcase unsetColumn:\n\t\treturn nil, unmarshalErrorf(\"invalid request: UnsetValue is unsupported for user defined types\")\n\tcase UDTMarshaler:\n\t\tvar buf []byte\n\t\tfor _, e := range udt.Elements {\n\t\t\tdata, err := v.MarshalUDT(e.Name, e.Type)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tbuf = appendBytes(buf, data)\n\t\t}\n\n\t\treturn buf, nil\n\tcase map[string]interface{}:\n\t\tvar buf []byte\n\t\tfor _, e := range udt.Elements {\n\t\t\tval, ok := v[e.Name]\n\n\t\t\tvar data []byte\n\n\t\t\tif ok {\n\t\t\t\tvar err error\n\t\t\t\tdata, err = Marshal(e.Type, val)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbuf = appendBytes(buf, data)\n\t\t}\n\n\t\treturn buf, nil\n\t}\n\n\tk := reflect.ValueOf(value)\n\tif k.Kind() == reflect.Ptr {\n\t\tif k.IsNil() {\n\t\t\treturn nil, marshalErrorf(\"cannot marshal %T into %s\", value, info)\n\t\t}\n\t\tk = k.Elem()\n\t}\n\n\tif k.Kind() != reflect.Struct || !k.IsValid() {\n\t\treturn nil, marshalErrorf(\"cannot marshal %T into %s\", value, info)\n\t}\n\n\tfields := make(map[string]reflect.Value)\n\tt := reflect.TypeOf(value)\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tsf := t.Field(i)\n\n\t\tif tag := sf.Tag.Get(\"cql\"); tag != \"\" {\n\t\t\tfields[tag] = k.Field(i)\n\t\t}\n\t}\n\n\tvar buf []byte\n\tfor _, e := range udt.Elements {\n\t\tf, ok := fields[e.Name]\n\t\tif !ok {\n\t\t\tf = k.FieldByName(e.Name)\n\t\t}\n\n\t\tvar data []byte\n\t\tif f.IsValid() && f.CanInterface() {\n\t\t\tvar err error\n\t\t\tdata, err = Marshal(e.Type, f.Interface())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tbuf = appendBytes(buf, data)\n\t}\n\n\treturn buf, nil\n}\n\nfunc unmarshalUDT(info TypeInfo, data []byte, value interface{}) error {\n\tswitch v := value.(type) {\n\tcase Unmarshaler:\n\t\treturn v.UnmarshalCQL(info, data)\n\tcase UDTUnmarshaler:\n\t\tudt := info.(UDTTypeInfo)\n\n\t\tfor id, e := range udt.Elements {\n\t\t\tif len(data) == 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif len(data) < 4 {\n\t\t\t\treturn unmarshalErrorf(\"can not unmarshal %s: field [%d]%s: unexpected eof\", info, id, e.Name)\n\t\t\t}\n\n\t\t\tvar p []byte\n\t\t\tp, data = readBytes(data)\n\t\t\tif err := v.UnmarshalUDT(e.Name, e.Type, p); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\tcase *map[string]interface{}:\n\t\tudt := info.(UDTTypeInfo)\n\n\t\trv := reflect.ValueOf(value)\n\t\tif rv.Kind() != reflect.Ptr {\n\t\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t\t}\n\n\t\trv = rv.Elem()\n\t\tt := rv.Type()\n\t\tif t.Kind() != reflect.Map {\n\t\t\treturn unmarshalErrorf(\"can not unmarshal %s into %T\", info, value)\n\t\t} else if data == nil {\n\t\t\trv.Set(reflect.Zero(t))\n\t\t\treturn nil\n\t\t}\n\n\t\trv.Set(reflect.MakeMap(t))\n\t\tm := *v\n\n\t\tfor id, e := range udt.Elements {\n\t\t\tif len(data) == 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif len(data) < 4 {\n\t\t\t\treturn unmarshalErrorf(\"can not unmarshal %s: field [%d]%s: unexpected eof\", info, id, e.Name)\n\t\t\t}\n\n\t\t\tvalType, err := goType(e.Type)\n\t\t\tif err != nil {\n\t\t\t\treturn unmarshalErrorf(\"can not unmarshal %s: %v\", info, err)\n\t\t\t}\n\n\t\t\tval := reflect.New(valType)\n\n\t\t\tvar p []byte\n\t\t\tp, data = readBytes(data)\n\n\t\t\tif err := Unmarshal(e.Type, p, val.Interface()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tm[e.Name] = val.Elem().Interface()\n\t\t}\n\n\t\treturn nil\n\t}\n\n\trv := reflect.ValueOf(value)\n\tif rv.Kind() != reflect.Ptr {\n\t\treturn unmarshalErrorf(\"can not unmarshal into non-pointer %T\", value)\n\t}\n\tk := rv.Elem()\n\tif k.Kind() != reflect.Struct || !k.IsValid() {\n\t\treturn unmarshalErrorf(\"cannot unmarshal %s into %T\", info, value)\n\t}\n\n\tif len(data) == 0 {\n\t\tif k.CanSet() {\n\t\t\tk.Set(reflect.Zero(k.Type()))\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tt := k.Type()\n\tfields := make(map[string]reflect.Value, t.NumField())\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tsf := t.Field(i)\n\n\t\tif tag := sf.Tag.Get(\"cql\"); tag != \"\" {\n\t\t\tfields[tag] = k.Field(i)\n\t\t}\n\t}\n\n\tudt := info.(UDTTypeInfo)\n\tfor id, e := range udt.Elements {\n\t\tif len(data) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tif len(data) < 4 {\n\t\t\t// UDT def does not match the column value\n\t\t\treturn unmarshalErrorf(\"can not unmarshal %s: field [%d]%s: unexpected eof\", info, id, e.Name)\n\t\t}\n\n\t\tvar p []byte\n\t\tp, data = readBytes(data)\n\n\t\tf, ok := fields[e.Name]\n\t\tif !ok {\n\t\t\tf = k.FieldByName(e.Name)\n\t\t\tif f == emptyValue {\n\t\t\t\t// skip fields which exist in the UDT but not in\n\t\t\t\t// the struct passed in\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif !f.IsValid() || !f.CanAddr() {\n\t\t\treturn unmarshalErrorf(\"cannot unmarshal %s into %T: field %v is not valid\", info, value, e.Name)\n\t\t}\n\n\t\tfk := f.Addr().Interface()\n\t\tif err := Unmarshal(e.Type, p, fk); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// TypeInfo describes a Cassandra specific data type.\ntype TypeInfo interface {\n\tType() Type\n\tVersion() byte\n\tCustom() string\n\n\t// New creates a pointer to an empty version of whatever type\n\t// is referenced by the TypeInfo receiver.\n\t//\n\t// If there is no corresponding Go type for the CQL type, New panics.\n\t//\n\t// Deprecated: Use NewWithError instead.\n\tNew() interface{}\n\n\t// NewWithError creates a pointer to an empty version of whatever type\n\t// is referenced by the TypeInfo receiver.\n\t//\n\t// If there is no corresponding Go type for the CQL type, NewWithError returns an error.\n\tNewWithError() (interface{}, error)\n}\n\ntype NativeType struct {\n\tproto  byte\n\ttyp    Type\n\tcustom string // only used for TypeCustom\n}\n\nfunc NewNativeType(proto byte, typ Type, custom string) NativeType {\n\treturn NativeType{proto, typ, custom}\n}\n\nfunc (t NativeType) NewWithError() (interface{}, error) {\n\ttyp, err := goType(t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn reflect.New(typ).Interface(), nil\n}\n\nfunc (t NativeType) New() interface{} {\n\tval, err := t.NewWithError()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn val\n}\n\nfunc (s NativeType) Type() Type {\n\treturn s.typ\n}\n\nfunc (s NativeType) Version() byte {\n\treturn s.proto\n}\n\nfunc (s NativeType) Custom() string {\n\treturn s.custom\n}\n\nfunc (s NativeType) String() string {\n\tswitch s.typ {\n\tcase TypeCustom:\n\t\treturn fmt.Sprintf(\"%s(%s)\", s.typ, s.custom)\n\tdefault:\n\t\treturn s.typ.String()\n\t}\n}\n\ntype CollectionType struct {\n\tNativeType\n\tKey  TypeInfo // only used for TypeMap\n\tElem TypeInfo // only used for TypeMap, TypeList and TypeSet\n}\n\nfunc (t CollectionType) NewWithError() (interface{}, error) {\n\ttyp, err := goType(t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn reflect.New(typ).Interface(), nil\n}\n\nfunc (t CollectionType) New() interface{} {\n\tval, err := t.NewWithError()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn val\n}\n\nfunc (c CollectionType) String() string {\n\tswitch c.typ {\n\tcase TypeMap:\n\t\treturn fmt.Sprintf(\"%s(%s, %s)\", c.typ, c.Key, c.Elem)\n\tcase TypeList, TypeSet:\n\t\treturn fmt.Sprintf(\"%s(%s)\", c.typ, c.Elem)\n\tcase TypeCustom:\n\t\treturn fmt.Sprintf(\"%s(%s)\", c.typ, c.custom)\n\tdefault:\n\t\treturn c.typ.String()\n\t}\n}\n\ntype TupleTypeInfo struct {\n\tNativeType\n\tElems []TypeInfo\n}\n\nfunc (t TupleTypeInfo) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(fmt.Sprintf(\"%s(\", t.typ))\n\tfor _, elem := range t.Elems {\n\t\tbuf.WriteString(fmt.Sprintf(\"%s, \", elem))\n\t}\n\tbuf.Truncate(buf.Len() - 2)\n\tbuf.WriteByte(')')\n\treturn buf.String()\n}\n\nfunc (t TupleTypeInfo) NewWithError() (interface{}, error) {\n\ttyp, err := goType(t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn reflect.New(typ).Interface(), nil\n}\n\nfunc (t TupleTypeInfo) New() interface{} {\n\tval, err := t.NewWithError()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn val\n}\n\ntype UDTField struct {\n\tName string\n\tType TypeInfo\n}\n\ntype UDTTypeInfo struct {\n\tNativeType\n\tKeySpace string\n\tName     string\n\tElements []UDTField\n}\n\nfunc (u UDTTypeInfo) NewWithError() (interface{}, error) {\n\ttyp, err := goType(u)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn reflect.New(typ).Interface(), nil\n}\n\nfunc (u UDTTypeInfo) New() interface{} {\n\tval, err := u.NewWithError()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn val\n}\n\nfunc (u UDTTypeInfo) String() string {\n\tbuf := &bytes.Buffer{}\n\n\tfmt.Fprintf(buf, \"%s.%s{\", u.KeySpace, u.Name)\n\tfirst := true\n\tfor _, e := range u.Elements {\n\t\tif !first {\n\t\t\tfmt.Fprint(buf, \",\")\n\t\t} else {\n\t\t\tfirst = false\n\t\t}\n\n\t\tfmt.Fprintf(buf, \"%s=%v\", e.Name, e.Type)\n\t}\n\tfmt.Fprint(buf, \"}\")\n\n\treturn buf.String()\n}\n\n// String returns a human readable name for the Cassandra datatype\n// described by t.\n// Type is the identifier of a Cassandra internal datatype.\ntype Type int\n\nconst (\n\tTypeCustom    Type = 0x0000\n\tTypeAscii     Type = 0x0001\n\tTypeBigInt    Type = 0x0002\n\tTypeBlob      Type = 0x0003\n\tTypeBoolean   Type = 0x0004\n\tTypeCounter   Type = 0x0005\n\tTypeDecimal   Type = 0x0006\n\tTypeDouble    Type = 0x0007\n\tTypeFloat     Type = 0x0008\n\tTypeInt       Type = 0x0009\n\tTypeText      Type = 0x000A\n\tTypeTimestamp Type = 0x000B\n\tTypeUUID      Type = 0x000C\n\tTypeVarchar   Type = 0x000D\n\tTypeVarint    Type = 0x000E\n\tTypeTimeUUID  Type = 0x000F\n\tTypeInet      Type = 0x0010\n\tTypeDate      Type = 0x0011\n\tTypeTime      Type = 0x0012\n\tTypeSmallInt  Type = 0x0013\n\tTypeTinyInt   Type = 0x0014\n\tTypeDuration  Type = 0x0015\n\tTypeList      Type = 0x0020\n\tTypeMap       Type = 0x0021\n\tTypeSet       Type = 0x0022\n\tTypeUDT       Type = 0x0030\n\tTypeTuple     Type = 0x0031\n)\n\n// String returns the name of the identifier.\nfunc (t Type) String() string {\n\tswitch t {\n\tcase TypeCustom:\n\t\treturn \"custom\"\n\tcase TypeAscii:\n\t\treturn \"ascii\"\n\tcase TypeBigInt:\n\t\treturn \"bigint\"\n\tcase TypeBlob:\n\t\treturn \"blob\"\n\tcase TypeBoolean:\n\t\treturn \"boolean\"\n\tcase TypeCounter:\n\t\treturn \"counter\"\n\tcase TypeDecimal:\n\t\treturn \"decimal\"\n\tcase TypeDouble:\n\t\treturn \"double\"\n\tcase TypeFloat:\n\t\treturn \"float\"\n\tcase TypeInt:\n\t\treturn \"int\"\n\tcase TypeText:\n\t\treturn \"text\"\n\tcase TypeTimestamp:\n\t\treturn \"timestamp\"\n\tcase TypeUUID:\n\t\treturn \"uuid\"\n\tcase TypeVarchar:\n\t\treturn \"varchar\"\n\tcase TypeTimeUUID:\n\t\treturn \"timeuuid\"\n\tcase TypeInet:\n\t\treturn \"inet\"\n\tcase TypeDate:\n\t\treturn \"date\"\n\tcase TypeDuration:\n\t\treturn \"duration\"\n\tcase TypeTime:\n\t\treturn \"time\"\n\tcase TypeSmallInt:\n\t\treturn \"smallint\"\n\tcase TypeTinyInt:\n\t\treturn \"tinyint\"\n\tcase TypeList:\n\t\treturn \"list\"\n\tcase TypeMap:\n\t\treturn \"map\"\n\tcase TypeSet:\n\t\treturn \"set\"\n\tcase TypeVarint:\n\t\treturn \"varint\"\n\tcase TypeTuple:\n\t\treturn \"tuple\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"unknown_type_%d\", t)\n\t}\n}\n\ntype MarshalError string\n\nfunc (m MarshalError) Error() string {\n\treturn string(m)\n}\n\nfunc marshalErrorf(format string, args ...interface{}) MarshalError {\n\treturn MarshalError(fmt.Sprintf(format, args...))\n}\n\ntype UnmarshalError string\n\nfunc (m UnmarshalError) Error() string {\n\treturn string(m)\n}\n\nfunc unmarshalErrorf(format string, args ...interface{}) UnmarshalError {\n\treturn UnmarshalError(fmt.Sprintf(format, args...))\n}\n"
        },
        {
          "name": "marshal_test.go",
          "type": "blob",
          "size": 58.169921875,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"math\"\n\t\"math/big\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"gopkg.in/inf.v0\"\n)\n\ntype AliasInt int\ntype AliasUint uint\ntype AliasUint8 uint8\ntype AliasUint16 uint16\ntype AliasUint32 uint32\ntype AliasUint64 uint64\n\nvar marshalTests = []struct {\n\tInfo           TypeInfo\n\tData           []byte\n\tValue          interface{}\n\tMarshalError   error\n\tUnmarshalError error\n}{\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"hello world\"),\n\t\t[]byte(\"hello world\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"hello world\"),\n\t\t\"hello world\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(nil),\n\t\t[]byte(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"hello world\"),\n\t\tMyString(\"hello world\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"HELLO WORLD\"),\n\t\tCustomString(\"hello world\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBlob},\n\t\t[]byte(\"hello\\x00\"),\n\t\t[]byte(\"hello\\x00\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBlob},\n\t\t[]byte(nil),\n\t\t[]byte(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimeUUID},\n\t\t[]byte{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\tfunc() UUID {\n\t\t\tx, _ := UUIDFromBytes([]byte{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0})\n\t\t\treturn x\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimeUUID},\n\t\t[]byte{0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\t[]byte{0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\tMarshalError(\"can not marshal []byte 6 bytes long into timeuuid, must be exactly 16 bytes long\"),\n\t\tUnmarshalError(\"unable to parse UUID: UUIDs must be exactly 16 bytes long\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimeUUID},\n\t\t[]byte{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\t[16]byte{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\"),\n\t\t0,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x01\\x02\\x03\\x04\"),\n\t\tint(16909060),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x01\\x02\\x03\\x04\"),\n\t\tAliasInt(16909060),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x80\\x00\\x00\\x00\"),\n\t\tint32(math.MinInt32),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x7f\\xff\\xff\\xff\"),\n\t\tint32(math.MaxInt32),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\"),\n\t\t\"0\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x01\\x02\\x03\\x04\"),\n\t\t\"16909060\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x80\\x00\\x00\\x00\"),\n\t\t\"-2147483648\", // math.MinInt32\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x7f\\xff\\xff\\xff\"),\n\t\t\"2147483647\", // math.MaxInt32\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t0,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\"),\n\t\t72623859790382856,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\tint64(math.MinInt64),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x7f\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tint64(math.MaxInt64),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\"0\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\"),\n\t\t\"72623859790382856\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\"-9223372036854775808\", // math.MinInt64\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x7f\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\t\"9223372036854775807\", // math.MaxInt64\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBoolean},\n\t\t[]byte(\"\\x00\"),\n\t\tfalse,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBoolean},\n\t\t[]byte(\"\\x01\"),\n\t\ttrue,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeFloat},\n\t\t[]byte(\"\\x40\\x49\\x0f\\xdb\"),\n\t\tfloat32(3.14159265),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDouble},\n\t\t[]byte(\"\\x40\\x09\\x21\\xfb\\x53\\xc8\\xd4\\xf1\"),\n\t\tfloat64(3.14159265),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\"),\n\t\tinf.NewDec(0, 0),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x64\"),\n\t\tinf.NewDec(100, 0),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x02\\x19\"),\n\t\tdecimalize(\"0.25\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x13\\xD5\\a;\\x20\\x14\\xA2\\x91\"),\n\t\tdecimalize(\"-0.0012095473475870063\"), // From the iconara/cql-rb test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x13*\\xF8\\xC4\\xDF\\xEB]o\"),\n\t\tdecimalize(\"0.0012095473475870063\"), // From the iconara/cql-rb test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x12\\xF2\\xD8\\x02\\xB6R\\x7F\\x99\\xEE\\x98#\\x99\\xA9V\"),\n\t\tdecimalize(\"-1042342234234.123423435647768234\"), // From the iconara/cql-rb test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\r\\nJ\\x04\\\"^\\x91\\x04\\x8a\\xb1\\x18\\xfe\"),\n\t\tdecimalize(\"1243878957943.1234124191998\"), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x06\\xe5\\xde]\\x98Y\"),\n\t\tdecimalize(\"-112233.441191\"), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x14\\x00\\xfa\\xce\"),\n\t\tdecimalize(\"0.00000000000000064206\"), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\x00\\x00\\x00\\x14\\xff\\x052\"),\n\t\tdecimalize(\"-0.00000000000000064206\"), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\xff\\xff\\xff\\x9c\\x00\\xfa\\xce\"),\n\t\tinf.NewDec(64206, -100), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 4, typ: TypeTime},\n\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\ttime.Duration(int64(1376387523000)),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 4, typ: TypeTime},\n\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\tint64(1376387523000),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\ttime.Date(2013, time.August, 13, 9, 52, 3, 0, time.UTC),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\tint64(1376387523000),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa2\\xc3\\xc2\\x9a\\xe0F\\x91\\x06\"),\n\t\tDuration{Months: 1233, Days: 123213, Nanoseconds: 2312323},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa1\\xc3\\xc2\\x99\\xe0F\\x91\\x05\"),\n\t\tDuration{Months: -1233, Days: -123213, Nanoseconds: -2312323},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x02\\x04\\x80\\xe6\"),\n\t\tDuration{Months: 1, Days: 2, Nanoseconds: 115},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x02\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x02\"),\n\t\t[]int{1, 2},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x02\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x02\"),\n\t\t[2]int{1, 2},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeSet},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x02\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x02\"),\n\t\t[]int{1, 2},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeSet},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte{0, 0}, // encoding of a list should always include the size of the collection\n\t\t[]int{},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x01\\x00\\x03foo\\x00\\x04\\x00\\x00\\x00\\x01\"),\n\t\tmap[string]int{\"foo\": 1},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte{0, 0},\n\t\tmap[string]int{},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeVarchar},\n\t\t},\n\t\tbytes.Join([][]byte{\n\t\t\t[]byte(\"\\x00\\x01\\xFF\\xFF\"),\n\t\t\tbytes.Repeat([]byte(\"X\"), math.MaxUint16)}, []byte(\"\")),\n\t\t[]string{strings.Repeat(\"X\", math.MaxUint16)},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeVarchar},\n\t\t},\n\t\tbytes.Join([][]byte{\n\t\t\t[]byte(\"\\x00\\x01\\xFF\\xFF\"),\n\t\t\tbytes.Repeat([]byte(\"X\"), math.MaxUint16),\n\t\t\t[]byte(\"\\xFF\\xFF\"),\n\t\t\tbytes.Repeat([]byte(\"Y\"), math.MaxUint16)}, []byte(\"\")),\n\t\tmap[string]string{\n\t\t\tstrings.Repeat(\"X\", math.MaxUint16): strings.Repeat(\"Y\", math.MaxUint16),\n\t\t},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"\\x00\"),\n\t\t0,\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"\\x37\\xE2\\x3C\\xEC\"),\n\t\tint32(937573612),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"\\x37\\xE2\\x3C\\xEC\"),\n\t\tbig.NewInt(937573612),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"\\x03\\x9EV \\x15\\f\\x03\\x9DK\\x18\\xCDI\\\\$?\\a[\"),\n\t\tbigintize(\"1231312312331283012830129382342342412123\"), // From the iconara/cql-rb test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"\\xC9v\\x8D:\\x86\"),\n\t\tbig.NewInt(-234234234234), // From the iconara/cql-rb test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(\"f\\x1e\\xfd\\xf2\\xe3\\xb1\\x9f|\\x04_\\x15\"),\n\t\tbigintize(\"123456789123456789123456789\"), // From the datastax/python-driver test suite\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarint},\n\t\t[]byte(nil),\n\t\tnil,\n\t\tnil,\n\t\tUnmarshalError(\"can not unmarshal into non-pointer <nil>\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\x7F\\x00\\x00\\x01\"),\n\t\tnet.ParseIP(\"127.0.0.1\").To4(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\xFF\\xFF\\xFF\\xFF\"),\n\t\tnet.ParseIP(\"255.255.255.255\").To4(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\x7F\\x00\\x00\\x01\"),\n\t\t\"127.0.0.1\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\xFF\\xFF\\xFF\\xFF\"),\n\t\t\"255.255.255.255\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\x21\\xDA\\x00\\xd3\\x00\\x00\\x2f\\x3b\\x02\\xaa\\x00\\xff\\xfe\\x28\\x9c\\x5a\"),\n\t\t\"21da:d3:0:2f3b:2aa:ff:fe28:9c5a\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\xfe\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x02\\xb3\\xff\\xfe\\x1e\\x83\\x29\"),\n\t\t\"fe80::202:b3ff:fe1e:8329\",\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\x21\\xDA\\x00\\xd3\\x00\\x00\\x2f\\x3b\\x02\\xaa\\x00\\xff\\xfe\\x28\\x9c\\x5a\"),\n\t\tnet.ParseIP(\"21da:d3:0:2f3b:2aa:ff:fe28:9c5a\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\xfe\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x02\\xb3\\xff\\xfe\\x1e\\x83\\x29\"),\n\t\tnet.ParseIP(\"fe80::202:b3ff:fe1e:8329\"),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(nil),\n\t\tnil,\n\t\tnil,\n\t\tUnmarshalError(\"can not unmarshal into non-pointer <nil>\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"nullable string\"),\n\t\tfunc() *string {\n\t\t\tvalue := \"nullable string\"\n\t\t\treturn &value\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(nil),\n\t\t(*string)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x7f\\xff\\xff\\xff\"),\n\t\tfunc() *int {\n\t\t\tvar value int = math.MaxInt32\n\t\t\treturn &value\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(nil),\n\t\t(*int)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimeUUID},\n\t\t[]byte{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\t&UUID{0x3d, 0xcd, 0x98, 0x0, 0xf3, 0xd9, 0x11, 0xbf, 0x86, 0xd4, 0xb8, 0xe8, 0x56, 0x2c, 0xc, 0xd0},\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimeUUID},\n\t\t[]byte(nil),\n\t\t(*UUID)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\tfunc() *time.Time {\n\t\t\tt := time.Date(2013, time.August, 13, 9, 52, 3, 0, time.UTC)\n\t\t\treturn &t\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t[]byte(nil),\n\t\t(*time.Time)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBoolean},\n\t\t[]byte(\"\\x00\"),\n\t\tfunc() *bool {\n\t\t\tb := false\n\t\t\treturn &b\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBoolean},\n\t\t[]byte(\"\\x01\"),\n\t\tfunc() *bool {\n\t\t\tb := true\n\t\t\treturn &b\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBoolean},\n\t\t[]byte(nil),\n\t\t(*bool)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeFloat},\n\t\t[]byte(\"\\x40\\x49\\x0f\\xdb\"),\n\t\tfunc() *float32 {\n\t\t\tf := float32(3.14159265)\n\t\t\treturn &f\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeFloat},\n\t\t[]byte(nil),\n\t\t(*float32)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDouble},\n\t\t[]byte(\"\\x40\\x09\\x21\\xfb\\x53\\xc8\\xd4\\xf1\"),\n\t\tfunc() *float64 {\n\t\t\td := float64(3.14159265)\n\t\t\treturn &d\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDouble},\n\t\t[]byte(nil),\n\t\t(*float64)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(\"\\x7F\\x00\\x00\\x01\"),\n\t\tfunc() *net.IP {\n\t\t\tip := net.ParseIP(\"127.0.0.1\").To4()\n\t\t\treturn &ip\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInet},\n\t\t[]byte(nil),\n\t\t(*net.IP)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x02\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x02\"),\n\t\tfunc() *[]int {\n\t\t\tl := []int{1, 2}\n\t\t\treturn &l\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 3, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 3, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\"),\n\t\tfunc() *[]int {\n\t\t\tl := []int{1, 2}\n\t\t\treturn &l\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(nil),\n\t\t(*[]int)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x01\\x00\\x03foo\\x00\\x04\\x00\\x00\\x00\\x01\"),\n\t\tfunc() *map[string]int {\n\t\t\tm := map[string]int{\"foo\": 1}\n\t\t\treturn &m\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(nil),\n\t\t(*map[string]int)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(\"HELLO WORLD\"),\n\t\tfunc() *CustomString {\n\t\t\tcustomString := CustomString(\"hello world\")\n\t\t\treturn &customString\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte(nil),\n\t\t(*CustomString)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x7f\\xff\"),\n\t\t32767, // math.MaxInt16\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x7f\\xff\"),\n\t\t\"32767\", // math.MaxInt16\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x00\\x01\"),\n\t\tint16(1),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tint16(-1),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x00\\xff\"),\n\t\tuint8(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tuint16(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tuint32(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tuint64(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x00\\xff\"),\n\t\tAliasUint8(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tAliasUint16(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tAliasUint32(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tAliasUint64(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tAliasUint(65535),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\x7f\"),\n\t\t127, // math.MaxInt8\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\x7f\"),\n\t\t\"127\", // math.MaxInt8\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\x01\"),\n\t\tint16(1),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tint16(-1),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tuint8(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tuint64(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tuint32(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tuint16(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tuint(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tAliasUint8(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tAliasUint64(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tAliasUint32(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tAliasUint16(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTinyInt},\n\t\t[]byte(\"\\xff\"),\n\t\tAliasUint(255),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\"),\n\t\tuint8(math.MaxUint8),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\"),\n\t\tuint64(math.MaxUint16),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\"),\n\t\tuint64(math.MaxUint32),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tuint64(math.MaxUint64),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tuint32(math.MaxUint32),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tuint64(math.MaxUint32),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBlob},\n\t\t[]byte(nil),\n\t\t([]byte)(nil),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeVarchar},\n\t\t[]byte{},\n\t\tfunc() interface{} {\n\t\t\tvar s string\n\t\t\treturn &s\n\t\t}(),\n\t\tnil,\n\t\tnil,\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeTime},\n\t\tencBigInt(1000),\n\t\ttime.Duration(1000),\n\t\tnil,\n\t\tnil,\n\t},\n}\n\nvar unmarshalTests = []struct {\n\tInfo           TypeInfo\n\tData           []byte\n\tValue          interface{}\n\tUnmarshalError error\n}{\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x01\\x00\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x00\\x01\\x00\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tuint16(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x01\\x00\\x00\"),\n\t\tuint16(0),\n\t\tUnmarshalError(\"unmarshal int: value 65536 out of range for uint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\"),\n\t\tuint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for uint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tuint16(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\"),\n\t\tuint16(0),\n\t\tUnmarshalError(\"unmarshal int: value 65536 out of range for uint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tuint32(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for uint32\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\"),\n\t\tuint32(0),\n\t\tUnmarshalError(\"unmarshal int: value 4294967296 out of range for uint32\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\xff\\xff\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeSmallInt},\n\t\t[]byte(\"\\x01\\x00\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x00\\x01\\x00\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint16(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeInt},\n\t\t[]byte(\"\\x00\\x01\\x00\\x00\"),\n\t\tAliasUint16(0),\n\t\tUnmarshalError(\"unmarshal int: value 65536 out of range for gocql.AliasUint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\"),\n\t\tAliasUint8(0),\n\t\tUnmarshalError(\"unmarshal int: value 256 out of range for gocql.AliasUint8\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint16(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\"),\n\t\tAliasUint16(0),\n\t\tUnmarshalError(\"unmarshal int: value 65536 out of range for gocql.AliasUint16\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"),\n\t\tAliasUint32(0),\n\t\tUnmarshalError(\"unmarshal int: value -1 out of range for gocql.AliasUint32\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeBigInt},\n\t\t[]byte(\"\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\"),\n\t\tAliasUint32(0),\n\t\tUnmarshalError(\"unmarshal int: value 4294967296 out of range for gocql.AliasUint32\"),\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 3, typ: TypeList},\n\t\t\tElem:       NativeType{proto: 3, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\"), // truncated data\n\t\tfunc() *[]int {\n\t\t\tl := []int{1, 2}\n\t\t\treturn &l\n\t\t}(),\n\t\tUnmarshalError(\"unmarshal list: unexpected eof\"),\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x01\\x00\\x03fo\"),\n\t\tmap[string]int{\"foo\": 1},\n\t\tUnmarshalError(\"unmarshal map: unexpected eof\"),\n\t},\n\t{\n\t\tCollectionType{\n\t\t\tNativeType: NativeType{proto: 2, typ: TypeMap},\n\t\t\tKey:        NativeType{proto: 2, typ: TypeVarchar},\n\t\t\tElem:       NativeType{proto: 2, typ: TypeInt},\n\t\t},\n\t\t[]byte(\"\\x00\\x01\\x00\\x03foo\\x00\\x04\\x00\\x00\"),\n\t\tmap[string]int{\"foo\": 1},\n\t\tUnmarshalError(\"unmarshal map: unexpected eof\"),\n\t},\n\t{\n\t\tNativeType{proto: 2, typ: TypeDecimal},\n\t\t[]byte(\"\\xff\\xff\\xff\"),\n\t\tinf.NewDec(0, 0), // From the datastax/python-driver test suite\n\t\tUnmarshalError(\"inf.Dec needs at least 4 bytes, while value has only 3\"),\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa2\\xc3\\xc2\\x9a\\xe0F\\x91\"),\n\t\tDuration{},\n\t\tUnmarshalError(\"failed to unmarshal duration into *gocql.Duration: failed to extract nanoseconds: data expect to have 9 bytes, but it has only 8\"),\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa2\\xc3\\xc2\\x9a\"),\n\t\tDuration{},\n\t\tUnmarshalError(\"failed to unmarshal duration into *gocql.Duration: failed to extract nanoseconds: unexpected eof\"),\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa2\\xc3\\xc2\"),\n\t\tDuration{},\n\t\tUnmarshalError(\"failed to unmarshal duration into *gocql.Duration: failed to extract days: data expect to have 5 bytes, but it has only 4\"),\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\\xa2\"),\n\t\tDuration{},\n\t\tUnmarshalError(\"failed to unmarshal duration into *gocql.Duration: failed to extract days: unexpected eof\"),\n\t},\n\t{\n\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t[]byte(\"\\x89\"),\n\t\tDuration{},\n\t\tUnmarshalError(\"failed to unmarshal duration into *gocql.Duration: failed to extract month: data expect to have 2 bytes, but it has only 1\"),\n\t},\n}\n\nfunc decimalize(s string) *inf.Dec {\n\ti, _ := new(inf.Dec).SetString(s)\n\treturn i\n}\n\nfunc bigintize(s string) *big.Int {\n\ti, _ := new(big.Int).SetString(s, 10)\n\treturn i\n}\n\nfunc TestMarshal_Encode(t *testing.T) {\n\tfor i, test := range marshalTests {\n\t\tif test.MarshalError == nil {\n\t\t\tdata, err := Marshal(test.Info, test.Value)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"marshalTest[%d]: %v\", i, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !bytes.Equal(data, test.Data) {\n\t\t\t\tt.Errorf(\"marshalTest[%d]: expected %q, got %q (%#v)\", i, test.Data, data, test.Value)\n\t\t\t}\n\t\t} else {\n\t\t\tif _, err := Marshal(test.Info, test.Value); err != test.MarshalError {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%t): %#v returned error %#v, want %#v.\", i, test.Info, test.Value, test.Value, err, test.MarshalError)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestMarshal_Decode(t *testing.T) {\n\tfor i, test := range marshalTests {\n\t\tif test.UnmarshalError == nil {\n\t\t\tv := reflect.New(reflect.TypeOf(test.Value))\n\t\t\terr := Unmarshal(test.Info, test.Data, v.Interface())\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): %v\", i, test.Info, test.Value, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(v.Elem().Interface(), test.Value) {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): expected %#v, got %#v.\", i, test.Info, test.Value, test.Value, v.Elem().Interface())\n\t\t\t}\n\t\t} else {\n\t\t\tif err := Unmarshal(test.Info, test.Data, test.Value); err != test.UnmarshalError {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): %#v returned error %#v, want %#v.\", i, test.Info, test.Value, test.Value, err, test.UnmarshalError)\n\t\t\t}\n\t\t}\n\t}\n\tfor i, test := range unmarshalTests {\n\t\tv := reflect.New(reflect.TypeOf(test.Value))\n\t\tif test.UnmarshalError == nil {\n\t\t\terr := Unmarshal(test.Info, test.Data, v.Interface())\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): %v\", i, test.Info, test.Value, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(v.Elem().Interface(), test.Value) {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): expected %#v, got %#v.\", i, test.Info, test.Value, test.Value, v.Elem().Interface())\n\t\t\t}\n\t\t} else {\n\t\t\tif err := Unmarshal(test.Info, test.Data, v.Interface()); err != test.UnmarshalError {\n\t\t\t\tt.Errorf(\"unmarshalTest[%d] (%v=>%T): %#v returned error %#v, want %#v.\", i, test.Info, test.Value, test.Value, err, test.UnmarshalError)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestMarshalVarint(t *testing.T) {\n\tvarintTests := []struct {\n\t\tValue       interface{}\n\t\tMarshaled   []byte\n\t\tUnmarshaled *big.Int\n\t}{\n\t\t{\n\t\t\tValue:       int8(0),\n\t\t\tMarshaled:   []byte(\"\\x00\"),\n\t\t\tUnmarshaled: big.NewInt(0),\n\t\t},\n\t\t{\n\t\t\tValue:       uint8(255),\n\t\t\tMarshaled:   []byte(\"\\x00\\xFF\"),\n\t\t\tUnmarshaled: big.NewInt(255),\n\t\t},\n\t\t{\n\t\t\tValue:       int8(-1),\n\t\t\tMarshaled:   []byte(\"\\xFF\"),\n\t\t\tUnmarshaled: big.NewInt(-1),\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(math.MaxInt32),\n\t\t\tMarshaled:   []byte(\"\\x7F\\xFF\\xFF\\xFF\"),\n\t\t\tUnmarshaled: big.NewInt(math.MaxInt32),\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(int64(math.MaxInt32) + 1),\n\t\t\tMarshaled:   []byte(\"\\x00\\x80\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: big.NewInt(int64(math.MaxInt32) + 1),\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(math.MinInt32),\n\t\t\tMarshaled:   []byte(\"\\x80\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: big.NewInt(math.MinInt32),\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(int64(math.MinInt32) - 1),\n\t\t\tMarshaled:   []byte(\"\\xFF\\x7F\\xFF\\xFF\\xFF\"),\n\t\t\tUnmarshaled: big.NewInt(int64(math.MinInt32) - 1),\n\t\t},\n\t\t{\n\t\t\tValue:       math.MinInt64,\n\t\t\tMarshaled:   []byte(\"\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: big.NewInt(math.MinInt64),\n\t\t},\n\t\t{\n\t\t\tValue:       uint64(math.MaxInt64) + 1,\n\t\t\tMarshaled:   []byte(\"\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: bigintize(\"9223372036854775808\"),\n\t\t},\n\t\t{\n\t\t\tValue:       bigintize(\"2361183241434822606848\"), // 2**71\n\t\t\tMarshaled:   []byte(\"\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: bigintize(\"2361183241434822606848\"),\n\t\t},\n\t\t{\n\t\t\tValue:       bigintize(\"-9223372036854775809\"), // -2**63 - 1\n\t\t\tMarshaled:   []byte(\"\\xFF\\x7F\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\"),\n\t\t\tUnmarshaled: bigintize(\"-9223372036854775809\"),\n\t\t},\n\t}\n\n\tfor i, test := range varintTests {\n\t\tdata, err := Marshal(NativeType{proto: 2, typ: TypeVarint}, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error marshaling varint: %v (test #%d)\", err, i)\n\t\t}\n\n\t\tif !bytes.Equal(test.Marshaled, data) {\n\t\t\tt.Errorf(\"marshaled varint mismatch: expected %v, got %v (test #%d)\", test.Marshaled, data, i)\n\t\t}\n\n\t\tbinder := new(big.Int)\n\t\terr = Unmarshal(NativeType{proto: 2, typ: TypeVarint}, test.Marshaled, binder)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error unmarshaling varint: %v (test #%d)\", err, i)\n\t\t}\n\n\t\tif test.Unmarshaled.Cmp(binder) != 0 {\n\t\t\tt.Errorf(\"unmarshaled varint mismatch: expected %v, got %v (test #%d)\", test.Unmarshaled, binder, i)\n\t\t}\n\t}\n\n\tvarintUint64Tests := []struct {\n\t\tValue       interface{}\n\t\tMarshaled   []byte\n\t\tUnmarshaled uint64\n\t}{\n\t\t{\n\t\t\tValue:       int8(0),\n\t\t\tMarshaled:   []byte(\"\\x00\"),\n\t\t\tUnmarshaled: 0,\n\t\t},\n\t\t{\n\t\t\tValue:       uint8(255),\n\t\t\tMarshaled:   []byte(\"\\x00\\xFF\"),\n\t\t\tUnmarshaled: 255,\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(math.MaxInt32),\n\t\t\tMarshaled:   []byte(\"\\x7F\\xFF\\xFF\\xFF\"),\n\t\t\tUnmarshaled: uint64(math.MaxInt32),\n\t\t},\n\t\t{\n\t\t\tValue:       big.NewInt(int64(math.MaxInt32) + 1),\n\t\t\tMarshaled:   []byte(\"\\x00\\x80\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: uint64(int64(math.MaxInt32) + 1),\n\t\t},\n\t\t{\n\t\t\tValue:       uint64(math.MaxInt64) + 1,\n\t\t\tMarshaled:   []byte(\"\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"),\n\t\t\tUnmarshaled: 9223372036854775808,\n\t\t},\n\t\t{\n\t\t\tValue:       uint64(math.MaxUint64),\n\t\t\tMarshaled:   []byte(\"\\x00\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\"),\n\t\t\tUnmarshaled: uint64(math.MaxUint64),\n\t\t},\n\t}\n\n\tfor i, test := range varintUint64Tests {\n\t\tdata, err := Marshal(NativeType{proto: 2, typ: TypeVarint}, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error marshaling varint: %v (test #%d)\", err, i)\n\t\t}\n\n\t\tif !bytes.Equal(test.Marshaled, data) {\n\t\t\tt.Errorf(\"marshaled varint mismatch: expected %v, got %v (test #%d)\", test.Marshaled, data, i)\n\t\t}\n\n\t\tvar binder uint64\n\t\terr = Unmarshal(NativeType{proto: 2, typ: TypeVarint}, test.Marshaled, &binder)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error unmarshaling varint to uint64: %v (test #%d)\", err, i)\n\t\t}\n\n\t\tif test.Unmarshaled != binder {\n\t\t\tt.Errorf(\"unmarshaled varint mismatch: expected %v, got %v (test #%d)\", test.Unmarshaled, binder, i)\n\t\t}\n\t}\n}\n\nfunc equalStringPointerSlice(leftList, rightList []*string) bool {\n\tif len(leftList) != len(rightList) {\n\t\treturn false\n\t}\n\tfor index := range leftList {\n\t\tif !reflect.DeepEqual(rightList[index], leftList[index]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc TestMarshalList(t *testing.T) {\n\ttypeInfoV2 := CollectionType{\n\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\tElem:       NativeType{proto: 2, typ: TypeVarchar},\n\t}\n\ttypeInfoV3 := CollectionType{\n\t\tNativeType: NativeType{proto: 3, typ: TypeList},\n\t\tElem:       NativeType{proto: 3, typ: TypeVarchar},\n\t}\n\n\ttype tc struct {\n\t\ttypeInfo CollectionType\n\t\tinput    []*string\n\t\texpected []*string\n\t}\n\n\tvalueA := \"valueA\"\n\tvalueB := \"valueB\"\n\tvalueEmpty := \"\"\n\ttestCases := []tc{\n\t\t{\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{&valueA},\n\t\t\texpected: []*string{&valueA},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{&valueA, &valueB},\n\t\t\texpected: []*string{&valueA, &valueB},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{&valueA, &valueEmpty, &valueB},\n\t\t\texpected: []*string{&valueA, &valueEmpty, &valueB},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{&valueEmpty},\n\t\t\texpected: []*string{&valueEmpty},\n\t\t},\n\t\t{\n\t\t\t// nil values are marshalled to empty values for protocol < 3\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{nil},\n\t\t\texpected: []*string{&valueEmpty},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV2,\n\t\t\tinput:    []*string{&valueA, nil, &valueB},\n\t\t\texpected: []*string{&valueA, &valueEmpty, &valueB},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV3,\n\t\t\tinput:    []*string{&valueEmpty},\n\t\t\texpected: []*string{&valueEmpty},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV3,\n\t\t\tinput:    []*string{nil},\n\t\t\texpected: []*string{nil},\n\t\t},\n\t\t{\n\t\t\ttypeInfo: typeInfoV3,\n\t\t\tinput:    []*string{&valueA, nil, &valueB},\n\t\t\texpected: []*string{&valueA, nil, &valueB},\n\t\t},\n\t}\n\n\tlistDatas := [][]byte{}\n\tfor _, c := range testCases {\n\t\tlistData, marshalErr := Marshal(c.typeInfo, c.input)\n\t\tif nil != marshalErr {\n\t\t\tt.Errorf(\"Error marshal %+v of type %+v: %s\", c.input, c.typeInfo, marshalErr)\n\t\t}\n\t\tlistDatas = append(listDatas, listData)\n\t}\n\n\toutputLists := [][]*string{}\n\n\tvar outputList []*string\n\n\tfor i, listData := range listDatas {\n\t\tif unmarshalErr := Unmarshal(testCases[i].typeInfo, listData, &outputList); nil != unmarshalErr {\n\t\t\tt.Error(unmarshalErr)\n\t\t}\n\t\tresultList := []interface{}{}\n\t\tfor i := range outputList {\n\t\t\tif outputList[i] != nil {\n\t\t\t\tresultList = append(resultList, *outputList[i])\n\t\t\t} else {\n\t\t\t\tresultList = append(resultList, nil)\n\t\t\t}\n\t\t}\n\t\toutputLists = append(outputLists, outputList)\n\t}\n\n\tfor index, c := range testCases {\n\t\toutputList := outputLists[index]\n\t\tif !equalStringPointerSlice(c.expected, outputList) {\n\t\t\tt.Errorf(\"Lists %+v not equal to lists %+v, but should\", c.expected, outputList)\n\t\t}\n\t}\n}\n\ntype CustomString string\n\nfunc (c CustomString) MarshalCQL(info TypeInfo) ([]byte, error) {\n\treturn []byte(strings.ToUpper(string(c))), nil\n}\nfunc (c *CustomString) UnmarshalCQL(info TypeInfo, data []byte) error {\n\t*c = CustomString(strings.ToLower(string(data)))\n\treturn nil\n}\n\ntype MyString string\n\nvar typeLookupTest = []struct {\n\tTypeName     string\n\tExpectedType Type\n}{\n\t{\"AsciiType\", TypeAscii},\n\t{\"LongType\", TypeBigInt},\n\t{\"BytesType\", TypeBlob},\n\t{\"BooleanType\", TypeBoolean},\n\t{\"CounterColumnType\", TypeCounter},\n\t{\"DecimalType\", TypeDecimal},\n\t{\"DoubleType\", TypeDouble},\n\t{\"FloatType\", TypeFloat},\n\t{\"Int32Type\", TypeInt},\n\t{\"DateType\", TypeTimestamp},\n\t{\"TimestampType\", TypeTimestamp},\n\t{\"UUIDType\", TypeUUID},\n\t{\"UTF8Type\", TypeVarchar},\n\t{\"IntegerType\", TypeVarint},\n\t{\"TimeUUIDType\", TypeTimeUUID},\n\t{\"InetAddressType\", TypeInet},\n\t{\"MapType\", TypeMap},\n\t{\"ListType\", TypeList},\n\t{\"SetType\", TypeSet},\n\t{\"unknown\", TypeCustom},\n\t{\"ShortType\", TypeSmallInt},\n\t{\"ByteType\", TypeTinyInt},\n}\n\nfunc testType(t *testing.T, cassType string, expectedType Type) {\n\tif computedType := getApacheCassandraType(apacheCassandraTypePrefix + cassType); computedType != expectedType {\n\t\tt.Errorf(\"Cassandra custom type lookup for %s failed. Expected %s, got %s.\", cassType, expectedType.String(), computedType.String())\n\t}\n}\n\nfunc TestLookupCassType(t *testing.T) {\n\tfor _, lookupTest := range typeLookupTest {\n\t\ttestType(t, lookupTest.TypeName, lookupTest.ExpectedType)\n\t}\n}\n\ntype MyPointerMarshaler struct{}\n\nfunc (m *MyPointerMarshaler) MarshalCQL(_ TypeInfo) ([]byte, error) {\n\treturn []byte{42}, nil\n}\n\nfunc TestMarshalPointer(t *testing.T) {\n\tm := &MyPointerMarshaler{}\n\ttyp := NativeType{proto: 2, typ: TypeInt}\n\n\tdata, err := Marshal(typ, m)\n\n\tif err != nil {\n\t\tt.Errorf(\"Pointer marshaling failed. Error: %s\", err)\n\t}\n\tif len(data) != 1 || data[0] != 42 {\n\t\tt.Errorf(\"Pointer marshaling failed. Expected %+v, got %+v\", []byte{42}, data)\n\t}\n}\n\nfunc TestMarshalTime(t *testing.T) {\n\tdurationS := \"1h10m10s\"\n\tduration, _ := time.ParseDuration(durationS)\n\texpectedData := encBigInt(duration.Nanoseconds())\n\tvar marshalTimeTests = []struct {\n\t\tInfo  TypeInfo\n\t\tData  []byte\n\t\tValue interface{}\n\t}{\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeTime},\n\t\t\texpectedData,\n\t\t\tduration.Nanoseconds(),\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeTime},\n\t\t\texpectedData,\n\t\t\tduration,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeTime},\n\t\t\texpectedData,\n\t\t\t&duration,\n\t\t},\n\t}\n\n\tfor i, test := range marshalTimeTests {\n\t\tt.Log(i, test)\n\t\tdata, err := Marshal(test.Info, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"marshalTest[%d]: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif !bytes.Equal(data, test.Data) {\n\t\t\tt.Errorf(\"marshalTest[%d]: expected %x (%v), got %x (%v) for time %s\", i,\n\t\t\t\ttest.Data, decInt(test.Data), data, decInt(data), test.Value)\n\t\t}\n\t}\n}\n\nfunc TestMarshalTimestamp(t *testing.T) {\n\tvar marshalTimestampTests = []struct {\n\t\tInfo  TypeInfo\n\t\tData  []byte\n\t\tValue interface{}\n\t}{\n\t\t{\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\t\ttime.Date(2013, time.August, 13, 9, 52, 3, 0, time.UTC),\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\x00\\x00\\x01\\x40\\x77\\x16\\xe1\\xb8\"),\n\t\t\tint64(1376387523000),\n\t\t},\n\t\t{\n\t\t\t// 9223372036854 is the maximum time representable in ms since the epoch\n\t\t\t// with int64 if using UnixNano to convert\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\x00\\x00\\x08\\x63\\x7b\\xd0\\x5a\\xf6\"),\n\t\t\ttime.Date(2262, time.April, 11, 23, 47, 16, 854775807, time.UTC),\n\t\t},\n\t\t{\n\t\t\t// One nanosecond after causes overflow when using UnixNano\n\t\t\t// Instead it should resolve to the same time in ms\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\x00\\x00\\x08\\x63\\x7b\\xd0\\x5a\\xf6\"),\n\t\t\ttime.Date(2262, time.April, 11, 23, 47, 16, 854775808, time.UTC),\n\t\t},\n\t\t{\n\t\t\t// -9223372036855 is the minimum time representable in ms since the epoch\n\t\t\t// with int64 if using UnixNano to convert\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\xff\\xff\\xf7\\x9c\\x84\\x2f\\xa5\\x09\"),\n\t\t\ttime.Date(1677, time.September, 21, 00, 12, 43, 145224192, time.UTC),\n\t\t},\n\t\t{\n\t\t\t// One nanosecond earlier causes overflow when using UnixNano\n\t\t\t// it should resolve to the same time in ms\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte(\"\\xff\\xff\\xf7\\x9c\\x84\\x2f\\xa5\\x09\"),\n\t\t\ttime.Date(1677, time.September, 21, 00, 12, 43, 145224191, time.UTC),\n\t\t},\n\t\t{\n\t\t\t// Store the zero time as a blank slice\n\t\t\tNativeType{proto: 2, typ: TypeTimestamp},\n\t\t\t[]byte{},\n\t\t\ttime.Time{},\n\t\t},\n\t}\n\n\tfor i, test := range marshalTimestampTests {\n\t\tt.Log(i, test)\n\t\tdata, err := Marshal(test.Info, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"marshalTest[%d]: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif !bytes.Equal(data, test.Data) {\n\t\t\tt.Errorf(\"marshalTest[%d]: expected %x (%v), got %x (%v) for time %s\", i,\n\t\t\t\ttest.Data, decBigInt(test.Data), data, decBigInt(data), test.Value)\n\t\t}\n\t}\n}\n\nfunc TestMarshalTuple(t *testing.T) {\n\tinfo := TupleTypeInfo{\n\t\tNativeType: NativeType{proto: 3, typ: TypeTuple},\n\t\tElems: []TypeInfo{\n\t\t\tNativeType{proto: 3, typ: TypeVarchar},\n\t\t\tNativeType{proto: 3, typ: TypeVarchar},\n\t\t},\n\t}\n\n\tstringToPtr := func(s string) *string { return &s }\n\tcheckString := func(t *testing.T, exp string, got string) {\n\t\tif got != exp {\n\t\t\tt.Errorf(\"expected string to be %v, got %v\", exp, got)\n\t\t}\n\t}\n\n\ttype tupleStruct struct {\n\t\tA string\n\t\tB *string\n\t}\n\tvar (\n\t\ts1 *string\n\t\ts2 *string\n\t)\n\n\ttestCases := []struct {\n\t\tname       string\n\t\texpected   []byte\n\t\tvalue      interface{}\n\t\tcheckValue interface{}\n\t\tcheck      func(*testing.T, interface{})\n\t}{\n\t\t{\n\t\t\tname:       \"interface-slice:two-strings\",\n\t\t\texpected:   []byte(\"\\x00\\x00\\x00\\x03foo\\x00\\x00\\x00\\x03bar\"),\n\t\t\tvalue:      []interface{}{\"foo\", \"bar\"},\n\t\t\tcheckValue: []interface{}{&s1, &s2},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tcheckString(t, \"foo\", *s1)\n\t\t\t\tcheckString(t, \"bar\", *s2)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:       \"interface-slice:one-string-one-nil-string\",\n\t\t\texpected:   []byte(\"\\x00\\x00\\x00\\x03foo\\xff\\xff\\xff\\xff\"),\n\t\t\tvalue:      []interface{}{\"foo\", nil},\n\t\t\tcheckValue: []interface{}{&s1, &s2},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tcheckString(t, \"foo\", *s1)\n\t\t\t\tif s2 != nil {\n\t\t\t\t\tt.Errorf(\"expected string to be nil, got %v\", *s2)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"struct:two-strings\",\n\t\t\texpected: []byte(\"\\x00\\x00\\x00\\x03foo\\x00\\x00\\x00\\x03bar\"),\n\t\t\tvalue: tupleStruct{\n\t\t\t\tA: \"foo\",\n\t\t\t\tB: stringToPtr(\"bar\"),\n\t\t\t},\n\t\t\tcheckValue: &tupleStruct{},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tgot := v.(*tupleStruct)\n\t\t\t\tif got.A != \"foo\" {\n\t\t\t\t\tt.Errorf(\"expected A string to be %v, got %v\", \"foo\", got.A)\n\t\t\t\t}\n\t\t\t\tif got.B == nil {\n\t\t\t\t\tt.Errorf(\"expected B string to be %v, got nil\", \"bar\")\n\t\t\t\t}\n\t\t\t\tif *got.B != \"bar\" {\n\t\t\t\t\tt.Errorf(\"expected B string to be %v, got %v\", \"bar\", got.B)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:       \"struct:one-string-one-nil-string\",\n\t\t\texpected:   []byte(\"\\x00\\x00\\x00\\x03foo\\xff\\xff\\xff\\xff\"),\n\t\t\tvalue:      tupleStruct{A: \"foo\", B: nil},\n\t\t\tcheckValue: &tupleStruct{},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tgot := v.(*tupleStruct)\n\t\t\t\tif got.A != \"foo\" {\n\t\t\t\t\tt.Errorf(\"expected A string to be %v, got %v\", \"foo\", got.A)\n\t\t\t\t}\n\t\t\t\tif got.B != nil {\n\t\t\t\t\tt.Errorf(\"expected B string to be nil, got %v\", *got.B)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"arrayslice:two-strings\",\n\t\t\texpected: []byte(\"\\x00\\x00\\x00\\x03foo\\x00\\x00\\x00\\x03bar\"),\n\t\t\tvalue: [2]*string{\n\t\t\t\tstringToPtr(\"foo\"),\n\t\t\t\tstringToPtr(\"bar\"),\n\t\t\t},\n\t\t\tcheckValue: &[2]*string{},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tgot := v.(*[2]*string)\n\t\t\t\tcheckString(t, \"foo\", *(got[0]))\n\t\t\t\tcheckString(t, \"bar\", *(got[1]))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"arrayslice:one-string-one-nil-string\",\n\t\t\texpected: []byte(\"\\x00\\x00\\x00\\x03foo\\xff\\xff\\xff\\xff\"),\n\t\t\tvalue: [2]*string{\n\t\t\t\tstringToPtr(\"foo\"),\n\t\t\t\tnil,\n\t\t\t},\n\t\t\tcheckValue: &[2]*string{},\n\t\t\tcheck: func(t *testing.T, v interface{}) {\n\t\t\t\tgot := v.(*[2]*string)\n\t\t\t\tcheckString(t, \"foo\", *(got[0]))\n\t\t\t\tif got[1] != nil {\n\t\t\t\t\tt.Errorf(\"expected string to be nil, got %v\", *got[1])\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tdata, err := Marshal(info, tc.value)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"marshalTest: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif !bytes.Equal(data, tc.expected) {\n\t\t\t\tt.Errorf(\"marshalTest: expected %x (%v), got %x (%v)\",\n\t\t\t\t\ttc.expected, decBigInt(tc.expected), data, decBigInt(data))\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\terr = Unmarshal(info, data, tc.checkValue)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"unmarshalTest: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\ttc.check(t, tc.checkValue)\n\t\t})\n\t}\n}\n\nfunc TestUnmarshalTuple(t *testing.T) {\n\tinfo := TupleTypeInfo{\n\t\tNativeType: NativeType{proto: 3, typ: TypeTuple},\n\t\tElems: []TypeInfo{\n\t\t\tNativeType{proto: 3, typ: TypeVarchar},\n\t\t\tNativeType{proto: 3, typ: TypeVarchar},\n\t\t},\n\t}\n\n\t// As per the CQL spec, a tuple is a sequence of \"bytes\" values.\n\t// Here we encode a null value (length -1) and the \"foo\" string (length 3)\n\n\tdata := []byte(\"\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x03foo\")\n\n\tt.Run(\"struct-ptr\", func(t *testing.T) {\n\t\tvar tmp struct {\n\t\t\tA *string\n\t\t\tB *string\n\t\t}\n\n\t\terr := Unmarshal(info, data, &tmp)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"unmarshalTest: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif tmp.A != nil || *tmp.B != \"foo\" {\n\t\t\tt.Errorf(\"unmarshalTest: expected [nil, foo], got [%v, %v]\", *tmp.A, *tmp.B)\n\t\t}\n\t})\n\tt.Run(\"struct-nonptr\", func(t *testing.T) {\n\t\tvar tmp struct {\n\t\t\tA string\n\t\t\tB string\n\t\t}\n\n\t\terr := Unmarshal(info, data, &tmp)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"unmarshalTest: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif tmp.A != \"\" || tmp.B != \"foo\" {\n\t\t\tt.Errorf(\"unmarshalTest: expected [nil, foo], got [%v, %v]\", tmp.A, tmp.B)\n\t\t}\n\t})\n\n\tt.Run(\"array\", func(t *testing.T) {\n\t\tvar tmp [2]*string\n\n\t\terr := Unmarshal(info, data, &tmp)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"unmarshalTest: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif tmp[0] != nil || *tmp[1] != \"foo\" {\n\t\t\tt.Errorf(\"unmarshalTest: expected [nil, foo], got [%v, %v]\", *tmp[0], *tmp[1])\n\t\t}\n\t})\n\tt.Run(\"array-nonptr\", func(t *testing.T) {\n\t\tvar tmp [2]string\n\n\t\terr := Unmarshal(info, data, &tmp)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"unmarshalTest: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif tmp[0] != \"\" || tmp[1] != \"foo\" {\n\t\t\tt.Errorf(\"unmarshalTest: expected [nil, foo], got [%v, %v]\", tmp[0], tmp[1])\n\t\t}\n\t})\n}\n\nfunc TestMarshalUDTMap(t *testing.T) {\n\ttypeInfo := UDTTypeInfo{NativeType{proto: 3, typ: TypeUDT}, \"\", \"xyz\", []UDTField{\n\t\t{Name: \"x\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t\t{Name: \"y\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t\t{Name: \"z\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t}}\n\n\tt.Run(\"partially bound\", func(t *testing.T) {\n\t\tvalue := map[string]interface{}{\n\t\t\t\"y\": 2,\n\t\t\t\"z\": 3,\n\t\t}\n\t\texpected := []byte(\"\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x03\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n\tt.Run(\"partially bound from the beginning\", func(t *testing.T) {\n\t\tvalue := map[string]interface{}{\n\t\t\t\"x\": 1,\n\t\t\t\"y\": 2,\n\t\t}\n\t\texpected := []byte(\"\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\xff\\xff\\xff\\xff\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n\tt.Run(\"fully bound\", func(t *testing.T) {\n\t\tvalue := map[string]interface{}{\n\t\t\t\"x\": 1,\n\t\t\t\"y\": 2,\n\t\t\t\"z\": 3,\n\t\t}\n\t\texpected := []byte(\"\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x03\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n}\n\nfunc TestMarshalUDTStruct(t *testing.T) {\n\ttypeInfo := UDTTypeInfo{NativeType{proto: 3, typ: TypeUDT}, \"\", \"xyz\", []UDTField{\n\t\t{Name: \"x\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t\t{Name: \"y\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t\t{Name: \"z\", Type: NativeType{proto: 3, typ: TypeInt}},\n\t}}\n\n\ttype xyzStruct struct {\n\t\tX int32 `cql:\"x\"`\n\t\tY int32 `cql:\"y\"`\n\t\tZ int32 `cql:\"z\"`\n\t}\n\ttype xyStruct struct {\n\t\tX int32 `cql:\"x\"`\n\t\tY int32 `cql:\"y\"`\n\t}\n\ttype yzStruct struct {\n\t\tY int32 `cql:\"y\"`\n\t\tZ int32 `cql:\"z\"`\n\t}\n\n\tt.Run(\"partially bound\", func(t *testing.T) {\n\t\tvalue := yzStruct{\n\t\t\tY: 2,\n\t\t\tZ: 3,\n\t\t}\n\t\texpected := []byte(\"\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x03\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n\tt.Run(\"partially bound from the beginning\", func(t *testing.T) {\n\t\tvalue := xyStruct{\n\t\t\tX: 1,\n\t\t\tY: 2,\n\t\t}\n\t\texpected := []byte(\"\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\xff\\xff\\xff\\xff\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n\tt.Run(\"fully bound\", func(t *testing.T) {\n\t\tvalue := xyzStruct{\n\t\t\tX: 1,\n\t\t\tY: 2,\n\t\t\tZ: 3,\n\t\t}\n\t\texpected := []byte(\"\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x03\")\n\n\t\tdata, err := Marshal(typeInfo, value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"got error %#v\", err)\n\t\t}\n\t\tif !bytes.Equal(data, expected) {\n\t\t\tt.Errorf(\"got value %x\", data)\n\t\t}\n\t})\n}\n\nfunc TestMarshalNil(t *testing.T) {\n\ttypes := []Type{\n\t\tTypeAscii,\n\t\tTypeBlob,\n\t\tTypeBoolean,\n\t\tTypeBigInt,\n\t\tTypeCounter,\n\t\tTypeDecimal,\n\t\tTypeDouble,\n\t\tTypeFloat,\n\t\tTypeInt,\n\t\tTypeTimestamp,\n\t\tTypeUUID,\n\t\tTypeVarchar,\n\t\tTypeVarint,\n\t\tTypeTimeUUID,\n\t\tTypeInet,\n\t}\n\n\tfor _, typ := range types {\n\t\tdata, err := Marshal(NativeType{proto: 3, typ: typ}, nil)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"unable to marshal nil %v: %v\\n\", typ, err)\n\t\t} else if data != nil {\n\t\t\tt.Errorf(\"expected to get nil byte for nil %v got % X\", typ, data)\n\t\t}\n\t}\n}\n\nfunc TestUnmarshalInetCopyBytes(t *testing.T) {\n\tdata := []byte{127, 0, 0, 1}\n\tvar ip net.IP\n\tif err := unmarshalInet(NativeType{proto: 2, typ: TypeInet}, data, &ip); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcopy(data, []byte{0xFF, 0xFF, 0xFF, 0xFF})\n\tip2 := net.IP(data)\n\tif !ip.Equal(net.IPv4(127, 0, 0, 1)) {\n\t\tt.Fatalf(\"IP memory shared with data: ip=%v ip2=%v\", ip, ip2)\n\t}\n}\n\nfunc TestUnmarshalDate(t *testing.T) {\n\tdata := []uint8{0x80, 0x0, 0x43, 0x31}\n\tvar date time.Time\n\tif err := unmarshalDate(NativeType{proto: 2, typ: TypeDate}, data, &date); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\texpectedDate := \"2017-02-04\"\n\tformattedDate := date.Format(\"2006-01-02\")\n\tif expectedDate != formattedDate {\n\t\tt.Errorf(\"marshalTest: expected %v, got %v\", expectedDate, formattedDate)\n\t\treturn\n\t}\n\tvar stringDate string\n\tif err2 := unmarshalDate(NativeType{proto: 2, typ: TypeDate}, data, &stringDate); err2 != nil {\n\t\tt.Fatal(err2)\n\t}\n\tif expectedDate != stringDate {\n\t\tt.Errorf(\"marshalTest: expected %v, got %v\", expectedDate, formattedDate)\n\t\treturn\n\t}\n}\n\nfunc TestMarshalDate(t *testing.T) {\n\tnow := time.Now().UTC()\n\ttimestamp := now.UnixNano() / int64(time.Millisecond)\n\texpectedData := encInt(int32(timestamp/86400000 + int64(1<<31)))\n\n\tvar marshalDateTests = []struct {\n\t\tInfo  TypeInfo\n\t\tData  []byte\n\t\tValue interface{}\n\t}{\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeDate},\n\t\t\texpectedData,\n\t\t\ttimestamp,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeDate},\n\t\t\texpectedData,\n\t\t\tnow,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeDate},\n\t\t\texpectedData,\n\t\t\t&now,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 4, typ: TypeDate},\n\t\t\texpectedData,\n\t\t\tnow.Format(\"2006-01-02\"),\n\t\t},\n\t}\n\n\tfor i, test := range marshalDateTests {\n\t\tt.Log(i, test)\n\t\tdata, err := Marshal(test.Info, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"marshalTest[%d]: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif !bytes.Equal(data, test.Data) {\n\t\t\tt.Errorf(\"marshalTest[%d]: expected %x (%v), got %x (%v) for time %s\", i,\n\t\t\t\ttest.Data, decInt(test.Data), data, decInt(data), test.Value)\n\t\t}\n\t}\n}\n\nfunc TestLargeDate(t *testing.T) {\n\tfarFuture := time.Date(999999, time.December, 31, 0, 0, 0, 0, time.UTC)\n\texpectedFutureData := encInt(int32(farFuture.UnixMilli()/86400000 + int64(1<<31)))\n\n\tfarPast := time.Date(-999999, time.January, 1, 0, 0, 0, 0, time.UTC)\n\texpectedPastData := encInt(int32(farPast.UnixMilli()/86400000 + int64(1<<31)))\n\n\tvar marshalDateTests = []struct {\n\t\tData         []byte\n\t\tValue        interface{}\n\t\tExpectedDate string\n\t}{\n\t\t{\n\t\t\texpectedFutureData,\n\t\t\tfarFuture,\n\t\t\t\"999999-12-31\",\n\t\t},\n\t\t{\n\t\t\texpectedPastData,\n\t\t\tfarPast,\n\t\t\t\"-999999-01-01\",\n\t\t},\n\t}\n\n\tnativeType := NativeType{proto: 4, typ: TypeDate}\n\n\tfor i, test := range marshalDateTests {\n\t\tt.Log(i, test)\n\n\t\tdata, err := Marshal(nativeType, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"largeDateTest[%d]: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif !bytes.Equal(data, test.Data) {\n\t\t\tt.Errorf(\"largeDateTest[%d]: expected %x (%v), got %x (%v) for time %s\", i,\n\t\t\t\ttest.Data, decInt(test.Data), data, decInt(data), test.Value)\n\t\t}\n\n\t\tvar date time.Time\n\t\tif err := Unmarshal(nativeType, data, &date); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tformattedDate := date.Format(\"2006-01-02\")\n\t\tif test.ExpectedDate != formattedDate {\n\t\t\tt.Fatalf(\"largeDateTest: expected %v, got %v\", test.ExpectedDate, formattedDate)\n\t\t}\n\t}\n}\n\nfunc BenchmarkUnmarshalVarchar(b *testing.B) {\n\tb.ReportAllocs()\n\tsrc := make([]byte, 1024)\n\tdst := make([]byte, len(src))\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif err := unmarshalVarchar(NativeType{}, src, &dst); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc TestMarshalDuration(t *testing.T) {\n\tdurationS := \"1h10m10s\"\n\tduration, _ := time.ParseDuration(durationS)\n\texpectedData := append([]byte{0, 0}, encVint(duration.Nanoseconds())...)\n\tvar marshalDurationTests = []struct {\n\t\tInfo  TypeInfo\n\t\tData  []byte\n\t\tValue interface{}\n\t}{\n\t\t{\n\t\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t\texpectedData,\n\t\t\tduration.Nanoseconds(),\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t\texpectedData,\n\t\t\tduration,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t\texpectedData,\n\t\t\tdurationS,\n\t\t},\n\t\t{\n\t\t\tNativeType{proto: 5, typ: TypeDuration},\n\t\t\texpectedData,\n\t\t\t&duration,\n\t\t},\n\t}\n\n\tfor i, test := range marshalDurationTests {\n\t\tt.Log(i, test)\n\t\tdata, err := Marshal(test.Info, test.Value)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"marshalTest[%d]: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif !bytes.Equal(data, test.Data) {\n\t\t\tt.Errorf(\"marshalTest[%d]: expected %x (%v), got %x (%v) for time %s\", i,\n\t\t\t\ttest.Data, decInt(test.Data), data, decInt(data), test.Value)\n\t\t}\n\t}\n}\n\nfunc TestReadCollectionSize(t *testing.T) {\n\tlistV2 := CollectionType{\n\t\tNativeType: NativeType{proto: 2, typ: TypeList},\n\t\tElem:       NativeType{proto: 2, typ: TypeVarchar},\n\t}\n\tlistV3 := CollectionType{\n\t\tNativeType: NativeType{proto: 3, typ: TypeList},\n\t\tElem:       NativeType{proto: 3, typ: TypeVarchar},\n\t}\n\n\ttests := []struct {\n\t\tname         string\n\t\tinfo         CollectionType\n\t\tdata         []byte\n\t\tisError      bool\n\t\texpectedSize int\n\t}{\n\t\t{\n\t\t\tname:    \"short read 0 proto 2\",\n\t\t\tinfo:    listV2,\n\t\t\tdata:    []byte{},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"short read 1 proto 2\",\n\t\t\tinfo:    listV2,\n\t\t\tdata:    []byte{0x01},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:         \"good read proto 2\",\n\t\t\tinfo:         listV2,\n\t\t\tdata:         []byte{0x01, 0x38},\n\t\t\texpectedSize: 0x0138,\n\t\t},\n\t\t{\n\t\t\tname:    \"short read 0 proto 3\",\n\t\t\tinfo:    listV3,\n\t\t\tdata:    []byte{},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"short read 1 proto 3\",\n\t\t\tinfo:    listV3,\n\t\t\tdata:    []byte{0x01},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"short read 2 proto 3\",\n\t\t\tinfo:    listV3,\n\t\t\tdata:    []byte{0x01, 0x38},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"short read 3 proto 3\",\n\t\t\tinfo:    listV3,\n\t\t\tdata:    []byte{0x01, 0x38, 0x42},\n\t\t\tisError: true,\n\t\t},\n\t\t{\n\t\t\tname:         \"good read proto 3\",\n\t\t\tinfo:         listV3,\n\t\t\tdata:         []byte{0x01, 0x38, 0x42, 0x22},\n\t\t\texpectedSize: 0x01384222,\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tsize, _, err := readCollectionSize(test.info, test.data)\n\t\t\tif test.isError {\n\t\t\t\tif err == nil {\n\t\t\t\t\tt.Fatal(\"Expected error, but it was nil\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"Expected no error, got %v\", err)\n\t\t\t\t}\n\t\t\t\tif size != test.expectedSize {\n\t\t\t\t\tt.Fatalf(\"Expected size of %d, but got %d\", test.expectedSize, size)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkUnmarshalUUID(b *testing.B) {\n\tb.ReportAllocs()\n\tsrc := make([]byte, 16)\n\tdst := UUID{}\n\tvar ti TypeInfo = NativeType{}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif err := unmarshalUUID(ti, src, &dst); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\nfunc TestUnmarshalUDT(t *testing.T) {\n\tinfo := UDTTypeInfo{\n\t\tNativeType: NativeType{proto: 4, typ: TypeUDT},\n\t\tName:       \"myudt\",\n\t\tKeySpace:   \"myks\",\n\t\tElements: []UDTField{\n\t\t\t{\n\t\t\t\tName: \"first\",\n\t\t\t\tType: NativeType{proto: 4, typ: TypeAscii},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"second\",\n\t\t\t\tType: NativeType{proto: 4, typ: TypeSmallInt},\n\t\t\t},\n\t\t},\n\t}\n\tdata := bytesWithLength( // UDT\n\t\tbytesWithLength([]byte(\"Hello\")),    // first\n\t\tbytesWithLength([]byte(\"\\x00\\x2a\")), // second\n\t)\n\tvalue := map[string]interface{}{}\n\texpectedErr := UnmarshalError(\"can not unmarshal into non-pointer map[string]interface {}\")\n\n\tif err := Unmarshal(info, data, value); err != expectedErr {\n\t\tt.Errorf(\"(%v=>%T): %#v returned error %#v, want %#v.\",\n\t\t\tinfo, value, value, err, expectedErr)\n\t}\n}\n\n// bytesWithLength concatenates all data slices and prepends the total length as uint32.\n// The length does not count the size of the uint32 used for writing the size.\nfunc bytesWithLength(data ...[]byte) []byte {\n\ttotalLen := 0\n\tfor i := range data {\n\t\ttotalLen += len(data[i])\n\t}\n\tif totalLen > math.MaxUint32 {\n\t\tpanic(\"total length overflows\")\n\t}\n\tret := make([]byte, totalLen+4)\n\tbinary.BigEndian.PutUint32(ret[:4], uint32(totalLen))\n\tbuf := ret[4:]\n\tfor i := range data {\n\t\tn := copy(buf, data[i])\n\t\tbuf = buf[n:]\n\t}\n\treturn ret\n}\n"
        },
        {
          "name": "metadata.go",
          "type": "blob",
          "size": 36.548828125,
          "content": "// Copyright (c) 2015 The gocql Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n)\n\n// schema metadata for a keyspace\ntype KeyspaceMetadata struct {\n\tName            string\n\tDurableWrites   bool\n\tStrategyClass   string\n\tStrategyOptions map[string]interface{}\n\tTables          map[string]*TableMetadata\n\tFunctions       map[string]*FunctionMetadata\n\tAggregates      map[string]*AggregateMetadata\n\t// Deprecated: use the MaterializedViews field for views and UserTypes field for udts instead.\n\tViews             map[string]*ViewMetadata\n\tMaterializedViews map[string]*MaterializedViewMetadata\n\tUserTypes         map[string]*UserTypeMetadata\n}\n\n// schema metadata for a table (a.k.a. column family)\ntype TableMetadata struct {\n\tKeyspace          string\n\tName              string\n\tKeyValidator      string\n\tComparator        string\n\tDefaultValidator  string\n\tKeyAliases        []string\n\tColumnAliases     []string\n\tValueAlias        string\n\tPartitionKey      []*ColumnMetadata\n\tClusteringColumns []*ColumnMetadata\n\tColumns           map[string]*ColumnMetadata\n\tOrderedColumns    []string\n}\n\n// schema metadata for a column\ntype ColumnMetadata struct {\n\tKeyspace        string\n\tTable           string\n\tName            string\n\tComponentIndex  int\n\tKind            ColumnKind\n\tValidator       string\n\tType            TypeInfo\n\tClusteringOrder string\n\tOrder           ColumnOrder\n\tIndex           ColumnIndexMetadata\n}\n\n// FunctionMetadata holds metadata for function constructs\ntype FunctionMetadata struct {\n\tKeyspace          string\n\tName              string\n\tArgumentTypes     []TypeInfo\n\tArgumentNames     []string\n\tBody              string\n\tCalledOnNullInput bool\n\tLanguage          string\n\tReturnType        TypeInfo\n}\n\n// AggregateMetadata holds metadata for aggregate constructs\ntype AggregateMetadata struct {\n\tKeyspace      string\n\tName          string\n\tArgumentTypes []TypeInfo\n\tFinalFunc     FunctionMetadata\n\tInitCond      string\n\tReturnType    TypeInfo\n\tStateFunc     FunctionMetadata\n\tStateType     TypeInfo\n\n\tstateFunc string\n\tfinalFunc string\n}\n\n// ViewMetadata holds the metadata for views.\n// Deprecated: this is kept for backwards compatibility issues. Use MaterializedViewMetadata.\ntype ViewMetadata struct {\n\tKeyspace   string\n\tName       string\n\tFieldNames []string\n\tFieldTypes []TypeInfo\n}\n\n// MaterializedViewMetadata holds the metadata for materialized views.\ntype MaterializedViewMetadata struct {\n\tKeyspace                string\n\tName                    string\n\tBaseTableId             UUID\n\tBaseTable               *TableMetadata\n\tBloomFilterFpChance     float64\n\tCaching                 map[string]string\n\tComment                 string\n\tCompaction              map[string]string\n\tCompression             map[string]string\n\tCrcCheckChance          float64\n\tDcLocalReadRepairChance float64\n\tDefaultTimeToLive       int\n\tExtensions              map[string]string\n\tGcGraceSeconds          int\n\tId                      UUID\n\tIncludeAllColumns       bool\n\tMaxIndexInterval        int\n\tMemtableFlushPeriodInMs int\n\tMinIndexInterval        int\n\tReadRepairChance        float64\n\tSpeculativeRetry        string\n\n\tbaseTableName string\n}\n\ntype UserTypeMetadata struct {\n\tKeyspace   string\n\tName       string\n\tFieldNames []string\n\tFieldTypes []TypeInfo\n}\n\n// the ordering of the column with regard to its comparator\ntype ColumnOrder bool\n\nconst (\n\tASC  ColumnOrder = false\n\tDESC ColumnOrder = true\n)\n\ntype ColumnIndexMetadata struct {\n\tName    string\n\tType    string\n\tOptions map[string]interface{}\n}\n\ntype ColumnKind int\n\nconst (\n\tColumnUnkownKind ColumnKind = iota\n\tColumnPartitionKey\n\tColumnClusteringKey\n\tColumnRegular\n\tColumnCompact\n\tColumnStatic\n)\n\nfunc (c ColumnKind) String() string {\n\tswitch c {\n\tcase ColumnPartitionKey:\n\t\treturn \"partition_key\"\n\tcase ColumnClusteringKey:\n\t\treturn \"clustering_key\"\n\tcase ColumnRegular:\n\t\treturn \"regular\"\n\tcase ColumnCompact:\n\t\treturn \"compact\"\n\tcase ColumnStatic:\n\t\treturn \"static\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"unknown_column_%d\", c)\n\t}\n}\n\nfunc (c *ColumnKind) UnmarshalCQL(typ TypeInfo, p []byte) error {\n\tif typ.Type() != TypeVarchar {\n\t\treturn unmarshalErrorf(\"unable to marshall %s into ColumnKind, expected Varchar\", typ)\n\t}\n\n\tkind, err := columnKindFromSchema(string(p))\n\tif err != nil {\n\t\treturn err\n\t}\n\t*c = kind\n\n\treturn nil\n}\n\nfunc columnKindFromSchema(kind string) (ColumnKind, error) {\n\tswitch kind {\n\tcase \"partition_key\":\n\t\treturn ColumnPartitionKey, nil\n\tcase \"clustering_key\", \"clustering\":\n\t\treturn ColumnClusteringKey, nil\n\tcase \"regular\":\n\t\treturn ColumnRegular, nil\n\tcase \"compact_value\":\n\t\treturn ColumnCompact, nil\n\tcase \"static\":\n\t\treturn ColumnStatic, nil\n\tdefault:\n\t\treturn -1, fmt.Errorf(\"unknown column kind: %q\", kind)\n\t}\n}\n\n// default alias values\nconst (\n\tDEFAULT_KEY_ALIAS    = \"key\"\n\tDEFAULT_COLUMN_ALIAS = \"column\"\n\tDEFAULT_VALUE_ALIAS  = \"value\"\n)\n\n// queries the cluster for schema information for a specific keyspace\ntype schemaDescriber struct {\n\tsession *Session\n\tmu      sync.Mutex\n\n\tcache map[string]*KeyspaceMetadata\n}\n\n// creates a session bound schema describer which will query and cache\n// keyspace metadata\nfunc newSchemaDescriber(session *Session) *schemaDescriber {\n\treturn &schemaDescriber{\n\t\tsession: session,\n\t\tcache:   map[string]*KeyspaceMetadata{},\n\t}\n}\n\n// returns the cached KeyspaceMetadata held by the describer for the named\n// keyspace.\nfunc (s *schemaDescriber) getSchema(keyspaceName string) (*KeyspaceMetadata, error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tmetadata, found := s.cache[keyspaceName]\n\tif !found {\n\t\t// refresh the cache for this keyspace\n\t\terr := s.refreshSchema(keyspaceName)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tmetadata = s.cache[keyspaceName]\n\t}\n\n\treturn metadata, nil\n}\n\n// clears the already cached keyspace metadata\nfunc (s *schemaDescriber) clearSchema(keyspaceName string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tdelete(s.cache, keyspaceName)\n}\n\n// forcibly updates the current KeyspaceMetadata held by the schema describer\n// for a given named keyspace.\nfunc (s *schemaDescriber) refreshSchema(keyspaceName string) error {\n\tvar err error\n\n\t// query the system keyspace for schema data\n\t// TODO retrieve concurrently\n\tkeyspace, err := getKeyspaceMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttables, err := getTableMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcolumns, err := getColumnMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfunctions, err := getFunctionsMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\taggregates, err := getAggregatesMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tviews, err := getViewsMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tmaterializedViews, err := getMaterializedViewsMetadata(s.session, keyspaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// organize the schema data\n\tcompileMetadata(s.session.cfg.ProtoVersion, keyspace, tables, columns, functions, aggregates, views,\n\t\tmaterializedViews, s.session.logger)\n\n\t// update the cache\n\ts.cache[keyspaceName] = keyspace\n\n\treturn nil\n}\n\n// \"compiles\" derived information about keyspace, table, and column metadata\n// for a keyspace from the basic queried metadata objects returned by\n// getKeyspaceMetadata, getTableMetadata, and getColumnMetadata respectively;\n// Links the metadata objects together and derives the column composition of\n// the partition key and clustering key for a table.\nfunc compileMetadata(\n\tprotoVersion int,\n\tkeyspace *KeyspaceMetadata,\n\ttables []TableMetadata,\n\tcolumns []ColumnMetadata,\n\tfunctions []FunctionMetadata,\n\taggregates []AggregateMetadata,\n\tviews []ViewMetadata,\n\tmaterializedViews []MaterializedViewMetadata,\n\tlogger StdLogger,\n) {\n\tkeyspace.Tables = make(map[string]*TableMetadata)\n\tfor i := range tables {\n\t\ttables[i].Columns = make(map[string]*ColumnMetadata)\n\n\t\tkeyspace.Tables[tables[i].Name] = &tables[i]\n\t}\n\tkeyspace.Functions = make(map[string]*FunctionMetadata, len(functions))\n\tfor i := range functions {\n\t\tkeyspace.Functions[functions[i].Name] = &functions[i]\n\t}\n\tkeyspace.Aggregates = make(map[string]*AggregateMetadata, len(aggregates))\n\tfor i, _ := range aggregates {\n\t\taggregates[i].FinalFunc = *keyspace.Functions[aggregates[i].finalFunc]\n\t\taggregates[i].StateFunc = *keyspace.Functions[aggregates[i].stateFunc]\n\t\tkeyspace.Aggregates[aggregates[i].Name] = &aggregates[i]\n\t}\n\tkeyspace.Views = make(map[string]*ViewMetadata, len(views))\n\tfor i := range views {\n\t\tkeyspace.Views[views[i].Name] = &views[i]\n\t}\n\t// Views currently holds the types and hasn't been deleted for backward compatibility issues.\n\t// That's why it's ok to copy Views into Types in this case. For the real Views use MaterializedViews.\n\ttypes := make([]UserTypeMetadata, len(views))\n\tfor i := range views {\n\t\ttypes[i].Keyspace = views[i].Keyspace\n\t\ttypes[i].Name = views[i].Name\n\t\ttypes[i].FieldNames = views[i].FieldNames\n\t\ttypes[i].FieldTypes = views[i].FieldTypes\n\t}\n\tkeyspace.UserTypes = make(map[string]*UserTypeMetadata, len(views))\n\tfor i := range types {\n\t\tkeyspace.UserTypes[types[i].Name] = &types[i]\n\t}\n\tkeyspace.MaterializedViews = make(map[string]*MaterializedViewMetadata, len(materializedViews))\n\tfor i, _ := range materializedViews {\n\t\tmaterializedViews[i].BaseTable = keyspace.Tables[materializedViews[i].baseTableName]\n\t\tkeyspace.MaterializedViews[materializedViews[i].Name] = &materializedViews[i]\n\t}\n\n\t// add columns from the schema data\n\tfor i := range columns {\n\t\tcol := &columns[i]\n\t\t// decode the validator for TypeInfo and order\n\t\tif col.ClusteringOrder != \"\" { // Cassandra 3.x+\n\t\t\tcol.Type = getCassandraType(col.Validator, logger)\n\t\t\tcol.Order = ASC\n\t\t\tif col.ClusteringOrder == \"desc\" {\n\t\t\t\tcol.Order = DESC\n\t\t\t}\n\t\t} else {\n\t\t\tvalidatorParsed := parseType(col.Validator, logger)\n\t\t\tcol.Type = validatorParsed.types[0]\n\t\t\tcol.Order = ASC\n\t\t\tif validatorParsed.reversed[0] {\n\t\t\t\tcol.Order = DESC\n\t\t\t}\n\t\t}\n\n\t\ttable, ok := keyspace.Tables[col.Table]\n\t\tif !ok {\n\t\t\t// if the schema is being updated we will race between seeing\n\t\t\t// the metadata be complete. Potentially we should check for\n\t\t\t// schema versions before and after reading the metadata and\n\t\t\t// if they dont match try again.\n\t\t\tcontinue\n\t\t}\n\n\t\ttable.Columns[col.Name] = col\n\t\ttable.OrderedColumns = append(table.OrderedColumns, col.Name)\n\t}\n\n\tif protoVersion == protoVersion1 {\n\t\tcompileV1Metadata(tables, logger)\n\t} else {\n\t\tcompileV2Metadata(tables, logger)\n\t}\n}\n\n// Compiles derived information from TableMetadata which have had\n// ColumnMetadata added already. V1 protocol does not return as much\n// column metadata as V2+ (because V1 doesn't support the \"type\" column in the\n// system.schema_columns table) so determining PartitionKey and ClusterColumns\n// is more complex.\nfunc compileV1Metadata(tables []TableMetadata, logger StdLogger) {\n\tfor i := range tables {\n\t\ttable := &tables[i]\n\n\t\t// decode the key validator\n\t\tkeyValidatorParsed := parseType(table.KeyValidator, logger)\n\t\t// decode the comparator\n\t\tcomparatorParsed := parseType(table.Comparator, logger)\n\n\t\t// the partition key length is the same as the number of types in the\n\t\t// key validator\n\t\ttable.PartitionKey = make([]*ColumnMetadata, len(keyValidatorParsed.types))\n\n\t\t// V1 protocol only returns \"regular\" columns from\n\t\t// system.schema_columns (there is no type field for columns)\n\t\t// so the alias information is used to\n\t\t// create the partition key and clustering columns\n\n\t\t// construct the partition key from the alias\n\t\tfor i := range table.PartitionKey {\n\t\t\tvar alias string\n\t\t\tif len(table.KeyAliases) > i {\n\t\t\t\talias = table.KeyAliases[i]\n\t\t\t} else if i == 0 {\n\t\t\t\talias = DEFAULT_KEY_ALIAS\n\t\t\t} else {\n\t\t\t\talias = DEFAULT_KEY_ALIAS + strconv.Itoa(i+1)\n\t\t\t}\n\n\t\t\tcolumn := &ColumnMetadata{\n\t\t\t\tKeyspace:       table.Keyspace,\n\t\t\t\tTable:          table.Name,\n\t\t\t\tName:           alias,\n\t\t\t\tType:           keyValidatorParsed.types[i],\n\t\t\t\tKind:           ColumnPartitionKey,\n\t\t\t\tComponentIndex: i,\n\t\t\t}\n\n\t\t\ttable.PartitionKey[i] = column\n\t\t\ttable.Columns[alias] = column\n\t\t}\n\n\t\t// determine the number of clustering columns\n\t\tsize := len(comparatorParsed.types)\n\t\tif comparatorParsed.isComposite {\n\t\t\tif len(comparatorParsed.collections) != 0 ||\n\t\t\t\t(len(table.ColumnAliases) == size-1 &&\n\t\t\t\t\tcomparatorParsed.types[size-1].Type() == TypeVarchar) {\n\t\t\t\tsize = size - 1\n\t\t\t}\n\t\t} else {\n\t\t\tif !(len(table.ColumnAliases) != 0 || len(table.Columns) == 0) {\n\t\t\t\tsize = 0\n\t\t\t}\n\t\t}\n\n\t\ttable.ClusteringColumns = make([]*ColumnMetadata, size)\n\n\t\tfor i := range table.ClusteringColumns {\n\t\t\tvar alias string\n\t\t\tif len(table.ColumnAliases) > i {\n\t\t\t\talias = table.ColumnAliases[i]\n\t\t\t} else if i == 0 {\n\t\t\t\talias = DEFAULT_COLUMN_ALIAS\n\t\t\t} else {\n\t\t\t\talias = DEFAULT_COLUMN_ALIAS + strconv.Itoa(i+1)\n\t\t\t}\n\n\t\t\torder := ASC\n\t\t\tif comparatorParsed.reversed[i] {\n\t\t\t\torder = DESC\n\t\t\t}\n\n\t\t\tcolumn := &ColumnMetadata{\n\t\t\t\tKeyspace:       table.Keyspace,\n\t\t\t\tTable:          table.Name,\n\t\t\t\tName:           alias,\n\t\t\t\tType:           comparatorParsed.types[i],\n\t\t\t\tOrder:          order,\n\t\t\t\tKind:           ColumnClusteringKey,\n\t\t\t\tComponentIndex: i,\n\t\t\t}\n\n\t\t\ttable.ClusteringColumns[i] = column\n\t\t\ttable.Columns[alias] = column\n\t\t}\n\n\t\tif size != len(comparatorParsed.types)-1 {\n\t\t\talias := DEFAULT_VALUE_ALIAS\n\t\t\tif len(table.ValueAlias) > 0 {\n\t\t\t\talias = table.ValueAlias\n\t\t\t}\n\t\t\t// decode the default validator\n\t\t\tdefaultValidatorParsed := parseType(table.DefaultValidator, logger)\n\t\t\tcolumn := &ColumnMetadata{\n\t\t\t\tKeyspace: table.Keyspace,\n\t\t\t\tTable:    table.Name,\n\t\t\t\tName:     alias,\n\t\t\t\tType:     defaultValidatorParsed.types[0],\n\t\t\t\tKind:     ColumnRegular,\n\t\t\t}\n\t\t\ttable.Columns[alias] = column\n\t\t}\n\t}\n}\n\n// The simpler compile case for V2+ protocol\nfunc compileV2Metadata(tables []TableMetadata, logger StdLogger) {\n\tfor i := range tables {\n\t\ttable := &tables[i]\n\n\t\tclusteringColumnCount := componentColumnCountOfType(table.Columns, ColumnClusteringKey)\n\t\ttable.ClusteringColumns = make([]*ColumnMetadata, clusteringColumnCount)\n\n\t\tif table.KeyValidator != \"\" {\n\t\t\tkeyValidatorParsed := parseType(table.KeyValidator, logger)\n\t\t\ttable.PartitionKey = make([]*ColumnMetadata, len(keyValidatorParsed.types))\n\t\t} else { // Cassandra 3.x+\n\t\t\tpartitionKeyCount := componentColumnCountOfType(table.Columns, ColumnPartitionKey)\n\t\t\ttable.PartitionKey = make([]*ColumnMetadata, partitionKeyCount)\n\t\t}\n\n\t\tfor _, columnName := range table.OrderedColumns {\n\t\t\tcolumn := table.Columns[columnName]\n\t\t\tif column.Kind == ColumnPartitionKey {\n\t\t\t\ttable.PartitionKey[column.ComponentIndex] = column\n\t\t\t} else if column.Kind == ColumnClusteringKey {\n\t\t\t\ttable.ClusteringColumns[column.ComponentIndex] = column\n\t\t\t}\n\t\t}\n\t}\n}\n\n// returns the count of coluns with the given \"kind\" value.\nfunc componentColumnCountOfType(columns map[string]*ColumnMetadata, kind ColumnKind) int {\n\tmaxComponentIndex := -1\n\tfor _, column := range columns {\n\t\tif column.Kind == kind && column.ComponentIndex > maxComponentIndex {\n\t\t\tmaxComponentIndex = column.ComponentIndex\n\t\t}\n\t}\n\treturn maxComponentIndex + 1\n}\n\n// query only for the keyspace metadata for the specified keyspace from system.schema_keyspace\nfunc getKeyspaceMetadata(session *Session, keyspaceName string) (*KeyspaceMetadata, error) {\n\tkeyspace := &KeyspaceMetadata{Name: keyspaceName}\n\n\tif session.useSystemSchema { // Cassandra 3.x+\n\t\tconst stmt = `\n\t\tSELECT durable_writes, replication\n\t\tFROM system_schema.keyspaces\n\t\tWHERE keyspace_name = ?`\n\n\t\tvar replication map[string]string\n\n\t\titer := session.control.query(stmt, keyspaceName)\n\t\tif iter.NumRows() == 0 {\n\t\t\treturn nil, ErrKeyspaceDoesNotExist\n\t\t}\n\t\titer.Scan(&keyspace.DurableWrites, &replication)\n\t\terr := iter.Close()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error querying keyspace schema: %v\", err)\n\t\t}\n\n\t\tkeyspace.StrategyClass = replication[\"class\"]\n\t\tdelete(replication, \"class\")\n\n\t\tkeyspace.StrategyOptions = make(map[string]interface{}, len(replication))\n\t\tfor k, v := range replication {\n\t\t\tkeyspace.StrategyOptions[k] = v\n\t\t}\n\t} else {\n\n\t\tconst stmt = `\n\t\tSELECT durable_writes, strategy_class, strategy_options\n\t\tFROM system.schema_keyspaces\n\t\tWHERE keyspace_name = ?`\n\n\t\tvar strategyOptionsJSON []byte\n\n\t\titer := session.control.query(stmt, keyspaceName)\n\t\tif iter.NumRows() == 0 {\n\t\t\treturn nil, ErrKeyspaceDoesNotExist\n\t\t}\n\t\titer.Scan(&keyspace.DurableWrites, &keyspace.StrategyClass, &strategyOptionsJSON)\n\t\terr := iter.Close()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error querying keyspace schema: %v\", err)\n\t\t}\n\n\t\terr = json.Unmarshal(strategyOptionsJSON, &keyspace.StrategyOptions)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\"invalid JSON value '%s' as strategy_options for in keyspace '%s': %v\",\n\t\t\t\tstrategyOptionsJSON, keyspace.Name, err,\n\t\t\t)\n\t\t}\n\t}\n\n\treturn keyspace, nil\n}\n\n// query for only the table metadata in the specified keyspace from system.schema_columnfamilies\nfunc getTableMetadata(session *Session, keyspaceName string) ([]TableMetadata, error) {\n\n\tvar (\n\t\titer *Iter\n\t\tscan func(iter *Iter, table *TableMetadata) bool\n\t\tstmt string\n\n\t\tkeyAliasesJSON    []byte\n\t\tcolumnAliasesJSON []byte\n\t)\n\n\tif session.useSystemSchema { // Cassandra 3.x+\n\t\tstmt = `\n\t\tSELECT\n\t\t\ttable_name\n\t\tFROM system_schema.tables\n\t\tWHERE keyspace_name = ?`\n\n\t\tswitchIter := func() *Iter {\n\t\t\titer.Close()\n\t\t\tstmt = `\n\t\t\t\tSELECT\n\t\t\t\t\tview_name\n\t\t\t\tFROM system_schema.views\n\t\t\t\tWHERE keyspace_name = ?`\n\t\t\titer = session.control.query(stmt, keyspaceName)\n\t\t\treturn iter\n\t\t}\n\n\t\tscan = func(iter *Iter, table *TableMetadata) bool {\n\t\t\tr := iter.Scan(\n\t\t\t\t&table.Name,\n\t\t\t)\n\t\t\tif !r {\n\t\t\t\titer = switchIter()\n\t\t\t\tif iter != nil {\n\t\t\t\t\tswitchIter = func() *Iter { return nil }\n\t\t\t\t\tr = iter.Scan(&table.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn r\n\t\t}\n\t} else if session.cfg.ProtoVersion == protoVersion1 {\n\t\t// we have key aliases\n\t\tstmt = `\n\t\tSELECT\n\t\t\tcolumnfamily_name,\n\t\t\tkey_validator,\n\t\t\tcomparator,\n\t\t\tdefault_validator,\n\t\t\tkey_aliases,\n\t\t\tcolumn_aliases,\n\t\t\tvalue_alias\n\t\tFROM system.schema_columnfamilies\n\t\tWHERE keyspace_name = ?`\n\n\t\tscan = func(iter *Iter, table *TableMetadata) bool {\n\t\t\treturn iter.Scan(\n\t\t\t\t&table.Name,\n\t\t\t\t&table.KeyValidator,\n\t\t\t\t&table.Comparator,\n\t\t\t\t&table.DefaultValidator,\n\t\t\t\t&keyAliasesJSON,\n\t\t\t\t&columnAliasesJSON,\n\t\t\t\t&table.ValueAlias,\n\t\t\t)\n\t\t}\n\t} else {\n\t\tstmt = `\n\t\tSELECT\n\t\t\tcolumnfamily_name,\n\t\t\tkey_validator,\n\t\t\tcomparator,\n\t\t\tdefault_validator\n\t\tFROM system.schema_columnfamilies\n\t\tWHERE keyspace_name = ?`\n\n\t\tscan = func(iter *Iter, table *TableMetadata) bool {\n\t\t\treturn iter.Scan(\n\t\t\t\t&table.Name,\n\t\t\t\t&table.KeyValidator,\n\t\t\t\t&table.Comparator,\n\t\t\t\t&table.DefaultValidator,\n\t\t\t)\n\t\t}\n\t}\n\n\titer = session.control.query(stmt, keyspaceName)\n\n\ttables := []TableMetadata{}\n\ttable := TableMetadata{Keyspace: keyspaceName}\n\n\tfor scan(iter, &table) {\n\t\tvar err error\n\n\t\t// decode the key aliases\n\t\tif keyAliasesJSON != nil {\n\t\t\ttable.KeyAliases = []string{}\n\t\t\terr = json.Unmarshal(keyAliasesJSON, &table.KeyAliases)\n\t\t\tif err != nil {\n\t\t\t\titer.Close()\n\t\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\t\"invalid JSON value '%s' as key_aliases for in table '%s': %v\",\n\t\t\t\t\tkeyAliasesJSON, table.Name, err,\n\t\t\t\t)\n\t\t\t}\n\t\t}\n\n\t\t// decode the column aliases\n\t\tif columnAliasesJSON != nil {\n\t\t\ttable.ColumnAliases = []string{}\n\t\t\terr = json.Unmarshal(columnAliasesJSON, &table.ColumnAliases)\n\t\t\tif err != nil {\n\t\t\t\titer.Close()\n\t\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\t\"invalid JSON value '%s' as column_aliases for in table '%s': %v\",\n\t\t\t\t\tcolumnAliasesJSON, table.Name, err,\n\t\t\t\t)\n\t\t\t}\n\t\t}\n\n\t\ttables = append(tables, table)\n\t\ttable = TableMetadata{Keyspace: keyspaceName}\n\t}\n\n\terr := iter.Close()\n\tif err != nil && err != ErrNotFound {\n\t\treturn nil, fmt.Errorf(\"error querying table schema: %v\", err)\n\t}\n\n\treturn tables, nil\n}\n\nfunc (s *Session) scanColumnMetadataV1(keyspace string) ([]ColumnMetadata, error) {\n\t// V1 does not support the type column, and all returned rows are\n\t// of kind \"regular\".\n\tconst stmt = `\n\t\tSELECT\n\t\t\t\tcolumnfamily_name,\n\t\t\t\tcolumn_name,\n\t\t\t\tcomponent_index,\n\t\t\t\tvalidator,\n\t\t\t\tindex_name,\n\t\t\t\tindex_type,\n\t\t\t\tindex_options\n\t\t\tFROM system.schema_columns\n\t\t\tWHERE keyspace_name = ?`\n\n\tvar columns []ColumnMetadata\n\n\trows := s.control.query(stmt, keyspace).Scanner()\n\tfor rows.Next() {\n\t\tvar (\n\t\t\tcolumn           = ColumnMetadata{Keyspace: keyspace}\n\t\t\tindexOptionsJSON []byte\n\t\t)\n\n\t\t// all columns returned by V1 are regular\n\t\tcolumn.Kind = ColumnRegular\n\n\t\terr := rows.Scan(&column.Table,\n\t\t\t&column.Name,\n\t\t\t&column.ComponentIndex,\n\t\t\t&column.Validator,\n\t\t\t&column.Index.Name,\n\t\t\t&column.Index.Type,\n\t\t\t&indexOptionsJSON)\n\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif len(indexOptionsJSON) > 0 {\n\t\t\terr := json.Unmarshal(indexOptionsJSON, &column.Index.Options)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\t\"invalid JSON value '%s' as index_options for column '%s' in table '%s': %v\",\n\t\t\t\t\tindexOptionsJSON,\n\t\t\t\t\tcolumn.Name,\n\t\t\t\t\tcolumn.Table,\n\t\t\t\t\terr)\n\t\t\t}\n\t\t}\n\n\t\tcolumns = append(columns, column)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn columns, nil\n}\n\nfunc (s *Session) scanColumnMetadataV2(keyspace string) ([]ColumnMetadata, error) {\n\t// V2+ supports the type column\n\tconst stmt = `\n\t\t\tSELECT\n\t\t\t\tcolumnfamily_name,\n\t\t\t\tcolumn_name,\n\t\t\t\tcomponent_index,\n\t\t\t\tvalidator,\n\t\t\t\tindex_name,\n\t\t\t\tindex_type,\n\t\t\t\tindex_options,\n\t\t\t\ttype\n\t\t\tFROM system.schema_columns\n\t\t\tWHERE keyspace_name = ?`\n\n\tvar columns []ColumnMetadata\n\n\trows := s.control.query(stmt, keyspace).Scanner()\n\tfor rows.Next() {\n\t\tvar (\n\t\t\tcolumn           = ColumnMetadata{Keyspace: keyspace}\n\t\t\tindexOptionsJSON []byte\n\t\t)\n\n\t\terr := rows.Scan(&column.Table,\n\t\t\t&column.Name,\n\t\t\t&column.ComponentIndex,\n\t\t\t&column.Validator,\n\t\t\t&column.Index.Name,\n\t\t\t&column.Index.Type,\n\t\t\t&indexOptionsJSON,\n\t\t\t&column.Kind,\n\t\t)\n\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif len(indexOptionsJSON) > 0 {\n\t\t\terr := json.Unmarshal(indexOptionsJSON, &column.Index.Options)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\t\"invalid JSON value '%s' as index_options for column '%s' in table '%s': %v\",\n\t\t\t\t\tindexOptionsJSON,\n\t\t\t\t\tcolumn.Name,\n\t\t\t\t\tcolumn.Table,\n\t\t\t\t\terr)\n\t\t\t}\n\t\t}\n\n\t\tcolumns = append(columns, column)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn columns, nil\n\n}\n\nfunc (s *Session) scanColumnMetadataSystem(keyspace string) ([]ColumnMetadata, error) {\n\tconst stmt = `\n\t\t\tSELECT\n\t\t\t\ttable_name,\n\t\t\t\tcolumn_name,\n\t\t\t\tclustering_order,\n\t\t\t\ttype,\n\t\t\t\tkind,\n\t\t\t\tposition\n\t\t\tFROM system_schema.columns\n\t\t\tWHERE keyspace_name = ?`\n\n\tvar columns []ColumnMetadata\n\n\trows := s.control.query(stmt, keyspace).Scanner()\n\tfor rows.Next() {\n\t\tcolumn := ColumnMetadata{Keyspace: keyspace}\n\n\t\terr := rows.Scan(&column.Table,\n\t\t\t&column.Name,\n\t\t\t&column.ClusteringOrder,\n\t\t\t&column.Validator,\n\t\t\t&column.Kind,\n\t\t\t&column.ComponentIndex,\n\t\t)\n\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tcolumns = append(columns, column)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO(zariel): get column index info from system_schema.indexes\n\n\treturn columns, nil\n}\n\n// query for only the column metadata in the specified keyspace from system.schema_columns\nfunc getColumnMetadata(session *Session, keyspaceName string) ([]ColumnMetadata, error) {\n\tvar (\n\t\tcolumns []ColumnMetadata\n\t\terr     error\n\t)\n\n\t// Deal with differences in protocol versions\n\tif session.cfg.ProtoVersion == 1 {\n\t\tcolumns, err = session.scanColumnMetadataV1(keyspaceName)\n\t} else if session.useSystemSchema { // Cassandra 3.x+\n\t\tcolumns, err = session.scanColumnMetadataSystem(keyspaceName)\n\t} else {\n\t\tcolumns, err = session.scanColumnMetadataV2(keyspaceName)\n\t}\n\n\tif err != nil && err != ErrNotFound {\n\t\treturn nil, fmt.Errorf(\"error querying column schema: %v\", err)\n\t}\n\n\treturn columns, nil\n}\n\nfunc getTypeInfo(t string, logger StdLogger) TypeInfo {\n\tif strings.HasPrefix(t, apacheCassandraTypePrefix) {\n\t\tt = apacheToCassandraType(t)\n\t}\n\treturn getCassandraType(t, logger)\n}\n\nfunc getViewsMetadata(session *Session, keyspaceName string) ([]ViewMetadata, error) {\n\tif session.cfg.ProtoVersion == protoVersion1 {\n\t\treturn nil, nil\n\t}\n\tvar tableName string\n\tif session.useSystemSchema {\n\t\ttableName = \"system_schema.types\"\n\t} else {\n\t\ttableName = \"system.schema_usertypes\"\n\t}\n\tstmt := fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttype_name,\n\t\t\tfield_names,\n\t\t\tfield_types\n\t\tFROM %s\n\t\tWHERE keyspace_name = ?`, tableName)\n\n\tvar views []ViewMetadata\n\n\trows := session.control.query(stmt, keyspaceName).Scanner()\n\tfor rows.Next() {\n\t\tview := ViewMetadata{Keyspace: keyspaceName}\n\t\tvar argumentTypes []string\n\t\terr := rows.Scan(&view.Name,\n\t\t\t&view.FieldNames,\n\t\t\t&argumentTypes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tview.FieldTypes = make([]TypeInfo, len(argumentTypes))\n\t\tfor i, argumentType := range argumentTypes {\n\t\t\tview.FieldTypes[i] = getTypeInfo(argumentType, session.logger)\n\t\t}\n\t\tviews = append(views, view)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn views, nil\n}\n\nfunc getMaterializedViewsMetadata(session *Session, keyspaceName string) ([]MaterializedViewMetadata, error) {\n\tif !session.useSystemSchema {\n\t\treturn nil, nil\n\t}\n\tvar tableName = \"system_schema.views\"\n\tstmt := fmt.Sprintf(`\n\t\tSELECT\n\t\t\tview_name,\n\t\t\tbase_table_id,\n\t\t\tbase_table_name,\n\t\t\tbloom_filter_fp_chance,\n\t\t\tcaching,\n\t\t\tcomment,\n\t\t\tcompaction,\n\t\t\tcompression,\n\t\t\tcrc_check_chance,\n\t\t\tdclocal_read_repair_chance,\n\t\t\tdefault_time_to_live,\n\t\t\textensions,\n\t\t\tgc_grace_seconds,\n\t\t\tid,\n\t\t\tinclude_all_columns,\n\t\t\tmax_index_interval,\n\t\t\tmemtable_flush_period_in_ms,\n\t\t\tmin_index_interval,\n\t\t\tread_repair_chance,\n\t\t\tspeculative_retry\n\t\tFROM %s\n\t\tWHERE keyspace_name = ?`, tableName)\n\n\tvar materializedViews []MaterializedViewMetadata\n\n\trows := session.control.query(stmt, keyspaceName).Scanner()\n\tfor rows.Next() {\n\t\tmaterializedView := MaterializedViewMetadata{Keyspace: keyspaceName}\n\t\terr := rows.Scan(&materializedView.Name,\n\t\t\t&materializedView.BaseTableId,\n\t\t\t&materializedView.baseTableName,\n\t\t\t&materializedView.BloomFilterFpChance,\n\t\t\t&materializedView.Caching,\n\t\t\t&materializedView.Comment,\n\t\t\t&materializedView.Compaction,\n\t\t\t&materializedView.Compression,\n\t\t\t&materializedView.CrcCheckChance,\n\t\t\t&materializedView.DcLocalReadRepairChance,\n\t\t\t&materializedView.DefaultTimeToLive,\n\t\t\t&materializedView.Extensions,\n\t\t\t&materializedView.GcGraceSeconds,\n\t\t\t&materializedView.Id,\n\t\t\t&materializedView.IncludeAllColumns,\n\t\t\t&materializedView.MaxIndexInterval,\n\t\t\t&materializedView.MemtableFlushPeriodInMs,\n\t\t\t&materializedView.MinIndexInterval,\n\t\t\t&materializedView.ReadRepairChance,\n\t\t\t&materializedView.SpeculativeRetry,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmaterializedViews = append(materializedViews, materializedView)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn materializedViews, nil\n}\n\nfunc getFunctionsMetadata(session *Session, keyspaceName string) ([]FunctionMetadata, error) {\n\tif session.cfg.ProtoVersion == protoVersion1 || !session.hasAggregatesAndFunctions {\n\t\treturn nil, nil\n\t}\n\tvar tableName string\n\tif session.useSystemSchema {\n\t\ttableName = \"system_schema.functions\"\n\t} else {\n\t\ttableName = \"system.schema_functions\"\n\t}\n\tstmt := fmt.Sprintf(`\n\t\tSELECT\n\t\t\tfunction_name,\n\t\t\targument_types,\n\t\t\targument_names,\n\t\t\tbody,\n\t\t\tcalled_on_null_input,\n\t\t\tlanguage,\n\t\t\treturn_type\n\t\tFROM %s\n\t\tWHERE keyspace_name = ?`, tableName)\n\n\tvar functions []FunctionMetadata\n\n\trows := session.control.query(stmt, keyspaceName).Scanner()\n\tfor rows.Next() {\n\t\tfunction := FunctionMetadata{Keyspace: keyspaceName}\n\t\tvar argumentTypes []string\n\t\tvar returnType string\n\t\terr := rows.Scan(&function.Name,\n\t\t\t&argumentTypes,\n\t\t\t&function.ArgumentNames,\n\t\t\t&function.Body,\n\t\t\t&function.CalledOnNullInput,\n\t\t\t&function.Language,\n\t\t\t&returnType,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfunction.ReturnType = getTypeInfo(returnType, session.logger)\n\t\tfunction.ArgumentTypes = make([]TypeInfo, len(argumentTypes))\n\t\tfor i, argumentType := range argumentTypes {\n\t\t\tfunction.ArgumentTypes[i] = getTypeInfo(argumentType, session.logger)\n\t\t}\n\t\tfunctions = append(functions, function)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn functions, nil\n}\n\nfunc getAggregatesMetadata(session *Session, keyspaceName string) ([]AggregateMetadata, error) {\n\tif session.cfg.ProtoVersion == protoVersion1 || !session.hasAggregatesAndFunctions {\n\t\treturn nil, nil\n\t}\n\tvar tableName string\n\tif session.useSystemSchema {\n\t\ttableName = \"system_schema.aggregates\"\n\t} else {\n\t\ttableName = \"system.schema_aggregates\"\n\t}\n\n\tstmt := fmt.Sprintf(`\n\t\tSELECT\n\t\t\taggregate_name,\n\t\t\targument_types,\n\t\t\tfinal_func,\n\t\t\tinitcond,\n\t\t\treturn_type,\n\t\t\tstate_func,\n\t\t\tstate_type\n\t\tFROM %s\n\t\tWHERE keyspace_name = ?`, tableName)\n\n\tvar aggregates []AggregateMetadata\n\n\trows := session.control.query(stmt, keyspaceName).Scanner()\n\tfor rows.Next() {\n\t\taggregate := AggregateMetadata{Keyspace: keyspaceName}\n\t\tvar argumentTypes []string\n\t\tvar returnType string\n\t\tvar stateType string\n\t\terr := rows.Scan(&aggregate.Name,\n\t\t\t&argumentTypes,\n\t\t\t&aggregate.finalFunc,\n\t\t\t&aggregate.InitCond,\n\t\t\t&returnType,\n\t\t\t&aggregate.stateFunc,\n\t\t\t&stateType,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\taggregate.ReturnType = getTypeInfo(returnType, session.logger)\n\t\taggregate.StateType = getTypeInfo(stateType, session.logger)\n\t\taggregate.ArgumentTypes = make([]TypeInfo, len(argumentTypes))\n\t\tfor i, argumentType := range argumentTypes {\n\t\t\taggregate.ArgumentTypes[i] = getTypeInfo(argumentType, session.logger)\n\t\t}\n\t\taggregates = append(aggregates, aggregate)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn aggregates, nil\n}\n\n// type definition parser state\ntype typeParser struct {\n\tinput  string\n\tindex  int\n\tlogger StdLogger\n}\n\n// the type definition parser result\ntype typeParserResult struct {\n\tisComposite bool\n\ttypes       []TypeInfo\n\treversed    []bool\n\tcollections map[string]TypeInfo\n}\n\n// Parse the type definition used for validator and comparator schema data\nfunc parseType(def string, logger StdLogger) typeParserResult {\n\tparser := &typeParser{input: def, logger: logger}\n\treturn parser.parse()\n}\n\nconst (\n\tREVERSED_TYPE   = \"org.apache.cassandra.db.marshal.ReversedType\"\n\tCOMPOSITE_TYPE  = \"org.apache.cassandra.db.marshal.CompositeType\"\n\tCOLLECTION_TYPE = \"org.apache.cassandra.db.marshal.ColumnToCollectionType\"\n\tLIST_TYPE       = \"org.apache.cassandra.db.marshal.ListType\"\n\tSET_TYPE        = \"org.apache.cassandra.db.marshal.SetType\"\n\tMAP_TYPE        = \"org.apache.cassandra.db.marshal.MapType\"\n)\n\n// represents a class specification in the type def AST\ntype typeParserClassNode struct {\n\tname   string\n\tparams []typeParserParamNode\n\t// this is the segment of the input string that defined this node\n\tinput string\n}\n\n// represents a class parameter in the type def AST\ntype typeParserParamNode struct {\n\tname  *string\n\tclass typeParserClassNode\n}\n\nfunc (t *typeParser) parse() typeParserResult {\n\t// parse the AST\n\tast, ok := t.parseClassNode()\n\tif !ok {\n\t\t// treat this is a custom type\n\t\treturn typeParserResult{\n\t\t\tisComposite: false,\n\t\t\ttypes: []TypeInfo{\n\t\t\t\tNativeType{\n\t\t\t\t\ttyp:    TypeCustom,\n\t\t\t\t\tcustom: t.input,\n\t\t\t\t},\n\t\t\t},\n\t\t\treversed:    []bool{false},\n\t\t\tcollections: nil,\n\t\t}\n\t}\n\n\t// interpret the AST\n\tif strings.HasPrefix(ast.name, COMPOSITE_TYPE) {\n\t\tcount := len(ast.params)\n\n\t\t// look for a collections param\n\t\tlast := ast.params[count-1]\n\t\tcollections := map[string]TypeInfo{}\n\t\tif strings.HasPrefix(last.class.name, COLLECTION_TYPE) {\n\t\t\tcount--\n\n\t\t\tfor _, param := range last.class.params {\n\t\t\t\t// decode the name\n\t\t\t\tvar name string\n\t\t\t\tdecoded, err := hex.DecodeString(*param.name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.logger.Printf(\n\t\t\t\t\t\t\"Error parsing type '%s', contains collection name '%s' with an invalid format: %v\",\n\t\t\t\t\t\tt.input,\n\t\t\t\t\t\t*param.name,\n\t\t\t\t\t\terr,\n\t\t\t\t\t)\n\t\t\t\t\t// just use the provided name\n\t\t\t\t\tname = *param.name\n\t\t\t\t} else {\n\t\t\t\t\tname = string(decoded)\n\t\t\t\t}\n\t\t\t\tcollections[name] = param.class.asTypeInfo()\n\t\t\t}\n\t\t}\n\n\t\ttypes := make([]TypeInfo, count)\n\t\treversed := make([]bool, count)\n\n\t\tfor i, param := range ast.params[:count] {\n\t\t\tclass := param.class\n\t\t\treversed[i] = strings.HasPrefix(class.name, REVERSED_TYPE)\n\t\t\tif reversed[i] {\n\t\t\t\tclass = class.params[0].class\n\t\t\t}\n\t\t\ttypes[i] = class.asTypeInfo()\n\t\t}\n\n\t\treturn typeParserResult{\n\t\t\tisComposite: true,\n\t\t\ttypes:       types,\n\t\t\treversed:    reversed,\n\t\t\tcollections: collections,\n\t\t}\n\t} else {\n\t\t// not composite, so one type\n\t\tclass := *ast\n\t\treversed := strings.HasPrefix(class.name, REVERSED_TYPE)\n\t\tif reversed {\n\t\t\tclass = class.params[0].class\n\t\t}\n\t\ttypeInfo := class.asTypeInfo()\n\n\t\treturn typeParserResult{\n\t\t\tisComposite: false,\n\t\t\ttypes:       []TypeInfo{typeInfo},\n\t\t\treversed:    []bool{reversed},\n\t\t}\n\t}\n}\n\nfunc (class *typeParserClassNode) asTypeInfo() TypeInfo {\n\tif strings.HasPrefix(class.name, LIST_TYPE) {\n\t\telem := class.params[0].class.asTypeInfo()\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{\n\t\t\t\ttyp: TypeList,\n\t\t\t},\n\t\t\tElem: elem,\n\t\t}\n\t}\n\tif strings.HasPrefix(class.name, SET_TYPE) {\n\t\telem := class.params[0].class.asTypeInfo()\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{\n\t\t\t\ttyp: TypeSet,\n\t\t\t},\n\t\t\tElem: elem,\n\t\t}\n\t}\n\tif strings.HasPrefix(class.name, MAP_TYPE) {\n\t\tkey := class.params[0].class.asTypeInfo()\n\t\telem := class.params[1].class.asTypeInfo()\n\t\treturn CollectionType{\n\t\t\tNativeType: NativeType{\n\t\t\t\ttyp: TypeMap,\n\t\t\t},\n\t\t\tKey:  key,\n\t\t\tElem: elem,\n\t\t}\n\t}\n\n\t// must be a simple type or custom type\n\tinfo := NativeType{typ: getApacheCassandraType(class.name)}\n\tif info.typ == TypeCustom {\n\t\t// add the entire class definition\n\t\tinfo.custom = class.input\n\t}\n\treturn info\n}\n\n// CLASS := ID [ PARAMS ]\nfunc (t *typeParser) parseClassNode() (node *typeParserClassNode, ok bool) {\n\tt.skipWhitespace()\n\n\tstartIndex := t.index\n\n\tname, ok := t.nextIdentifier()\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\tparams, ok := t.parseParamNodes()\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\tendIndex := t.index\n\n\tnode = &typeParserClassNode{\n\t\tname:   name,\n\t\tparams: params,\n\t\tinput:  t.input[startIndex:endIndex],\n\t}\n\treturn node, true\n}\n\n// PARAMS := \"(\" PARAM { \",\" PARAM } \")\"\n// PARAM := [ PARAM_NAME \":\" ] CLASS\n// PARAM_NAME := ID\nfunc (t *typeParser) parseParamNodes() (params []typeParserParamNode, ok bool) {\n\tt.skipWhitespace()\n\n\t// the params are optional\n\tif t.index == len(t.input) || t.input[t.index] != '(' {\n\t\treturn nil, true\n\t}\n\n\tparams = []typeParserParamNode{}\n\n\t// consume the '('\n\tt.index++\n\n\tt.skipWhitespace()\n\n\tfor t.input[t.index] != ')' {\n\t\t// look for a named param, but if no colon, then we want to backup\n\t\tbackupIndex := t.index\n\n\t\t// name will be a hex encoded version of a utf-8 string\n\t\tname, ok := t.nextIdentifier()\n\t\tif !ok {\n\t\t\treturn nil, false\n\t\t}\n\t\thasName := true\n\n\t\t// TODO handle '=>' used for DynamicCompositeType\n\n\t\tt.skipWhitespace()\n\n\t\tif t.input[t.index] == ':' {\n\t\t\t// there is a name for this parameter\n\n\t\t\t// consume the ':'\n\t\t\tt.index++\n\n\t\t\tt.skipWhitespace()\n\t\t} else {\n\t\t\t// no name, backup\n\t\t\thasName = false\n\t\t\tt.index = backupIndex\n\t\t}\n\n\t\t// parse the next full parameter\n\t\tclassNode, ok := t.parseClassNode()\n\t\tif !ok {\n\t\t\treturn nil, false\n\t\t}\n\n\t\tif hasName {\n\t\t\tparams = append(\n\t\t\t\tparams,\n\t\t\t\ttypeParserParamNode{name: &name, class: *classNode},\n\t\t\t)\n\t\t} else {\n\t\t\tparams = append(\n\t\t\t\tparams,\n\t\t\t\ttypeParserParamNode{class: *classNode},\n\t\t\t)\n\t\t}\n\n\t\tt.skipWhitespace()\n\n\t\tif t.input[t.index] == ',' {\n\t\t\t// consume the comma\n\t\t\tt.index++\n\n\t\t\tt.skipWhitespace()\n\t\t}\n\t}\n\n\t// consume the ')'\n\tt.index++\n\n\treturn params, true\n}\n\nfunc (t *typeParser) skipWhitespace() {\n\tfor t.index < len(t.input) && isWhitespaceChar(t.input[t.index]) {\n\t\tt.index++\n\t}\n}\n\nfunc isWhitespaceChar(c byte) bool {\n\treturn c == ' ' || c == '\\n' || c == '\\t'\n}\n\n// ID := LETTER { LETTER }\n// LETTER := \"0\"...\"9\" | \"a\"...\"z\" | \"A\"...\"Z\" | \"-\" | \"+\" | \".\" | \"_\" | \"&\"\nfunc (t *typeParser) nextIdentifier() (id string, found bool) {\n\tstartIndex := t.index\n\tfor t.index < len(t.input) && isIdentifierChar(t.input[t.index]) {\n\t\tt.index++\n\t}\n\tif startIndex == t.index {\n\t\treturn \"\", false\n\t}\n\treturn t.input[startIndex:t.index], true\n}\n\nfunc isIdentifierChar(c byte) bool {\n\treturn (c >= '0' && c <= '9') ||\n\t\t(c >= 'a' && c <= 'z') ||\n\t\t(c >= 'A' && c <= 'Z') ||\n\t\tc == '-' ||\n\t\tc == '+' ||\n\t\tc == '.' ||\n\t\tc == '_' ||\n\t\tc == '&'\n}\n"
        },
        {
          "name": "metadata_test.go",
          "type": "blob",
          "size": 30.103515625,
          "content": "// Copyright (c) 2015 The gocql Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"strconv\"\n\t\"testing\"\n)\n\n// Tests V1 and V2 metadata \"compilation\" from example data which might be returned\n// from metadata schema queries (see getKeyspaceMetadata, getTableMetadata, and getColumnMetadata)\nfunc TestCompileMetadata(t *testing.T) {\n\t// V1 tests - these are all based on real examples from the integration test ccm cluster\n\tlog := &defaultLogger{}\n\tkeyspace := &KeyspaceMetadata{\n\t\tName: \"V1Keyspace\",\n\t}\n\ttables := []TableMetadata{\n\t\t{\n\t\t\t// This table, found in the system keyspace, has no key aliases or column aliases\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"Schema\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{},\n\t\t\tColumnAliases:    []string{},\n\t\t\tValueAlias:       \"\",\n\t\t},\n\t\t{\n\t\t\t// This table, found in the system keyspace, has key aliases, column aliases, and a value alias.\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"hints\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.UUIDType\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.Int32Type)\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{\"target_id\"},\n\t\t\tColumnAliases:    []string{\"hint_id\", \"message_version\"},\n\t\t\tValueAlias:       \"mutation\",\n\t\t},\n\t\t{\n\t\t\t// This table, found in the system keyspace, has a comparator with collections, but no column aliases\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"peers\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.InetAddressType\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(746f6b656e73:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)))\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{\"peer\"},\n\t\t\tColumnAliases:    []string{},\n\t\t\tValueAlias:       \"\",\n\t\t},\n\t\t{\n\t\t\t// This table, found in the system keyspace, has a column alias, but not a composite comparator\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"IndexInfo\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UTF8Type)\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{\"table_name\"},\n\t\t\tColumnAliases:    []string{\"index_name\"},\n\t\t\tValueAlias:       \"\",\n\t\t},\n\t\t{\n\t\t\t// This table, found in the gocql_test keyspace following an integration test run, has a composite comparator with collections as well as a column alias\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"wiki_page\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(74616773:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type),6174746163686d656e7473:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.BytesType)))\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{\"title\"},\n\t\t\tColumnAliases:    []string{\"revid\"},\n\t\t\tValueAlias:       \"\",\n\t\t},\n\t\t{\n\t\t\t// This is a made up example with multiple unnamed aliases\n\t\t\tKeyspace:         \"V1Keyspace\",\n\t\t\tName:             \"no_names\",\n\t\t\tKeyValidator:     \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UUIDType,org.apache.cassandra.db.marshal.UUIDType)\",\n\t\t\tComparator:       \"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.Int32Type)\",\n\t\t\tDefaultValidator: \"org.apache.cassandra.db.marshal.BytesType\",\n\t\t\tKeyAliases:       []string{},\n\t\t\tColumnAliases:    []string{},\n\t\t\tValueAlias:       \"\",\n\t\t},\n\t}\n\tcolumns := []ColumnMetadata{\n\t\t// Here are the regular columns from the peers table for testing regular columns\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"data_center\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"host_id\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UUIDType\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"rack\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"release_version\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"rpc_address\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.InetAddressType\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"schema_version\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UUIDType\"},\n\t\t{Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"tokens\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)\"},\n\t}\n\tcompileMetadata(1, keyspace, tables, columns, nil, nil, nil, nil, log)\n\tassertKeyspaceMetadata(\n\t\tt,\n\t\tkeyspace,\n\t\t&KeyspaceMetadata{\n\t\t\tName: \"V1Keyspace\",\n\t\t\tTables: map[string]*TableMetadata{\n\t\t\t\t\"Schema\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"key\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeBlob},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"key\": {\n\t\t\t\t\t\t\tName: \"key\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeBlob},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"hints\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"target_id\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"hint_id\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeTimeUUID},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"message_version\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"target_id\": {\n\t\t\t\t\t\t\tName: \"target_id\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"hint_id\": {\n\t\t\t\t\t\t\tName:  \"hint_id\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeTimeUUID},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"message_version\": {\n\t\t\t\t\t\t\tName:  \"message_version\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"mutation\": {\n\t\t\t\t\t\t\tName: \"mutation\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeBlob},\n\t\t\t\t\t\t\tKind: ColumnRegular,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"peers\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"peer\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeInet},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"peer\": {\n\t\t\t\t\t\t\tName: \"peer\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeInet},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"data_center\":     {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"data_center\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\", Type: NativeType{typ: TypeVarchar}},\n\t\t\t\t\t\t\"host_id\":         {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"host_id\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UUIDType\", Type: NativeType{typ: TypeUUID}},\n\t\t\t\t\t\t\"rack\":            {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"rack\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\", Type: NativeType{typ: TypeVarchar}},\n\t\t\t\t\t\t\"release_version\": {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"release_version\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UTF8Type\", Type: NativeType{typ: TypeVarchar}},\n\t\t\t\t\t\t\"rpc_address\":     {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"rpc_address\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.InetAddressType\", Type: NativeType{typ: TypeInet}},\n\t\t\t\t\t\t\"schema_version\":  {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"schema_version\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.UUIDType\", Type: NativeType{typ: TypeUUID}},\n\t\t\t\t\t\t\"tokens\":          {Keyspace: \"V1Keyspace\", Table: \"peers\", Kind: ColumnRegular, Name: \"tokens\", ComponentIndex: 0, Validator: \"org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)\", Type: CollectionType{NativeType: NativeType{typ: TypeSet}}},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"IndexInfo\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"table_name\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"index_name\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: DESC,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"table_name\": {\n\t\t\t\t\t\t\tName: \"table_name\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"index_name\": {\n\t\t\t\t\t\t\tName:  \"index_name\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: DESC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"value\": {\n\t\t\t\t\t\t\tName: \"value\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeBlob},\n\t\t\t\t\t\t\tKind: ColumnRegular,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"wiki_page\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"title\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"revid\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeTimeUUID},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"title\": {\n\t\t\t\t\t\t\tName: \"title\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"revid\": {\n\t\t\t\t\t\t\tName: \"revid\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeTimeUUID},\n\t\t\t\t\t\t\tKind: ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"no_names\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"key\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"key2\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"column\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"column2\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"column3\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"key\": {\n\t\t\t\t\t\t\tName: \"key\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"key2\": {\n\t\t\t\t\t\t\tName: \"key2\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeUUID},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"column\": {\n\t\t\t\t\t\t\tName:  \"column\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"column2\": {\n\t\t\t\t\t\t\tName:  \"column2\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"column3\": {\n\t\t\t\t\t\t\tName:  \"column3\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeInt},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"value\": {\n\t\t\t\t\t\t\tName: \"value\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeBlob},\n\t\t\t\t\t\t\tKind: ColumnRegular,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t)\n\n\t// V2 test - V2+ protocol is simpler so here are some toy examples to verify that the mapping works\n\tkeyspace = &KeyspaceMetadata{\n\t\tName: \"V2Keyspace\",\n\t}\n\ttables = []TableMetadata{\n\t\t{\n\t\t\tKeyspace: \"V2Keyspace\",\n\t\t\tName:     \"Table1\",\n\t\t},\n\t\t{\n\t\t\tKeyspace: \"V2Keyspace\",\n\t\t\tName:     \"Table2\",\n\t\t},\n\t}\n\tcolumns = []ColumnMetadata{\n\t\t{\n\t\t\tKeyspace:       \"V2Keyspace\",\n\t\t\tTable:          \"Table1\",\n\t\t\tName:           \"KEY1\",\n\t\t\tKind:           ColumnPartitionKey,\n\t\t\tComponentIndex: 0,\n\t\t\tValidator:      \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t},\n\t\t{\n\t\t\tKeyspace:       \"V2Keyspace\",\n\t\t\tTable:          \"Table1\",\n\t\t\tName:           \"Key1\",\n\t\t\tKind:           ColumnPartitionKey,\n\t\t\tComponentIndex: 0,\n\t\t\tValidator:      \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t},\n\t\t{\n\t\t\tKeyspace:       \"V2Keyspace\",\n\t\t\tTable:          \"Table2\",\n\t\t\tName:           \"Column1\",\n\t\t\tKind:           ColumnPartitionKey,\n\t\t\tComponentIndex: 0,\n\t\t\tValidator:      \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t},\n\t\t{\n\t\t\tKeyspace:       \"V2Keyspace\",\n\t\t\tTable:          \"Table2\",\n\t\t\tName:           \"Column2\",\n\t\t\tKind:           ColumnClusteringKey,\n\t\t\tComponentIndex: 0,\n\t\t\tValidator:      \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t},\n\t\t{\n\t\t\tKeyspace:       \"V2Keyspace\",\n\t\t\tTable:          \"Table2\",\n\t\t\tName:           \"Column3\",\n\t\t\tKind:           ColumnClusteringKey,\n\t\t\tComponentIndex: 1,\n\t\t\tValidator:      \"org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UTF8Type)\",\n\t\t},\n\t\t{\n\t\t\tKeyspace:  \"V2Keyspace\",\n\t\t\tTable:     \"Table2\",\n\t\t\tName:      \"Column4\",\n\t\t\tKind:      ColumnRegular,\n\t\t\tValidator: \"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\t},\n\t}\n\tcompileMetadata(2, keyspace, tables, columns, nil, nil, nil, nil, log)\n\tassertKeyspaceMetadata(\n\t\tt,\n\t\tkeyspace,\n\t\t&KeyspaceMetadata{\n\t\t\tName: \"V2Keyspace\",\n\t\t\tTables: map[string]*TableMetadata{\n\t\t\t\t\"Table1\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"Key1\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"KEY1\": {\n\t\t\t\t\t\t\tName: \"KEY1\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"Key1\": {\n\t\t\t\t\t\t\tName: \"Key1\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"Table2\": {\n\t\t\t\t\tPartitionKey: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"Column1\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tClusteringColumns: []*ColumnMetadata{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"Column2\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:  \"Column3\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: DESC,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tColumns: map[string]*ColumnMetadata{\n\t\t\t\t\t\t\"Column1\": {\n\t\t\t\t\t\t\tName: \"Column1\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnPartitionKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"Column2\": {\n\t\t\t\t\t\t\tName:  \"Column2\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: ASC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"Column3\": {\n\t\t\t\t\t\t\tName:  \"Column3\",\n\t\t\t\t\t\t\tType:  NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tOrder: DESC,\n\t\t\t\t\t\t\tKind:  ColumnClusteringKey,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"Column4\": {\n\t\t\t\t\t\t\tName: \"Column4\",\n\t\t\t\t\t\t\tType: NativeType{typ: TypeVarchar},\n\t\t\t\t\t\t\tKind: ColumnRegular,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t)\n}\n\n// Helper function for asserting that actual metadata returned was as expected\nfunc assertKeyspaceMetadata(t *testing.T, actual, expected *KeyspaceMetadata) {\n\tif len(expected.Tables) != len(actual.Tables) {\n\t\tt.Errorf(\"Expected len(%s.Tables) to be %v but was %v\", expected.Name, len(expected.Tables), len(actual.Tables))\n\t}\n\tfor keyT := range expected.Tables {\n\t\tet := expected.Tables[keyT]\n\t\tat, found := actual.Tables[keyT]\n\n\t\tif !found {\n\t\t\tt.Errorf(\"Expected %s.Tables[%s] but was not found\", expected.Name, keyT)\n\t\t} else {\n\t\t\tif keyT != at.Name {\n\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Name to be %v but was %v\", expected.Name, keyT, keyT, at.Name)\n\t\t\t}\n\t\t\tif len(et.PartitionKey) != len(at.PartitionKey) {\n\t\t\t\tt.Errorf(\"Expected len(%s.Tables[%s].PartitionKey) to be %v but was %v\", expected.Name, keyT, len(et.PartitionKey), len(at.PartitionKey))\n\t\t\t} else {\n\t\t\t\tfor i := range et.PartitionKey {\n\t\t\t\t\tif et.PartitionKey[i].Name != at.PartitionKey[i].Name {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].Name to be '%v' but was '%v'\", expected.Name, keyT, i, et.PartitionKey[i].Name, at.PartitionKey[i].Name)\n\t\t\t\t\t}\n\t\t\t\t\tif expected.Name != at.PartitionKey[i].Keyspace {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].Keyspace to be '%v' but was '%v'\", expected.Name, keyT, i, expected.Name, at.PartitionKey[i].Keyspace)\n\t\t\t\t\t}\n\t\t\t\t\tif keyT != at.PartitionKey[i].Table {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].Table to be '%v' but was '%v'\", expected.Name, keyT, i, keyT, at.PartitionKey[i].Table)\n\t\t\t\t\t}\n\t\t\t\t\tif et.PartitionKey[i].Type.Type() != at.PartitionKey[i].Type.Type() {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].Type.Type to be %v but was %v\", expected.Name, keyT, i, et.PartitionKey[i].Type.Type(), at.PartitionKey[i].Type.Type())\n\t\t\t\t\t}\n\t\t\t\t\tif i != at.PartitionKey[i].ComponentIndex {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].ComponentIndex to be %v but was %v\", expected.Name, keyT, i, i, at.PartitionKey[i].ComponentIndex)\n\t\t\t\t\t}\n\t\t\t\t\tif ColumnPartitionKey != at.PartitionKey[i].Kind {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].PartitionKey[%d].Kind to be '%v' but was '%v'\", expected.Name, keyT, i, ColumnPartitionKey, at.PartitionKey[i].Kind)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(et.ClusteringColumns) != len(at.ClusteringColumns) {\n\t\t\t\tt.Errorf(\"Expected len(%s.Tables[%s].ClusteringColumns) to be %v but was %v\", expected.Name, keyT, len(et.ClusteringColumns), len(at.ClusteringColumns))\n\t\t\t} else {\n\t\t\t\tfor i := range et.ClusteringColumns {\n\t\t\t\t\tif at.ClusteringColumns[i] == nil {\n\t\t\t\t\t\tt.Fatalf(\"Unexpected nil value: %s.Tables[%s].ClusteringColumns[%d]\", expected.Name, keyT, i)\n\t\t\t\t\t}\n\t\t\t\t\tif et.ClusteringColumns[i].Name != at.ClusteringColumns[i].Name {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Name to be '%v' but was '%v'\", expected.Name, keyT, i, et.ClusteringColumns[i].Name, at.ClusteringColumns[i].Name)\n\t\t\t\t\t}\n\t\t\t\t\tif expected.Name != at.ClusteringColumns[i].Keyspace {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Keyspace to be '%v' but was '%v'\", expected.Name, keyT, i, expected.Name, at.ClusteringColumns[i].Keyspace)\n\t\t\t\t\t}\n\t\t\t\t\tif keyT != at.ClusteringColumns[i].Table {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Table to be '%v' but was '%v'\", expected.Name, keyT, i, keyT, at.ClusteringColumns[i].Table)\n\t\t\t\t\t}\n\t\t\t\t\tif et.ClusteringColumns[i].Type.Type() != at.ClusteringColumns[i].Type.Type() {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Type.Type to be %v but was %v\", expected.Name, keyT, i, et.ClusteringColumns[i].Type.Type(), at.ClusteringColumns[i].Type.Type())\n\t\t\t\t\t}\n\t\t\t\t\tif i != at.ClusteringColumns[i].ComponentIndex {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].ComponentIndex to be %v but was %v\", expected.Name, keyT, i, i, at.ClusteringColumns[i].ComponentIndex)\n\t\t\t\t\t}\n\t\t\t\t\tif et.ClusteringColumns[i].Order != at.ClusteringColumns[i].Order {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Order to be %v but was %v\", expected.Name, keyT, i, et.ClusteringColumns[i].Order, at.ClusteringColumns[i].Order)\n\t\t\t\t\t}\n\t\t\t\t\tif ColumnClusteringKey != at.ClusteringColumns[i].Kind {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].ClusteringColumns[%d].Kind to be '%v' but was '%v'\", expected.Name, keyT, i, ColumnClusteringKey, at.ClusteringColumns[i].Kind)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(et.Columns) != len(at.Columns) {\n\t\t\t\teKeys := make([]string, 0, len(et.Columns))\n\t\t\t\tfor key := range et.Columns {\n\t\t\t\t\teKeys = append(eKeys, key)\n\t\t\t\t}\n\t\t\t\taKeys := make([]string, 0, len(at.Columns))\n\t\t\t\tfor key := range at.Columns {\n\t\t\t\t\taKeys = append(aKeys, key)\n\t\t\t\t}\n\t\t\t\tt.Errorf(\"Expected len(%s.Tables[%s].Columns) to be %v (keys:%v) but was %v (keys:%v)\", expected.Name, keyT, len(et.Columns), eKeys, len(at.Columns), aKeys)\n\t\t\t} else {\n\t\t\t\tfor keyC := range et.Columns {\n\t\t\t\t\tec := et.Columns[keyC]\n\t\t\t\t\tac, found := at.Columns[keyC]\n\n\t\t\t\t\tif !found {\n\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s] but was not found\", expected.Name, keyT, keyC)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif keyC != ac.Name {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Name to be '%v' but was '%v'\", expected.Name, keyT, keyC, keyC, at.Name)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif expected.Name != ac.Keyspace {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Keyspace to be '%v' but was '%v'\", expected.Name, keyT, keyC, expected.Name, ac.Keyspace)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif keyT != ac.Table {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Table to be '%v' but was '%v'\", expected.Name, keyT, keyC, keyT, ac.Table)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ec.Type.Type() != ac.Type.Type() {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Type.Type to be %v but was %v\", expected.Name, keyT, keyC, ec.Type.Type(), ac.Type.Type())\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ec.Order != ac.Order {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Order to be %v but was %v\", expected.Name, keyT, keyC, ec.Order, ac.Order)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ec.Kind != ac.Kind {\n\t\t\t\t\t\t\tt.Errorf(\"Expected %s.Tables[%s].Columns[%s].Kind to be '%v' but was '%v'\", expected.Name, keyT, keyC, ec.Kind, ac.Kind)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Tests the cassandra type definition parser\nfunc TestTypeParser(t *testing.T) {\n\t// native type\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.UTF8Type\",\n\t\tassertTypeInfo{Type: TypeVarchar},\n\t)\n\n\t// reversed\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UUIDType)\",\n\t\tassertTypeInfo{Type: TypeUUID, Reversed: true},\n\t)\n\n\t// set\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.Int32Type)\",\n\t\tassertTypeInfo{\n\t\t\tType: TypeSet,\n\t\t\tElem: &assertTypeInfo{Type: TypeInt},\n\t\t},\n\t)\n\n\t// list\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType)\",\n\t\tassertTypeInfo{\n\t\t\tType: TypeList,\n\t\t\tElem: &assertTypeInfo{Type: TypeTimeUUID},\n\t\t},\n\t)\n\n\t// map\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\" org.apache.cassandra.db.marshal.MapType( org.apache.cassandra.db.marshal.UUIDType , org.apache.cassandra.db.marshal.BytesType ) \",\n\t\tassertTypeInfo{\n\t\t\tType: TypeMap,\n\t\t\tKey:  &assertTypeInfo{Type: TypeUUID},\n\t\t\tElem: &assertTypeInfo{Type: TypeBlob},\n\t\t},\n\t)\n\n\t// custom\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.UserType(sandbox,61646472657373,737472656574:org.apache.cassandra.db.marshal.UTF8Type,63697479:org.apache.cassandra.db.marshal.UTF8Type,7a6970:org.apache.cassandra.db.marshal.Int32Type)\",\n\t\tassertTypeInfo{Type: TypeCustom, Custom: \"org.apache.cassandra.db.marshal.UserType(sandbox,61646472657373,737472656574:org.apache.cassandra.db.marshal.UTF8Type,63697479:org.apache.cassandra.db.marshal.UTF8Type,7a6970:org.apache.cassandra.db.marshal.Int32Type)\"},\n\t)\n\tassertParseNonCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.DynamicCompositeType(u=>org.apache.cassandra.db.marshal.UUIDType,d=>org.apache.cassandra.db.marshal.DateType,t=>org.apache.cassandra.db.marshal.TimeUUIDType,b=>org.apache.cassandra.db.marshal.BytesType,s=>org.apache.cassandra.db.marshal.UTF8Type,B=>org.apache.cassandra.db.marshal.BooleanType,a=>org.apache.cassandra.db.marshal.AsciiType,l=>org.apache.cassandra.db.marshal.LongType,i=>org.apache.cassandra.db.marshal.IntegerType,x=>org.apache.cassandra.db.marshal.LexicalUUIDType)\",\n\t\tassertTypeInfo{Type: TypeCustom, Custom: \"org.apache.cassandra.db.marshal.DynamicCompositeType(u=>org.apache.cassandra.db.marshal.UUIDType,d=>org.apache.cassandra.db.marshal.DateType,t=>org.apache.cassandra.db.marshal.TimeUUIDType,b=>org.apache.cassandra.db.marshal.BytesType,s=>org.apache.cassandra.db.marshal.UTF8Type,B=>org.apache.cassandra.db.marshal.BooleanType,a=>org.apache.cassandra.db.marshal.AsciiType,l=>org.apache.cassandra.db.marshal.LongType,i=>org.apache.cassandra.db.marshal.IntegerType,x=>org.apache.cassandra.db.marshal.LexicalUUIDType)\"},\n\t)\n\n\t// composite defs\n\tassertParseCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)\",\n\t\t[]assertTypeInfo{\n\t\t\t{Type: TypeVarchar},\n\t\t},\n\t\tnil,\n\t)\n\tassertParseCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.DateType),org.apache.cassandra.db.marshal.UTF8Type)\",\n\t\t[]assertTypeInfo{\n\t\t\t{Type: TypeTimestamp, Reversed: true},\n\t\t\t{Type: TypeVarchar},\n\t\t},\n\t\tnil,\n\t)\n\tassertParseCompositeType(\n\t\tt,\n\t\t\"org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(726f77735f6d6572676564:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.LongType)))\",\n\t\t[]assertTypeInfo{\n\t\t\t{Type: TypeVarchar},\n\t\t},\n\t\tmap[string]assertTypeInfo{\n\t\t\t\"rows_merged\": {\n\t\t\t\tType: TypeMap,\n\t\t\t\tKey:  &assertTypeInfo{Type: TypeInt},\n\t\t\t\tElem: &assertTypeInfo{Type: TypeBigInt},\n\t\t\t},\n\t\t},\n\t)\n}\n\n// expected data holder\ntype assertTypeInfo struct {\n\tType     Type\n\tReversed bool\n\tElem     *assertTypeInfo\n\tKey      *assertTypeInfo\n\tCustom   string\n}\n\n// Helper function for asserting that the type parser returns the expected\n// results for the given definition\nfunc assertParseNonCompositeType(\n\tt *testing.T,\n\tdef string,\n\ttypeExpected assertTypeInfo,\n) {\n\n\tlog := &defaultLogger{}\n\tresult := parseType(def, log)\n\tif len(result.reversed) != 1 {\n\t\tt.Errorf(\"%s expected %d reversed values but there were %d\", def, 1, len(result.reversed))\n\t}\n\n\tassertParseNonCompositeTypes(\n\t\tt,\n\t\tdef,\n\t\t[]assertTypeInfo{typeExpected},\n\t\tresult.types,\n\t)\n\n\t// expect no composite part of the result\n\tif result.isComposite {\n\t\tt.Errorf(\"%s: Expected not composite\", def)\n\t}\n\tif result.collections != nil {\n\t\tt.Errorf(\"%s: Expected nil collections: %v\", def, result.collections)\n\t}\n}\n\n// Helper function for asserting that the type parser returns the expected\n// results for the given definition\nfunc assertParseCompositeType(\n\tt *testing.T,\n\tdef string,\n\ttypesExpected []assertTypeInfo,\n\tcollectionsExpected map[string]assertTypeInfo,\n) {\n\n\tlog := &defaultLogger{}\n\tresult := parseType(def, log)\n\tif len(result.reversed) != len(typesExpected) {\n\t\tt.Errorf(\"%s expected %d reversed values but there were %d\", def, len(typesExpected), len(result.reversed))\n\t}\n\n\tassertParseNonCompositeTypes(\n\t\tt,\n\t\tdef,\n\t\ttypesExpected,\n\t\tresult.types,\n\t)\n\n\t// expect composite part of the result\n\tif !result.isComposite {\n\t\tt.Errorf(\"%s: Expected composite\", def)\n\t}\n\tif result.collections == nil {\n\t\tt.Errorf(\"%s: Expected non-nil collections: %v\", def, result.collections)\n\t}\n\n\tfor name, typeExpected := range collectionsExpected {\n\t\t// check for an actual type for this name\n\t\ttypeActual, found := result.collections[name]\n\t\tif !found {\n\t\t\tt.Errorf(\"%s.tcollections: Expected param named %s but there wasn't\", def, name)\n\t\t} else {\n\t\t\t// remove the actual from the collection so we can detect extras\n\t\t\tdelete(result.collections, name)\n\n\t\t\t// check the type\n\t\t\tassertParseNonCompositeTypes(\n\t\t\t\tt,\n\t\t\t\tdef+\"collections[\"+name+\"]\",\n\t\t\t\t[]assertTypeInfo{typeExpected},\n\t\t\t\t[]TypeInfo{typeActual},\n\t\t\t)\n\t\t}\n\t}\n\n\tif len(result.collections) != 0 {\n\t\tt.Errorf(\"%s.collections: Expected no more types in collections, but there was %v\", def, result.collections)\n\t}\n}\n\n// Helper function for asserting that the type parser returns the expected\n// results for the given definition\nfunc assertParseNonCompositeTypes(\n\tt *testing.T,\n\tcontext string,\n\ttypesExpected []assertTypeInfo,\n\ttypesActual []TypeInfo,\n) {\n\tif len(typesActual) != len(typesExpected) {\n\t\tt.Errorf(\"%s: Expected %d types, but there were %d\", context, len(typesExpected), len(typesActual))\n\t}\n\n\tfor i := range typesExpected {\n\t\ttypeExpected := typesExpected[i]\n\t\ttypeActual := typesActual[i]\n\n\t\t// shadow copy the context for local modification\n\t\tcontext := context\n\t\tif len(typesExpected) > 1 {\n\t\t\tcontext = context + \"[\" + strconv.Itoa(i) + \"]\"\n\t\t}\n\n\t\t// check the type\n\t\tif typeActual.Type() != typeExpected.Type {\n\t\t\tt.Errorf(\"%s: Expected to parse Type to %s but was %s\", context, typeExpected.Type, typeActual.Type())\n\t\t}\n\t\t// check the custom\n\t\tif typeActual.Custom() != typeExpected.Custom {\n\t\t\tt.Errorf(\"%s: Expected to parse Custom %s but was %s\", context, typeExpected.Custom, typeActual.Custom())\n\t\t}\n\n\t\tcollection, _ := typeActual.(CollectionType)\n\t\t// check the elem\n\t\tif typeExpected.Elem != nil {\n\t\t\tif collection.Elem == nil {\n\t\t\t\tt.Errorf(\"%s: Expected to parse Elem, but was nil \", context)\n\t\t\t} else {\n\t\t\t\tassertParseNonCompositeTypes(\n\t\t\t\t\tt,\n\t\t\t\t\tcontext+\".Elem\",\n\t\t\t\t\t[]assertTypeInfo{*typeExpected.Elem},\n\t\t\t\t\t[]TypeInfo{collection.Elem},\n\t\t\t\t)\n\t\t\t}\n\t\t} else if collection.Elem != nil {\n\t\t\tt.Errorf(\"%s: Expected to not parse Elem, but was %+v\", context, collection.Elem)\n\t\t}\n\n\t\t// check the key\n\t\tif typeExpected.Key != nil {\n\t\t\tif collection.Key == nil {\n\t\t\t\tt.Errorf(\"%s: Expected to parse Key, but was nil \", context)\n\t\t\t} else {\n\t\t\t\tassertParseNonCompositeTypes(\n\t\t\t\t\tt,\n\t\t\t\t\tcontext+\".Key\",\n\t\t\t\t\t[]assertTypeInfo{*typeExpected.Key},\n\t\t\t\t\t[]TypeInfo{collection.Key},\n\t\t\t\t)\n\t\t\t}\n\t\t} else if collection.Key != nil {\n\t\t\tt.Errorf(\"%s: Expected to not parse Key, but was %+v\", context, collection.Key)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "policies.go",
          "type": "blob",
          "size": 29.400390625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\n//This file will be the future home for more policies\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/hailocab/go-hostpool\"\n)\n\n// cowHostList implements a copy on write host list, its equivalent type is []*HostInfo\ntype cowHostList struct {\n\tlist atomic.Value\n\tmu   sync.Mutex\n}\n\nfunc (c *cowHostList) String() string {\n\treturn fmt.Sprintf(\"%+v\", c.get())\n}\n\nfunc (c *cowHostList) get() []*HostInfo {\n\t// TODO(zariel): should we replace this with []*HostInfo?\n\tl, ok := c.list.Load().(*[]*HostInfo)\n\tif !ok {\n\t\treturn nil\n\t}\n\treturn *l\n}\n\n// add will add a host if it not already in the list\nfunc (c *cowHostList) add(host *HostInfo) bool {\n\tc.mu.Lock()\n\tl := c.get()\n\n\tif n := len(l); n == 0 {\n\t\tl = []*HostInfo{host}\n\t} else {\n\t\tnewL := make([]*HostInfo, n+1)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tif host.Equal(l[i]) {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tnewL[i] = l[i]\n\t\t}\n\t\tnewL[n] = host\n\t\tl = newL\n\t}\n\n\tc.list.Store(&l)\n\tc.mu.Unlock()\n\treturn true\n}\n\nfunc (c *cowHostList) remove(ip net.IP) bool {\n\tc.mu.Lock()\n\tl := c.get()\n\tsize := len(l)\n\tif size == 0 {\n\t\tc.mu.Unlock()\n\t\treturn false\n\t}\n\n\tfound := false\n\tnewL := make([]*HostInfo, 0, size)\n\tfor i := 0; i < len(l); i++ {\n\t\tif !l[i].ConnectAddress().Equal(ip) {\n\t\t\tnewL = append(newL, l[i])\n\t\t} else {\n\t\t\tfound = true\n\t\t}\n\t}\n\n\tif !found {\n\t\tc.mu.Unlock()\n\t\treturn false\n\t}\n\n\tnewL = newL[: size-1 : size-1]\n\tc.list.Store(&newL)\n\tc.mu.Unlock()\n\n\treturn true\n}\n\n// RetryableQuery is an interface that represents a query or batch statement that\n// exposes the correct functions for the retry policy logic to evaluate correctly.\ntype RetryableQuery interface {\n\tAttempts() int\n\tSetConsistency(c Consistency)\n\tGetConsistency() Consistency\n\tContext() context.Context\n}\n\ntype RetryType uint16\n\nconst (\n\tRetry         RetryType = 0x00 // retry on same connection\n\tRetryNextHost RetryType = 0x01 // retry on another connection\n\tIgnore        RetryType = 0x02 // ignore error and return result\n\tRethrow       RetryType = 0x03 // raise error and stop retrying\n)\n\n// ErrUnknownRetryType is returned if the retry policy returns a retry type\n// unknown to the query executor.\nvar ErrUnknownRetryType = errors.New(\"unknown retry type returned by retry policy\")\n\n// RetryPolicy interface is used by gocql to determine if a query can be attempted\n// again after a retryable error has been received. The interface allows gocql\n// users to implement their own logic to determine if a query can be attempted\n// again.\n//\n// See SimpleRetryPolicy as an example of implementing and using a RetryPolicy\n// interface.\ntype RetryPolicy interface {\n\tAttempt(RetryableQuery) bool\n\tGetRetryType(error) RetryType\n}\n\n// SimpleRetryPolicy has simple logic for attempting a query a fixed number of times.\n//\n// See below for examples of usage:\n//\n//\t//Assign to the cluster\n//\tcluster.RetryPolicy = &gocql.SimpleRetryPolicy{NumRetries: 3}\n//\n//\t//Assign to a query\n//\tquery.RetryPolicy(&gocql.SimpleRetryPolicy{NumRetries: 1})\ntype SimpleRetryPolicy struct {\n\tNumRetries int //Number of times to retry a query\n}\n\n// Attempt tells gocql to attempt the query again based on query.Attempts being less\n// than the NumRetries defined in the policy.\nfunc (s *SimpleRetryPolicy) Attempt(q RetryableQuery) bool {\n\treturn q.Attempts() <= s.NumRetries\n}\n\nfunc (s *SimpleRetryPolicy) GetRetryType(err error) RetryType {\n\treturn RetryNextHost\n}\n\n// ExponentialBackoffRetryPolicy sleeps between attempts\ntype ExponentialBackoffRetryPolicy struct {\n\tNumRetries int\n\tMin, Max   time.Duration\n}\n\nfunc (e *ExponentialBackoffRetryPolicy) Attempt(q RetryableQuery) bool {\n\tif q.Attempts() > e.NumRetries {\n\t\treturn false\n\t}\n\ttime.Sleep(e.napTime(q.Attempts()))\n\treturn true\n}\n\n// used to calculate exponentially growing time\nfunc getExponentialTime(min time.Duration, max time.Duration, attempts int) time.Duration {\n\tif min <= 0 {\n\t\tmin = 100 * time.Millisecond\n\t}\n\tif max <= 0 {\n\t\tmax = 10 * time.Second\n\t}\n\tminFloat := float64(min)\n\tnapDuration := minFloat * math.Pow(2, float64(attempts-1))\n\t// add some jitter\n\tnapDuration += rand.Float64()*minFloat - (minFloat / 2)\n\tif napDuration > float64(max) {\n\t\treturn time.Duration(max)\n\t}\n\treturn time.Duration(napDuration)\n}\n\nfunc (e *ExponentialBackoffRetryPolicy) GetRetryType(err error) RetryType {\n\treturn RetryNextHost\n}\n\n// DowngradingConsistencyRetryPolicy: Next retry will be with the next consistency level\n// provided in the slice\n//\n// On a read timeout: the operation is retried with the next provided consistency\n// level.\n//\n// On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n// and at least one replica acknowledged the write, the operation is\n// retried with the next consistency level.  Furthermore, for other\n// write types, if at least one replica acknowledged the write, the\n// timeout is ignored.\n//\n// On an unavailable exception: if at least one replica is alive, the\n// operation is retried with the next provided consistency level.\n\ntype DowngradingConsistencyRetryPolicy struct {\n\tConsistencyLevelsToTry []Consistency\n}\n\nfunc (d *DowngradingConsistencyRetryPolicy) Attempt(q RetryableQuery) bool {\n\tcurrentAttempt := q.Attempts()\n\n\tif currentAttempt > len(d.ConsistencyLevelsToTry) {\n\t\treturn false\n\t} else if currentAttempt > 0 {\n\t\tq.SetConsistency(d.ConsistencyLevelsToTry[currentAttempt-1])\n\t}\n\treturn true\n}\n\nfunc (d *DowngradingConsistencyRetryPolicy) GetRetryType(err error) RetryType {\n\tswitch t := err.(type) {\n\tcase *RequestErrUnavailable:\n\t\tif t.Alive > 0 {\n\t\t\treturn Retry\n\t\t}\n\t\treturn Rethrow\n\tcase *RequestErrWriteTimeout:\n\t\tif t.WriteType == \"SIMPLE\" || t.WriteType == \"BATCH\" || t.WriteType == \"COUNTER\" {\n\t\t\tif t.Received > 0 {\n\t\t\t\treturn Ignore\n\t\t\t}\n\t\t\treturn Rethrow\n\t\t}\n\t\tif t.WriteType == \"UNLOGGED_BATCH\" {\n\t\t\treturn Retry\n\t\t}\n\t\treturn Rethrow\n\tcase *RequestErrReadTimeout:\n\t\treturn Retry\n\tdefault:\n\t\treturn RetryNextHost\n\t}\n}\n\nfunc (e *ExponentialBackoffRetryPolicy) napTime(attempts int) time.Duration {\n\treturn getExponentialTime(e.Min, e.Max, attempts)\n}\n\ntype HostStateNotifier interface {\n\tAddHost(host *HostInfo)\n\tRemoveHost(host *HostInfo)\n\tHostUp(host *HostInfo)\n\tHostDown(host *HostInfo)\n}\n\ntype KeyspaceUpdateEvent struct {\n\tKeyspace string\n\tChange   string\n}\n\ntype HostTierer interface {\n\t// HostTier returns an integer specifying how far a host is from the client.\n\t// Tier must start at 0.\n\t// The value is used to prioritize closer hosts during host selection.\n\t// For example this could be:\n\t// 0 - local rack, 1 - local DC, 2 - remote DC\n\t// or:\n\t// 0 - local DC, 1 - remote DC\n\tHostTier(host *HostInfo) uint\n\n\t// This function returns the maximum possible host tier\n\tMaxHostTier() uint\n}\n\n// HostSelectionPolicy is an interface for selecting\n// the most appropriate host to execute a given query.\n// HostSelectionPolicy instances cannot be shared between sessions.\ntype HostSelectionPolicy interface {\n\tHostStateNotifier\n\tSetPartitioner\n\tKeyspaceChanged(KeyspaceUpdateEvent)\n\tInit(*Session)\n\tIsLocal(host *HostInfo) bool\n\t// Pick returns an iteration function over selected hosts.\n\t// Multiple attempts of a single query execution won't call the returned NextHost function concurrently,\n\t// so it's safe to have internal state without additional synchronization as long as every call to Pick returns\n\t// a different instance of NextHost.\n\tPick(ExecutableQuery) NextHost\n}\n\n// SelectedHost is an interface returned when picking a host from a host\n// selection policy.\ntype SelectedHost interface {\n\tInfo() *HostInfo\n\tMark(error)\n}\n\ntype selectedHost HostInfo\n\nfunc (host *selectedHost) Info() *HostInfo {\n\treturn (*HostInfo)(host)\n}\n\nfunc (host *selectedHost) Mark(err error) {}\n\n// NextHost is an iteration function over picked hosts\ntype NextHost func() SelectedHost\n\n// RoundRobinHostPolicy is a round-robin load balancing policy, where each host\n// is tried sequentially for each query.\nfunc RoundRobinHostPolicy() HostSelectionPolicy {\n\treturn &roundRobinHostPolicy{}\n}\n\ntype roundRobinHostPolicy struct {\n\thosts           cowHostList\n\tlastUsedHostIdx uint64\n}\n\nfunc (r *roundRobinHostPolicy) IsLocal(*HostInfo) bool              { return true }\nfunc (r *roundRobinHostPolicy) KeyspaceChanged(KeyspaceUpdateEvent) {}\nfunc (r *roundRobinHostPolicy) SetPartitioner(partitioner string)   {}\nfunc (r *roundRobinHostPolicy) Init(*Session)                       {}\n\nfunc (r *roundRobinHostPolicy) Pick(qry ExecutableQuery) NextHost {\n\tnextStartOffset := atomic.AddUint64(&r.lastUsedHostIdx, 1)\n\treturn roundRobbin(int(nextStartOffset), r.hosts.get())\n}\n\nfunc (r *roundRobinHostPolicy) AddHost(host *HostInfo) {\n\tr.hosts.add(host)\n}\n\nfunc (r *roundRobinHostPolicy) RemoveHost(host *HostInfo) {\n\tr.hosts.remove(host.ConnectAddress())\n}\n\nfunc (r *roundRobinHostPolicy) HostUp(host *HostInfo) {\n\tr.AddHost(host)\n}\n\nfunc (r *roundRobinHostPolicy) HostDown(host *HostInfo) {\n\tr.RemoveHost(host)\n}\n\nfunc ShuffleReplicas() func(*tokenAwareHostPolicy) {\n\treturn func(t *tokenAwareHostPolicy) {\n\t\tt.shuffleReplicas = true\n\t}\n}\n\n// NonLocalReplicasFallback enables fallback to replicas that are not considered local.\n//\n// TokenAwareHostPolicy used with DCAwareHostPolicy fallback first selects replicas by partition key in local DC, then\n// falls back to other nodes in the local DC. Enabling NonLocalReplicasFallback causes TokenAwareHostPolicy\n// to first select replicas by partition key in local DC, then replicas by partition key in remote DCs and fall back\n// to other nodes in local DC.\nfunc NonLocalReplicasFallback() func(policy *tokenAwareHostPolicy) {\n\treturn func(t *tokenAwareHostPolicy) {\n\t\tt.nonLocalReplicasFallback = true\n\t}\n}\n\n// TokenAwareHostPolicy is a token aware host selection policy, where hosts are\n// selected based on the partition key, so queries are sent to the host which\n// owns the partition. Fallback is used when routing information is not available.\nfunc TokenAwareHostPolicy(fallback HostSelectionPolicy, opts ...func(*tokenAwareHostPolicy)) HostSelectionPolicy {\n\tp := &tokenAwareHostPolicy{fallback: fallback}\n\tfor _, opt := range opts {\n\t\topt(p)\n\t}\n\treturn p\n}\n\n// clusterMeta holds metadata about cluster topology.\n// It is used inside atomic.Value and shallow copies are used when replacing it,\n// so fields should not be modified in-place. Instead, to modify a field a copy of the field should be made\n// and the pointer in clusterMeta updated to point to the new value.\ntype clusterMeta struct {\n\t// replicas is map[keyspace]map[token]hosts\n\treplicas  map[string]tokenRingReplicas\n\ttokenRing *tokenRing\n}\n\ntype tokenAwareHostPolicy struct {\n\tfallback            HostSelectionPolicy\n\tgetKeyspaceMetadata func(keyspace string) (*KeyspaceMetadata, error)\n\tgetKeyspaceName     func() string\n\n\tshuffleReplicas          bool\n\tnonLocalReplicasFallback bool\n\n\t// mu protects writes to hosts, partitioner, metadata.\n\t// reads can be unlocked as long as they are not used for updating state later.\n\tmu          sync.Mutex\n\thosts       cowHostList\n\tpartitioner string\n\tmetadata    atomic.Value // *clusterMeta\n\n\tlogger StdLogger\n}\n\nfunc (t *tokenAwareHostPolicy) Init(s *Session) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.getKeyspaceMetadata != nil {\n\t\t// Init was already called.\n\t\t// See https://github.com/scylladb/gocql/issues/94.\n\t\tpanic(\"sharing token aware host selection policy between sessions is not supported\")\n\t}\n\tt.getKeyspaceMetadata = s.KeyspaceMetadata\n\tt.getKeyspaceName = func() string { return s.cfg.Keyspace }\n\tt.logger = s.logger\n}\n\nfunc (t *tokenAwareHostPolicy) IsLocal(host *HostInfo) bool {\n\treturn t.fallback.IsLocal(host)\n}\n\nfunc (t *tokenAwareHostPolicy) KeyspaceChanged(update KeyspaceUpdateEvent) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tmeta := t.getMetadataForUpdate()\n\tt.updateReplicas(meta, update.Keyspace)\n\tt.metadata.Store(meta)\n}\n\n// updateReplicas updates replicas in clusterMeta.\n// It must be called with t.mu mutex locked.\n// meta must not be nil and it's replicas field will be updated.\nfunc (t *tokenAwareHostPolicy) updateReplicas(meta *clusterMeta, keyspace string) {\n\tnewReplicas := make(map[string]tokenRingReplicas, len(meta.replicas))\n\n\tks, err := t.getKeyspaceMetadata(keyspace)\n\tif err == nil {\n\t\tstrat := getStrategy(ks, t.logger)\n\t\tif strat != nil {\n\t\t\tif meta != nil && meta.tokenRing != nil {\n\t\t\t\tnewReplicas[keyspace] = strat.replicaMap(meta.tokenRing)\n\t\t\t}\n\t\t}\n\t}\n\n\tfor ks, replicas := range meta.replicas {\n\t\tif ks != keyspace {\n\t\t\tnewReplicas[ks] = replicas\n\t\t}\n\t}\n\n\tmeta.replicas = newReplicas\n}\n\nfunc (t *tokenAwareHostPolicy) SetPartitioner(partitioner string) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\tif t.partitioner != partitioner {\n\t\tt.fallback.SetPartitioner(partitioner)\n\t\tt.partitioner = partitioner\n\t\tmeta := t.getMetadataForUpdate()\n\t\tmeta.resetTokenRing(t.partitioner, t.hosts.get(), t.logger)\n\t\tt.updateReplicas(meta, t.getKeyspaceName())\n\t\tt.metadata.Store(meta)\n\t}\n}\n\nfunc (t *tokenAwareHostPolicy) AddHost(host *HostInfo) {\n\tt.mu.Lock()\n\tif t.hosts.add(host) {\n\t\tmeta := t.getMetadataForUpdate()\n\t\tmeta.resetTokenRing(t.partitioner, t.hosts.get(), t.logger)\n\t\tt.updateReplicas(meta, t.getKeyspaceName())\n\t\tt.metadata.Store(meta)\n\t}\n\tt.mu.Unlock()\n\n\tt.fallback.AddHost(host)\n}\n\nfunc (t *tokenAwareHostPolicy) AddHosts(hosts []*HostInfo) {\n\tt.mu.Lock()\n\n\tfor _, host := range hosts {\n\t\tt.hosts.add(host)\n\t}\n\n\tmeta := t.getMetadataForUpdate()\n\tmeta.resetTokenRing(t.partitioner, t.hosts.get(), t.logger)\n\tt.updateReplicas(meta, t.getKeyspaceName())\n\tt.metadata.Store(meta)\n\n\tt.mu.Unlock()\n\n\tfor _, host := range hosts {\n\t\tt.fallback.AddHost(host)\n\t}\n}\n\nfunc (t *tokenAwareHostPolicy) RemoveHost(host *HostInfo) {\n\tt.mu.Lock()\n\tif t.hosts.remove(host.ConnectAddress()) {\n\t\tmeta := t.getMetadataForUpdate()\n\t\tmeta.resetTokenRing(t.partitioner, t.hosts.get(), t.logger)\n\t\tt.updateReplicas(meta, t.getKeyspaceName())\n\t\tt.metadata.Store(meta)\n\t}\n\tt.mu.Unlock()\n\n\tt.fallback.RemoveHost(host)\n}\n\nfunc (t *tokenAwareHostPolicy) HostUp(host *HostInfo) {\n\tt.fallback.HostUp(host)\n}\n\nfunc (t *tokenAwareHostPolicy) HostDown(host *HostInfo) {\n\tt.fallback.HostDown(host)\n}\n\n// getMetadataReadOnly returns current cluster metadata.\n// Metadata uses copy on write, so the returned value should be only used for reading.\n// To obtain a copy that could be updated, use getMetadataForUpdate instead.\nfunc (t *tokenAwareHostPolicy) getMetadataReadOnly() *clusterMeta {\n\tmeta, _ := t.metadata.Load().(*clusterMeta)\n\treturn meta\n}\n\n// getMetadataForUpdate returns clusterMeta suitable for updating.\n// It is a SHALLOW copy of current metadata in case it was already set or new empty clusterMeta otherwise.\n// This function should be called with t.mu mutex locked and the mutex should not be released before\n// storing the new metadata.\nfunc (t *tokenAwareHostPolicy) getMetadataForUpdate() *clusterMeta {\n\tmetaReadOnly := t.getMetadataReadOnly()\n\tmeta := new(clusterMeta)\n\tif metaReadOnly != nil {\n\t\t*meta = *metaReadOnly\n\t}\n\treturn meta\n}\n\n// resetTokenRing creates a new tokenRing.\n// It must be called with t.mu locked.\nfunc (m *clusterMeta) resetTokenRing(partitioner string, hosts []*HostInfo, logger StdLogger) {\n\tif partitioner == \"\" {\n\t\t// partitioner not yet set\n\t\treturn\n\t}\n\n\t// create a new token ring\n\ttokenRing, err := newTokenRing(partitioner, hosts)\n\tif err != nil {\n\t\tlogger.Printf(\"Unable to update the token ring due to error: %s\", err)\n\t\treturn\n\t}\n\n\t// replace the token ring\n\tm.tokenRing = tokenRing\n}\n\nfunc (t *tokenAwareHostPolicy) Pick(qry ExecutableQuery) NextHost {\n\tif qry == nil {\n\t\treturn t.fallback.Pick(qry)\n\t}\n\n\troutingKey, err := qry.GetRoutingKey()\n\tif err != nil {\n\t\treturn t.fallback.Pick(qry)\n\t} else if routingKey == nil {\n\t\treturn t.fallback.Pick(qry)\n\t}\n\n\tmeta := t.getMetadataReadOnly()\n\tif meta == nil || meta.tokenRing == nil {\n\t\treturn t.fallback.Pick(qry)\n\t}\n\n\ttoken := meta.tokenRing.partitioner.Hash(routingKey)\n\tht := meta.replicas[qry.Keyspace()].replicasFor(token)\n\n\tvar replicas []*HostInfo\n\tif ht == nil {\n\t\thost, _ := meta.tokenRing.GetHostForToken(token)\n\t\treplicas = []*HostInfo{host}\n\t} else {\n\t\treplicas = ht.hosts\n\t\tif t.shuffleReplicas {\n\t\t\treplicas = shuffleHosts(replicas)\n\t\t}\n\t}\n\n\tvar (\n\t\tfallbackIter NextHost\n\t\ti, j, k      int\n\t\tremote       [][]*HostInfo\n\t\ttierer       HostTierer\n\t\ttiererOk     bool\n\t\tmaxTier      uint\n\t)\n\n\tif tierer, tiererOk = t.fallback.(HostTierer); tiererOk {\n\t\tmaxTier = tierer.MaxHostTier()\n\t} else {\n\t\tmaxTier = 1\n\t}\n\n\tif t.nonLocalReplicasFallback {\n\t\tremote = make([][]*HostInfo, maxTier)\n\t}\n\n\tused := make(map[*HostInfo]bool, len(replicas))\n\treturn func() SelectedHost {\n\t\tfor i < len(replicas) {\n\t\t\th := replicas[i]\n\t\t\ti++\n\n\t\t\tvar tier uint\n\t\t\tif tiererOk {\n\t\t\t\ttier = tierer.HostTier(h)\n\t\t\t} else if t.fallback.IsLocal(h) {\n\t\t\t\ttier = 0\n\t\t\t} else {\n\t\t\t\ttier = 1\n\t\t\t}\n\n\t\t\tif tier != 0 {\n\t\t\t\tif t.nonLocalReplicasFallback {\n\t\t\t\t\tremote[tier-1] = append(remote[tier-1], h)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif h.IsUp() {\n\t\t\t\tused[h] = true\n\t\t\t\treturn (*selectedHost)(h)\n\t\t\t}\n\t\t}\n\n\t\tif t.nonLocalReplicasFallback {\n\t\t\tfor j < len(remote) && k < len(remote[j]) {\n\t\t\t\th := remote[j][k]\n\t\t\t\tk++\n\n\t\t\t\tif k >= len(remote[j]) {\n\t\t\t\t\tj++\n\t\t\t\t\tk = 0\n\t\t\t\t}\n\n\t\t\t\tif h.IsUp() {\n\t\t\t\t\tused[h] = true\n\t\t\t\t\treturn (*selectedHost)(h)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif fallbackIter == nil {\n\t\t\t// fallback\n\t\t\tfallbackIter = t.fallback.Pick(qry)\n\t\t}\n\n\t\t// filter the token aware selected hosts from the fallback hosts\n\t\tfor fallbackHost := fallbackIter(); fallbackHost != nil; fallbackHost = fallbackIter() {\n\t\t\tif !used[fallbackHost.Info()] {\n\t\t\t\tused[fallbackHost.Info()] = true\n\t\t\t\treturn fallbackHost\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n}\n\n// HostPoolHostPolicy is a host policy which uses the bitly/go-hostpool library\n// to distribute queries between hosts and prevent sending queries to\n// unresponsive hosts. When creating the host pool that is passed to the policy\n// use an empty slice of hosts as the hostpool will be populated later by gocql.\n// See below for examples of usage:\n//\n//\t// Create host selection policy using a simple host pool\n//\tcluster.PoolConfig.HostSelectionPolicy = HostPoolHostPolicy(hostpool.New(nil))\n//\n//\t// Create host selection policy using an epsilon greedy pool\n//\tcluster.PoolConfig.HostSelectionPolicy = HostPoolHostPolicy(\n//\t    hostpool.NewEpsilonGreedy(nil, 0, &hostpool.LinearEpsilonValueCalculator{}),\n//\t)\nfunc HostPoolHostPolicy(hp hostpool.HostPool) HostSelectionPolicy {\n\treturn &hostPoolHostPolicy{hostMap: map[string]*HostInfo{}, hp: hp}\n}\n\ntype hostPoolHostPolicy struct {\n\thp      hostpool.HostPool\n\tmu      sync.RWMutex\n\thostMap map[string]*HostInfo\n}\n\nfunc (r *hostPoolHostPolicy) Init(*Session)                       {}\nfunc (r *hostPoolHostPolicy) KeyspaceChanged(KeyspaceUpdateEvent) {}\nfunc (r *hostPoolHostPolicy) SetPartitioner(string)               {}\nfunc (r *hostPoolHostPolicy) IsLocal(*HostInfo) bool              { return true }\n\nfunc (r *hostPoolHostPolicy) SetHosts(hosts []*HostInfo) {\n\tpeers := make([]string, len(hosts))\n\thostMap := make(map[string]*HostInfo, len(hosts))\n\n\tfor i, host := range hosts {\n\t\tip := host.ConnectAddress().String()\n\t\tpeers[i] = ip\n\t\thostMap[ip] = host\n\t}\n\n\tr.mu.Lock()\n\tr.hp.SetHosts(peers)\n\tr.hostMap = hostMap\n\tr.mu.Unlock()\n}\n\nfunc (r *hostPoolHostPolicy) AddHost(host *HostInfo) {\n\tip := host.ConnectAddress().String()\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\t// If the host addr is present and isn't nil return\n\tif h, ok := r.hostMap[ip]; ok && h != nil {\n\t\treturn\n\t}\n\t// otherwise, add the host to the map\n\tr.hostMap[ip] = host\n\t// and construct a new peer list to give to the HostPool\n\thosts := make([]string, 0, len(r.hostMap))\n\tfor addr := range r.hostMap {\n\t\thosts = append(hosts, addr)\n\t}\n\n\tr.hp.SetHosts(hosts)\n}\n\nfunc (r *hostPoolHostPolicy) RemoveHost(host *HostInfo) {\n\tip := host.ConnectAddress().String()\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tif _, ok := r.hostMap[ip]; !ok {\n\t\treturn\n\t}\n\n\tdelete(r.hostMap, ip)\n\thosts := make([]string, 0, len(r.hostMap))\n\tfor _, host := range r.hostMap {\n\t\thosts = append(hosts, host.ConnectAddress().String())\n\t}\n\n\tr.hp.SetHosts(hosts)\n}\n\nfunc (r *hostPoolHostPolicy) HostUp(host *HostInfo) {\n\tr.AddHost(host)\n}\n\nfunc (r *hostPoolHostPolicy) HostDown(host *HostInfo) {\n\tr.RemoveHost(host)\n}\n\nfunc (r *hostPoolHostPolicy) Pick(qry ExecutableQuery) NextHost {\n\treturn func() SelectedHost {\n\t\tr.mu.RLock()\n\t\tdefer r.mu.RUnlock()\n\n\t\tif len(r.hostMap) == 0 {\n\t\t\treturn nil\n\t\t}\n\n\t\thostR := r.hp.Get()\n\t\thost, ok := r.hostMap[hostR.Host()]\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\n\t\treturn selectedHostPoolHost{\n\t\t\tpolicy: r,\n\t\t\tinfo:   host,\n\t\t\thostR:  hostR,\n\t\t}\n\t}\n}\n\n// selectedHostPoolHost is a host returned by the hostPoolHostPolicy and\n// implements the SelectedHost interface\ntype selectedHostPoolHost struct {\n\tpolicy *hostPoolHostPolicy\n\tinfo   *HostInfo\n\thostR  hostpool.HostPoolResponse\n}\n\nfunc (host selectedHostPoolHost) Info() *HostInfo {\n\treturn host.info\n}\n\nfunc (host selectedHostPoolHost) Mark(err error) {\n\tip := host.info.ConnectAddress().String()\n\n\thost.policy.mu.RLock()\n\tdefer host.policy.mu.RUnlock()\n\n\tif _, ok := host.policy.hostMap[ip]; !ok {\n\t\t// host was removed between pick and mark\n\t\treturn\n\t}\n\n\thost.hostR.Mark(err)\n}\n\ntype dcAwareRR struct {\n\tlocal           string\n\tlocalHosts      cowHostList\n\tremoteHosts     cowHostList\n\tlastUsedHostIdx uint64\n}\n\n// DCAwareRoundRobinPolicy is a host selection policies which will prioritize and\n// return hosts which are in the local datacentre before returning hosts in all\n// other datercentres\nfunc DCAwareRoundRobinPolicy(localDC string) HostSelectionPolicy {\n\treturn &dcAwareRR{local: localDC}\n}\n\nfunc (d *dcAwareRR) Init(*Session)                       {}\nfunc (d *dcAwareRR) KeyspaceChanged(KeyspaceUpdateEvent) {}\nfunc (d *dcAwareRR) SetPartitioner(p string)             {}\n\nfunc (d *dcAwareRR) IsLocal(host *HostInfo) bool {\n\treturn host.DataCenter() == d.local\n}\n\nfunc (d *dcAwareRR) AddHost(host *HostInfo) {\n\tif d.IsLocal(host) {\n\t\td.localHosts.add(host)\n\t} else {\n\t\td.remoteHosts.add(host)\n\t}\n}\n\nfunc (d *dcAwareRR) RemoveHost(host *HostInfo) {\n\tif d.IsLocal(host) {\n\t\td.localHosts.remove(host.ConnectAddress())\n\t} else {\n\t\td.remoteHosts.remove(host.ConnectAddress())\n\t}\n}\n\nfunc (d *dcAwareRR) HostUp(host *HostInfo)   { d.AddHost(host) }\nfunc (d *dcAwareRR) HostDown(host *HostInfo) { d.RemoveHost(host) }\n\n// This function is supposed to be called in a fashion\n// roundRobbin(offset, hostsPriority1, hostsPriority2, hostsPriority3 ... )\n//\n// E.g. for DC-naive strategy:\n// roundRobbin(offset, allHosts)\n//\n// For tiered and DC-aware strategy:\n// roundRobbin(offset, localHosts, remoteHosts)\nfunc roundRobbin(shift int, hosts ...[]*HostInfo) NextHost {\n\tcurrentLayer := 0\n\tcurrentlyObserved := 0\n\n\treturn func() SelectedHost {\n\n\t\t// iterate over layers\n\t\tfor {\n\t\t\tif currentLayer == len(hosts) {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tcurrentLayerSize := len(hosts[currentLayer])\n\n\t\t\t// iterate over hosts within a layer\n\t\t\tfor {\n\t\t\t\tcurrentlyObserved++\n\t\t\t\tif currentlyObserved > currentLayerSize {\n\t\t\t\t\tcurrentLayer++\n\t\t\t\t\tcurrentlyObserved = 0\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\th := hosts[currentLayer][(shift+currentlyObserved)%currentLayerSize]\n\n\t\t\t\tif h.IsUp() {\n\t\t\t\t\treturn (*selectedHost)(h)\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (d *dcAwareRR) Pick(q ExecutableQuery) NextHost {\n\tnextStartOffset := atomic.AddUint64(&d.lastUsedHostIdx, 1)\n\treturn roundRobbin(int(nextStartOffset), d.localHosts.get(), d.remoteHosts.get())\n}\n\n// RackAwareRoundRobinPolicy is a host selection policies which will prioritize and\n// return hosts which are in the local rack, before hosts in the local datacenter but\n// a different rack, before hosts in all other datercentres\n\ntype rackAwareRR struct {\n\t// lastUsedHostIdx keeps the index of the last used host.\n\t// It is accessed atomically and needs to be aligned to 64 bits, so we\n\t// keep it first in the struct. Do not move it or add new struct members\n\t// before it.\n\tlastUsedHostIdx uint64\n\tlocalDC         string\n\tlocalRack       string\n\thosts           []cowHostList\n}\n\nfunc RackAwareRoundRobinPolicy(localDC string, localRack string) HostSelectionPolicy {\n\thosts := make([]cowHostList, 3)\n\treturn &rackAwareRR{localDC: localDC, localRack: localRack, hosts: hosts}\n}\n\nfunc (d *rackAwareRR) Init(*Session)                       {}\nfunc (d *rackAwareRR) KeyspaceChanged(KeyspaceUpdateEvent) {}\nfunc (d *rackAwareRR) SetPartitioner(p string)             {}\n\nfunc (d *rackAwareRR) MaxHostTier() uint {\n\treturn 2\n}\n\nfunc (d *rackAwareRR) HostTier(host *HostInfo) uint {\n\tif host.DataCenter() == d.localDC {\n\t\tif host.Rack() == d.localRack {\n\t\t\treturn 0\n\t\t} else {\n\t\t\treturn 1\n\t\t}\n\t} else {\n\t\treturn 2\n\t}\n}\n\nfunc (d *rackAwareRR) IsLocal(host *HostInfo) bool {\n\treturn d.HostTier(host) == 0\n}\n\nfunc (d *rackAwareRR) AddHost(host *HostInfo) {\n\tdist := d.HostTier(host)\n\td.hosts[dist].add(host)\n}\n\nfunc (d *rackAwareRR) RemoveHost(host *HostInfo) {\n\tdist := d.HostTier(host)\n\td.hosts[dist].remove(host.ConnectAddress())\n}\n\nfunc (d *rackAwareRR) HostUp(host *HostInfo)   { d.AddHost(host) }\nfunc (d *rackAwareRR) HostDown(host *HostInfo) { d.RemoveHost(host) }\n\nfunc (d *rackAwareRR) Pick(q ExecutableQuery) NextHost {\n\tnextStartOffset := atomic.AddUint64(&d.lastUsedHostIdx, 1)\n\treturn roundRobbin(int(nextStartOffset), d.hosts[0].get(), d.hosts[1].get(), d.hosts[2].get())\n}\n\n// ReadyPolicy defines a policy for when a HostSelectionPolicy can be used. After\n// each host connects during session initialization, the Ready method will be\n// called. If you only need a single Host to be up you can wrap a\n// HostSelectionPolicy policy with SingleHostReadyPolicy.\ntype ReadyPolicy interface {\n\tReady() bool\n}\n\n// SingleHostReadyPolicy wraps a HostSelectionPolicy and returns Ready after a\n// single host has been added via HostUp\nfunc SingleHostReadyPolicy(p HostSelectionPolicy) *singleHostReadyPolicy {\n\treturn &singleHostReadyPolicy{\n\t\tHostSelectionPolicy: p,\n\t}\n}\n\ntype singleHostReadyPolicy struct {\n\tHostSelectionPolicy\n\tready    bool\n\treadyMux sync.Mutex\n}\n\nfunc (s *singleHostReadyPolicy) HostUp(host *HostInfo) {\n\ts.HostSelectionPolicy.HostUp(host)\n\n\ts.readyMux.Lock()\n\ts.ready = true\n\ts.readyMux.Unlock()\n}\n\nfunc (s *singleHostReadyPolicy) Ready() bool {\n\ts.readyMux.Lock()\n\tready := s.ready\n\ts.readyMux.Unlock()\n\tif !ready {\n\t\treturn false\n\t}\n\n\t// in case the wrapped policy is also a ReadyPolicy, defer to that\n\tif rdy, ok := s.HostSelectionPolicy.(ReadyPolicy); ok {\n\t\treturn rdy.Ready()\n\t}\n\treturn true\n}\n\n// ConvictionPolicy interface is used by gocql to determine if a host should be\n// marked as DOWN based on the error and host info\ntype ConvictionPolicy interface {\n\t// Implementations should return `true` if the host should be convicted, `false` otherwise.\n\tAddFailure(error error, host *HostInfo) bool\n\t//Implementations should clear out any convictions or state regarding the host.\n\tReset(host *HostInfo)\n}\n\n// SimpleConvictionPolicy implements a ConvictionPolicy which convicts all hosts\n// regardless of error\ntype SimpleConvictionPolicy struct {\n}\n\nfunc (e *SimpleConvictionPolicy) AddFailure(error error, host *HostInfo) bool {\n\treturn true\n}\n\nfunc (e *SimpleConvictionPolicy) Reset(host *HostInfo) {}\n\n// ReconnectionPolicy interface is used by gocql to determine if reconnection\n// can be attempted after connection error. The interface allows gocql users\n// to implement their own logic to determine how to attempt reconnection.\ntype ReconnectionPolicy interface {\n\tGetInterval(currentRetry int) time.Duration\n\tGetMaxRetries() int\n}\n\n// ConstantReconnectionPolicy has simple logic for returning a fixed reconnection interval.\n//\n// Examples of usage:\n//\n//\tcluster.ReconnectionPolicy = &gocql.ConstantReconnectionPolicy{MaxRetries: 10, Interval: 8 * time.Second}\ntype ConstantReconnectionPolicy struct {\n\tMaxRetries int\n\tInterval   time.Duration\n}\n\nfunc (c *ConstantReconnectionPolicy) GetInterval(currentRetry int) time.Duration {\n\treturn c.Interval\n}\n\nfunc (c *ConstantReconnectionPolicy) GetMaxRetries() int {\n\treturn c.MaxRetries\n}\n\n// ExponentialReconnectionPolicy returns a growing reconnection interval.\ntype ExponentialReconnectionPolicy struct {\n\tMaxRetries      int\n\tInitialInterval time.Duration\n\tMaxInterval     time.Duration\n}\n\nfunc (e *ExponentialReconnectionPolicy) GetInterval(currentRetry int) time.Duration {\n\tmax := e.MaxInterval\n\tif max < e.InitialInterval {\n\t\tmax = math.MaxInt16 * time.Second\n\t}\n\treturn getExponentialTime(e.InitialInterval, max, currentRetry)\n}\n\nfunc (e *ExponentialReconnectionPolicy) GetMaxRetries() int {\n\treturn e.MaxRetries\n}\n\ntype SpeculativeExecutionPolicy interface {\n\tAttempts() int\n\tDelay() time.Duration\n}\n\ntype NonSpeculativeExecution struct{}\n\nfunc (sp NonSpeculativeExecution) Attempts() int        { return 0 } // No additional attempts\nfunc (sp NonSpeculativeExecution) Delay() time.Duration { return 1 } // The delay. Must be positive to be used in a ticker.\n\ntype SimpleSpeculativeExecution struct {\n\tNumAttempts  int\n\tTimeoutDelay time.Duration\n}\n\nfunc (sp *SimpleSpeculativeExecution) Attempts() int        { return sp.NumAttempts }\nfunc (sp *SimpleSpeculativeExecution) Delay() time.Duration { return sp.TimeoutDelay }\n"
        },
        {
          "name": "policies_test.go",
          "type": "blob",
          "size": 30.0869140625,
          "content": "// Copyright (c) 2015 The gocql Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hailocab/go-hostpool\"\n)\n\n// Tests of the round-robin host selection policy implementation\nfunc TestRoundRobbin(t *testing.T) {\n\tpolicy := RoundRobinHostPolicy()\n\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(0, 0, 0, 1)},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(0, 0, 0, 2)},\n\t}\n\n\tfor _, host := range hosts {\n\t\tpolicy.AddHost(host)\n\t}\n\n\tgot := make(map[string]bool)\n\tit := policy.Pick(nil)\n\tfor h := it(); h != nil; h = it() {\n\t\tid := h.Info().hostId\n\t\tif got[id] {\n\t\t\tt.Fatalf(\"got duplicate host: %v\", id)\n\t\t}\n\t\tgot[id] = true\n\t}\n\tif len(got) != len(hosts) {\n\t\tt.Fatalf(\"expected %d hosts got %d\", len(hosts), len(got))\n\t}\n}\n\n// Tests of the token-aware host selection policy implementation with a\n// round-robin host selection policy fallback.\nfunc TestHostPolicy_TokenAware_SimpleStrategy(t *testing.T) {\n\tconst keyspace = \"myKeyspace\"\n\tpolicy := TokenAwareHostPolicy(RoundRobinHostPolicy())\n\tpolicyInternal := policy.(*tokenAwareHostPolicy)\n\tpolicyInternal.getKeyspaceName = func() string { return keyspace }\n\tpolicyInternal.getKeyspaceMetadata = func(ks string) (*KeyspaceMetadata, error) {\n\t\treturn nil, errors.New(\"not initalized\")\n\t}\n\n\tquery := &Query{routingInfo: &queryRoutingInfo{}}\n\tquery.getKeyspace = func() string { return keyspace }\n\n\titer := policy.Pick(nil)\n\tif iter == nil {\n\t\tt.Fatal(\"host iterator was nil\")\n\t}\n\tactual := iter()\n\tif actual != nil {\n\t\tt.Fatalf(\"expected nil from iterator, but was %v\", actual)\n\t}\n\n\t// set the hosts\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(10, 0, 0, 1), tokens: []string{\"00\"}},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(10, 0, 0, 2), tokens: []string{\"25\"}},\n\t\t{hostId: \"2\", connectAddress: net.IPv4(10, 0, 0, 3), tokens: []string{\"50\"}},\n\t\t{hostId: \"3\", connectAddress: net.IPv4(10, 0, 0, 4), tokens: []string{\"75\"}},\n\t}\n\tfor _, host := range &hosts {\n\t\tpolicy.AddHost(host)\n\t}\n\n\tpolicy.SetPartitioner(\"OrderedPartitioner\")\n\n\tpolicyInternal.getKeyspaceMetadata = func(keyspaceName string) (*KeyspaceMetadata, error) {\n\t\tif keyspaceName != keyspace {\n\t\t\treturn nil, fmt.Errorf(\"unknown keyspace: %s\", keyspaceName)\n\t\t}\n\t\treturn &KeyspaceMetadata{\n\t\t\tName:          keyspace,\n\t\t\tStrategyClass: \"SimpleStrategy\",\n\t\t\tStrategyOptions: map[string]interface{}{\n\t\t\t\t\"class\":              \"SimpleStrategy\",\n\t\t\t\t\"replication_factor\": 2,\n\t\t\t},\n\t\t}, nil\n\t}\n\tpolicy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: keyspace})\n\n\t// The SimpleStrategy above should generate the following replicas.\n\t// It's handy to have as reference here.\n\tassertDeepEqual(t, \"replicas\", map[string]tokenRingReplicas{\n\t\t\"myKeyspace\": {\n\t\t\t{orderedToken(\"00\"), []*HostInfo{hosts[0], hosts[1]}},\n\t\t\t{orderedToken(\"25\"), []*HostInfo{hosts[1], hosts[2]}},\n\t\t\t{orderedToken(\"50\"), []*HostInfo{hosts[2], hosts[3]}},\n\t\t\t{orderedToken(\"75\"), []*HostInfo{hosts[3], hosts[0]}},\n\t\t},\n\t}, policyInternal.getMetadataReadOnly().replicas)\n\n\t// now the token ring is configured\n\tquery.RoutingKey([]byte(\"20\"))\n\titer = policy.Pick(query)\n\t// first token-aware hosts\n\texpectHosts(t, \"hosts[0]\", iter, \"1\")\n\texpectHosts(t, \"hosts[1]\", iter, \"2\")\n\t// then rest of the hosts\n\texpectHosts(t, \"rest\", iter, \"0\", \"3\")\n\texpectNoMoreHosts(t, iter)\n}\n\n// Tests of the host pool host selection policy implementation\nfunc TestHostPolicy_HostPool(t *testing.T) {\n\tpolicy := HostPoolHostPolicy(hostpool.New(nil))\n\n\thosts := []*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(10, 0, 0, 0)},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(10, 0, 0, 1)},\n\t}\n\n\t// Using set host to control the ordering of the hosts as calling \"AddHost\" iterates the map\n\t// which will result in an unpredictable ordering\n\tpolicy.(*hostPoolHostPolicy).SetHosts(hosts)\n\n\t// the first host selected is actually at [1], but this is ok for RR\n\t// interleaved iteration should always increment the host\n\titer := policy.Pick(nil)\n\tactualA := iter()\n\tif actualA.Info().HostID() != \"0\" {\n\t\tt.Errorf(\"Expected hosts[0] but was hosts[%s]\", actualA.Info().HostID())\n\t}\n\tactualA.Mark(nil)\n\n\tactualB := iter()\n\tif actualB.Info().HostID() != \"1\" {\n\t\tt.Errorf(\"Expected hosts[1] but was hosts[%s]\", actualB.Info().HostID())\n\t}\n\tactualB.Mark(fmt.Errorf(\"error\"))\n\n\tactualC := iter()\n\tif actualC.Info().HostID() != \"0\" {\n\t\tt.Errorf(\"Expected hosts[0] but was hosts[%s]\", actualC.Info().HostID())\n\t}\n\tactualC.Mark(nil)\n\n\tactualD := iter()\n\tif actualD.Info().HostID() != \"0\" {\n\t\tt.Errorf(\"Expected hosts[0] but was hosts[%s]\", actualD.Info().HostID())\n\t}\n\tactualD.Mark(nil)\n}\n\nfunc TestHostPolicy_RoundRobin_NilHostInfo(t *testing.T) {\n\tpolicy := RoundRobinHostPolicy()\n\n\thost := &HostInfo{hostId: \"host-1\"}\n\tpolicy.AddHost(host)\n\n\titer := policy.Pick(nil)\n\tnext := iter()\n\tif next == nil {\n\t\tt.Fatal(\"got nil host\")\n\t} else if v := next.Info(); v == nil {\n\t\tt.Fatal(\"got nil HostInfo\")\n\t} else if v.HostID() != host.HostID() {\n\t\tt.Fatalf(\"expected host %v got %v\", host, v)\n\t}\n\n\tnext = iter()\n\tif next != nil {\n\t\tt.Errorf(\"expected to get nil host got %+v\", next)\n\t\tif next.Info() == nil {\n\t\t\tt.Fatalf(\"HostInfo is nil\")\n\t\t}\n\t}\n}\n\nfunc TestHostPolicy_TokenAware_NilHostInfo(t *testing.T) {\n\tpolicy := TokenAwareHostPolicy(RoundRobinHostPolicy())\n\tpolicyInternal := policy.(*tokenAwareHostPolicy)\n\tpolicyInternal.getKeyspaceName = func() string { return \"myKeyspace\" }\n\tpolicyInternal.getKeyspaceMetadata = func(ks string) (*KeyspaceMetadata, error) {\n\t\treturn nil, errors.New(\"not initialized\")\n\t}\n\n\thosts := [...]*HostInfo{\n\t\t{connectAddress: net.IPv4(10, 0, 0, 0), tokens: []string{\"00\"}},\n\t\t{connectAddress: net.IPv4(10, 0, 0, 1), tokens: []string{\"25\"}},\n\t\t{connectAddress: net.IPv4(10, 0, 0, 2), tokens: []string{\"50\"}},\n\t\t{connectAddress: net.IPv4(10, 0, 0, 3), tokens: []string{\"75\"}},\n\t}\n\tfor _, host := range hosts {\n\t\tpolicy.AddHost(host)\n\t}\n\tpolicy.SetPartitioner(\"OrderedPartitioner\")\n\n\tquery := &Query{routingInfo: &queryRoutingInfo{}}\n\tquery.getKeyspace = func() string { return \"myKeyspace\" }\n\tquery.RoutingKey([]byte(\"20\"))\n\n\titer := policy.Pick(query)\n\tnext := iter()\n\tif next == nil {\n\t\tt.Fatal(\"got nil host\")\n\t} else if v := next.Info(); v == nil {\n\t\tt.Fatal(\"got nil HostInfo\")\n\t} else if !v.ConnectAddress().Equal(hosts[1].ConnectAddress()) {\n\t\tt.Fatalf(\"expected peer 1 got %v\", v.ConnectAddress())\n\t}\n\n\t// Empty the hosts to trigger the panic when using the fallback.\n\tfor _, host := range hosts {\n\t\tpolicy.RemoveHost(host)\n\t}\n\n\tnext = iter()\n\tif next != nil {\n\t\tt.Errorf(\"expected to get nil host got %+v\", next)\n\t\tif next.Info() == nil {\n\t\t\tt.Fatalf(\"HostInfo is nil\")\n\t\t}\n\t}\n}\n\nfunc TestCOWList_Add(t *testing.T) {\n\tvar cow cowHostList\n\n\ttoAdd := [...]net.IP{net.IPv4(10, 0, 0, 1), net.IPv4(10, 0, 0, 2), net.IPv4(10, 0, 0, 3)}\n\n\tfor _, addr := range toAdd {\n\t\tif !cow.add(&HostInfo{connectAddress: addr}) {\n\t\t\tt.Fatal(\"did not add peer which was not in the set\")\n\t\t}\n\t}\n\n\thosts := cow.get()\n\tif len(hosts) != len(toAdd) {\n\t\tt.Fatalf(\"expected to have %d hosts got %d\", len(toAdd), len(hosts))\n\t}\n\n\tset := make(map[string]bool)\n\tfor _, host := range hosts {\n\t\tset[string(host.ConnectAddress())] = true\n\t}\n\n\tfor _, addr := range toAdd {\n\t\tif !set[string(addr)] {\n\t\t\tt.Errorf(\"addr was not in the host list: %q\", addr)\n\t\t}\n\t}\n}\n\n// TestSimpleRetryPolicy makes sure that we only allow 1 + numRetries attempts\nfunc TestSimpleRetryPolicy(t *testing.T) {\n\tq := &Query{routingInfo: &queryRoutingInfo{}}\n\n\t// this should allow a total of 3 tries.\n\trt := &SimpleRetryPolicy{NumRetries: 2}\n\n\tcases := []struct {\n\t\tattempts int\n\t\tallow    bool\n\t}{\n\t\t{0, true},\n\t\t{1, true},\n\t\t{2, true},\n\t\t{3, false},\n\t\t{4, false},\n\t\t{5, false},\n\t}\n\n\tfor _, c := range cases {\n\t\tq.metrics = preFilledQueryMetrics(map[string]*hostMetrics{\"127.0.0.1\": {Attempts: c.attempts}})\n\t\tif c.allow && !rt.Attempt(q) {\n\t\t\tt.Fatalf(\"should allow retry after %d attempts\", c.attempts)\n\t\t}\n\t\tif !c.allow && rt.Attempt(q) {\n\t\t\tt.Fatalf(\"should not allow retry after %d attempts\", c.attempts)\n\t\t}\n\t}\n}\n\nfunc TestExponentialBackoffPolicy(t *testing.T) {\n\t// test with defaults\n\tsut := &ExponentialBackoffRetryPolicy{NumRetries: 2}\n\n\tcases := []struct {\n\t\tattempts int\n\t\tdelay    time.Duration\n\t}{\n\n\t\t{1, 100 * time.Millisecond},\n\t\t{2, (2) * 100 * time.Millisecond},\n\t\t{3, (2 * 2) * 100 * time.Millisecond},\n\t\t{4, (2 * 2 * 2) * 100 * time.Millisecond},\n\t}\n\tfor _, c := range cases {\n\t\t// test 100 times for each case\n\t\tfor i := 0; i < 100; i++ {\n\t\t\td := sut.napTime(c.attempts)\n\t\t\tif d < c.delay-(100*time.Millisecond)/2 {\n\t\t\t\tt.Fatalf(\"Delay %d less than jitter min of %d\", d, c.delay-100*time.Millisecond/2)\n\t\t\t}\n\t\t\tif d > c.delay+(100*time.Millisecond)/2 {\n\t\t\t\tt.Fatalf(\"Delay %d greater than jitter max of %d\", d, c.delay+100*time.Millisecond/2)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestDowngradingConsistencyRetryPolicy(t *testing.T) {\n\n\tq := &Query{cons: LocalQuorum, routingInfo: &queryRoutingInfo{}}\n\n\trewt0 := &RequestErrWriteTimeout{\n\t\tReceived:  0,\n\t\tWriteType: \"SIMPLE\",\n\t}\n\n\trewt1 := &RequestErrWriteTimeout{\n\t\tReceived:  1,\n\t\tWriteType: \"BATCH\",\n\t}\n\n\trewt2 := &RequestErrWriteTimeout{\n\t\tWriteType: \"UNLOGGED_BATCH\",\n\t}\n\n\trert := &RequestErrReadTimeout{}\n\n\treu0 := &RequestErrUnavailable{\n\t\tAlive: 0,\n\t}\n\n\treu1 := &RequestErrUnavailable{\n\t\tAlive: 1,\n\t}\n\n\t// this should allow a total of 3 tries.\n\tconsistencyLevels := []Consistency{Three, Two, One}\n\trt := &DowngradingConsistencyRetryPolicy{ConsistencyLevelsToTry: consistencyLevels}\n\tcases := []struct {\n\t\tattempts  int\n\t\tallow     bool\n\t\terr       error\n\t\tretryType RetryType\n\t}{\n\t\t{0, true, rewt0, Rethrow},\n\t\t{3, true, rewt1, Ignore},\n\t\t{1, true, rewt2, Retry},\n\t\t{2, true, rert, Retry},\n\t\t{4, false, reu0, Rethrow},\n\t\t{16, false, reu1, Retry},\n\t}\n\n\tfor _, c := range cases {\n\t\tq.metrics = preFilledQueryMetrics(map[string]*hostMetrics{\"127.0.0.1\": {Attempts: c.attempts}})\n\t\tif c.retryType != rt.GetRetryType(c.err) {\n\t\t\tt.Fatalf(\"retry type should be %v\", c.retryType)\n\t\t}\n\t\tif c.allow && !rt.Attempt(q) {\n\t\t\tt.Fatalf(\"should allow retry after %d attempts\", c.attempts)\n\t\t}\n\t\tif !c.allow && rt.Attempt(q) {\n\t\t\tt.Fatalf(\"should not allow retry after %d attempts\", c.attempts)\n\t\t}\n\t}\n}\n\n// expectHosts makes sure that the next len(hostIDs) returned from iter is a permutation of hostIDs.\nfunc expectHosts(t *testing.T, msg string, iter NextHost, hostIDs ...string) {\n\tt.Helper()\n\n\texpectedHostIDs := make(map[string]struct{}, len(hostIDs))\n\tfor i := range hostIDs {\n\t\texpectedHostIDs[hostIDs[i]] = struct{}{}\n\t}\n\n\texpectedStr := func() string {\n\t\tkeys := make([]string, 0, len(expectedHostIDs))\n\t\tfor k := range expectedHostIDs {\n\t\t\tkeys = append(keys, k)\n\t\t}\n\t\tsort.Strings(keys)\n\t\treturn strings.Join(keys, \", \")\n\t}\n\n\tfor len(expectedHostIDs) > 0 {\n\t\thost := iter()\n\t\tif host == nil || host.Info() == nil {\n\t\t\tt.Fatalf(\"%s: expected hostID one of {%s}, but got nil\", msg, expectedStr())\n\t\t}\n\t\thostID := host.Info().HostID()\n\t\tif _, ok := expectedHostIDs[hostID]; !ok {\n\t\t\tt.Fatalf(\"%s: expected host ID one of {%s}, but got %s\", msg, expectedStr(), hostID)\n\t\t}\n\t\tdelete(expectedHostIDs, hostID)\n\t}\n}\n\nfunc expectNoMoreHosts(t *testing.T, iter NextHost) {\n\tt.Helper()\n\thost := iter()\n\tif host == nil {\n\t\t// success\n\t\treturn\n\t}\n\tinfo := host.Info()\n\tif info == nil {\n\t\tt.Fatalf(\"expected no more hosts, but got host with nil Info()\")\n\t\treturn\n\t}\n\tt.Fatalf(\"expected no more hosts, but got %s\", info.HostID())\n}\n\nfunc TestHostPolicy_DCAwareRR(t *testing.T) {\n\tp := DCAwareRoundRobinPolicy(\"local\")\n\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.ParseIP(\"10.0.0.1\"), dataCenter: \"local\"},\n\t\t{hostId: \"1\", connectAddress: net.ParseIP(\"10.0.0.2\"), dataCenter: \"local\"},\n\t\t{hostId: \"2\", connectAddress: net.ParseIP(\"10.0.0.3\"), dataCenter: \"remote\"},\n\t\t{hostId: \"3\", connectAddress: net.ParseIP(\"10.0.0.4\"), dataCenter: \"remote\"},\n\t}\n\n\tfor _, host := range hosts {\n\t\tp.AddHost(host)\n\t}\n\n\tgot := make(map[string]bool, len(hosts))\n\tvar dcs []string\n\n\tit := p.Pick(nil)\n\tfor h := it(); h != nil; h = it() {\n\t\tid := h.Info().hostId\n\t\tdc := h.Info().dataCenter\n\n\t\tif got[id] {\n\t\t\tt.Fatalf(\"got duplicate host %s\", id)\n\t\t}\n\t\tgot[id] = true\n\t\tdcs = append(dcs, dc)\n\t}\n\n\tif len(got) != len(hosts) {\n\t\tt.Fatalf(\"expected %d hosts got %d\", len(hosts), len(got))\n\t}\n\n\tvar remote bool\n\tfor _, dc := range dcs {\n\t\tif dc == \"local\" {\n\t\t\tif remote {\n\t\t\t\tt.Fatalf(\"got local dc after remote: %v\", dcs)\n\t\t\t}\n\t\t} else {\n\t\t\tremote = true\n\t\t}\n\t}\n\n}\n\n// Tests of the token-aware host selection policy implementation with a\n// DC aware round-robin host selection policy fallback\n// with {\"class\": \"NetworkTopologyStrategy\", \"a\": 1, \"b\": 1, \"c\": 1} replication.\nfunc TestHostPolicy_TokenAware(t *testing.T) {\n\tconst keyspace = \"myKeyspace\"\n\tpolicy := TokenAwareHostPolicy(DCAwareRoundRobinPolicy(\"local\"))\n\tpolicyInternal := policy.(*tokenAwareHostPolicy)\n\tpolicyInternal.getKeyspaceName = func() string { return keyspace }\n\tpolicyInternal.getKeyspaceMetadata = func(ks string) (*KeyspaceMetadata, error) {\n\t\treturn nil, errors.New(\"not initialized\")\n\t}\n\n\tquery := &Query{routingInfo: &queryRoutingInfo{}}\n\tquery.getKeyspace = func() string { return keyspace }\n\n\titer := policy.Pick(nil)\n\tif iter == nil {\n\t\tt.Fatal(\"host iterator was nil\")\n\t}\n\tactual := iter()\n\tif actual != nil {\n\t\tt.Fatalf(\"expected nil from iterator, but was %v\", actual)\n\t}\n\n\t// set the hosts\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(10, 0, 0, 1), tokens: []string{\"05\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(10, 0, 0, 2), tokens: []string{\"10\"}, dataCenter: \"local\"},\n\t\t{hostId: \"2\", connectAddress: net.IPv4(10, 0, 0, 3), tokens: []string{\"15\"}, dataCenter: \"remote2\"},\n\t\t{hostId: \"3\", connectAddress: net.IPv4(10, 0, 0, 4), tokens: []string{\"20\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"4\", connectAddress: net.IPv4(10, 0, 0, 5), tokens: []string{\"25\"}, dataCenter: \"local\"},\n\t\t{hostId: \"5\", connectAddress: net.IPv4(10, 0, 0, 6), tokens: []string{\"30\"}, dataCenter: \"remote2\"},\n\t\t{hostId: \"6\", connectAddress: net.IPv4(10, 0, 0, 7), tokens: []string{\"35\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"7\", connectAddress: net.IPv4(10, 0, 0, 8), tokens: []string{\"40\"}, dataCenter: \"local\"},\n\t\t{hostId: \"8\", connectAddress: net.IPv4(10, 0, 0, 9), tokens: []string{\"45\"}, dataCenter: \"remote2\"},\n\t\t{hostId: \"9\", connectAddress: net.IPv4(10, 0, 0, 10), tokens: []string{\"50\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"10\", connectAddress: net.IPv4(10, 0, 0, 11), tokens: []string{\"55\"}, dataCenter: \"local\"},\n\t\t{hostId: \"11\", connectAddress: net.IPv4(10, 0, 0, 12), tokens: []string{\"60\"}, dataCenter: \"remote2\"},\n\t}\n\tfor _, host := range hosts {\n\t\tpolicy.AddHost(host)\n\t}\n\n\t// the token ring is not setup without the partitioner, but the fallback\n\t// should work\n\tif actual := policy.Pick(nil)(); actual == nil {\n\t\tt.Fatal(\"expected to get host from fallback got nil\")\n\t}\n\n\tquery.RoutingKey([]byte(\"30\"))\n\tif actual := policy.Pick(query)(); actual == nil {\n\t\tt.Fatal(\"expected to get host from fallback got nil\")\n\t}\n\n\tpolicy.SetPartitioner(\"OrderedPartitioner\")\n\n\tpolicyInternal.getKeyspaceMetadata = func(keyspaceName string) (*KeyspaceMetadata, error) {\n\t\tif keyspaceName != keyspace {\n\t\t\treturn nil, fmt.Errorf(\"unknown keyspace: %s\", keyspaceName)\n\t\t}\n\t\treturn &KeyspaceMetadata{\n\t\t\tName:          keyspace,\n\t\t\tStrategyClass: \"NetworkTopologyStrategy\",\n\t\t\tStrategyOptions: map[string]interface{}{\n\t\t\t\t\"class\":   \"NetworkTopologyStrategy\",\n\t\t\t\t\"local\":   1,\n\t\t\t\t\"remote1\": 1,\n\t\t\t\t\"remote2\": 1,\n\t\t\t},\n\t\t}, nil\n\t}\n\tpolicy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: \"myKeyspace\"})\n\n\t// The NetworkTopologyStrategy above should generate the following replicas.\n\t// It's handy to have as reference here.\n\tassertDeepEqual(t, \"replicas\", map[string]tokenRingReplicas{\n\t\t\"myKeyspace\": {\n\t\t\t{orderedToken(\"05\"), []*HostInfo{hosts[0], hosts[1], hosts[2]}},\n\t\t\t{orderedToken(\"10\"), []*HostInfo{hosts[1], hosts[2], hosts[3]}},\n\t\t\t{orderedToken(\"15\"), []*HostInfo{hosts[2], hosts[3], hosts[4]}},\n\t\t\t{orderedToken(\"20\"), []*HostInfo{hosts[3], hosts[4], hosts[5]}},\n\t\t\t{orderedToken(\"25\"), []*HostInfo{hosts[4], hosts[5], hosts[6]}},\n\t\t\t{orderedToken(\"30\"), []*HostInfo{hosts[5], hosts[6], hosts[7]}},\n\t\t\t{orderedToken(\"35\"), []*HostInfo{hosts[6], hosts[7], hosts[8]}},\n\t\t\t{orderedToken(\"40\"), []*HostInfo{hosts[7], hosts[8], hosts[9]}},\n\t\t\t{orderedToken(\"45\"), []*HostInfo{hosts[8], hosts[9], hosts[10]}},\n\t\t\t{orderedToken(\"50\"), []*HostInfo{hosts[9], hosts[10], hosts[11]}},\n\t\t\t{orderedToken(\"55\"), []*HostInfo{hosts[10], hosts[11], hosts[0]}},\n\t\t\t{orderedToken(\"60\"), []*HostInfo{hosts[11], hosts[0], hosts[1]}},\n\t\t},\n\t}, policyInternal.getMetadataReadOnly().replicas)\n\n\t// now the token ring is configured\n\tquery.RoutingKey([]byte(\"23\"))\n\titer = policy.Pick(query)\n\t// first should be host with matching token from the local DC\n\texpectHosts(t, \"matching token from local DC\", iter, \"4\")\n\t// next are in non-deterministic order\n\texpectHosts(t, \"rest\", iter, \"0\", \"1\", \"2\", \"3\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\")\n\texpectNoMoreHosts(t, iter)\n}\n\n// Tests of the token-aware host selection policy implementation with a\n// DC aware round-robin host selection policy fallback\n// with {\"class\": \"NetworkTopologyStrategy\", \"a\": 2, \"b\": 2, \"c\": 2} replication.\nfunc TestHostPolicy_TokenAware_NetworkStrategy(t *testing.T) {\n\tconst keyspace = \"myKeyspace\"\n\tpolicy := TokenAwareHostPolicy(DCAwareRoundRobinPolicy(\"local\"), NonLocalReplicasFallback())\n\tpolicyInternal := policy.(*tokenAwareHostPolicy)\n\tpolicyInternal.getKeyspaceName = func() string { return keyspace }\n\tpolicyInternal.getKeyspaceMetadata = func(ks string) (*KeyspaceMetadata, error) {\n\t\treturn nil, errors.New(\"not initialized\")\n\t}\n\n\tquery := &Query{routingInfo: &queryRoutingInfo{}}\n\tquery.getKeyspace = func() string { return keyspace }\n\n\titer := policy.Pick(nil)\n\tif iter == nil {\n\t\tt.Fatal(\"host iterator was nil\")\n\t}\n\tactual := iter()\n\tif actual != nil {\n\t\tt.Fatalf(\"expected nil from iterator, but was %v\", actual)\n\t}\n\n\t// set the hosts\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(10, 0, 0, 1), tokens: []string{\"05\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(10, 0, 0, 2), tokens: []string{\"10\"}, dataCenter: \"local\"},\n\t\t{hostId: \"2\", connectAddress: net.IPv4(10, 0, 0, 3), tokens: []string{\"15\"}, dataCenter: \"remote2\"},\n\t\t{hostId: \"3\", connectAddress: net.IPv4(10, 0, 0, 4), tokens: []string{\"20\"}, dataCenter: \"remote1\"}, // 1\n\t\t{hostId: \"4\", connectAddress: net.IPv4(10, 0, 0, 5), tokens: []string{\"25\"}, dataCenter: \"local\"},   // 2\n\t\t{hostId: \"5\", connectAddress: net.IPv4(10, 0, 0, 6), tokens: []string{\"30\"}, dataCenter: \"remote2\"}, // 3\n\t\t{hostId: \"6\", connectAddress: net.IPv4(10, 0, 0, 7), tokens: []string{\"35\"}, dataCenter: \"remote1\"}, // 4\n\t\t{hostId: \"7\", connectAddress: net.IPv4(10, 0, 0, 8), tokens: []string{\"40\"}, dataCenter: \"local\"},   // 5\n\t\t{hostId: \"8\", connectAddress: net.IPv4(10, 0, 0, 9), tokens: []string{\"45\"}, dataCenter: \"remote2\"}, // 6\n\t\t{hostId: \"9\", connectAddress: net.IPv4(10, 0, 0, 10), tokens: []string{\"50\"}, dataCenter: \"remote1\"},\n\t\t{hostId: \"10\", connectAddress: net.IPv4(10, 0, 0, 11), tokens: []string{\"55\"}, dataCenter: \"local\"},\n\t\t{hostId: \"11\", connectAddress: net.IPv4(10, 0, 0, 12), tokens: []string{\"60\"}, dataCenter: \"remote2\"},\n\t}\n\tfor _, host := range hosts {\n\t\tpolicy.AddHost(host)\n\t}\n\n\tpolicy.SetPartitioner(\"OrderedPartitioner\")\n\n\tpolicyInternal.getKeyspaceMetadata = func(keyspaceName string) (*KeyspaceMetadata, error) {\n\t\tif keyspaceName != keyspace {\n\t\t\treturn nil, fmt.Errorf(\"unknown keyspace: %s\", keyspaceName)\n\t\t}\n\t\treturn &KeyspaceMetadata{\n\t\t\tName:          keyspace,\n\t\t\tStrategyClass: \"NetworkTopologyStrategy\",\n\t\t\tStrategyOptions: map[string]interface{}{\n\t\t\t\t\"class\":   \"NetworkTopologyStrategy\",\n\t\t\t\t\"local\":   2,\n\t\t\t\t\"remote1\": 2,\n\t\t\t\t\"remote2\": 2,\n\t\t\t},\n\t\t}, nil\n\t}\n\tpolicy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: keyspace})\n\n\t// The NetworkTopologyStrategy above should generate the following replicas.\n\t// It's handy to have as reference here.\n\tassertDeepEqual(t, \"replicas\", map[string]tokenRingReplicas{\n\t\tkeyspace: {\n\t\t\t{orderedToken(\"05\"), []*HostInfo{hosts[0], hosts[1], hosts[2], hosts[3], hosts[4], hosts[5]}},\n\t\t\t{orderedToken(\"10\"), []*HostInfo{hosts[1], hosts[2], hosts[3], hosts[4], hosts[5], hosts[6]}},\n\t\t\t{orderedToken(\"15\"), []*HostInfo{hosts[2], hosts[3], hosts[4], hosts[5], hosts[6], hosts[7]}},\n\t\t\t{orderedToken(\"20\"), []*HostInfo{hosts[3], hosts[4], hosts[5], hosts[6], hosts[7], hosts[8]}},\n\t\t\t{orderedToken(\"25\"), []*HostInfo{hosts[4], hosts[5], hosts[6], hosts[7], hosts[8], hosts[9]}},\n\t\t\t{orderedToken(\"30\"), []*HostInfo{hosts[5], hosts[6], hosts[7], hosts[8], hosts[9], hosts[10]}},\n\t\t\t{orderedToken(\"35\"), []*HostInfo{hosts[6], hosts[7], hosts[8], hosts[9], hosts[10], hosts[11]}},\n\t\t\t{orderedToken(\"40\"), []*HostInfo{hosts[7], hosts[8], hosts[9], hosts[10], hosts[11], hosts[0]}},\n\t\t\t{orderedToken(\"45\"), []*HostInfo{hosts[8], hosts[9], hosts[10], hosts[11], hosts[0], hosts[1]}},\n\t\t\t{orderedToken(\"50\"), []*HostInfo{hosts[9], hosts[10], hosts[11], hosts[0], hosts[1], hosts[2]}},\n\t\t\t{orderedToken(\"55\"), []*HostInfo{hosts[10], hosts[11], hosts[0], hosts[1], hosts[2], hosts[3]}},\n\t\t\t{orderedToken(\"60\"), []*HostInfo{hosts[11], hosts[0], hosts[1], hosts[2], hosts[3], hosts[4]}},\n\t\t},\n\t}, policyInternal.getMetadataReadOnly().replicas)\n\n\t// now the token ring is configured\n\tquery.RoutingKey([]byte(\"18\"))\n\titer = policy.Pick(query)\n\t// first should be hosts with matching token from the local DC\n\texpectHosts(t, \"matching token from local DC\", iter, \"4\", \"7\")\n\t// rest should be hosts with matching token from remote DCs\n\texpectHosts(t, \"matching token from remote DCs\", iter, \"3\", \"5\", \"6\", \"8\")\n\t// followed by other hosts\n\texpectHosts(t, \"rest\", iter, \"0\", \"1\", \"2\", \"9\", \"10\", \"11\")\n\texpectNoMoreHosts(t, iter)\n}\n\nfunc TestHostPolicy_RackAwareRR(t *testing.T) {\n\tp := RackAwareRoundRobinPolicy(\"local\", \"b\")\n\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.ParseIP(\"10.0.0.1\"), dataCenter: \"local\", rack: \"a\"},\n\t\t{hostId: \"1\", connectAddress: net.ParseIP(\"10.0.0.2\"), dataCenter: \"local\", rack: \"a\"},\n\t\t{hostId: \"2\", connectAddress: net.ParseIP(\"10.0.0.3\"), dataCenter: \"local\", rack: \"b\"},\n\t\t{hostId: \"3\", connectAddress: net.ParseIP(\"10.0.0.4\"), dataCenter: \"local\", rack: \"b\"},\n\t\t{hostId: \"4\", connectAddress: net.ParseIP(\"10.0.0.5\"), dataCenter: \"remote\", rack: \"a\"},\n\t\t{hostId: \"5\", connectAddress: net.ParseIP(\"10.0.0.6\"), dataCenter: \"remote\", rack: \"a\"},\n\t\t{hostId: \"6\", connectAddress: net.ParseIP(\"10.0.0.7\"), dataCenter: \"remote\", rack: \"b\"},\n\t\t{hostId: \"7\", connectAddress: net.ParseIP(\"10.0.0.8\"), dataCenter: \"remote\", rack: \"b\"},\n\t}\n\n\tfor _, host := range hosts {\n\t\tp.AddHost(host)\n\t}\n\n\tit := p.Pick(nil)\n\n\t// Must start with rack-local hosts\n\texpectHosts(t, \"rack-local hosts\", it, \"3\", \"2\")\n\t// Then dc-local hosts\n\texpectHosts(t, \"dc-local hosts\", it, \"0\", \"1\")\n\t// Then the remote hosts\n\texpectHosts(t, \"remote hosts\", it, \"4\", \"5\", \"6\", \"7\")\n\texpectNoMoreHosts(t, it)\n}\n\n// Tests of the token-aware host selection policy implementation with a\n// DC & Rack aware round-robin host selection policy fallback\nfunc TestHostPolicy_TokenAware_RackAware(t *testing.T) {\n\tconst keyspace = \"myKeyspace\"\n\tpolicy := TokenAwareHostPolicy(RackAwareRoundRobinPolicy(\"local\", \"b\"))\n\tpolicyWithFallback := TokenAwareHostPolicy(RackAwareRoundRobinPolicy(\"local\", \"b\"), NonLocalReplicasFallback())\n\n\tpolicyInternal := policy.(*tokenAwareHostPolicy)\n\tpolicyInternal.getKeyspaceName = func() string { return keyspace }\n\tpolicyInternal.getKeyspaceMetadata = func(ks string) (*KeyspaceMetadata, error) {\n\t\treturn nil, errors.New(\"not initialized\")\n\t}\n\n\tpolicyWithFallbackInternal := policyWithFallback.(*tokenAwareHostPolicy)\n\tpolicyWithFallbackInternal.getKeyspaceName = policyInternal.getKeyspaceName\n\tpolicyWithFallbackInternal.getKeyspaceMetadata = policyInternal.getKeyspaceMetadata\n\n\tquery := &Query{routingInfo: &queryRoutingInfo{}}\n\tquery.getKeyspace = func() string { return keyspace }\n\n\titer := policy.Pick(nil)\n\tif iter == nil {\n\t\tt.Fatal(\"host iterator was nil\")\n\t}\n\tactual := iter()\n\tif actual != nil {\n\t\tt.Fatalf(\"expected nil from iterator, but was %v\", actual)\n\t}\n\n\t// set the hosts\n\thosts := [...]*HostInfo{\n\t\t{hostId: \"0\", connectAddress: net.IPv4(10, 0, 0, 1), tokens: []string{\"05\"}, dataCenter: \"remote\", rack: \"a\"},\n\t\t{hostId: \"1\", connectAddress: net.IPv4(10, 0, 0, 2), tokens: []string{\"10\"}, dataCenter: \"remote\", rack: \"b\"},\n\t\t{hostId: \"2\", connectAddress: net.IPv4(10, 0, 0, 3), tokens: []string{\"15\"}, dataCenter: \"local\", rack: \"a\"},\n\t\t{hostId: \"3\", connectAddress: net.IPv4(10, 0, 0, 4), tokens: []string{\"20\"}, dataCenter: \"local\", rack: \"b\"},\n\t\t{hostId: \"4\", connectAddress: net.IPv4(10, 0, 0, 5), tokens: []string{\"25\"}, dataCenter: \"remote\", rack: \"a\"},\n\t\t{hostId: \"5\", connectAddress: net.IPv4(10, 0, 0, 6), tokens: []string{\"30\"}, dataCenter: \"remote\", rack: \"b\"},\n\t\t{hostId: \"6\", connectAddress: net.IPv4(10, 0, 0, 7), tokens: []string{\"35\"}, dataCenter: \"local\", rack: \"a\"},\n\t\t{hostId: \"7\", connectAddress: net.IPv4(10, 0, 0, 8), tokens: []string{\"40\"}, dataCenter: \"local\", rack: \"b\"},\n\t\t{hostId: \"8\", connectAddress: net.IPv4(10, 0, 0, 9), tokens: []string{\"45\"}, dataCenter: \"remote\", rack: \"a\"},\n\t\t{hostId: \"9\", connectAddress: net.IPv4(10, 0, 0, 10), tokens: []string{\"50\"}, dataCenter: \"remote\", rack: \"b\"},\n\t\t{hostId: \"10\", connectAddress: net.IPv4(10, 0, 0, 11), tokens: []string{\"55\"}, dataCenter: \"local\", rack: \"a\"},\n\t\t{hostId: \"11\", connectAddress: net.IPv4(10, 0, 0, 12), tokens: []string{\"60\"}, dataCenter: \"local\", rack: \"b\"},\n\t}\n\tfor _, host := range hosts {\n\t\tpolicy.AddHost(host)\n\t\tpolicyWithFallback.AddHost(host)\n\t}\n\n\t// the token ring is not setup without the partitioner, but the fallback\n\t// should work\n\tif actual := policy.Pick(nil)(); actual == nil {\n\t\tt.Fatal(\"expected to get host from fallback got nil\")\n\t}\n\n\tquery.RoutingKey([]byte(\"30\"))\n\tif actual := policy.Pick(query)(); actual == nil {\n\t\tt.Fatal(\"expected to get host from fallback got nil\")\n\t}\n\n\tpolicy.SetPartitioner(\"OrderedPartitioner\")\n\tpolicyWithFallback.SetPartitioner(\"OrderedPartitioner\")\n\n\tpolicyInternal.getKeyspaceMetadata = func(keyspaceName string) (*KeyspaceMetadata, error) {\n\t\tif keyspaceName != keyspace {\n\t\t\treturn nil, fmt.Errorf(\"unknown keyspace: %s\", keyspaceName)\n\t\t}\n\t\treturn &KeyspaceMetadata{\n\t\t\tName:          keyspace,\n\t\t\tStrategyClass: \"NetworkTopologyStrategy\",\n\t\t\tStrategyOptions: map[string]interface{}{\n\t\t\t\t\"class\":  \"NetworkTopologyStrategy\",\n\t\t\t\t\"local\":  2,\n\t\t\t\t\"remote\": 2,\n\t\t\t},\n\t\t}, nil\n\t}\n\tpolicyWithFallbackInternal.getKeyspaceMetadata = policyInternal.getKeyspaceMetadata\n\tpolicy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: \"myKeyspace\"})\n\tpolicyWithFallback.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: \"myKeyspace\"})\n\n\t// The NetworkTopologyStrategy above should generate the following replicas.\n\t// It's handy to have as reference here.\n\tassertDeepEqual(t, \"replicas\", map[string]tokenRingReplicas{\n\t\t\"myKeyspace\": {\n\t\t\t{orderedToken(\"05\"), []*HostInfo{hosts[0], hosts[1], hosts[2], hosts[3]}},\n\t\t\t{orderedToken(\"10\"), []*HostInfo{hosts[1], hosts[2], hosts[3], hosts[4]}},\n\t\t\t{orderedToken(\"15\"), []*HostInfo{hosts[2], hosts[3], hosts[4], hosts[5]}},\n\t\t\t{orderedToken(\"20\"), []*HostInfo{hosts[3], hosts[4], hosts[5], hosts[6]}},\n\t\t\t{orderedToken(\"25\"), []*HostInfo{hosts[4], hosts[5], hosts[6], hosts[7]}},\n\t\t\t{orderedToken(\"30\"), []*HostInfo{hosts[5], hosts[6], hosts[7], hosts[8]}},\n\t\t\t{orderedToken(\"35\"), []*HostInfo{hosts[6], hosts[7], hosts[8], hosts[9]}},\n\t\t\t{orderedToken(\"40\"), []*HostInfo{hosts[7], hosts[8], hosts[9], hosts[10]}},\n\t\t\t{orderedToken(\"45\"), []*HostInfo{hosts[8], hosts[9], hosts[10], hosts[11]}},\n\t\t\t{orderedToken(\"50\"), []*HostInfo{hosts[9], hosts[10], hosts[11], hosts[0]}},\n\t\t\t{orderedToken(\"55\"), []*HostInfo{hosts[10], hosts[11], hosts[0], hosts[1]}},\n\t\t\t{orderedToken(\"60\"), []*HostInfo{hosts[11], hosts[0], hosts[1], hosts[2]}},\n\t\t},\n\t}, policyInternal.getMetadataReadOnly().replicas)\n\n\tquery.RoutingKey([]byte(\"23\"))\n\n\t// now the token ring is configured\n\t// Test the policy with fallback\n\titer = policyWithFallback.Pick(query)\n\n\t// first should be host with matching token from the local DC & rack\n\texpectHosts(t, \"matching token from local DC and local rack\", iter, \"7\")\n\t// next should be host with matching token from local DC and other rack\n\texpectHosts(t, \"matching token from local DC and non-local rack\", iter, \"6\")\n\t// next should be hosts with matching token from other DC, in any order\n\texpectHosts(t, \"matching token from non-local DC\", iter, \"4\", \"5\")\n\t// then the local DC & rack that didn't match the token\n\texpectHosts(t, \"non-matching token from local DC and local rack\", iter, \"3\", \"11\")\n\t// then the local DC & other rack that didn't match the token\n\texpectHosts(t, \"non-matching token from local DC and non-local rack\", iter, \"2\", \"10\")\n\t// finally, the other DC that didn't match the token\n\texpectHosts(t, \"non-matching token from non-local DC\", iter, \"0\", \"1\", \"8\", \"9\")\n\texpectNoMoreHosts(t, iter)\n\n\t// Test the policy without fallback\n\titer = policy.Pick(query)\n\n\t// first should be host with matching token from the local DC & Rack\n\texpectHosts(t, \"matching token from local DC and local rack\", iter, \"7\")\n\t// next should be the other two hosts from local DC & rack\n\texpectHosts(t, \"non-matching token local DC and local rack\", iter, \"3\", \"11\")\n\t// then the three hosts from the local DC but other rack\n\texpectHosts(t, \"local DC, non-local rack\", iter, \"2\", \"6\", \"10\")\n\t// then the 6 hosts from the other DC\n\texpectHosts(t, \"non-local DC\", iter, \"0\", \"1\", \"4\", \"5\", \"8\", \"9\")\n\texpectNoMoreHosts(t, iter)\n}\n"
        },
        {
          "name": "prepared_cache.go",
          "type": "blob",
          "size": 2.326171875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"sync\"\n\n\t\"github.com/gocql/gocql/internal/lru\"\n)\n\nconst defaultMaxPreparedStmts = 1000\n\n// preparedLRU is the prepared statement cache\ntype preparedLRU struct {\n\tmu  sync.Mutex\n\tlru *lru.Cache\n}\n\nfunc (p *preparedLRU) clear() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tfor p.lru.Len() > 0 {\n\t\tp.lru.RemoveOldest()\n\t}\n}\n\nfunc (p *preparedLRU) add(key string, val *inflightPrepare) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\tp.lru.Add(key, val)\n}\n\nfunc (p *preparedLRU) remove(key string) bool {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\treturn p.lru.Remove(key)\n}\n\nfunc (p *preparedLRU) execIfMissing(key string, fn func(lru *lru.Cache) *inflightPrepare) (*inflightPrepare, bool) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tval, ok := p.lru.Get(key)\n\tif ok {\n\t\treturn val.(*inflightPrepare), true\n\t}\n\n\treturn fn(p.lru), false\n}\n\nfunc (p *preparedLRU) keyFor(hostID, keyspace, statement string) string {\n\t// TODO: we should just use a struct for the key in the map\n\treturn hostID + keyspace + statement\n}\n\nfunc (p *preparedLRU) evictPreparedID(key string, id []byte) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tval, ok := p.lru.Get(key)\n\tif !ok {\n\t\treturn\n\t}\n\n\tifp, ok := val.(*inflightPrepare)\n\tif !ok {\n\t\treturn\n\t}\n\n\tselect {\n\tcase <-ifp.done:\n\t\tif bytes.Equal(id, ifp.preparedStatment.id) {\n\t\t\tp.lru.Remove(key)\n\t\t}\n\tdefault:\n\t}\n\n}\n"
        },
        {
          "name": "query_executor.go",
          "type": "blob",
          "size": 5.8603515625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype ExecutableQuery interface {\n\tborrowForExecution()    // Used to ensure that the query stays alive for lifetime of a particular execution goroutine.\n\treleaseAfterExecution() // Used when a goroutine finishes its execution attempts, either with ok result or an error.\n\texecute(ctx context.Context, conn *Conn) *Iter\n\tattempt(keyspace string, end, start time.Time, iter *Iter, host *HostInfo)\n\tretryPolicy() RetryPolicy\n\tspeculativeExecutionPolicy() SpeculativeExecutionPolicy\n\tGetRoutingKey() ([]byte, error)\n\tKeyspace() string\n\tTable() string\n\tIsIdempotent() bool\n\n\twithContext(context.Context) ExecutableQuery\n\n\tRetryableQuery\n}\n\ntype queryExecutor struct {\n\tpool   *policyConnPool\n\tpolicy HostSelectionPolicy\n}\n\nfunc (q *queryExecutor) attemptQuery(ctx context.Context, qry ExecutableQuery, conn *Conn) *Iter {\n\tstart := time.Now()\n\titer := qry.execute(ctx, conn)\n\tend := time.Now()\n\n\tqry.attempt(q.pool.keyspace, end, start, iter, conn.host)\n\n\treturn iter\n}\n\nfunc (q *queryExecutor) speculate(ctx context.Context, qry ExecutableQuery, sp SpeculativeExecutionPolicy,\n\thostIter NextHost, results chan *Iter) *Iter {\n\tticker := time.NewTicker(sp.Delay())\n\tdefer ticker.Stop()\n\n\tfor i := 0; i < sp.Attempts(); i++ {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\tqry.borrowForExecution() // ensure liveness in case of executing Query to prevent races with Query.Release().\n\t\t\tgo q.run(ctx, qry, hostIter, results)\n\t\tcase <-ctx.Done():\n\t\t\treturn &Iter{err: ctx.Err()}\n\t\tcase iter := <-results:\n\t\t\treturn iter\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (q *queryExecutor) executeQuery(qry ExecutableQuery) (*Iter, error) {\n\thostIter := q.policy.Pick(qry)\n\n\t// check if the query is not marked as idempotent, if\n\t// it is, we force the policy to NonSpeculative\n\tsp := qry.speculativeExecutionPolicy()\n\tif !qry.IsIdempotent() || sp.Attempts() == 0 {\n\t\treturn q.do(qry.Context(), qry, hostIter), nil\n\t}\n\n\t// When speculative execution is enabled, we could be accessing the host iterator from multiple goroutines below.\n\t// To ensure we don't call it concurrently, we wrap the returned NextHost function here to synchronize access to it.\n\tvar mu sync.Mutex\n\torigHostIter := hostIter\n\thostIter = func() SelectedHost {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\treturn origHostIter()\n\t}\n\n\tctx, cancel := context.WithCancel(qry.Context())\n\tdefer cancel()\n\n\tresults := make(chan *Iter, 1)\n\n\t// Launch the main execution\n\tqry.borrowForExecution() // ensure liveness in case of executing Query to prevent races with Query.Release().\n\tgo q.run(ctx, qry, hostIter, results)\n\n\t// The speculative executions are launched _in addition_ to the main\n\t// execution, on a timer. So Speculation{2} would make 3 executions running\n\t// in total.\n\tif iter := q.speculate(ctx, qry, sp, hostIter, results); iter != nil {\n\t\treturn iter, nil\n\t}\n\n\tselect {\n\tcase iter := <-results:\n\t\treturn iter, nil\n\tcase <-ctx.Done():\n\t\treturn &Iter{err: ctx.Err()}, nil\n\t}\n}\n\nfunc (q *queryExecutor) do(ctx context.Context, qry ExecutableQuery, hostIter NextHost) *Iter {\n\tselectedHost := hostIter()\n\trt := qry.retryPolicy()\n\n\tvar lastErr error\n\tvar iter *Iter\n\tfor selectedHost != nil {\n\t\thost := selectedHost.Info()\n\t\tif host == nil || !host.IsUp() {\n\t\t\tselectedHost = hostIter()\n\t\t\tcontinue\n\t\t}\n\n\t\tpool, ok := q.pool.getPool(host)\n\t\tif !ok {\n\t\t\tselectedHost = hostIter()\n\t\t\tcontinue\n\t\t}\n\n\t\tconn := pool.Pick()\n\t\tif conn == nil {\n\t\t\tselectedHost = hostIter()\n\t\t\tcontinue\n\t\t}\n\n\t\titer = q.attemptQuery(ctx, qry, conn)\n\t\titer.host = selectedHost.Info()\n\t\t// Update host\n\t\tswitch iter.err {\n\t\tcase context.Canceled, context.DeadlineExceeded, ErrNotFound:\n\t\t\t// those errors represents logical errors, they should not count\n\t\t\t// toward removing a node from the pool\n\t\t\tselectedHost.Mark(nil)\n\t\t\treturn iter\n\t\tdefault:\n\t\t\tselectedHost.Mark(iter.err)\n\t\t}\n\n\t\t// Exit if the query was successful\n\t\t// or query is not idempotent or no retry policy defined\n\t\tif iter.err == nil || !qry.IsIdempotent() || rt == nil {\n\t\t\treturn iter\n\t\t}\n\n\t\tattemptsReached := !rt.Attempt(qry)\n\t\tretryType := rt.GetRetryType(iter.err)\n\n\t\tvar stopRetries bool\n\n\t\t// If query is unsuccessful, check the error with RetryPolicy to retry\n\t\tswitch retryType {\n\t\tcase Retry:\n\t\t\t// retry on the same host\n\t\tcase RetryNextHost:\n\t\t\t// retry on the next host\n\t\t\tselectedHost = hostIter()\n\t\tcase Ignore:\n\t\t\titer.err = nil\n\t\t\tstopRetries = true\n\t\tcase Rethrow:\n\t\t\tstopRetries = true\n\t\tdefault:\n\t\t\t// Undefined? Return nil and error, this will panic in the requester\n\t\t\treturn &Iter{err: ErrUnknownRetryType}\n\t\t}\n\n\t\tif stopRetries || attemptsReached {\n\t\t\treturn iter\n\t\t}\n\n\t\tlastErr = iter.err\n\t\tcontinue\n\t}\n\n\tif lastErr != nil {\n\t\treturn &Iter{err: lastErr}\n\t}\n\n\treturn &Iter{err: ErrNoConnections}\n}\n\nfunc (q *queryExecutor) run(ctx context.Context, qry ExecutableQuery, hostIter NextHost, results chan<- *Iter) {\n\tselect {\n\tcase results <- q.do(ctx, qry, hostIter):\n\tcase <-ctx.Done():\n\t}\n\tqry.releaseAfterExecution()\n}\n"
        },
        {
          "name": "ring.go",
          "type": "blob",
          "size": 3.9599609375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\ntype ring struct {\n\t// endpoints are the set of endpoints which the driver will attempt to connect\n\t// to in the case it can not reach any of its hosts. They are also used to boot\n\t// strap the initial connection.\n\tendpoints []*HostInfo\n\n\tmu sync.RWMutex\n\t// hosts are the set of all hosts in the cassandra ring that we know of.\n\t// key of map is host_id.\n\thosts map[string]*HostInfo\n\t// hostIPToUUID maps host native address to host_id.\n\thostIPToUUID map[string]string\n\n\thostList []*HostInfo\n\tpos      uint32\n\n\t// TODO: we should store the ring metadata here also.\n}\n\nfunc (r *ring) rrHost() *HostInfo {\n\tr.mu.RLock()\n\tdefer r.mu.RUnlock()\n\tif len(r.hostList) == 0 {\n\t\treturn nil\n\t}\n\n\tpos := int(atomic.AddUint32(&r.pos, 1) - 1)\n\treturn r.hostList[pos%len(r.hostList)]\n}\n\nfunc (r *ring) getHostByIP(ip string) (*HostInfo, bool) {\n\tr.mu.RLock()\n\tdefer r.mu.RUnlock()\n\thi, ok := r.hostIPToUUID[ip]\n\treturn r.hosts[hi], ok\n}\n\nfunc (r *ring) getHost(hostID string) *HostInfo {\n\tr.mu.RLock()\n\thost := r.hosts[hostID]\n\tr.mu.RUnlock()\n\treturn host\n}\n\nfunc (r *ring) allHosts() []*HostInfo {\n\tr.mu.RLock()\n\thosts := make([]*HostInfo, 0, len(r.hosts))\n\tfor _, host := range r.hosts {\n\t\thosts = append(hosts, host)\n\t}\n\tr.mu.RUnlock()\n\treturn hosts\n}\n\nfunc (r *ring) currentHosts() map[string]*HostInfo {\n\tr.mu.RLock()\n\thosts := make(map[string]*HostInfo, len(r.hosts))\n\tfor k, v := range r.hosts {\n\t\thosts[k] = v\n\t}\n\tr.mu.RUnlock()\n\treturn hosts\n}\n\nfunc (r *ring) addOrUpdate(host *HostInfo) *HostInfo {\n\tif existingHost, ok := r.addHostIfMissing(host); ok {\n\t\texistingHost.update(host)\n\t\thost = existingHost\n\t}\n\treturn host\n}\n\nfunc (r *ring) addHostIfMissing(host *HostInfo) (*HostInfo, bool) {\n\tif host.invalidConnectAddr() {\n\t\tpanic(fmt.Sprintf(\"invalid host: %v\", host))\n\t}\n\thostID := host.HostID()\n\n\tr.mu.Lock()\n\tif r.hosts == nil {\n\t\tr.hosts = make(map[string]*HostInfo)\n\t}\n\tif r.hostIPToUUID == nil {\n\t\tr.hostIPToUUID = make(map[string]string)\n\t}\n\n\texisting, ok := r.hosts[hostID]\n\tif !ok {\n\t\tr.hosts[hostID] = host\n\t\tr.hostIPToUUID[host.nodeToNodeAddress().String()] = hostID\n\t\texisting = host\n\t\tr.hostList = append(r.hostList, host)\n\t}\n\tr.mu.Unlock()\n\treturn existing, ok\n}\n\nfunc (r *ring) removeHost(hostID string) bool {\n\tr.mu.Lock()\n\tif r.hosts == nil {\n\t\tr.hosts = make(map[string]*HostInfo)\n\t}\n\tif r.hostIPToUUID == nil {\n\t\tr.hostIPToUUID = make(map[string]string)\n\t}\n\n\th, ok := r.hosts[hostID]\n\tif ok {\n\t\tfor i, host := range r.hostList {\n\t\t\tif host.HostID() == hostID {\n\t\t\t\tr.hostList = append(r.hostList[:i], r.hostList[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tdelete(r.hostIPToUUID, h.nodeToNodeAddress().String())\n\t}\n\tdelete(r.hosts, hostID)\n\tr.mu.Unlock()\n\treturn ok\n}\n\ntype clusterMetadata struct {\n\tmu          sync.RWMutex\n\tpartitioner string\n}\n\nfunc (c *clusterMetadata) setPartitioner(partitioner string) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.partitioner != partitioner {\n\t\t// TODO: update other things now\n\t\tc.partitioner = partitioner\n\t}\n}\n"
        },
        {
          "name": "ring_test.go",
          "type": "blob",
          "size": 2.052734375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestRing_AddHostIfMissing_Missing(t *testing.T) {\n\tring := &ring{}\n\n\thost := &HostInfo{hostId: MustRandomUUID().String(), connectAddress: net.IPv4(1, 1, 1, 1)}\n\th1, ok := ring.addHostIfMissing(host)\n\tif ok {\n\t\tt.Fatal(\"host was reported as already existing\")\n\t} else if !h1.Equal(host) {\n\t\tt.Fatalf(\"hosts not equal that are returned %v != %v\", h1, host)\n\t} else if h1 != host {\n\t\tt.Fatalf(\"returned host same pointer: %p != %p\", h1, host)\n\t}\n}\n\nfunc TestRing_AddHostIfMissing_Existing(t *testing.T) {\n\tring := &ring{}\n\n\thost := &HostInfo{hostId: MustRandomUUID().String(), connectAddress: net.IPv4(1, 1, 1, 1)}\n\tring.addHostIfMissing(host)\n\n\th2 := &HostInfo{hostId: host.hostId, connectAddress: net.IPv4(2, 2, 2, 2)}\n\n\th1, ok := ring.addHostIfMissing(h2)\n\tif !ok {\n\t\tt.Fatal(\"host was not reported as already existing\")\n\t} else if !h1.Equal(host) {\n\t\tt.Fatalf(\"hosts not equal that are returned %v != %v\", h1, host)\n\t} else if h1 != host {\n\t\tt.Fatalf(\"returned host same pointer: %p != %p\", h1, host)\n\t}\n}\n"
        },
        {
          "name": "session.go",
          "type": "blob",
          "size": 63.833984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/gocql/gocql/internal/lru\"\n)\n\n// Session is the interface used by users to interact with the database.\n//\n// It's safe for concurrent use by multiple goroutines and a typical usage\n// scenario is to have one global session object to interact with the\n// whole Cassandra cluster.\n//\n// This type extends the Node interface by adding a convenient query builder\n// and automatically sets a default consistency level on all operations\n// that do not have a consistency level set.\ntype Session struct {\n\tcons                Consistency\n\tpageSize            int\n\tprefetch            float64\n\troutingKeyInfoCache routingKeyInfoLRU\n\tschemaDescriber     *schemaDescriber\n\ttrace               Tracer\n\tqueryObserver       QueryObserver\n\tbatchObserver       BatchObserver\n\tconnectObserver     ConnectObserver\n\tframeObserver       FrameHeaderObserver\n\tstreamObserver      StreamObserver\n\thostSource          *ringDescriber\n\tringRefresher       *refreshDebouncer\n\tstmtsLRU            *preparedLRU\n\n\tconnCfg *ConnConfig\n\n\texecutor *queryExecutor\n\tpool     *policyConnPool\n\tpolicy   HostSelectionPolicy\n\n\tring     ring\n\tmetadata clusterMetadata\n\n\tmu sync.RWMutex\n\n\tcontrol *controlConn\n\n\t// event handlers\n\tnodeEvents   *eventDebouncer\n\tschemaEvents *eventDebouncer\n\n\t// ring metadata\n\tuseSystemSchema           bool\n\thasAggregatesAndFunctions bool\n\n\tcfg ClusterConfig\n\n\tctx    context.Context\n\tcancel context.CancelFunc\n\n\t// sessionStateMu protects isClosed and isInitialized.\n\tsessionStateMu sync.RWMutex\n\t// isClosed is true once Session.Close is finished.\n\tisClosed bool\n\t// isClosing bool is true once Session.Close is started.\n\tisClosing bool\n\t// isInitialized is true once Session.init succeeds.\n\t// you can use initialized() to read the value.\n\tisInitialized bool\n\n\tlogger StdLogger\n}\n\nvar queryPool = &sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &Query{routingInfo: &queryRoutingInfo{}, refCount: 1}\n\t},\n}\n\nfunc addrsToHosts(addrs []string, defaultPort int, logger StdLogger) ([]*HostInfo, error) {\n\tvar hosts []*HostInfo\n\tfor _, hostaddr := range addrs {\n\t\tresolvedHosts, err := hostInfo(hostaddr, defaultPort)\n\t\tif err != nil {\n\t\t\t// Try other hosts if unable to resolve DNS name\n\t\t\tif _, ok := err.(*net.DNSError); ok {\n\t\t\t\tlogger.Printf(\"gocql: dns error: %v\\n\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\n\t\thosts = append(hosts, resolvedHosts...)\n\t}\n\tif len(hosts) == 0 {\n\t\treturn nil, errors.New(\"failed to resolve any of the provided hostnames\")\n\t}\n\treturn hosts, nil\n}\n\n// NewSession wraps an existing Node.\nfunc NewSession(cfg ClusterConfig) (*Session, error) {\n\t// Check that hosts in the ClusterConfig is not empty\n\tif len(cfg.Hosts) < 1 {\n\t\treturn nil, ErrNoHosts\n\t}\n\n\t// Check that either Authenticator is set or AuthProvider, not both\n\tif cfg.Authenticator != nil && cfg.AuthProvider != nil {\n\t\treturn nil, errors.New(\"Can't use both Authenticator and AuthProvider in cluster config.\")\n\t}\n\n\t// TODO: we should take a context in here at some point\n\tctx, cancel := context.WithCancel(context.TODO())\n\n\ts := &Session{\n\t\tcons:            cfg.Consistency,\n\t\tprefetch:        0.25,\n\t\tcfg:             cfg,\n\t\tpageSize:        cfg.PageSize,\n\t\tstmtsLRU:        &preparedLRU{lru: lru.New(cfg.MaxPreparedStmts)},\n\t\tconnectObserver: cfg.ConnectObserver,\n\t\tctx:             ctx,\n\t\tcancel:          cancel,\n\t\tlogger:          cfg.logger(),\n\t}\n\n\ts.schemaDescriber = newSchemaDescriber(s)\n\n\ts.nodeEvents = newEventDebouncer(\"NodeEvents\", s.handleNodeEvent, s.logger)\n\ts.schemaEvents = newEventDebouncer(\"SchemaEvents\", s.handleSchemaEvent, s.logger)\n\n\ts.routingKeyInfoCache.lru = lru.New(cfg.MaxRoutingKeyInfo)\n\n\ts.hostSource = &ringDescriber{session: s}\n\ts.ringRefresher = newRefreshDebouncer(ringRefreshDebounceTime, func() error { return refreshRing(s.hostSource) })\n\n\tif cfg.PoolConfig.HostSelectionPolicy == nil {\n\t\tcfg.PoolConfig.HostSelectionPolicy = RoundRobinHostPolicy()\n\t}\n\ts.pool = cfg.PoolConfig.buildPool(s)\n\n\ts.policy = cfg.PoolConfig.HostSelectionPolicy\n\ts.policy.Init(s)\n\n\ts.executor = &queryExecutor{\n\t\tpool:   s.pool,\n\t\tpolicy: cfg.PoolConfig.HostSelectionPolicy,\n\t}\n\n\ts.queryObserver = cfg.QueryObserver\n\ts.batchObserver = cfg.BatchObserver\n\ts.connectObserver = cfg.ConnectObserver\n\ts.frameObserver = cfg.FrameHeaderObserver\n\ts.streamObserver = cfg.StreamObserver\n\n\t//Check the TLS Config before trying to connect to anything external\n\tconnCfg, err := connConfig(&s.cfg)\n\tif err != nil {\n\t\t//TODO: Return a typed error\n\t\treturn nil, fmt.Errorf(\"gocql: unable to create session: %v\", err)\n\t}\n\ts.connCfg = connCfg\n\n\tif err := s.init(); err != nil {\n\t\ts.Close()\n\t\tif err == ErrNoConnectionsStarted {\n\t\t\t//This error used to be generated inside NewSession & returned directly\n\t\t\t//Forward it on up to be backwards compatible\n\t\t\treturn nil, ErrNoConnectionsStarted\n\t\t} else {\n\t\t\t// TODO(zariel): dont wrap this error in fmt.Errorf, return a typed error\n\t\t\treturn nil, fmt.Errorf(\"gocql: unable to create session: %v\", err)\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\nfunc (s *Session) init() error {\n\thosts, err := addrsToHosts(s.cfg.Hosts, s.cfg.Port, s.logger)\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.ring.endpoints = hosts\n\n\tif !s.cfg.disableControlConn {\n\t\ts.control = createControlConn(s)\n\t\tif s.cfg.ProtoVersion == 0 {\n\t\t\tproto, err := s.control.discoverProtocol(hosts)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to discover protocol version: %v\", err)\n\t\t\t} else if proto == 0 {\n\t\t\t\treturn errors.New(\"unable to discovery protocol version\")\n\t\t\t}\n\n\t\t\t// TODO(zariel): we really only need this in 1 place\n\t\t\ts.cfg.ProtoVersion = proto\n\t\t\ts.connCfg.ProtoVersion = proto\n\t\t}\n\n\t\tif err := s.control.connect(hosts); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif !s.cfg.DisableInitialHostLookup {\n\t\t\tvar partitioner string\n\t\t\tnewHosts, partitioner, err := s.hostSource.GetHosts()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ts.policy.SetPartitioner(partitioner)\n\t\t\tfilteredHosts := make([]*HostInfo, 0, len(newHosts))\n\t\t\tfor _, host := range newHosts {\n\t\t\t\tif !s.cfg.filterHost(host) {\n\t\t\t\t\tfilteredHosts = append(filteredHosts, host)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\thosts = filteredHosts\n\t\t}\n\t}\n\n\tfor _, host := range hosts {\n\t\t// In case when host lookup is disabled and when we are in unit tests,\n\t\t// host are not discovered, and we are missing host ID information used\n\t\t// by internal logic.\n\t\t// Associate random UUIDs here with all hosts missing this information.\n\t\tif len(host.HostID()) == 0 {\n\t\t\thost.SetHostID(MustRandomUUID().String())\n\t\t}\n\t}\n\n\thostMap := make(map[string]*HostInfo, len(hosts))\n\tfor _, host := range hosts {\n\t\thostMap[host.HostID()] = host\n\t}\n\n\thosts = hosts[:0]\n\t// each host will increment left and decrement it after connecting and once\n\t// there's none left, we'll close hostCh\n\tvar left int64\n\t// we will receive up to len(hostMap) of messages so create a buffer so we\n\t// don't end up stuck in a goroutine if we stopped listening\n\tconnectedCh := make(chan struct{}, len(hostMap))\n\t// we add one here because we don't want to end up closing hostCh until we're\n\t// done looping and the decerement code might be reached before we've looped\n\t// again\n\tatomic.AddInt64(&left, 1)\n\tfor _, host := range hostMap {\n\t\thost := s.ring.addOrUpdate(host)\n\t\tif s.cfg.filterHost(host) {\n\t\t\tcontinue\n\t\t}\n\n\t\tatomic.AddInt64(&left, 1)\n\t\tgo func() {\n\t\t\ts.pool.addHost(host)\n\t\t\tconnectedCh <- struct{}{}\n\n\t\t\t// if there are no hosts left, then close the hostCh to unblock the loop\n\t\t\t// below if its still waiting\n\t\t\tif atomic.AddInt64(&left, -1) == 0 {\n\t\t\t\tclose(connectedCh)\n\t\t\t}\n\t\t}()\n\n\t\thosts = append(hosts, host)\n\t}\n\t// once we're done looping we subtract the one we initially added and check\n\t// to see if we should close\n\tif atomic.AddInt64(&left, -1) == 0 {\n\t\tclose(connectedCh)\n\t}\n\n\t// before waiting for them to connect, add them all to the policy so we can\n\t// utilize efficiencies by calling AddHosts if the policy supports it\n\ttype bulkAddHosts interface {\n\t\tAddHosts([]*HostInfo)\n\t}\n\tif v, ok := s.policy.(bulkAddHosts); ok {\n\t\tv.AddHosts(hosts)\n\t} else {\n\t\tfor _, host := range hosts {\n\t\t\ts.policy.AddHost(host)\n\t\t}\n\t}\n\n\treadyPolicy, _ := s.policy.(ReadyPolicy)\n\t// now loop over connectedCh until it's closed (meaning we've connected to all)\n\t// or until the policy says we're ready\n\tfor range connectedCh {\n\t\tif readyPolicy != nil && readyPolicy.Ready() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// TODO(zariel): we probably dont need this any more as we verify that we\n\t// can connect to one of the endpoints supplied by using the control conn.\n\t// See if there are any connections in the pool\n\tif s.cfg.ReconnectInterval > 0 {\n\t\tgo s.reconnectDownedHosts(s.cfg.ReconnectInterval)\n\t}\n\n\t// If we disable the initial host lookup, we need to still check if the\n\t// cluster is using the newer system schema or not... however, if control\n\t// connection is disable, we really have no choice, so we just make our\n\t// best guess...\n\tif !s.cfg.disableControlConn && s.cfg.DisableInitialHostLookup {\n\t\tnewer, _ := checkSystemSchema(s.control)\n\t\ts.useSystemSchema = newer\n\t} else {\n\t\tversion := s.ring.rrHost().Version()\n\t\ts.useSystemSchema = version.AtLeast(3, 0, 0)\n\t\ts.hasAggregatesAndFunctions = version.AtLeast(2, 2, 0)\n\t}\n\n\tif s.pool.Size() == 0 {\n\t\treturn ErrNoConnectionsStarted\n\t}\n\n\t// Invoke KeyspaceChanged to let the policy cache the session keyspace\n\t// parameters. This is used by tokenAwareHostPolicy to discover replicas.\n\tif !s.cfg.disableControlConn && s.cfg.Keyspace != \"\" {\n\t\ts.policy.KeyspaceChanged(KeyspaceUpdateEvent{Keyspace: s.cfg.Keyspace})\n\t}\n\n\ts.sessionStateMu.Lock()\n\ts.isInitialized = true\n\ts.sessionStateMu.Unlock()\n\n\treturn nil\n}\n\n// AwaitSchemaAgreement will wait until schema versions across all nodes in the\n// cluster are the same (as seen from the point of view of the control connection).\n// The maximum amount of time this takes is governed\n// by the MaxWaitSchemaAgreement setting in the configuration (default: 60s).\n// AwaitSchemaAgreement returns an error in case schema versions are not the same\n// after the timeout specified in MaxWaitSchemaAgreement elapses.\nfunc (s *Session) AwaitSchemaAgreement(ctx context.Context) error {\n\tif s.cfg.disableControlConn {\n\t\treturn errNoControl\n\t}\n\treturn s.control.withConn(func(conn *Conn) *Iter {\n\t\treturn &Iter{err: conn.awaitSchemaAgreement(ctx)}\n\t}).err\n}\n\nfunc (s *Session) reconnectDownedHosts(intv time.Duration) {\n\treconnectTicker := time.NewTicker(intv)\n\tdefer reconnectTicker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-reconnectTicker.C:\n\t\t\thosts := s.ring.allHosts()\n\n\t\t\t// Print session.ring for debug.\n\t\t\tif gocqlDebug {\n\t\t\t\tbuf := bytes.NewBufferString(\"Session.ring:\")\n\t\t\t\tfor _, h := range hosts {\n\t\t\t\t\tbuf.WriteString(\"[\" + h.ConnectAddress().String() + \":\" + h.State().String() + \"]\")\n\t\t\t\t}\n\t\t\t\ts.logger.Println(buf.String())\n\t\t\t}\n\n\t\t\tfor _, h := range hosts {\n\t\t\t\tif h.IsUp() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// we let the pool call handleNodeConnected to change the host state\n\t\t\t\ts.pool.addHost(h)\n\t\t\t}\n\t\tcase <-s.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// SetConsistency sets the default consistency level for this session. This\n// setting can also be changed on a per-query basis and the default value\n// is Quorum.\nfunc (s *Session) SetConsistency(cons Consistency) {\n\ts.mu.Lock()\n\ts.cons = cons\n\ts.mu.Unlock()\n}\n\n// SetPageSize sets the default page size for this session. A value <= 0 will\n// disable paging. This setting can also be changed on a per-query basis.\nfunc (s *Session) SetPageSize(n int) {\n\ts.mu.Lock()\n\ts.pageSize = n\n\ts.mu.Unlock()\n}\n\n// SetPrefetch sets the default threshold for pre-fetching new pages. If\n// there are only p*pageSize rows remaining, the next page will be requested\n// automatically. This value can also be changed on a per-query basis and\n// the default value is 0.25.\nfunc (s *Session) SetPrefetch(p float64) {\n\ts.mu.Lock()\n\ts.prefetch = p\n\ts.mu.Unlock()\n}\n\n// SetTrace sets the default tracer for this session. This setting can also\n// be changed on a per-query basis.\nfunc (s *Session) SetTrace(trace Tracer) {\n\ts.mu.Lock()\n\ts.trace = trace\n\ts.mu.Unlock()\n}\n\n// Query generates a new query object for interacting with the database.\n// Further details of the query may be tweaked using the resulting query\n// value before the query is executed. Query is automatically prepared\n// if it has not previously been executed.\nfunc (s *Session) Query(stmt string, values ...interface{}) *Query {\n\tqry := queryPool.Get().(*Query)\n\tqry.session = s\n\tqry.stmt = stmt\n\tqry.values = values\n\tqry.defaultsFromSession()\n\treturn qry\n}\n\ntype QueryInfo struct {\n\tId          []byte\n\tArgs        []ColumnInfo\n\tRval        []ColumnInfo\n\tPKeyColumns []int\n}\n\n// Bind generates a new query object based on the query statement passed in.\n// The query is automatically prepared if it has not previously been executed.\n// The binding callback allows the application to define which query argument\n// values will be marshalled as part of the query execution.\n// During execution, the meta data of the prepared query will be routed to the\n// binding callback, which is responsible for producing the query argument values.\nfunc (s *Session) Bind(stmt string, b func(q *QueryInfo) ([]interface{}, error)) *Query {\n\tqry := queryPool.Get().(*Query)\n\tqry.session = s\n\tqry.stmt = stmt\n\tqry.binding = b\n\tqry.defaultsFromSession()\n\treturn qry\n}\n\n// Close closes all connections. The session is unusable after this\n// operation.\nfunc (s *Session) Close() {\n\n\ts.sessionStateMu.Lock()\n\tif s.isClosing {\n\t\ts.sessionStateMu.Unlock()\n\t\treturn\n\t}\n\ts.isClosing = true\n\ts.sessionStateMu.Unlock()\n\n\tif s.pool != nil {\n\t\ts.pool.Close()\n\t}\n\n\tif s.control != nil {\n\t\ts.control.close()\n\t}\n\n\tif s.nodeEvents != nil {\n\t\ts.nodeEvents.stop()\n\t}\n\n\tif s.schemaEvents != nil {\n\t\ts.schemaEvents.stop()\n\t}\n\n\tif s.ringRefresher != nil {\n\t\ts.ringRefresher.stop()\n\t}\n\n\tif s.cancel != nil {\n\t\ts.cancel()\n\t}\n\n\ts.sessionStateMu.Lock()\n\ts.isClosed = true\n\ts.sessionStateMu.Unlock()\n}\n\nfunc (s *Session) Closed() bool {\n\ts.sessionStateMu.RLock()\n\tclosed := s.isClosed\n\ts.sessionStateMu.RUnlock()\n\treturn closed\n}\n\nfunc (s *Session) initialized() bool {\n\ts.sessionStateMu.RLock()\n\tinitialized := s.isInitialized\n\ts.sessionStateMu.RUnlock()\n\treturn initialized\n}\n\nfunc (s *Session) executeQuery(qry *Query) (it *Iter) {\n\t// fail fast\n\tif s.Closed() {\n\t\treturn &Iter{err: ErrSessionClosed}\n\t}\n\n\titer, err := s.executor.executeQuery(qry)\n\tif err != nil {\n\t\treturn &Iter{err: err}\n\t}\n\tif iter == nil {\n\t\tpanic(\"nil iter\")\n\t}\n\n\treturn iter\n}\n\nfunc (s *Session) removeHost(h *HostInfo) {\n\ts.policy.RemoveHost(h)\n\thostID := h.HostID()\n\ts.pool.removeHost(hostID)\n\ts.ring.removeHost(hostID)\n}\n\n// KeyspaceMetadata returns the schema metadata for the keyspace specified. Returns an error if the keyspace does not exist.\nfunc (s *Session) KeyspaceMetadata(keyspace string) (*KeyspaceMetadata, error) {\n\t// fail fast\n\tif s.Closed() {\n\t\treturn nil, ErrSessionClosed\n\t} else if keyspace == \"\" {\n\t\treturn nil, ErrNoKeyspace\n\t}\n\n\treturn s.schemaDescriber.getSchema(keyspace)\n}\n\nfunc (s *Session) getConn() *Conn {\n\thosts := s.ring.allHosts()\n\tfor _, host := range hosts {\n\t\tif !host.IsUp() {\n\t\t\tcontinue\n\t\t}\n\n\t\tpool, ok := s.pool.getPool(host)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t} else if conn := pool.Pick(); conn != nil {\n\t\t\treturn conn\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// returns routing key indexes and type info\nfunc (s *Session) routingKeyInfo(ctx context.Context, stmt string) (*routingKeyInfo, error) {\n\ts.routingKeyInfoCache.mu.Lock()\n\n\tentry, cached := s.routingKeyInfoCache.lru.Get(stmt)\n\tif cached {\n\t\t// done accessing the cache\n\t\ts.routingKeyInfoCache.mu.Unlock()\n\t\t// the entry is an inflight struct similar to that used by\n\t\t// Conn to prepare statements\n\t\tinflight := entry.(*inflightCachedEntry)\n\n\t\t// wait for any inflight work\n\t\tinflight.wg.Wait()\n\n\t\tif inflight.err != nil {\n\t\t\treturn nil, inflight.err\n\t\t}\n\n\t\tkey, _ := inflight.value.(*routingKeyInfo)\n\n\t\treturn key, nil\n\t}\n\n\t// create a new inflight entry while the data is created\n\tinflight := new(inflightCachedEntry)\n\tinflight.wg.Add(1)\n\tdefer inflight.wg.Done()\n\ts.routingKeyInfoCache.lru.Add(stmt, inflight)\n\ts.routingKeyInfoCache.mu.Unlock()\n\n\tvar (\n\t\tinfo         *preparedStatment\n\t\tpartitionKey []*ColumnMetadata\n\t)\n\n\tconn := s.getConn()\n\tif conn == nil {\n\t\t// TODO: better error?\n\t\tinflight.err = errors.New(\"gocql: unable to fetch prepared info: no connection available\")\n\t\treturn nil, inflight.err\n\t}\n\n\t// get the query info for the statement\n\tinfo, inflight.err = conn.prepareStatement(ctx, stmt, nil)\n\tif inflight.err != nil {\n\t\t// don't cache this error\n\t\ts.routingKeyInfoCache.Remove(stmt)\n\t\treturn nil, inflight.err\n\t}\n\n\t// TODO: it would be nice to mark hosts here but as we are not using the policies\n\t// to fetch hosts we cant\n\n\tif info.request.colCount == 0 {\n\t\t// no arguments, no routing key, and no error\n\t\treturn nil, nil\n\t}\n\n\ttable := info.request.table\n\tkeyspace := info.request.keyspace\n\n\tif len(info.request.pkeyColumns) > 0 {\n\t\t// proto v4 dont need to calculate primary key columns\n\t\ttypes := make([]TypeInfo, len(info.request.pkeyColumns))\n\t\tfor i, col := range info.request.pkeyColumns {\n\t\t\ttypes[i] = info.request.columns[col].TypeInfo\n\t\t}\n\n\t\troutingKeyInfo := &routingKeyInfo{\n\t\t\tindexes:  info.request.pkeyColumns,\n\t\t\ttypes:    types,\n\t\t\tkeyspace: keyspace,\n\t\t\ttable:    table,\n\t\t}\n\n\t\tinflight.value = routingKeyInfo\n\t\treturn routingKeyInfo, nil\n\t}\n\n\tvar keyspaceMetadata *KeyspaceMetadata\n\tkeyspaceMetadata, inflight.err = s.KeyspaceMetadata(info.request.columns[0].Keyspace)\n\tif inflight.err != nil {\n\t\t// don't cache this error\n\t\ts.routingKeyInfoCache.Remove(stmt)\n\t\treturn nil, inflight.err\n\t}\n\n\ttableMetadata, found := keyspaceMetadata.Tables[table]\n\tif !found {\n\t\t// unlikely that the statement could be prepared and the metadata for\n\t\t// the table couldn't be found, but this may indicate either a bug\n\t\t// in the metadata code, or that the table was just dropped.\n\t\tinflight.err = ErrNoMetadata\n\t\t// don't cache this error\n\t\ts.routingKeyInfoCache.Remove(stmt)\n\t\treturn nil, inflight.err\n\t}\n\n\tpartitionKey = tableMetadata.PartitionKey\n\n\tsize := len(partitionKey)\n\troutingKeyInfo := &routingKeyInfo{\n\t\tindexes:  make([]int, size),\n\t\ttypes:    make([]TypeInfo, size),\n\t\tkeyspace: keyspace,\n\t\ttable:    table,\n\t}\n\n\tfor keyIndex, keyColumn := range partitionKey {\n\t\t// set an indicator for checking if the mapping is missing\n\t\troutingKeyInfo.indexes[keyIndex] = -1\n\n\t\t// find the column in the query info\n\t\tfor argIndex, boundColumn := range info.request.columns {\n\t\t\tif keyColumn.Name == boundColumn.Name {\n\t\t\t\t// there may be many such bound columns, pick the first\n\t\t\t\troutingKeyInfo.indexes[keyIndex] = argIndex\n\t\t\t\troutingKeyInfo.types[keyIndex] = boundColumn.TypeInfo\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif routingKeyInfo.indexes[keyIndex] == -1 {\n\t\t\t// missing a routing key column mapping\n\t\t\t// no routing key, and no error\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\n\t// cache this result\n\tinflight.value = routingKeyInfo\n\n\treturn routingKeyInfo, nil\n}\n\nfunc (b *Batch) execute(ctx context.Context, conn *Conn) *Iter {\n\treturn conn.executeBatch(ctx, b)\n}\n\n// Exec executes a batch operation and returns nil if successful\n// otherwise an error is returned describing the failure.\nfunc (b *Batch) Exec() error {\n\titer := b.session.executeBatch(b)\n\treturn iter.Close()\n}\n\nfunc (s *Session) executeBatch(batch *Batch) *Iter {\n\t// fail fast\n\tif s.Closed() {\n\t\treturn &Iter{err: ErrSessionClosed}\n\t}\n\n\t// Prevent the execution of the batch if greater than the limit\n\t// Currently batches have a limit of 65536 queries.\n\t// https://datastax-oss.atlassian.net/browse/JAVA-229\n\tif batch.Size() > BatchSizeMaximum {\n\t\treturn &Iter{err: ErrTooManyStmts}\n\t}\n\n\titer, err := s.executor.executeQuery(batch)\n\tif err != nil {\n\t\treturn &Iter{err: err}\n\t}\n\n\treturn iter\n}\n\n// ExecuteBatch executes a batch operation and returns nil if successful\n// otherwise an error is returned describing the failure.\nfunc (s *Session) ExecuteBatch(batch *Batch) error {\n\titer := s.executeBatch(batch)\n\treturn iter.Close()\n}\n\n// ExecuteBatchCAS executes a batch operation and returns true if successful and\n// an iterator (to scan additional rows if more than one conditional statement)\n// was sent.\n// Further scans on the interator must also remember to include\n// the applied boolean as the first argument to *Iter.Scan\nfunc (s *Session) ExecuteBatchCAS(batch *Batch, dest ...interface{}) (applied bool, iter *Iter, err error) {\n\titer = s.executeBatch(batch)\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\titer.Close()\n\t\treturn false, nil, err\n\t}\n\n\tif len(iter.Columns()) > 1 {\n\t\tdest = append([]interface{}{&applied}, dest...)\n\t\titer.Scan(dest...)\n\t} else {\n\t\titer.Scan(&applied)\n\t}\n\n\treturn applied, iter, nil\n}\n\n// MapExecuteBatchCAS executes a batch operation much like ExecuteBatchCAS,\n// however it accepts a map rather than a list of arguments for the initial\n// scan.\nfunc (s *Session) MapExecuteBatchCAS(batch *Batch, dest map[string]interface{}) (applied bool, iter *Iter, err error) {\n\titer = s.executeBatch(batch)\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\titer.Close()\n\t\treturn false, nil, err\n\t}\n\titer.MapScan(dest)\n\tapplied = dest[\"[applied]\"].(bool)\n\tdelete(dest, \"[applied]\")\n\n\t// we usually close here, but instead of closing, just returin an error\n\t// if MapScan failed. Although Close just returns err, using Close\n\t// here might be confusing as we are not actually closing the iter\n\treturn applied, iter, iter.err\n}\n\ntype hostMetrics struct {\n\t// Attempts is count of how many times this query has been attempted for this host.\n\t// An attempt is either a retry or fetching next page of results.\n\tAttempts int\n\n\t// TotalLatency is the sum of attempt latencies for this host in nanoseconds.\n\tTotalLatency int64\n}\n\ntype queryMetrics struct {\n\tl sync.RWMutex\n\tm map[string]*hostMetrics\n\t// totalAttempts is total number of attempts.\n\t// Equal to sum of all hostMetrics' Attempts.\n\ttotalAttempts int\n}\n\n// preFilledQueryMetrics initializes new queryMetrics based on per-host supplied data.\nfunc preFilledQueryMetrics(m map[string]*hostMetrics) *queryMetrics {\n\tqm := &queryMetrics{m: m}\n\tfor _, hm := range qm.m {\n\t\tqm.totalAttempts += hm.Attempts\n\t}\n\treturn qm\n}\n\n// hostMetrics returns a snapshot of metrics for given host.\n// If the metrics for host don't exist, they are created.\nfunc (qm *queryMetrics) hostMetrics(host *HostInfo) *hostMetrics {\n\tqm.l.Lock()\n\tmetrics := qm.hostMetricsLocked(host)\n\tcopied := new(hostMetrics)\n\t*copied = *metrics\n\tqm.l.Unlock()\n\treturn copied\n}\n\n// hostMetricsLocked gets or creates host metrics for given host.\n// It must be called only while holding qm.l lock.\nfunc (qm *queryMetrics) hostMetricsLocked(host *HostInfo) *hostMetrics {\n\tmetrics, exists := qm.m[host.ConnectAddress().String()]\n\tif !exists {\n\t\t// if the host is not in the map, it means it's been accessed for the first time\n\t\tmetrics = &hostMetrics{}\n\t\tqm.m[host.ConnectAddress().String()] = metrics\n\t}\n\n\treturn metrics\n}\n\n// attempts returns the number of times the query was executed.\nfunc (qm *queryMetrics) attempts() int {\n\tqm.l.Lock()\n\tattempts := qm.totalAttempts\n\tqm.l.Unlock()\n\treturn attempts\n}\n\nfunc (qm *queryMetrics) latency() int64 {\n\tqm.l.Lock()\n\tvar (\n\t\tattempts int\n\t\tlatency  int64\n\t)\n\tfor _, metric := range qm.m {\n\t\tattempts += metric.Attempts\n\t\tlatency += metric.TotalLatency\n\t}\n\tqm.l.Unlock()\n\tif attempts > 0 {\n\t\treturn latency / int64(attempts)\n\t}\n\treturn 0\n}\n\n// attempt adds given number of attempts and latency for given host.\n// It returns previous total attempts.\n// If needsHostMetrics is true, a copy of updated hostMetrics is returned.\nfunc (qm *queryMetrics) attempt(addAttempts int, addLatency time.Duration,\n\thost *HostInfo, needsHostMetrics bool) (int, *hostMetrics) {\n\tqm.l.Lock()\n\n\ttotalAttempts := qm.totalAttempts\n\tqm.totalAttempts += addAttempts\n\n\tupdateHostMetrics := qm.hostMetricsLocked(host)\n\tupdateHostMetrics.Attempts += addAttempts\n\tupdateHostMetrics.TotalLatency += addLatency.Nanoseconds()\n\n\tvar hostMetricsCopy *hostMetrics\n\tif needsHostMetrics {\n\t\thostMetricsCopy = new(hostMetrics)\n\t\t*hostMetricsCopy = *updateHostMetrics\n\t}\n\n\tqm.l.Unlock()\n\treturn totalAttempts, hostMetricsCopy\n}\n\n// Query represents a CQL statement that can be executed.\ntype Query struct {\n\tstmt                  string\n\tvalues                []interface{}\n\tcons                  Consistency\n\tpageSize              int\n\troutingKey            []byte\n\tpageState             []byte\n\tprefetch              float64\n\ttrace                 Tracer\n\tobserver              QueryObserver\n\tsession               *Session\n\tconn                  *Conn\n\trt                    RetryPolicy\n\tspec                  SpeculativeExecutionPolicy\n\tbinding               func(q *QueryInfo) ([]interface{}, error)\n\tserialCons            SerialConsistency\n\tdefaultTimestamp      bool\n\tdefaultTimestampValue int64\n\tdisableSkipMetadata   bool\n\tcontext               context.Context\n\tidempotent            bool\n\tcustomPayload         map[string][]byte\n\tmetrics               *queryMetrics\n\trefCount              uint32\n\n\tdisableAutoPage bool\n\n\t// getKeyspace is field so that it can be overriden in tests\n\tgetKeyspace func() string\n\n\t// used by control conn queries to prevent triggering a write to systems\n\t// tables in AWS MCS see\n\tskipPrepare bool\n\n\t// routingInfo is a pointer because Query can be copied and copyable struct can't hold a mutex.\n\troutingInfo *queryRoutingInfo\n}\n\ntype queryRoutingInfo struct {\n\t// mu protects contents of queryRoutingInfo.\n\tmu sync.RWMutex\n\n\tkeyspace string\n\n\ttable string\n}\n\nfunc (q *Query) defaultsFromSession() {\n\ts := q.session\n\n\ts.mu.RLock()\n\tq.cons = s.cons\n\tq.pageSize = s.pageSize\n\tq.trace = s.trace\n\tq.observer = s.queryObserver\n\tq.prefetch = s.prefetch\n\tq.rt = s.cfg.RetryPolicy\n\tq.serialCons = s.cfg.SerialConsistency\n\tq.defaultTimestamp = s.cfg.DefaultTimestamp\n\tq.idempotent = s.cfg.DefaultIdempotence\n\tq.metrics = &queryMetrics{m: make(map[string]*hostMetrics)}\n\n\tq.spec = &NonSpeculativeExecution{}\n\ts.mu.RUnlock()\n}\n\n// Statement returns the statement that was used to generate this query.\nfunc (q Query) Statement() string {\n\treturn q.stmt\n}\n\n// Values returns the values passed in via Bind.\n// This can be used by a wrapper type that needs to access the bound values.\nfunc (q Query) Values() []interface{} {\n\treturn q.values\n}\n\n// String implements the stringer interface.\nfunc (q Query) String() string {\n\treturn fmt.Sprintf(\"[query statement=%q values=%+v consistency=%s]\", q.stmt, q.values, q.cons)\n}\n\n// Attempts returns the number of times the query was executed.\nfunc (q *Query) Attempts() int {\n\treturn q.metrics.attempts()\n}\n\nfunc (q *Query) AddAttempts(i int, host *HostInfo) {\n\tq.metrics.attempt(i, 0, host, false)\n}\n\n// Latency returns the average amount of nanoseconds per attempt of the query.\nfunc (q *Query) Latency() int64 {\n\treturn q.metrics.latency()\n}\n\nfunc (q *Query) AddLatency(l int64, host *HostInfo) {\n\tq.metrics.attempt(0, time.Duration(l)*time.Nanosecond, host, false)\n}\n\n// Consistency sets the consistency level for this query. If no consistency\n// level have been set, the default consistency level of the cluster\n// is used.\nfunc (q *Query) Consistency(c Consistency) *Query {\n\tq.cons = c\n\treturn q\n}\n\n// GetConsistency returns the currently configured consistency level for\n// the query.\nfunc (q *Query) GetConsistency() Consistency {\n\treturn q.cons\n}\n\n// Same as Consistency but without a return value\nfunc (q *Query) SetConsistency(c Consistency) {\n\tq.cons = c\n}\n\n// CustomPayload sets the custom payload level for this query.\nfunc (q *Query) CustomPayload(customPayload map[string][]byte) *Query {\n\tq.customPayload = customPayload\n\treturn q\n}\n\nfunc (q *Query) Context() context.Context {\n\tif q.context == nil {\n\t\treturn context.Background()\n\t}\n\treturn q.context\n}\n\n// Trace enables tracing of this query. Look at the documentation of the\n// Tracer interface to learn more about tracing.\nfunc (q *Query) Trace(trace Tracer) *Query {\n\tq.trace = trace\n\treturn q\n}\n\n// Observer enables query-level observer on this query.\n// The provided observer will be called every time this query is executed.\nfunc (q *Query) Observer(observer QueryObserver) *Query {\n\tq.observer = observer\n\treturn q\n}\n\n// PageSize will tell the iterator to fetch the result in pages of size n.\n// This is useful for iterating over large result sets, but setting the\n// page size too low might decrease the performance. This feature is only\n// available in Cassandra 2 and onwards.\nfunc (q *Query) PageSize(n int) *Query {\n\tq.pageSize = n\n\treturn q\n}\n\n// DefaultTimestamp will enable the with default timestamp flag on the query.\n// If enable, this will replace the server side assigned\n// timestamp as default timestamp. Note that a timestamp in the query itself\n// will still override this timestamp. This is entirely optional.\n//\n// Only available on protocol >= 3\nfunc (q *Query) DefaultTimestamp(enable bool) *Query {\n\tq.defaultTimestamp = enable\n\treturn q\n}\n\n// WithTimestamp will enable the with default timestamp flag on the query\n// like DefaultTimestamp does. But also allows to define value for timestamp.\n// It works the same way as USING TIMESTAMP in the query itself, but\n// should not break prepared query optimization.\n//\n// Only available on protocol >= 3\nfunc (q *Query) WithTimestamp(timestamp int64) *Query {\n\tq.DefaultTimestamp(true)\n\tq.defaultTimestampValue = timestamp\n\treturn q\n}\n\n// RoutingKey sets the routing key to use when a token aware connection\n// pool is used to optimize the routing of this query.\nfunc (q *Query) RoutingKey(routingKey []byte) *Query {\n\tq.routingKey = routingKey\n\treturn q\n}\n\nfunc (q *Query) withContext(ctx context.Context) ExecutableQuery {\n\t// I really wish go had covariant types\n\treturn q.WithContext(ctx)\n}\n\n// WithContext returns a shallow copy of q with its context\n// set to ctx.\n//\n// The provided context controls the entire lifetime of executing a\n// query, queries will be canceled and return once the context is\n// canceled.\nfunc (q *Query) WithContext(ctx context.Context) *Query {\n\tq2 := *q\n\tq2.context = ctx\n\treturn &q2\n}\n\n// Deprecate: does nothing, cancel the context passed to WithContext\nfunc (q *Query) Cancel() {\n\t// TODO: delete\n}\n\nfunc (q *Query) execute(ctx context.Context, conn *Conn) *Iter {\n\treturn conn.executeQuery(ctx, q)\n}\n\nfunc (q *Query) attempt(keyspace string, end, start time.Time, iter *Iter, host *HostInfo) {\n\tlatency := end.Sub(start)\n\tattempt, metricsForHost := q.metrics.attempt(1, latency, host, q.observer != nil)\n\n\tif q.observer != nil {\n\t\tq.observer.ObserveQuery(q.Context(), ObservedQuery{\n\t\t\tKeyspace:  keyspace,\n\t\t\tStatement: q.stmt,\n\t\t\tValues:    q.values,\n\t\t\tStart:     start,\n\t\t\tEnd:       end,\n\t\t\tRows:      iter.numRows,\n\t\t\tHost:      host,\n\t\t\tMetrics:   metricsForHost,\n\t\t\tErr:       iter.err,\n\t\t\tAttempt:   attempt,\n\t\t})\n\t}\n}\n\nfunc (q *Query) retryPolicy() RetryPolicy {\n\treturn q.rt\n}\n\n// Keyspace returns the keyspace the query will be executed against.\nfunc (q *Query) Keyspace() string {\n\tif q.getKeyspace != nil {\n\t\treturn q.getKeyspace()\n\t}\n\tif q.routingInfo.keyspace != \"\" {\n\t\treturn q.routingInfo.keyspace\n\t}\n\n\tif q.session == nil {\n\t\treturn \"\"\n\t}\n\t// TODO(chbannis): this should be parsed from the query or we should let\n\t// this be set by users.\n\treturn q.session.cfg.Keyspace\n}\n\n// Table returns name of the table the query will be executed against.\nfunc (q *Query) Table() string {\n\treturn q.routingInfo.table\n}\n\n// GetRoutingKey gets the routing key to use for routing this query. If\n// a routing key has not been explicitly set, then the routing key will\n// be constructed if possible using the keyspace's schema and the query\n// info for this query statement. If the routing key cannot be determined\n// then nil will be returned with no error. On any error condition,\n// an error description will be returned.\nfunc (q *Query) GetRoutingKey() ([]byte, error) {\n\tif q.routingKey != nil {\n\t\treturn q.routingKey, nil\n\t} else if q.binding != nil && len(q.values) == 0 {\n\t\t// If this query was created using session.Bind we wont have the query\n\t\t// values yet, so we have to pass down to the next policy.\n\t\t// TODO: Remove this and handle this case\n\t\treturn nil, nil\n\t}\n\n\t// try to determine the routing key\n\troutingKeyInfo, err := q.session.routingKeyInfo(q.Context(), q.stmt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif routingKeyInfo != nil {\n\t\tq.routingInfo.mu.Lock()\n\t\tq.routingInfo.keyspace = routingKeyInfo.keyspace\n\t\tq.routingInfo.table = routingKeyInfo.table\n\t\tq.routingInfo.mu.Unlock()\n\t}\n\treturn createRoutingKey(routingKeyInfo, q.values)\n}\n\nfunc (q *Query) shouldPrepare() bool {\n\n\tstmt := strings.TrimLeftFunc(strings.TrimRightFunc(q.stmt, func(r rune) bool {\n\t\treturn unicode.IsSpace(r) || r == ';'\n\t}), unicode.IsSpace)\n\n\tvar stmtType string\n\tif n := strings.IndexFunc(stmt, unicode.IsSpace); n >= 0 {\n\t\tstmtType = strings.ToLower(stmt[:n])\n\t}\n\tif stmtType == \"begin\" {\n\t\tif n := strings.LastIndexFunc(stmt, unicode.IsSpace); n >= 0 {\n\t\t\tstmtType = strings.ToLower(stmt[n+1:])\n\t\t}\n\t}\n\tswitch stmtType {\n\tcase \"select\", \"insert\", \"update\", \"delete\", \"batch\":\n\t\treturn true\n\t}\n\treturn false\n}\n\n// SetPrefetch sets the default threshold for pre-fetching new pages. If\n// there are only p*pageSize rows remaining, the next page will be requested\n// automatically.\nfunc (q *Query) Prefetch(p float64) *Query {\n\tq.prefetch = p\n\treturn q\n}\n\n// RetryPolicy sets the policy to use when retrying the query.\nfunc (q *Query) RetryPolicy(r RetryPolicy) *Query {\n\tq.rt = r\n\treturn q\n}\n\n// SetSpeculativeExecutionPolicy sets the execution policy\nfunc (q *Query) SetSpeculativeExecutionPolicy(sp SpeculativeExecutionPolicy) *Query {\n\tq.spec = sp\n\treturn q\n}\n\n// speculativeExecutionPolicy fetches the policy\nfunc (q *Query) speculativeExecutionPolicy() SpeculativeExecutionPolicy {\n\treturn q.spec\n}\n\n// IsIdempotent returns whether the query is marked as idempotent.\n// Non-idempotent query won't be retried.\n// See \"Retries and speculative execution\" in package docs for more details.\nfunc (q *Query) IsIdempotent() bool {\n\treturn q.idempotent\n}\n\n// Idempotent marks the query as being idempotent or not depending on\n// the value.\n// Non-idempotent query won't be retried.\n// See \"Retries and speculative execution\" in package docs for more details.\nfunc (q *Query) Idempotent(value bool) *Query {\n\tq.idempotent = value\n\treturn q\n}\n\n// Bind sets query arguments of query. This can also be used to rebind new query arguments\n// to an existing query instance.\nfunc (q *Query) Bind(v ...interface{}) *Query {\n\tq.values = v\n\tq.pageState = nil\n\treturn q\n}\n\n// SerialConsistency sets the consistency level for the\n// serial phase of conditional updates. That consistency can only be\n// either SERIAL or LOCAL_SERIAL and if not present, it defaults to\n// SERIAL. This option will be ignored for anything else that a\n// conditional update/insert.\nfunc (q *Query) SerialConsistency(cons SerialConsistency) *Query {\n\tq.serialCons = cons\n\treturn q\n}\n\n// PageState sets the paging state for the query to resume paging from a specific\n// point in time. Setting this will disable to query paging for this query, and\n// must be used for all subsequent pages.\nfunc (q *Query) PageState(state []byte) *Query {\n\tq.pageState = state\n\tq.disableAutoPage = true\n\treturn q\n}\n\n// NoSkipMetadata will override the internal result metadata cache so that the driver does not\n// send skip_metadata for queries, this means that the result will always contain\n// the metadata to parse the rows and will not reuse the metadata from the prepared\n// statement. This should only be used to work around cassandra bugs, such as when using\n// CAS operations which do not end in Cas.\n//\n// See https://issues.apache.org/jira/browse/CASSANDRA-11099\n// https://github.com/apache/cassandra-gocql-driver/issues/612\nfunc (q *Query) NoSkipMetadata() *Query {\n\tq.disableSkipMetadata = true\n\treturn q\n}\n\n// Exec executes the query without returning any rows.\nfunc (q *Query) Exec() error {\n\treturn q.Iter().Close()\n}\n\nfunc isUseStatement(stmt string) bool {\n\tif len(stmt) < 3 {\n\t\treturn false\n\t}\n\n\treturn strings.EqualFold(stmt[0:3], \"use\")\n}\n\n// Iter executes the query and returns an iterator capable of iterating\n// over all results.\nfunc (q *Query) Iter() *Iter {\n\tif isUseStatement(q.stmt) {\n\t\treturn &Iter{err: ErrUseStmt}\n\t}\n\t// if the query was specifically run on a connection then re-use that\n\t// connection when fetching the next results\n\tif q.conn != nil {\n\t\treturn q.conn.executeQuery(q.Context(), q)\n\t}\n\treturn q.session.executeQuery(q)\n}\n\n// MapScan executes the query, copies the columns of the first selected\n// row into the map pointed at by m and discards the rest. If no rows\n// were selected, ErrNotFound is returned.\nfunc (q *Query) MapScan(m map[string]interface{}) error {\n\titer := q.Iter()\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\treturn err\n\t}\n\titer.MapScan(m)\n\treturn iter.Close()\n}\n\n// Scan executes the query, copies the columns of the first selected\n// row into the values pointed at by dest and discards the rest. If no rows\n// were selected, ErrNotFound is returned.\nfunc (q *Query) Scan(dest ...interface{}) error {\n\titer := q.Iter()\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\treturn err\n\t}\n\titer.Scan(dest...)\n\treturn iter.Close()\n}\n\n// ScanCAS executes a lightweight transaction (i.e. an UPDATE or INSERT\n// statement containing an IF clause). If the transaction fails because\n// the existing values did not match, the previous values will be stored\n// in dest.\n//\n// As for INSERT .. IF NOT EXISTS, previous values will be returned as if\n// SELECT * FROM. So using ScanCAS with INSERT is inherently prone to\n// column mismatching. Use MapScanCAS to capture them safely.\nfunc (q *Query) ScanCAS(dest ...interface{}) (applied bool, err error) {\n\tq.disableSkipMetadata = true\n\titer := q.Iter()\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\treturn false, err\n\t}\n\tif len(iter.Columns()) > 1 {\n\t\tdest = append([]interface{}{&applied}, dest...)\n\t\titer.Scan(dest...)\n\t} else {\n\t\titer.Scan(&applied)\n\t}\n\treturn applied, iter.Close()\n}\n\n// MapScanCAS executes a lightweight transaction (i.e. an UPDATE or INSERT\n// statement containing an IF clause). If the transaction fails because\n// the existing values did not match, the previous values will be stored\n// in dest map.\n//\n// As for INSERT .. IF NOT EXISTS, previous values will be returned as if\n// SELECT * FROM. So using ScanCAS with INSERT is inherently prone to\n// column mismatching. MapScanCAS is added to capture them safely.\nfunc (q *Query) MapScanCAS(dest map[string]interface{}) (applied bool, err error) {\n\tq.disableSkipMetadata = true\n\titer := q.Iter()\n\tif err := iter.checkErrAndNotFound(); err != nil {\n\t\treturn false, err\n\t}\n\titer.MapScan(dest)\n\tapplied = dest[\"[applied]\"].(bool)\n\tdelete(dest, \"[applied]\")\n\n\treturn applied, iter.Close()\n}\n\n// Release releases a query back into a pool of queries. Released Queries\n// cannot be reused.\n//\n// Example:\n//\n//\tqry := session.Query(\"SELECT * FROM my_table\")\n//\tqry.Exec()\n//\tqry.Release()\nfunc (q *Query) Release() {\n\tq.decRefCount()\n}\n\n// reset zeroes out all fields of a query so that it can be safely pooled.\nfunc (q *Query) reset() {\n\t*q = Query{routingInfo: &queryRoutingInfo{}, refCount: 1}\n}\n\nfunc (q *Query) incRefCount() {\n\tatomic.AddUint32(&q.refCount, 1)\n}\n\nfunc (q *Query) decRefCount() {\n\tif res := atomic.AddUint32(&q.refCount, ^uint32(0)); res == 0 {\n\t\t// do release\n\t\tq.reset()\n\t\tqueryPool.Put(q)\n\t}\n}\n\nfunc (q *Query) borrowForExecution() {\n\tq.incRefCount()\n}\n\nfunc (q *Query) releaseAfterExecution() {\n\tq.decRefCount()\n}\n\n// Iter represents an iterator that can be used to iterate over all rows that\n// were returned by a query. The iterator might send additional queries to the\n// database during the iteration if paging was enabled.\ntype Iter struct {\n\terr     error\n\tpos     int\n\tmeta    resultMetadata\n\tnumRows int\n\tnext    *nextIter\n\thost    *HostInfo\n\n\tframer *framer\n\tclosed int32\n}\n\n// Host returns the host which the query was sent to.\nfunc (iter *Iter) Host() *HostInfo {\n\treturn iter.host\n}\n\n// Columns returns the name and type of the selected columns.\nfunc (iter *Iter) Columns() []ColumnInfo {\n\treturn iter.meta.columns\n}\n\ntype Scanner interface {\n\t// Next advances the row pointer to point at the next row, the row is valid until\n\t// the next call of Next. It returns true if there is a row which is available to be\n\t// scanned into with Scan.\n\t// Next must be called before every call to Scan.\n\tNext() bool\n\n\t// Scan copies the current row's columns into dest. If the length of dest does not equal\n\t// the number of columns returned in the row an error is returned. If an error is encountered\n\t// when unmarshalling a column into the value in dest an error is returned and the row is invalidated\n\t// until the next call to Next.\n\t// Next must be called before calling Scan, if it is not an error is returned.\n\tScan(...interface{}) error\n\n\t// Err returns the if there was one during iteration that resulted in iteration being unable to complete.\n\t// Err will also release resources held by the iterator, the Scanner should not used after being called.\n\tErr() error\n}\n\ntype iterScanner struct {\n\titer  *Iter\n\tcols  [][]byte\n\tvalid bool\n}\n\nfunc (is *iterScanner) Next() bool {\n\titer := is.iter\n\tif iter.err != nil {\n\t\treturn false\n\t}\n\n\tif iter.pos >= iter.numRows {\n\t\tif iter.next != nil {\n\t\t\tis.iter = iter.next.fetch()\n\t\t\treturn is.Next()\n\t\t}\n\t\treturn false\n\t}\n\n\tfor i := 0; i < len(is.cols); i++ {\n\t\tcol, err := iter.readColumn()\n\t\tif err != nil {\n\t\t\titer.err = err\n\t\t\treturn false\n\t\t}\n\t\tis.cols[i] = col\n\t}\n\titer.pos++\n\tis.valid = true\n\n\treturn true\n}\n\nfunc scanColumn(p []byte, col ColumnInfo, dest []interface{}) (int, error) {\n\tif dest[0] == nil {\n\t\treturn 1, nil\n\t}\n\n\tif col.TypeInfo.Type() == TypeTuple {\n\t\t// this will panic, actually a bug, please report\n\t\ttuple := col.TypeInfo.(TupleTypeInfo)\n\n\t\tcount := len(tuple.Elems)\n\t\t// here we pass in a slice of the struct which has the number number of\n\t\t// values as elements in the tuple\n\t\tif err := Unmarshal(col.TypeInfo, p, dest[:count]); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\treturn count, nil\n\t} else {\n\t\tif err := Unmarshal(col.TypeInfo, p, dest[0]); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\treturn 1, nil\n\t}\n}\n\nfunc (is *iterScanner) Scan(dest ...interface{}) error {\n\tif !is.valid {\n\t\treturn errors.New(\"gocql: Scan called without calling Next\")\n\t}\n\n\titer := is.iter\n\t// currently only support scanning into an expand tuple, such that its the same\n\t// as scanning in more values from a single column\n\tif len(dest) != iter.meta.actualColCount {\n\t\treturn fmt.Errorf(\"gocql: not enough columns to scan into: have %d want %d\", len(dest), iter.meta.actualColCount)\n\t}\n\n\t// i is the current position in dest, could posible replace it and just use\n\t// slices of dest\n\ti := 0\n\tvar err error\n\tfor _, col := range iter.meta.columns {\n\t\tvar n int\n\t\tn, err = scanColumn(is.cols[i], col, dest[i:])\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\ti += n\n\t}\n\n\tis.valid = false\n\treturn err\n}\n\nfunc (is *iterScanner) Err() error {\n\titer := is.iter\n\tis.iter = nil\n\tis.cols = nil\n\tis.valid = false\n\treturn iter.Close()\n}\n\n// Scanner returns a row Scanner which provides an interface to scan rows in a manner which is\n// similar to database/sql. The iter should NOT be used again after calling this method.\nfunc (iter *Iter) Scanner() Scanner {\n\tif iter == nil {\n\t\treturn nil\n\t}\n\n\treturn &iterScanner{iter: iter, cols: make([][]byte, len(iter.meta.columns))}\n}\n\nfunc (iter *Iter) readColumn() ([]byte, error) {\n\treturn iter.framer.readBytesInternal()\n}\n\n// Scan consumes the next row of the iterator and copies the columns of the\n// current row into the values pointed at by dest. Use nil as a dest value\n// to skip the corresponding column. Scan might send additional queries\n// to the database to retrieve the next set of rows if paging was enabled.\n//\n// Scan returns true if the row was successfully unmarshaled or false if the\n// end of the result set was reached or if an error occurred. Close should\n// be called afterwards to retrieve any potential errors.\nfunc (iter *Iter) Scan(dest ...interface{}) bool {\n\tif iter.err != nil {\n\t\treturn false\n\t}\n\n\tif iter.pos >= iter.numRows {\n\t\tif iter.next != nil {\n\t\t\t*iter = *iter.next.fetch()\n\t\t\treturn iter.Scan(dest...)\n\t\t}\n\t\treturn false\n\t}\n\n\tif iter.next != nil && iter.pos >= iter.next.pos {\n\t\titer.next.fetchAsync()\n\t}\n\n\t// currently only support scanning into an expand tuple, such that its the same\n\t// as scanning in more values from a single column\n\tif len(dest) != iter.meta.actualColCount {\n\t\titer.err = fmt.Errorf(\"gocql: not enough columns to scan into: have %d want %d\", len(dest), iter.meta.actualColCount)\n\t\treturn false\n\t}\n\n\t// i is the current position in dest, could posible replace it and just use\n\t// slices of dest\n\ti := 0\n\tfor _, col := range iter.meta.columns {\n\t\tcolBytes, err := iter.readColumn()\n\t\tif err != nil {\n\t\t\titer.err = err\n\t\t\treturn false\n\t\t}\n\n\t\tn, err := scanColumn(colBytes, col, dest[i:])\n\t\tif err != nil {\n\t\t\titer.err = err\n\t\t\treturn false\n\t\t}\n\t\ti += n\n\t}\n\n\titer.pos++\n\treturn true\n}\n\n// GetCustomPayload returns any parsed custom payload results if given in the\n// response from Cassandra. Note that the result is not a copy.\n//\n// This additional feature of CQL Protocol v4\n// allows additional results and query information to be returned by\n// custom QueryHandlers running in your C* cluster.\n// See https://datastax.github.io/java-driver/manual/custom_payloads/\nfunc (iter *Iter) GetCustomPayload() map[string][]byte {\n\tif iter.framer != nil {\n\t\treturn iter.framer.customPayload\n\t}\n\treturn nil\n}\n\n// Warnings returns any warnings generated if given in the response from Cassandra.\n//\n// This is only available starting with CQL Protocol v4.\nfunc (iter *Iter) Warnings() []string {\n\tif iter.framer != nil {\n\t\treturn iter.framer.header.warnings\n\t}\n\treturn nil\n}\n\n// Close closes the iterator and returns any errors that happened during\n// the query or the iteration.\nfunc (iter *Iter) Close() error {\n\tif atomic.CompareAndSwapInt32(&iter.closed, 0, 1) {\n\t\tif iter.framer != nil {\n\t\t\titer.framer = nil\n\t\t}\n\t}\n\n\treturn iter.err\n}\n\n// WillSwitchPage detects if iterator reached end of current page\n// and the next page is available.\nfunc (iter *Iter) WillSwitchPage() bool {\n\treturn iter.pos >= iter.numRows && iter.next != nil\n}\n\n// checkErrAndNotFound handle error and NotFound in one method.\nfunc (iter *Iter) checkErrAndNotFound() error {\n\tif iter.err != nil {\n\t\treturn iter.err\n\t} else if iter.numRows == 0 {\n\t\treturn ErrNotFound\n\t}\n\treturn nil\n}\n\n// PageState return the current paging state for a query which can be used for\n// subsequent queries to resume paging this point.\nfunc (iter *Iter) PageState() []byte {\n\treturn iter.meta.pagingState\n}\n\n// NumRows returns the number of rows in this pagination, it will update when new\n// pages are fetched, it is not the value of the total number of rows this iter\n// will return unless there is only a single page returned.\nfunc (iter *Iter) NumRows() int {\n\treturn iter.numRows\n}\n\n// nextIter holds state for fetching a single page in an iterator.\n// single page might be attempted multiple times due to retries.\ntype nextIter struct {\n\tqry   *Query\n\tpos   int\n\toncea sync.Once\n\tonce  sync.Once\n\tnext  *Iter\n}\n\nfunc (n *nextIter) fetchAsync() {\n\tn.oncea.Do(func() {\n\t\tgo n.fetch()\n\t})\n}\n\nfunc (n *nextIter) fetch() *Iter {\n\tn.once.Do(func() {\n\t\t// if the query was specifically run on a connection then re-use that\n\t\t// connection when fetching the next results\n\t\tif n.qry.conn != nil {\n\t\t\tn.next = n.qry.conn.executeQuery(n.qry.Context(), n.qry)\n\t\t} else {\n\t\t\tn.next = n.qry.session.executeQuery(n.qry)\n\t\t}\n\t})\n\treturn n.next\n}\n\ntype Batch struct {\n\tType                  BatchType\n\tEntries               []BatchEntry\n\tCons                  Consistency\n\troutingKey            []byte\n\tCustomPayload         map[string][]byte\n\trt                    RetryPolicy\n\tspec                  SpeculativeExecutionPolicy\n\ttrace                 Tracer\n\tobserver              BatchObserver\n\tsession               *Session\n\tserialCons            SerialConsistency\n\tdefaultTimestamp      bool\n\tdefaultTimestampValue int64\n\tcontext               context.Context\n\tcancelBatch           func()\n\tkeyspace              string\n\tmetrics               *queryMetrics\n\n\t// routingInfo is a pointer because Query can be copied and copyable struct can't hold a mutex.\n\troutingInfo *queryRoutingInfo\n}\n\n// NewBatch creates a new batch operation using defaults defined in the cluster\n//\n// Deprecated: use session.Batch instead\nfunc (s *Session) NewBatch(typ BatchType) *Batch {\n\treturn s.Batch(typ)\n}\n\n// Batch creates a new batch operation using defaults defined in the cluster\nfunc (s *Session) Batch(typ BatchType) *Batch {\n\ts.mu.RLock()\n\tbatch := &Batch{\n\t\tType:             typ,\n\t\trt:               s.cfg.RetryPolicy,\n\t\tserialCons:       s.cfg.SerialConsistency,\n\t\ttrace:            s.trace,\n\t\tobserver:         s.batchObserver,\n\t\tsession:          s,\n\t\tCons:             s.cons,\n\t\tdefaultTimestamp: s.cfg.DefaultTimestamp,\n\t\tkeyspace:         s.cfg.Keyspace,\n\t\tmetrics:          &queryMetrics{m: make(map[string]*hostMetrics)},\n\t\tspec:             &NonSpeculativeExecution{},\n\t\troutingInfo:      &queryRoutingInfo{},\n\t}\n\n\ts.mu.RUnlock()\n\treturn batch\n}\n\n// Trace enables tracing of this batch. Look at the documentation of the\n// Tracer interface to learn more about tracing.\nfunc (b *Batch) Trace(trace Tracer) *Batch {\n\tb.trace = trace\n\treturn b\n}\n\n// Observer enables batch-level observer on this batch.\n// The provided observer will be called every time this batched query is executed.\nfunc (b *Batch) Observer(observer BatchObserver) *Batch {\n\tb.observer = observer\n\treturn b\n}\n\nfunc (b *Batch) Keyspace() string {\n\treturn b.keyspace\n}\n\n// Batch has no reasonable eqivalent of Query.Table().\nfunc (b *Batch) Table() string {\n\treturn b.routingInfo.table\n}\n\n// Attempts returns the number of attempts made to execute the batch.\nfunc (b *Batch) Attempts() int {\n\treturn b.metrics.attempts()\n}\n\nfunc (b *Batch) AddAttempts(i int, host *HostInfo) {\n\tb.metrics.attempt(i, 0, host, false)\n}\n\n// Latency returns the average number of nanoseconds to execute a single attempt of the batch.\nfunc (b *Batch) Latency() int64 {\n\treturn b.metrics.latency()\n}\n\nfunc (b *Batch) AddLatency(l int64, host *HostInfo) {\n\tb.metrics.attempt(0, time.Duration(l)*time.Nanosecond, host, false)\n}\n\n// GetConsistency returns the currently configured consistency level for the batch\n// operation.\nfunc (b *Batch) GetConsistency() Consistency {\n\treturn b.Cons\n}\n\n// SetConsistency sets the currently configured consistency level for the batch\n// operation.\nfunc (b *Batch) SetConsistency(c Consistency) {\n\tb.Cons = c\n}\n\nfunc (b *Batch) Context() context.Context {\n\tif b.context == nil {\n\t\treturn context.Background()\n\t}\n\treturn b.context\n}\n\nfunc (b *Batch) IsIdempotent() bool {\n\tfor _, entry := range b.Entries {\n\t\tif !entry.Idempotent {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (b *Batch) speculativeExecutionPolicy() SpeculativeExecutionPolicy {\n\treturn b.spec\n}\n\nfunc (b *Batch) SpeculativeExecutionPolicy(sp SpeculativeExecutionPolicy) *Batch {\n\tb.spec = sp\n\treturn b\n}\n\n// Query adds the query to the batch operation\nfunc (b *Batch) Query(stmt string, args ...interface{}) *Batch {\n\tb.Entries = append(b.Entries, BatchEntry{Stmt: stmt, Args: args})\n\treturn b\n}\n\n// Bind adds the query to the batch operation and correlates it with a binding callback\n// that will be invoked when the batch is executed. The binding callback allows the application\n// to define which query argument values will be marshalled as part of the batch execution.\nfunc (b *Batch) Bind(stmt string, bind func(q *QueryInfo) ([]interface{}, error)) {\n\tb.Entries = append(b.Entries, BatchEntry{Stmt: stmt, binding: bind})\n}\n\nfunc (b *Batch) retryPolicy() RetryPolicy {\n\treturn b.rt\n}\n\n// RetryPolicy sets the retry policy to use when executing the batch operation\nfunc (b *Batch) RetryPolicy(r RetryPolicy) *Batch {\n\tb.rt = r\n\treturn b\n}\n\nfunc (b *Batch) withContext(ctx context.Context) ExecutableQuery {\n\treturn b.WithContext(ctx)\n}\n\n// WithContext returns a shallow copy of b with its context\n// set to ctx.\n//\n// The provided context controls the entire lifetime of executing a\n// query, queries will be canceled and return once the context is\n// canceled.\nfunc (b *Batch) WithContext(ctx context.Context) *Batch {\n\tb2 := *b\n\tb2.context = ctx\n\treturn &b2\n}\n\n// Deprecate: does nothing, cancel the context passed to WithContext\nfunc (*Batch) Cancel() {\n\t// TODO: delete\n}\n\n// Size returns the number of batch statements to be executed by the batch operation.\nfunc (b *Batch) Size() int {\n\treturn len(b.Entries)\n}\n\n// SerialConsistency sets the consistency level for the\n// serial phase of conditional updates. That consistency can only be\n// either SERIAL or LOCAL_SERIAL and if not present, it defaults to\n// SERIAL. This option will be ignored for anything else that a\n// conditional update/insert.\n//\n// Only available for protocol 3 and above\nfunc (b *Batch) SerialConsistency(cons SerialConsistency) *Batch {\n\tb.serialCons = cons\n\treturn b\n}\n\n// DefaultTimestamp will enable the with default timestamp flag on the query.\n// If enable, this will replace the server side assigned\n// timestamp as default timestamp. Note that a timestamp in the query itself\n// will still override this timestamp. This is entirely optional.\n//\n// Only available on protocol >= 3\nfunc (b *Batch) DefaultTimestamp(enable bool) *Batch {\n\tb.defaultTimestamp = enable\n\treturn b\n}\n\n// WithTimestamp will enable the with default timestamp flag on the query\n// like DefaultTimestamp does. But also allows to define value for timestamp.\n// It works the same way as USING TIMESTAMP in the query itself, but\n// should not break prepared query optimization.\n//\n// Only available on protocol >= 3\nfunc (b *Batch) WithTimestamp(timestamp int64) *Batch {\n\tb.DefaultTimestamp(true)\n\tb.defaultTimestampValue = timestamp\n\treturn b\n}\n\nfunc (b *Batch) attempt(keyspace string, end, start time.Time, iter *Iter, host *HostInfo) {\n\tlatency := end.Sub(start)\n\tattempt, metricsForHost := b.metrics.attempt(1, latency, host, b.observer != nil)\n\n\tif b.observer == nil {\n\t\treturn\n\t}\n\n\tstatements := make([]string, len(b.Entries))\n\tvalues := make([][]interface{}, len(b.Entries))\n\n\tfor i, entry := range b.Entries {\n\t\tstatements[i] = entry.Stmt\n\t\tvalues[i] = entry.Args\n\t}\n\n\tb.observer.ObserveBatch(b.Context(), ObservedBatch{\n\t\tKeyspace:   keyspace,\n\t\tStatements: statements,\n\t\tValues:     values,\n\t\tStart:      start,\n\t\tEnd:        end,\n\t\t// Rows not used in batch observations // TODO - might be able to support it when using BatchCAS\n\t\tHost:    host,\n\t\tMetrics: metricsForHost,\n\t\tErr:     iter.err,\n\t\tAttempt: attempt,\n\t})\n}\n\nfunc (b *Batch) GetRoutingKey() ([]byte, error) {\n\tif b.routingKey != nil {\n\t\treturn b.routingKey, nil\n\t}\n\n\tif len(b.Entries) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tentry := b.Entries[0]\n\tif entry.binding != nil {\n\t\t// bindings do not have the values let's skip it like Query does.\n\t\treturn nil, nil\n\t}\n\t// try to determine the routing key\n\troutingKeyInfo, err := b.session.routingKeyInfo(b.Context(), entry.Stmt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn createRoutingKey(routingKeyInfo, entry.Args)\n}\n\nfunc createRoutingKey(routingKeyInfo *routingKeyInfo, values []interface{}) ([]byte, error) {\n\tif routingKeyInfo == nil {\n\t\treturn nil, nil\n\t}\n\n\tif len(routingKeyInfo.indexes) == 1 {\n\t\t// single column routing key\n\t\troutingKey, err := Marshal(\n\t\t\troutingKeyInfo.types[0],\n\t\t\tvalues[routingKeyInfo.indexes[0]],\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn routingKey, nil\n\t}\n\n\t// composite routing key\n\tbuf := bytes.NewBuffer(make([]byte, 0, 256))\n\tfor i := range routingKeyInfo.indexes {\n\t\tencoded, err := Marshal(\n\t\t\troutingKeyInfo.types[i],\n\t\t\tvalues[routingKeyInfo.indexes[i]],\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlenBuf := []byte{0x00, 0x00}\n\t\tbinary.BigEndian.PutUint16(lenBuf, uint16(len(encoded)))\n\t\tbuf.Write(lenBuf)\n\t\tbuf.Write(encoded)\n\t\tbuf.WriteByte(0x00)\n\t}\n\troutingKey := buf.Bytes()\n\treturn routingKey, nil\n}\n\nfunc (b *Batch) borrowForExecution() {\n\t// empty, because Batch has no equivalent of Query.Release()\n\t// that would race with speculative executions.\n}\n\nfunc (b *Batch) releaseAfterExecution() {\n\t// empty, because Batch has no equivalent of Query.Release()\n\t// that would race with speculative executions.\n}\n\ntype BatchType byte\n\nconst (\n\tLoggedBatch   BatchType = 0\n\tUnloggedBatch BatchType = 1\n\tCounterBatch  BatchType = 2\n)\n\ntype BatchEntry struct {\n\tStmt       string\n\tArgs       []interface{}\n\tIdempotent bool\n\tbinding    func(q *QueryInfo) ([]interface{}, error)\n}\n\ntype ColumnInfo struct {\n\tKeyspace string\n\tTable    string\n\tName     string\n\tTypeInfo TypeInfo\n}\n\nfunc (c ColumnInfo) String() string {\n\treturn fmt.Sprintf(\"[column keyspace=%s table=%s name=%s type=%v]\", c.Keyspace, c.Table, c.Name, c.TypeInfo)\n}\n\n// routing key indexes LRU cache\ntype routingKeyInfoLRU struct {\n\tlru *lru.Cache\n\tmu  sync.Mutex\n}\n\ntype routingKeyInfo struct {\n\tindexes  []int\n\ttypes    []TypeInfo\n\tkeyspace string\n\ttable    string\n}\n\nfunc (r *routingKeyInfo) String() string {\n\treturn fmt.Sprintf(\"routing key index=%v types=%v\", r.indexes, r.types)\n}\n\nfunc (r *routingKeyInfoLRU) Remove(key string) {\n\tr.mu.Lock()\n\tr.lru.Remove(key)\n\tr.mu.Unlock()\n}\n\n// Max adjusts the maximum size of the cache and cleans up the oldest records if\n// the new max is lower than the previous value. Not concurrency safe.\nfunc (r *routingKeyInfoLRU) Max(max int) {\n\tr.mu.Lock()\n\tfor r.lru.Len() > max {\n\t\tr.lru.RemoveOldest()\n\t}\n\tr.lru.MaxEntries = max\n\tr.mu.Unlock()\n}\n\ntype inflightCachedEntry struct {\n\twg    sync.WaitGroup\n\terr   error\n\tvalue interface{}\n}\n\n// Tracer is the interface implemented by query tracers. Tracers have the\n// ability to obtain a detailed event log of all events that happened during\n// the execution of a query from Cassandra. Gathering this information might\n// be essential for debugging and optimizing queries, but this feature should\n// not be used on production systems with very high load.\ntype Tracer interface {\n\tTrace(traceId []byte)\n}\n\ntype traceWriter struct {\n\tsession *Session\n\tw       io.Writer\n\tmu      sync.Mutex\n}\n\n// NewTraceWriter returns a simple Tracer implementation that outputs\n// the event log in a textual format.\nfunc NewTraceWriter(session *Session, w io.Writer) Tracer {\n\treturn &traceWriter{session: session, w: w}\n}\n\nfunc (t *traceWriter) Trace(traceId []byte) {\n\tvar (\n\t\tcoordinator string\n\t\tduration    int\n\t)\n\titer := t.session.control.query(`SELECT coordinator, duration\n\t\t\tFROM system_traces.sessions\n\t\t\tWHERE session_id = ?`, traceId)\n\n\titer.Scan(&coordinator, &duration)\n\tif err := iter.Close(); err != nil {\n\t\tt.mu.Lock()\n\t\tfmt.Fprintln(t.w, \"Error:\", err)\n\t\tt.mu.Unlock()\n\t\treturn\n\t}\n\n\tvar (\n\t\ttimestamp time.Time\n\t\tactivity  string\n\t\tsource    string\n\t\telapsed   int\n\t\tthread    string\n\t)\n\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\tfmt.Fprintf(t.w, \"Tracing session %016x (coordinator: %s, duration: %v):\\n\",\n\t\ttraceId, coordinator, time.Duration(duration)*time.Microsecond)\n\n\titer = t.session.control.query(`SELECT event_id, activity, source, source_elapsed, thread\n\t\t\tFROM system_traces.events\n\t\t\tWHERE session_id = ?`, traceId)\n\n\tfor iter.Scan(&timestamp, &activity, &source, &elapsed, &thread) {\n\t\tfmt.Fprintf(t.w, \"%s: %s [%s] (source: %s, elapsed: %d)\\n\",\n\t\t\ttimestamp.Format(\"2006/01/02 15:04:05.999999\"), activity, thread, source, elapsed)\n\t}\n\n\tif err := iter.Close(); err != nil {\n\t\tfmt.Fprintln(t.w, \"Error:\", err)\n\t}\n}\n\ntype ObservedQuery struct {\n\tKeyspace  string\n\tStatement string\n\n\t// Values holds a slice of bound values for the query.\n\t// Do not modify the values here, they are shared with multiple goroutines.\n\tValues []interface{}\n\n\tStart time.Time // time immediately before the query was called\n\tEnd   time.Time // time immediately after the query returned\n\n\t// Rows is the number of rows in the current iter.\n\t// In paginated queries, rows from previous scans are not counted.\n\t// Rows is not used in batch queries and remains at the default value\n\tRows int\n\n\t// Host is the informations about the host that performed the query\n\tHost *HostInfo\n\n\t// The metrics per this host\n\tMetrics *hostMetrics\n\n\t// Err is the error in the query.\n\t// It only tracks network errors or errors of bad cassandra syntax, in particular selects with no match return nil error\n\tErr error\n\n\t// Attempt is the index of attempt at executing this query.\n\t// The first attempt is number zero and any retries have non-zero attempt number.\n\tAttempt int\n}\n\n// QueryObserver is the interface implemented by query observers / stat collectors.\n//\n// Experimental, this interface and use may change\ntype QueryObserver interface {\n\t// ObserveQuery gets called on every query to cassandra, including all queries in an iterator when paging is enabled.\n\t// It doesn't get called if there is no query because the session is closed or there are no connections available.\n\t// The error reported only shows query errors, i.e. if a SELECT is valid but finds no matches it will be nil.\n\tObserveQuery(context.Context, ObservedQuery)\n}\n\ntype ObservedBatch struct {\n\tKeyspace   string\n\tStatements []string\n\n\t// Values holds a slice of bound values for each statement.\n\t// Values[i] are bound values passed to Statements[i].\n\t// Do not modify the values here, they are shared with multiple goroutines.\n\tValues [][]interface{}\n\n\tStart time.Time // time immediately before the batch query was called\n\tEnd   time.Time // time immediately after the batch query returned\n\n\t// Host is the informations about the host that performed the batch\n\tHost *HostInfo\n\n\t// Err is the error in the batch query.\n\t// It only tracks network errors or errors of bad cassandra syntax, in particular selects with no match return nil error\n\tErr error\n\n\t// The metrics per this host\n\tMetrics *hostMetrics\n\n\t// Attempt is the index of attempt at executing this query.\n\t// The first attempt is number zero and any retries have non-zero attempt number.\n\tAttempt int\n}\n\n// BatchObserver is the interface implemented by batch observers / stat collectors.\ntype BatchObserver interface {\n\t// ObserveBatch gets called on every batch query to cassandra.\n\t// It also gets called once for each query in a batch.\n\t// It doesn't get called if there is no query because the session is closed or there are no connections available.\n\t// The error reported only shows query errors, i.e. if a SELECT is valid but finds no matches it will be nil.\n\t// Unlike QueryObserver.ObserveQuery it does no reporting on rows read.\n\tObserveBatch(context.Context, ObservedBatch)\n}\n\ntype ObservedConnect struct {\n\t// Host is the information about the host about to connect\n\tHost *HostInfo\n\n\tStart time.Time // time immediately before the dial is called\n\tEnd   time.Time // time immediately after the dial returned\n\n\t// Err is the connection error (if any)\n\tErr error\n}\n\n// ConnectObserver is the interface implemented by connect observers / stat collectors.\ntype ConnectObserver interface {\n\t// ObserveConnect gets called when a new connection to cassandra is made.\n\tObserveConnect(ObservedConnect)\n}\n\ntype Error struct {\n\tCode    int\n\tMessage string\n}\n\nfunc (e Error) Error() string {\n\treturn e.Message\n}\n\nvar (\n\tErrNotFound             = errors.New(\"not found\")\n\tErrUnavailable          = errors.New(\"unavailable\")\n\tErrUnsupported          = errors.New(\"feature not supported\")\n\tErrTooManyStmts         = errors.New(\"too many statements\")\n\tErrUseStmt              = errors.New(\"use statements aren't supported. Please see https://github.com/apache/cassandra-gocql-driver for explanation.\")\n\tErrSessionClosed        = errors.New(\"session has been closed\")\n\tErrNoConnections        = errors.New(\"gocql: no hosts available in the pool\")\n\tErrNoKeyspace           = errors.New(\"no keyspace provided\")\n\tErrKeyspaceDoesNotExist = errors.New(\"keyspace does not exist\")\n\tErrNoMetadata           = errors.New(\"no metadata available\")\n)\n\ntype ErrProtocol struct{ error }\n\nfunc NewErrProtocol(format string, args ...interface{}) error {\n\treturn ErrProtocol{fmt.Errorf(format, args...)}\n}\n\n// BatchSizeMaximum is the maximum number of statements a batch operation can have.\n// This limit is set by cassandra and could change in the future.\nconst BatchSizeMaximum = 65535\n"
        },
        {
          "name": "session_connect_test.go",
          "type": "blob",
          "size": 2.2001953125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n)\n\ntype OneConnTestServer struct {\n\tErr  error\n\tAddr net.IP\n\tPort int\n\n\tlistener   net.Listener\n\tacceptChan chan struct{}\n\tmu         sync.Mutex\n\tclosed     bool\n}\n\nfunc NewOneConnTestServer() (*OneConnTestServer, error) {\n\tlstn, err := net.Listen(\"tcp4\", \"localhost:0\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\taddr, port := parseAddressPort(lstn.Addr().String())\n\treturn &OneConnTestServer{\n\t\tlistener:   lstn,\n\t\tacceptChan: make(chan struct{}),\n\t\tAddr:       addr,\n\t\tPort:       port,\n\t}, nil\n}\n\nfunc (c *OneConnTestServer) Accepted() chan struct{} {\n\treturn c.acceptChan\n}\n\nfunc (c *OneConnTestServer) Close() {\n\tc.lockedClose()\n}\n\nfunc (c *OneConnTestServer) Serve() {\n\tconn, err := c.listener.Accept()\n\tc.Err = err\n\tif conn != nil {\n\t\tconn.Close()\n\t}\n\tc.lockedClose()\n}\n\nfunc (c *OneConnTestServer) lockedClose() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif !c.closed {\n\t\tclose(c.acceptChan)\n\t\tc.listener.Close()\n\t\tc.closed = true\n\t}\n}\n\nfunc parseAddressPort(hostPort string) (net.IP, int) {\n\thost, portStr, err := net.SplitHostPort(hostPort)\n\tif err != nil {\n\t\treturn net.ParseIP(\"\"), 0\n\t}\n\tport, _ := strconv.Atoi(portStr)\n\treturn net.ParseIP(host), port\n}\n"
        },
        {
          "name": "session_test.go",
          "type": "blob",
          "size": 11.125,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestSessionAPI(t *testing.T) {\n\tcfg := &ClusterConfig{}\n\n\ts := &Session{\n\t\tcfg:    *cfg,\n\t\tcons:   Quorum,\n\t\tpolicy: RoundRobinHostPolicy(),\n\t\tlogger: cfg.logger(),\n\t}\n\n\ts.pool = cfg.PoolConfig.buildPool(s)\n\ts.executor = &queryExecutor{\n\t\tpool:   s.pool,\n\t\tpolicy: s.policy,\n\t}\n\tdefer s.Close()\n\n\ts.SetConsistency(All)\n\tif s.cons != All {\n\t\tt.Fatalf(\"expected consistency 'All', got '%v'\", s.cons)\n\t}\n\n\ts.SetPageSize(100)\n\tif s.pageSize != 100 {\n\t\tt.Fatalf(\"expected pageSize 100, got %v\", s.pageSize)\n\t}\n\n\ts.SetPrefetch(0.75)\n\tif s.prefetch != 0.75 {\n\t\tt.Fatalf(\"expceted prefetch 0.75, got %v\", s.prefetch)\n\t}\n\n\ttrace := &traceWriter{}\n\n\ts.SetTrace(trace)\n\tif s.trace != trace {\n\t\tt.Fatalf(\"expected traceWriter '%v',got '%v'\", trace, s.trace)\n\t}\n\n\tqry := s.Query(\"test\", 1)\n\tif v, ok := qry.values[0].(int); !ok {\n\t\tt.Fatalf(\"expected qry.values[0] to be an int, got %v\", qry.values[0])\n\t} else if v != 1 {\n\t\tt.Fatalf(\"expceted qry.values[0] to be 1, got %v\", v)\n\t} else if qry.stmt != \"test\" {\n\t\tt.Fatalf(\"expected qry.stmt to be 'test', got '%v'\", qry.stmt)\n\t}\n\n\tboundQry := s.Bind(\"test\", func(q *QueryInfo) ([]interface{}, error) {\n\t\treturn nil, nil\n\t})\n\tif boundQry.binding == nil {\n\t\tt.Fatal(\"expected qry.binding to be defined, got nil\")\n\t} else if boundQry.stmt != \"test\" {\n\t\tt.Fatalf(\"expected qry.stmt to be 'test', got '%v'\", boundQry.stmt)\n\t}\n\n\titr := s.executeQuery(qry)\n\tif itr.err != ErrNoConnections {\n\t\tt.Fatalf(\"expected itr.err to be '%v', got '%v'\", ErrNoConnections, itr.err)\n\t}\n\n\ttestBatch := s.Batch(LoggedBatch)\n\ttestBatch.Query(\"test\")\n\terr := s.ExecuteBatch(testBatch)\n\n\tif err != ErrNoConnections {\n\t\tt.Fatalf(\"expected session.ExecuteBatch to return '%v', got '%v'\", ErrNoConnections, err)\n\t}\n\n\ts.Close()\n\tif !s.Closed() {\n\t\tt.Fatal(\"expected s.Closed() to be true, got false\")\n\t}\n\t//Should just return cleanly\n\ts.Close()\n\n\terr = s.ExecuteBatch(testBatch)\n\tif err != ErrSessionClosed {\n\t\tt.Fatalf(\"expected session.ExecuteBatch to return '%v', got '%v'\", ErrSessionClosed, err)\n\t}\n}\n\ntype funcQueryObserver func(context.Context, ObservedQuery)\n\nfunc (f funcQueryObserver) ObserveQuery(ctx context.Context, o ObservedQuery) {\n\tf(ctx, o)\n}\n\nfunc TestQueryBasicAPI(t *testing.T) {\n\tqry := &Query{routingInfo: &queryRoutingInfo{}}\n\n\t// Initiate host\n\tip := \"127.0.0.1\"\n\n\tqry.metrics = preFilledQueryMetrics(map[string]*hostMetrics{ip: {Attempts: 0, TotalLatency: 0}})\n\tif qry.Latency() != 0 {\n\t\tt.Fatalf(\"expected Query.Latency() to return 0, got %v\", qry.Latency())\n\t}\n\n\tqry.metrics = preFilledQueryMetrics(map[string]*hostMetrics{ip: {Attempts: 2, TotalLatency: 4}})\n\tif qry.Attempts() != 2 {\n\t\tt.Fatalf(\"expected Query.Attempts() to return 2, got %v\", qry.Attempts())\n\t}\n\tif qry.Latency() != 2 {\n\t\tt.Fatalf(\"expected Query.Latency() to return 2, got %v\", qry.Latency())\n\t}\n\n\tqry.AddAttempts(2, &HostInfo{hostname: ip, connectAddress: net.ParseIP(ip), port: 9042})\n\tif qry.Attempts() != 4 {\n\t\tt.Fatalf(\"expected Query.Attempts() to return 4, got %v\", qry.Attempts())\n\t}\n\n\tqry.Consistency(All)\n\tif qry.GetConsistency() != All {\n\t\tt.Fatalf(\"expected Query.GetConsistency to return 'All', got '%s'\", qry.GetConsistency())\n\t}\n\n\ttrace := &traceWriter{}\n\tqry.Trace(trace)\n\tif qry.trace != trace {\n\t\tt.Fatalf(\"expected Query.Trace to be '%v', got '%v'\", trace, qry.trace)\n\t}\n\n\tobserver := funcQueryObserver(func(context.Context, ObservedQuery) {})\n\tqry.Observer(observer)\n\tif qry.observer == nil { // can't compare func to func, checking not nil instead\n\t\tt.Fatal(\"expected Query.QueryObserver to be set, got nil\")\n\t}\n\n\tqry.PageSize(10)\n\tif qry.pageSize != 10 {\n\t\tt.Fatalf(\"expected Query.PageSize to be 10, got %v\", qry.pageSize)\n\t}\n\n\tqry.Prefetch(0.75)\n\tif qry.prefetch != 0.75 {\n\t\tt.Fatalf(\"expected Query.Prefetch to be 0.75, got %v\", qry.prefetch)\n\t}\n\n\trt := &SimpleRetryPolicy{NumRetries: 3}\n\tif qry.RetryPolicy(rt); qry.rt != rt {\n\t\tt.Fatalf(\"expected Query.RetryPolicy to be '%v', got '%v'\", rt, qry.rt)\n\t}\n\n\tqry.Bind(qry)\n\tif qry.values[0] != qry {\n\t\tt.Fatalf(\"expected Query.Values[0] to be '%v', got '%v'\", qry, qry.values[0])\n\t}\n}\n\nfunc TestQueryShouldPrepare(t *testing.T) {\n\ttoPrepare := []string{\"select * \", \"INSERT INTO\", \"update table\", \"delete from\", \"begin batch\"}\n\tcantPrepare := []string{\"create table\", \"USE table\", \"LIST keyspaces\", \"alter table\", \"drop table\", \"grant user\", \"revoke user\"}\n\tq := &Query{routingInfo: &queryRoutingInfo{}}\n\n\tfor i := 0; i < len(toPrepare); i++ {\n\t\tq.stmt = toPrepare[i]\n\t\tif !q.shouldPrepare() {\n\t\t\tt.Fatalf(\"expected Query.shouldPrepare to return true, got false for statement '%v'\", toPrepare[i])\n\t\t}\n\t}\n\n\tfor i := 0; i < len(cantPrepare); i++ {\n\t\tq.stmt = cantPrepare[i]\n\t\tif q.shouldPrepare() {\n\t\t\tt.Fatalf(\"expected Query.shouldPrepare to return false, got true for statement '%v'\", cantPrepare[i])\n\t\t}\n\t}\n}\n\nfunc TestBatchBasicAPI(t *testing.T) {\n\n\tcfg := &ClusterConfig{RetryPolicy: &SimpleRetryPolicy{NumRetries: 2}}\n\n\ts := &Session{\n\t\tcfg:    *cfg,\n\t\tcons:   Quorum,\n\t\tlogger: cfg.logger(),\n\t}\n\tdefer s.Close()\n\n\ts.pool = cfg.PoolConfig.buildPool(s)\n\n\t// Test UnloggedBatch\n\tb := s.Batch(UnloggedBatch)\n\tif b.Type != UnloggedBatch {\n\t\tt.Fatalf(\"expceted batch.Type to be '%v', got '%v'\", UnloggedBatch, b.Type)\n\t} else if b.rt != cfg.RetryPolicy {\n\t\tt.Fatalf(\"expceted batch.RetryPolicy to be '%v', got '%v'\", cfg.RetryPolicy, b.rt)\n\t}\n\n\t// Test LoggedBatch\n\tb = s.Batch(LoggedBatch)\n\tif b.Type != LoggedBatch {\n\t\tt.Fatalf(\"expected batch.Type to be '%v', got '%v'\", LoggedBatch, b.Type)\n\t}\n\n\tip := \"127.0.0.1\"\n\n\t// Test attempts\n\tb.metrics = preFilledQueryMetrics(map[string]*hostMetrics{ip: {Attempts: 1}})\n\tif b.Attempts() != 1 {\n\t\tt.Fatalf(\"expected batch.Attempts() to return %v, got %v\", 1, b.Attempts())\n\t}\n\n\tb.AddAttempts(2, &HostInfo{hostname: ip, connectAddress: net.ParseIP(ip), port: 9042})\n\tif b.Attempts() != 3 {\n\t\tt.Fatalf(\"expected batch.Attempts() to return %v, got %v\", 3, b.Attempts())\n\t}\n\n\t// Test latency\n\tif b.Latency() != 0 {\n\t\tt.Fatalf(\"expected batch.Latency() to be 0, got %v\", b.Latency())\n\t}\n\n\tb.metrics = preFilledQueryMetrics(map[string]*hostMetrics{ip: {Attempts: 1, TotalLatency: 4}})\n\tif b.Latency() != 4 {\n\t\tt.Fatalf(\"expected batch.Latency() to return %v, got %v\", 4, b.Latency())\n\t}\n\n\t// Test Consistency\n\tb.Cons = One\n\tif b.GetConsistency() != One {\n\t\tt.Fatalf(\"expected batch.GetConsistency() to return 'One', got '%s'\", b.GetConsistency())\n\t}\n\n\ttrace := &traceWriter{}\n\tb.Trace(trace)\n\tif b.trace != trace {\n\t\tt.Fatalf(\"expected batch.Trace to be '%v', got '%v'\", trace, b.trace)\n\t}\n\n\t// Test batch.Query()\n\tb.Query(\"test\", 1)\n\tif b.Entries[0].Stmt != \"test\" {\n\t\tt.Fatalf(\"expected batch.Entries[0].Statement to be 'test', got '%v'\", b.Entries[0].Stmt)\n\t} else if b.Entries[0].Args[0].(int) != 1 {\n\t\tt.Fatalf(\"expected batch.Entries[0].Args[0] to be 1, got %v\", b.Entries[0].Args[0])\n\t}\n\n\tb.Bind(\"test2\", func(q *QueryInfo) ([]interface{}, error) {\n\t\treturn nil, nil\n\t})\n\n\tif b.Entries[1].Stmt != \"test2\" {\n\t\tt.Fatalf(\"expected batch.Entries[1].Statement to be 'test2', got '%v'\", b.Entries[1].Stmt)\n\t} else if b.Entries[1].binding == nil {\n\t\tt.Fatal(\"expected batch.Entries[1].binding to be defined, got nil\")\n\t}\n\n\t// Test RetryPolicy\n\tr := &SimpleRetryPolicy{NumRetries: 4}\n\n\tb.RetryPolicy(r)\n\tif b.rt != r {\n\t\tt.Fatalf(\"expected batch.RetryPolicy to be '%v', got '%v'\", r, b.rt)\n\t}\n\n\tif b.Size() != 2 {\n\t\tt.Fatalf(\"expected batch.Size() to return 2, got %v\", b.Size())\n\t}\n\n}\n\nfunc TestConsistencyNames(t *testing.T) {\n\tnames := map[fmt.Stringer]string{\n\t\tAny:         \"ANY\",\n\t\tOne:         \"ONE\",\n\t\tTwo:         \"TWO\",\n\t\tThree:       \"THREE\",\n\t\tQuorum:      \"QUORUM\",\n\t\tAll:         \"ALL\",\n\t\tLocalQuorum: \"LOCAL_QUORUM\",\n\t\tEachQuorum:  \"EACH_QUORUM\",\n\t\tSerial:      \"SERIAL\",\n\t\tLocalSerial: \"LOCAL_SERIAL\",\n\t\tLocalOne:    \"LOCAL_ONE\",\n\t}\n\n\tfor k, v := range names {\n\t\tif k.String() != v {\n\t\t\tt.Fatalf(\"expected '%v', got '%v'\", v, k.String())\n\t\t}\n\t}\n}\n\nfunc TestIsUseStatement(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput string\n\t\texp   bool\n\t}{\n\t\t{\"USE foo\", true},\n\t\t{\"USe foo\", true},\n\t\t{\"UsE foo\", true},\n\t\t{\"Use foo\", true},\n\t\t{\"uSE foo\", true},\n\t\t{\"uSe foo\", true},\n\t\t{\"usE foo\", true},\n\t\t{\"use foo\", true},\n\t\t{\"SELECT \", false},\n\t\t{\"UPDATE \", false},\n\t\t{\"INSERT \", false},\n\t\t{\"\", false},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tv := isUseStatement(tc.input)\n\t\tif v != tc.exp {\n\t\t\tt.Fatalf(\"expected %v but got %v for statement %q\", tc.exp, v, tc.input)\n\t\t}\n\t}\n}\n\ntype simpleTestRetryPolycy struct {\n\tRetryType  RetryType\n\tNumRetries int\n}\n\nfunc (p *simpleTestRetryPolycy) Attempt(q RetryableQuery) bool {\n\treturn q.Attempts() <= p.NumRetries\n}\n\nfunc (p *simpleTestRetryPolycy) GetRetryType(error) RetryType {\n\treturn p.RetryType\n}\n\n// TestRetryType_IgnoreRethrow verify that with Ignore/Rethrow retry types:\n// - retries stopped\n// - return error is nil on Ignore\n// - return error is not nil on Rethrow\n// - observed error is not nil\nfunc TestRetryType_IgnoreRethrow(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tvar observedErr error\n\tvar observedAttempts int\n\n\tresetObserved := func() {\n\t\tobservedErr = nil\n\t\tobservedAttempts = 0\n\t}\n\n\tobserver := funcQueryObserver(func(ctx context.Context, o ObservedQuery) {\n\t\tobservedErr = o.Err\n\t\tobservedAttempts++\n\t})\n\n\tfor _, caseParams := range []struct {\n\t\tretries   int\n\t\tretryType RetryType\n\t}{\n\t\t{0, Ignore},  // check that error ignored even on last attempt\n\t\t{1, Ignore},  // check thet ignore stops retries\n\t\t{1, Rethrow}, // check thet rethrow stops retries\n\t} {\n\t\tretryPolicy := &simpleTestRetryPolycy{RetryType: caseParams.retryType, NumRetries: caseParams.retries}\n\n\t\terr := session.Query(\"INSERT INTO gocql_test.invalid_table(value) VALUES(1)\").Idempotent(true).RetryPolicy(retryPolicy).Observer(observer).Exec()\n\n\t\tif err != nil && caseParams.retryType == Ignore {\n\t\t\tt.Fatalf(\"[%v] Expected no error, got: %s\", caseParams.retryType, err)\n\t\t}\n\n\t\tif err == nil && caseParams.retryType == Rethrow {\n\t\t\tt.Fatalf(\"[%v] Expected unconfigured table error, got: nil\", caseParams.retryType)\n\t\t}\n\n\t\tif observedErr == nil {\n\t\t\tt.Fatal(\"Expected unconfigured table error in Obserer, got: nil\")\n\t\t}\n\n\t\tif observedAttempts > 1 {\n\t\t\tt.Fatalf(\"Expected one attempt, got: %d\", observedAttempts)\n\t\t}\n\n\t\tresetObserved()\n\t}\n}\n"
        },
        {
          "name": "session_unit_test.go",
          "type": "blob",
          "size": 1.904296875,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"context\"\n\t\"testing\"\n)\n\nfunc TestAsyncSessionInit(t *testing.T) {\n\t// Build a 3 node cluster to test host metric mapping\n\tvar addresses = []string{\n\t\t\"127.0.0.1\",\n\t\t\"127.0.0.2\",\n\t\t\"127.0.0.3\",\n\t}\n\t// only build 1 of the servers so that we can test not connecting to the last\n\t// one\n\tsrv := NewTestServerWithAddress(addresses[0]+\":0\", t, defaultProto, context.Background())\n\tdefer srv.Stop()\n\n\t// just choose any port\n\tcluster := testCluster(defaultProto, srv.Address, addresses[1]+\":9999\", addresses[2]+\":9999\")\n\tcluster.PoolConfig.HostSelectionPolicy = SingleHostReadyPolicy(RoundRobinHostPolicy())\n\tdb, err := cluster.CreateSession()\n\tif err != nil {\n\t\tt.Fatalf(\"NewCluster: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// make sure the session works\n\tif err := db.Query(\"void\").Exec(); err != nil {\n\t\tt.Fatalf(\"unexpected error from void\")\n\t}\n}\n"
        },
        {
          "name": "stress_test.go",
          "type": "blob",
          "size": 2.5087890625,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"sync/atomic\"\n\n\t\"testing\"\n)\n\nfunc BenchmarkConnStress(b *testing.B) {\n\tconst workers = 16\n\n\tcluster := createCluster()\n\tcluster.NumConns = 1\n\tsession := createSessionFromCluster(cluster, b)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE IF NOT EXISTS conn_stress (id int primary key)\"); err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tvar seed uint64\n\twriter := func(pb *testing.PB) {\n\t\tseed := atomic.AddUint64(&seed, 1)\n\t\tvar i uint64 = 0\n\t\tfor pb.Next() {\n\t\t\tif err := session.Query(\"insert into conn_stress (id) values (?)\", i*seed).Exec(); err != nil {\n\t\t\t\tb.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti++\n\t\t}\n\t}\n\n\tb.SetParallelism(workers)\n\tb.RunParallel(writer)\n}\n\nfunc BenchmarkConnRoutingKey(b *testing.B) {\n\tconst workers = 16\n\n\tcluster := createCluster()\n\tcluster.NumConns = 1\n\tcluster.PoolConfig.HostSelectionPolicy = TokenAwareHostPolicy(RoundRobinHostPolicy())\n\tsession := createSessionFromCluster(cluster, b)\n\tdefer session.Close()\n\n\tif err := createTable(session, \"CREATE TABLE IF NOT EXISTS routing_key_stress (id int primary key)\"); err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tvar seed uint64\n\twriter := func(pb *testing.PB) {\n\t\tseed := atomic.AddUint64(&seed, 1)\n\t\tvar i uint64 = 0\n\t\tquery := session.Query(\"insert into routing_key_stress (id) values (?)\")\n\n\t\tfor pb.Next() {\n\t\t\tif _, err := query.Bind(i * seed).GetRoutingKey(); err != nil {\n\t\t\t\tb.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti++\n\t\t}\n\t}\n\n\tb.SetParallelism(workers)\n\tb.RunParallel(writer)\n}\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "token.go",
          "type": "blob",
          "size": 6.072265625,
          "content": "// Copyright (c) 2015 The gocql Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"crypto/md5\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/gocql/gocql/internal/murmur\"\n)\n\n// a token partitioner\ntype partitioner interface {\n\tName() string\n\tHash([]byte) token\n\tParseString(string) token\n}\n\n// a token\ntype token interface {\n\tfmt.Stringer\n\tLess(token) bool\n}\n\n// murmur3 partitioner and token\ntype murmur3Partitioner struct{}\ntype murmur3Token int64\n\nfunc (p murmur3Partitioner) Name() string {\n\treturn \"Murmur3Partitioner\"\n}\n\nfunc (p murmur3Partitioner) Hash(partitionKey []byte) token {\n\th1 := murmur.Murmur3H1(partitionKey)\n\treturn murmur3Token(h1)\n}\n\n// murmur3 little-endian, 128-bit hash, but returns only h1\nfunc (p murmur3Partitioner) ParseString(str string) token {\n\tval, _ := strconv.ParseInt(str, 10, 64)\n\treturn murmur3Token(val)\n}\n\nfunc (m murmur3Token) String() string {\n\treturn strconv.FormatInt(int64(m), 10)\n}\n\nfunc (m murmur3Token) Less(token token) bool {\n\treturn m < token.(murmur3Token)\n}\n\n// order preserving partitioner and token\ntype orderedPartitioner struct{}\ntype orderedToken string\n\nfunc (p orderedPartitioner) Name() string {\n\treturn \"OrderedPartitioner\"\n}\n\nfunc (p orderedPartitioner) Hash(partitionKey []byte) token {\n\t// the partition key is the token\n\treturn orderedToken(partitionKey)\n}\n\nfunc (p orderedPartitioner) ParseString(str string) token {\n\treturn orderedToken(str)\n}\n\nfunc (o orderedToken) String() string {\n\treturn string(o)\n}\n\nfunc (o orderedToken) Less(token token) bool {\n\treturn o < token.(orderedToken)\n}\n\n// random partitioner and token\ntype randomPartitioner struct{}\ntype randomToken big.Int\n\nfunc (r randomPartitioner) Name() string {\n\treturn \"RandomPartitioner\"\n}\n\n// 2 ** 128\nvar maxHashInt, _ = new(big.Int).SetString(\"340282366920938463463374607431768211456\", 10)\n\nfunc (p randomPartitioner) Hash(partitionKey []byte) token {\n\tsum := md5.Sum(partitionKey)\n\tval := new(big.Int)\n\tval.SetBytes(sum[:])\n\tif sum[0] > 127 {\n\t\tval.Sub(val, maxHashInt)\n\t\tval.Abs(val)\n\t}\n\n\treturn (*randomToken)(val)\n}\n\nfunc (p randomPartitioner) ParseString(str string) token {\n\tval := new(big.Int)\n\tval.SetString(str, 10)\n\treturn (*randomToken)(val)\n}\n\nfunc (r *randomToken) String() string {\n\treturn (*big.Int)(r).String()\n}\n\nfunc (r *randomToken) Less(token token) bool {\n\treturn -1 == (*big.Int)(r).Cmp((*big.Int)(token.(*randomToken)))\n}\n\ntype hostToken struct {\n\ttoken token\n\thost  *HostInfo\n}\n\nfunc (ht hostToken) String() string {\n\treturn fmt.Sprintf(\"{token=%v host=%v}\", ht.token, ht.host.HostID())\n}\n\n// a data structure for organizing the relationship between tokens and hosts\ntype tokenRing struct {\n\tpartitioner partitioner\n\n\t// tokens map token range to primary replica.\n\t// The elements in tokens are sorted by token ascending.\n\t// The range for a given item in tokens starts after preceding range and ends with the token specified in\n\t// token. The end token is part of the range.\n\t// The lowest (i.e. index 0) range wraps around the ring (its preceding range is the one with largest index).\n\ttokens []hostToken\n\n\thosts []*HostInfo\n}\n\nfunc newTokenRing(partitioner string, hosts []*HostInfo) (*tokenRing, error) {\n\ttokenRing := &tokenRing{\n\t\thosts: hosts,\n\t}\n\n\tif strings.HasSuffix(partitioner, \"Murmur3Partitioner\") {\n\t\ttokenRing.partitioner = murmur3Partitioner{}\n\t} else if strings.HasSuffix(partitioner, \"OrderedPartitioner\") {\n\t\ttokenRing.partitioner = orderedPartitioner{}\n\t} else if strings.HasSuffix(partitioner, \"RandomPartitioner\") {\n\t\ttokenRing.partitioner = randomPartitioner{}\n\t} else {\n\t\treturn nil, fmt.Errorf(\"unsupported partitioner '%s'\", partitioner)\n\t}\n\n\tfor _, host := range hosts {\n\t\tfor _, strToken := range host.Tokens() {\n\t\t\ttoken := tokenRing.partitioner.ParseString(strToken)\n\t\t\ttokenRing.tokens = append(tokenRing.tokens, hostToken{token, host})\n\t\t}\n\t}\n\n\tsort.Sort(tokenRing)\n\n\treturn tokenRing, nil\n}\n\nfunc (t *tokenRing) Len() int {\n\treturn len(t.tokens)\n}\n\nfunc (t *tokenRing) Less(i, j int) bool {\n\treturn t.tokens[i].token.Less(t.tokens[j].token)\n}\n\nfunc (t *tokenRing) Swap(i, j int) {\n\tt.tokens[i], t.tokens[j] = t.tokens[j], t.tokens[i]\n}\n\nfunc (t *tokenRing) String() string {\n\tbuf := &bytes.Buffer{}\n\tbuf.WriteString(\"TokenRing(\")\n\tif t.partitioner != nil {\n\t\tbuf.WriteString(t.partitioner.Name())\n\t}\n\tbuf.WriteString(\"){\")\n\tsep := \"\"\n\tfor i, th := range t.tokens {\n\t\tbuf.WriteString(sep)\n\t\tsep = \",\"\n\t\tbuf.WriteString(\"\\n\\t[\")\n\t\tbuf.WriteString(strconv.Itoa(i))\n\t\tbuf.WriteString(\"]\")\n\t\tbuf.WriteString(th.token.String())\n\t\tbuf.WriteString(\":\")\n\t\tbuf.WriteString(th.host.ConnectAddress().String())\n\t}\n\tbuf.WriteString(\"\\n}\")\n\treturn string(buf.Bytes())\n}\n\nfunc (t *tokenRing) GetHostForToken(token token) (host *HostInfo, endToken token) {\n\tif t == nil || len(t.tokens) == 0 {\n\t\treturn nil, nil\n\t}\n\n\t// find the primary replica\n\tp := sort.Search(len(t.tokens), func(i int) bool {\n\t\treturn !t.tokens[i].token.Less(token)\n\t})\n\n\tif p == len(t.tokens) {\n\t\t// wrap around to the first in the ring\n\t\tp = 0\n\t}\n\n\tv := t.tokens[p]\n\treturn v.host, v.token\n}\n"
        },
        {
          "name": "token_test.go",
          "type": "blob",
          "size": 11.2841796875,
          "content": "// Copyright (c) 2015 The gocql Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"net\"\n\t\"sort\"\n\t\"strconv\"\n\t\"testing\"\n)\n\n// Tests of the murmur3Patitioner\nfunc TestMurmur3Partitioner(t *testing.T) {\n\ttoken := murmur3Partitioner{}.ParseString(\"-1053604476080545076\")\n\n\tif \"-1053604476080545076\" != token.String() {\n\t\tt.Errorf(\"Expected '-1053604476080545076' but was '%s'\", token)\n\t}\n\n\t// at least verify that the partitioner\n\t// doesn't return nil\n\tpk, _ := marshalInt(nil, 1)\n\ttoken = murmur3Partitioner{}.Hash(pk)\n\tif token == nil {\n\t\tt.Fatal(\"token was nil\")\n\t}\n}\n\n// Tests of the murmur3Token\nfunc TestMurmur3Token(t *testing.T) {\n\tif murmur3Token(42).Less(murmur3Token(42)) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n\tif !murmur3Token(-42).Less(murmur3Token(42)) {\n\t\tt.Errorf(\"Expected Less to return true, but was false\")\n\t}\n\tif murmur3Token(42).Less(murmur3Token(-42)) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n}\n\n// Tests of the orderedPartitioner\nfunc TestOrderedPartitioner(t *testing.T) {\n\t// at least verify that the partitioner\n\t// doesn't return nil\n\tp := orderedPartitioner{}\n\tpk, _ := marshalInt(nil, 1)\n\ttoken := p.Hash(pk)\n\tif token == nil {\n\t\tt.Fatal(\"token was nil\")\n\t}\n\n\tstr := token.String()\n\tparsedToken := p.ParseString(str)\n\n\tif !bytes.Equal([]byte(token.(orderedToken)), []byte(parsedToken.(orderedToken))) {\n\t\tt.Errorf(\"Failed to convert to and from a string %s expected %x but was %x\",\n\t\t\tstr,\n\t\t\t[]byte(token.(orderedToken)),\n\t\t\t[]byte(parsedToken.(orderedToken)),\n\t\t)\n\t}\n}\n\n// Tests of the orderedToken\nfunc TestOrderedToken(t *testing.T) {\n\tif orderedToken([]byte{0, 0, 4, 2}).Less(orderedToken([]byte{0, 0, 4, 2})) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n\tif !orderedToken([]byte{0, 0, 3}).Less(orderedToken([]byte{0, 0, 4, 2})) {\n\t\tt.Errorf(\"Expected Less to return true, but was false\")\n\t}\n\tif orderedToken([]byte{0, 0, 4, 2}).Less(orderedToken([]byte{0, 0, 3})) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n}\n\n// Tests of the randomPartitioner\nfunc TestRandomPartitioner(t *testing.T) {\n\t// at least verify that the partitioner\n\t// doesn't return nil\n\tp := randomPartitioner{}\n\tpk, _ := marshalInt(nil, 1)\n\ttoken := p.Hash(pk)\n\tif token == nil {\n\t\tt.Fatal(\"token was nil\")\n\t}\n\n\tstr := token.String()\n\tparsedToken := p.ParseString(str)\n\n\tif (*big.Int)(token.(*randomToken)).Cmp((*big.Int)(parsedToken.(*randomToken))) != 0 {\n\t\tt.Errorf(\"Failed to convert to and from a string %s expected %v but was %v\",\n\t\t\tstr,\n\t\t\ttoken,\n\t\t\tparsedToken,\n\t\t)\n\t}\n}\n\nfunc TestRandomPartitionerMatchesReference(t *testing.T) {\n\t// example taken from datastax python driver\n\t//    >>> from cassandra.metadata import MD5Token\n\t//    >>> MD5Token.hash_fn(\"test\")\n\t//    12707736894140473154801792860916528374L\n\tvar p randomPartitioner\n\texpect := \"12707736894140473154801792860916528374\"\n\tactual := p.Hash([]byte(\"test\")).String()\n\tif actual != expect {\n\t\tt.Errorf(\"expected random partitioner to generate tokens in the same way as the reference\"+\n\t\t\t\" python client. Expected %s, but got %s\", expect, actual)\n\t}\n}\n\n// Tests of the randomToken\nfunc TestRandomToken(t *testing.T) {\n\tif ((*randomToken)(big.NewInt(42))).Less((*randomToken)(big.NewInt(42))) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n\tif !((*randomToken)(big.NewInt(41))).Less((*randomToken)(big.NewInt(42))) {\n\t\tt.Errorf(\"Expected Less to return true, but was false\")\n\t}\n\tif ((*randomToken)(big.NewInt(42))).Less((*randomToken)(big.NewInt(41))) {\n\t\tt.Errorf(\"Expected Less to return false, but was true\")\n\t}\n}\n\ntype intToken int\n\nfunc (i intToken) String() string        { return strconv.Itoa(int(i)) }\nfunc (i intToken) Less(token token) bool { return i < token.(intToken) }\n\n// Test of the token ring implementation based on example at the start of this\n// page of documentation:\n// http://www.datastax.com/docs/0.8/cluster_architecture/partitioning\nfunc TestTokenRing_Int(t *testing.T) {\n\thost0 := &HostInfo{}\n\thost25 := &HostInfo{}\n\thost50 := &HostInfo{}\n\thost75 := &HostInfo{}\n\tring := &tokenRing{\n\t\tpartitioner: nil,\n\t\t// these tokens and hosts are out of order to test sorting\n\t\ttokens: []hostToken{\n\t\t\t{intToken(0), host0},\n\t\t\t{intToken(50), host50},\n\t\t\t{intToken(75), host75},\n\t\t\t{intToken(25), host25},\n\t\t},\n\t}\n\n\tsort.Sort(ring)\n\n\tif host, endToken := ring.GetHostForToken(intToken(0)); host != host0 || endToken != intToken(0) {\n\t\tt.Error(\"Expected host 0 for token 0\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(1)); host != host25 || endToken != intToken(25) {\n\t\tt.Error(\"Expected host 25 for token 1\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(24)); host != host25 || endToken != intToken(25) {\n\t\tt.Error(\"Expected host 25 for token 24\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(25)); host != host25 || endToken != intToken(25) {\n\t\tt.Error(\"Expected host 25 for token 25\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(26)); host != host50 || endToken != intToken(50) {\n\t\tt.Error(\"Expected host 50 for token 26\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(49)); host != host50 || endToken != intToken(50) {\n\t\tt.Error(\"Expected host 50 for token 49\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(50)); host != host50 || endToken != intToken(50) {\n\t\tt.Error(\"Expected host 50 for token 50\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(51)); host != host75 || endToken != intToken(75) {\n\t\tt.Error(\"Expected host 75 for token 51\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(74)); host != host75 || endToken != intToken(75) {\n\t\tt.Error(\"Expected host 75 for token 74\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(75)); host != host75 || endToken != intToken(75) {\n\t\tt.Error(\"Expected host 75 for token 75\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(76)); host != host0 || endToken != intToken(0) {\n\t\tt.Error(\"Expected host 0 for token 76\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(99)); host != host0 || endToken != intToken(0) {\n\t\tt.Error(\"Expected host 0 for token 99\")\n\t}\n\tif host, endToken := ring.GetHostForToken(intToken(100)); host != host0 || endToken != intToken(0) {\n\t\tt.Error(\"Expected host 0 for token 100\")\n\t}\n}\n\n// Test for the behavior of a nil pointer to tokenRing\nfunc TestTokenRing_Nil(t *testing.T) {\n\tvar ring *tokenRing = nil\n\n\tif host, endToken := ring.GetHostForToken(nil); host != nil || endToken != nil {\n\t\tt.Error(\"Expected nil for nil token ring\")\n\t}\n}\n\n// Test of the recognition of the partitioner class\nfunc TestTokenRing_UnknownPartition(t *testing.T) {\n\t_, err := newTokenRing(\"UnknownPartitioner\", nil)\n\tif err == nil {\n\t\tt.Error(\"Expected error for unknown partitioner value, but was nil\")\n\t}\n}\n\nfunc hostsForTests(n int) []*HostInfo {\n\thosts := make([]*HostInfo, n)\n\tfor i := 0; i < n; i++ {\n\t\thost := &HostInfo{\n\t\t\tconnectAddress: net.IPv4(1, 1, 1, byte(n)),\n\t\t\ttokens:         []string{fmt.Sprintf(\"%d\", n)},\n\t\t}\n\n\t\thosts[i] = host\n\t}\n\treturn hosts\n}\n\n// Test of the tokenRing with the Murmur3Partitioner\nfunc TestTokenRing_Murmur3(t *testing.T) {\n\t// Note, strings are parsed directly to int64, they are not murmur3 hashed\n\thosts := hostsForTests(4)\n\tring, err := newTokenRing(\"Murmur3Partitioner\", hosts)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create token ring due to error: %v\", err)\n\t}\n\n\tp := murmur3Partitioner{}\n\n\tfor _, host := range hosts {\n\t\tactual, _ := ring.GetHostForToken(p.ParseString(host.tokens[0]))\n\t\tif !actual.ConnectAddress().Equal(host.ConnectAddress()) {\n\t\t\tt.Errorf(\"Expected address %v for token %q, but was %v\", host.ConnectAddress(),\n\t\t\t\thost.tokens[0], actual.ConnectAddress())\n\t\t}\n\t}\n\n\tactual, _ := ring.GetHostForToken(p.ParseString(\"12\"))\n\tif !actual.ConnectAddress().Equal(hosts[1].ConnectAddress()) {\n\t\tt.Errorf(\"Expected address 1 for token \\\"12\\\", but was %s\", actual.ConnectAddress())\n\t}\n\n\tactual, _ = ring.GetHostForToken(p.ParseString(\"24324545443332\"))\n\tif !actual.ConnectAddress().Equal(hosts[0].ConnectAddress()) {\n\t\tt.Errorf(\"Expected address 0 for token \\\"24324545443332\\\", but was %s\", actual.ConnectAddress())\n\t}\n}\n\n// Test of the tokenRing with the OrderedPartitioner\nfunc TestTokenRing_Ordered(t *testing.T) {\n\t// Tokens here more or less are similar layout to the int tokens above due\n\t// to each numeric character translating to a consistently offset byte.\n\thosts := hostsForTests(4)\n\tring, err := newTokenRing(\"OrderedPartitioner\", hosts)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create token ring due to error: %v\", err)\n\t}\n\n\tp := orderedPartitioner{}\n\n\tvar actual *HostInfo\n\tfor _, host := range hosts {\n\t\tactual, _ := ring.GetHostForToken(p.ParseString(host.tokens[0]))\n\t\tif !actual.ConnectAddress().Equal(host.ConnectAddress()) {\n\t\t\tt.Errorf(\"Expected address %v for token %q, but was %v\", host.ConnectAddress(),\n\t\t\t\thost.tokens[0], actual.ConnectAddress())\n\t\t}\n\t}\n\n\tactual, _ = ring.GetHostForToken(p.ParseString(\"12\"))\n\tif !actual.peer.Equal(hosts[1].peer) {\n\t\tt.Errorf(\"Expected address 1 for token \\\"12\\\", but was %s\", actual.ConnectAddress())\n\t}\n\n\tactual, _ = ring.GetHostForToken(p.ParseString(\"24324545443332\"))\n\tif !actual.ConnectAddress().Equal(hosts[1].ConnectAddress()) {\n\t\tt.Errorf(\"Expected address 1 for token \\\"24324545443332\\\", but was %s\", actual.ConnectAddress())\n\t}\n}\n\n// Test of the tokenRing with the RandomPartitioner\nfunc TestTokenRing_Random(t *testing.T) {\n\t// String tokens are parsed into big.Int in base 10\n\thosts := hostsForTests(4)\n\tring, err := newTokenRing(\"RandomPartitioner\", hosts)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create token ring due to error: %v\", err)\n\t}\n\n\tp := randomPartitioner{}\n\n\tvar actual *HostInfo\n\tfor _, host := range hosts {\n\t\tactual, _ := ring.GetHostForToken(p.ParseString(host.tokens[0]))\n\t\tif !actual.ConnectAddress().Equal(host.ConnectAddress()) {\n\t\t\tt.Errorf(\"Expected address %v for token %q, but was %v\", host.ConnectAddress(),\n\t\t\t\thost.tokens[0], actual.ConnectAddress())\n\t\t}\n\t}\n\n\tactual, _ = ring.GetHostForToken(p.ParseString(\"12\"))\n\tif !actual.peer.Equal(hosts[1].peer) {\n\t\tt.Errorf(\"Expected address 1 for token \\\"12\\\", but was %s\", actual.ConnectAddress())\n\t}\n\n\tactual, _ = ring.GetHostForToken(p.ParseString(\"24324545443332\"))\n\tif !actual.ConnectAddress().Equal(hosts[0].ConnectAddress()) {\n\t\tt.Errorf(\"Expected address 1 for token \\\"24324545443332\\\", but was %s\", actual.ConnectAddress())\n\t}\n}\n"
        },
        {
          "name": "topology.go",
          "type": "blob",
          "size": 8.33984375,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n)\n\ntype hostTokens struct {\n\t// token is end (inclusive) of token range these hosts belong to\n\ttoken token\n\thosts []*HostInfo\n}\n\n// tokenRingReplicas maps token ranges to list of replicas.\n// The elements in tokenRingReplicas are sorted by token ascending.\n// The range for a given item in tokenRingReplicas starts after preceding range and ends with the token specified in\n// token. The end token is part of the range.\n// The lowest (i.e. index 0) range wraps around the ring (its preceding range is the one with largest index).\ntype tokenRingReplicas []hostTokens\n\nfunc (h tokenRingReplicas) Less(i, j int) bool { return h[i].token.Less(h[j].token) }\nfunc (h tokenRingReplicas) Len() int           { return len(h) }\nfunc (h tokenRingReplicas) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\n\nfunc (h tokenRingReplicas) replicasFor(t token) *hostTokens {\n\tif len(h) == 0 {\n\t\treturn nil\n\t}\n\n\tp := sort.Search(len(h), func(i int) bool {\n\t\treturn !h[i].token.Less(t)\n\t})\n\n\tif p >= len(h) {\n\t\t// rollover\n\t\tp = 0\n\t}\n\n\treturn &h[p]\n}\n\ntype placementStrategy interface {\n\treplicaMap(tokenRing *tokenRing) tokenRingReplicas\n\treplicationFactor(dc string) int\n}\n\nfunc getReplicationFactorFromOpts(val interface{}) (int, error) {\n\tswitch v := val.(type) {\n\tcase int:\n\t\tif v < 0 {\n\t\t\treturn 0, fmt.Errorf(\"invalid replication_factor %d\", v)\n\t\t}\n\t\treturn v, nil\n\tcase string:\n\t\tn, err := strconv.Atoi(v)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"invalid replication_factor %q: %v\", v, err)\n\t\t} else if n < 0 {\n\t\t\treturn 0, fmt.Errorf(\"invalid replication_factor %d\", n)\n\t\t}\n\t\treturn n, nil\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"unknown replication_factor type %T\", v)\n\t}\n}\n\nfunc getStrategy(ks *KeyspaceMetadata, logger StdLogger) placementStrategy {\n\tswitch {\n\tcase strings.Contains(ks.StrategyClass, \"SimpleStrategy\"):\n\t\trf, err := getReplicationFactorFromOpts(ks.StrategyOptions[\"replication_factor\"])\n\t\tif err != nil {\n\t\t\tlogger.Printf(\"parse rf for keyspace %q: %v\", ks.Name, err)\n\t\t\treturn nil\n\t\t}\n\t\treturn &simpleStrategy{rf: rf}\n\tcase strings.Contains(ks.StrategyClass, \"NetworkTopologyStrategy\"):\n\t\tdcs := make(map[string]int)\n\t\tfor dc, rf := range ks.StrategyOptions {\n\t\t\tif dc == \"class\" {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\trf, err := getReplicationFactorFromOpts(rf)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Println(\"parse rf for keyspace %q, dc %q: %v\", err)\n\t\t\t\t// skip DC if the rf is invalid/unsupported, so that we can at least work with other working DCs.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdcs[dc] = rf\n\t\t}\n\t\treturn &networkTopology{dcs: dcs}\n\tcase strings.Contains(ks.StrategyClass, \"LocalStrategy\"):\n\t\treturn nil\n\tdefault:\n\t\tlogger.Printf(\"parse rf for keyspace %q: unsupported strategy class: %v\", ks.StrategyClass)\n\t\treturn nil\n\t}\n}\n\ntype simpleStrategy struct {\n\trf int\n}\n\nfunc (s *simpleStrategy) replicationFactor(dc string) int {\n\treturn s.rf\n}\n\nfunc (s *simpleStrategy) replicaMap(tokenRing *tokenRing) tokenRingReplicas {\n\ttokens := tokenRing.tokens\n\tring := make(tokenRingReplicas, len(tokens))\n\n\tfor i, th := range tokens {\n\t\treplicas := make([]*HostInfo, 0, s.rf)\n\t\tseen := make(map[*HostInfo]bool)\n\n\t\tfor j := 0; j < len(tokens) && len(replicas) < s.rf; j++ {\n\t\t\th := tokens[(i+j)%len(tokens)]\n\t\t\tif !seen[h.host] {\n\t\t\t\treplicas = append(replicas, h.host)\n\t\t\t\tseen[h.host] = true\n\t\t\t}\n\t\t}\n\n\t\tring[i] = hostTokens{th.token, replicas}\n\t}\n\n\tsort.Sort(ring)\n\n\treturn ring\n}\n\ntype networkTopology struct {\n\tdcs map[string]int\n}\n\nfunc (n *networkTopology) replicationFactor(dc string) int {\n\treturn n.dcs[dc]\n}\n\nfunc (n *networkTopology) haveRF(replicaCounts map[string]int) bool {\n\tif len(replicaCounts) != len(n.dcs) {\n\t\treturn false\n\t}\n\n\tfor dc, rf := range n.dcs {\n\t\tif rf != replicaCounts[dc] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\nfunc (n *networkTopology) replicaMap(tokenRing *tokenRing) tokenRingReplicas {\n\tdcRacks := make(map[string]map[string]struct{}, len(n.dcs))\n\t// skipped hosts in a dc\n\tskipped := make(map[string][]*HostInfo, len(n.dcs))\n\t// number of replicas per dc\n\treplicasInDC := make(map[string]int, len(n.dcs))\n\t// dc -> racks\n\tseenDCRacks := make(map[string]map[string]struct{}, len(n.dcs))\n\n\tfor _, h := range tokenRing.hosts {\n\t\tdc := h.DataCenter()\n\t\track := h.Rack()\n\n\t\tracks, ok := dcRacks[dc]\n\t\tif !ok {\n\t\t\tracks = make(map[string]struct{})\n\t\t\tdcRacks[dc] = racks\n\t\t}\n\t\tracks[rack] = struct{}{}\n\t}\n\n\tfor dc, racks := range dcRacks {\n\t\treplicasInDC[dc] = 0\n\t\tseenDCRacks[dc] = make(map[string]struct{}, len(racks))\n\t}\n\n\ttokens := tokenRing.tokens\n\treplicaRing := make(tokenRingReplicas, 0, len(tokens))\n\n\tvar totalRF int\n\tfor _, rf := range n.dcs {\n\t\ttotalRF += rf\n\t}\n\n\tfor i, th := range tokenRing.tokens {\n\t\tif rf := n.dcs[th.host.DataCenter()]; rf == 0 {\n\t\t\t// skip this token since no replica in this datacenter.\n\t\t\tcontinue\n\t\t}\n\n\t\tfor k, v := range skipped {\n\t\t\tskipped[k] = v[:0]\n\t\t}\n\n\t\tfor dc := range n.dcs {\n\t\t\treplicasInDC[dc] = 0\n\t\t\tfor rack := range seenDCRacks[dc] {\n\t\t\t\tdelete(seenDCRacks[dc], rack)\n\t\t\t}\n\t\t}\n\n\t\treplicas := make([]*HostInfo, 0, totalRF)\n\t\tfor j := 0; j < len(tokens) && (len(replicas) < totalRF && !n.haveRF(replicasInDC)); j++ {\n\t\t\t// TODO: ensure we dont add the same host twice\n\t\t\tp := i + j\n\t\t\tif p >= len(tokens) {\n\t\t\t\tp -= len(tokens)\n\t\t\t}\n\t\t\th := tokens[p].host\n\n\t\t\tdc := h.DataCenter()\n\t\t\track := h.Rack()\n\n\t\t\trf := n.dcs[dc]\n\t\t\tif rf == 0 {\n\t\t\t\t// skip this DC, dont know about it or replication factor is zero\n\t\t\t\tcontinue\n\t\t\t} else if replicasInDC[dc] >= rf {\n\t\t\t\tif replicasInDC[dc] > rf {\n\t\t\t\t\tpanic(fmt.Sprintf(\"replica overflow. rf=%d have=%d in dc %q\", rf, replicasInDC[dc], dc))\n\t\t\t\t}\n\n\t\t\t\t// have enough replicas in this DC\n\t\t\t\tcontinue\n\t\t\t} else if _, ok := dcRacks[dc][rack]; !ok {\n\t\t\t\t// dont know about this rack\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tracks := seenDCRacks[dc]\n\t\t\tif _, ok := racks[rack]; ok && len(racks) == len(dcRacks[dc]) {\n\t\t\t\t// we have been through all the racks and dont have RF yet, add this\n\t\t\t\treplicas = append(replicas, h)\n\t\t\t\treplicasInDC[dc]++\n\t\t\t} else if !ok {\n\t\t\t\tif racks == nil {\n\t\t\t\t\tracks = make(map[string]struct{}, 1)\n\t\t\t\t\tseenDCRacks[dc] = racks\n\t\t\t\t}\n\n\t\t\t\t// new rack\n\t\t\t\tracks[rack] = struct{}{}\n\t\t\t\treplicas = append(replicas, h)\n\t\t\t\tr := replicasInDC[dc] + 1\n\n\t\t\t\tif len(racks) == len(dcRacks[dc]) {\n\t\t\t\t\t// if we have been through all the racks, drain the rest of the skipped\n\t\t\t\t\t// hosts until we have RF. The next iteration will skip in the block\n\t\t\t\t\t// above\n\t\t\t\t\tskippedHosts := skipped[dc]\n\t\t\t\t\tvar k int\n\t\t\t\t\tfor ; k < len(skippedHosts) && r+k < rf; k++ {\n\t\t\t\t\t\tsh := skippedHosts[k]\n\t\t\t\t\t\treplicas = append(replicas, sh)\n\t\t\t\t\t}\n\t\t\t\t\tr += k\n\t\t\t\t\tskipped[dc] = skippedHosts[k:]\n\t\t\t\t}\n\t\t\t\treplicasInDC[dc] = r\n\t\t\t} else {\n\t\t\t\t// already seen this rack, keep hold of this host incase\n\t\t\t\t// we dont get enough for rf\n\t\t\t\tskipped[dc] = append(skipped[dc], h)\n\t\t\t}\n\t\t}\n\n\t\tif len(replicas) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"no replicas for token: %v\", th.token))\n\t\t} else if !replicas[0].Equal(th.host) {\n\t\t\tpanic(fmt.Sprintf(\"first replica is not the primary replica for the token: expected %v got %v\", replicas[0].ConnectAddress(), th.host.ConnectAddress()))\n\t\t}\n\n\t\treplicaRing = append(replicaRing, hostTokens{th.token, replicas})\n\t}\n\n\tdcsWithReplicas := 0\n\tfor _, dc := range n.dcs {\n\t\tif dc > 0 {\n\t\t\tdcsWithReplicas++\n\t\t}\n\t}\n\n\tif dcsWithReplicas == len(dcRacks) && len(replicaRing) != len(tokens) {\n\t\tpanic(fmt.Sprintf(\"token map different size to token ring: got %d expected %d\", len(replicaRing), len(tokens)))\n\t}\n\n\treturn replicaRing\n}\n"
        },
        {
          "name": "topology_test.go",
          "type": "blob",
          "size": 5.857421875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"testing\"\n)\n\nfunc TestPlacementStrategy_SimpleStrategy(t *testing.T) {\n\thost0 := &HostInfo{hostId: \"0\"}\n\thost25 := &HostInfo{hostId: \"25\"}\n\thost50 := &HostInfo{hostId: \"50\"}\n\thost75 := &HostInfo{hostId: \"75\"}\n\n\ttokens := []hostToken{\n\t\t{intToken(0), host0},\n\t\t{intToken(25), host25},\n\t\t{intToken(50), host50},\n\t\t{intToken(75), host75},\n\t}\n\n\thosts := []*HostInfo{host0, host25, host50, host75}\n\n\tstrat := &simpleStrategy{rf: 2}\n\ttokenReplicas := strat.replicaMap(&tokenRing{hosts: hosts, tokens: tokens})\n\tif len(tokenReplicas) != len(tokens) {\n\t\tt.Fatalf(\"expected replica map to have %d items but has %d\", len(tokens), len(tokenReplicas))\n\t}\n\n\tfor _, replicas := range tokenReplicas {\n\t\tif len(replicas.hosts) != strat.rf {\n\t\t\tt.Errorf(\"expected to have %d replicas got %d for token=%v\", strat.rf, len(replicas.hosts), replicas.token)\n\t\t}\n\t}\n\n\tfor i, token := range tokens {\n\t\tht := tokenReplicas.replicasFor(token.token)\n\t\tif ht.token != token.token {\n\t\t\tt.Errorf(\"token %v not in replica map: %v\", token, ht.hosts)\n\t\t}\n\n\t\tfor j, replica := range ht.hosts {\n\t\t\texp := tokens[(i+j)%len(tokens)].host\n\t\t\tif exp != replica {\n\t\t\t\tt.Errorf(\"expected host %v to be a replica of %v got %v\", exp.hostId, token, replica.hostId)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestPlacementStrategy_NetworkStrategy(t *testing.T) {\n\tconst (\n\t\ttotalDCs   = 3\n\t\tracksPerDC = 3\n\t\thostsPerDC = 5\n\t)\n\n\ttests := []struct {\n\t\tname                   string\n\t\tstrat                  *networkTopology\n\t\texpectedReplicaMapSize int\n\t}{\n\t\t{\n\t\t\tname: \"full\",\n\t\t\tstrat: &networkTopology{\n\t\t\t\tdcs: map[string]int{\n\t\t\t\t\t\"dc1\": 1,\n\t\t\t\t\t\"dc2\": 2,\n\t\t\t\t\t\"dc3\": 3,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedReplicaMapSize: hostsPerDC * totalDCs,\n\t\t},\n\t\t{\n\t\t\tname: \"missing\",\n\t\t\tstrat: &networkTopology{\n\t\t\t\tdcs: map[string]int{\n\t\t\t\t\t\"dc2\": 2,\n\t\t\t\t\t\"dc3\": 3,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedReplicaMapSize: hostsPerDC * 2,\n\t\t},\n\t\t{\n\t\t\tname: \"zero\",\n\t\t\tstrat: &networkTopology{\n\t\t\t\tdcs: map[string]int{\n\t\t\t\t\t\"dc1\": 0,\n\t\t\t\t\t\"dc2\": 2,\n\t\t\t\t\t\"dc3\": 3,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedReplicaMapSize: hostsPerDC * 2,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttest := test\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tvar (\n\t\t\t\thosts  []*HostInfo\n\t\t\t\ttokens []hostToken\n\t\t\t)\n\t\t\tdcRing := make(map[string][]hostToken, totalDCs)\n\t\t\tfor i := 0; i < totalDCs; i++ {\n\t\t\t\tvar dcTokens []hostToken\n\t\t\t\tdc := fmt.Sprintf(\"dc%d\", i+1)\n\n\t\t\t\tfor j := 0; j < hostsPerDC; j++ {\n\t\t\t\t\track := fmt.Sprintf(\"rack%d\", (j%racksPerDC)+1)\n\n\t\t\t\t\th := &HostInfo{hostId: fmt.Sprintf(\"%s:%s:%d\", dc, rack, j), dataCenter: dc, rack: rack}\n\n\t\t\t\t\ttoken := hostToken{\n\t\t\t\t\t\ttoken: orderedToken([]byte(h.hostId)),\n\t\t\t\t\t\thost:  h,\n\t\t\t\t\t}\n\n\t\t\t\t\ttokens = append(tokens, token)\n\t\t\t\t\tdcTokens = append(dcTokens, token)\n\n\t\t\t\t\thosts = append(hosts, h)\n\t\t\t\t}\n\n\t\t\t\tsort.Sort(&tokenRing{tokens: dcTokens})\n\t\t\t\tdcRing[dc] = dcTokens\n\t\t\t}\n\n\t\t\tif len(tokens) != hostsPerDC*totalDCs {\n\t\t\t\tt.Fatalf(\"expected %d tokens in the ring got %d\", hostsPerDC*totalDCs, len(tokens))\n\t\t\t}\n\t\t\tsort.Sort(&tokenRing{tokens: tokens})\n\n\t\t\tvar expReplicas int\n\t\t\tfor _, rf := range test.strat.dcs {\n\t\t\t\texpReplicas += rf\n\t\t\t}\n\n\t\t\ttokenReplicas := test.strat.replicaMap(&tokenRing{hosts: hosts, tokens: tokens})\n\t\t\tif len(tokenReplicas) != test.expectedReplicaMapSize {\n\t\t\t\tt.Fatalf(\"expected replica map to have %d items but has %d\", test.expectedReplicaMapSize,\n\t\t\t\t\tlen(tokenReplicas))\n\t\t\t}\n\t\t\tif !sort.IsSorted(tokenReplicas) {\n\t\t\t\tt.Fatal(\"replica map was not sorted by token\")\n\t\t\t}\n\n\t\t\tfor token, replicas := range tokenReplicas {\n\t\t\t\tif len(replicas.hosts) != expReplicas {\n\t\t\t\t\tt.Fatalf(\"expected to have %d replicas got %d for token=%v\", expReplicas, len(replicas.hosts), token)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor dc, rf := range test.strat.dcs {\n\t\t\t\tif rf == 0 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tdcTokens := dcRing[dc]\n\t\t\t\tfor i, th := range dcTokens {\n\t\t\t\t\ttoken := th.token\n\t\t\t\t\tallReplicas := tokenReplicas.replicasFor(token)\n\t\t\t\t\tif allReplicas.token != token {\n\t\t\t\t\t\tt.Fatalf(\"token %v not in replica map\", token)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar replicas []*HostInfo\n\t\t\t\t\tfor _, replica := range allReplicas.hosts {\n\t\t\t\t\t\tif replica.dataCenter == dc {\n\t\t\t\t\t\t\treplicas = append(replicas, replica)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif len(replicas) != rf {\n\t\t\t\t\t\tt.Fatalf(\"expected %d replicas in dc %q got %d\", rf, dc, len(replicas))\n\t\t\t\t\t}\n\n\t\t\t\t\tvar lastRack string\n\t\t\t\t\tfor j, replica := range replicas {\n\t\t\t\t\t\t// expected is in the next rack\n\t\t\t\t\t\tvar exp *HostInfo\n\t\t\t\t\t\tif lastRack == \"\" {\n\t\t\t\t\t\t\t// primary, first replica\n\t\t\t\t\t\t\texp = dcTokens[(i+j)%len(dcTokens)].host\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tfor k := 0; k < len(dcTokens); k++ {\n\t\t\t\t\t\t\t\t// walk around the ring from i + j to find the next host the\n\t\t\t\t\t\t\t\t// next rack\n\t\t\t\t\t\t\t\tp := (i + j + k) % len(dcTokens)\n\t\t\t\t\t\t\t\th := dcTokens[p].host\n\t\t\t\t\t\t\t\tif h.rack != lastRack {\n\t\t\t\t\t\t\t\t\texp = h\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif exp.rack == lastRack {\n\t\t\t\t\t\t\t\tpanic(\"no more racks\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlastRack = replica.rack\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "tuple_test.go",
          "type": "blob",
          "size": 10.419921875,
          "content": "//go:build all || integration\n// +build all integration\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestTupleSimple(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_test(\n\t\tid int,\n\t\tcoord frozen<tuple<int, int>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = session.Query(\"INSERT INTO tuple_test(id, coord) VALUES(?, (?, ?))\", 1, 100, -100).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tid    int\n\t\tcoord struct {\n\t\t\tx int\n\t\t\ty int\n\t\t}\n\t)\n\n\titer := session.Query(\"SELECT id, coord FROM tuple_test WHERE id=?\", 1)\n\tif err := iter.Scan(&id, &coord.x, &coord.y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif id != 1 {\n\t\tt.Errorf(\"expected to get id=1 got: %v\", id)\n\t} else if coord.x != 100 {\n\t\tt.Errorf(\"expected to get coord.x=100 got: %v\", coord.x)\n\t} else if coord.y != -100 {\n\t\tt.Errorf(\"expected to get coord.y=-100 got: %v\", coord.y)\n\t}\n\n}\n\nfunc TestTuple_NullTuple(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_nil_test(\n\t\tid int,\n\t\tcoord frozen<tuple<int, int>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconst id = 1\n\n\terr = session.Query(\"INSERT INTO tuple_nil_test(id, coord) VALUES(?, (?, ?))\", id, nil, nil).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tx := new(int)\n\ty := new(int)\n\titer := session.Query(\"SELECT coord FROM tuple_nil_test WHERE id=?\", id)\n\tif err := iter.Scan(&x, &y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif x != nil {\n\t\tt.Fatalf(\"should be nil got %+#v\", x)\n\t} else if y != nil {\n\t\tt.Fatalf(\"should be nil got %+#v\", y)\n\t}\n\n}\n\nfunc TestTuple_TupleNotSet(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_not_set_test(\n\t\tid int,\n\t\tcoord frozen<tuple<int, int>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconst id = 1\n\n\terr = session.Query(\"INSERT INTO tuple_not_set_test(id,coord) VALUES(?, (?,?))\", id, 1, 2).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr = session.Query(\"INSERT INTO tuple_not_set_test(id) VALUES(?)\", id+1).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tx := new(int)\n\ty := new(int)\n\titer := session.Query(\"SELECT coord FROM tuple_not_set_test WHERE id=?\", id)\n\tif err := iter.Scan(x, y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif x == nil || *x != 1 {\n\t\tt.Fatalf(\"x should be %d got %+#v, value=%d\", 1, x, *x)\n\t}\n\tif y == nil || *y != 2 {\n\t\tt.Fatalf(\"y should be %d got %+#v, value=%d\", 2, y, *y)\n\t}\n\n\t// Check if the supplied targets are reset to nil\n\titer = session.Query(\"SELECT coord FROM tuple_not_set_test WHERE id=?\", id+1)\n\tif err := iter.Scan(x, y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif x == nil || *x != 0 {\n\t\tt.Fatalf(\"x should be %d got %+#v, value=%d\", 0, x, *x)\n\t}\n\tif y == nil || *y != 0 {\n\t\tt.Fatalf(\"y should be %d got %+#v, value=%d\", 0, y, *y)\n\t}\n}\n\nfunc TestTupleMapScan(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_map_scan(\n\t\tid int,\n\t\tval frozen<tuple<int, int>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := session.Query(`INSERT INTO tuple_map_scan (id, val) VALUES (1, (1, 2));`).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := make(map[string]interface{})\n\terr = session.Query(`SELECT * FROM tuple_map_scan`).MapScan(m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif m[\"val[0]\"] != 1 {\n\t\tt.Fatalf(\"expacted val[0] to be %d but was %d\", 1, m[\"val[0]\"])\n\t}\n\tif m[\"val[1]\"] != 2 {\n\t\tt.Fatalf(\"expacted val[1] to be %d but was %d\", 2, m[\"val[1]\"])\n\t}\n}\n\nfunc TestTupleMapScanNil(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_map_scan_nil(\n\t\t\tid int,\n\t\t\tval frozen<tuple<int, int>>,\n\n\t\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := session.Query(`INSERT INTO tuple_map_scan_nil (id, val) VALUES (?,(?,?));`, 1, nil, nil).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := make(map[string]interface{})\n\terr = session.Query(`SELECT * FROM tuple_map_scan_nil`).MapScan(m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif m[\"val[0]\"] != 0 {\n\t\tt.Fatalf(\"expacted val[0] to be %d but was %d\", 0, m[\"val[0]\"])\n\t}\n\tif m[\"val[1]\"] != 0 {\n\t\tt.Fatalf(\"expacted val[1] to be %d but was %d\", 0, m[\"val[1]\"])\n\t}\n}\n\nfunc TestTupleMapScanNotSet(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_map_scan_not_set(\n\t\t\tid int,\n\t\t\tval frozen<tuple<int, int>>,\n\n\t\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := session.Query(`INSERT INTO tuple_map_scan_not_set (id) VALUES (?);`, 1).Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := make(map[string]interface{})\n\terr = session.Query(`SELECT * FROM tuple_map_scan_not_set`).MapScan(m)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif m[\"val[0]\"] != 0 {\n\t\tt.Fatalf(\"expacted val[0] to be %d but was %d\", 0, m[\"val[0]\"])\n\t}\n\tif m[\"val[1]\"] != 0 {\n\t\tt.Fatalf(\"expacted val[1] to be %d but was %d\", 0, m[\"val[1]\"])\n\t}\n}\n\nfunc TestTupleLastFieldEmpty(t *testing.T) {\n\t// Regression test - empty value used to be treated as NULL value in the last tuple field\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\terr := createTable(session, `CREATE TABLE gocql_test.tuple_last_field_empty(\n\t\t\tid int,\n\t\t\tval frozen<tuple<text, text>>,\n\n\t\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := session.Query(`INSERT INTO tuple_last_field_empty (id, val) VALUES (?,(?,?));`, 1, \"abc\", \"\").Exec(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar e1, e2 *string\n\tif err := session.Query(\"SELECT val FROM tuple_last_field_empty WHERE id = ?\", 1).Scan(&e1, &e2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif e1 == nil {\n\t\tt.Fatal(\"expected e1 not to be nil\")\n\t}\n\tif *e1 != \"abc\" {\n\t\tt.Fatalf(\"expected e1 to be equal to \\\"abc\\\", but is %v\", *e2)\n\t}\n\tif e2 == nil {\n\t\tt.Fatal(\"expected e2 not to be nil\")\n\t}\n\tif *e2 != \"\" {\n\t\tt.Fatalf(\"expected e2 to be an empty string, but is %v\", *e2)\n\t}\n}\n\nfunc TestTuple_NestedCollection(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.nested_tuples(\n\t\tid int,\n\t\tval list<frozen<tuple<int, text>>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype typ struct {\n\t\tA int\n\t\tB string\n\t}\n\n\ttests := []struct {\n\t\tname string\n\t\tval  interface{}\n\t}{\n\t\t{name: \"slice\", val: [][]interface{}{{1, \"2\"}, {3, \"4\"}}},\n\t\t{name: \"array\", val: [][2]interface{}{{1, \"2\"}, {3, \"4\"}}},\n\t\t{name: \"struct\", val: []typ{{1, \"2\"}, {3, \"4\"}}},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tif err := session.Query(`INSERT INTO nested_tuples (id, val) VALUES (?, ?);`, i, test.val).Exec(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\trv := reflect.ValueOf(test.val)\n\t\t\tres := reflect.New(rv.Type()).Elem().Addr().Interface()\n\n\t\t\terr = session.Query(`SELECT val FROM nested_tuples WHERE id=?`, i).Scan(res)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tresVal := reflect.ValueOf(res).Elem().Interface()\n\t\t\tif !reflect.DeepEqual(test.val, resVal) {\n\t\t\t\tt.Fatalf(\"unmarshaled value not equal to the original value: expected %#v, got %#v\", test.val, resVal)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTuple_NullableNestedCollection(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"tuple types are only available of proto>=3\")\n\t}\n\n\terr := createTable(session, `CREATE TABLE gocql_test.nested_tuples_with_nulls(\n\t\tid int,\n\t\tval list<frozen<tuple<text, text>>>,\n\n\t\tprimary key(id))`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype typ struct {\n\t\tA *string\n\t\tB *string\n\t}\n\n\tptrStr := func(s string) *string {\n\t\tret := new(string)\n\t\t*ret = s\n\t\treturn ret\n\t}\n\n\ttests := []struct {\n\t\tname string\n\t\tval  interface{}\n\t}{\n\t\t{name: \"slice\", val: [][]*string{{ptrStr(\"1\"), nil}, {nil, ptrStr(\"2\")}, {ptrStr(\"3\"), ptrStr(\"\")}}},\n\t\t{name: \"array\", val: [][2]*string{{ptrStr(\"1\"), nil}, {nil, ptrStr(\"2\")}, {ptrStr(\"3\"), ptrStr(\"\")}}},\n\t\t{name: \"struct\", val: []typ{{ptrStr(\"1\"), nil}, {nil, ptrStr(\"2\")}, {ptrStr(\"3\"), ptrStr(\"\")}}},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tif err := session.Query(`INSERT INTO nested_tuples_with_nulls (id, val) VALUES (?, ?);`, i, test.val).Exec(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\trv := reflect.ValueOf(test.val)\n\t\t\tres := reflect.New(rv.Type()).Interface()\n\n\t\t\terr = session.Query(`SELECT val FROM nested_tuples_with_nulls WHERE id=?`, i).Scan(res)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tresVal := reflect.ValueOf(res).Elem().Interface()\n\t\t\tif !reflect.DeepEqual(test.val, resVal) {\n\t\t\t\tt.Fatalf(\"unmarshaled value not equal to the original value: expected %#v, got %#v\", test.val, resVal)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "udt_test.go",
          "type": "blob",
          "size": 11.8017578125,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n)\n\ntype position struct {\n\tLat     int    `cql:\"lat\"`\n\tLon     int    `cql:\"lon\"`\n\tPadding string `json:\"padding\"`\n}\n\n// NOTE: due to current implementation details it is not currently possible to use\n// a pointer receiver type for the UDTMarshaler interface to handle UDT's\nfunc (p position) MarshalUDT(name string, info TypeInfo) ([]byte, error) {\n\tswitch name {\n\tcase \"lat\":\n\t\treturn Marshal(info, p.Lat)\n\tcase \"lon\":\n\t\treturn Marshal(info, p.Lon)\n\tcase \"padding\":\n\t\treturn Marshal(info, p.Padding)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown column for position: %q\", name)\n\t}\n}\n\nfunc (p *position) UnmarshalUDT(name string, info TypeInfo, data []byte) error {\n\tswitch name {\n\tcase \"lat\":\n\t\treturn Unmarshal(info, data, &p.Lat)\n\tcase \"lon\":\n\t\treturn Unmarshal(info, data, &p.Lon)\n\tcase \"padding\":\n\t\treturn Unmarshal(info, data, &p.Padding)\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown column for position: %q\", name)\n\t}\n}\n\nfunc TestUDT_Marshaler(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.position(\n\t\tlat int,\n\t\tlon int,\n\t\tpadding text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.houses(\n\t\tid int,\n\t\tname text,\n\t\tloc frozen<position>,\n\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tconst (\n\t\texpLat = -1\n\t\texpLon = 2\n\t)\n\tpad := strings.Repeat(\"X\", 1000)\n\n\terr = session.Query(\"INSERT INTO houses(id, name, loc) VALUES(?, ?, ?)\", 1, \"test\", &position{expLat, expLon, pad}).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpos := &position{}\n\n\terr = session.Query(\"SELECT loc FROM houses WHERE id = ?\", 1).Scan(pos)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif pos.Lat != expLat {\n\t\tt.Errorf(\"expeceted lat to be be %d got %d\", expLat, pos.Lat)\n\t}\n\tif pos.Lon != expLon {\n\t\tt.Errorf(\"expeceted lon to be be %d got %d\", expLon, pos.Lon)\n\t}\n\tif pos.Padding != pad {\n\t\tt.Errorf(\"expected to get padding %q got %q\\n\", pad, pos.Padding)\n\t}\n}\n\nfunc TestUDT_Reflect(t *testing.T) {\n\t// Uses reflection instead of implementing the marshaling type\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.horse(\n\t\tname text,\n\t\towner text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.horse_race(\n\t\tposition int,\n\t\thorse frozen<horse>,\n\n\t\tprimary key(position)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype horse struct {\n\t\tName  string `cql:\"name\"`\n\t\tOwner string `cql:\"owner\"`\n\t}\n\n\tinsertedHorse := &horse{\n\t\tName:  \"pony\",\n\t\tOwner: \"jim\",\n\t}\n\n\terr = session.Query(\"INSERT INTO horse_race(position, horse) VALUES(?, ?)\", 1, insertedHorse).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tretrievedHorse := &horse{}\n\terr = session.Query(\"SELECT horse FROM horse_race WHERE position = ?\", 1).Scan(retrievedHorse)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif *retrievedHorse != *insertedHorse {\n\t\tt.Fatalf(\"expected to get %+v got %+v\", insertedHorse, retrievedHorse)\n\t}\n}\n\nfunc TestUDT_Proto2error(t *testing.T) {\n\t// TODO(zariel): move this to marshal test?\n\t_, err := Marshal(NativeType{custom: \"org.apache.cassandra.db.marshal.UserType.Type\", proto: 2}, 1)\n\tif err != ErrorUDTUnavailable {\n\t\tt.Fatalf(\"expected %v got %v\", ErrUnavailable, err)\n\t}\n}\n\nfunc TestUDT_NullObject(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.udt_null_type(\n\t\tname text,\n\t\towner text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.udt_null_table(\n\t\tid uuid,\n\t\tudt_col frozen<udt_null_type>,\n\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype col struct {\n\t\tName  string `cql:\"name\"`\n\t\tOwner string `cql:\"owner\"`\n\t}\n\n\tid := TimeUUID()\n\terr = session.Query(\"INSERT INTO udt_null_table(id) VALUES(?)\", id).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treadCol := &col{\n\t\tName:  \"temp\",\n\t\tOwner: \"temp\",\n\t}\n\n\terr = session.Query(\"SELECT udt_col FROM udt_null_table WHERE id = ?\", id).Scan(readCol)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif readCol.Name != \"\" {\n\t\tt.Errorf(\"expected empty string to be returned for null udt: got %q\", readCol.Name)\n\t}\n\tif readCol.Owner != \"\" {\n\t\tt.Errorf(\"expected empty string to be returned for null udt: got %q\", readCol.Owner)\n\t}\n}\n\nfunc TestMapScanUDT(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.log_entry (\n\t\tcreated_timestamp timestamp,\n\t\tmessage text\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.requests_by_id (\n\t\tid uuid PRIMARY KEY,\n\t\ttype int,\n\t\tlog_entries list<frozen <log_entry>>\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tentry := []struct {\n\t\tCreatedTimestamp time.Time `cql:\"created_timestamp\"`\n\t\tMessage          string    `cql:\"message\"`\n\t}{\n\t\t{\n\t\t\tCreatedTimestamp: time.Now().Truncate(time.Millisecond),\n\t\t\tMessage:          \"test time now\",\n\t\t},\n\t}\n\n\tid, _ := RandomUUID()\n\tconst typ = 1\n\n\terr = session.Query(\"INSERT INTO requests_by_id(id, type, log_entries) VALUES (?, ?, ?)\", id, typ, entry).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\trawResult := map[string]interface{}{}\n\terr = session.Query(`SELECT * FROM requests_by_id WHERE id = ?`, id).MapScan(rawResult)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tlogEntries, ok := rawResult[\"log_entries\"].([]map[string]interface{})\n\tif !ok {\n\t\tt.Fatal(\"log_entries not in scanned map\")\n\t}\n\n\tif len(logEntries) != 1 {\n\t\tt.Fatalf(\"expected to get 1 log_entry got %d\", len(logEntries))\n\t}\n\n\tlogEntry := logEntries[0]\n\n\ttimestamp, ok := logEntry[\"created_timestamp\"]\n\tif !ok {\n\t\tt.Error(\"created_timestamp not unmarshalled into map\")\n\t} else {\n\t\tif ts, ok := timestamp.(time.Time); ok {\n\t\t\tif !ts.In(time.UTC).Equal(entry[0].CreatedTimestamp.In(time.UTC)) {\n\t\t\t\tt.Errorf(\"created_timestamp not equal to stored: got %v expected %v\", ts.In(time.UTC), entry[0].CreatedTimestamp.In(time.UTC))\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"created_timestamp was not time.Time got: %T\", timestamp)\n\t\t}\n\t}\n\n\tmessage, ok := logEntry[\"message\"]\n\tif !ok {\n\t\tt.Error(\"message not unmarshalled into map\")\n\t} else {\n\t\tif ts, ok := message.(string); ok {\n\t\t\tif ts != message {\n\t\t\t\tt.Errorf(\"message not equal to stored: got %v expected %v\", ts, entry[0].Message)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"message was not string got: %T\", message)\n\t\t}\n\t}\n}\n\nfunc TestUDT_MissingField(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.missing_field(\n\t\tname text,\n\t\towner text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.missing_field(\n\t\tid uuid,\n\t\tudt_col frozen<udt_null_type>,\n\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype col struct {\n\t\tName string `cql:\"name\"`\n\t}\n\n\twriteCol := &col{\n\t\tName: \"test\",\n\t}\n\n\tid := TimeUUID()\n\terr = session.Query(\"INSERT INTO missing_field(id, udt_col) VALUES(?, ?)\", id, writeCol).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treadCol := &col{}\n\terr = session.Query(\"SELECT udt_col FROM missing_field WHERE id = ?\", id).Scan(readCol)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif readCol.Name != writeCol.Name {\n\t\tt.Errorf(\"expected %q: got %q\", writeCol.Name, readCol.Name)\n\t}\n}\n\nfunc TestUDT_EmptyCollections(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.nil_collections(\n\t\ta list<text>,\n\t\tb map<text, text>,\n\t\tc set<text>\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.nil_collections(\n\t\tid uuid,\n\t\tudt_col frozen<nil_collections>,\n\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype udt struct {\n\t\tA []string          `cql:\"a\"`\n\t\tB map[string]string `cql:\"b\"`\n\t\tC []string          `cql:\"c\"`\n\t}\n\n\tid := TimeUUID()\n\terr = session.Query(\"INSERT INTO nil_collections(id, udt_col) VALUES(?, ?)\", id, &udt{}).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar val udt\n\terr = session.Query(\"SELECT udt_col FROM nil_collections WHERE id=?\", id).Scan(&val)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif val.A != nil {\n\t\tt.Errorf(\"expected to get nil got %#+v\", val.A)\n\t}\n\tif val.B != nil {\n\t\tt.Errorf(\"expected to get nil got %#+v\", val.B)\n\t}\n\tif val.C != nil {\n\t\tt.Errorf(\"expected to get nil got %#+v\", val.C)\n\t}\n}\n\nfunc TestUDT_UpdateField(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.update_field_udt(\n\t\tname text,\n\t\towner text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.update_field(\n\t\tid uuid,\n\t\tudt_col frozen<update_field_udt>,\n\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttype col struct {\n\t\tName  string `cql:\"name\"`\n\t\tOwner string `cql:\"owner\"`\n\t\tData  string `cql:\"data\"`\n\t}\n\n\twriteCol := &col{\n\t\tName:  \"test-name\",\n\t\tOwner: \"test-owner\",\n\t}\n\n\tid := TimeUUID()\n\terr = session.Query(\"INSERT INTO update_field(id, udt_col) VALUES(?, ?)\", id, writeCol).Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := createTable(session, `ALTER TYPE gocql_test.update_field_udt ADD data text;`); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treadCol := &col{}\n\terr = session.Query(\"SELECT udt_col FROM update_field WHERE id = ?\", id).Scan(readCol)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif *readCol != *writeCol {\n\t\tt.Errorf(\"expected %+v: got %+v\", *writeCol, *readCol)\n\t}\n}\n\nfunc TestUDT_ScanNullUDT(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tif session.cfg.ProtoVersion < protoVersion3 {\n\t\tt.Skip(\"UDT are only available on protocol >= 3\")\n\t}\n\n\terr := createTable(session, `CREATE TYPE gocql_test.scan_null_udt_position(\n\t\tlat int,\n\t\tlon int,\n\t\tpadding text);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = createTable(session, `CREATE TABLE gocql_test.scan_null_udt_houses(\n\t\tid int,\n\t\tname text,\n\t\tloc frozen<position>,\n\t\tprimary key(id)\n\t);`)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = session.Query(\"INSERT INTO scan_null_udt_houses(id, name) VALUES(?, ?)\", 1, \"test\").Exec()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpos := &position{}\n\n\terr = session.Query(\"SELECT loc FROM scan_null_udt_houses WHERE id = ?\", 1).Scan(pos)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "uuid.go",
          "type": "blob",
          "size": 9.20703125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2012, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\n// The uuid package can be used to generate and parse universally unique\n// identifiers, a standardized format in the form of a 128 bit number.\n//\n// http://tools.ietf.org/html/rfc4122\n\nimport (\n\t\"crypto/rand\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\ntype UUID [16]byte\n\nvar hardwareAddr []byte\nvar clockSeq uint32\n\nconst (\n\tVariantNCSCompat = 0\n\tVariantIETF      = 2\n\tVariantMicrosoft = 6\n\tVariantFuture    = 7\n)\n\nfunc init() {\n\tif interfaces, err := net.Interfaces(); err == nil {\n\t\tfor _, i := range interfaces {\n\t\t\tif i.Flags&net.FlagLoopback == 0 && len(i.HardwareAddr) > 0 {\n\t\t\t\thardwareAddr = i.HardwareAddr\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif hardwareAddr == nil {\n\t\t// If we failed to obtain the MAC address of the current computer,\n\t\t// we will use a randomly generated 6 byte sequence instead and set\n\t\t// the multicast bit as recommended in RFC 4122.\n\t\thardwareAddr = make([]byte, 6)\n\t\t_, err := io.ReadFull(rand.Reader, hardwareAddr)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\thardwareAddr[0] = hardwareAddr[0] | 0x01\n\t}\n\n\t// initialize the clock sequence with a random number\n\tvar clockSeqRand [2]byte\n\tio.ReadFull(rand.Reader, clockSeqRand[:])\n\tclockSeq = uint32(clockSeqRand[1])<<8 | uint32(clockSeqRand[0])\n}\n\n// ParseUUID parses a 32 digit hexadecimal number (that might contain hypens)\n// representing an UUID.\nfunc ParseUUID(input string) (UUID, error) {\n\tvar u UUID\n\tj := 0\n\tfor _, r := range input {\n\t\tswitch {\n\t\tcase r == '-' && j&1 == 0:\n\t\t\tcontinue\n\t\tcase r >= '0' && r <= '9' && j < 32:\n\t\t\tu[j/2] |= byte(r-'0') << uint(4-j&1*4)\n\t\tcase r >= 'a' && r <= 'f' && j < 32:\n\t\t\tu[j/2] |= byte(r-'a'+10) << uint(4-j&1*4)\n\t\tcase r >= 'A' && r <= 'F' && j < 32:\n\t\t\tu[j/2] |= byte(r-'A'+10) << uint(4-j&1*4)\n\t\tdefault:\n\t\t\treturn UUID{}, fmt.Errorf(\"invalid UUID %q\", input)\n\t\t}\n\t\tj += 1\n\t}\n\tif j != 32 {\n\t\treturn UUID{}, fmt.Errorf(\"invalid UUID %q\", input)\n\t}\n\treturn u, nil\n}\n\n// UUIDFromBytes converts a raw byte slice to an UUID.\nfunc UUIDFromBytes(input []byte) (UUID, error) {\n\tvar u UUID\n\tif len(input) != 16 {\n\t\treturn u, errors.New(\"UUIDs must be exactly 16 bytes long\")\n\t}\n\n\tcopy(u[:], input)\n\treturn u, nil\n}\n\nfunc MustRandomUUID() UUID {\n\tuuid, err := RandomUUID()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn uuid\n}\n\n// RandomUUID generates a totally random UUID (version 4) as described in\n// RFC 4122.\nfunc RandomUUID() (UUID, error) {\n\tvar u UUID\n\t_, err := io.ReadFull(rand.Reader, u[:])\n\tif err != nil {\n\t\treturn u, err\n\t}\n\tu[6] &= 0x0F // clear version\n\tu[6] |= 0x40 // set version to 4 (random uuid)\n\tu[8] &= 0x3F // clear variant\n\tu[8] |= 0x80 // set to IETF variant\n\treturn u, nil\n}\n\nvar timeBase = time.Date(1582, time.October, 15, 0, 0, 0, 0, time.UTC).Unix()\n\n// getTimestamp converts time to UUID (version 1) timestamp.\n// It must be an interval of 100-nanoseconds since timeBase.\nfunc getTimestamp(t time.Time) int64 {\n\tutcTime := t.In(time.UTC)\n\tts := int64(utcTime.Unix()-timeBase)*10000000 + int64(utcTime.Nanosecond()/100)\n\n\treturn ts\n}\n\n// TimeUUID generates a new time based UUID (version 1) using the current\n// time as the timestamp.\nfunc TimeUUID() UUID {\n\treturn UUIDFromTime(time.Now())\n}\n\n// The min and max clock values for a UUID.\n//\n// Cassandra's TimeUUIDType compares the lsb parts as signed byte arrays.\n// Thus, the min value for each byte is -128 and the max is +127.\nconst (\n\tminClock = 0x8080\n\tmaxClock = 0x7f7f\n)\n\n// The min and max node values for a UUID.\n//\n// See explanation about Cassandra's TimeUUIDType comparison logic above.\nvar (\n\tminNode = []byte{0x80, 0x80, 0x80, 0x80, 0x80, 0x80}\n\tmaxNode = []byte{0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f}\n)\n\n// MinTimeUUID generates a \"fake\" time based UUID (version 1) which will be\n// the smallest possible UUID generated for the provided timestamp.\n//\n// UUIDs generated by this function are not unique and are mostly suitable only\n// in queries to select a time range of a Cassandra's TimeUUID column.\nfunc MinTimeUUID(t time.Time) UUID {\n\treturn TimeUUIDWith(getTimestamp(t), minClock, minNode)\n}\n\n// MaxTimeUUID generates a \"fake\" time based UUID (version 1) which will be\n// the biggest possible UUID generated for the provided timestamp.\n//\n// UUIDs generated by this function are not unique and are mostly suitable only\n// in queries to select a time range of a Cassandra's TimeUUID column.\nfunc MaxTimeUUID(t time.Time) UUID {\n\treturn TimeUUIDWith(getTimestamp(t), maxClock, maxNode)\n}\n\n// UUIDFromTime generates a new time based UUID (version 1) as described in\n// RFC 4122. This UUID contains the MAC address of the node that generated\n// the UUID, the given timestamp and a sequence number.\nfunc UUIDFromTime(t time.Time) UUID {\n\tts := getTimestamp(t)\n\tclock := atomic.AddUint32(&clockSeq, 1)\n\n\treturn TimeUUIDWith(ts, clock, hardwareAddr)\n}\n\n// TimeUUIDWith generates a new time based UUID (version 1) as described in\n// RFC4122 with given parameters. t is the number of 100's of nanoseconds\n// since 15 Oct 1582 (60bits). clock is the number of clock sequence (14bits).\n// node is a slice to gurarantee the uniqueness of the UUID (up to 6bytes).\n// Note: calling this function does not increment the static clock sequence.\nfunc TimeUUIDWith(t int64, clock uint32, node []byte) UUID {\n\tvar u UUID\n\n\tu[0], u[1], u[2], u[3] = byte(t>>24), byte(t>>16), byte(t>>8), byte(t)\n\tu[4], u[5] = byte(t>>40), byte(t>>32)\n\tu[6], u[7] = byte(t>>56)&0x0F, byte(t>>48)\n\n\tu[8] = byte(clock >> 8)\n\tu[9] = byte(clock)\n\n\tcopy(u[10:], node)\n\n\tu[6] |= 0x10 // set version to 1 (time based uuid)\n\tu[8] &= 0x3F // clear variant\n\tu[8] |= 0x80 // set to IETF variant\n\n\treturn u\n}\n\n// String returns the UUID in it's canonical form, a 32 digit hexadecimal\n// number in the form of xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.\nfunc (u UUID) String() string {\n\tvar offsets = [...]int{0, 2, 4, 6, 9, 11, 14, 16, 19, 21, 24, 26, 28, 30, 32, 34}\n\tconst hexString = \"0123456789abcdef\"\n\tr := make([]byte, 36)\n\tfor i, b := range u {\n\t\tr[offsets[i]] = hexString[b>>4]\n\t\tr[offsets[i]+1] = hexString[b&0xF]\n\t}\n\tr[8] = '-'\n\tr[13] = '-'\n\tr[18] = '-'\n\tr[23] = '-'\n\treturn string(r)\n\n}\n\n// Bytes returns the raw byte slice for this UUID. A UUID is always 128 bits\n// (16 bytes) long.\nfunc (u UUID) Bytes() []byte {\n\treturn u[:]\n}\n\n// Variant returns the variant of this UUID. This package will only generate\n// UUIDs in the IETF variant.\nfunc (u UUID) Variant() int {\n\tx := u[8]\n\tif x&0x80 == 0 {\n\t\treturn VariantNCSCompat\n\t}\n\tif x&0x40 == 0 {\n\t\treturn VariantIETF\n\t}\n\tif x&0x20 == 0 {\n\t\treturn VariantMicrosoft\n\t}\n\treturn VariantFuture\n}\n\n// Version extracts the version of this UUID variant. The RFC 4122 describes\n// five kinds of UUIDs.\nfunc (u UUID) Version() int {\n\treturn int(u[6] & 0xF0 >> 4)\n}\n\n// Node extracts the MAC address of the node who generated this UUID. It will\n// return nil if the UUID is not a time based UUID (version 1).\nfunc (u UUID) Node() []byte {\n\tif u.Version() != 1 {\n\t\treturn nil\n\t}\n\treturn u[10:]\n}\n\n// Clock extracts the clock sequence of this UUID. It will return zero if the\n// UUID is not a time based UUID (version 1).\nfunc (u UUID) Clock() uint32 {\n\tif u.Version() != 1 {\n\t\treturn 0\n\t}\n\n\t// Clock sequence is the lower 14bits of u[8:10]\n\treturn uint32(u[8]&0x3F)<<8 | uint32(u[9])\n}\n\n// Timestamp extracts the timestamp information from a time based UUID\n// (version 1).\nfunc (u UUID) Timestamp() int64 {\n\tif u.Version() != 1 {\n\t\treturn 0\n\t}\n\treturn int64(uint64(u[0])<<24|uint64(u[1])<<16|\n\t\tuint64(u[2])<<8|uint64(u[3])) +\n\t\tint64(uint64(u[4])<<40|uint64(u[5])<<32) +\n\t\tint64(uint64(u[6]&0x0F)<<56|uint64(u[7])<<48)\n}\n\n// Time is like Timestamp, except that it returns a time.Time.\nfunc (u UUID) Time() time.Time {\n\tif u.Version() != 1 {\n\t\treturn time.Time{}\n\t}\n\tt := u.Timestamp()\n\tsec := t / 1e7\n\tnsec := (t % 1e7) * 100\n\treturn time.Unix(sec+timeBase, nsec).UTC()\n}\n\n// Marshaling for JSON\nfunc (u UUID) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + u.String() + `\"`), nil\n}\n\n// Unmarshaling for JSON\nfunc (u *UUID) UnmarshalJSON(data []byte) error {\n\tstr := strings.Trim(string(data), `\"`)\n\tif len(str) > 36 {\n\t\treturn fmt.Errorf(\"invalid JSON UUID %s\", str)\n\t}\n\n\tparsed, err := ParseUUID(str)\n\tif err == nil {\n\t\tcopy(u[:], parsed[:])\n\t}\n\n\treturn err\n}\n\nfunc (u UUID) MarshalText() ([]byte, error) {\n\treturn []byte(u.String()), nil\n}\n\nfunc (u *UUID) UnmarshalText(text []byte) (err error) {\n\t*u, err = ParseUUID(string(text))\n\treturn\n}\n"
        },
        {
          "name": "uuid_test.go",
          "type": "blob",
          "size": 9.2275390625,
          "content": "//go:build all || unit\n// +build all unit\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"bytes\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestUUIDNil(t *testing.T) {\n\tvar uuid UUID\n\twant, got := \"00000000-0000-0000-0000-000000000000\", uuid.String()\n\tif want != got {\n\t\tt.Fatalf(\"TestNil: expected %q got %q\", want, got)\n\t}\n}\n\nvar testsUUID = []struct {\n\tinput   string\n\tvariant int\n\tversion int\n}{\n\t{\"b4f00409-cef8-4822-802c-deb20704c365\", VariantIETF, 4},\n\t{\"B4F00409-CEF8-4822-802C-DEB20704C365\", VariantIETF, 4}, //Use capital letters\n\t{\"f81d4fae-7dec-11d0-a765-00a0c91e6bf6\", VariantIETF, 1},\n\t{\"00000000-7dec-11d0-a765-00a0c91e6bf6\", VariantIETF, 1},\n\t{\"3051a8d7-aea7-1801-e0bf-bc539dd60cf3\", VariantFuture, 1},\n\t{\"3051a8d7-aea7-2801-e0bf-bc539dd60cf3\", VariantFuture, 2},\n\t{\"3051a8d7-aea7-3801-e0bf-bc539dd60cf3\", VariantFuture, 3},\n\t{\"3051a8d7-aea7-4801-e0bf-bc539dd60cf3\", VariantFuture, 4},\n\t{\"3051a8d7-aea7-3801-e0bf-bc539dd60cf3\", VariantFuture, 5},\n\t{\"d0e817e1-e4b1-1801-3fe6-b4b60ccecf9d\", VariantNCSCompat, 0},\n\t{\"d0e817e1-e4b1-1801-bfe6-b4b60ccecf9d\", VariantIETF, 1},\n\t{\"d0e817e1-e4b1-1801-dfe6-b4b60ccecf9d\", VariantMicrosoft, 0},\n\t{\"d0e817e1-e4b1-1801-ffe6-b4b60ccecf9d\", VariantFuture, 0},\n}\n\nfunc TestPredefinedUUID(t *testing.T) {\n\tfor i := range testsUUID {\n\t\tuuid, err := ParseUUID(testsUUID[i].input)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ParseUUID #%d: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif str := uuid.String(); str != strings.ToLower(testsUUID[i].input) {\n\t\t\tt.Errorf(\"String #%d: expected %q got %q\", i, testsUUID[i].input, str)\n\t\t\tcontinue\n\t\t}\n\n\t\tif variant := uuid.Variant(); variant != testsUUID[i].variant {\n\t\t\tt.Errorf(\"Variant #%d: expected %d got %d\", i, testsUUID[i].variant, variant)\n\t\t}\n\n\t\tif testsUUID[i].variant == VariantIETF {\n\t\t\tif version := uuid.Version(); version != testsUUID[i].version {\n\t\t\t\tt.Errorf(\"Version #%d: expected %d got %d\", i, testsUUID[i].version, version)\n\t\t\t}\n\t\t}\n\n\t\tjson, err := uuid.MarshalJSON()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"MarshalJSON #%d: %v\", i, err)\n\t\t}\n\t\texpectedJson := `\"` + strings.ToLower(testsUUID[i].input) + `\"`\n\t\tif string(json) != expectedJson {\n\t\t\tt.Errorf(\"MarshalJSON #%d: expected %v got %v\", i, expectedJson, string(json))\n\t\t}\n\n\t\tvar unmarshaled UUID\n\t\terr = unmarshaled.UnmarshalJSON(json)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"UnmarshalJSON #%d: %v\", i, err)\n\t\t}\n\t\tif unmarshaled != uuid {\n\t\t\tt.Errorf(\"UnmarshalJSON #%d: expected %v got %v\", i, uuid, unmarshaled)\n\t\t}\n\t}\n}\n\nfunc TestInvalidUUIDCharacter(t *testing.T) {\n\t_, err := ParseUUID(\"z4f00409-cef8-4822-802c-deb20704c365\")\n\tif err == nil || !strings.Contains(err.Error(), \"invalid UUID\") {\n\t\tt.Fatalf(\"expected invalid UUID error, got '%v' \", err)\n\t}\n}\n\nfunc TestInvalidUUIDLength(t *testing.T) {\n\t_, err := ParseUUID(\"4f00\")\n\tif err == nil || !strings.Contains(err.Error(), \"invalid UUID\") {\n\t\tt.Fatalf(\"expected invalid UUID error, got '%v' \", err)\n\t}\n\n\t_, err = UUIDFromBytes(TimeUUID().Bytes()[:15])\n\tif err == nil || err.Error() != \"UUIDs must be exactly 16 bytes long\" {\n\t\tt.Fatalf(\"expected error '%v', got '%v'\", \"UUIDs must be exactly 16 bytes long\", err)\n\t}\n}\n\nfunc TestRandomUUID(t *testing.T) {\n\tfor i := 0; i < 20; i++ {\n\t\tuuid, err := RandomUUID()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"RandomUUID: %v\", err)\n\t\t}\n\t\tif variant := uuid.Variant(); variant != VariantIETF {\n\t\t\tt.Errorf(\"wrong variant. expected %d got %d\", VariantIETF, variant)\n\t\t}\n\t\tif version := uuid.Version(); version != 4 {\n\t\t\tt.Errorf(\"wrong version. expected %d got %d\", 4, version)\n\t\t}\n\t}\n}\n\nfunc TestRandomUUIDInvalidAPICalls(t *testing.T) {\n\tuuid, err := RandomUUID()\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error %v\", err)\n\t}\n\n\tif node := uuid.Node(); node != nil {\n\t\tt.Fatalf(\"expected nil, got %v\", node)\n\t}\n\n\tif stamp := uuid.Timestamp(); stamp != 0 {\n\t\tt.Fatalf(\"expceted 0, got %v\", stamp)\n\t}\n\tzeroT := time.Time{}\n\tif to := uuid.Time(); to != zeroT {\n\t\tt.Fatalf(\"expected %v, got %v\", zeroT, to)\n\t}\n}\n\nfunc TestUUIDFromTime(t *testing.T) {\n\tdate := time.Date(1982, 5, 5, 12, 34, 56, 400, time.UTC)\n\tuuid := UUIDFromTime(date)\n\n\tif uuid.Time() != date {\n\t\tt.Errorf(\"embedded time incorrect. Expected %v got %v\", date, uuid.Time())\n\t}\n}\n\nfunc TestTimeUUIDWith(t *testing.T) {\n\tutcTime := time.Date(1982, 5, 5, 12, 34, 56, 400, time.UTC)\n\tts := int64(utcTime.Unix()-timeBase)*10000000 + int64(utcTime.Nanosecond()/100)\n\tclockSeq := uint32(0x3FFF)           // Max number of clock sequence.\n\tnode := [7]byte{0, 1, 2, 3, 4, 5, 6} // The last element should be ignored.\n\tuuid := TimeUUIDWith(ts, clockSeq, node[:])\n\n\tif got := uuid.Variant(); got != VariantIETF {\n\t\tt.Errorf(\"wrong variant. expected %d got %d\", VariantIETF, got)\n\t}\n\tif got, want := uuid.Version(), 1; got != want {\n\t\tt.Errorf(\"wrong version. Expected %v got %v\", want, got)\n\t}\n\tif got := uuid.Timestamp(); got != int64(ts) {\n\t\tt.Errorf(\"wrong timestamp. Expected %v got %v\", ts, got)\n\t}\n\tif got := uuid.Clock(); uint32(got) != clockSeq {\n\t\tt.Errorf(\"wrong clock. expected %v got %v\", clockSeq, got)\n\t}\n\tif got, want := uuid.Node(), node[:6]; !bytes.Equal(got, want) {\n\t\tt.Errorf(\"wrong node. expected %x, bot %x\", want, got)\n\t}\n}\n\nfunc TestParseUUID(t *testing.T) {\n\tuuid, _ := ParseUUID(\"486f3a88-775b-11e3-ae07-d231feb1dc81\")\n\tif uuid.Time() != time.Date(2014, 1, 7, 5, 19, 29, 222516000, time.UTC) {\n\t\tt.Errorf(\"Expected date of 1/7/2014 at 5:19:29.222516, got %v\", uuid.Time())\n\t}\n}\n\nfunc TestTimeUUID(t *testing.T) {\n\tvar node []byte\n\ttimestamp := int64(0)\n\tfor i := 0; i < 20; i++ {\n\t\tuuid := TimeUUID()\n\n\t\tif variant := uuid.Variant(); variant != VariantIETF {\n\t\t\tt.Errorf(\"wrong variant. expected %d got %d\", VariantIETF, variant)\n\t\t}\n\t\tif version := uuid.Version(); version != 1 {\n\t\t\tt.Errorf(\"wrong version. expected %d got %d\", 1, version)\n\t\t}\n\n\t\tif n := uuid.Node(); !bytes.Equal(n, node) && i > 0 {\n\t\t\tt.Errorf(\"wrong node. expected %x, got %x\", node, n)\n\t\t} else if i == 0 {\n\t\t\tnode = n\n\t\t}\n\n\t\tts := uuid.Timestamp()\n\t\tif ts < timestamp {\n\t\t\tt.Errorf(\"timestamps must grow: timestamp=%v ts=%v\", timestamp, ts)\n\t\t}\n\t\ttimestamp = ts\n\t}\n}\n\nfunc TestUnmarshalJSON(t *testing.T) {\n\tvar withHyphens, withoutHypens, tooLong UUID\n\n\twithHyphens.UnmarshalJSON([]byte(`\"486f3a88-775b-11e3-ae07-d231feb1dc81\"`))\n\tif withHyphens.Time().Truncate(time.Second) != time.Date(2014, 1, 7, 5, 19, 29, 0, time.UTC) {\n\t\tt.Errorf(\"Expected date of 1/7/2014 at 5:19:29, got %v\", withHyphens.Time())\n\t}\n\n\twithoutHypens.UnmarshalJSON([]byte(`\"486f3a88775b11e3ae07d231feb1dc81\"`))\n\tif withoutHypens.Time().Truncate(time.Second) != time.Date(2014, 1, 7, 5, 19, 29, 0, time.UTC) {\n\t\tt.Errorf(\"Expected date of 1/7/2014 at 5:19:29, got %v\", withoutHypens.Time())\n\t}\n\n\terr := tooLong.UnmarshalJSON([]byte(`\"486f3a88-775b-11e3-ae07-d231feb1dc81486f3a88\"`))\n\tif err == nil {\n\t\tt.Errorf(\"no error for invalid JSON UUID\")\n\t}\n\n}\n\nfunc TestMarshalText(t *testing.T) {\n\tu, err := ParseUUID(\"486f3a88-775b-11e3-ae07-d231feb1dc81\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttext, err := u.MarshalText()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar u2 UUID\n\tif err := u2.UnmarshalText(text); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif u != u2 {\n\t\tt.Fatalf(\"uuids not equal after marshalling: before=%s after=%s\", u, u2)\n\t}\n}\n\nfunc TestMinTimeUUID(t *testing.T) {\n\taTime := time.Now()\n\tminTimeUUID := MinTimeUUID(aTime)\n\n\tts := aTime.Unix()\n\ttsFromUUID := minTimeUUID.Time().Unix()\n\tif ts != tsFromUUID {\n\t\tt.Errorf(\"timestamps are not equal: expected %d, got %d\", ts, tsFromUUID)\n\t}\n\n\tclockFromUUID := minTimeUUID.Clock()\n\t// clear two most significant bits, as they are used for IETF variant\n\tif minClock&0x3FFF != clockFromUUID {\n\t\tt.Errorf(\"clocks are not equal: expected %08b, got %08b\", minClock&0x3FFF, clockFromUUID)\n\t}\n\n\tnodeFromUUID := minTimeUUID.Node()\n\tif !bytes.Equal(minNode, nodeFromUUID) {\n\t\tt.Errorf(\"nodes are not equal: expected %08b, got %08b\", minNode, nodeFromUUID)\n\t}\n}\n\nfunc TestMaxTimeUUID(t *testing.T) {\n\taTime := time.Now()\n\tmaxTimeUUID := MaxTimeUUID(aTime)\n\n\tts := aTime.Unix()\n\ttsFromUUID := maxTimeUUID.Time().Unix()\n\tif ts != tsFromUUID {\n\t\tt.Errorf(\"timestamps are not equal: expected %d, got %d\", ts, tsFromUUID)\n\t}\n\n\tclockFromUUID := maxTimeUUID.Clock()\n\tif maxClock&0x3FFF != clockFromUUID {\n\t\tt.Errorf(\"clocks are not equal: expected %08b, got %08b\", maxClock&0x3FFF, clockFromUUID)\n\t}\n\n\tnodeFromUUID := maxTimeUUID.Node()\n\tif !bytes.Equal(maxNode, nodeFromUUID) {\n\t\tt.Errorf(\"nodes are not equal:  expected %08b, got %08b\", maxNode, nodeFromUUID)\n\t}\n}\n"
        },
        {
          "name": "version.go",
          "type": "blob",
          "size": 2.0498046875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport \"runtime/debug\"\n\nconst (\n\tdefaultDriverName = \"github.com/apache/cassandra-gocql-driver\"\n\n\t// This string MUST have this value since we explicitly test against the\n\t// current main package returned by runtime/debug below.  Also note the\n\t// package name used here may change in a future (2.x) release; in that case\n\t// this constant will be updated as well.\n\tmainPackage = \"github.com/gocql/gocql\"\n)\n\nvar driverName string\n\nvar driverVersion string\n\nfunc init() {\n\tbuildInfo, ok := debug.ReadBuildInfo()\n\tif ok {\n\t\tfor _, d := range buildInfo.Deps {\n\t\t\tif d.Path == mainPackage {\n\t\t\t\tdriverName = defaultDriverName\n\t\t\t\tdriverVersion = d.Version\n\t\t\t\t// If there's a replace directive in play for the gocql package\n\t\t\t\t// then use that information for path and version instead.  This\n\t\t\t\t// will allow forks or other local packages to clearly identify\n\t\t\t\t// themselves as distinct from mainPackage above.\n\t\t\t\tif d.Replace != nil {\n\t\t\t\t\tdriverName = d.Replace.Path\n\t\t\t\t\tdriverVersion = d.Replace.Version\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "wiki_test.go",
          "type": "blob",
          "size": 7.5478515625,
          "content": "//go:build all || cassandra\n// +build all cassandra\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n/*\n * Content before git sha 34fdeebefcbf183ed7f916f931aa0586fdaa1b40\n * Copyright (c) 2016, The Gocql authors,\n * provided under the BSD-3-Clause License.\n * See the NOTICE file distributed with this work for additional information.\n */\n\npackage gocql\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"sort\"\n\t\"testing\"\n\t\"time\"\n\n\t\"gopkg.in/inf.v0\"\n)\n\ntype WikiPage struct {\n\tTitle       string\n\tRevId       UUID\n\tBody        string\n\tViews       int64\n\tProtected   bool\n\tModified    time.Time\n\tRating      *inf.Dec\n\tTags        []string\n\tAttachments map[string]WikiAttachment\n}\n\ntype WikiAttachment []byte\n\nvar wikiTestData = []*WikiPage{\n\t{\n\t\tTitle:    \"Frontpage\",\n\t\tRevId:    TimeUUID(),\n\t\tBody:     \"Welcome to this wiki page!\",\n\t\tRating:   inf.NewDec(131, 3),\n\t\tModified: time.Date(2013, time.August, 13, 9, 52, 3, 0, time.UTC),\n\t\tTags:     []string{\"start\", \"important\", \"test\"},\n\t\tAttachments: map[string]WikiAttachment{\n\t\t\t\"logo\":    WikiAttachment(\"\\x00company logo\\x00\"),\n\t\t\t\"favicon\": WikiAttachment(\"favicon.ico\"),\n\t\t},\n\t},\n\t{\n\t\tTitle:    \"Foobar\",\n\t\tRevId:    TimeUUID(),\n\t\tBody:     \"foo::Foo f = new foo::Foo(foo::Foo::INIT);\",\n\t\tModified: time.Date(2013, time.August, 13, 9, 52, 3, 0, time.UTC),\n\t},\n}\n\ntype WikiTest struct {\n\tsession *Session\n\ttb      testing.TB\n\n\ttable string\n}\n\nfunc CreateSchema(session *Session, tb testing.TB, table string) *WikiTest {\n\ttable = \"wiki_\" + table\n\tif err := createTable(session, fmt.Sprintf(\"DROP TABLE IF EXISTS gocql_test.%s\", table)); err != nil {\n\t\ttb.Fatal(\"CreateSchema:\", err)\n\t}\n\n\terr := createTable(session, fmt.Sprintf(`CREATE TABLE gocql_test.%s (\n\t\t\ttitle       varchar,\n\t\t\trevid       timeuuid,\n\t\t\tbody        varchar,\n\t\t\tviews       bigint,\n\t\t\tprotected   boolean,\n\t\t\tmodified    timestamp,\n\t\t\trating      decimal,\n\t\t\ttags        set<varchar>,\n\t\t\tattachments map<varchar, blob>,\n\t\t\tPRIMARY KEY (title, revid)\n\t\t)`, table))\n\n\tif err != nil {\n\t\ttb.Fatal(\"CreateSchema:\", err)\n\t}\n\n\treturn &WikiTest{\n\t\tsession: session,\n\t\ttb:      tb,\n\t\ttable:   table,\n\t}\n}\n\nfunc (w *WikiTest) CreatePages(n int) {\n\tvar page WikiPage\n\tt0 := time.Now()\n\tfor i := 0; i < n; i++ {\n\t\tpage.Title = fmt.Sprintf(\"generated_%d\", (i&16)+1)\n\t\tpage.Modified = t0.Add(time.Duration(i-n) * time.Minute)\n\t\tpage.RevId = UUIDFromTime(page.Modified)\n\t\tpage.Body = fmt.Sprintf(\"text %d\", i)\n\t\tif err := w.InsertPage(&page); err != nil {\n\t\t\tw.tb.Error(\"CreatePages:\", err)\n\t\t}\n\t}\n}\n\nfunc (w *WikiTest) InsertPage(page *WikiPage) error {\n\treturn w.session.Query(fmt.Sprintf(`INSERT INTO %s\n\t\t(title, revid, body, views, protected, modified, rating, tags, attachments)\n\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)`, w.table),\n\t\tpage.Title, page.RevId, page.Body, page.Views, page.Protected,\n\t\tpage.Modified, page.Rating, page.Tags, page.Attachments).Exec()\n}\n\nfunc (w *WikiTest) SelectPage(page *WikiPage, title string, revid UUID) error {\n\treturn w.session.Query(fmt.Sprintf(`SELECT title, revid, body, views, protected,\n\t\tmodified,tags, attachments, rating\n\t\tFROM %s WHERE title = ? AND revid = ? LIMIT 1`, w.table),\n\t\ttitle, revid).Scan(&page.Title, &page.RevId,\n\t\t&page.Body, &page.Views, &page.Protected, &page.Modified, &page.Tags,\n\t\t&page.Attachments, &page.Rating)\n}\n\nfunc (w *WikiTest) GetPageCount() int {\n\tvar count int\n\tif err := w.session.Query(fmt.Sprintf(`SELECT COUNT(*) FROM %s`, w.table)).Scan(&count); err != nil {\n\t\tw.tb.Error(\"GetPageCount\", err)\n\t}\n\treturn count\n}\n\nfunc TestWikiCreateSchema(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tCreateSchema(session, t, \"create\")\n}\n\nfunc BenchmarkWikiCreateSchema(b *testing.B) {\n\tb.StopTimer()\n\tsession := createSession(b)\n\tdefer func() {\n\t\tb.StopTimer()\n\t\tsession.Close()\n\t}()\n\n\tb.StartTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tCreateSchema(session, b, \"bench_create\")\n\t}\n}\n\nfunc TestWikiCreatePages(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tw := CreateSchema(session, t, \"create_pages\")\n\n\tnumPages := 5\n\tw.CreatePages(numPages)\n\tif count := w.GetPageCount(); count != numPages {\n\t\tt.Errorf(\"expected %d pages, got %d pages.\", numPages, count)\n\t}\n}\n\nfunc BenchmarkWikiCreatePages(b *testing.B) {\n\tb.StopTimer()\n\tsession := createSession(b)\n\tdefer func() {\n\t\tb.StopTimer()\n\t\tsession.Close()\n\t}()\n\n\tw := CreateSchema(session, b, \"bench_create_pages\")\n\n\tb.StartTimer()\n\n\tw.CreatePages(b.N)\n}\n\nfunc BenchmarkWikiSelectAllPages(b *testing.B) {\n\tb.StopTimer()\n\tsession := createSession(b)\n\tdefer func() {\n\t\tb.StopTimer()\n\t\tsession.Close()\n\t}()\n\tw := CreateSchema(session, b, \"bench_select_all\")\n\n\tw.CreatePages(100)\n\tb.StartTimer()\n\n\tvar page WikiPage\n\tfor i := 0; i < b.N; i++ {\n\t\titer := session.Query(fmt.Sprintf(`SELECT title, revid, body, views, protected,\n\t\t\tmodified, tags, attachments, rating\n\t\t\tFROM %s`, w.table)).Iter()\n\t\tfor iter.Scan(&page.Title, &page.RevId, &page.Body, &page.Views,\n\t\t\t&page.Protected, &page.Modified, &page.Tags, &page.Attachments,\n\t\t\t&page.Rating) {\n\t\t\t// pass\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkWikiSelectSinglePage(b *testing.B) {\n\tb.StopTimer()\n\tsession := createSession(b)\n\tdefer func() {\n\t\tb.StopTimer()\n\t\tsession.Close()\n\t}()\n\tw := CreateSchema(session, b, \"bench_select_single\")\n\tpages := make([]WikiPage, 100)\n\tw.CreatePages(len(pages))\n\titer := session.Query(fmt.Sprintf(`SELECT title, revid FROM %s`, w.table)).Iter()\n\tfor i := 0; i < len(pages); i++ {\n\t\tif !iter.Scan(&pages[i].Title, &pages[i].RevId) {\n\t\t\tpages = pages[:i]\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tb.Error(err)\n\t}\n\tb.StartTimer()\n\n\tvar page WikiPage\n\tfor i := 0; i < b.N; i++ {\n\t\tp := &pages[i%len(pages)]\n\t\tif err := w.SelectPage(&page, p.Title, p.RevId); err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkWikiSelectPageCount(b *testing.B) {\n\tb.StopTimer()\n\tsession := createSession(b)\n\tdefer func() {\n\t\tb.StopTimer()\n\t\tsession.Close()\n\t}()\n\n\tw := CreateSchema(session, b, \"bench_page_count\")\n\tconst numPages = 10\n\tw.CreatePages(numPages)\n\tb.StartTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif count := w.GetPageCount(); count != numPages {\n\t\t\tb.Errorf(\"expected %d pages, got %d pages.\", numPages, count)\n\t\t}\n\t}\n}\n\nfunc TestWikiTypicalCRUD(t *testing.T) {\n\tsession := createSession(t)\n\tdefer session.Close()\n\n\tw := CreateSchema(session, t, \"crud\")\n\n\tfor _, page := range wikiTestData {\n\t\tif err := w.InsertPage(page); err != nil {\n\t\t\tt.Error(\"InsertPage:\", err)\n\t\t}\n\t}\n\tif count := w.GetPageCount(); count != len(wikiTestData) {\n\t\tt.Errorf(\"count: expected %d, got %d\\n\", len(wikiTestData), count)\n\t}\n\tfor _, original := range wikiTestData {\n\t\tpage := new(WikiPage)\n\t\tif err := w.SelectPage(page, original.Title, original.RevId); err != nil {\n\t\t\tt.Error(\"SelectPage:\", err)\n\t\t\tcontinue\n\t\t}\n\t\tsort.Sort(sort.StringSlice(page.Tags))\n\t\tsort.Sort(sort.StringSlice(original.Tags))\n\t\tif !reflect.DeepEqual(page, original) {\n\t\t\tt.Errorf(\"page: expected %#v, got %#v\\n\", original, page)\n\t\t}\n\t}\n}\n"
        }
      ]
    }
  ]
}