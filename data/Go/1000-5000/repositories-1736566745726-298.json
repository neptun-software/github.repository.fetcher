{
  "metadata": {
    "timestamp": 1736566745726,
    "page": 298,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hashicorp/memberlist",
      "stars": 3707,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.263671875,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n.vagrant/\n\n"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 2.712890625,
          "content": "# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: MPL-2.0\n\nlinters:\n  disable-all: true\n  enable:\n    # (TODO) uncomment after fixing the lint issues\n    # - gofmt\n    - govet\n    # - unconvert\n    # - staticcheck\n    # - ineffassign\n    # - unparam\n    - forbidigo\n\nissues:\n  # Disable the default exclude list so that all excludes are explicitly\n  # defined in this file.\n  exclude-use-default: false\n\n  exclude-rules:\n    # Temp Ignore SA9004: only the first constant in this group has an explicit type\n    # https://staticcheck.io/docs/checks#SA9004\n    - linters: [staticcheck]\n      text: 'SA9004:'\n\n    - linters: [staticcheck]\n      text: 'SA1019: Package github.com/golang/protobuf/jsonpb is deprecated'\n\n    - linters: [staticcheck]\n      text: 'SA1019: Package github.com/golang/protobuf/proto is deprecated'\n\n    - linters: [staticcheck]\n      text: 'SA1019: ptypes.MarshalAny is deprecated'\n\n    - linters: [staticcheck]\n      text: 'SA1019: ptypes.UnmarshalAny is deprecated'\n\n    - linters: [staticcheck]\n      text: 'SA1019: package github.com/golang/protobuf/ptypes is deprecated'  \n\n    # An argument that always receives the same value is often not a problem.\n    - linters: [unparam]\n      text: 'always receives'\n\n    # Often functions will implement an interface that returns an error without\n    # needing to return an error. Sometimes the error return value is unnecessary\n    # but a linter can not tell the difference.\n    - linters: [unparam]\n      text: 'result \\d+ \\(error\\) is always nil'\n\n    # Allow unused parameters to start with an underscore. Arguments with a name\n    # of '_' are already ignored.\n    # Ignoring longer names that start with underscore allow for better\n    # self-documentation than a single underscore by itself.  Underscore arguments\n    # should generally only be used when a function is implementing an interface.\n    - linters: [unparam]\n      text: '`_[^`]*` is unused'\n\n    # Temp ignore some common unused parameters so that unparam can be added\n    # incrementally.\n    - linters: [unparam]\n      text: '`(t|resp|req|entMeta)` is unused'\n\nlinters-settings:\n  gofmt:\n    simplify: true\n  forbidigo:\n    # Forbid the following identifiers (list of regexp).\n    forbid:\n      - '\\brequire\\.New\\b(# Use package-level functions with explicit TestingT)?'\n      - '\\bassert\\.New\\b(# Use package-level functions with explicit TestingT)?'\n      - '\\bmetrics\\.IncrCounter\\b(# Use labeled metrics)?'\n      - '\\bmetrics\\.AddSample\\b(# Use labeled metrics)?'\n      - '\\bmetrics\\.MeasureSince\\b(# Use labeled metrics)?'\n      - '\\bmetrics\\.SetGauge\\b(# Use labeled metrics)?'\n    # Exclude godoc examples from forbidigo checks.\n    # Default: true\n    exclude_godoc_examples: false\n\nrun:\n  timeout: 10m\n  concurrency: 4\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 15.6376953125,
          "content": "Copyright (c) 2013 HashiCorp, Inc.\n\nMozilla Public License, version 2.0\n\n1. Definitions\n\n1.1. “Contributor”\n\n     means each individual or legal entity that creates, contributes to the\n     creation of, or owns Covered Software.\n\n1.2. “Contributor Version”\n\n     means the combination of the Contributions of others (if any) used by a\n     Contributor and that particular Contributor’s Contribution.\n\n1.3. “Contribution”\n\n     means Covered Software of a particular Contributor.\n\n1.4. “Covered Software”\n\n     means Source Code Form to which the initial Contributor has attached the\n     notice in Exhibit A, the Executable Form of such Source Code Form, and\n     Modifications of such Source Code Form, in each case including portions\n     thereof.\n\n1.5. “Incompatible With Secondary Licenses”\n     means\n\n     a. that the initial Contributor has attached the notice described in\n        Exhibit B to the Covered Software; or\n\n     b. that the Covered Software was made available under the terms of version\n        1.1 or earlier of the License, but not also under the terms of a\n        Secondary License.\n\n1.6. “Executable Form”\n\n     means any form of the work other than Source Code Form.\n\n1.7. “Larger Work”\n\n     means a work that combines Covered Software with other material, in a separate\n     file or files, that is not Covered Software.\n\n1.8. “License”\n\n     means this document.\n\n1.9. “Licensable”\n\n     means having the right to grant, to the maximum extent possible, whether at the\n     time of the initial grant or subsequently, any and all of the rights conveyed by\n     this License.\n\n1.10. “Modifications”\n\n     means any of the following:\n\n     a. any file in Source Code Form that results from an addition to, deletion\n        from, or modification of the contents of Covered Software; or\n\n     b. any new file in Source Code Form that contains any Covered Software.\n\n1.11. “Patent Claims” of a Contributor\n\n      means any patent claim(s), including without limitation, method, process,\n      and apparatus claims, in any patent Licensable by such Contributor that\n      would be infringed, but for the grant of the License, by the making,\n      using, selling, offering for sale, having made, import, or transfer of\n      either its Contributions or its Contributor Version.\n\n1.12. “Secondary License”\n\n      means either the GNU General Public License, Version 2.0, the GNU Lesser\n      General Public License, Version 2.1, the GNU Affero General Public\n      License, Version 3.0, or any later versions of those licenses.\n\n1.13. “Source Code Form”\n\n      means the form of the work preferred for making modifications.\n\n1.14. “You” (or “Your”)\n\n      means an individual or a legal entity exercising rights under this\n      License. For legal entities, “You” includes any entity that controls, is\n      controlled by, or is under common control with You. For purposes of this\n      definition, “control” means (a) the power, direct or indirect, to cause\n      the direction or management of such entity, whether by contract or\n      otherwise, or (b) ownership of more than fifty percent (50%) of the\n      outstanding shares or beneficial ownership of such entity.\n\n\n2. License Grants and Conditions\n\n2.1. Grants\n\n     Each Contributor hereby grants You a world-wide, royalty-free,\n     non-exclusive license:\n\n     a. under intellectual property rights (other than patent or trademark)\n        Licensable by such Contributor to use, reproduce, make available,\n        modify, display, perform, distribute, and otherwise exploit its\n        Contributions, either on an unmodified basis, with Modifications, or as\n        part of a Larger Work; and\n\n     b. under Patent Claims of such Contributor to make, use, sell, offer for\n        sale, have made, import, and otherwise transfer either its Contributions\n        or its Contributor Version.\n\n2.2. Effective Date\n\n     The licenses granted in Section 2.1 with respect to any Contribution become\n     effective for each Contribution on the date the Contributor first distributes\n     such Contribution.\n\n2.3. Limitations on Grant Scope\n\n     The licenses granted in this Section 2 are the only rights granted under this\n     License. No additional rights or licenses will be implied from the distribution\n     or licensing of Covered Software under this License. Notwithstanding Section\n     2.1(b) above, no patent license is granted by a Contributor:\n\n     a. for any code that a Contributor has removed from Covered Software; or\n\n     b. for infringements caused by: (i) Your and any other third party’s\n        modifications of Covered Software, or (ii) the combination of its\n        Contributions with other software (except as part of its Contributor\n        Version); or\n\n     c. under Patent Claims infringed by Covered Software in the absence of its\n        Contributions.\n\n     This License does not grant any rights in the trademarks, service marks, or\n     logos of any Contributor (except as may be necessary to comply with the\n     notice requirements in Section 3.4).\n\n2.4. Subsequent Licenses\n\n     No Contributor makes additional grants as a result of Your choice to\n     distribute the Covered Software under a subsequent version of this License\n     (see Section 10.2) or under the terms of a Secondary License (if permitted\n     under the terms of Section 3.3).\n\n2.5. Representation\n\n     Each Contributor represents that the Contributor believes its Contributions\n     are its original creation(s) or it has sufficient rights to grant the\n     rights to its Contributions conveyed by this License.\n\n2.6. Fair Use\n\n     This License is not intended to limit any rights You have under applicable\n     copyright doctrines of fair use, fair dealing, or other equivalents.\n\n2.7. Conditions\n\n     Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in\n     Section 2.1.\n\n\n3. Responsibilities\n\n3.1. Distribution of Source Form\n\n     All distribution of Covered Software in Source Code Form, including any\n     Modifications that You create or to which You contribute, must be under the\n     terms of this License. You must inform recipients that the Source Code Form\n     of the Covered Software is governed by the terms of this License, and how\n     they can obtain a copy of this License. You may not attempt to alter or\n     restrict the recipients’ rights in the Source Code Form.\n\n3.2. Distribution of Executable Form\n\n     If You distribute Covered Software in Executable Form then:\n\n     a. such Covered Software must also be made available in Source Code Form,\n        as described in Section 3.1, and You must inform recipients of the\n        Executable Form how they can obtain a copy of such Source Code Form by\n        reasonable means in a timely manner, at a charge no more than the cost\n        of distribution to the recipient; and\n\n     b. You may distribute such Executable Form under the terms of this License,\n        or sublicense it under different terms, provided that the license for\n        the Executable Form does not attempt to limit or alter the recipients’\n        rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\n\n     You may create and distribute a Larger Work under terms of Your choice,\n     provided that You also comply with the requirements of this License for the\n     Covered Software. If the Larger Work is a combination of Covered Software\n     with a work governed by one or more Secondary Licenses, and the Covered\n     Software is not Incompatible With Secondary Licenses, this License permits\n     You to additionally distribute such Covered Software under the terms of\n     such Secondary License(s), so that the recipient of the Larger Work may, at\n     their option, further distribute the Covered Software under the terms of\n     either this License or such Secondary License(s).\n\n3.4. Notices\n\n     You may not remove or alter the substance of any license notices (including\n     copyright notices, patent notices, disclaimers of warranty, or limitations\n     of liability) contained within the Source Code Form of the Covered\n     Software, except that You may alter any license notices to the extent\n     required to remedy known factual inaccuracies.\n\n3.5. Application of Additional Terms\n\n     You may choose to offer, and to charge a fee for, warranty, support,\n     indemnity or liability obligations to one or more recipients of Covered\n     Software. However, You may do so only on Your own behalf, and not on behalf\n     of any Contributor. You must make it absolutely clear that any such\n     warranty, support, indemnity, or liability obligation is offered by You\n     alone, and You hereby agree to indemnify every Contributor for any\n     liability incurred by such Contributor as a result of warranty, support,\n     indemnity or liability terms You offer. You may include additional\n     disclaimers of warranty and limitations of liability specific to any\n     jurisdiction.\n\n4. Inability to Comply Due to Statute or Regulation\n\n   If it is impossible for You to comply with any of the terms of this License\n   with respect to some or all of the Covered Software due to statute, judicial\n   order, or regulation then You must: (a) comply with the terms of this License\n   to the maximum extent possible; and (b) describe the limitations and the code\n   they affect. Such description must be placed in a text file included with all\n   distributions of the Covered Software under this License. Except to the\n   extent prohibited by statute or regulation, such description must be\n   sufficiently detailed for a recipient of ordinary skill to be able to\n   understand it.\n\n5. Termination\n\n5.1. The rights granted under this License will terminate automatically if You\n     fail to comply with any of its terms. However, if You become compliant,\n     then the rights granted under this License from a particular Contributor\n     are reinstated (a) provisionally, unless and until such Contributor\n     explicitly and finally terminates Your grants, and (b) on an ongoing basis,\n     if such Contributor fails to notify You of the non-compliance by some\n     reasonable means prior to 60 days after You have come back into compliance.\n     Moreover, Your grants from a particular Contributor are reinstated on an\n     ongoing basis if such Contributor notifies You of the non-compliance by\n     some reasonable means, this is the first time You have received notice of\n     non-compliance with this License from such Contributor, and You become\n     compliant prior to 30 days after Your receipt of the notice.\n\n5.2. If You initiate litigation against any entity by asserting a patent\n     infringement claim (excluding declaratory judgment actions, counter-claims,\n     and cross-claims) alleging that a Contributor Version directly or\n     indirectly infringes any patent, then the rights granted to You by any and\n     all Contributors for the Covered Software under Section 2.1 of this License\n     shall terminate.\n\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user\n     license agreements (excluding distributors and resellers) which have been\n     validly granted by You or Your distributors under this License prior to\n     termination shall survive termination.\n\n6. Disclaimer of Warranty\n\n   Covered Software is provided under this License on an “as is” basis, without\n   warranty of any kind, either expressed, implied, or statutory, including,\n   without limitation, warranties that the Covered Software is free of defects,\n   merchantable, fit for a particular purpose or non-infringing. The entire\n   risk as to the quality and performance of the Covered Software is with You.\n   Should any Covered Software prove defective in any respect, You (not any\n   Contributor) assume the cost of any necessary servicing, repair, or\n   correction. This disclaimer of warranty constitutes an essential part of this\n   License. No use of  any Covered Software is authorized under this License\n   except under this disclaimer.\n\n7. Limitation of Liability\n\n   Under no circumstances and under no legal theory, whether tort (including\n   negligence), contract, or otherwise, shall any Contributor, or anyone who\n   distributes Covered Software as permitted above, be liable to You for any\n   direct, indirect, special, incidental, or consequential damages of any\n   character including, without limitation, damages for lost profits, loss of\n   goodwill, work stoppage, computer failure or malfunction, or any and all\n   other commercial damages or losses, even if such party shall have been\n   informed of the possibility of such damages. This limitation of liability\n   shall not apply to liability for death or personal injury resulting from such\n   party’s negligence to the extent applicable law prohibits such limitation.\n   Some jurisdictions do not allow the exclusion or limitation of incidental or\n   consequential damages, so this exclusion and limitation may not apply to You.\n\n8. Litigation\n\n   Any litigation relating to this License may be brought only in the courts of\n   a jurisdiction where the defendant maintains its principal place of business\n   and such litigation shall be governed by laws of that jurisdiction, without\n   reference to its conflict-of-law provisions. Nothing in this Section shall\n   prevent a party’s ability to bring cross-claims or counter-claims.\n\n9. Miscellaneous\n\n   This License represents the complete agreement concerning the subject matter\n   hereof. If any provision of this License is held to be unenforceable, such\n   provision shall be reformed only to the extent necessary to make it\n   enforceable. Any law or regulation which provides that the language of a\n   contract shall be construed against the drafter shall not be used to construe\n   this License against a Contributor.\n\n\n10. Versions of the License\n\n10.1. New Versions\n\n      Mozilla Foundation is the license steward. Except as provided in Section\n      10.3, no one other than the license steward has the right to modify or\n      publish new versions of this License. Each version will be given a\n      distinguishing version number.\n\n10.2. Effect of New Versions\n\n      You may distribute the Covered Software under the terms of the version of\n      the License under which You originally received the Covered Software, or\n      under the terms of any subsequent version published by the license\n      steward.\n\n10.3. Modified Versions\n\n      If you create software not governed by this License, and you want to\n      create a new license for such software, you may create and use a modified\n      version of this License if you rename the license and remove any\n      references to the name of the license steward (except to note that such\n      modified license differs from this License).\n\n10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses\n      If You choose to distribute Source Code Form that is Incompatible With\n      Secondary Licenses under the terms of this version of the License, the\n      notice described in Exhibit B of this License must be attached.\n\nExhibit A - Source Code Form License Notice\n\n      This Source Code Form is subject to the\n      terms of the Mozilla Public License, v.\n      2.0. If a copy of the MPL was not\n      distributed with this file, You can\n      obtain one at\n      http://mozilla.org/MPL/2.0/.\n\nIf it is not possible or desirable to put the notice in a particular file, then\nYou may include the notice in a location (such as a LICENSE file in a relevant\ndirectory) where a recipient would be likely to look for such a notice.\n\nYou may add additional accurate notices of copyright ownership.\n\nExhibit B - “Incompatible With Secondary Licenses” Notice\n\n      This Source Code Form is “Incompatible\n      With Secondary Licenses”, as defined by\n      the Mozilla Public License, v. 2.0.\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.654296875,
          "content": "SHELL := bash\n\nGOFILES ?= $(shell go list ./... | grep -v /vendor/)\n\ndefault: test\n\ntest: vet subnet\n\tgo test ./...\n\ninteg: subnet\n\tINTEG_TESTS=yes go test ./...\n\nsubnet:\n\t./test/setup_subnet.sh\n\ncov:\n\tgo test ./... -coverprofile=coverage.out\n\tgo tool cover -html=coverage.out\n\nformat:\n\t@echo \"--> Running go fmt\"\n\t@go fmt $(GOFILES)\n\nvet:\n\t@echo \"--> Running go vet\"\n\t@go vet -tags '$(GOTAGS)' $(GOFILES); if [ $$? -eq 1 ]; then \\\n\t\techo \"\"; \\\n\t\techo \"Vet found suspicious constructs. Please check the reported constructs\"; \\\n\t\techo \"and fix them if necessary before submitting the code for review.\"; \\\n\t\texit 1; \\\n\tfi\n\n.PHONY: default test integ subnet cov format vet\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.7021484375,
          "content": "# memberlist [![GoDoc](https://godoc.org/github.com/hashicorp/memberlist?status.png)](https://godoc.org/github.com/hashicorp/memberlist) [![CircleCI](https://circleci.com/gh/hashicorp/memberlist.svg?style=svg)](https://circleci.com/gh/hashicorp/memberlist)\n\nmemberlist is a [Go](http://www.golang.org) library that manages cluster\nmembership and member failure detection using a gossip based protocol.\n\nThe use cases for such a library are far-reaching: all distributed systems\nrequire membership, and memberlist is a re-usable solution to managing\ncluster membership and node failure detection.\n\nmemberlist is eventually consistent but converges quickly on average.\nThe speed at which it converges can be heavily tuned via various knobs\non the protocol. Node failures are detected and network partitions are partially\ntolerated by attempting to communicate to potentially dead nodes through\nmultiple routes.\n\n## Building\n\nIf you wish to build memberlist you'll need Go version 1.2+ installed.\n\nPlease check your installation with:\n\n```\ngo version\n```\n\n## Usage\n\nMemberlist is surprisingly simple to use. An example is shown below:\n\n```go\n/* Create the initial memberlist from a safe configuration.\n   Please reference the godoc for other default config types.\n   http://godoc.org/github.com/hashicorp/memberlist#Config\n*/\nlist, err := memberlist.Create(memberlist.DefaultLocalConfig())\nif err != nil {\n\tpanic(\"Failed to create memberlist: \" + err.Error())\n}\n\n// Join an existing cluster by specifying at least one known member.\nn, err := list.Join([]string{\"1.2.3.4\"})\nif err != nil {\n\tpanic(\"Failed to join cluster: \" + err.Error())\n}\n\n// Ask for members of the cluster\nfor _, member := range list.Members() {\n\tfmt.Printf(\"Member: %s %s\\n\", member.Name, member.Addr)\n}\n\n// Continue doing whatever you need, memberlist will maintain membership\n// information in the background. Delegates can be used for receiving\n// events when members join or leave.\n```\n\nThe most difficult part of memberlist is configuring it since it has many\navailable knobs in order to tune state propagation delay and convergence times.\nMemberlist provides a default configuration that offers a good starting point,\nbut errs on the side of caution, choosing values that are optimized for\nhigher convergence at the cost of higher bandwidth usage.\n\nFor complete documentation, see the associated [Godoc](http://godoc.org/github.com/hashicorp/memberlist).\n\n## Protocol\n\nmemberlist is based on [\"SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol\"](http://ieeexplore.ieee.org/document/1028914/). However, we extend the protocol in a number of ways:\n\n* Several extensions are made to increase propagation speed and\nconvergence rate.\n* Another set of extensions, that we call Lifeguard, are made to make memberlist more robust in the presence of slow message processing (due to factors such as CPU starvation, and network delay or loss).\n\nFor details on all of these extensions, please read our paper \"[Lifeguard : SWIM-ing with Situational Awareness](https://arxiv.org/abs/1707.00788)\", along with the memberlist source.  We welcome any questions related\nto the protocol on our issue tracker.\n\n## Metrics Emission and Compatibility\n\nThis library can emit metrics using either `github.com/armon/go-metrics` or `github.com/hashicorp/go-metrics`. Choosing between the libraries is controlled via build tags. \n\n**Build Tags**\n* `armonmetrics` - Using this tag will cause metrics to be routed to `armon/go-metrics`\n* `hashicorpmetrics` - Using this tag will cause all metrics to be routed to `hashicorp/go-metrics`\n\nIf no build tag is specified, the default behavior is to use `armon/go-metrics`. \n\n**Deprecating `armon/go-metrics`**\n\nEmitting metrics to `armon/go-metrics` is officially deprecated. Usage of `armon/go-metrics` will remain the default until mid-2025 with opt-in support continuing to the end of 2025.\n\n**Migration**\nTo migrate an application currently using the older `armon/go-metrics` to instead use `hashicorp/go-metrics` the following should be done.\n\n1. Upgrade libraries using `armon/go-metrics` to consume `hashicorp/go-metrics/compat` instead. This should involve only changing import statements. All repositories in the `hashicorp` namespace\n2. Update an applications library dependencies to those that have the compatibility layer configured.\n3. Update the application to use `hashicorp/go-metrics` for configuring metrics export instead of `armon/go-metrics`\n   * Replace all application imports of `github.com/armon/go-metrics` with `github.com/hashicorp/go-metrics`\n   * Instrument your build system to build with the `hashicorpmetrics` tag.\n\nEventually once the default behavior changes to use `hashicorp/go-metrics` by default (mid-2025), you can drop the `hashicorpmetrics` build tag.\n"
        },
        {
          "name": "alive_delegate.go",
          "type": "blob",
          "size": 0.6455078125,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n// AliveDelegate is used to involve a client in processing\n// a node \"alive\" message. When a node joins, either through\n// a UDP gossip or TCP push/pull, we update the state of\n// that node via an alive message. This can be used to filter\n// a node out and prevent it from being considered a peer\n// using application specific logic.\ntype AliveDelegate interface {\n\t// NotifyAlive is invoked when a message about a live\n\t// node is received from the network.  Returning a non-nil\n\t// error prevents the node from being considered a peer.\n\tNotifyAlive(peer *Node) error\n}\n"
        },
        {
          "name": "awareness.go",
          "type": "blob",
          "size": 1.9794921875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n)\n\n// awareness manages a simple metric for tracking the estimated health of the\n// local node. Health is primary the node's ability to respond in the soft\n// real-time manner required for correct health checking of other nodes in the\n// cluster.\ntype awareness struct {\n\tsync.RWMutex\n\n\t// max is the upper threshold for the timeout scale (the score will be\n\t// constrained to be from 0 <= score < max).\n\tmax int\n\n\t// score is the current awareness score. Lower values are healthier and\n\t// zero is the minimum value.\n\tscore int\n\n\t// metricLabels is the slice of labels to put on all emitted metrics\n\tmetricLabels []metrics.Label\n}\n\n// newAwareness returns a new awareness object.\nfunc newAwareness(max int, metricLabels []metrics.Label) *awareness {\n\treturn &awareness{\n\t\tmax:          max,\n\t\tscore:        0,\n\t\tmetricLabels: metricLabels,\n\t}\n}\n\n// ApplyDelta takes the given delta and applies it to the score in a thread-safe\n// manner. It also enforces a floor of zero and a max of max, so deltas may not\n// change the overall score if it's railed at one of the extremes.\nfunc (a *awareness) ApplyDelta(delta int) {\n\ta.Lock()\n\tinitial := a.score\n\ta.score += delta\n\tif a.score < 0 {\n\t\ta.score = 0\n\t} else if a.score > (a.max - 1) {\n\t\ta.score = (a.max - 1)\n\t}\n\tfinal := a.score\n\ta.Unlock()\n\n\tif initial != final {\n\t\tmetrics.SetGaugeWithLabels([]string{\"memberlist\", \"health\", \"score\"}, float32(final), a.metricLabels)\n\t}\n}\n\n// GetHealthScore returns the raw health score.\nfunc (a *awareness) GetHealthScore() int {\n\ta.RLock()\n\tscore := a.score\n\ta.RUnlock()\n\treturn score\n}\n\n// ScaleTimeout takes the given duration and scales it based on the current\n// score. Less healthyness will lead to longer timeouts.\nfunc (a *awareness) ScaleTimeout(timeout time.Duration) time.Duration {\n\ta.RLock()\n\tscore := a.score\n\ta.RUnlock()\n\treturn timeout * (time.Duration(score) + 1)\n}\n"
        },
        {
          "name": "awareness_test.go",
          "type": "blob",
          "size": 0.9833984375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestAwareness(t *testing.T) {\n\tcases := []struct {\n\t\tdelta   int\n\t\tscore   int\n\t\ttimeout time.Duration\n\t}{\n\t\t{0, 0, 1 * time.Second},\n\t\t{-1, 0, 1 * time.Second},\n\t\t{-10, 0, 1 * time.Second},\n\t\t{1, 1, 2 * time.Second},\n\t\t{-1, 0, 1 * time.Second},\n\t\t{10, 7, 8 * time.Second},\n\t\t{-1, 6, 7 * time.Second},\n\t\t{-1, 5, 6 * time.Second},\n\t\t{-1, 4, 5 * time.Second},\n\t\t{-1, 3, 4 * time.Second},\n\t\t{-1, 2, 3 * time.Second},\n\t\t{-1, 1, 2 * time.Second},\n\t\t{-1, 0, 1 * time.Second},\n\t\t{-1, 0, 1 * time.Second},\n\t}\n\n\ta := newAwareness(8, nil)\n\tfor i, c := range cases {\n\t\ta.ApplyDelta(c.delta)\n\t\tif a.GetHealthScore() != c.score {\n\t\t\tt.Errorf(\"case %d: score mismatch %d != %d\", i, a.score, c.score)\n\t\t}\n\t\tif timeout := a.ScaleTimeout(1 * time.Second); timeout != c.timeout {\n\t\t\tt.Errorf(\"case %d: scaled timeout mismatch %9.6f != %9.6f\",\n\t\t\t\ti, timeout.Seconds(), c.timeout.Seconds())\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "broadcast.go",
          "type": "blob",
          "size": 3.3349609375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n/*\nThe broadcast mechanism works by maintaining a sorted list of messages to be\nsent out. When a message is to be broadcast, the retransmit count\nis set to zero and appended to the queue. The retransmit count serves\nas the \"priority\", ensuring that newer messages get sent first. Once\na message hits the retransmit limit, it is removed from the queue.\n\nAdditionally, older entries can be invalidated by new messages that\nare contradictory. For example, if we send \"{suspect M1 inc: 1},\nthen a following {alive M1 inc: 2} will invalidate that message\n*/\n\ntype memberlistBroadcast struct {\n\tnode   string\n\tmsg    []byte\n\tnotify chan struct{}\n}\n\nfunc (b *memberlistBroadcast) Invalidates(other Broadcast) bool {\n\t// Check if that broadcast is a memberlist type\n\tmb, ok := other.(*memberlistBroadcast)\n\tif !ok {\n\t\treturn false\n\t}\n\n\t// Invalidates any message about the same node\n\treturn b.node == mb.node\n}\n\n// memberlist.NamedBroadcast optional interface\nfunc (b *memberlistBroadcast) Name() string {\n\treturn b.node\n}\n\nfunc (b *memberlistBroadcast) Message() []byte {\n\treturn b.msg\n}\n\nfunc (b *memberlistBroadcast) Finished() {\n\tselect {\n\tcase b.notify <- struct{}{}:\n\tdefault:\n\t}\n}\n\n// encodeAndBroadcast encodes a message and enqueues it for broadcast. Fails\n// silently if there is an encoding error.\nfunc (m *Memberlist) encodeAndBroadcast(node string, msgType messageType, msg interface{}) {\n\tm.encodeBroadcastNotify(node, msgType, msg, nil)\n}\n\n// encodeBroadcastNotify encodes a message and enqueues it for broadcast\n// and notifies the given channel when transmission is finished. Fails\n// silently if there is an encoding error.\nfunc (m *Memberlist) encodeBroadcastNotify(node string, msgType messageType, msg interface{}, notify chan struct{}) {\n\tbuf, err := encode(msgType, msg, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to encode message for broadcast: %s\", err)\n\t} else {\n\t\tm.queueBroadcast(node, buf.Bytes(), notify)\n\t}\n}\n\n// queueBroadcast is used to start dissemination of a message. It will be\n// sent up to a configured number of times. The message could potentially\n// be invalidated by a future message about the same node\nfunc (m *Memberlist) queueBroadcast(node string, msg []byte, notify chan struct{}) {\n\tb := &memberlistBroadcast{node, msg, notify}\n\tm.broadcasts.QueueBroadcast(b)\n}\n\n// getBroadcasts is used to return a slice of broadcasts to send up to\n// a maximum byte size, while imposing a per-broadcast overhead. This is used\n// to fill a UDP packet with piggybacked data\nfunc (m *Memberlist) getBroadcasts(overhead, limit int) [][]byte {\n\t// Get memberlist messages first\n\ttoSend := m.broadcasts.GetBroadcasts(overhead, limit)\n\n\t// Check if the user has anything to broadcast\n\td := m.config.Delegate\n\tif d != nil {\n\t\t// Determine the bytes used already\n\t\tbytesUsed := 0\n\t\tfor _, msg := range toSend {\n\t\t\tbytesUsed += len(msg) + overhead\n\t\t}\n\n\t\t// Check space remaining for user messages\n\t\tavail := limit - bytesUsed\n\t\tif avail > overhead+userMsgOverhead {\n\t\t\tuserMsgs := d.GetBroadcasts(overhead+userMsgOverhead, avail)\n\n\t\t\t// Frame each user message\n\t\t\tfor _, msg := range userMsgs {\n\t\t\t\tbuf := make([]byte, 1, len(msg)+1)\n\t\t\t\tbuf[0] = byte(userMsg)\n\t\t\t\tbuf = append(buf, msg...)\n\t\t\t\ttoSend = append(toSend, buf)\n\t\t\t}\n\t\t}\n\t}\n\treturn toSend\n}\n"
        },
        {
          "name": "broadcast_test.go",
          "type": "blob",
          "size": 0.6328125,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestMemberlistBroadcast_Invalidates(t *testing.T) {\n\tm1 := &memberlistBroadcast{\"test\", nil, nil}\n\tm2 := &memberlistBroadcast{\"foo\", nil, nil}\n\n\tif m1.Invalidates(m2) || m2.Invalidates(m1) {\n\t\tt.Fatalf(\"unexpected invalidation\")\n\t}\n\n\tif !m1.Invalidates(m1) {\n\t\tt.Fatalf(\"expected invalidation\")\n\t}\n}\n\nfunc TestMemberlistBroadcast_Message(t *testing.T) {\n\tm1 := &memberlistBroadcast{\"test\", []byte(\"test\"), nil}\n\tmsg := m1.Message()\n\tif !reflect.DeepEqual(msg, []byte(\"test\")) {\n\t\tt.Fatalf(\"messages do not match\")\n\t}\n}\n"
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 15.6884765625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n\t\"github.com/hashicorp/go-multierror\"\n)\n\ntype Config struct {\n\t// The name of this node. This must be unique in the cluster.\n\tName string\n\n\t// Transport is a hook for providing custom code to communicate with\n\t// other nodes. If this is left nil, then memberlist will by default\n\t// make a NetTransport using BindAddr and BindPort from this structure.\n\tTransport Transport\n\n\t// Label is an optional set of bytes to include on the outside of each\n\t// packet and stream.\n\t//\n\t// If gossip encryption is enabled and this is set it is treated as GCM\n\t// authenticated data.\n\tLabel string\n\n\t// SkipInboundLabelCheck skips the check that inbound packets and gossip\n\t// streams need to be label prefixed.\n\tSkipInboundLabelCheck bool\n\n\t// Configuration related to what address to bind to and ports to\n\t// listen on. The port is used for both UDP and TCP gossip. It is\n\t// assumed other nodes are running on this port, but they do not need\n\t// to.\n\tBindAddr string\n\tBindPort int\n\n\t// Configuration related to what address to advertise to other\n\t// cluster members. Used for nat traversal.\n\tAdvertiseAddr string\n\tAdvertisePort int\n\n\t// ProtocolVersion is the configured protocol version that we\n\t// will _speak_. This must be between ProtocolVersionMin and\n\t// ProtocolVersionMax.\n\tProtocolVersion uint8\n\n\t// TCPTimeout is the timeout for establishing a stream connection with\n\t// a remote node for a full state sync, and for stream read and write\n\t// operations. This is a legacy name for backwards compatibility, but\n\t// should really be called StreamTimeout now that we have generalized\n\t// the transport.\n\tTCPTimeout time.Duration\n\n\t// IndirectChecks is the number of nodes that will be asked to perform\n\t// an indirect probe of a node in the case a direct probe fails. Memberlist\n\t// waits for an ack from any single indirect node, so increasing this\n\t// number will increase the likelihood that an indirect probe will succeed\n\t// at the expense of bandwidth.\n\tIndirectChecks int\n\n\t// RetransmitMult is the multiplier for the number of retransmissions\n\t// that are attempted for messages broadcasted over gossip. The actual\n\t// count of retransmissions is calculated using the formula:\n\t//\n\t//   Retransmits = RetransmitMult * log(N+1)\n\t//\n\t// This allows the retransmits to scale properly with cluster size. The\n\t// higher the multiplier, the more likely a failed broadcast is to converge\n\t// at the expense of increased bandwidth.\n\tRetransmitMult int\n\n\t// SuspicionMult is the multiplier for determining the time an\n\t// inaccessible node is considered suspect before declaring it dead.\n\t// The actual timeout is calculated using the formula:\n\t//\n\t//   SuspicionTimeout = SuspicionMult * log(N+1) * ProbeInterval\n\t//\n\t// This allows the timeout to scale properly with expected propagation\n\t// delay with a larger cluster size. The higher the multiplier, the longer\n\t// an inaccessible node is considered part of the cluster before declaring\n\t// it dead, giving that suspect node more time to refute if it is indeed\n\t// still alive.\n\tSuspicionMult int\n\n\t// SuspicionMaxTimeoutMult is the multiplier applied to the\n\t// SuspicionTimeout used as an upper bound on detection time. This max\n\t// timeout is calculated using the formula:\n\t//\n\t// SuspicionMaxTimeout = SuspicionMaxTimeoutMult * SuspicionTimeout\n\t//\n\t// If everything is working properly, confirmations from other nodes will\n\t// accelerate suspicion timers in a manner which will cause the timeout\n\t// to reach the base SuspicionTimeout before that elapses, so this value\n\t// will typically only come into play if a node is experiencing issues\n\t// communicating with other nodes. It should be set to a something fairly\n\t// large so that a node having problems will have a lot of chances to\n\t// recover before falsely declaring other nodes as failed, but short\n\t// enough for a legitimately isolated node to still make progress marking\n\t// nodes failed in a reasonable amount of time.\n\tSuspicionMaxTimeoutMult int\n\n\t// PushPullInterval is the interval between complete state syncs.\n\t// Complete state syncs are done with a single node over TCP and are\n\t// quite expensive relative to standard gossiped messages. Setting this\n\t// to zero will disable state push/pull syncs completely.\n\t//\n\t// Setting this interval lower (more frequent) will increase convergence\n\t// speeds across larger clusters at the expense of increased bandwidth\n\t// usage.\n\tPushPullInterval time.Duration\n\n\t// ProbeInterval and ProbeTimeout are used to configure probing\n\t// behavior for memberlist.\n\t//\n\t// ProbeInterval is the interval between random node probes. Setting\n\t// this lower (more frequent) will cause the memberlist cluster to detect\n\t// failed nodes more quickly at the expense of increased bandwidth usage.\n\t//\n\t// ProbeTimeout is the timeout to wait for an ack from a probed node\n\t// before assuming it is unhealthy. This should be set to 99-percentile\n\t// of RTT (round-trip time) on your network.\n\tProbeInterval time.Duration\n\tProbeTimeout  time.Duration\n\n\t// DisableTcpPings will turn off the fallback TCP pings that are attempted\n\t// if the direct UDP ping fails. These get pipelined along with the\n\t// indirect UDP pings.\n\tDisableTcpPings bool\n\n\t// DisableTcpPingsForNode is like DisableTcpPings, but lets you control\n\t// whether to perform TCP pings on a node-by-node basis.\n\tDisableTcpPingsForNode func(nodeName string) bool\n\n\t// AwarenessMaxMultiplier will increase the probe interval if the node\n\t// becomes aware that it might be degraded and not meeting the soft real\n\t// time requirements to reliably probe other nodes.\n\tAwarenessMaxMultiplier int\n\n\t// GossipInterval and GossipNodes are used to configure the gossip\n\t// behavior of memberlist.\n\t//\n\t// GossipInterval is the interval between sending messages that need\n\t// to be gossiped that haven't been able to piggyback on probing messages.\n\t// If this is set to zero, non-piggyback gossip is disabled. By lowering\n\t// this value (more frequent) gossip messages are propagated across\n\t// the cluster more quickly at the expense of increased bandwidth.\n\t//\n\t// GossipNodes is the number of random nodes to send gossip messages to\n\t// per GossipInterval. Increasing this number causes the gossip messages\n\t// to propagate across the cluster more quickly at the expense of\n\t// increased bandwidth.\n\t//\n\t// GossipToTheDeadTime is the interval after which a node has died that\n\t// we will still try to gossip to it. This gives it a chance to refute.\n\tGossipInterval      time.Duration\n\tGossipNodes         int\n\tGossipToTheDeadTime time.Duration\n\n\t// GossipVerifyIncoming controls whether to enforce encryption for incoming\n\t// gossip. It is used for upshifting from unencrypted to encrypted gossip on\n\t// a running cluster.\n\tGossipVerifyIncoming bool\n\n\t// GossipVerifyOutgoing controls whether to enforce encryption for outgoing\n\t// gossip. It is used for upshifting from unencrypted to encrypted gossip on\n\t// a running cluster.\n\tGossipVerifyOutgoing bool\n\n\t// EnableCompression is used to control message compression. This can\n\t// be used to reduce bandwidth usage at the cost of slightly more CPU\n\t// utilization. This is only available starting at protocol version 1.\n\tEnableCompression bool\n\n\t// SecretKey is used to initialize the primary encryption key in a keyring.\n\t// The primary encryption key is the only key used to encrypt messages and\n\t// the first key used while attempting to decrypt messages. Providing a\n\t// value for this primary key will enable message-level encryption and\n\t// verification, and automatically install the key onto the keyring.\n\t// The value should be either 16, 24, or 32 bytes to select AES-128,\n\t// AES-192, or AES-256.\n\tSecretKey []byte\n\n\t// The keyring holds all of the encryption keys used internally. It is\n\t// automatically initialized using the SecretKey and SecretKeys values.\n\tKeyring *Keyring\n\n\t// Delegate and Events are delegates for receiving and providing\n\t// data to memberlist via callback mechanisms. For Delegate, see\n\t// the Delegate interface. For Events, see the EventDelegate interface.\n\t//\n\t// The DelegateProtocolMin/Max are used to guarantee protocol-compatibility\n\t// for any custom messages that the delegate might do (broadcasts,\n\t// local/remote state, etc.). If you don't set these, then the protocol\n\t// versions will just be zero, and version compliance won't be done.\n\tDelegate                Delegate\n\tDelegateProtocolVersion uint8\n\tDelegateProtocolMin     uint8\n\tDelegateProtocolMax     uint8\n\tEvents                  EventDelegate\n\tConflict                ConflictDelegate\n\tMerge                   MergeDelegate\n\tPing                    PingDelegate\n\tAlive                   AliveDelegate\n\n\t// DNSConfigPath points to the system's DNS config file, usually located\n\t// at /etc/resolv.conf. It can be overridden via config for easier testing.\n\tDNSConfigPath string\n\n\t// LogOutput is the writer where logs should be sent. If this is not\n\t// set, logging will go to stderr by default. You cannot specify both LogOutput\n\t// and Logger at the same time.\n\tLogOutput io.Writer\n\n\t// Logger is a custom logger which you provide. If Logger is set, it will use\n\t// this for the internal logger. If Logger is not set, it will fall back to the\n\t// behavior for using LogOutput. You cannot specify both LogOutput and Logger\n\t// at the same time.\n\tLogger *log.Logger\n\n\t// Size of Memberlist's internal channel which handles UDP messages. The\n\t// size of this determines the size of the queue which Memberlist will keep\n\t// while UDP messages are handled.\n\tHandoffQueueDepth int\n\n\t// Maximum number of bytes that memberlist will put in a packet (this\n\t// will be for UDP packets by default with a NetTransport). A safe value\n\t// for this is typically 1400 bytes (which is the default). However,\n\t// depending on your network's MTU (Maximum Transmission Unit) you may\n\t// be able to increase this to get more content into each gossip packet.\n\t// This is a legacy name for backward compatibility but should really be\n\t// called PacketBufferSize now that we have generalized the transport.\n\tUDPBufferSize int\n\n\t// DeadNodeReclaimTime controls the time before a dead node's name can be\n\t// reclaimed by one with a different address or port. By default, this is 0,\n\t// meaning nodes cannot be reclaimed this way.\n\tDeadNodeReclaimTime time.Duration\n\n\t// RequireNodeNames controls if the name of a node is required when sending\n\t// a message to that node.\n\tRequireNodeNames bool\n\n\t// CIDRsAllowed If nil, allow any connection (default), otherwise specify all networks\n\t// allowed to connect (you must specify IPv6/IPv4 separately)\n\t// Using [] will block all connections.\n\tCIDRsAllowed []net.IPNet\n\n\t// MetricLabels is a map of optional labels to apply to all metrics emitted.\n\tMetricLabels []metrics.Label\n\n\t// QueueCheckInterval is the interval at which we check the message\n\t// queue to apply the warning and max depth.\n\tQueueCheckInterval time.Duration\n\n\t// MsgpackUseNewTimeFormat when set to true, force the underlying msgpack\n\t// codec to use the new format of time.Time when encoding (used in\n\t// go-msgpack v1.1.5 by default). Decoding is not affected, as all\n\t// go-msgpack v2.1.0+ decoders know how to decode both formats.\n\tMsgpackUseNewTimeFormat bool\n}\n\n// ParseCIDRs return a possible empty list of all Network that have been parsed\n// In case of error, it returns succesfully parsed CIDRs and the last error found\nfunc ParseCIDRs(v []string) ([]net.IPNet, error) {\n\tnets := make([]net.IPNet, 0)\n\tif v == nil {\n\t\treturn nets, nil\n\t}\n\tvar errs error\n\thasErrors := false\n\tfor _, p := range v {\n\t\t_, net, err := net.ParseCIDR(strings.TrimSpace(p))\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"invalid cidr: %s\", p)\n\t\t\terrs = multierror.Append(errs, err)\n\t\t\thasErrors = true\n\t\t} else {\n\t\t\tnets = append(nets, *net)\n\t\t}\n\t}\n\tif !hasErrors {\n\t\terrs = nil\n\t}\n\treturn nets, errs\n}\n\n// DefaultLANConfig returns a sane set of configurations for Memberlist.\n// It uses the hostname as the node name, and otherwise sets very conservative\n// values that are sane for most LAN environments. The default configuration\n// errs on the side of caution, choosing values that are optimized\n// for higher convergence at the cost of higher bandwidth usage. Regardless,\n// these values are a good starting point when getting started with memberlist.\nfunc DefaultLANConfig() *Config {\n\thostname, _ := os.Hostname()\n\treturn &Config{\n\t\tName:                    hostname,\n\t\tBindAddr:                \"0.0.0.0\",\n\t\tBindPort:                7946,\n\t\tAdvertiseAddr:           \"\",\n\t\tAdvertisePort:           7946,\n\t\tProtocolVersion:         ProtocolVersion2Compatible,\n\t\tTCPTimeout:              10 * time.Second,       // Timeout after 10 seconds\n\t\tIndirectChecks:          3,                      // Use 3 nodes for the indirect ping\n\t\tRetransmitMult:          4,                      // Retransmit a message 4 * log(N+1) nodes\n\t\tSuspicionMult:           4,                      // Suspect a node for 4 * log(N+1) * Interval\n\t\tSuspicionMaxTimeoutMult: 6,                      // For 10k nodes this will give a max timeout of 120 seconds\n\t\tPushPullInterval:        30 * time.Second,       // Low frequency\n\t\tProbeTimeout:            500 * time.Millisecond, // Reasonable RTT time for LAN\n\t\tProbeInterval:           1 * time.Second,        // Failure check every second\n\t\tDisableTcpPings:         false,                  // TCP pings are safe, even with mixed versions\n\t\tAwarenessMaxMultiplier:  8,                      // Probe interval backs off to 8 seconds\n\n\t\tGossipNodes:          3,                      // Gossip to 3 nodes\n\t\tGossipInterval:       200 * time.Millisecond, // Gossip more rapidly\n\t\tGossipToTheDeadTime:  30 * time.Second,       // Same as push/pull\n\t\tGossipVerifyIncoming: true,\n\t\tGossipVerifyOutgoing: true,\n\n\t\tEnableCompression: true, // Enable compression by default\n\n\t\tSecretKey: nil,\n\t\tKeyring:   nil,\n\n\t\tDNSConfigPath: \"/etc/resolv.conf\",\n\n\t\tHandoffQueueDepth: 1024,\n\t\tUDPBufferSize:     1400,\n\t\tCIDRsAllowed:      nil, // same as allow all\n\n\t\tQueueCheckInterval: 30 * time.Second,\n\t}\n}\n\n// DefaultWANConfig works like DefaultConfig, however it returns a configuration\n// that is optimized for most WAN environments. The default configuration is\n// still very conservative and errs on the side of caution.\nfunc DefaultWANConfig() *Config {\n\tconf := DefaultLANConfig()\n\tconf.TCPTimeout = 30 * time.Second\n\tconf.SuspicionMult = 6\n\tconf.PushPullInterval = 60 * time.Second\n\tconf.ProbeTimeout = 3 * time.Second\n\tconf.ProbeInterval = 5 * time.Second\n\tconf.GossipNodes = 4 // Gossip less frequently, but to an additional node\n\tconf.GossipInterval = 500 * time.Millisecond\n\tconf.GossipToTheDeadTime = 60 * time.Second\n\treturn conf\n}\n\n// IPMustBeChecked return true if IPAllowed must be called\nfunc (c *Config) IPMustBeChecked() bool {\n\treturn len(c.CIDRsAllowed) > 0\n}\n\n// IPAllowed return an error if access to memberlist is denied\nfunc (c *Config) IPAllowed(ip net.IP) error {\n\tif !c.IPMustBeChecked() {\n\t\treturn nil\n\t}\n\tfor _, n := range c.CIDRsAllowed {\n\t\tif n.Contains(ip) {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"%s is not allowed\", ip)\n}\n\n// DefaultLocalConfig works like DefaultConfig, however it returns a configuration\n// that is optimized for a local loopback environments. The default configuration is\n// still very conservative and errs on the side of caution.\nfunc DefaultLocalConfig() *Config {\n\tconf := DefaultLANConfig()\n\tconf.TCPTimeout = time.Second\n\tconf.IndirectChecks = 1\n\tconf.RetransmitMult = 2\n\tconf.SuspicionMult = 3\n\tconf.PushPullInterval = 15 * time.Second\n\tconf.ProbeTimeout = 200 * time.Millisecond\n\tconf.ProbeInterval = time.Second\n\tconf.GossipInterval = 100 * time.Millisecond\n\tconf.GossipToTheDeadTime = 15 * time.Second\n\treturn conf\n}\n\n// Returns whether or not encryption is enabled\nfunc (c *Config) EncryptionEnabled() bool {\n\treturn c.Keyring != nil && len(c.Keyring.GetKeys()) > 0\n}\n"
        },
        {
          "name": "config_test.go",
          "type": "blob",
          "size": 2.2392578125,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\nfunc Test_IsValidAddressDefaults(t *testing.T) {\n\ttests := []string{\n\t\t\"127.0.0.1\",\n\t\t\"127.0.0.5\",\n\t\t\"10.0.0.9\",\n\t\t\"172.16.0.7\",\n\t\t\"192.168.2.1\",\n\t\t\"fe80::aede:48ff:fe00:1122\",\n\t\t\"::1\",\n\t}\n\tconfig := DefaultLANConfig()\n\tfor _, ip := range tests {\n\t\tlocalV4 := net.ParseIP(ip)\n\t\tif err := config.IPAllowed(localV4); err != nil {\n\t\t\tt.Fatalf(\"IP %s Localhost Should be accepted for LAN\", ip)\n\t\t}\n\t}\n\tconfig = DefaultWANConfig()\n\tfor _, ip := range tests {\n\t\tlocalV4 := net.ParseIP(ip)\n\t\tif err := config.IPAllowed(localV4); err != nil {\n\t\t\tt.Fatalf(\"IP %s Localhost Should be accepted for WAN\", ip)\n\t\t}\n\t}\n}\n\nfunc Test_IsValidAddressOverride(t *testing.T) {\n\tt.Parallel()\n\tcases := []struct {\n\t\tname    string\n\t\tallow   []string\n\t\tsuccess []string\n\t\tfail    []string\n\t}{\n\t\t{\n\t\t\tname:    \"Default, nil allows all\",\n\t\t\tallow:   nil,\n\t\t\tsuccess: []string{\"127.0.0.5\", \"10.0.0.9\", \"192.168.1.7\", \"::1\"},\n\t\t\tfail:    []string{},\n\t\t},\n\t\t{\n\t\t\tname:    \"Only IPv4\",\n\t\t\tallow:   []string{\"0.0.0.0/0\"},\n\t\t\tsuccess: []string{\"127.0.0.5\", \"10.0.0.9\", \"192.168.1.7\"},\n\t\t\tfail:    []string{\"fe80::38bc:4dff:fe62:b1ae\", \"::1\"},\n\t\t},\n\t\t{\n\t\t\tname:    \"Only IPv6\",\n\t\t\tallow:   []string{\"::0/0\"},\n\t\t\tsuccess: []string{\"fe80::38bc:4dff:fe62:b1ae\", \"::1\"},\n\t\t\tfail:    []string{\"127.0.0.5\", \"10.0.0.9\", \"192.168.1.7\"},\n\t\t},\n\t\t{\n\t\t\tname:    \"Only 127.0.0.0/8 and ::1\",\n\t\t\tallow:   []string{\"::1/128\", \"127.0.0.0/8\"},\n\t\t\tsuccess: []string{\"127.0.0.5\", \"::1\"},\n\t\t\tfail:    []string{\"::2\", \"178.250.0.187\", \"10.0.0.9\", \"192.168.1.7\", \"fe80::38bc:4dff:fe62:b1ae\"},\n\t\t},\n\t}\n\n\tfor _, testCase := range cases {\n\t\tt.Run(testCase.name, func(t *testing.T) {\n\t\t\tconfig := DefaultLANConfig()\n\t\t\tvar err error\n\t\t\tconfig.CIDRsAllowed, err = ParseCIDRs(testCase.allow)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed parsing %s\", testCase.allow)\n\t\t\t}\n\t\t\tfor _, ips := range testCase.success {\n\t\t\t\tip := net.ParseIP(ips)\n\t\t\t\tif err := config.IPAllowed(ip); err != nil {\n\t\t\t\t\tt.Fatalf(\"Test case with %s should pass\", ip)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, ips := range testCase.fail {\n\t\t\t\tip := net.ParseIP(ips)\n\t\t\t\tif err := config.IPAllowed(ip); err == nil {\n\t\t\t\t\tt.Fatalf(\"Test case with %s should fail\", ip)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\n\t}\n\n}\n"
        },
        {
          "name": "conflict_delegate.go",
          "type": "blob",
          "size": 0.435546875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n// ConflictDelegate is a used to inform a client that\n// a node has attempted to join which would result in a\n// name conflict. This happens if two clients are configured\n// with the same name but different addresses.\ntype ConflictDelegate interface {\n\t// NotifyConflict is invoked when a name conflict is detected\n\tNotifyConflict(existing, other *Node)\n}\n"
        },
        {
          "name": "delegate.go",
          "type": "blob",
          "size": 1.8759765625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n// Delegate is the interface that clients must implement if they want to hook\n// into the gossip layer of Memberlist. All the methods must be thread-safe,\n// as they can and generally will be called concurrently.\ntype Delegate interface {\n\t// NodeMeta is used to retrieve meta-data about the current node\n\t// when broadcasting an alive message. It's length is limited to\n\t// the given byte size. This metadata is available in the Node structure.\n\tNodeMeta(limit int) []byte\n\n\t// NotifyMsg is called when a user-data message is received.\n\t// Care should be taken that this method does not block, since doing\n\t// so would block the entire UDP packet receive loop. Additionally, the byte\n\t// slice may be modified after the call returns, so it should be copied if needed\n\tNotifyMsg([]byte)\n\n\t// GetBroadcasts is called when user data messages can be broadcast.\n\t// It can return a list of buffers to send. Each buffer should assume an\n\t// overhead as provided with a limit on the total byte size allowed.\n\t// The total byte size of the resulting data to send must not exceed\n\t// the limit. Care should be taken that this method does not block,\n\t// since doing so would block the entire UDP packet receive loop.\n\tGetBroadcasts(overhead, limit int) [][]byte\n\n\t// LocalState is used for a TCP Push/Pull. This is sent to\n\t// the remote side in addition to the membership information. Any\n\t// data can be sent here. See MergeRemoteState as well. The `join`\n\t// boolean indicates this is for a join instead of a push/pull.\n\tLocalState(join bool) []byte\n\n\t// MergeRemoteState is invoked after a TCP Push/Pull. This is the\n\t// state received from the remote side and is the result of the\n\t// remote side's LocalState call. The 'join'\n\t// boolean indicates this is for a join instead of a push/pull.\n\tMergeRemoteState(buf []byte, join bool)\n}\n"
        },
        {
          "name": "event_delegate.go",
          "type": "blob",
          "size": 1.94140625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n// EventDelegate is a simpler delegate that is used only to receive\n// notifications about members joining and leaving. The methods in this\n// delegate may be called by multiple goroutines, but never concurrently.\n// This allows you to reason about ordering.\ntype EventDelegate interface {\n\t// NotifyJoin is invoked when a node is detected to have joined.\n\t// The Node argument must not be modified.\n\tNotifyJoin(*Node)\n\n\t// NotifyLeave is invoked when a node is detected to have left.\n\t// The Node argument must not be modified.\n\tNotifyLeave(*Node)\n\n\t// NotifyUpdate is invoked when a node is detected to have\n\t// updated, usually involving the meta data. The Node argument\n\t// must not be modified.\n\tNotifyUpdate(*Node)\n}\n\n// ChannelEventDelegate is used to enable an application to receive\n// events about joins and leaves over a channel instead of a direct\n// function call.\n//\n// Care must be taken that events are processed in a timely manner from\n// the channel, since this delegate will block until an event can be sent.\ntype ChannelEventDelegate struct {\n\tCh chan<- NodeEvent\n}\n\n// NodeEventType are the types of events that can be sent from the\n// ChannelEventDelegate.\ntype NodeEventType int\n\nconst (\n\tNodeJoin NodeEventType = iota\n\tNodeLeave\n\tNodeUpdate\n)\n\n// NodeEvent is a single event related to node activity in the memberlist.\n// The Node member of this struct must not be directly modified. It is passed\n// as a pointer to avoid unnecessary copies. If you wish to modify the node,\n// make a copy first.\ntype NodeEvent struct {\n\tEvent NodeEventType\n\tNode  *Node\n}\n\nfunc (c *ChannelEventDelegate) NotifyJoin(n *Node) {\n\tnode := *n\n\tc.Ch <- NodeEvent{NodeJoin, &node}\n}\n\nfunc (c *ChannelEventDelegate) NotifyLeave(n *Node) {\n\tnode := *n\n\tc.Ch <- NodeEvent{NodeLeave, &node}\n}\n\nfunc (c *ChannelEventDelegate) NotifyUpdate(n *Node) {\n\tnode := *n\n\tc.Ch <- NodeEvent{NodeUpdate, &node}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.8642578125,
          "content": "module github.com/hashicorp/memberlist\n\ngo 1.20\n\nrequire (\n\tgithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c\n\tgithub.com/hashicorp/go-metrics v0.5.4\n\tgithub.com/hashicorp/go-msgpack/v2 v2.1.1\n\tgithub.com/hashicorp/go-multierror v1.0.0\n\tgithub.com/hashicorp/go-sockaddr v1.0.0\n\tgithub.com/miekg/dns v1.1.26\n\tgithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529\n\tgithub.com/stretchr/testify v1.4.0\n)\n\nrequire (\n\tgithub.com/armon/go-metrics v0.4.1 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/hashicorp/errwrap v1.0.0 // indirect\n\tgithub.com/hashicorp/go-immutable-radix v1.0.0 // indirect\n\tgithub.com/hashicorp/golang-lru v0.5.0 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgolang.org/x/crypto v0.14.0 // indirect\n\tgolang.org/x/net v0.16.0 // indirect\n\tgolang.org/x/sys v0.13.0 // indirect\n\tgopkg.in/yaml.v2 v2.4.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 17.6318359375,
          "content": "cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ngithub.com/DataDog/datadog-go v3.2.0+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/armon/go-metrics v0.4.1 h1:hR91U9KYmb6bLBYLQjyM+3j+rcd/UhE+G78SFnF8gJA=\ngithub.com/armon/go-metrics v0.4.1/go.mod h1:E6amYzXo6aW1tqzoZGT755KkbgrJsSdpwZ+3JqfkOG4=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=\ngithub.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c h1:964Od4U6p2jUkFxvCydnIczKteheJEzHRToSGK3Bnlw=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/hashicorp/errwrap v1.0.0 h1:hLrqtEDnRye3+sgx6z4qVLNuviH3MR5aQ0ykNJa/UYA=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\ngithub.com/hashicorp/go-immutable-radix v1.0.0 h1:AKDB1HM5PWEA7i4nhcpwOrO2byshxBjXVn/J/3+z5/0=\ngithub.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-metrics v0.5.4 h1:8mmPiIJkTPPEbAiV97IxdAGNdRdaWwVap1BU6elejKY=\ngithub.com/hashicorp/go-metrics v0.5.4/go.mod h1:CG5yz4NZ/AI/aQt9Ucm/vdBnbh7fvmv4lxZ350i+QQI=\ngithub.com/hashicorp/go-msgpack/v2 v2.1.1 h1:xQEY9yB2wnHitoSzk/B9UjXWRQ67QKu5AOm8aFp8N3I=\ngithub.com/hashicorp/go-msgpack/v2 v2.1.1/go.mod h1:upybraOAblm4S7rx0+jeNy+CWWhzywQsSRV5033mMu4=\ngithub.com/hashicorp/go-multierror v1.0.0 h1:iVjPR7a6H0tWELX5NxNe7bYopibicUzc7uPribsnS6o=\ngithub.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\ngithub.com/hashicorp/go-retryablehttp v0.5.3/go.mod h1:9B5zBasrRhHXnJnui7y6sL7es7NDiJgTc6Er0maI1Xs=\ngithub.com/hashicorp/go-sockaddr v1.0.0 h1:GeH6tui99pF4NJgfnhp+L6+FfobzVW3Ah46sLo0ICXs=\ngithub.com/hashicorp/go-sockaddr v1.0.0/go.mod h1:7Xibr9yA9JjQq1JpNB2Vw7kxv8xerXegt+ozgdvDeDU=\ngithub.com/hashicorp/go-uuid v1.0.0 h1:RS8zrF7PhGwyNPOtxSClXXj9HA8feRnJzgnI1RJCSnM=\ngithub.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/golang-lru v0.5.0 h1:CL2msUPvZTLb5O648aiLNJw3hnBxN2+1Jq8rCOH9wdo=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.9/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/miekg/dns v1.1.26 h1:gPxPSwALAeHJSjarOs00QjVdV9QoBvc1D2ujQUr5BzU=\ngithub.com/miekg/dns v1.1.26/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/pascaldekloe/goe v0.1.0 h1:cBOtyMzM9HTpWjXfbbunk26uA6nG3a8n06Wieeh0MwY=\ngithub.com/pascaldekloe/goe v0.1.0/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.4.0/go.mod h1:e9GMxYsXl05ICDXkRhurwBS4Q3OK1iX/F2sw+iXX5zU=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.1/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.9.1/go.mod h1:yhUN8i9wzaXS3w1O07YhxHEBxD+W35wd8bs7vj7HSQ4=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.0.8/go.mod h1:7Qr8sr6344vo1JqZ6HhLceV9o3AJ1Ff+GxbHq6oeK9A=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529 h1:nn5Wsu0esKSJiIVhscUtVbo7ada43DJhG55ua/hjS5I=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529/go.mod h1:DxrIzT+xaE7yg65j358z/aeFdxmN0P9QXhEzd20vsDc=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0 h1:2E4SXV/wtOkTonXsotYi4li6zVWxYlZuYNCXe9XRJyk=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB+9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.14.0 h1:wBqGXzWJW6m1XrIKlAH0Hs1JJ7+9KBwnIO8v66Q9cHc=\ngolang.org/x/crypto v0.14.0/go.mod h1:MVFd36DqK4CsrnJYDkBA3VC4m2GkXAM0PvzMCn4JQf4=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.16.0 h1:7eBu7KsSvFDtSXUIDbh3aqlK4DPsZ1rByC8PFfBThos=\ngolang.org/x/net v0.16.0/go.mod h1:NxSsAGuq816PNPmqtQdLE42eU2Fs7NoRIZrHJAlaCOE=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a h1:DcqTD9SDLc+1P/r1EmRBwnVsrOwW+kk2vWf9n+1sGhs=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.13.0 h1:Af8nKPmuFypiUBjVoU9V20FiaFXOcuZI21p0ycVYYGE=\ngolang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\n"
        },
        {
          "name": "integ_test.go",
          "type": "blob",
          "size": 2.0400390625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)\n\n// CheckInteg will skip a test if integration testing is not enabled.\nfunc CheckInteg(t *testing.T) {\n\tif !IsInteg() {\n\t\tt.SkipNow()\n\t}\n}\n\n// IsInteg returns a boolean telling you if we're in integ testing mode.\nfunc IsInteg() bool {\n\treturn os.Getenv(\"INTEG_TESTS\") != \"\"\n}\n\n// Tests the memberlist by creating a cluster of 100 nodes\n// and checking that we get strong convergence of changes.\nfunc TestMemberlist_Integ(t *testing.T) {\n\tCheckInteg(t)\n\n\tnum := 16\n\tvar members []*Memberlist\n\n\tsecret := []byte{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n\teventCh := make(chan NodeEvent, num)\n\n\taddr := \"127.0.0.1\"\n\tfor i := 0; i < num; i++ {\n\t\tc := DefaultLANConfig()\n\t\tc.Name = fmt.Sprintf(\"%s:%d\", addr, 12345+i)\n\t\tc.BindAddr = addr\n\t\tc.BindPort = 12345 + i\n\t\tc.ProbeInterval = 20 * time.Millisecond\n\t\tc.ProbeTimeout = 100 * time.Millisecond\n\t\tc.GossipInterval = 20 * time.Millisecond\n\t\tc.PushPullInterval = 200 * time.Millisecond\n\t\tc.SecretKey = secret\n\t\tc.Logger = log.New(os.Stderr, c.Name, log.LstdFlags)\n\n\t\tif i == 0 {\n\t\t\tc.Events = &ChannelEventDelegate{eventCh}\n\t\t}\n\n\t\tm, err := Create(c)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t\t}\n\t\tdefer m.Shutdown()\n\n\t\tmembers = append(members, m)\n\n\t\tif i > 0 {\n\t\t\tlast := members[i-1]\n\t\t\tnum, err := m.Join([]string{last.config.Name + \"/\" + last.config.Name})\n\t\t\tif num == 0 || err != nil {\n\t\t\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Wait and print debug info\n\tbreakTimer := time.After(250 * time.Millisecond)\nWAIT:\n\tfor {\n\t\tselect {\n\t\tcase e := <-eventCh:\n\t\t\tif e.Event == NodeJoin {\n\t\t\t\tt.Logf(\"[DEBUG] Node join: %v (%d)\", *e.Node, members[0].NumMembers())\n\t\t\t} else {\n\t\t\t\tt.Logf(\"[DEBUG] Node leave: %v (%d)\", *e.Node, members[0].NumMembers())\n\t\t\t}\n\t\tcase <-breakTimer:\n\t\t\tbreak WAIT\n\t\t}\n\t}\n\n\tfor idx, m := range members {\n\t\tgot := m.NumMembers()\n\t\tif got != num {\n\t\t\tt.Errorf(\"bad num members at idx %d. Expected %d. Got %d.\",\n\t\t\t\tidx, num, got)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "keyring.go",
          "type": "blob",
          "size": 4.4677734375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"sync\"\n)\n\ntype Keyring struct {\n\t// Keys stores the key data used during encryption and decryption. It is\n\t// ordered in such a way where the first key (index 0) is the primary key,\n\t// which is used for encrypting messages, and is the first key tried during\n\t// message decryption.\n\tkeys [][]byte\n\n\t// The keyring lock is used while performing IO operations on the keyring.\n\tl sync.Mutex\n}\n\n// Init allocates substructures\nfunc (k *Keyring) init() {\n\tk.keys = make([][]byte, 0)\n}\n\n// NewKeyring constructs a new container for a set of encryption keys. The\n// keyring contains all key data used internally by memberlist.\n//\n// While creating a new keyring, you must do one of:\n//   - Omit keys and primary key, effectively disabling encryption\n//   - Pass a set of keys plus the primary key\n//   - Pass only a primary key\n//\n// If only a primary key is passed, then it will be automatically added to the\n// keyring. If creating a keyring with multiple keys, one key must be designated\n// primary by passing it as the primaryKey. If the primaryKey does not exist in\n// the list of secondary keys, it will be automatically added at position 0.\n//\n// A key should be either 16, 24, or 32 bytes to select AES-128,\n// AES-192, or AES-256.\nfunc NewKeyring(keys [][]byte, primaryKey []byte) (*Keyring, error) {\n\tkeyring := &Keyring{}\n\tkeyring.init()\n\n\tif len(keys) > 0 || len(primaryKey) > 0 {\n\t\tif len(primaryKey) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"Empty primary key not allowed\")\n\t\t}\n\t\tif err := keyring.AddKey(primaryKey); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfor _, key := range keys {\n\t\t\tif err := keyring.AddKey(key); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn keyring, nil\n}\n\n// ValidateKey will check to see if the key is valid and returns an error if not.\n//\n// key should be either 16, 24, or 32 bytes to select AES-128,\n// AES-192, or AES-256.\nfunc ValidateKey(key []byte) error {\n\tif l := len(key); l != 16 && l != 24 && l != 32 {\n\t\treturn fmt.Errorf(\"key size must be 16, 24 or 32 bytes\")\n\t}\n\treturn nil\n}\n\n// AddKey will install a new key on the ring. Adding a key to the ring will make\n// it available for use in decryption. If the key already exists on the ring,\n// this function will just return noop.\n//\n// key should be either 16, 24, or 32 bytes to select AES-128,\n// AES-192, or AES-256.\nfunc (k *Keyring) AddKey(key []byte) error {\n\tif err := ValidateKey(key); err != nil {\n\t\treturn err\n\t}\n\n\t// No-op if key is already installed\n\tfor _, installedKey := range k.keys {\n\t\tif bytes.Equal(installedKey, key) {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tkeys := append(k.keys, key)\n\tprimaryKey := k.GetPrimaryKey()\n\tif primaryKey == nil {\n\t\tprimaryKey = key\n\t}\n\tk.installKeys(keys, primaryKey)\n\treturn nil\n}\n\n// UseKey changes the key used to encrypt messages. This is the only key used to\n// encrypt messages, so peers should know this key before this method is called.\nfunc (k *Keyring) UseKey(key []byte) error {\n\tfor _, installedKey := range k.keys {\n\t\tif bytes.Equal(key, installedKey) {\n\t\t\tk.installKeys(k.keys, key)\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"Requested key is not in the keyring\")\n}\n\n// RemoveKey drops a key from the keyring. This will return an error if the key\n// requested for removal is currently at position 0 (primary key).\nfunc (k *Keyring) RemoveKey(key []byte) error {\n\tif bytes.Equal(key, k.keys[0]) {\n\t\treturn fmt.Errorf(\"Removing the primary key is not allowed\")\n\t}\n\tfor i, installedKey := range k.keys {\n\t\tif bytes.Equal(key, installedKey) {\n\t\t\tkeys := append(k.keys[:i], k.keys[i+1:]...)\n\t\t\tk.installKeys(keys, k.keys[0])\n\t\t}\n\t}\n\treturn nil\n}\n\n// installKeys will take out a lock on the keyring, and replace the keys with a\n// new set of keys. The key indicated by primaryKey will be installed as the new\n// primary key.\nfunc (k *Keyring) installKeys(keys [][]byte, primaryKey []byte) {\n\tk.l.Lock()\n\tdefer k.l.Unlock()\n\n\tnewKeys := [][]byte{primaryKey}\n\tfor _, key := range keys {\n\t\tif !bytes.Equal(key, primaryKey) {\n\t\t\tnewKeys = append(newKeys, key)\n\t\t}\n\t}\n\tk.keys = newKeys\n}\n\n// GetKeys returns the current set of keys on the ring.\nfunc (k *Keyring) GetKeys() [][]byte {\n\tk.l.Lock()\n\tdefer k.l.Unlock()\n\n\treturn k.keys\n}\n\n// GetPrimaryKey returns the key on the ring at position 0. This is the key used\n// for encrypting messages, and is the first key tried for decrypting messages.\nfunc (k *Keyring) GetPrimaryKey() (key []byte) {\n\tk.l.Lock()\n\tdefer k.l.Unlock()\n\n\tif len(k.keys) > 0 {\n\t\tkey = k.keys[0]\n\t}\n\treturn\n}\n"
        },
        {
          "name": "keyring_test.go",
          "type": "blob",
          "size": 3.6943359375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n)\n\nvar TestKeys [][]byte = [][]byte{\n\t[]byte{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15},\n\t[]byte{15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0},\n\t[]byte{8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7},\n}\n\nfunc TestKeyring_EmptyRing(t *testing.T) {\n\t// Keyrings can be created with no encryption keys (disabled encryption)\n\tkeyring, err := NewKeyring(nil, nil)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tkeys := keyring.GetKeys()\n\tif len(keys) != 0 {\n\t\tt.Fatalf(\"Expected 0 keys but have %d\", len(keys))\n\t}\n}\n\nfunc TestKeyring_PrimaryOnly(t *testing.T) {\n\t// Keyrings can be created using only a primary key\n\tkeyring, err := NewKeyring(nil, TestKeys[0])\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tkeys := keyring.GetKeys()\n\tif len(keys) != 1 {\n\t\tt.Fatalf(\"Expected 1 key but have %d\", len(keys))\n\t}\n}\n\nfunc TestKeyring_GetPrimaryKey(t *testing.T) {\n\tkeyring, err := NewKeyring(TestKeys, TestKeys[1])\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\t// GetPrimaryKey returns correct key\n\tprimaryKey := keyring.GetPrimaryKey()\n\tif !bytes.Equal(primaryKey, TestKeys[1]) {\n\t\tt.Fatalf(\"Unexpected primary key: %v\", primaryKey)\n\t}\n}\n\nfunc TestKeyring_AddRemoveUse(t *testing.T) {\n\tkeyring, err := NewKeyring(nil, TestKeys[1])\n\tif err != nil {\n\t\tt.Fatalf(\"err :%s\", err)\n\t}\n\n\t// Use non-existent key throws error\n\tif err := keyring.UseKey(TestKeys[2]); err == nil {\n\t\tt.Fatalf(\"Expected key not installed error\")\n\t}\n\n\t// Add key to ring\n\tif err := keyring.AddKey(TestKeys[2]); err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tkeys := keyring.GetKeys()\n\tif !bytes.Equal(keys[0], TestKeys[1]) {\n\t\tt.Fatalf(\"Unexpected primary key change\")\n\t}\n\n\tif len(keys) != 2 {\n\t\tt.Fatalf(\"Expected 2 keys but have %d\", len(keys))\n\t}\n\n\t// Use key that exists should succeed\n\tif err := keyring.UseKey(TestKeys[2]); err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tprimaryKey := keyring.GetPrimaryKey()\n\tif !bytes.Equal(primaryKey, TestKeys[2]) {\n\t\tt.Fatalf(\"Unexpected primary key: %v\", primaryKey)\n\t}\n\n\t// Removing primary key should fail\n\tif err := keyring.RemoveKey(TestKeys[2]); err == nil {\n\t\tt.Fatalf(\"Expected primary key removal error\")\n\t}\n\n\t// Removing non-primary key should succeed\n\tif err := keyring.RemoveKey(TestKeys[1]); err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tkeys = keyring.GetKeys()\n\tif len(keys) != 1 {\n\t\tt.Fatalf(\"Expected 1 key but have %d\", len(keys))\n\t}\n}\n\nfunc TestKeyRing_MultiKeyEncryptDecrypt(t *testing.T) {\n\tplaintext := []byte(\"this is a plain text message\")\n\textra := []byte(\"random data\")\n\n\tkeyring, err := NewKeyring(TestKeys, TestKeys[0])\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\t// First encrypt using the primary key and make sure we can decrypt\n\tvar buf bytes.Buffer\n\terr = encryptPayload(1, TestKeys[0], plaintext, extra, &buf)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tmsg, err := decryptPayload(keyring.GetKeys(), buf.Bytes(), extra)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tif !bytes.Equal(msg, plaintext) {\n\t\tt.Fatalf(\"bad: %v\", msg)\n\t}\n\n\t// Now encrypt with a secondary key and try decrypting again.\n\tbuf.Reset()\n\terr = encryptPayload(1, TestKeys[2], plaintext, extra, &buf)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tmsg, err = decryptPayload(keyring.GetKeys(), buf.Bytes(), extra)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tif !bytes.Equal(msg, plaintext) {\n\t\tt.Fatalf(\"bad: %v\", msg)\n\t}\n\n\t// Remove a key from the ring, and then try decrypting again\n\tif err := keyring.RemoveKey(TestKeys[2]); err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tmsg, err = decryptPayload(keyring.GetKeys(), buf.Bytes(), extra)\n\tif err == nil {\n\t\tt.Fatalf(\"Expected no keys to decrypt message\")\n\t}\n}\n"
        },
        {
          "name": "label.go",
          "type": "blob",
          "size": 4.7919921875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n)\n\n// General approach is to prefix all packets and streams with the same structure:\n//\n// magic type byte (244): uint8\n// length of label name:  uint8 (because labels can't be longer than 255 bytes)\n// label name:            []uint8\n\n// LabelMaxSize is the maximum length of a packet or stream label.\nconst LabelMaxSize = 255\n\n// AddLabelHeaderToPacket prefixes outgoing packets with the correct header if\n// the label is not empty.\nfunc AddLabelHeaderToPacket(buf []byte, label string) ([]byte, error) {\n\tif label == \"\" {\n\t\treturn buf, nil\n\t}\n\tif len(label) > LabelMaxSize {\n\t\treturn nil, fmt.Errorf(\"label %q is too long\", label)\n\t}\n\n\treturn makeLabelHeader(label, buf), nil\n}\n\n// RemoveLabelHeaderFromPacket removes any label header from the provided\n// packet and returns it along with the remaining packet contents.\nfunc RemoveLabelHeaderFromPacket(buf []byte) (newBuf []byte, label string, err error) {\n\tif len(buf) == 0 {\n\t\treturn buf, \"\", nil // can't possibly be labeled\n\t}\n\n\t// [type:byte] [size:byte] [size bytes]\n\n\tmsgType := messageType(buf[0])\n\tif msgType != hasLabelMsg {\n\t\treturn buf, \"\", nil\n\t}\n\n\tif len(buf) < 2 {\n\t\treturn nil, \"\", fmt.Errorf(\"cannot decode label; packet has been truncated\")\n\t}\n\n\tsize := int(buf[1])\n\tif size < 1 {\n\t\treturn nil, \"\", fmt.Errorf(\"label header cannot be empty when present\")\n\t}\n\n\tif len(buf) < 2+size {\n\t\treturn nil, \"\", fmt.Errorf(\"cannot decode label; packet has been truncated\")\n\t}\n\n\tlabel = string(buf[2 : 2+size])\n\tnewBuf = buf[2+size:]\n\n\treturn newBuf, label, nil\n}\n\n// AddLabelHeaderToStream prefixes outgoing streams with the correct header if\n// the label is not empty.\nfunc AddLabelHeaderToStream(conn net.Conn, label string) error {\n\tif label == \"\" {\n\t\treturn nil\n\t}\n\tif len(label) > LabelMaxSize {\n\t\treturn fmt.Errorf(\"label %q is too long\", label)\n\t}\n\n\theader := makeLabelHeader(label, nil)\n\n\t_, err := conn.Write(header)\n\treturn err\n}\n\n// RemoveLabelHeaderFromStream removes any label header from the beginning of\n// the stream if present and returns it along with an updated conn with that\n// header removed.\n//\n// Note that on error it is the caller's responsibility to close the\n// connection.\nfunc RemoveLabelHeaderFromStream(conn net.Conn) (net.Conn, string, error) {\n\tbr := bufio.NewReader(conn)\n\n\t// First check for the type byte.\n\tpeeked, err := br.Peek(1)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\t// It is safe to return the original net.Conn at this point because\n\t\t\t// it never contained any data in the first place so we don't have\n\t\t\t// to splice the buffer into the conn because both are empty.\n\t\t\treturn conn, \"\", nil\n\t\t}\n\t\treturn nil, \"\", err\n\t}\n\n\tmsgType := messageType(peeked[0])\n\tif msgType != hasLabelMsg {\n\t\tconn, err = newPeekedConnFromBufferedReader(conn, br, 0)\n\t\treturn conn, \"\", err\n\t}\n\n\t// We are guaranteed to get a size byte as well.\n\tpeeked, err = br.Peek(2)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn nil, \"\", fmt.Errorf(\"cannot decode label; stream has been truncated\")\n\t\t}\n\t\treturn nil, \"\", err\n\t}\n\n\tsize := int(peeked[1])\n\tif size < 1 {\n\t\treturn nil, \"\", fmt.Errorf(\"label header cannot be empty when present\")\n\t}\n\t// NOTE: we don't have to check this against LabelMaxSize because a byte\n\t// already has a max value of 255.\n\n\t// Once we know the size we can peek the label as well. Note that since we\n\t// are using the default bufio.Reader size of 4096, the entire label header\n\t// fits in the initial buffer fill so this should be free.\n\tpeeked, err = br.Peek(2 + size)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn nil, \"\", fmt.Errorf(\"cannot decode label; stream has been truncated\")\n\t\t}\n\t\treturn nil, \"\", err\n\t}\n\n\tlabel := string(peeked[2 : 2+size])\n\n\tconn, err = newPeekedConnFromBufferedReader(conn, br, 2+size)\n\tif err != nil {\n\t\treturn nil, \"\", err\n\t}\n\n\treturn conn, label, nil\n}\n\n// newPeekedConnFromBufferedReader will splice the buffer contents after the\n// offset into the provided net.Conn and return the result so that the rest of\n// the buffer contents are returned first when reading from the returned\n// peekedConn before moving on to the unbuffered conn contents.\nfunc newPeekedConnFromBufferedReader(conn net.Conn, br *bufio.Reader, offset int) (*peekedConn, error) {\n\t// Extract any of the readahead buffer.\n\tpeeked, err := br.Peek(br.Buffered())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &peekedConn{\n\t\tPeeked: peeked[offset:],\n\t\tConn:   conn,\n\t}, nil\n}\n\nfunc makeLabelHeader(label string, rest []byte) []byte {\n\tnewBuf := make([]byte, 2, 2+len(label)+len(rest))\n\tnewBuf[0] = byte(hasLabelMsg)\n\tnewBuf[1] = byte(len(label))\n\tnewBuf = append(newBuf, []byte(label)...)\n\tif len(rest) > 0 {\n\t\tnewBuf = append(newBuf, []byte(rest)...)\n\t}\n\treturn newBuf\n}\n\nfunc labelOverhead(label string) int {\n\tif label == \"\" {\n\t\treturn 0\n\t}\n\treturn 2 + len(label)\n}\n"
        },
        {
          "name": "label_test.go",
          "type": "blob",
          "size": 9.357421875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"net\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestAddLabelHeaderToPacket(t *testing.T) {\n\ttype testcase struct {\n\t\tbuf          []byte\n\t\tlabel        string\n\t\texpectPacket []byte\n\t\texpectErr    string\n\t}\n\n\trun := func(t *testing.T, tc testcase) {\n\t\tgot, err := AddLabelHeaderToPacket(tc.buf, tc.label)\n\t\tif tc.expectErr != \"\" {\n\t\t\trequire.Error(t, err)\n\t\t\trequire.Contains(t, err.Error(), tc.expectErr)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, tc.expectPacket, got)\n\t\t}\n\t}\n\n\tlongLabel := strings.Repeat(\"a\", 255)\n\n\tcases := map[string]testcase{\n\t\t\"nil buf with no label\": testcase{\n\t\t\tbuf:          nil,\n\t\t\tlabel:        \"\",\n\t\t\texpectPacket: nil,\n\t\t},\n\t\t\"nil buf with label\": testcase{\n\t\t\tbuf:          nil,\n\t\t\tlabel:        \"foo\",\n\t\t\texpectPacket: append([]byte{byte(hasLabelMsg), 3}, []byte(\"foo\")...),\n\t\t},\n\t\t\"message with label\": testcase{\n\t\t\tbuf:          []byte(\"something\"),\n\t\t\tlabel:        \"foo\",\n\t\t\texpectPacket: append([]byte{byte(hasLabelMsg), 3}, []byte(\"foosomething\")...),\n\t\t},\n\t\t\"message with no label\": testcase{\n\t\t\tbuf:          []byte(\"something\"),\n\t\t\tlabel:        \"\",\n\t\t\texpectPacket: []byte(\"something\"),\n\t\t},\n\t\t\"message with almost too long label\": testcase{\n\t\t\tbuf:          []byte(\"something\"),\n\t\t\tlabel:        longLabel,\n\t\t\texpectPacket: append([]byte{byte(hasLabelMsg), 255}, []byte(longLabel+\"something\")...),\n\t\t},\n\t\t\"label too long by one byte\": testcase{\n\t\t\tbuf:       []byte(\"something\"),\n\t\t\tlabel:     longLabel + \"x\",\n\t\t\texpectErr: `label \"` + longLabel + `x\" is too long`,\n\t\t},\n\t}\n\n\tfor name, tc := range cases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trun(t, tc)\n\t\t})\n\t}\n}\n\nfunc TestRemoveLabelHeaderFromPacket(t *testing.T) {\n\ttype testcase struct {\n\t\tbuf          []byte\n\t\texpectLabel  string\n\t\texpectPacket []byte\n\t\texpectErr    string\n\t}\n\n\trun := func(t *testing.T, tc testcase) {\n\t\tgotBuf, gotLabel, err := RemoveLabelHeaderFromPacket(tc.buf)\n\t\tif tc.expectErr != \"\" {\n\t\t\trequire.Error(t, err)\n\t\t\trequire.Contains(t, err.Error(), tc.expectErr)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, tc.expectPacket, gotBuf)\n\t\t\trequire.Equal(t, tc.expectLabel, gotLabel)\n\t\t}\n\t}\n\n\tcases := map[string]testcase{\n\t\t\"empty buf\": testcase{\n\t\t\tbuf:          []byte{},\n\t\t\texpectLabel:  \"\",\n\t\t\texpectPacket: []byte{},\n\t\t},\n\t\t\"ping with no label\": testcase{\n\t\t\tbuf:          buildBuffer(t, pingMsg, \"blah\"),\n\t\t\texpectLabel:  \"\",\n\t\t\texpectPacket: buildBuffer(t, pingMsg, \"blah\"),\n\t\t},\n\t\t\"error with no label\": testcase{ // 2021-10: largest standard message type\n\t\t\tbuf:          buildBuffer(t, errMsg, \"blah\"),\n\t\t\texpectLabel:  \"\",\n\t\t\texpectPacket: buildBuffer(t, errMsg, \"blah\"),\n\t\t},\n\t\t\"v1 encrypt with no label\": testcase{ // 2021-10: highest encryption version\n\t\t\tbuf:          buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t\texpectLabel:  \"\",\n\t\t\texpectPacket: buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t},\n\t\t\"buf too small for label\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, \"x\"),\n\t\t\texpectErr: `cannot decode label; packet has been truncated`,\n\t\t},\n\t\t\"buf too small for label size\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg),\n\t\t\texpectErr: `cannot decode label; packet has been truncated`,\n\t\t},\n\t\t\"label empty\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, 0, \"x\"),\n\t\t\texpectErr: `label header cannot be empty when present`,\n\t\t},\n\t\t\"label truncated\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, 2, \"x\"),\n\t\t\texpectErr: `cannot decode label; packet has been truncated`,\n\t\t},\n\t\t\"ping with label\": testcase{\n\t\t\tbuf:          buildBuffer(t, hasLabelMsg, 3, \"abc\", pingMsg, \"blah\"),\n\t\t\texpectLabel:  \"abc\",\n\t\t\texpectPacket: buildBuffer(t, pingMsg, \"blah\"),\n\t\t},\n\t\t\"error with label\": testcase{ // 2021-10: largest standard message type\n\t\t\tbuf:          buildBuffer(t, hasLabelMsg, 3, \"abc\", errMsg, \"blah\"),\n\t\t\texpectLabel:  \"abc\",\n\t\t\texpectPacket: buildBuffer(t, errMsg, \"blah\"),\n\t\t},\n\t\t\"v1 encrypt with label\": testcase{ // 2021-10: highest encryption version\n\t\t\tbuf:          buildBuffer(t, hasLabelMsg, 3, \"abc\", maxEncryptionVersion, \"blah\"),\n\t\t\texpectLabel:  \"abc\",\n\t\t\texpectPacket: buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t},\n\t}\n\n\tfor name, tc := range cases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trun(t, tc)\n\t\t})\n\t}\n}\n\nfunc TestAddLabelHeaderToStream(t *testing.T) {\n\ttype testcase struct {\n\t\tlabel      string\n\t\texpectData []byte\n\t\texpectErr  string\n\t}\n\n\tsuffixData := \"EXTRA DATA\"\n\n\trun := func(t *testing.T, tc testcase) {\n\t\tserver, client := net.Pipe()\n\t\tdefer server.Close()\n\t\tdefer client.Close()\n\n\t\tvar (\n\t\t\tdataCh = make(chan []byte, 1)\n\t\t\terrCh  = make(chan error, 1)\n\t\t)\n\t\tgo func() {\n\t\t\tvar buf bytes.Buffer\n\t\t\t_, err := io.Copy(&buf, server)\n\t\t\tif err != nil {\n\t\t\t\terrCh <- err\n\t\t\t}\n\t\t\tdataCh <- buf.Bytes()\n\t\t}()\n\n\t\terr := AddLabelHeaderToStream(client, tc.label)\n\t\tif tc.expectErr != \"\" {\n\t\t\trequire.Error(t, err)\n\t\t\trequire.Contains(t, err.Error(), tc.expectErr)\n\t\t\treturn\n\t\t}\n\t\trequire.NoError(t, err)\n\n\t\tclient.Write([]byte(suffixData))\n\t\tclient.Close()\n\n\t\texpect := make([]byte, 0, len(suffixData)+len(tc.expectData))\n\t\texpect = append(expect, tc.expectData...)\n\t\texpect = append(expect, suffixData...)\n\n\t\tselect {\n\t\tcase err := <-errCh:\n\t\t\trequire.NoError(t, err)\n\t\tcase got := <-dataCh:\n\t\t\trequire.Equal(t, expect, got)\n\t\t}\n\t}\n\n\tlongLabel := strings.Repeat(\"a\", 255)\n\n\tcases := map[string]testcase{\n\t\t\"no label\": testcase{\n\t\t\tlabel:      \"\",\n\t\t\texpectData: nil,\n\t\t},\n\t\t\"with label\": testcase{\n\t\t\tlabel:      \"foo\",\n\t\t\texpectData: buildBuffer(t, hasLabelMsg, 3, \"foo\"),\n\t\t},\n\t\t\"almost too long label\": testcase{\n\t\t\tlabel:      longLabel,\n\t\t\texpectData: buildBuffer(t, hasLabelMsg, 255, longLabel),\n\t\t},\n\t\t\"label too long by one byte\": testcase{\n\t\t\tlabel:     longLabel + \"x\",\n\t\t\texpectErr: `label \"` + longLabel + `x\" is too long`,\n\t\t},\n\t}\n\n\tfor name, tc := range cases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trun(t, tc)\n\t\t})\n\t}\n}\n\nfunc TestRemoveLabelHeaderFromStream(t *testing.T) {\n\ttype testcase struct {\n\t\tbuf         []byte\n\t\texpectLabel string\n\t\texpectData  []byte\n\t\texpectErr   string\n\t}\n\n\trun := func(t *testing.T, tc testcase) {\n\t\tserver, client := net.Pipe()\n\t\tdefer server.Close()\n\t\tdefer client.Close()\n\n\t\tvar (\n\t\t\terrCh = make(chan error, 1)\n\t\t)\n\t\tgo func() {\n\t\t\t_, err := server.Write(tc.buf)\n\t\t\tif err != nil {\n\t\t\t\terrCh <- err\n\t\t\t}\n\t\t\tserver.Close()\n\t\t}()\n\n\t\tnewConn, gotLabel, err := RemoveLabelHeaderFromStream(client)\n\t\tif tc.expectErr != \"\" {\n\t\t\trequire.Error(t, err)\n\t\t\trequire.Contains(t, err.Error(), tc.expectErr)\n\t\t\treturn\n\t\t}\n\t\trequire.NoError(t, err)\n\n\t\tgotBuf, err := io.ReadAll(newConn)\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, tc.expectData, gotBuf)\n\t\trequire.Equal(t, tc.expectLabel, gotLabel)\n\t}\n\n\tcases := map[string]testcase{\n\t\t\"empty buf\": testcase{\n\t\t\tbuf:         []byte{},\n\t\t\texpectLabel: \"\",\n\t\t\texpectData:  []byte{},\n\t\t},\n\t\t\"ping with no label\": testcase{\n\t\t\tbuf:         buildBuffer(t, pingMsg, \"blah\"),\n\t\t\texpectLabel: \"\",\n\t\t\texpectData:  buildBuffer(t, pingMsg, \"blah\"),\n\t\t},\n\t\t\"error with no label\": testcase{ // 2021-10: largest standard message type\n\t\t\tbuf:         buildBuffer(t, errMsg, \"blah\"),\n\t\t\texpectLabel: \"\",\n\t\t\texpectData:  buildBuffer(t, errMsg, \"blah\"),\n\t\t},\n\t\t\"v1 encrypt with no label\": testcase{ // 2021-10: highest encryption version\n\t\t\tbuf:         buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t\texpectLabel: \"\",\n\t\t\texpectData:  buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t},\n\t\t\"buf too small for label\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, \"x\"),\n\t\t\texpectErr: `cannot decode label; stream has been truncated`,\n\t\t},\n\t\t\"buf too small for label size\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg),\n\t\t\texpectErr: `cannot decode label; stream has been truncated`,\n\t\t},\n\t\t\"label empty\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, 0, \"x\"),\n\t\t\texpectErr: `label header cannot be empty when present`,\n\t\t},\n\t\t\"label truncated\": testcase{\n\t\t\tbuf:       buildBuffer(t, hasLabelMsg, 2, \"x\"),\n\t\t\texpectErr: `cannot decode label; stream has been truncated`,\n\t\t},\n\t\t\"ping with label\": testcase{\n\t\t\tbuf:         buildBuffer(t, hasLabelMsg, 3, \"abc\", pingMsg, \"blah\"),\n\t\t\texpectLabel: \"abc\",\n\t\t\texpectData:  buildBuffer(t, pingMsg, \"blah\"),\n\t\t},\n\t\t\"error with label\": testcase{ // 2021-10: largest standard message type\n\t\t\tbuf:         buildBuffer(t, hasLabelMsg, 3, \"abc\", errMsg, \"blah\"),\n\t\t\texpectLabel: \"abc\",\n\t\t\texpectData:  buildBuffer(t, errMsg, \"blah\"),\n\t\t},\n\t\t\"v1 encrypt with label\": testcase{ // 2021-10: highest encryption version\n\t\t\tbuf:         buildBuffer(t, hasLabelMsg, 3, \"abc\", maxEncryptionVersion, \"blah\"),\n\t\t\texpectLabel: \"abc\",\n\t\t\texpectData:  buildBuffer(t, maxEncryptionVersion, \"blah\"),\n\t\t},\n\t}\n\n\tfor name, tc := range cases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trun(t, tc)\n\t\t})\n\t}\n}\n\nfunc buildBuffer(t *testing.T, stuff ...interface{}) []byte {\n\tvar buf bytes.Buffer\n\tfor _, item := range stuff {\n\t\tswitch x := item.(type) {\n\t\tcase int:\n\t\t\tx2 := uint(x)\n\t\t\tif x2 > 255 {\n\t\t\t\tt.Fatalf(\"int is too big\")\n\t\t\t}\n\t\t\tbuf.WriteByte(byte(x2))\n\t\tcase byte:\n\t\t\tbuf.WriteByte(byte(x))\n\t\tcase messageType:\n\t\t\tbuf.WriteByte(byte(x))\n\t\tcase encryptionVersion:\n\t\t\tbuf.WriteByte(byte(x))\n\t\tcase string:\n\t\t\tbuf.Write([]byte(x))\n\t\tcase []byte:\n\t\t\tbuf.Write(x)\n\t\tdefault:\n\t\t\tt.Fatalf(\"unexpected type %T\", item)\n\t\t}\n\t}\n\treturn buf.Bytes()\n}\n\nfunc TestLabelOverhead(t *testing.T) {\n\trequire.Equal(t, 0, labelOverhead(\"\"))\n\trequire.Equal(t, 3, labelOverhead(\"a\"))\n\trequire.Equal(t, 9, labelOverhead(\"abcdefg\"))\n}\n"
        },
        {
          "name": "logging.go",
          "type": "blob",
          "size": 0.51171875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"net\"\n)\n\nfunc LogAddress(addr net.Addr) string {\n\tif addr == nil {\n\t\treturn \"from=<unknown address>\"\n\t}\n\n\treturn fmt.Sprintf(\"from=%s\", addr.String())\n}\n\nfunc LogStringAddress(addr string) string {\n\tif addr == \"\" {\n\t\treturn \"from=<unknown address>\"\n\t}\n\n\treturn fmt.Sprintf(\"from=%s\", addr)\n}\n\nfunc LogConn(conn net.Conn) string {\n\tif conn == nil {\n\t\treturn LogAddress(nil)\n\t}\n\n\treturn LogAddress(conn.RemoteAddr())\n}\n"
        },
        {
          "name": "logging_test.go",
          "type": "blob",
          "size": 0.8505859375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n)\n\nfunc TestLogging_Address(t *testing.T) {\n\ts := LogAddress(nil)\n\tif s != \"from=<unknown address>\" {\n\t\tt.Fatalf(\"bad: %s\", s)\n\t}\n\n\taddr, err := net.ResolveIPAddr(\"ip4\", \"127.0.0.1\")\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\ts = LogAddress(addr)\n\tif s != \"from=127.0.0.1\" {\n\t\tt.Fatalf(\"bad: %s\", s)\n\t}\n}\n\nfunc TestLogging_Conn(t *testing.T) {\n\ts := LogConn(nil)\n\tif s != \"from=<unknown address>\" {\n\t\tt.Fatalf(\"bad: %s\", s)\n\t}\n\n\tln, err := net.Listen(\"tcp\", \":0\")\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tconn, err := net.Dial(\"tcp\", ln.Addr().String())\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tdefer conn.Close()\n\n\ts = LogConn(conn)\n\tif s != fmt.Sprintf(\"from=%s\", conn.RemoteAddr().String()) {\n\t\tt.Fatalf(\"bad: %s\", s)\n\t}\n}\n"
        },
        {
          "name": "memberlist.go",
          "type": "blob",
          "size": 22.9453125,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\n/*\nmemberlist is a library that manages cluster\nmembership and member failure detection using a gossip based protocol.\n\nThe use cases for such a library are far-reaching: all distributed systems\nrequire membership, and memberlist is a re-usable solution to managing\ncluster membership and node failure detection.\n\nmemberlist is eventually consistent but converges quickly on average.\nThe speed at which it converges can be heavily tuned via various knobs\non the protocol. Node failures are detected and network partitions are partially\ntolerated by attempting to communicate to potentially dead nodes through\nmultiple routes.\n*/\npackage memberlist\n\nimport (\n\t\"container/list\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n\t\"github.com/hashicorp/go-multierror\"\n\t\"github.com/hashicorp/go-sockaddr\"\n\t\"github.com/miekg/dns\"\n)\n\nvar errNodeNamesAreRequired = errors.New(\"memberlist: node names are required by configuration but one was not provided\")\n\ntype Memberlist struct {\n\tsequenceNum uint32 // Local sequence number\n\tincarnation uint32 // Local incarnation number\n\tnumNodes    uint32 // Number of known nodes (estimate)\n\tpushPullReq uint32 // Number of push/pull requests\n\n\tadvertiseLock sync.RWMutex\n\tadvertiseAddr net.IP\n\tadvertisePort uint16\n\n\tconfig         *Config\n\tshutdown       int32 // Used as an atomic boolean value\n\tshutdownCh     chan struct{}\n\tleave          int32 // Used as an atomic boolean value\n\tleaveBroadcast chan struct{}\n\n\tshutdownLock sync.Mutex // Serializes calls to Shutdown\n\tleaveLock    sync.Mutex // Serializes calls to Leave\n\n\ttransport NodeAwareTransport\n\n\thandoffCh            chan struct{}\n\thighPriorityMsgQueue *list.List\n\tlowPriorityMsgQueue  *list.List\n\tmsgQueueLock         sync.Mutex\n\n\tnodeLock   sync.RWMutex\n\tnodes      []*nodeState          // Known nodes\n\tnodeMap    map[string]*nodeState // Maps Node.Name -> NodeState\n\tnodeTimers map[string]*suspicion // Maps Node.Name -> suspicion timer\n\tawareness  *awareness\n\n\ttickerLock sync.Mutex\n\ttickers    []*time.Ticker\n\tstopTick   chan struct{}\n\tprobeIndex int\n\n\tackLock     sync.Mutex\n\tackHandlers map[uint32]*ackHandler\n\n\tbroadcasts *TransmitLimitedQueue\n\n\tlogger *log.Logger\n\n\t// metricLabels is the slice of labels to put on all emitted metrics\n\tmetricLabels []metrics.Label\n}\n\n// BuildVsnArray creates the array of Vsn\nfunc (conf *Config) BuildVsnArray() []uint8 {\n\treturn []uint8{\n\t\tProtocolVersionMin, ProtocolVersionMax, conf.ProtocolVersion,\n\t\tconf.DelegateProtocolMin, conf.DelegateProtocolMax,\n\t\tconf.DelegateProtocolVersion,\n\t}\n}\n\n// newMemberlist creates the network listeners.\n// Does not schedule execution of background maintenance.\nfunc newMemberlist(conf *Config) (*Memberlist, error) {\n\tif conf.ProtocolVersion < ProtocolVersionMin {\n\t\treturn nil, fmt.Errorf(\"Protocol version '%d' too low. Must be in range: [%d, %d]\",\n\t\t\tconf.ProtocolVersion, ProtocolVersionMin, ProtocolVersionMax)\n\t} else if conf.ProtocolVersion > ProtocolVersionMax {\n\t\treturn nil, fmt.Errorf(\"Protocol version '%d' too high. Must be in range: [%d, %d]\",\n\t\t\tconf.ProtocolVersion, ProtocolVersionMin, ProtocolVersionMax)\n\t}\n\n\tif len(conf.SecretKey) > 0 {\n\t\tif conf.Keyring == nil {\n\t\t\tkeyring, err := NewKeyring(nil, conf.SecretKey)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tconf.Keyring = keyring\n\t\t} else {\n\t\t\tif err := conf.Keyring.AddKey(conf.SecretKey); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif err := conf.Keyring.UseKey(conf.SecretKey); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tif conf.LogOutput != nil && conf.Logger != nil {\n\t\treturn nil, fmt.Errorf(\"Cannot specify both LogOutput and Logger. Please choose a single log configuration setting.\")\n\t}\n\n\tlogDest := conf.LogOutput\n\tif logDest == nil {\n\t\tlogDest = os.Stderr\n\t}\n\n\tlogger := conf.Logger\n\tif logger == nil {\n\t\tlogger = log.New(logDest, \"\", log.LstdFlags)\n\t}\n\n\t// Set up a network transport by default if a custom one wasn't given\n\t// by the config.\n\ttransport := conf.Transport\n\tif transport == nil {\n\t\tnc := &NetTransportConfig{\n\t\t\tBindAddrs:    []string{conf.BindAddr},\n\t\t\tBindPort:     conf.BindPort,\n\t\t\tLogger:       logger,\n\t\t\tMetricLabels: conf.MetricLabels,\n\t\t}\n\n\t\t// See comment below for details about the retry in here.\n\t\tmakeNetRetry := func(limit int) (*NetTransport, error) {\n\t\t\tvar err error\n\t\t\tfor try := 0; try < limit; try++ {\n\t\t\t\tvar nt *NetTransport\n\t\t\t\tif nt, err = NewNetTransport(nc); err == nil {\n\t\t\t\t\treturn nt, nil\n\t\t\t\t}\n\t\t\t\tif strings.Contains(err.Error(), \"address already in use\") {\n\t\t\t\t\tlogger.Printf(\"[DEBUG] memberlist: Got bind error: %v\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, fmt.Errorf(\"failed to obtain an address: %v\", err)\n\t\t}\n\n\t\t// The dynamic bind port operation is inherently racy because\n\t\t// even though we are using the kernel to find a port for us, we\n\t\t// are attempting to bind multiple protocols (and potentially\n\t\t// multiple addresses) with the same port number. We build in a\n\t\t// few retries here since this often gets transient errors in\n\t\t// busy unit tests.\n\t\tlimit := 1\n\t\tif conf.BindPort == 0 {\n\t\t\tlimit = 10\n\t\t}\n\n\t\tnt, err := makeNetRetry(limit)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Could not set up network transport: %v\", err)\n\t\t}\n\t\tif conf.BindPort == 0 {\n\t\t\tport := nt.GetAutoBindPort()\n\t\t\tconf.BindPort = port\n\t\t\tconf.AdvertisePort = port\n\t\t\tlogger.Printf(\"[DEBUG] memberlist: Using dynamic bind port %d\", port)\n\t\t}\n\t\ttransport = nt\n\t}\n\n\tnodeAwareTransport, ok := transport.(NodeAwareTransport)\n\tif !ok {\n\t\tlogger.Printf(\"[DEBUG] memberlist: configured Transport is not a NodeAwareTransport and some features may not work as desired\")\n\t\tnodeAwareTransport = &shimNodeAwareTransport{transport}\n\t}\n\n\tif len(conf.Label) > LabelMaxSize {\n\t\treturn nil, fmt.Errorf(\"could not use %q as a label: too long\", conf.Label)\n\t}\n\n\tif conf.Label != \"\" {\n\t\tnodeAwareTransport = &labelWrappedTransport{\n\t\t\tlabel:              conf.Label,\n\t\t\tNodeAwareTransport: nodeAwareTransport,\n\t\t}\n\t}\n\n\tm := &Memberlist{\n\t\tconfig:               conf,\n\t\tshutdownCh:           make(chan struct{}),\n\t\tleaveBroadcast:       make(chan struct{}, 1),\n\t\ttransport:            nodeAwareTransport,\n\t\thandoffCh:            make(chan struct{}, 1),\n\t\thighPriorityMsgQueue: list.New(),\n\t\tlowPriorityMsgQueue:  list.New(),\n\t\tnodeMap:              make(map[string]*nodeState),\n\t\tnodeTimers:           make(map[string]*suspicion),\n\t\tawareness:            newAwareness(conf.AwarenessMaxMultiplier, conf.MetricLabels),\n\t\tackHandlers:          make(map[uint32]*ackHandler),\n\t\tbroadcasts:           &TransmitLimitedQueue{RetransmitMult: conf.RetransmitMult},\n\t\tlogger:               logger,\n\t\tmetricLabels:         conf.MetricLabels,\n\t}\n\tm.broadcasts.NumNodes = func() int {\n\t\treturn m.estNumNodes()\n\t}\n\n\t// Get the final advertise address from the transport, which may need\n\t// to see which address we bound to. We'll refresh this each time we\n\t// send out an alive message.\n\tif _, _, err := m.refreshAdvertise(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tgo m.streamListen()\n\tgo m.packetListen()\n\tgo m.packetHandler()\n\tgo m.checkBroadcastQueueDepth()\n\treturn m, nil\n}\n\n// Create will create a new Memberlist using the given configuration.\n// This will not connect to any other node (see Join) yet, but will start\n// all the listeners to allow other nodes to join this memberlist.\n// After creating a Memberlist, the configuration given should not be\n// modified by the user anymore.\nfunc Create(conf *Config) (*Memberlist, error) {\n\tm, err := newMemberlist(conf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := m.setAlive(); err != nil {\n\t\tm.Shutdown()\n\t\treturn nil, err\n\t}\n\tm.schedule()\n\treturn m, nil\n}\n\n// Join is used to take an existing Memberlist and attempt to join a cluster\n// by contacting all the given hosts and performing a state sync. Initially,\n// the Memberlist only contains our own state, so doing this will cause\n// remote nodes to become aware of the existence of this node, effectively\n// joining the cluster.\n//\n// This returns the number of hosts successfully contacted and an error if\n// none could be reached. If an error is returned, the node did not successfully\n// join the cluster.\nfunc (m *Memberlist) Join(existing []string) (int, error) {\n\tnumSuccess := 0\n\tvar errs error\n\tfor _, exist := range existing {\n\t\taddrs, err := m.resolveAddr(exist)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"Failed to resolve %s: %v\", exist, err)\n\t\t\terrs = multierror.Append(errs, err)\n\t\t\tm.logger.Printf(\"[WARN] memberlist: %v\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, addr := range addrs {\n\t\t\thp := joinHostPort(addr.ip.String(), addr.port)\n\t\t\ta := Address{Addr: hp, Name: addr.nodeName}\n\t\t\tif err := m.pushPullNode(a, true); err != nil {\n\t\t\t\terr = fmt.Errorf(\"Failed to join %s: %v\", a.Addr, err)\n\t\t\t\terrs = multierror.Append(errs, err)\n\t\t\t\tm.logger.Printf(\"[DEBUG] memberlist: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnumSuccess++\n\t\t}\n\n\t}\n\tif numSuccess > 0 {\n\t\terrs = nil\n\t}\n\treturn numSuccess, errs\n}\n\n// ipPort holds information about a node we want to try to join.\ntype ipPort struct {\n\tip       net.IP\n\tport     uint16\n\tnodeName string // optional\n}\n\n// tcpLookupIP is a helper to initiate a TCP-based DNS lookup for the given host.\n// The built-in Go resolver will do a UDP lookup first, and will only use TCP if\n// the response has the truncate bit set, which isn't common on DNS servers like\n// Consul's. By doing the TCP lookup directly, we get the best chance for the\n// largest list of hosts to join. Since joins are relatively rare events, it's ok\n// to do this rather expensive operation.\nfunc (m *Memberlist) tcpLookupIP(host string, defaultPort uint16, nodeName string) ([]ipPort, error) {\n\t// Don't attempt any TCP lookups against non-fully qualified domain\n\t// names, since those will likely come from the resolv.conf file.\n\tif !strings.Contains(host, \".\") {\n\t\treturn nil, nil\n\t}\n\n\t// Make sure the domain name is terminated with a dot (we know there's\n\t// at least one character at this point).\n\tdn := host\n\tif dn[len(dn)-1] != '.' {\n\t\tdn = dn + \".\"\n\t}\n\n\t// See if we can find a server to try.\n\tcc, err := dns.ClientConfigFromFile(m.config.DNSConfigPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(cc.Servers) > 0 {\n\t\t// We support host:port in the DNS config, but need to add the\n\t\t// default port if one is not supplied.\n\t\tserver := cc.Servers[0]\n\t\tif !hasPort(server) {\n\t\t\tserver = net.JoinHostPort(server, cc.Port)\n\t\t}\n\n\t\t// Do the lookup.\n\t\tc := new(dns.Client)\n\t\tc.Net = \"tcp\"\n\t\tmsg := new(dns.Msg)\n\t\tmsg.SetQuestion(dn, dns.TypeANY)\n\t\tin, _, err := c.Exchange(msg, server)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Handle any IPs we get back that we can attempt to join.\n\t\tvar ips []ipPort\n\t\tfor _, r := range in.Answer {\n\t\t\tswitch rr := r.(type) {\n\t\t\tcase (*dns.A):\n\t\t\t\tips = append(ips, ipPort{ip: rr.A, port: defaultPort, nodeName: nodeName})\n\t\t\tcase (*dns.AAAA):\n\t\t\t\tips = append(ips, ipPort{ip: rr.AAAA, port: defaultPort, nodeName: nodeName})\n\t\t\tcase (*dns.CNAME):\n\t\t\t\tm.logger.Printf(\"[DEBUG] memberlist: Ignoring CNAME RR in TCP-first answer for '%s'\", host)\n\t\t\t}\n\t\t}\n\t\treturn ips, nil\n\t}\n\n\treturn nil, nil\n}\n\n// resolveAddr is used to resolve the address into an address,\n// port, and error. If no port is given, use the default\nfunc (m *Memberlist) resolveAddr(hostStr string) ([]ipPort, error) {\n\t// First peel off any leading node name. This is optional.\n\tnodeName := \"\"\n\tif slashIdx := strings.Index(hostStr, \"/\"); slashIdx >= 0 {\n\t\tif slashIdx == 0 {\n\t\t\treturn nil, fmt.Errorf(\"empty node name provided\")\n\t\t}\n\t\tnodeName = hostStr[0:slashIdx]\n\t\thostStr = hostStr[slashIdx+1:]\n\t}\n\n\t// This captures the supplied port, or the default one.\n\thostStr = ensurePort(hostStr, m.config.BindPort)\n\thost, sport, err := net.SplitHostPort(hostStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlport, err := strconv.ParseUint(sport, 10, 16)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tport := uint16(lport)\n\n\t// If it looks like an IP address we are done. The SplitHostPort() above\n\t// will make sure the host part is in good shape for parsing, even for\n\t// IPv6 addresses.\n\tif ip := net.ParseIP(host); ip != nil {\n\t\treturn []ipPort{\n\t\t\tipPort{ip: ip, port: port, nodeName: nodeName},\n\t\t}, nil\n\t}\n\n\t// First try TCP so we have the best chance for the largest list of\n\t// hosts to join. If this fails it's not fatal since this isn't a standard\n\t// way to query DNS, and we have a fallback below.\n\tips, err := m.tcpLookupIP(host, port, nodeName)\n\tif err != nil {\n\t\tm.logger.Printf(\"[DEBUG] memberlist: TCP-first lookup failed for '%s', falling back to UDP: %s\", hostStr, err)\n\t}\n\tif len(ips) > 0 {\n\t\treturn ips, nil\n\t}\n\n\t// If TCP didn't yield anything then use the normal Go resolver which\n\t// will try UDP, then might possibly try TCP again if the UDP response\n\t// indicates it was truncated.\n\tans, err := net.LookupIP(host)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tips = make([]ipPort, 0, len(ans))\n\tfor _, ip := range ans {\n\t\tips = append(ips, ipPort{ip: ip, port: port, nodeName: nodeName})\n\t}\n\treturn ips, nil\n}\n\n// setAlive is used to mark this node as being alive. This is the same\n// as if we received an alive notification our own network channel for\n// ourself.\nfunc (m *Memberlist) setAlive() error {\n\t// Get the final advertise address from the transport, which may need\n\t// to see which address we bound to.\n\taddr, port, err := m.refreshAdvertise()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Check if this is a public address without encryption\n\tipAddr, err := sockaddr.NewIPAddr(addr.String())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Failed to parse interface addresses: %v\", err)\n\t}\n\tifAddrs := []sockaddr.IfAddr{\n\t\tsockaddr.IfAddr{\n\t\t\tSockAddr: ipAddr,\n\t\t},\n\t}\n\t_, publicIfs, err := sockaddr.IfByRFC(\"6890\", ifAddrs)\n\tif len(publicIfs) > 0 && !m.config.EncryptionEnabled() {\n\t\tm.logger.Printf(\"[WARN] memberlist: Binding to public address without encryption!\")\n\t}\n\n\t// Set any metadata from the delegate.\n\tvar meta []byte\n\tif m.config.Delegate != nil {\n\t\tmeta = m.config.Delegate.NodeMeta(MetaMaxSize)\n\t\tif len(meta) > MetaMaxSize {\n\t\t\tpanic(\"Node meta data provided is longer than the limit\")\n\t\t}\n\t}\n\n\ta := alive{\n\t\tIncarnation: m.nextIncarnation(),\n\t\tNode:        m.config.Name,\n\t\tAddr:        addr,\n\t\tPort:        uint16(port),\n\t\tMeta:        meta,\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&a, nil, true)\n\n\treturn nil\n}\n\nfunc (m *Memberlist) getAdvertise() (net.IP, uint16) {\n\tm.advertiseLock.RLock()\n\tdefer m.advertiseLock.RUnlock()\n\treturn m.advertiseAddr, m.advertisePort\n}\n\nfunc (m *Memberlist) setAdvertise(addr net.IP, port int) {\n\tm.advertiseLock.Lock()\n\tdefer m.advertiseLock.Unlock()\n\tm.advertiseAddr = addr\n\tm.advertisePort = uint16(port)\n}\n\nfunc (m *Memberlist) refreshAdvertise() (net.IP, int, error) {\n\taddr, port, err := m.transport.FinalAdvertiseAddr(\n\t\tm.config.AdvertiseAddr, m.config.AdvertisePort)\n\tif err != nil {\n\t\treturn nil, 0, fmt.Errorf(\"Failed to get final advertise address: %v\", err)\n\t}\n\tm.setAdvertise(addr, port)\n\treturn addr, port, nil\n}\n\n// LocalNode is used to return the local Node\nfunc (m *Memberlist) LocalNode() *Node {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\tstate := m.nodeMap[m.config.Name]\n\treturn &state.Node\n}\n\n// UpdateNode is used to trigger re-advertising the local node. This is\n// primarily used with a Delegate to support dynamic updates to the local\n// meta data.  This will block until the update message is successfully\n// broadcasted to a member of the cluster, if any exist or until a specified\n// timeout is reached.\nfunc (m *Memberlist) UpdateNode(timeout time.Duration) error {\n\t// Get the node meta data\n\tvar meta []byte\n\tif m.config.Delegate != nil {\n\t\tmeta = m.config.Delegate.NodeMeta(MetaMaxSize)\n\t\tif len(meta) > MetaMaxSize {\n\t\t\tpanic(\"Node meta data provided is longer than the limit\")\n\t\t}\n\t}\n\n\t// Get the existing node\n\tm.nodeLock.RLock()\n\tstate := m.nodeMap[m.config.Name]\n\tm.nodeLock.RUnlock()\n\n\t// Format a new alive message\n\ta := alive{\n\t\tIncarnation: m.nextIncarnation(),\n\t\tNode:        m.config.Name,\n\t\tAddr:        state.Addr,\n\t\tPort:        state.Port,\n\t\tMeta:        meta,\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tnotifyCh := make(chan struct{})\n\tm.aliveNode(&a, notifyCh, true)\n\n\t// Wait for the broadcast or a timeout\n\tif m.anyAlive() {\n\t\tvar timeoutCh <-chan time.Time\n\t\tif timeout > 0 {\n\t\t\ttimeoutCh = time.After(timeout)\n\t\t}\n\t\tselect {\n\t\tcase <-notifyCh:\n\t\tcase <-timeoutCh:\n\t\t\treturn fmt.Errorf(\"timeout waiting for update broadcast\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Deprecated: SendTo is deprecated in favor of SendBestEffort, which requires a node to\n// target. If you don't have a node then use SendToAddress.\nfunc (m *Memberlist) SendTo(to net.Addr, msg []byte) error {\n\ta := Address{Addr: to.String(), Name: \"\"}\n\treturn m.SendToAddress(a, msg)\n}\n\nfunc (m *Memberlist) SendToAddress(a Address, msg []byte) error {\n\t// Encode as a user message\n\tbuf := make([]byte, 1, len(msg)+1)\n\tbuf[0] = byte(userMsg)\n\tbuf = append(buf, msg...)\n\n\t// Send the message\n\treturn m.rawSendMsgPacket(a, nil, buf)\n}\n\n// Deprecated: SendToUDP is deprecated in favor of SendBestEffort.\nfunc (m *Memberlist) SendToUDP(to *Node, msg []byte) error {\n\treturn m.SendBestEffort(to, msg)\n}\n\n// Deprecated: SendToTCP is deprecated in favor of SendReliable.\nfunc (m *Memberlist) SendToTCP(to *Node, msg []byte) error {\n\treturn m.SendReliable(to, msg)\n}\n\n// SendBestEffort uses the unreliable packet-oriented interface of the transport\n// to target a user message at the given node (this does not use the gossip\n// mechanism). The maximum size of the message depends on the configured\n// UDPBufferSize for this memberlist instance.\nfunc (m *Memberlist) SendBestEffort(to *Node, msg []byte) error {\n\t// Encode as a user message\n\tbuf := make([]byte, 1, len(msg)+1)\n\tbuf[0] = byte(userMsg)\n\tbuf = append(buf, msg...)\n\n\t// Send the message\n\ta := Address{Addr: to.Address(), Name: to.Name}\n\treturn m.rawSendMsgPacket(a, to, buf)\n}\n\n// SendReliable uses the reliable stream-oriented interface of the transport to\n// target a user message at the given node (this does not use the gossip\n// mechanism). Delivery is guaranteed if no error is returned, and there is no\n// limit on the size of the message.\nfunc (m *Memberlist) SendReliable(to *Node, msg []byte) error {\n\treturn m.sendUserMsg(to.FullAddress(), msg)\n}\n\n// Members returns a list of all known live nodes. The node structures\n// returned must not be modified. If you wish to modify a Node, make a\n// copy first.\nfunc (m *Memberlist) Members() []*Node {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\n\tnodes := make([]*Node, 0, len(m.nodes))\n\tfor _, n := range m.nodes {\n\t\tif !n.DeadOrLeft() {\n\t\t\tnodes = append(nodes, &n.Node)\n\t\t}\n\t}\n\n\treturn nodes\n}\n\n// NumMembers returns the number of alive nodes currently known. Between\n// the time of calling this and calling Members, the number of alive nodes\n// may have changed, so this shouldn't be used to determine how many\n// members will be returned by Members.\nfunc (m *Memberlist) NumMembers() (alive int) {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\n\tfor _, n := range m.nodes {\n\t\tif !n.DeadOrLeft() {\n\t\t\talive++\n\t\t}\n\t}\n\n\treturn\n}\n\n// Leave will broadcast a leave message but will not shutdown the background\n// listeners, meaning the node will continue participating in gossip and state\n// updates.\n//\n// This will block until the leave message is successfully broadcasted to\n// a member of the cluster, if any exist or until a specified timeout\n// is reached.\n//\n// This method is safe to call multiple times, but must not be called\n// after the cluster is already shut down.\nfunc (m *Memberlist) Leave(timeout time.Duration) error {\n\tm.leaveLock.Lock()\n\tdefer m.leaveLock.Unlock()\n\n\tif m.hasShutdown() {\n\t\tpanic(\"leave after shutdown\")\n\t}\n\n\tif !m.hasLeft() {\n\t\tatomic.StoreInt32(&m.leave, 1)\n\n\t\tm.nodeLock.Lock()\n\t\tstate, ok := m.nodeMap[m.config.Name]\n\t\tm.nodeLock.Unlock()\n\t\tif !ok {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Leave but we're not in the node map.\")\n\t\t\treturn nil\n\t\t}\n\n\t\t// This dead message is special, because Node and From are the\n\t\t// same. This helps other nodes figure out that a node left\n\t\t// intentionally. When Node equals From, other nodes know for\n\t\t// sure this node is gone.\n\t\td := dead{\n\t\t\tIncarnation: state.Incarnation,\n\t\t\tNode:        state.Name,\n\t\t\tFrom:        state.Name,\n\t\t}\n\t\tm.deadNode(&d)\n\n\t\t// Block until the broadcast goes out\n\t\tif m.anyAlive() {\n\t\t\tvar timeoutCh <-chan time.Time\n\t\t\tif timeout > 0 {\n\t\t\t\ttimeoutCh = time.After(timeout)\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-m.leaveBroadcast:\n\t\t\tcase <-timeoutCh:\n\t\t\t\treturn fmt.Errorf(\"timeout waiting for leave broadcast\")\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Check for any other alive node.\nfunc (m *Memberlist) anyAlive() bool {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\tfor _, n := range m.nodes {\n\t\tif !n.DeadOrLeft() && n.Name != m.config.Name {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// GetHealthScore gives this instance's idea of how well it is meeting the soft\n// real-time requirements of the protocol. Lower numbers are better, and zero\n// means \"totally healthy\".\nfunc (m *Memberlist) GetHealthScore() int {\n\treturn m.awareness.GetHealthScore()\n}\n\n// ProtocolVersion returns the protocol version currently in use by\n// this memberlist.\nfunc (m *Memberlist) ProtocolVersion() uint8 {\n\t// NOTE: This method exists so that in the future we can control\n\t// any locking if necessary, if we change the protocol version at\n\t// runtime, etc.\n\treturn m.config.ProtocolVersion\n}\n\n// Shutdown will stop any background maintenance of network activity\n// for this memberlist, causing it to appear \"dead\". A leave message\n// will not be broadcasted prior, so the cluster being left will have\n// to detect this node's shutdown using probing. If you wish to more\n// gracefully exit the cluster, call Leave prior to shutting down.\n//\n// This method is safe to call multiple times.\nfunc (m *Memberlist) Shutdown() error {\n\tm.shutdownLock.Lock()\n\tdefer m.shutdownLock.Unlock()\n\n\tif m.hasShutdown() {\n\t\treturn nil\n\t}\n\n\t// Shut down the transport first, which should block until it's\n\t// completely torn down. If we kill the memberlist-side handlers\n\t// those I/O handlers might get stuck.\n\tif err := m.transport.Shutdown(); err != nil {\n\t\tm.logger.Printf(\"[ERR] Failed to shutdown transport: %v\", err)\n\t}\n\n\t// Now tear down everything else.\n\tatomic.StoreInt32(&m.shutdown, 1)\n\tclose(m.shutdownCh)\n\tm.deschedule()\n\treturn nil\n}\n\nfunc (m *Memberlist) hasShutdown() bool {\n\treturn atomic.LoadInt32(&m.shutdown) == 1\n}\n\nfunc (m *Memberlist) hasLeft() bool {\n\treturn atomic.LoadInt32(&m.leave) == 1\n}\n\nfunc (m *Memberlist) getNodeState(addr string) NodeStateType {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\n\tn := m.nodeMap[addr]\n\treturn n.State\n}\n\nfunc (m *Memberlist) getNodeStateChange(addr string) time.Time {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\n\tn := m.nodeMap[addr]\n\treturn n.StateChange\n}\n\nfunc (m *Memberlist) changeNode(addr string, f func(*nodeState)) {\n\tm.nodeLock.Lock()\n\tdefer m.nodeLock.Unlock()\n\n\tn := m.nodeMap[addr]\n\tf(n)\n}\n\n// checkBroadcastQueueDepth periodically checks the size of the broadcast queue\n// to see if it is too large\nfunc (m *Memberlist) checkBroadcastQueueDepth() {\n\tfor {\n\t\tselect {\n\t\tcase <-time.After(m.config.QueueCheckInterval):\n\t\t\tnumq := m.broadcasts.NumQueued()\n\t\t\tmetrics.AddSampleWithLabels([]string{\"memberlist\", \"queue\", \"broadcasts\"}, float32(numq), m.metricLabels)\n\t\tcase <-m.shutdownCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "memberlist_test.go",
          "type": "blob",
          "size": 49.833984375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\tiretry \"github.com/hashicorp/memberlist/internal/retry\"\n\t\"github.com/miekg/dns\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar bindLock sync.Mutex\nvar bindNum byte = 10\n\nfunc getBindAddrNet(network byte) net.IP {\n\tbindLock.Lock()\n\tdefer bindLock.Unlock()\n\n\tresult := net.IPv4(127, 0, network, bindNum)\n\tbindNum++\n\tif bindNum > 255 {\n\t\tbindNum = 10\n\t}\n\n\treturn result\n}\n\nfunc getBindAddr() net.IP {\n\treturn getBindAddrNet(0)\n}\n\nfunc testConfigNet(tb testing.TB, network byte) *Config {\n\ttb.Helper()\n\n\tconfig := DefaultLANConfig()\n\tconfig.BindAddr = getBindAddrNet(network).String()\n\tconfig.Name = config.BindAddr\n\tconfig.BindPort = 0 // choose free port\n\tconfig.RequireNodeNames = true\n\tconfig.Logger = log.New(os.Stderr, config.Name, log.LstdFlags)\n\treturn config\n}\n\nfunc testConfig(tb testing.TB) *Config {\n\treturn testConfigNet(tb, 0)\n}\n\nfunc yield() {\n\ttime.Sleep(250 * time.Millisecond)\n}\n\ntype MockDelegate struct {\n\tmu          sync.Mutex\n\tmeta        []byte\n\tmsgs        [][]byte\n\tbroadcasts  [][]byte\n\tstate       []byte\n\tremoteState []byte\n}\n\nfunc (m *MockDelegate) setMeta(meta []byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tm.meta = meta\n}\n\nfunc (m *MockDelegate) setState(state []byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tm.state = state\n}\n\nfunc (m *MockDelegate) setBroadcasts(broadcasts [][]byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tm.broadcasts = broadcasts\n}\n\nfunc (m *MockDelegate) getRemoteState() []byte {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tout := make([]byte, len(m.remoteState))\n\tcopy(out, m.remoteState)\n\treturn out\n}\n\nfunc (m *MockDelegate) getMessages() [][]byte {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tout := make([][]byte, len(m.msgs))\n\tfor i, msg := range m.msgs {\n\t\tout[i] = make([]byte, len(msg))\n\t\tcopy(out[i], msg)\n\t}\n\treturn out\n}\n\nfunc (m *MockDelegate) NodeMeta(limit int) []byte {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\treturn m.meta\n}\n\nfunc (m *MockDelegate) NotifyMsg(msg []byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tcp := make([]byte, len(msg))\n\tcopy(cp, msg)\n\tm.msgs = append(m.msgs, cp)\n}\n\nfunc (m *MockDelegate) GetBroadcasts(overhead, limit int) [][]byte {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tb := m.broadcasts\n\tm.broadcasts = nil\n\treturn b\n}\n\nfunc (m *MockDelegate) LocalState(join bool) []byte {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\treturn m.state\n}\n\nfunc (m *MockDelegate) MergeRemoteState(s []byte, join bool) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tm.remoteState = s\n}\n\nfunc GetMemberlist(tb testing.TB, f func(c *Config)) *Memberlist {\n\tc := testConfig(tb)\n\tc.BindPort = 0 // assign a free port\n\tif f != nil {\n\t\tf(c)\n\t}\n\n\tm, err := newMemberlist(c)\n\trequire.NoError(tb, err)\n\treturn m\n}\n\nfunc TestDefaultLANConfig_protocolVersion(t *testing.T) {\n\tc := DefaultLANConfig()\n\tif c.ProtocolVersion != ProtocolVersion2Compatible {\n\t\tt.Fatalf(\"should be max: %d\", c.ProtocolVersion)\n\t}\n}\n\nfunc TestCreate_protocolVersion(t *testing.T) {\n\tcases := []struct {\n\t\tname    string\n\t\tversion uint8\n\t\terr     bool\n\t}{\n\t\t{\"min\", ProtocolVersionMin, false},\n\t\t{\"max\", ProtocolVersionMax, false},\n\t\t// TODO(mitchellh): uncommon when we're over 0\n\t\t//{\"uncommon\", ProtocolVersionMin - 1, true},\n\t\t{\"max+1\", ProtocolVersionMax + 1, true},\n\t\t{\"min-1\", ProtocolVersionMax - 1, false},\n\t}\n\n\tfor _, tc := range cases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tc := DefaultLANConfig()\n\t\t\tc.BindAddr = getBindAddr().String()\n\t\t\tc.ProtocolVersion = tc.version\n\n\t\t\tm, err := Create(c)\n\t\t\tif err == nil {\n\t\t\t\trequire.NoError(t, m.Shutdown())\n\t\t\t}\n\n\t\t\tif tc.err && err == nil {\n\t\t\t\tt.Fatalf(\"Should've failed with version: %d\", tc.version)\n\t\t\t} else if !tc.err && err != nil {\n\t\t\t\tt.Fatalf(\"Version '%d' error: %s\", tc.version, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestCreate_secretKey(t *testing.T) {\n\tcases := []struct {\n\t\tname string\n\t\tkey  []byte\n\t\terr  bool\n\t}{\n\t\t{\"size-0\", make([]byte, 0), false},\n\t\t{\"abc\", []byte(\"abc\"), true},\n\t\t{\"size-16\", make([]byte, 16), false},\n\t\t{\"size-38\", make([]byte, 38), true},\n\t}\n\n\tfor _, tc := range cases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tc := DefaultLANConfig()\n\t\t\tc.BindAddr = getBindAddr().String()\n\t\t\tc.SecretKey = tc.key\n\n\t\t\tm, err := Create(c)\n\t\t\tif err == nil {\n\t\t\t\trequire.NoError(t, m.Shutdown())\n\t\t\t}\n\n\t\t\tif tc.err && err == nil {\n\t\t\t\tt.Fatalf(\"Should've failed with key: %#v\", tc.key)\n\t\t\t} else if !tc.err && err != nil {\n\t\t\t\tt.Fatalf(\"Key '%#v' error: %s\", tc.key, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestCreate_secretKeyEmpty(t *testing.T) {\n\tc := DefaultLANConfig()\n\tc.BindAddr = getBindAddr().String()\n\tc.SecretKey = make([]byte, 0)\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tif m.config.EncryptionEnabled() {\n\t\tt.Fatalf(\"Expected encryption to be disabled\")\n\t}\n}\n\nfunc TestCreate_checkBroadcastQueueMetrics(t *testing.T) {\n\tsink := registerInMemorySink(t)\n\tc := DefaultLANConfig()\n\tc.QueueCheckInterval = 1 * time.Second\n\tc.BindAddr = getBindAddr().String()\n\tc.SecretKey = make([]byte, 0)\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\ttime.Sleep(3 * time.Second)\n\n\tsampleName := \"consul.usage.test.memberlist.queue.broadcasts\"\n\tverifySampleExists(t, sampleName, sink)\n}\n\nfunc TestCreate_keyringOnly(t *testing.T) {\n\tc := DefaultLANConfig()\n\tc.BindAddr = getBindAddr().String()\n\n\tkeyring, err := NewKeyring(nil, make([]byte, 16))\n\trequire.NoError(t, err)\n\tc.Keyring = keyring\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tif !m.config.EncryptionEnabled() {\n\t\tt.Fatalf(\"Expected encryption to be enabled\")\n\t}\n}\n\nfunc TestCreate_keyringAndSecretKey(t *testing.T) {\n\tc := DefaultLANConfig()\n\tc.BindAddr = getBindAddr().String()\n\n\tkeyring, err := NewKeyring(nil, make([]byte, 16))\n\trequire.NoError(t, err)\n\tc.Keyring = keyring\n\tc.SecretKey = []byte{1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tif !m.config.EncryptionEnabled() {\n\t\tt.Fatalf(\"Expected encryption to be enabled\")\n\t}\n\n\tringKeys := c.Keyring.GetKeys()\n\tif !bytes.Equal(c.SecretKey, ringKeys[0]) {\n\t\tt.Fatalf(\"Unexpected primary key %v\", ringKeys[0])\n\t}\n}\n\nfunc TestCreate_invalidLoggerSettings(t *testing.T) {\n\tc := DefaultLANConfig()\n\tc.BindAddr = getBindAddr().String()\n\tc.Logger = log.New(ioutil.Discard, \"\", log.LstdFlags)\n\tc.LogOutput = ioutil.Discard\n\n\tm, err := Create(c)\n\tif err == nil {\n\t\trequire.NoError(t, m.Shutdown())\n\t\tt.Fatal(\"Memberlist should not allow both LogOutput and Logger to be set, but it did not raise an error\")\n\t}\n}\n\nfunc TestCreate(t *testing.T) {\n\tc := testConfig(t)\n\tc.ProtocolVersion = ProtocolVersionMin\n\tc.DelegateProtocolVersion = 13\n\tc.DelegateProtocolMin = 12\n\tc.DelegateProtocolMax = 24\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tyield()\n\n\tmembers := m.Members()\n\tif len(members) != 1 {\n\t\tt.Fatalf(\"bad number of members\")\n\t}\n\n\tif members[0].PMin != ProtocolVersionMin {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n\n\tif members[0].PMax != ProtocolVersionMax {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n\n\tif members[0].PCur != c.ProtocolVersion {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n\n\tif members[0].DMin != c.DelegateProtocolMin {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n\n\tif members[0].DMax != c.DelegateProtocolMax {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n\n\tif members[0].DCur != c.DelegateProtocolVersion {\n\t\tt.Fatalf(\"bad: %#v\", members[0])\n\t}\n}\n\nfunc TestMemberList_CreateShutdown(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tm.schedule()\n\trequire.NoError(t, m.Shutdown())\n}\n\nfunc TestMemberList_ResolveAddr(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\tdefaultPort := uint16(m.config.BindPort)\n\n\ttype testCase struct {\n\t\tname           string\n\t\tin             string\n\t\texpectErr      bool\n\t\tignoreExpectIP bool\n\t\texpect         []ipPort\n\t}\n\n\tbaseCases := []testCase{\n\t\t{\n\t\t\tname:           \"localhost\",\n\t\t\tin:             \"localhost\",\n\t\t\tignoreExpectIP: true,\n\t\t\texpect: []ipPort{\n\t\t\t\t{port: defaultPort},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"ipv6 pair\",\n\t\t\tin:   \"[::1]:80\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{ip: net.IPv6loopback, port: 80},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"ipv6 non-pair\",\n\t\t\tin:   \"[::1]\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{ip: net.IPv6loopback, port: defaultPort},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"hostless port\",\n\t\t\tin:        \":80\",\n\t\t\texpectErr: true,\n\t\t},\n\t\t{\n\t\t\tname:           \"hostname port combo\",\n\t\t\tin:             \"localhost:80\",\n\t\t\tignoreExpectIP: true,\n\t\t\texpect: []ipPort{\n\t\t\t\t{port: 80},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"too high port\",\n\t\t\tin:        \"localhost:80000\",\n\t\t\texpectErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"ipv4 port combo\",\n\t\t\tin:   \"127.0.0.1:80\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{ip: net.IPv4(127, 0, 0, 1), port: 80},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"ipv6 port combo\",\n\t\t\tin:   \"[2001:db8:a0b:12f0::1]:80\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{\n\t\t\t\t\tip:   net.IP{0x20, 0x01, 0x0d, 0xb8, 0x0a, 0x0b, 0x12, 0xf0, 0, 0, 0, 0, 0, 0, 0, 0x1},\n\t\t\t\t\tport: 80,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"ipv4 port combo with empty tag\",\n\t\t\tin:        \"/127.0.0.1:80\",\n\t\t\texpectErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"ipv4 only\",\n\t\t\tin:   \"127.0.0.1\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{ip: net.IPv4(127, 0, 0, 1), port: defaultPort},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"ipv6 only\",\n\t\t\tin:   \"[2001:db8:a0b:12f0::1]\",\n\t\t\texpect: []ipPort{\n\t\t\t\t{\n\t\t\t\t\tip:   net.IP{0x20, 0x01, 0x0d, 0xb8, 0x0a, 0x0b, 0x12, 0xf0, 0, 0, 0, 0, 0, 0, 0, 0x1},\n\t\t\t\t\tport: defaultPort,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\t// explode the cases to include tagged versions of everything\n\tvar cases []testCase\n\tfor _, tc := range baseCases {\n\t\tcases = append(cases, tc)\n\t\tif !strings.Contains(tc.in, \"/\") { // don't double tag already tagged cases\n\t\t\ttc2 := testCase{\n\t\t\t\tname:           tc.name + \" (tagged)\",\n\t\t\t\tin:             \"foo.bar/\" + tc.in,\n\t\t\t\texpectErr:      tc.expectErr,\n\t\t\t\tignoreExpectIP: tc.ignoreExpectIP,\n\t\t\t}\n\t\t\tfor _, ipp := range tc.expect {\n\t\t\t\ttc2.expect = append(tc2.expect, ipPort{\n\t\t\t\t\tip:       ipp.ip,\n\t\t\t\t\tport:     ipp.port,\n\t\t\t\t\tnodeName: \"foo.bar\",\n\t\t\t\t})\n\t\t\t}\n\t\t\tcases = append(cases, tc2)\n\t\t}\n\t}\n\n\tfor _, tc := range cases {\n\t\ttc := tc\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\t\t\tgot, err := m.resolveAddr(tc.in)\n\t\t\tif tc.expectErr {\n\t\t\t\trequire.Error(t, err)\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tif tc.ignoreExpectIP {\n\t\t\t\t\tif len(got) > 1 {\n\t\t\t\t\t\tgot = got[0:1]\n\t\t\t\t\t}\n\t\t\t\t\tfor i := 0; i < len(got); i++ {\n\t\t\t\t\t\tgot[i].ip = nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trequire.Equal(t, tc.expect, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype dnsHandler struct {\n\tt *testing.T\n}\n\nfunc (h dnsHandler) ServeDNS(w dns.ResponseWriter, r *dns.Msg) {\n\tif len(r.Question) != 1 {\n\t\th.t.Fatalf(\"bad: %#v\", r.Question)\n\t}\n\n\tname := \"join.service.consul.\"\n\tquestion := r.Question[0]\n\tif question.Name != name || question.Qtype != dns.TypeANY {\n\t\th.t.Fatalf(\"bad: %#v\", question)\n\t}\n\n\tm := new(dns.Msg)\n\tm.SetReply(r)\n\tm.Authoritative = true\n\tm.RecursionAvailable = false\n\tm.Answer = append(m.Answer, &dns.A{\n\t\tHdr: dns.RR_Header{\n\t\t\tName:   name,\n\t\t\tRrtype: dns.TypeA,\n\t\t\tClass:  dns.ClassINET},\n\t\tA: net.ParseIP(\"127.0.0.1\"),\n\t})\n\tm.Answer = append(m.Answer, &dns.AAAA{\n\t\tHdr: dns.RR_Header{\n\t\t\tName:   name,\n\t\t\tRrtype: dns.TypeAAAA,\n\t\t\tClass:  dns.ClassINET},\n\t\tAAAA: net.ParseIP(\"2001:db8:a0b:12f0::1\"),\n\t})\n\tif err := w.WriteMsg(m); err != nil {\n\t\th.t.Fatalf(\"err: %v\", err)\n\t}\n}\n\nfunc TestMemberList_ResolveAddr_TCP_First(t *testing.T) {\n\tbind := \"127.0.0.1:8600\"\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tserver := &dns.Server{\n\t\tAddr:              bind,\n\t\tHandler:           dnsHandler{t},\n\t\tNet:               \"tcp\",\n\t\tNotifyStartedFunc: wg.Done,\n\t}\n\tdefer server.Shutdown()\n\n\tgo func() {\n\t\tif err := server.ListenAndServe(); err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n\t\t\tt.Errorf(\"err: %v\", err)\n\t\t}\n\t}()\n\twg.Wait()\n\n\ttmpFile, err := ioutil.TempFile(\"\", \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tdefer os.Remove(tmpFile.Name())\n\n\tcontent := []byte(fmt.Sprintf(\"nameserver %s\", bind))\n\tif _, err := tmpFile.Write(content); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tif err := tmpFile.Close(); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.DNSConfigPath = tmpFile.Name()\n\t})\n\tm.setAlive()\n\tm.schedule()\n\tdefer m.Shutdown()\n\n\t// Try with and without the trailing dot.\n\thosts := []string{\n\t\t\"join.service.consul.\",\n\t\t\"join.service.consul\",\n\t}\n\tfor _, host := range hosts {\n\t\tips, err := m.resolveAddr(host)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"err: %v\", err)\n\t\t}\n\t\tport := uint16(m.config.BindPort)\n\t\texpected := []ipPort{\n\t\t\t// Go now parses IPs like this and returns IP4-mapped IPv6 address.\n\t\t\t// Confusingly if you print it you see the same as the input since\n\t\t\t// IP.String converts IP4-mapped addresses back to dotted decimal notation\n\t\t\t// but the underlying IP bytes don't compare as equal to the actual IPv4\n\t\t\t// bytes the resolver will get from DNS.\n\t\t\tipPort{ip: net.ParseIP(\"127.0.0.1\").To4(), port: port, nodeName: \"\"},\n\t\t\tipPort{ip: net.ParseIP(\"2001:db8:a0b:12f0::1\"), port: port, nodeName: \"\"},\n\t\t}\n\t\trequire.Equal(t, expected, ips)\n\t}\n}\n\nfunc TestMemberList_Members(t *testing.T) {\n\tn1 := &Node{Name: \"test\"}\n\tn2 := &Node{Name: \"test2\"}\n\tn3 := &Node{Name: \"test3\"}\n\n\tm := &Memberlist{}\n\tnodes := []*nodeState{\n\t\t&nodeState{Node: *n1, State: StateAlive},\n\t\t&nodeState{Node: *n2, State: StateDead},\n\t\t&nodeState{Node: *n3, State: StateSuspect},\n\t}\n\tm.nodes = nodes\n\n\tmembers := m.Members()\n\tif !reflect.DeepEqual(members, []*Node{n1, n3}) {\n\t\tt.Fatalf(\"bad members\")\n\t}\n}\n\nfunc TestMemberlist_Join(t *testing.T) {\n\tc1 := testConfig(t)\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 1 {\n\t\tt.Fatalf(\"unexpected 1: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Check the hosts\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n\tif m2.estNumNodes() != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n}\n\nfunc TestMemberlist_Join_with_Labels(t *testing.T) {\n\ttestMemberlist_Join_with_Labels(t, nil)\n}\nfunc TestMemberlist_Join_with_Labels_and_Encryption(t *testing.T) {\n\tsecretKey := TestKeys[0]\n\ttestMemberlist_Join_with_Labels(t, secretKey)\n}\nfunc testMemberlist_Join_with_Labels(t *testing.T, secretKey []byte) {\n\tc1 := testConfig(t)\n\tc1.Label = \"blah\"\n\tc1.SecretKey = secretKey\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfig(t)\n\tc2.Label = \"blah\"\n\tc2.BindPort = bindPort\n\tc2.SecretKey = secretKey\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tcheckHost := func(t *testing.T, m *Memberlist, expected int) {\n\t\tassert.Equal(t, expected, len(m.Members()))\n\t\tassert.Equal(t, expected, m.estNumNodes())\n\t}\n\n\trunStep(t, \"same label can join\", func(t *testing.T) {\n\t\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, num)\n\n\t\t// Check the hosts\n\t\tcheckHost(t, m2, 2)\n\t\tcheckHost(t, m1, 2)\n\t})\n\n\t// Create a third node that uses no label\n\tc3 := testConfig(t)\n\tc3.Label = \"\"\n\tc3.BindPort = bindPort\n\tc3.SecretKey = secretKey\n\tm3, err := Create(c3)\n\trequire.NoError(t, err)\n\tdefer m3.Shutdown()\n\n\trunStep(t, \"no label cannot join\", func(t *testing.T) {\n\t\t_, err := m3.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\t\trequire.Error(t, err)\n\n\t\t// Check the failed host\n\t\tcheckHost(t, m3, 1)\n\t\t// Check the existing hosts\n\t\tcheckHost(t, m2, 2)\n\t\tcheckHost(t, m1, 2)\n\t})\n\n\t// Create a fourth node that uses a mismatched label\n\tc4 := testConfig(t)\n\tc4.Label = \"not-blah\"\n\tc4.BindPort = bindPort\n\tc4.SecretKey = secretKey\n\tm4, err := Create(c4)\n\trequire.NoError(t, err)\n\tdefer m4.Shutdown()\n\n\trunStep(t, \"mismatched label cannot join\", func(t *testing.T) {\n\t\t_, err := m4.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\t\trequire.Error(t, err)\n\n\t\t// Check the failed host\n\t\tcheckHost(t, m4, 1)\n\t\t// Check the previous failed host\n\t\tcheckHost(t, m3, 1)\n\t\t// Check the existing hosts\n\t\tcheckHost(t, m2, 2)\n\t\tcheckHost(t, m1, 2)\n\t})\n}\n\nfunc TestMemberlist_JoinDifferentNetworksUniqueMask(t *testing.T) {\n\tc1 := testConfigNet(t, 0)\n\tc1.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/8\"})\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfigNet(t, 1)\n\tc2.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/8\"})\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 1 {\n\t\tt.Fatalf(\"unexpected 1: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Check the hosts\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n\tif m2.estNumNodes() != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n}\n\nfunc TestMemberlist_JoinDifferentNetworksMultiMasks(t *testing.T) {\n\tc1 := testConfigNet(t, 0)\n\tc1.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/24\", \"127.0.1.0/24\"})\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfigNet(t, 1)\n\tc2.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/24\", \"127.0.1.0/24\"})\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\terr = joinAndTestMemberShip(t, m2, []string{m1.config.Name + \"/\" + m1.config.BindAddr}, 2)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Create a rogue node that allows all networks\n\t// It should see others, but will not be seen by others\n\tc3 := testConfigNet(t, 2)\n\tc3.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/8\"})\n\tc3.BindPort = bindPort\n\n\tm3, err := Create(c3)\n\trequire.NoError(t, err)\n\tdefer m3.Shutdown()\n\n\t// The rogue can see others, but others cannot see it\n\terr = joinAndTestMemberShip(t, m3, []string{m1.config.Name + \"/\" + m1.config.BindAddr}, 3)\n\t// For the node itself, everything seems fine, it should see others\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// m1 and m2 should not see newcomer however\n\tif len(m1.Members()) != 2 {\n\t\tt.Fatalf(\"m1 should have 2 nodes! %v\", m1.Members())\n\t}\n\tif m1.estNumNodes() != 2 {\n\t\tt.Fatalf(\"m1 should have 2 est. nodes! %v\", m1.estNumNodes())\n\t}\n\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"m2 should have 2 nodes! %v\", m2.Members())\n\t}\n\tif m2.estNumNodes() != 2 {\n\t\tt.Fatalf(\"m2 should have 2 est. nodes! %v\", m2.estNumNodes())\n\t}\n\n\t// Another rogue, this time with a config that denies itself\n\t// Create a rogue node that allows all networks\n\t// It should see others, but will not be seen by others\n\tc4 := testConfigNet(t, 2)\n\tc4.CIDRsAllowed, _ = ParseCIDRs([]string{\"127.0.0.0/24\", \"127.0.1.0/24\"})\n\tc4.BindPort = bindPort\n\n\tm4, err := Create(c4)\n\trequire.NoError(t, err)\n\tdefer m4.Shutdown()\n\n\t// This time, the node should not even see itself, so 2 expected nodes\n\terr = joinAndTestMemberShip(t, m4, []string{m1.config.BindAddr, m2.config.BindAddr}, 2)\n\t// m1 and m2 should not see newcomer however\n\tif len(m1.Members()) != 2 {\n\t\tt.Fatalf(\"m1 should have 2 nodes! %v\", m1.Members())\n\t}\n\tif m1.estNumNodes() != 2 {\n\t\tt.Fatalf(\"m1 should have 2 est. nodes! %v\", m1.estNumNodes())\n\t}\n\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"m2 should have 2 nodes! %v\", m2.Members())\n\t}\n\tif m2.estNumNodes() != 2 {\n\t\tt.Fatalf(\"m2 should have 2 est. nodes! %v\", m2.estNumNodes())\n\t}\n}\n\ntype CustomMergeDelegate struct {\n\tinvoked bool\n\tt       *testing.T\n}\n\nfunc (c *CustomMergeDelegate) NotifyMerge(nodes []*Node) error {\n\tc.t.Logf(\"Cancel merge\")\n\tc.invoked = true\n\treturn fmt.Errorf(\"Custom merge canceled\")\n}\n\nfunc TestMemberlist_Join_Cancel(t *testing.T) {\n\tc1 := testConfig(t)\n\tmerge1 := &CustomMergeDelegate{t: t}\n\tc1.Merge = merge1\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\tmerge2 := &CustomMergeDelegate{t: t}\n\tc2.Merge = merge2\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 0 {\n\t\tt.Fatalf(\"unexpected 0: %d\", num)\n\t}\n\tif !strings.Contains(err.Error(), \"Custom merge canceled\") {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Check the hosts\n\tif len(m2.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 nodes! %v\", m2.Members())\n\t}\n\tif len(m1.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 nodes! %v\", m1.Members())\n\t}\n\n\t// Check delegate invocation\n\tif !merge1.invoked {\n\t\tt.Fatalf(\"should invoke delegate\")\n\t}\n\tif !merge2.invoked {\n\t\tt.Fatalf(\"should invoke delegate\")\n\t}\n}\n\ntype CustomAliveDelegate struct {\n\tIgnore string\n\tcount  int\n\n\tt *testing.T\n}\n\nfunc (c *CustomAliveDelegate) NotifyAlive(peer *Node) error {\n\tc.count++\n\tif peer.Name == c.Ignore {\n\t\treturn nil\n\t}\n\tc.t.Logf(\"Cancel alive\")\n\treturn fmt.Errorf(\"Custom alive canceled\")\n}\n\nfunc TestMemberlist_Join_Cancel_Passive(t *testing.T) {\n\tc1 := testConfig(t)\n\talive1 := &CustomAliveDelegate{\n\t\tIgnore: c1.Name,\n\t\tt:      t,\n\t}\n\tc1.Alive = alive1\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\talive2 := &CustomAliveDelegate{\n\t\tIgnore: c2.Name,\n\t\tt:      t,\n\t}\n\tc2.Alive = alive2\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 1 {\n\t\tt.Fatalf(\"unexpected 1: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\t// Check the hosts\n\tif len(m2.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 nodes! %v\", m2.Members())\n\t}\n\tif len(m1.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 nodes! %v\", m1.Members())\n\t}\n\n\t// Check delegate invocation\n\tif alive1.count == 0 {\n\t\tt.Fatalf(\"should invoke delegate: %d\", alive1.count)\n\t}\n\tif alive2.count == 0 {\n\t\tt.Fatalf(\"should invoke delegate: %d\", alive2.count)\n\t}\n}\n\nfunc TestMemberlist_Join_protocolVersions(t *testing.T) {\n\tc1 := testConfig(t)\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tc3 := testConfig(t)\n\tc3.BindPort = bindPort\n\tc3.ProtocolVersion = ProtocolVersionMax\n\n\tm3, err := Create(c3)\n\trequire.NoError(t, err)\n\tdefer m3.Shutdown()\n\n\t_, err = m1.Join([]string{c2.Name + \"/\" + c2.BindAddr})\n\trequire.NoError(t, err)\n\n\tyield()\n\n\t_, err = m1.Join([]string{c3.Name + \"/\" + c3.BindAddr})\n\trequire.NoError(t, err)\n}\n\nfunc joinAndTestMemberShip(t *testing.T, self *Memberlist, membersToJoin []string, expectedMembers int) error {\n\tt.Helper()\n\tnum, err := self.Join(membersToJoin)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif num != len(membersToJoin) {\n\t\tt.Fatalf(\"unexpected %d, was expecting %d to be joined\", num, len(membersToJoin))\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\t// Check the hosts\n\tif len(self.Members()) != expectedMembers {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", self.Members())\n\t}\n\tif len(self.Members()) != expectedMembers {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", self.Members())\n\t}\n\treturn nil\n}\n\nfunc TestMemberlist_Leave(t *testing.T) {\n\tnewConfig := func() *Config {\n\t\tc := testConfig(t)\n\t\tc.GossipInterval = time.Millisecond\n\t\treturn c\n\t}\n\n\tc1 := newConfig()\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := newConfig()\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\terr = joinAndTestMemberShip(t, m2, []string{m1.config.Name + \"/\" + m1.config.BindAddr}, 2)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Leave\n\terr = m1.Leave(time.Second)\n\trequire.NoError(t, err)\n\n\t// Wait for leave\n\ttime.Sleep(10 * time.Millisecond)\n\n\t// m1 should think dead\n\tif len(m1.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 node\")\n\t}\n\n\tif len(m2.Members()) != 1 {\n\t\tt.Fatalf(\"should have 1 node\")\n\t}\n\n\tif m2.nodeMap[c1.Name].State != StateLeft {\n\t\tt.Fatalf(\"bad state\")\n\t}\n}\n\nfunc TestMemberlist_JoinShutdown(t *testing.T) {\n\tnewConfig := func() *Config {\n\t\tc := testConfig(t)\n\t\tc.ProbeInterval = time.Millisecond\n\t\tc.ProbeTimeout = 100 * time.Microsecond\n\t\tc.SuspicionMaxTimeoutMult = 1\n\t\treturn c\n\t}\n\n\tc1 := newConfig()\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := newConfig()\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 1 {\n\t\tt.Fatalf(\"unexpected 1: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\t// Check the hosts\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n\n\trequire.NoError(t, m1.Shutdown())\n\n\twaitForCondition(t, func() (bool, string) {\n\t\tn := len(m2.Members())\n\t\treturn n == 1, fmt.Sprintf(\"expected 1 node, got %d\", n)\n\t})\n}\n\nfunc TestMemberlist_delegateMeta(t *testing.T) {\n\tc1 := testConfig(t)\n\tc1.Delegate = &MockDelegate{meta: []byte(\"web\")}\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\tc2.Delegate = &MockDelegate{meta: []byte(\"lb\")}\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\t_, err = m1.Join([]string{c2.Name + \"/\" + c2.BindAddr})\n\trequire.NoError(t, err)\n\n\tyield()\n\n\tvar roles map[string]string\n\n\t// Check the roles of members of m1\n\tm1m := m1.Members()\n\tif len(m1m) != 2 {\n\t\tt.Fatalf(\"bad: %#v\", m1m)\n\t}\n\n\troles = make(map[string]string)\n\tfor _, m := range m1m {\n\t\troles[m.Name] = string(m.Meta)\n\t}\n\n\tif r := roles[c1.Name]; r != \"web\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n\t}\n\n\tif r := roles[c2.Name]; r != \"lb\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n\t}\n\n\t// Check the roles of members of m2\n\tm2m := m2.Members()\n\tif len(m2m) != 2 {\n\t\tt.Fatalf(\"bad: %#v\", m2m)\n\t}\n\n\troles = make(map[string]string)\n\tfor _, m := range m2m {\n\t\troles[m.Name] = string(m.Meta)\n\t}\n\n\tif r := roles[c1.Name]; r != \"web\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n\t}\n\n\tif r := roles[c2.Name]; r != \"lb\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n\t}\n}\n\nfunc TestMemberlist_delegateMeta_Update(t *testing.T) {\n\tc1 := testConfig(t)\n\tmock1 := &MockDelegate{meta: []byte(\"web\")}\n\tc1.Delegate = mock1\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\tmock2 := &MockDelegate{meta: []byte(\"lb\")}\n\tc2.Delegate = mock2\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\t_, err = m1.Join([]string{c2.Name + \"/\" + c2.BindAddr})\n\trequire.NoError(t, err)\n\n\tyield()\n\n\t// Update the meta data roles\n\tmock1.setMeta([]byte(\"api\"))\n\tmock2.setMeta([]byte(\"db\"))\n\n\terr = m1.UpdateNode(0)\n\trequire.NoError(t, err)\n\terr = m2.UpdateNode(0)\n\trequire.NoError(t, err)\n\n\tyield()\n\n\t// Check the updates have propagated\n\tvar roles map[string]string\n\n\t// Check the roles of members of m1\n\tm1m := m1.Members()\n\tif len(m1m) != 2 {\n\t\tt.Fatalf(\"bad: %#v\", m1m)\n\t}\n\n\troles = make(map[string]string)\n\tfor _, m := range m1m {\n\t\troles[m.Name] = string(m.Meta)\n\t}\n\n\tif r := roles[c1.Name]; r != \"api\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n\t}\n\n\tif r := roles[c2.Name]; r != \"db\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n\t}\n\n\t// Check the roles of members of m2\n\tm2m := m2.Members()\n\tif len(m2m) != 2 {\n\t\tt.Fatalf(\"bad: %#v\", m2m)\n\t}\n\n\troles = make(map[string]string)\n\tfor _, m := range m2m {\n\t\troles[m.Name] = string(m.Meta)\n\t}\n\n\tif r := roles[c1.Name]; r != \"api\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n\t}\n\n\tif r := roles[c2.Name]; r != \"db\" {\n\t\tt.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n\t}\n}\n\nfunc TestMemberlist_UserData(t *testing.T) {\n\tnewConfig := func() (*Config, *MockDelegate) {\n\t\td := &MockDelegate{}\n\t\tc := testConfig(t)\n\t\t// Set the gossip/pushpull intervals fast enough to get a reasonable test,\n\t\t// but slow enough to avoid \"sendto: operation not permitted\"\n\t\tc.GossipInterval = 100 * time.Millisecond\n\t\tc.PushPullInterval = 100 * time.Millisecond\n\t\tc.Delegate = d\n\t\treturn c, d\n\t}\n\n\tc1, d1 := newConfig()\n\td1.setState([]byte(\"something\"))\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tbcasts := make([][]byte, 256)\n\tfor i := range bcasts {\n\t\tbcasts[i] = []byte(fmt.Sprintf(\"%d\", i))\n\t}\n\n\t// Create a second node\n\tc2, d2 := newConfig()\n\tc2.BindPort = bindPort\n\n\t// Second delegate has things to send\n\td2.setBroadcasts(bcasts)\n\td2.setState([]byte(\"my state\"))\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\tif num != 1 {\n\t\tt.Fatalf(\"unexpected 1: %d\", num)\n\t}\n\trequire.NoError(t, err)\n\n\t// Check the hosts\n\tif m2.NumMembers() != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n\n\t// Wait for a little while\n\tiretry.Run(t, func(r *iretry.R) {\n\t\tmsgs1 := d1.getMessages()\n\n\t\t// Ensure we got the messages. Ordering of messages is not guaranteed so just\n\t\t// check we got them both in either order.\n\t\trequire.ElementsMatch(r, bcasts, msgs1)\n\n\t\trs1 := d1.getRemoteState()\n\t\trs2 := d2.getRemoteState()\n\n\t\t// Check the push/pull state\n\t\tif !reflect.DeepEqual(rs1, []byte(\"my state\")) {\n\t\t\tr.Fatalf(\"bad state %s\", rs1)\n\t\t}\n\t\tif !reflect.DeepEqual(rs2, []byte(\"something\")) {\n\t\t\tr.Fatalf(\"bad state %s\", rs2)\n\t\t}\n\t})\n}\n\nfunc TestMemberlist_SendTo(t *testing.T) {\n\tnewConfig := func() (*Config, *MockDelegate, net.IP) {\n\t\td := &MockDelegate{}\n\t\tc := testConfig(t)\n\t\tc.GossipInterval = time.Millisecond\n\t\tc.PushPullInterval = time.Millisecond\n\t\tc.Delegate = d\n\t\treturn c, d, net.ParseIP(c.BindAddr)\n\t}\n\n\tc1, d1, _ := newConfig()\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tc2, d2, addr2 := newConfig()\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, num)\n\n\t// Check the hosts\n\trequire.Equal(t, 2, m2.NumMembers(), \"should have 2 nodes! %v\", m2.Members())\n\n\t// Try to do a direct send\n\tm2Addr := &net.UDPAddr{\n\t\tIP:   addr2,\n\t\tPort: bindPort,\n\t}\n\tm2Address := Address{\n\t\tAddr: m2Addr.String(),\n\t\tName: m2.config.Name,\n\t}\n\tif err := m1.SendToAddress(m2Address, []byte(\"ping\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tm1Addr := &net.UDPAddr{\n\t\tIP:   net.ParseIP(m1.config.BindAddr),\n\t\tPort: bindPort,\n\t}\n\tm1Address := Address{\n\t\tAddr: m1Addr.String(),\n\t\tName: m1.config.Name,\n\t}\n\tif err := m2.SendToAddress(m1Address, []byte(\"pong\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\twaitForCondition(t, func() (bool, string) {\n\t\tmsgs := d1.getMessages()\n\t\treturn len(msgs) == 1, fmt.Sprintf(\"expected 1 message, got %d\", len(msgs))\n\t})\n\n\tmsgs1 := d1.getMessages()\n\tif !reflect.DeepEqual(msgs1[0], []byte(\"pong\")) {\n\t\tt.Fatalf(\"bad msg %v\", msgs1[0])\n\t}\n\n\twaitForCondition(t, func() (bool, string) {\n\t\tmsgs := d2.getMessages()\n\t\treturn len(msgs) == 1, fmt.Sprintf(\"expected 1 message, got %d\", len(msgs))\n\t})\n\tmsgs2 := d2.getMessages()\n\tif !reflect.DeepEqual(msgs2[0], []byte(\"ping\")) {\n\t\tt.Fatalf(\"bad msg %v\", msgs2[0])\n\t}\n}\n\nfunc waitForCondition(t *testing.T, fn func() (bool, string)) {\n\tstart := time.Now()\n\n\tvar msg string\n\tfor time.Since(start) < 20*time.Second {\n\t\tvar done bool\n\t\tdone, msg = fn()\n\t\tif done {\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(5 * time.Millisecond)\n\t}\n\tt.Fatalf(\"timeout waiting for condition: %v\", msg)\n}\n\nfunc TestMemberlistProtocolVersion(t *testing.T) {\n\tc := testConfig(t)\n\tc.ProtocolVersion = ProtocolVersionMax\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tresult := m.ProtocolVersion()\n\tif result != ProtocolVersionMax {\n\t\tt.Fatalf(\"bad: %d\", result)\n\t}\n}\n\nfunc TestMemberlist_Join_DeadNode(t *testing.T) {\n\tc1 := testConfig(t)\n\tc1.TCPTimeout = 50 * time.Millisecond\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second \"node\", which is just a TCP listener that\n\t// does not ever respond. This is to test our deadlines\n\taddr2 := getBindAddr()\n\tlist, err := net.Listen(\"tcp\", net.JoinHostPort(addr2.String(), strconv.Itoa(bindPort)))\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tdefer list.Close()\n\n\t// Ensure we don't hang forever\n\ttimer := time.AfterFunc(100*time.Millisecond, func() {\n\t\tpanic(\"should have timed out by now\")\n\t})\n\tdefer timer.Stop()\n\n\tnum, err := m1.Join([]string{\"fake/\" + addr2.String()})\n\tif num != 0 {\n\t\tt.Fatalf(\"unexpected 0: %d\", num)\n\t}\n\tif err == nil {\n\t\tt.Fatal(\"expect err\")\n\t}\n}\n\n// Tests that nodes running different versions of the protocol can successfully\n// discover each other and add themselves to their respective member lists.\nfunc TestMemberlist_Join_Protocol_Compatibility(t *testing.T) {\n\ttestProtocolVersionPair := func(t *testing.T, pv1 uint8, pv2 uint8) {\n\t\tt.Helper()\n\n\t\tc1 := testConfig(t)\n\t\tc1.ProtocolVersion = pv1\n\n\t\tm1, err := Create(c1)\n\t\trequire.NoError(t, err)\n\t\tdefer m1.Shutdown()\n\n\t\tbindPort := m1.config.BindPort\n\n\t\tc2 := testConfig(t)\n\t\tc2.BindPort = bindPort\n\t\tc2.ProtocolVersion = pv2\n\n\t\tm2, err := Create(c2)\n\t\trequire.NoError(t, err)\n\t\tdefer m2.Shutdown()\n\n\t\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, num)\n\n\t\t// Check the hosts\n\t\tif len(m2.Members()) != 2 {\n\t\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t\t}\n\n\t\t// Check the hosts\n\t\tif len(m1.Members()) != 2 {\n\t\t\tt.Fatalf(\"should have 2 nodes! %v\", m1.Members())\n\t\t}\n\t}\n\n\tt.Run(\"2,1\", func(t *testing.T) {\n\t\ttestProtocolVersionPair(t, 2, 1)\n\t})\n\tt.Run(\"2,3\", func(t *testing.T) {\n\t\ttestProtocolVersionPair(t, 2, 3)\n\t})\n\tt.Run(\"3,2\", func(t *testing.T) {\n\t\ttestProtocolVersionPair(t, 3, 2)\n\t})\n\tt.Run(\"3,1\", func(t *testing.T) {\n\t\ttestProtocolVersionPair(t, 3, 1)\n\t})\n}\n\nvar (\n\tipv6LoopbackAvailableOnce sync.Once\n\tipv6LoopbackAvailable     bool\n)\n\nfunc isIPv6LoopbackAvailable(t *testing.T) bool {\n\tconst ipv6LoopbackAddress = \"::1\"\n\tipv6LoopbackAvailableOnce.Do(func() {\n\t\tifaces, err := net.Interfaces()\n\t\trequire.NoError(t, err)\n\n\t\tfor _, iface := range ifaces {\n\t\t\tif iface.Flags&net.FlagLoopback == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\taddrs, err := iface.Addrs()\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor _, addr := range addrs {\n\t\t\t\tipaddr := addr.(*net.IPNet)\n\t\t\t\tif ipaddr.IP.String() == ipv6LoopbackAddress {\n\t\t\t\t\tipv6LoopbackAvailable = true\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tipv6LoopbackAvailable = false\n\t\tt.Logf(\"IPv6 loopback address %q not found, disabling tests that require it\", ipv6LoopbackAddress)\n\t})\n\n\treturn ipv6LoopbackAvailable\n}\n\nfunc TestMemberlist_Join_IPv6(t *testing.T) {\n\tif !isIPv6LoopbackAvailable(t) {\n\t\tt.SkipNow()\n\t\treturn\n\t}\n\t// Since this binds to all interfaces we need to exclude other tests\n\t// from grabbing an interface.\n\tbindLock.Lock()\n\tdefer bindLock.Unlock()\n\n\tc1 := DefaultLANConfig()\n\tc1.Name = \"A\"\n\tc1.BindAddr = \"[::1]\"\n\tc1.BindPort = 0 // choose free\n\tc1.Logger = log.New(os.Stderr, c1.Name, log.LstdFlags)\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\t// Create a second node\n\tc2 := DefaultLANConfig()\n\tc2.Name = \"B\"\n\tc2.BindAddr = \"[::1]\"\n\tc2.BindPort = 0 // choose free\n\tc2.Logger = log.New(os.Stderr, c2.Name, log.LstdFlags)\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{fmt.Sprintf(\"%s/%s:%d\", m1.config.Name, m1.config.BindAddr, m1.config.BindPort)})\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, num)\n\n\t// Check the hosts\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n\n\tif len(m1.Members()) != 2 {\n\t\tt.Fatalf(\"should have 2 nodes! %v\", m2.Members())\n\t}\n}\n\nfunc reservePort(t *testing.T, ip net.IP, purpose string) int {\n\tfor i := 0; i < 10; i++ {\n\t\ttcpAddr := &net.TCPAddr{IP: ip, Port: 0}\n\t\ttcpLn, err := net.ListenTCP(\"tcp\", tcpAddr)\n\t\tif err != nil {\n\t\t\tif strings.Contains(err.Error(), \"address already in use\") {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\n\t\tport := tcpLn.Addr().(*net.TCPAddr).Port\n\n\t\tudpAddr := &net.UDPAddr{IP: ip, Port: port}\n\t\tudpLn, err := net.ListenUDP(\"udp\", udpAddr)\n\t\tif err != nil {\n\t\t\t_ = tcpLn.Close()\n\t\t\tif strings.Contains(err.Error(), \"address already in use\") {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\n\t\tt.Logf(\"Using dynamic bind port %d for %s\", port, purpose)\n\t\t_ = tcpLn.Close()\n\t\t_ = udpLn.Close()\n\t\treturn port\n\t}\n\n\tt.Fatalf(\"could not find a free TCP+UDP port to listen on for %s\", purpose)\n\tpanic(\"IMPOSSIBLE\")\n}\n\nfunc TestAdvertiseAddr(t *testing.T) {\n\tbindAddr := getBindAddr()\n\tadvertiseAddr := getBindAddr()\n\n\tbindPort := reservePort(t, bindAddr, \"BIND\")\n\tadvertisePort := reservePort(t, advertiseAddr, \"ADVERTISE\")\n\n\tc := DefaultLANConfig()\n\tc.BindAddr = bindAddr.String()\n\tc.BindPort = bindPort\n\tc.Name = c.BindAddr\n\n\tc.AdvertiseAddr = advertiseAddr.String()\n\tc.AdvertisePort = advertisePort\n\n\tm, err := Create(c)\n\trequire.NoError(t, err)\n\tdefer m.Shutdown()\n\n\tyield()\n\n\tmembers := m.Members()\n\trequire.Equal(t, 1, len(members))\n\n\trequire.Equal(t, advertiseAddr.String(), members[0].Addr.String())\n\trequire.Equal(t, advertisePort, int(members[0].Port))\n}\n\ntype MockConflict struct {\n\texisting *Node\n\tother    *Node\n}\n\nfunc (m *MockConflict) NotifyConflict(existing, other *Node) {\n\tm.existing = existing\n\tm.other = other\n}\n\nfunc TestMemberlist_conflictDelegate(t *testing.T) {\n\tc1 := testConfig(t)\n\tmock := &MockConflict{}\n\tc1.Conflict = mock\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Ensure name conflict\n\tc2 := testConfig(t)\n\tc2.Name = c1.Name\n\tc2.BindPort = bindPort\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m1.Join([]string{c2.Name + \"/\" + c2.BindAddr})\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, num)\n\n\tyield()\n\n\t// Ensure we were notified\n\tif mock.existing == nil || mock.other == nil {\n\t\tt.Fatalf(\"should get notified mock.existing=%v  VS mock.other=%v\", mock.existing, mock.other)\n\t}\n\tif mock.existing.Name != mock.other.Name {\n\t\tt.Fatalf(\"bad: %v %v\", mock.existing, mock.other)\n\t}\n}\n\ntype MockPing struct {\n\tmu      sync.Mutex\n\tother   *Node\n\trtt     time.Duration\n\tpayload []byte\n}\n\nfunc (m *MockPing) NotifyPingComplete(other *Node, rtt time.Duration, payload []byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tm.other = other\n\tm.rtt = rtt\n\tm.payload = payload\n}\n\nfunc (m *MockPing) getContents() (*Node, time.Duration, []byte) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\treturn m.other, m.rtt, m.payload\n}\n\nconst DEFAULT_PAYLOAD = \"whatever\"\n\nfunc (m *MockPing) AckPayload() []byte {\n\treturn []byte(DEFAULT_PAYLOAD)\n}\n\nfunc TestMemberlist_PingDelegate(t *testing.T) {\n\tnewConfig := func() *Config {\n\t\tc := testConfig(t)\n\t\tc.ProbeInterval = 100 * time.Millisecond\n\t\tc.Ping = &MockPing{}\n\t\treturn c\n\t}\n\n\tc1 := newConfig()\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\t// Create a second node\n\tc2 := newConfig()\n\tc2.BindPort = bindPort\n\tmock := c2.Ping.(*MockPing)\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{m1.config.Name + \"/\" + m1.config.BindAddr})\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, num)\n\n\twaitUntilSize(t, m1, 2)\n\twaitUntilSize(t, m2, 2)\n\n\ttime.Sleep(2 * c1.ProbeInterval)\n\n\trequire.NoError(t, m1.Shutdown())\n\trequire.NoError(t, m2.Shutdown())\n\n\tmOther, mRTT, mPayload := mock.getContents()\n\n\t// Ensure we were notified\n\tif mOther == nil {\n\t\tt.Fatalf(\"should get notified\")\n\t}\n\n\tif !reflect.DeepEqual(mOther, m1.LocalNode()) {\n\t\tt.Fatalf(\"not notified about the correct node; expected: %+v; actual: %+v\",\n\t\t\tm2.LocalNode(), mOther)\n\t}\n\n\tif mRTT <= 0 {\n\t\tt.Fatalf(\"rtt should be greater than 0\")\n\t}\n\n\tif bytes.Compare(mPayload, []byte(DEFAULT_PAYLOAD)) != 0 {\n\t\tt.Fatalf(\"incorrect payload. expected: %v; actual: %v\",\n\t\t\t[]byte(DEFAULT_PAYLOAD), mPayload)\n\t}\n}\n\nfunc waitUntilSize(t *testing.T, m *Memberlist, expected int) {\n\tt.Helper()\n\tretry(t, 15, 500*time.Millisecond, func(failf func(string, ...interface{})) {\n\t\tt.Helper()\n\n\t\tif m.NumMembers() != expected {\n\t\t\tfailf(\"%s expected to have %d members but had: %v\", m.config.Name, expected, m.Members())\n\t\t}\n\t})\n}\n\nfunc isPortFree(t *testing.T, addr string, port int) error {\n\tt.Helper()\n\n\tip := net.ParseIP(addr)\n\ttcpAddr := &net.TCPAddr{IP: ip, Port: port}\n\ttcpLn, err := net.ListenTCP(\"tcp\", tcpAddr)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := tcpLn.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tudpAddr := &net.UDPAddr{IP: ip, Port: port}\n\tudpLn, err := net.ListenUDP(\"udp\", udpAddr)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn udpLn.Close()\n}\n\nfunc waitUntilPortIsFree(t *testing.T, m *Memberlist) {\n\tt.Helper()\n\n\t// wait until we know for certain that m1 is dead dead\n\taddr := m.config.BindAddr\n\tport := m.config.BindPort\n\n\tretry(t, 15, 250*time.Millisecond, func(failf func(string, ...interface{})) {\n\t\tt.Helper()\n\n\t\tif err := isPortFree(t, addr, port); err != nil {\n\t\t\tfailf(\"%s port is not yet free\", m.config.Name)\n\t\t}\n\t})\n}\n\n// This test should follow the recommended upgrade guide:\n// https://www.consul.io/docs/agent/encryption.html#configuring-gossip-encryption-on-an-existing-cluster\n//\n// We will use two nodes for this: m0 and m1\n//\n// 0. Start with nodes without encryption.\n// 1. Set an encryption key and set GossipVerifyIncoming=false and GossipVerifyOutgoing=false to all nodes.\n// 2. Change GossipVerifyOutgoing=true to all nodes.\n// 3. Change GossipVerifyIncoming=true to all nodes.\nfunc TestMemberlist_EncryptedGossipTransition(t *testing.T) {\n\t// ensure these all get the same general set of customizations\n\tpretty := make(map[string]string) // addr->shortName\n\tnewConfig := func(shortName string, addr string) *Config {\n\t\tt.Helper()\n\n\t\tconf := DefaultLANConfig()\n\t\tif addr == \"\" {\n\t\t\taddr = getBindAddr().String()\n\t\t}\n\t\tconf.Name = addr\n\t\t// conf.Name = shortName\n\t\tconf.BindAddr = addr\n\t\tconf.BindPort = 0\n\t\t// Set the gossip interval fast enough to get a reasonable test,\n\t\t// but slow enough to avoid \"sendto: operation not permitted\"\n\t\tconf.GossipInterval = 100 * time.Millisecond\n\t\tconf.Logger = log.New(os.Stderr, shortName, log.LstdFlags)\n\n\t\tpretty[conf.Name] = shortName\n\t\treturn conf\n\t}\n\n\tvar bindPort int\n\tcreateOK := func(conf *Config) *Memberlist {\n\t\tt.Helper()\n\n\t\tif bindPort > 0 {\n\t\t\tconf.BindPort = bindPort\n\t\t} else {\n\t\t\t// try a range of port numbers until something sticks\n\t\t}\n\t\tm, err := Create(conf)\n\t\trequire.NoError(t, err)\n\n\t\tif bindPort == 0 {\n\t\t\tbindPort = m.config.BindPort\n\t\t}\n\t\treturn m\n\t}\n\n\tjoinOK := func(src, dst *Memberlist, numNodes int) {\n\t\tt.Helper()\n\n\t\tsrcName, dstName := pretty[src.config.Name], pretty[dst.config.Name]\n\t\tt.Logf(\"Node %s[%s] joining node %s[%s]\", srcName, src.config.Name, dstName, dst.config.Name)\n\n\t\tnum, err := src.Join([]string{dst.config.Name + \"/\" + dst.config.BindAddr})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, 1, num)\n\n\t\twaitUntilSize(t, src, numNodes)\n\t\twaitUntilSize(t, dst, numNodes)\n\n\t\t// Check the hosts\n\t\trequire.Equal(t, numNodes, len(src.Members()), \"nodes: %v\", src.Members())\n\t\trequire.Equal(t, numNodes, src.estNumNodes(), \"nodes: %v\", src.Members())\n\t\trequire.Equal(t, numNodes, len(dst.Members()), \"nodes: %v\", dst.Members())\n\t\trequire.Equal(t, numNodes, dst.estNumNodes(), \"nodes: %v\", dst.Members())\n\t}\n\n\tleaveOK := func(m *Memberlist, why string) {\n\t\tt.Helper()\n\n\t\tname := pretty[m.config.Name]\n\t\tt.Logf(\"Node %s[%s] is leaving %s\", name, m.config.Name, why)\n\t\terr := m.Leave(time.Second)\n\t\trequire.NoError(t, err)\n\t}\n\n\tshutdownOK := func(m *Memberlist, why string) {\n\t\tt.Helper()\n\n\t\tname := pretty[m.config.Name]\n\t\tt.Logf(\"Node %s[%s] is shutting down %s\", name, m.config.Name, why)\n\t\terr := m.Shutdown()\n\t\trequire.NoError(t, err)\n\n\t\t// Double check that it genuinely shutdown.\n\t\twaitUntilPortIsFree(t, m)\n\t}\n\n\tleaveAndShutdown := func(leaver, bystander *Memberlist, why string) {\n\t\tt.Helper()\n\n\t\tleaveOK(leaver, why)\n\t\twaitUntilSize(t, bystander, 1)\n\t\tshutdownOK(leaver, why)\n\t\twaitUntilSize(t, bystander, 1)\n\t}\n\n\t// ==== STEP 0 ====\n\n\t// Create a first cluster of 2 nodes with no gossip encryption settings.\n\tconf0 := newConfig(\"m0\", \"\")\n\tm0 := createOK(conf0)\n\tdefer m0.Shutdown()\n\n\tconf1 := newConfig(\"m1\", \"\")\n\tm1 := createOK(conf1)\n\tdefer m1.Shutdown()\n\n\tjoinOK(m1, m0, 2)\n\n\tt.Logf(\"==== STEP 0 complete: two node unencrypted cluster ====\")\n\n\t// ==== STEP 1 ====\n\n\t// Take down m0, upgrade to first stage of gossip transition settings.\n\tleaveAndShutdown(m0, m1, \"to upgrade gossip to first stage\")\n\n\t// Resurrect the first node with the first stage of gossip transition settings.\n\tconf0 = newConfig(\"m0\", m0.config.BindAddr)\n\tconf0.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tconf0.GossipVerifyIncoming = false\n\tconf0.GossipVerifyOutgoing = false\n\tm0 = createOK(conf0)\n\tdefer m0.Shutdown()\n\n\t// Join the second node. m1 has no encryption while m0 has encryption configured and\n\t// can receive encrypted gossip, but will not encrypt outgoing gossip.\n\tjoinOK(m0, m1, 2)\n\n\tleaveAndShutdown(m1, m0, \"to upgrade gossip to first stage\")\n\n\t// Resurrect the second node with the first stage of gossip transition settings.\n\tconf1 = newConfig(\"m1\", m1.config.BindAddr)\n\tconf1.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tconf1.GossipVerifyIncoming = false\n\tconf1.GossipVerifyOutgoing = false\n\tm1 = createOK(conf1)\n\tdefer m1.Shutdown()\n\n\t// Join the first node. Both have encryption configured and can receive\n\t// encrypted gossip, but will not encrypt outgoing gossip.\n\tjoinOK(m1, m0, 2)\n\n\tt.Logf(\"==== STEP 1 complete: two node encryption-aware cluster ====\")\n\n\t// ==== STEP 2 ====\n\n\t// Take down m0, upgrade to second stage of gossip transition settings.\n\tleaveAndShutdown(m0, m1, \"to upgrade gossip to second stage\")\n\n\t// Resurrect the first node with the second stage of gossip transition settings.\n\tconf0 = newConfig(\"m0\", m0.config.BindAddr)\n\tconf0.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tconf0.GossipVerifyIncoming = false\n\tm0 = createOK(conf0)\n\tdefer m0.Shutdown()\n\n\t// Join the second node. At this step, both nodes have encryption\n\t// configured but only m0 is sending encrypted gossip.\n\tjoinOK(m0, m1, 2)\n\n\tleaveAndShutdown(m1, m0, \"to upgrade gossip to second stage\")\n\n\t// Resurrect the second node with the second stage of gossip transition settings.\n\tconf1 = newConfig(\"m1\", m1.config.BindAddr)\n\tconf1.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tconf1.GossipVerifyIncoming = false\n\tm1 = createOK(conf1)\n\tdefer m1.Shutdown()\n\n\t// Join the first node. Both have encryption configured and can receive\n\t// encrypted gossip, and encrypt outgoing gossip, but aren't forcing\n\t// incoming gossip is encrypted.\n\tjoinOK(m1, m0, 2)\n\n\tt.Logf(\"==== STEP 2 complete: two node encryption-aware cluster being encrypted ====\")\n\n\t// ==== STEP 3 ====\n\n\t// Take down m0, upgrade to final stage of gossip transition settings.\n\tleaveAndShutdown(m0, m1, \"to upgrade gossip to final stage\")\n\n\t// Resurrect the first node with the final stage of gossip transition settings.\n\tconf0 = newConfig(\"m0\", m0.config.BindAddr)\n\tconf0.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tm0 = createOK(conf0)\n\tdefer m0.Shutdown()\n\n\t// Join the second node. At this step, both nodes have encryption\n\t// configured and are sending it, bu tonly m0 is verifying inbound gossip\n\t// is encrypted.\n\tjoinOK(m0, m1, 2)\n\n\tleaveAndShutdown(m1, m0, \"to upgrade gossip to final stage\")\n\n\t// Resurrect the second node with the final stage of gossip transition settings.\n\tconf1 = newConfig(\"m1\", m1.config.BindAddr)\n\tconf1.SecretKey = []byte(\"Hi16ZXu2lNCRVwtr20khAg==\")\n\tm1 = createOK(conf1)\n\tdefer m1.Shutdown()\n\n\t// Join the first node. Both have encryption configured and fully in\n\t// enforcement.\n\tjoinOK(m1, m0, 2)\n\n\tt.Logf(\"==== STEP 3 complete: two node encrypted cluster locked down ====\")\n}\n\n// Consul bug, rapid restart (before failure detection),\n// with an updated meta data. Should be at incarnation 1 for\n// both.\n//\n// This test is uncommented because it requires that either we\n// can rebind the socket (SO_REUSEPORT) which Go does not allow,\n// OR we must disable the address conflict checking in memberlist.\n// I just comment out that code to test this case.\n//\n//func TestMemberlist_Restart_delegateMeta_Update(t *testing.T) {\n//    c1 := testConfig()\n//    c2 := testConfig()\n//    mock1 := &MockDelegate{meta: []byte(\"web\")}\n//    mock2 := &MockDelegate{meta: []byte(\"lb\")}\n//    c1.Delegate = mock1\n//    c2.Delegate = mock2\n\n//    m1, err := Create(c1)\n//    if err != nil {\n//        t.Fatalf(\"err: %s\", err)\n//    }\n//    defer m1.Shutdown()\n\n//    m2, err := Create(c2)\n//    if err != nil {\n//        t.Fatalf(\"err: %s\", err)\n//    }\n//    defer m2.Shutdown()\n\n//    _, err = m1.Join([]string{c2.BindAddr})\n//    if err != nil {\n//        t.Fatalf(\"err: %s\", err)\n//    }\n\n//    yield()\n\n//    // Recreate m1 with updated meta\n//    m1.Shutdown()\n//    c3 := testConfig()\n//    c3.Name = c1.Name\n//    c3.Delegate = mock1\n//    c3.GossipInterval = time.Millisecond\n//    mock1.meta = []byte(\"api\")\n\n//    m1, err = Create(c3)\n//    if err != nil {\n//        t.Fatalf(\"err: %s\", err)\n//    }\n//    defer m1.Shutdown()\n\n//    _, err = m1.Join([]string{c2.BindAddr})\n//    if err != nil {\n//        t.Fatalf(\"err: %s\", err)\n//    }\n\n//    yield()\n//    yield()\n\n//    // Check the updates have propagated\n//    var roles map[string]string\n\n//    // Check the roles of members of m1\n//    m1m := m1.Members()\n//    if len(m1m) != 2 {\n//        t.Fatalf(\"bad: %#v\", m1m)\n//    }\n\n//    roles = make(map[string]string)\n//    for _, m := range m1m {\n//        roles[m.Name] = string(m.Meta)\n//    }\n\n//    if r := roles[c1.Name]; r != \"api\" {\n//        t.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n//    }\n\n//    if r := roles[c2.Name]; r != \"lb\" {\n//        t.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n//    }\n\n//    // Check the roles of members of m2\n//    m2m := m2.Members()\n//    if len(m2m) != 2 {\n//        t.Fatalf(\"bad: %#v\", m2m)\n//    }\n\n//    roles = make(map[string]string)\n//    for _, m := range m2m {\n//        roles[m.Name] = string(m.Meta)\n//    }\n\n//    if r := roles[c1.Name]; r != \"api\" {\n//        t.Fatalf(\"bad role for %s: %s\", c1.Name, r)\n//    }\n\n//    if r := roles[c2.Name]; r != \"lb\" {\n//        t.Fatalf(\"bad role for %s: %s\", c2.Name, r)\n//    }\n//}\n\nfunc runStep(t *testing.T, name string, fn func(t *testing.T)) {\n\tt.Helper()\n\tif !t.Run(name, fn) {\n\t\tt.FailNow()\n\t}\n}\n"
        },
        {
          "name": "merge_delegate.go",
          "type": "blob",
          "size": 0.625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\n// MergeDelegate is used to involve a client in\n// a potential cluster merge operation. Namely, when\n// a node does a TCP push/pull (as part of a join),\n// the delegate is involved and allowed to cancel the join\n// based on custom logic. The merge delegate is NOT invoked\n// as part of the push-pull anti-entropy.\ntype MergeDelegate interface {\n\t// NotifyMerge is invoked when a merge could take place.\n\t// Provides a list of the nodes known by the peer. If\n\t// the return value is non-nil, the merge is canceled.\n\tNotifyMerge(peers []*Node) error\n}\n"
        },
        {
          "name": "mock_transport.go",
          "type": "blob",
          "size": 4.35546875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"strconv\"\n\t\"time\"\n)\n\n// MockNetwork is used as a factory that produces MockTransport instances which\n// are uniquely addressed and wired up to talk to each other.\ntype MockNetwork struct {\n\ttransportsByAddr map[string]*MockTransport\n\ttransportsByName map[string]*MockTransport\n\tport             int\n}\n\n// NewTransport returns a new MockTransport with a unique address, wired up to\n// talk to the other transports in the MockNetwork.\nfunc (n *MockNetwork) NewTransport(name string) *MockTransport {\n\tn.port += 1\n\taddr := fmt.Sprintf(\"127.0.0.1:%d\", n.port)\n\ttransport := &MockTransport{\n\t\tnet:      n,\n\t\taddr:     &MockAddress{addr, name},\n\t\tpacketCh: make(chan *Packet),\n\t\tstreamCh: make(chan net.Conn),\n\t}\n\n\tif n.transportsByAddr == nil {\n\t\tn.transportsByAddr = make(map[string]*MockTransport)\n\t}\n\tn.transportsByAddr[addr] = transport\n\n\tif n.transportsByName == nil {\n\t\tn.transportsByName = make(map[string]*MockTransport)\n\t}\n\tn.transportsByName[name] = transport\n\n\treturn transport\n}\n\n// MockAddress is a wrapper which adds the net.Addr interface to our mock\n// address scheme.\ntype MockAddress struct {\n\taddr string\n\tname string\n}\n\n// See net.Addr.\nfunc (a *MockAddress) Network() string {\n\treturn \"mock\"\n}\n\n// See net.Addr.\nfunc (a *MockAddress) String() string {\n\treturn a.addr\n}\n\n// MockTransport directly plumbs messages to other transports its MockNetwork.\ntype MockTransport struct {\n\tnet      *MockNetwork\n\taddr     *MockAddress\n\tpacketCh chan *Packet\n\tstreamCh chan net.Conn\n}\n\nvar _ NodeAwareTransport = (*MockTransport)(nil)\n\n// See Transport.\nfunc (t *MockTransport) FinalAdvertiseAddr(string, int) (net.IP, int, error) {\n\thost, portStr, err := net.SplitHostPort(t.addr.String())\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\tip := net.ParseIP(host)\n\tif ip == nil {\n\t\treturn nil, 0, fmt.Errorf(\"Failed to parse IP %q\", host)\n\t}\n\n\tport, err := strconv.ParseInt(portStr, 10, 16)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\treturn ip, int(port), nil\n}\n\n// See Transport.\nfunc (t *MockTransport) WriteTo(b []byte, addr string) (time.Time, error) {\n\ta := Address{Addr: addr, Name: \"\"}\n\treturn t.WriteToAddress(b, a)\n}\n\n// See NodeAwareTransport.\nfunc (t *MockTransport) WriteToAddress(b []byte, a Address) (time.Time, error) {\n\tdest, err := t.getPeer(a)\n\tif err != nil {\n\t\treturn time.Time{}, err\n\t}\n\n\tnow := time.Now()\n\tdest.packetCh <- &Packet{\n\t\tBuf:       b,\n\t\tFrom:      t.addr,\n\t\tTimestamp: now,\n\t}\n\treturn now, nil\n}\n\n// See Transport.\nfunc (t *MockTransport) PacketCh() <-chan *Packet {\n\treturn t.packetCh\n}\n\n// See NodeAwareTransport.\nfunc (t *MockTransport) IngestPacket(conn net.Conn, addr net.Addr, now time.Time, shouldClose bool) error {\n\tif shouldClose {\n\t\tdefer conn.Close()\n\t}\n\n\t// Copy everything from the stream into packet buffer.\n\tvar buf bytes.Buffer\n\tif _, err := io.Copy(&buf, conn); err != nil {\n\t\treturn fmt.Errorf(\"failed to read packet: %v\", err)\n\t}\n\n\t// Check the length - it needs to have at least one byte to be a proper\n\t// message. This is checked elsewhere for writes coming in directly from\n\t// the UDP socket.\n\tif n := buf.Len(); n < 1 {\n\t\treturn fmt.Errorf(\"packet too short (%d bytes) %s\", n, LogAddress(addr))\n\t}\n\n\t// Inject the packet.\n\tt.packetCh <- &Packet{\n\t\tBuf:       buf.Bytes(),\n\t\tFrom:      addr,\n\t\tTimestamp: now,\n\t}\n\treturn nil\n}\n\n// See Transport.\nfunc (t *MockTransport) DialTimeout(addr string, timeout time.Duration) (net.Conn, error) {\n\ta := Address{Addr: addr, Name: \"\"}\n\treturn t.DialAddressTimeout(a, timeout)\n}\n\n// See NodeAwareTransport.\nfunc (t *MockTransport) DialAddressTimeout(a Address, timeout time.Duration) (net.Conn, error) {\n\tdest, err := t.getPeer(a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp1, p2 := net.Pipe()\n\tdest.streamCh <- p1\n\treturn p2, nil\n}\n\n// See Transport.\nfunc (t *MockTransport) StreamCh() <-chan net.Conn {\n\treturn t.streamCh\n}\n\n// See NodeAwareTransport.\nfunc (t *MockTransport) IngestStream(conn net.Conn) error {\n\tt.streamCh <- conn\n\treturn nil\n}\n\n// See Transport.\nfunc (t *MockTransport) Shutdown() error {\n\treturn nil\n}\n\nfunc (t *MockTransport) getPeer(a Address) (*MockTransport, error) {\n\tvar (\n\t\tdest *MockTransport\n\t\tok   bool\n\t)\n\tif a.Name != \"\" {\n\t\tdest, ok = t.net.transportsByName[a.Name]\n\t} else {\n\t\tdest, ok = t.net.transportsByAddr[a.Addr]\n\t}\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"No route to %s\", a)\n\t}\n\treturn dest, nil\n}\n"
        },
        {
          "name": "net.go",
          "type": "blob",
          "size": 38.4384765625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"math\"\n\t\"net\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n\t\"github.com/hashicorp/go-msgpack/v2/codec\"\n)\n\n// This is the minimum and maximum protocol version that we can\n// _understand_. We're allowed to speak at any version within this\n// range. This range is inclusive.\nconst (\n\tProtocolVersionMin uint8 = 1\n\n\t// Version 3 added support for TCP pings but we kept the default\n\t// protocol version at 2 to ease transition to this new feature.\n\t// A memberlist speaking version 2 of the protocol will attempt\n\t// to TCP ping another memberlist who understands version 3 or\n\t// greater.\n\t//\n\t// Version 4 added support for nacks as part of indirect probes.\n\t// A memberlist speaking version 2 of the protocol will expect\n\t// nacks from another memberlist who understands version 4 or\n\t// greater, and likewise nacks will be sent to memberlists who\n\t// understand version 4 or greater.\n\tProtocolVersion2Compatible = 2\n\n\tProtocolVersionMax = 5\n)\n\n// messageType is an integer ID of a type of message that can be received\n// on network channels from other members.\ntype messageType uint8\n\n// The list of available message types.\n//\n// WARNING: ONLY APPEND TO THIS LIST! The numeric values are part of the\n// protocol itself.\nconst (\n\tpingMsg messageType = iota\n\tindirectPingMsg\n\tackRespMsg\n\tsuspectMsg\n\taliveMsg\n\tdeadMsg\n\tpushPullMsg\n\tcompoundMsg\n\tuserMsg // User mesg, not handled by us\n\tcompressMsg\n\tencryptMsg\n\tnackRespMsg\n\thasCrcMsg\n\terrMsg\n)\n\nconst (\n\t// hasLabelMsg has a deliberately high value so that you can disambiguate\n\t// it from the encryptionVersion header which is either 0/1 right now and\n\t// also any of the existing messageTypes\n\thasLabelMsg messageType = 244\n)\n\n// compressionType is used to specify the compression algorithm\ntype compressionType uint8\n\nconst (\n\tlzwAlgo compressionType = iota\n)\n\nconst (\n\tMetaMaxSize            = 512 // Maximum size for node meta data\n\tcompoundHeaderOverhead = 2   // Assumed header overhead\n\tcompoundOverhead       = 2   // Assumed overhead per entry in compoundHeader\n\tuserMsgOverhead        = 1\n\tblockingWarning        = 10 * time.Millisecond // Warn if a UDP packet takes this long to process\n\tmaxPushStateBytes      = 20 * 1024 * 1024\n\tmaxPushPullRequests    = 128 // Maximum number of concurrent push/pull requests\n)\n\n// ping request sent directly to node\ntype ping struct {\n\tSeqNo uint32\n\n\t// Node is sent so the target can verify they are\n\t// the intended recipient. This is to protect again an agent\n\t// restart with a new name.\n\tNode string\n\n\tSourceAddr []byte `codec:\",omitempty\"` // Source address, used for a direct reply\n\tSourcePort uint16 `codec:\",omitempty\"` // Source port, used for a direct reply\n\tSourceNode string `codec:\",omitempty\"` // Source name, used for a direct reply\n}\n\n// indirect ping sent to an indirect node\ntype indirectPingReq struct {\n\tSeqNo  uint32\n\tTarget []byte\n\tPort   uint16\n\n\t// Node is sent so the target can verify they are\n\t// the intended recipient. This is to protect against an agent\n\t// restart with a new name.\n\tNode string\n\n\tNack bool // true if we'd like a nack back\n\n\tSourceAddr []byte `codec:\",omitempty\"` // Source address, used for a direct reply\n\tSourcePort uint16 `codec:\",omitempty\"` // Source port, used for a direct reply\n\tSourceNode string `codec:\",omitempty\"` // Source name, used for a direct reply\n}\n\n// ack response is sent for a ping\ntype ackResp struct {\n\tSeqNo   uint32\n\tPayload []byte\n}\n\n// nack response is sent for an indirect ping when the pinger doesn't hear from\n// the ping-ee within the configured timeout. This lets the original node know\n// that the indirect ping attempt happened but didn't succeed.\ntype nackResp struct {\n\tSeqNo uint32\n}\n\n// err response is sent to relay the error from the remote end\ntype errResp struct {\n\tError string\n}\n\n// suspect is broadcast when we suspect a node is dead\ntype suspect struct {\n\tIncarnation uint32\n\tNode        string\n\tFrom        string // Include who is suspecting\n}\n\n// alive is broadcast when we know a node is alive.\n// Overloaded for nodes joining\ntype alive struct {\n\tIncarnation uint32\n\tNode        string\n\tAddr        []byte\n\tPort        uint16\n\tMeta        []byte\n\n\t// The versions of the protocol/delegate that are being spoken, order:\n\t// pmin, pmax, pcur, dmin, dmax, dcur\n\tVsn []uint8\n}\n\n// dead is broadcast when we confirm a node is dead\n// Overloaded for nodes leaving\ntype dead struct {\n\tIncarnation uint32\n\tNode        string\n\tFrom        string // Include who is suspecting\n}\n\n// pushPullHeader is used to inform the\n// otherside how many states we are transferring\ntype pushPullHeader struct {\n\tNodes        int\n\tUserStateLen int  // Encodes the byte lengh of user state\n\tJoin         bool // Is this a join request or a anti-entropy run\n}\n\n// userMsgHeader is used to encapsulate a userMsg\ntype userMsgHeader struct {\n\tUserMsgLen int // Encodes the byte lengh of user state\n}\n\n// pushNodeState is used for pushPullReq when we are\n// transferring out node states\ntype pushNodeState struct {\n\tName        string\n\tAddr        []byte\n\tPort        uint16\n\tMeta        []byte\n\tIncarnation uint32\n\tState       NodeStateType\n\tVsn         []uint8 // Protocol versions\n}\n\n// compress is used to wrap an underlying payload\n// using a specified compression algorithm\ntype compress struct {\n\tAlgo compressionType\n\tBuf  []byte\n}\n\n// msgHandoff is used to transfer a message between goroutines\ntype msgHandoff struct {\n\tmsgType messageType\n\tbuf     []byte\n\tfrom    net.Addr\n}\n\n// encryptionVersion returns the encryption version to use\nfunc (m *Memberlist) encryptionVersion() encryptionVersion {\n\tswitch m.ProtocolVersion() {\n\tcase 1:\n\t\treturn 0\n\tdefault:\n\t\treturn 1\n\t}\n}\n\n// streamListen is a long running goroutine that pulls incoming streams from the\n// transport and hands them off for processing.\nfunc (m *Memberlist) streamListen() {\n\tfor {\n\t\tselect {\n\t\tcase conn := <-m.transport.StreamCh():\n\t\t\tgo m.handleConn(conn)\n\n\t\tcase <-m.shutdownCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// handleConn handles a single incoming stream connection from the transport.\nfunc (m *Memberlist) handleConn(conn net.Conn) {\n\tdefer conn.Close()\n\tm.logger.Printf(\"[DEBUG] memberlist: Stream connection %s\", LogConn(conn))\n\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"tcp\", \"accept\"}, 1, m.metricLabels)\n\n\tconn.SetDeadline(time.Now().Add(m.config.TCPTimeout))\n\n\tvar (\n\t\tstreamLabel string\n\t\terr         error\n\t)\n\tconn, streamLabel, err = RemoveLabelHeaderFromStream(conn)\n\tif err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: failed to receive and remove the stream label header: %s %s\", err, LogConn(conn))\n\t\treturn\n\t}\n\n\tif m.config.SkipInboundLabelCheck {\n\t\tif streamLabel != \"\" {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: unexpected double stream label header: %s\", LogConn(conn))\n\t\t\treturn\n\t\t}\n\t\t// Set this from config so that the auth data assertions work below.\n\t\tstreamLabel = m.config.Label\n\t}\n\n\tif m.config.Label != streamLabel {\n\t\tm.logger.Printf(\"[ERR] memberlist: discarding stream with unacceptable label %q: %s\", streamLabel, LogConn(conn))\n\t\treturn\n\t}\n\n\tmsgType, bufConn, dec, err := m.readStream(conn, streamLabel)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: failed to receive: %s %s\", err, LogConn(conn))\n\n\t\t\tresp := errResp{err.Error()}\n\t\t\tout, err := encode(errMsg, &resp, m.config.MsgpackUseNewTimeFormat)\n\t\t\tif err != nil {\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to encode error response: %s\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\terr = m.rawSendMsgStream(conn, out.Bytes(), streamLabel)\n\t\t\tif err != nil {\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send error: %s %s\", err, LogConn(conn))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\tswitch msgType {\n\tcase userMsg:\n\t\tif err := m.readUserMsg(bufConn, dec); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to receive user message: %s %s\", err, LogConn(conn))\n\t\t}\n\tcase pushPullMsg:\n\t\t// Increment counter of pending push/pulls\n\t\tnumConcurrent := atomic.AddUint32(&m.pushPullReq, 1)\n\t\tdefer atomic.AddUint32(&m.pushPullReq, ^uint32(0))\n\n\t\t// Check if we have too many open push/pull requests\n\t\tif numConcurrent >= maxPushPullRequests {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Too many pending push/pull requests\")\n\t\t\treturn\n\t\t}\n\n\t\tjoin, remoteNodes, userState, err := m.readRemoteState(bufConn, dec)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to read remote state: %s %s\", err, LogConn(conn))\n\t\t\treturn\n\t\t}\n\n\t\tif err := m.sendLocalState(conn, join, streamLabel); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to push local state: %s %s\", err, LogConn(conn))\n\t\t\treturn\n\t\t}\n\n\t\tif err := m.mergeRemoteState(join, remoteNodes, userState); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed push/pull merge: %s %s\", err, LogConn(conn))\n\t\t\treturn\n\t\t}\n\tcase pingMsg:\n\t\tvar p ping\n\t\tif err := dec.Decode(&p); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode ping: %s %s\", err, LogConn(conn))\n\t\t\treturn\n\t\t}\n\n\t\tif p.Node != \"\" && p.Node != m.config.Name {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Got ping for unexpected node %s %s\", p.Node, LogConn(conn))\n\t\t\treturn\n\t\t}\n\n\t\tack := ackResp{p.SeqNo, nil}\n\t\tout, err := encode(ackRespMsg, &ack, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to encode ack: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\terr = m.rawSendMsgStream(conn, out.Bytes(), streamLabel)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send ack: %s %s\", err, LogConn(conn))\n\t\t\treturn\n\t\t}\n\tdefault:\n\t\tm.logger.Printf(\"[ERR] memberlist: Received invalid msgType (%d) %s\", msgType, LogConn(conn))\n\t}\n}\n\n// packetListen is a long running goroutine that pulls packets out of the\n// transport and hands them off for processing.\nfunc (m *Memberlist) packetListen() {\n\tfor {\n\t\tselect {\n\t\tcase packet := <-m.transport.PacketCh():\n\t\t\tm.ingestPacket(packet.Buf, packet.From, packet.Timestamp)\n\n\t\tcase <-m.shutdownCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (m *Memberlist) ingestPacket(buf []byte, from net.Addr, timestamp time.Time) {\n\tvar (\n\t\tpacketLabel string\n\t\terr         error\n\t)\n\tbuf, packetLabel, err = RemoveLabelHeaderFromPacket(buf)\n\tif err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: %v %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\n\tif m.config.SkipInboundLabelCheck {\n\t\tif packetLabel != \"\" {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: unexpected double packet label header: %s\", LogAddress(from))\n\t\t\treturn\n\t\t}\n\t\t// Set this from config so that the auth data assertions work below.\n\t\tpacketLabel = m.config.Label\n\t}\n\n\tif m.config.Label != packetLabel {\n\t\tm.logger.Printf(\"[ERR] memberlist: discarding packet with unacceptable label %q: %s\", packetLabel, LogAddress(from))\n\t\treturn\n\t}\n\n\t// Check if encryption is enabled\n\tif m.config.EncryptionEnabled() {\n\t\t// Decrypt the payload\n\t\tauthData := []byte(packetLabel)\n\t\tplain, err := decryptPayload(m.config.Keyring.GetKeys(), buf, authData)\n\t\tif err != nil {\n\t\t\tif !m.config.GossipVerifyIncoming {\n\t\t\t\t// Treat the message as plaintext\n\t\t\t\tplain = buf\n\t\t\t} else {\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Decrypt packet failed: %v %s\", err, LogAddress(from))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Continue processing the plaintext buffer\n\t\tbuf = plain\n\t}\n\n\t// See if there's a checksum included to verify the contents of the message\n\tif len(buf) >= 5 && messageType(buf[0]) == hasCrcMsg {\n\t\tcrc := crc32.ChecksumIEEE(buf[5:])\n\t\texpected := binary.BigEndian.Uint32(buf[1:5])\n\t\tif crc != expected {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Got invalid checksum for UDP packet: %x, %x\", crc, expected)\n\t\t\treturn\n\t\t}\n\t\tm.handleCommand(buf[5:], from, timestamp)\n\t} else {\n\t\tm.handleCommand(buf, from, timestamp)\n\t}\n}\n\nfunc (m *Memberlist) handleCommand(buf []byte, from net.Addr, timestamp time.Time) {\n\tif len(buf) < 1 {\n\t\tm.logger.Printf(\"[ERR] memberlist: missing message type byte %s\", LogAddress(from))\n\t\treturn\n\t}\n\t// Decode the message type\n\tmsgType := messageType(buf[0])\n\tbuf = buf[1:]\n\n\t// Switch on the msgType\n\tswitch msgType {\n\tcase compoundMsg:\n\t\tm.handleCompound(buf, from, timestamp)\n\tcase compressMsg:\n\t\tm.handleCompressed(buf, from, timestamp)\n\n\tcase pingMsg:\n\t\tm.handlePing(buf, from)\n\tcase indirectPingMsg:\n\t\tm.handleIndirectPing(buf, from)\n\tcase ackRespMsg:\n\t\tm.handleAck(buf, from, timestamp)\n\tcase nackRespMsg:\n\t\tm.handleNack(buf, from)\n\n\tcase suspectMsg:\n\t\tfallthrough\n\tcase aliveMsg:\n\t\tfallthrough\n\tcase deadMsg:\n\t\tfallthrough\n\tcase userMsg:\n\t\t// Determine the message queue, prioritize alive\n\t\tqueue := m.lowPriorityMsgQueue\n\t\tif msgType == aliveMsg {\n\t\t\tqueue = m.highPriorityMsgQueue\n\t\t}\n\n\t\t// Check for overflow and append if not full\n\t\tm.msgQueueLock.Lock()\n\t\tif queue.Len() >= m.config.HandoffQueueDepth {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: handler queue full, dropping message (%d) %s\", msgType, LogAddress(from))\n\t\t} else {\n\t\t\tqueue.PushBack(msgHandoff{msgType, buf, from})\n\t\t}\n\t\tm.msgQueueLock.Unlock()\n\n\t\t// Notify of pending message\n\t\tselect {\n\t\tcase m.handoffCh <- struct{}{}:\n\t\tdefault:\n\t\t}\n\n\tdefault:\n\t\tm.logger.Printf(\"[ERR] memberlist: msg type (%d) not supported %s\", msgType, LogAddress(from))\n\t}\n}\n\n// getNextMessage returns the next message to process in priority order, using LIFO\nfunc (m *Memberlist) getNextMessage() (msgHandoff, bool) {\n\tm.msgQueueLock.Lock()\n\tdefer m.msgQueueLock.Unlock()\n\n\tif el := m.highPriorityMsgQueue.Back(); el != nil {\n\t\tm.highPriorityMsgQueue.Remove(el)\n\t\tmsg := el.Value.(msgHandoff)\n\t\treturn msg, true\n\t} else if el := m.lowPriorityMsgQueue.Back(); el != nil {\n\t\tm.lowPriorityMsgQueue.Remove(el)\n\t\tmsg := el.Value.(msgHandoff)\n\t\treturn msg, true\n\t}\n\treturn msgHandoff{}, false\n}\n\n// packetHandler is a long running goroutine that processes messages received\n// over the packet interface, but is decoupled from the listener to avoid\n// blocking the listener which may cause ping/ack messages to be delayed.\nfunc (m *Memberlist) packetHandler() {\n\tfor {\n\t\tselect {\n\t\tcase <-m.handoffCh:\n\t\t\tfor {\n\t\t\t\tmsg, ok := m.getNextMessage()\n\t\t\t\tif !ok {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tmsgType := msg.msgType\n\t\t\t\tbuf := msg.buf\n\t\t\t\tfrom := msg.from\n\n\t\t\t\tswitch msgType {\n\t\t\t\tcase suspectMsg:\n\t\t\t\t\tm.handleSuspect(buf, from)\n\t\t\t\tcase aliveMsg:\n\t\t\t\t\tm.handleAlive(buf, from)\n\t\t\t\tcase deadMsg:\n\t\t\t\t\tm.handleDead(buf, from)\n\t\t\t\tcase userMsg:\n\t\t\t\t\tm.handleUser(buf, from)\n\t\t\t\tdefault:\n\t\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Message type (%d) not supported %s (packet handler)\", msgType, LogAddress(from))\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase <-m.shutdownCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (m *Memberlist) handleCompound(buf []byte, from net.Addr, timestamp time.Time) {\n\t// Decode the parts\n\ttrunc, parts, err := decodeCompoundMessage(buf)\n\tif err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode compound request: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\n\t// Log any truncation\n\tif trunc > 0 {\n\t\tm.logger.Printf(\"[WARN] memberlist: Compound request had %d truncated messages %s\", trunc, LogAddress(from))\n\t}\n\n\t// Handle each message\n\tfor _, part := range parts {\n\t\tm.handleCommand(part, from, timestamp)\n\t}\n}\n\nfunc (m *Memberlist) handlePing(buf []byte, from net.Addr) {\n\tvar p ping\n\tif err := decode(buf, &p); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode ping request: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\t// If node is provided, verify that it is for us\n\tif p.Node != \"\" && p.Node != m.config.Name {\n\t\tm.logger.Printf(\"[WARN] memberlist: Got ping for unexpected node '%s' %s\", p.Node, LogAddress(from))\n\t\treturn\n\t}\n\tvar ack ackResp\n\tack.SeqNo = p.SeqNo\n\tif m.config.Ping != nil {\n\t\tack.Payload = m.config.Ping.AckPayload()\n\t}\n\n\taddr := \"\"\n\tif len(p.SourceAddr) > 0 && p.SourcePort > 0 {\n\t\taddr = joinHostPort(net.IP(p.SourceAddr).String(), p.SourcePort)\n\t} else {\n\t\taddr = from.String()\n\t}\n\n\ta := Address{\n\t\tAddr: addr,\n\t\tName: p.SourceNode,\n\t}\n\tif err := m.encodeAndSendMsg(a, ackRespMsg, &ack); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send ack: %s %s\", err, LogAddress(from))\n\t}\n}\n\nfunc (m *Memberlist) handleIndirectPing(buf []byte, from net.Addr) {\n\tvar ind indirectPingReq\n\tif err := decode(buf, &ind); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode indirect ping request: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\n\t// For proto versions < 2, there is no port provided. Mask old\n\t// behavior by using the configured port.\n\tif m.ProtocolVersion() < 2 || ind.Port == 0 {\n\t\tind.Port = uint16(m.config.BindPort)\n\t}\n\n\t// Send a ping to the correct host.\n\tlocalSeqNo := m.nextSeqNo()\n\tselfAddr, selfPort := m.getAdvertise()\n\tping := ping{\n\t\tSeqNo: localSeqNo,\n\t\tNode:  ind.Node,\n\t\t// The outbound message is addressed FROM us.\n\t\tSourceAddr: selfAddr,\n\t\tSourcePort: selfPort,\n\t\tSourceNode: m.config.Name,\n\t}\n\n\t// Forward the ack back to the requestor. If the request encodes an origin\n\t// use that otherwise assume that the other end of the UDP socket is\n\t// usable.\n\tindAddr := \"\"\n\tif len(ind.SourceAddr) > 0 && ind.SourcePort > 0 {\n\t\tindAddr = joinHostPort(net.IP(ind.SourceAddr).String(), ind.SourcePort)\n\t} else {\n\t\tindAddr = from.String()\n\t}\n\n\t// Setup a response handler to relay the ack\n\tcancelCh := make(chan struct{})\n\trespHandler := func(payload []byte, timestamp time.Time) {\n\t\t// Try to prevent the nack if we've caught it in time.\n\t\tclose(cancelCh)\n\n\t\tack := ackResp{ind.SeqNo, nil}\n\t\ta := Address{\n\t\t\tAddr: indAddr,\n\t\t\tName: ind.SourceNode,\n\t\t}\n\t\tif err := m.encodeAndSendMsg(a, ackRespMsg, &ack); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to forward ack: %s %s\", err, LogStringAddress(indAddr))\n\t\t}\n\t}\n\tm.setAckHandler(localSeqNo, respHandler, m.config.ProbeTimeout)\n\n\t// Send the ping.\n\taddr := joinHostPort(net.IP(ind.Target).String(), ind.Port)\n\ta := Address{\n\t\tAddr: addr,\n\t\tName: ind.Node,\n\t}\n\tif err := m.encodeAndSendMsg(a, pingMsg, &ping); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send indirect ping: %s %s\", err, LogStringAddress(indAddr))\n\t}\n\n\t// Setup a timer to fire off a nack if no ack is seen in time.\n\tif ind.Nack {\n\t\tgo func() {\n\t\t\tselect {\n\t\t\tcase <-cancelCh:\n\t\t\t\treturn\n\t\t\tcase <-time.After(m.config.ProbeTimeout):\n\t\t\t\tnack := nackResp{ind.SeqNo}\n\t\t\t\ta := Address{\n\t\t\t\t\tAddr: indAddr,\n\t\t\t\t\tName: ind.SourceNode,\n\t\t\t\t}\n\t\t\t\tif err := m.encodeAndSendMsg(a, nackRespMsg, &nack); err != nil {\n\t\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send nack: %s %s\", err, LogStringAddress(indAddr))\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc (m *Memberlist) handleAck(buf []byte, from net.Addr, timestamp time.Time) {\n\tvar ack ackResp\n\tif err := decode(buf, &ack); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode ack response: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tm.invokeAckHandler(ack, timestamp)\n}\n\nfunc (m *Memberlist) handleNack(buf []byte, from net.Addr) {\n\tvar nack nackResp\n\tif err := decode(buf, &nack); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode nack response: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tm.invokeNackHandler(nack)\n}\n\nfunc (m *Memberlist) handleSuspect(buf []byte, from net.Addr) {\n\tvar sus suspect\n\tif err := decode(buf, &sus); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode suspect message: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tm.suspectNode(&sus)\n}\n\n// ensureCanConnect return the IP from a RemoteAddress\n// return error if this client must not connect\nfunc (m *Memberlist) ensureCanConnect(from net.Addr) error {\n\tif !m.config.IPMustBeChecked() {\n\t\treturn nil\n\t}\n\tsource := from.String()\n\tif source == \"pipe\" {\n\t\treturn nil\n\t}\n\thost, _, err := net.SplitHostPort(source)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tip := net.ParseIP(host)\n\tif ip == nil {\n\t\treturn fmt.Errorf(\"Cannot parse IP from %s\", host)\n\t}\n\treturn m.config.IPAllowed(ip)\n}\n\nfunc (m *Memberlist) handleAlive(buf []byte, from net.Addr) {\n\tif err := m.ensureCanConnect(from); err != nil {\n\t\tm.logger.Printf(\"[DEBUG] memberlist: Blocked alive message: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tvar live alive\n\tif err := decode(buf, &live); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode alive message: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tif m.config.IPMustBeChecked() {\n\t\tinnerIP := net.IP(live.Addr)\n\t\tif innerIP != nil {\n\t\t\tif err := m.config.IPAllowed(innerIP); err != nil {\n\t\t\t\tm.logger.Printf(\"[DEBUG] memberlist: Blocked alive.Addr=%s message from: %s %s\", innerIP.String(), err, LogAddress(from))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// For proto versions < 2, there is no port provided. Mask old\n\t// behavior by using the configured port\n\tif m.ProtocolVersion() < 2 || live.Port == 0 {\n\t\tlive.Port = uint16(m.config.BindPort)\n\t}\n\n\tm.aliveNode(&live, nil, false)\n}\n\nfunc (m *Memberlist) handleDead(buf []byte, from net.Addr) {\n\tvar d dead\n\tif err := decode(buf, &d); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decode dead message: %s %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\tm.deadNode(&d)\n}\n\n// handleUser is used to notify channels of incoming user data\nfunc (m *Memberlist) handleUser(buf []byte, from net.Addr) {\n\td := m.config.Delegate\n\tif d != nil {\n\t\td.NotifyMsg(buf)\n\t}\n}\n\n// handleCompressed is used to unpack a compressed message\nfunc (m *Memberlist) handleCompressed(buf []byte, from net.Addr, timestamp time.Time) {\n\t// Try to decode the payload\n\tpayload, err := decompressPayload(buf)\n\tif err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Failed to decompress payload: %v %s\", err, LogAddress(from))\n\t\treturn\n\t}\n\n\t// Recursively handle the payload\n\tm.handleCommand(payload, from, timestamp)\n}\n\n// encodeAndSendMsg is used to combine the encoding and sending steps\nfunc (m *Memberlist) encodeAndSendMsg(a Address, msgType messageType, msg interface{}) error {\n\tout, err := encode(msgType, msg, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := m.sendMsg(a, out.Bytes()); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// sendMsg is used to send a message via packet to another host. It will\n// opportunistically create a compoundMsg and piggy back other broadcasts.\nfunc (m *Memberlist) sendMsg(a Address, msg []byte) error {\n\t// Check if we can piggy back any messages\n\tbytesAvail := m.config.UDPBufferSize - len(msg) - compoundHeaderOverhead - labelOverhead(m.config.Label)\n\tif m.config.EncryptionEnabled() && m.config.GossipVerifyOutgoing {\n\t\tbytesAvail -= encryptOverhead(m.encryptionVersion())\n\t}\n\textra := m.getBroadcasts(compoundOverhead, bytesAvail)\n\n\t// Fast path if nothing to piggypack\n\tif len(extra) == 0 {\n\t\treturn m.rawSendMsgPacket(a, nil, msg)\n\t}\n\n\t// Join all the messages\n\tmsgs := make([][]byte, 0, 1+len(extra))\n\tmsgs = append(msgs, msg)\n\tmsgs = append(msgs, extra...)\n\n\t// Create a compound message\n\tcompound := makeCompoundMessage(msgs)\n\n\t// Send the message\n\treturn m.rawSendMsgPacket(a, nil, compound.Bytes())\n}\n\n// rawSendMsgPacket is used to send message via packet to another host without\n// modification, other than compression or encryption if enabled.\nfunc (m *Memberlist) rawSendMsgPacket(a Address, node *Node, msg []byte) error {\n\tif a.Name == \"\" && m.config.RequireNodeNames {\n\t\treturn errNodeNamesAreRequired\n\t}\n\n\t// Check if we have compression enabled\n\tif m.config.EnableCompression {\n\t\tbuf, err := compressPayload(msg, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Failed to compress payload: %v\", err)\n\t\t} else {\n\t\t\t// Only use compression if it reduced the size\n\t\t\tif buf.Len() < len(msg) {\n\t\t\t\tmsg = buf.Bytes()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Try to look up the destination node. Note this will only work if the\n\t// bare ip address is used as the node name, which is not guaranteed.\n\tif node == nil {\n\t\ttoAddr, _, err := net.SplitHostPort(a.Addr)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to parse address %q: %v\", a.Addr, err)\n\t\t\treturn err\n\t\t}\n\t\tm.nodeLock.RLock()\n\t\tnodeState, ok := m.nodeMap[toAddr]\n\t\tm.nodeLock.RUnlock()\n\t\tif ok {\n\t\t\tnode = &nodeState.Node\n\t\t}\n\t}\n\n\t// Add a CRC to the end of the payload if the recipient understands\n\t// ProtocolVersion >= 5\n\tif node != nil && node.PMax >= 5 {\n\t\tcrc := crc32.ChecksumIEEE(msg)\n\t\theader := make([]byte, 5, 5+len(msg))\n\t\theader[0] = byte(hasCrcMsg)\n\t\tbinary.BigEndian.PutUint32(header[1:], crc)\n\t\tmsg = append(header, msg...)\n\t}\n\n\t// Check if we have encryption enabled\n\tif m.config.EncryptionEnabled() && m.config.GossipVerifyOutgoing {\n\t\t// Encrypt the payload\n\t\tvar (\n\t\t\tprimaryKey  = m.config.Keyring.GetPrimaryKey()\n\t\t\tpacketLabel = []byte(m.config.Label)\n\t\t\tbuf         bytes.Buffer\n\t\t)\n\t\terr := encryptPayload(m.encryptionVersion(), primaryKey, msg, packetLabel, &buf)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Encryption of message failed: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\tmsg = buf.Bytes()\n\t}\n\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"udp\", \"sent\"}, float32(len(msg)), m.metricLabels)\n\t_, err := m.transport.WriteToAddress(msg, a)\n\treturn err\n}\n\n// rawSendMsgStream is used to stream a message to another host without\n// modification, other than applying compression and encryption if enabled.\nfunc (m *Memberlist) rawSendMsgStream(conn net.Conn, sendBuf []byte, streamLabel string) error {\n\t// Check if compression is enabled\n\tif m.config.EnableCompression {\n\t\tcompBuf, err := compressPayload(sendBuf, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERROR] memberlist: Failed to compress payload: %v\", err)\n\t\t} else {\n\t\t\tsendBuf = compBuf.Bytes()\n\t\t}\n\t}\n\n\t// Check if encryption is enabled\n\tif m.config.EncryptionEnabled() && m.config.GossipVerifyOutgoing {\n\t\tcrypt, err := m.encryptLocalState(sendBuf, streamLabel)\n\t\tif err != nil {\n\t\t\tm.logger.Printf(\"[ERROR] memberlist: Failed to encrypt local state: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\tsendBuf = crypt\n\t}\n\n\t// Write out the entire send buffer\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"tcp\", \"sent\"}, float32(len(sendBuf)), m.metricLabels)\n\n\tif n, err := conn.Write(sendBuf); err != nil {\n\t\treturn err\n\t} else if n != len(sendBuf) {\n\t\treturn fmt.Errorf(\"only %d of %d bytes written\", n, len(sendBuf))\n\t}\n\n\treturn nil\n}\n\n// sendUserMsg is used to stream a user message to another host.\nfunc (m *Memberlist) sendUserMsg(a Address, sendBuf []byte) error {\n\tif a.Name == \"\" && m.config.RequireNodeNames {\n\t\treturn errNodeNamesAreRequired\n\t}\n\n\tconn, err := m.transport.DialAddressTimeout(a, m.config.TCPTimeout)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer conn.Close()\n\n\tbufConn := bytes.NewBuffer(nil)\n\tif err := bufConn.WriteByte(byte(userMsg)); err != nil {\n\t\treturn err\n\t}\n\n\theader := userMsgHeader{UserMsgLen: len(sendBuf)}\n\thd := codec.MsgpackHandle{\n\t\tBasicHandle: codec.BasicHandle{\n\t\t\tTimeNotBuiltin: !m.config.MsgpackUseNewTimeFormat,\n\t\t},\n\t}\n\tenc := codec.NewEncoder(bufConn, &hd)\n\tif err := enc.Encode(&header); err != nil {\n\t\treturn err\n\t}\n\tif _, err := bufConn.Write(sendBuf); err != nil {\n\t\treturn err\n\t}\n\n\treturn m.rawSendMsgStream(conn, bufConn.Bytes(), m.config.Label)\n}\n\n// sendAndReceiveState is used to initiate a push/pull over a stream with a\n// remote host.\nfunc (m *Memberlist) sendAndReceiveState(a Address, join bool) ([]pushNodeState, []byte, error) {\n\tif a.Name == \"\" && m.config.RequireNodeNames {\n\t\treturn nil, nil, errNodeNamesAreRequired\n\t}\n\n\t// Attempt to connect\n\tconn, err := m.transport.DialAddressTimeout(a, m.config.TCPTimeout)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdefer conn.Close()\n\tm.logger.Printf(\"[DEBUG] memberlist: Initiating push/pull sync with: %s %s\", a.Name, conn.RemoteAddr())\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"tcp\", \"connect\"}, 1, m.metricLabels)\n\n\t// Send our state\n\tif err := m.sendLocalState(conn, join, m.config.Label); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tconn.SetDeadline(time.Now().Add(m.config.TCPTimeout))\n\tmsgType, bufConn, dec, err := m.readStream(conn, m.config.Label)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif msgType == errMsg {\n\t\tvar resp errResp\n\t\tif err := dec.Decode(&resp); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn nil, nil, fmt.Errorf(\"remote error: %v\", resp.Error)\n\t}\n\n\t// Quit if not push/pull\n\tif msgType != pushPullMsg {\n\t\terr := fmt.Errorf(\"received invalid msgType (%d), expected pushPullMsg (%d) %s\", msgType, pushPullMsg, LogConn(conn))\n\t\treturn nil, nil, err\n\t}\n\n\t// Read remote state\n\t_, remoteNodes, userState, err := m.readRemoteState(bufConn, dec)\n\treturn remoteNodes, userState, err\n}\n\n// sendLocalState is invoked to send our local state over a stream connection.\nfunc (m *Memberlist) sendLocalState(conn net.Conn, join bool, streamLabel string) error {\n\t// Setup a deadline\n\tconn.SetDeadline(time.Now().Add(m.config.TCPTimeout))\n\n\t// Prepare the local node state\n\tm.nodeLock.RLock()\n\tlocalNodes := make([]pushNodeState, len(m.nodes))\n\tfor idx, n := range m.nodes {\n\t\tlocalNodes[idx].Name = n.Name\n\t\tlocalNodes[idx].Addr = n.Addr\n\t\tlocalNodes[idx].Port = n.Port\n\t\tlocalNodes[idx].Incarnation = n.Incarnation\n\t\tlocalNodes[idx].State = n.State\n\t\tlocalNodes[idx].Meta = n.Meta\n\t\tlocalNodes[idx].Vsn = []uint8{\n\t\t\tn.PMin, n.PMax, n.PCur,\n\t\t\tn.DMin, n.DMax, n.DCur,\n\t\t}\n\t}\n\tm.nodeLock.RUnlock()\n\n\tnodeStateCounts := make(map[string]int)\n\tnodeStateCounts[StateAlive.metricsString()] = 0\n\tnodeStateCounts[StateLeft.metricsString()] = 0\n\tnodeStateCounts[StateDead.metricsString()] = 0\n\tnodeStateCounts[StateSuspect.metricsString()] = 0\n\n\tfor _, n := range localNodes {\n\t\tnodeStateCounts[n.State.metricsString()]++\n\t}\n\n\tfor nodeState, cnt := range nodeStateCounts {\n\t\tmetrics.SetGaugeWithLabels([]string{\"memberlist\", \"node\", \"instances\"},\n\t\t\tfloat32(cnt),\n\t\t\tappend(m.metricLabels, metrics.Label{Name: \"node_state\", Value: nodeState}))\n\t}\n\n\t// Get the delegate state\n\tvar userData []byte\n\tif m.config.Delegate != nil {\n\t\tuserData = m.config.Delegate.LocalState(join)\n\t}\n\n\t// Create a bytes buffer writer\n\tbufConn := bytes.NewBuffer(nil)\n\n\t// Send our node state\n\theader := pushPullHeader{Nodes: len(localNodes), UserStateLen: len(userData), Join: join}\n\thd := codec.MsgpackHandle{}\n\tenc := codec.NewEncoder(bufConn, &hd)\n\n\t// Begin state push\n\tif _, err := bufConn.Write([]byte{byte(pushPullMsg)}); err != nil {\n\t\treturn err\n\t}\n\n\tif err := enc.Encode(&header); err != nil {\n\t\treturn err\n\t}\n\tfor i := 0; i < header.Nodes; i++ {\n\t\tif err := enc.Encode(&localNodes[i]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Write the user state as well\n\tif userData != nil {\n\t\tif _, err := bufConn.Write(userData); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmoreBytes := binary.BigEndian.Uint32(bufConn.Bytes()[1:5])\n\tmetrics.SetGaugeWithLabels([]string{\"memberlist\", \"size\", \"local\"}, float32(moreBytes), m.metricLabels)\n\n\t// Get the send buffer\n\treturn m.rawSendMsgStream(conn, bufConn.Bytes(), streamLabel)\n}\n\n// encryptLocalState is used to help encrypt local state before sending\nfunc (m *Memberlist) encryptLocalState(sendBuf []byte, streamLabel string) ([]byte, error) {\n\tvar buf bytes.Buffer\n\n\t// Write the encryptMsg byte\n\tbuf.WriteByte(byte(encryptMsg))\n\n\t// Write the size of the message\n\tsizeBuf := make([]byte, 4)\n\tencVsn := m.encryptionVersion()\n\tencLen := encryptedLength(encVsn, len(sendBuf))\n\tbinary.BigEndian.PutUint32(sizeBuf, uint32(encLen))\n\tbuf.Write(sizeBuf)\n\n\t// Authenticated Data is:\n\t//\n\t//   [messageType; byte] [messageLength; uint32] [stream_label; optional]\n\t//\n\tdataBytes := appendBytes(buf.Bytes()[:5], []byte(streamLabel))\n\n\t// Write the encrypted cipher text to the buffer\n\tkey := m.config.Keyring.GetPrimaryKey()\n\terr := encryptPayload(encVsn, key, sendBuf, dataBytes, &buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn buf.Bytes(), nil\n}\n\n// decryptRemoteState is used to help decrypt the remote state\nfunc (m *Memberlist) decryptRemoteState(bufConn io.Reader, streamLabel string) ([]byte, error) {\n\t// Read in enough to determine message length\n\tcipherText := bytes.NewBuffer(nil)\n\tcipherText.WriteByte(byte(encryptMsg))\n\t_, err := io.CopyN(cipherText, bufConn, 4)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Ensure we aren't asked to download too much. This is to guard against\n\t// an attack vector where a huge amount of state is sent\n\tmoreBytes := binary.BigEndian.Uint32(cipherText.Bytes()[1:5])\n\tmetrics.AddSampleWithLabels([]string{\"memberlist\", \"size\", \"remote\"}, float32(moreBytes), m.metricLabels)\n\n\tif moreBytes > maxPushStateBytes {\n\t\treturn nil, fmt.Errorf(\"Remote node state is larger than limit (%d)\", moreBytes)\n\n\t}\n\n\t//Start reporting the size before you cross the limit\n\tif moreBytes > uint32(math.Floor(.6*maxPushStateBytes)) {\n\t\tm.logger.Printf(\"[WARN] memberlist: Remote node state size is (%d) limit is (%d)\", moreBytes, maxPushStateBytes)\n\t}\n\n\t// Read in the rest of the payload\n\t_, err = io.CopyN(cipherText, bufConn, int64(moreBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Decrypt the cipherText with some authenticated data\n\t//\n\t// Authenticated Data is:\n\t//\n\t//   [messageType; byte] [messageLength; uint32] [label_data; optional]\n\t//\n\tdataBytes := appendBytes(cipherText.Bytes()[:5], []byte(streamLabel))\n\tcipherBytes := cipherText.Bytes()[5:]\n\n\t// Decrypt the payload\n\tkeys := m.config.Keyring.GetKeys()\n\treturn decryptPayload(keys, cipherBytes, dataBytes)\n}\n\n// readStream is used to read messages from a stream connection, decrypting and\n// decompressing the stream if necessary.\n//\n// The provided streamLabel if present will be authenticated during decryption\n// of each message.\nfunc (m *Memberlist) readStream(conn net.Conn, streamLabel string) (messageType, io.Reader, *codec.Decoder, error) {\n\t// Created a buffered reader\n\tvar bufConn io.Reader = bufio.NewReader(conn)\n\n\t// Read the message type\n\tbuf := [1]byte{0}\n\tif _, err := io.ReadFull(bufConn, buf[:]); err != nil {\n\t\treturn 0, nil, nil, err\n\t}\n\tmsgType := messageType(buf[0])\n\n\t// Check if the message is encrypted\n\tif msgType == encryptMsg {\n\t\tif !m.config.EncryptionEnabled() {\n\t\t\treturn 0, nil, nil,\n\t\t\t\tfmt.Errorf(\"Remote state is encrypted and encryption is not configured\")\n\t\t}\n\n\t\tplain, err := m.decryptRemoteState(bufConn, streamLabel)\n\t\tif err != nil {\n\t\t\treturn 0, nil, nil, err\n\t\t}\n\n\t\t// Reset message type and bufConn\n\t\tmsgType = messageType(plain[0])\n\t\tbufConn = bytes.NewReader(plain[1:])\n\t} else if m.config.EncryptionEnabled() && m.config.GossipVerifyIncoming {\n\t\treturn 0, nil, nil,\n\t\t\tfmt.Errorf(\"Encryption is configured but remote state is not encrypted\")\n\t}\n\n\t// Get the msgPack decoders\n\thd := codec.MsgpackHandle{}\n\tdec := codec.NewDecoder(bufConn, &hd)\n\n\t// Check if we have a compressed message\n\tif msgType == compressMsg {\n\t\tvar c compress\n\t\tif err := dec.Decode(&c); err != nil {\n\t\t\treturn 0, nil, nil, err\n\t\t}\n\t\tdecomp, err := decompressBuffer(&c)\n\t\tif err != nil {\n\t\t\treturn 0, nil, nil, err\n\t\t}\n\n\t\t// Reset the message type\n\t\tmsgType = messageType(decomp[0])\n\n\t\t// Create a new bufConn\n\t\tbufConn = bytes.NewReader(decomp[1:])\n\n\t\t// Create a new decoder\n\t\tdec = codec.NewDecoder(bufConn, &hd)\n\t}\n\n\treturn msgType, bufConn, dec, nil\n}\n\n// readRemoteState is used to read the remote state from a connection\nfunc (m *Memberlist) readRemoteState(bufConn io.Reader, dec *codec.Decoder) (bool, []pushNodeState, []byte, error) {\n\t// Read the push/pull header\n\tvar header pushPullHeader\n\tif err := dec.Decode(&header); err != nil {\n\t\treturn false, nil, nil, err\n\t}\n\n\t// Allocate space for the transfer\n\tremoteNodes := make([]pushNodeState, header.Nodes)\n\n\t// Try to decode all the states\n\tfor i := 0; i < header.Nodes; i++ {\n\t\tif err := dec.Decode(&remoteNodes[i]); err != nil {\n\t\t\treturn false, nil, nil, err\n\t\t}\n\t}\n\n\t// Read the remote user state into a buffer\n\tvar userBuf []byte\n\tif header.UserStateLen > 0 {\n\t\tuserBuf = make([]byte, header.UserStateLen)\n\t\tbytes, err := io.ReadAtLeast(bufConn, userBuf, header.UserStateLen)\n\t\tif err == nil && bytes != header.UserStateLen {\n\t\t\terr = fmt.Errorf(\n\t\t\t\t\"Failed to read full user state (%d / %d)\",\n\t\t\t\tbytes, header.UserStateLen)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn false, nil, nil, err\n\t\t}\n\t}\n\n\t// For proto versions < 2, there is no port provided. Mask old\n\t// behavior by using the configured port\n\tfor idx := range remoteNodes {\n\t\tif m.ProtocolVersion() < 2 || remoteNodes[idx].Port == 0 {\n\t\t\tremoteNodes[idx].Port = uint16(m.config.BindPort)\n\t\t}\n\t}\n\n\treturn header.Join, remoteNodes, userBuf, nil\n}\n\n// mergeRemoteState is used to merge the remote state with our local state\nfunc (m *Memberlist) mergeRemoteState(join bool, remoteNodes []pushNodeState, userBuf []byte) error {\n\tif err := m.verifyProtocol(remoteNodes); err != nil {\n\t\treturn err\n\t}\n\n\t// Invoke the merge delegate if any\n\tif join && m.config.Merge != nil {\n\t\tnodes := make([]*Node, len(remoteNodes))\n\t\tfor idx, n := range remoteNodes {\n\t\t\tnodes[idx] = &Node{\n\t\t\t\tName:  n.Name,\n\t\t\t\tAddr:  n.Addr,\n\t\t\t\tPort:  n.Port,\n\t\t\t\tMeta:  n.Meta,\n\t\t\t\tState: n.State,\n\t\t\t\tPMin:  n.Vsn[0],\n\t\t\t\tPMax:  n.Vsn[1],\n\t\t\t\tPCur:  n.Vsn[2],\n\t\t\t\tDMin:  n.Vsn[3],\n\t\t\t\tDMax:  n.Vsn[4],\n\t\t\t\tDCur:  n.Vsn[5],\n\t\t\t}\n\t\t}\n\t\tif err := m.config.Merge.NotifyMerge(nodes); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Merge the membership state\n\tm.mergeState(remoteNodes)\n\n\t// Invoke the delegate for user state\n\tif userBuf != nil && m.config.Delegate != nil {\n\t\tm.config.Delegate.MergeRemoteState(userBuf, join)\n\t}\n\treturn nil\n}\n\n// readUserMsg is used to decode a userMsg from a stream.\nfunc (m *Memberlist) readUserMsg(bufConn io.Reader, dec *codec.Decoder) error {\n\t// Read the user message header\n\tvar header userMsgHeader\n\tif err := dec.Decode(&header); err != nil {\n\t\treturn err\n\t}\n\n\t// Read the user message into a buffer\n\tvar userBuf []byte\n\tif header.UserMsgLen > 0 {\n\t\tuserBuf = make([]byte, header.UserMsgLen)\n\t\tbytes, err := io.ReadAtLeast(bufConn, userBuf, header.UserMsgLen)\n\t\tif err == nil && bytes != header.UserMsgLen {\n\t\t\terr = fmt.Errorf(\n\t\t\t\t\"Failed to read full user message (%d / %d)\",\n\t\t\t\tbytes, header.UserMsgLen)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\td := m.config.Delegate\n\t\tif d != nil {\n\t\t\td.NotifyMsg(userBuf)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// sendPingAndWaitForAck makes a stream connection to the given address, sends\n// a ping, and waits for an ack. All of this is done as a series of blocking\n// operations, given the deadline. The bool return parameter is true if we\n// we able to round trip a ping to the other node.\nfunc (m *Memberlist) sendPingAndWaitForAck(a Address, ping ping, deadline time.Time) (bool, error) {\n\tif a.Name == \"\" && m.config.RequireNodeNames {\n\t\treturn false, errNodeNamesAreRequired\n\t}\n\n\tconn, err := m.transport.DialAddressTimeout(a, deadline.Sub(time.Now()))\n\tif err != nil {\n\t\t// If the node is actually dead we expect this to fail, so we\n\t\t// shouldn't spam the logs with it. After this point, errors\n\t\t// with the connection are real, unexpected errors and should\n\t\t// get propagated up.\n\t\treturn false, nil\n\t}\n\tdefer conn.Close()\n\tconn.SetDeadline(deadline)\n\n\tout, err := encode(pingMsg, &ping, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif err = m.rawSendMsgStream(conn, out.Bytes(), m.config.Label); err != nil {\n\t\treturn false, err\n\t}\n\n\tmsgType, _, dec, err := m.readStream(conn, m.config.Label)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif msgType != ackRespMsg {\n\t\treturn false, fmt.Errorf(\"Unexpected msgType (%d) from ping %s\", msgType, LogConn(conn))\n\t}\n\n\tvar ack ackResp\n\tif err = dec.Decode(&ack); err != nil {\n\t\treturn false, err\n\t}\n\n\tif ack.SeqNo != ping.SeqNo {\n\t\treturn false, fmt.Errorf(\"Sequence number from ack (%d) doesn't match ping (%d)\", ack.SeqNo, ping.SeqNo)\n\t}\n\n\treturn true, nil\n}\n"
        },
        {
          "name": "net_test.go",
          "type": "blob",
          "size": 20.52734375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"net\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-msgpack/v2/codec\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// As a regression we left this test very low-level and network-ey, even after\n// we abstracted the transport. We added some basic network-free transport tests\n// in transport_test.go to prove that we didn't hard code some network stuff\n// outside of NetTransport.\n\nfunc TestHandleCompoundPing(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\tudpAddr := udp.LocalAddr().(*net.UDPAddr)\n\n\t// Encode a ping\n\tping := ping{\n\t\tSeqNo:      42,\n\t\tSourceAddr: udpAddr.IP,\n\t\tSourcePort: uint16(udpAddr.Port),\n\t\tSourceNode: \"test\",\n\t}\n\tbuf, err := encode(pingMsg, ping, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Make a compound message\n\tcompound := makeCompoundMessage([][]byte{buf.Bytes(), buf.Bytes(), buf.Bytes()})\n\n\t// Send compound version\n\taddr := &net.UDPAddr{IP: net.ParseIP(m.config.BindAddr), Port: m.config.BindPort}\n\t_, err = udp.WriteTo(compound.Bytes(), addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Wait for responses\n\tdoneCh := make(chan struct{}, 1)\n\tgo func() {\n\t\tselect {\n\t\tcase <-doneCh:\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tpanic(\"timeout\")\n\t\t}\n\t}()\n\n\tfor i := 0; i < 3; i++ {\n\t\tin := make([]byte, 1500)\n\t\tn, _, err := udp.ReadFrom(in)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\t\tin = in[0:n]\n\n\t\tmsgType := messageType(in[0])\n\t\tif msgType != ackRespMsg {\n\t\t\tt.Fatalf(\"bad response %v\", in)\n\t\t}\n\n\t\tvar ack ackResp\n\t\tif err := decode(in[1:], &ack); err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\n\t\tif ack.SeqNo != 42 {\n\t\t\tt.Fatalf(\"bad sequence no\")\n\t\t}\n\t}\n\n\tdoneCh <- struct{}{}\n}\n\nfunc TestHandlePing(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\tudpAddr := udp.LocalAddr().(*net.UDPAddr)\n\n\t// Encode a ping\n\tping := ping{\n\t\tSeqNo:      42,\n\t\tSourceAddr: udpAddr.IP,\n\t\tSourcePort: uint16(udpAddr.Port),\n\t\tSourceNode: \"test\",\n\t}\n\tbuf, err := encode(pingMsg, ping, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Send\n\taddr := &net.UDPAddr{IP: net.ParseIP(m.config.BindAddr), Port: m.config.BindPort}\n\t_, err = udp.WriteTo(buf.Bytes(), addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Wait for response\n\tdoneCh := make(chan struct{}, 1)\n\tgo func() {\n\t\tselect {\n\t\tcase <-doneCh:\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tpanic(\"timeout\")\n\t\t}\n\t}()\n\n\tin := make([]byte, 1500)\n\tn, _, err := udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tmsgType := messageType(in[0])\n\tif msgType != ackRespMsg {\n\t\tt.Fatalf(\"bad response %v\", in)\n\t}\n\n\tvar ack ackResp\n\tif err := decode(in[1:], &ack); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tif ack.SeqNo != 42 {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n\n\tdoneCh <- struct{}{}\n}\n\nfunc TestHandlePing_WrongNode(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\tudpAddr := udp.LocalAddr().(*net.UDPAddr)\n\n\t// Encode a ping, wrong node!\n\tping := ping{\n\t\tSeqNo:      42,\n\t\tNode:       m.config.Name + \"-bad\",\n\t\tSourceAddr: udpAddr.IP,\n\t\tSourcePort: uint16(udpAddr.Port),\n\t\tSourceNode: \"test\",\n\t}\n\tbuf, err := encode(pingMsg, ping, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Send\n\taddr := &net.UDPAddr{IP: net.ParseIP(m.config.BindAddr), Port: m.config.BindPort}\n\t_, err = udp.WriteTo(buf.Bytes(), addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Wait for response\n\tudp.SetDeadline(time.Now().Add(50 * time.Millisecond))\n\tin := make([]byte, 1500)\n\t_, _, err = udp.ReadFrom(in)\n\n\t// Should get an i/o timeout\n\tif err == nil {\n\t\tt.Fatalf(\"expected err %s\", err)\n\t}\n}\n\nfunc TestHandleIndirectPing(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\tudpAddr := udp.LocalAddr().(*net.UDPAddr)\n\n\t// Encode an indirect ping\n\tind := indirectPingReq{\n\t\tSeqNo:      100,\n\t\tTarget:     net.ParseIP(m.config.BindAddr),\n\t\tPort:       uint16(m.config.BindPort),\n\t\tNode:       m.config.Name,\n\t\tSourceAddr: udpAddr.IP,\n\t\tSourcePort: uint16(udpAddr.Port),\n\t\tSourceNode: \"test\",\n\t}\n\tbuf, err := encode(indirectPingMsg, &ind, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Send\n\taddr := &net.UDPAddr{IP: net.ParseIP(m.config.BindAddr), Port: m.config.BindPort}\n\t_, err = udp.WriteTo(buf.Bytes(), addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Wait for response\n\tdoneCh := make(chan struct{}, 1)\n\tgo func() {\n\t\tselect {\n\t\tcase <-doneCh:\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tpanic(\"timeout\")\n\t\t}\n\t}()\n\n\tin := make([]byte, 1500)\n\tn, _, err := udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tmsgType := messageType(in[0])\n\tif msgType != ackRespMsg {\n\t\tt.Fatalf(\"bad response %v\", in)\n\t}\n\n\tvar ack ackResp\n\tif err := decode(in[1:], &ack); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tif ack.SeqNo != 100 {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n\n\tdoneCh <- struct{}{}\n}\n\nfunc TestTCPPing(t *testing.T) {\n\tvar tcp *net.TCPListener\n\tvar tcpAddr *net.TCPAddr\n\tfor port := 60000; port < 61000; port++ {\n\t\ttcpAddr = &net.TCPAddr{IP: net.ParseIP(\"127.0.0.1\"), Port: port}\n\t\ttcpLn, err := net.ListenTCP(\"tcp\", tcpAddr)\n\t\tif err == nil {\n\t\t\ttcp = tcpLn\n\t\t\tbreak\n\t\t}\n\t}\n\tif tcp == nil {\n\t\tt.Fatalf(\"no tcp listener\")\n\t}\n\n\ttcpAddr2 := Address{Addr: tcpAddr.String(), Name: \"test\"}\n\n\t// Note that tcp gets closed in the last test, so we avoid a deferred\n\t// Close() call here.\n\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\tpingTimeout := m.config.ProbeInterval\n\tpingTimeMax := m.config.ProbeInterval + 10*time.Millisecond\n\n\t// Do a normal round trip.\n\tpingOut := ping{SeqNo: 23, Node: \"mongo\"}\n\tpingErrCh := make(chan error, 1)\n\tgo func() {\n\t\ttcp.SetDeadline(time.Now().Add(pingTimeMax))\n\t\tconn, err := tcp.AcceptTCP()\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to connect: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\n\t\tmsgType, _, dec, err := m.readStream(conn, \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to read ping: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif msgType != pingMsg {\n\t\t\tpingErrCh <- fmt.Errorf(\"expecting ping, got message type (%d)\", msgType)\n\t\t\treturn\n\t\t}\n\n\t\tvar pingIn ping\n\t\tif err := dec.Decode(&pingIn); err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to decode ping: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif pingIn.SeqNo != pingOut.SeqNo {\n\t\t\tpingErrCh <- fmt.Errorf(\"sequence number isn't correct (%d) vs (%d)\", pingIn.SeqNo, pingOut.SeqNo)\n\t\t\treturn\n\t\t}\n\n\t\tif pingIn.Node != pingOut.Node {\n\t\t\tpingErrCh <- fmt.Errorf(\"node name isn't correct (%s) vs (%s)\", pingIn.Node, pingOut.Node)\n\t\t\treturn\n\t\t}\n\n\t\tack := ackResp{pingIn.SeqNo, nil}\n\t\tout, err := encode(ackRespMsg, &ack, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to encode ack: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\terr = m.rawSendMsgStream(conn, out.Bytes(), \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to send ack: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tpingErrCh <- nil\n\t}()\n\tdeadline := time.Now().Add(pingTimeout)\n\tdidContact, err := m.sendPingAndWaitForAck(tcpAddr2, pingOut, deadline)\n\tif err != nil {\n\t\tt.Fatalf(\"error trying to ping: %s\", err)\n\t}\n\tif !didContact {\n\t\tt.Fatalf(\"expected successful ping\")\n\t}\n\tif err = <-pingErrCh; err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Make sure a mis-matched sequence number is caught.\n\tgo func() {\n\t\ttcp.SetDeadline(time.Now().Add(pingTimeMax))\n\t\tconn, err := tcp.AcceptTCP()\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to connect: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\n\t\t_, _, dec, err := m.readStream(conn, \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to read ping: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tvar pingIn ping\n\t\tif err := dec.Decode(&pingIn); err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to decode ping: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tack := ackResp{pingIn.SeqNo + 1, nil}\n\t\tout, err := encode(ackRespMsg, &ack, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to encode ack: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\terr = m.rawSendMsgStream(conn, out.Bytes(), \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to send ack: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tpingErrCh <- nil\n\t}()\n\tdeadline = time.Now().Add(pingTimeout)\n\tdidContact, err = m.sendPingAndWaitForAck(tcpAddr2, pingOut, deadline)\n\tif err == nil || !strings.Contains(err.Error(), \"Sequence number\") {\n\t\tt.Fatalf(\"expected an error from mis-matched sequence number\")\n\t}\n\tif didContact {\n\t\tt.Fatalf(\"expected failed ping\")\n\t}\n\tif err = <-pingErrCh; err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Make sure an unexpected message type is handled gracefully.\n\tgo func() {\n\t\ttcp.SetDeadline(time.Now().Add(pingTimeMax))\n\t\tconn, err := tcp.AcceptTCP()\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to connect: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\n\t\t_, _, _, err = m.readStream(conn, \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to read ping: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tbogus := indirectPingReq{}\n\t\tout, err := encode(indirectPingMsg, &bogus, m.config.MsgpackUseNewTimeFormat)\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to encode bogus msg: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\terr = m.rawSendMsgStream(conn, out.Bytes(), \"\")\n\t\tif err != nil {\n\t\t\tpingErrCh <- fmt.Errorf(\"failed to send bogus msg: %s\", err)\n\t\t\treturn\n\t\t}\n\t\tpingErrCh <- nil\n\t}()\n\tdeadline = time.Now().Add(pingTimeout)\n\tdidContact, err = m.sendPingAndWaitForAck(tcpAddr2, pingOut, deadline)\n\tif err == nil || !strings.Contains(err.Error(), \"Unexpected msgType\") {\n\t\tt.Fatalf(\"expected an error from bogus message\")\n\t}\n\tif didContact {\n\t\tt.Fatalf(\"expected failed ping\")\n\t}\n\tif err = <-pingErrCh; err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Make sure failed I/O respects the deadline. In this case we try the\n\t// common case of the receiving node being totally down.\n\ttcp.Close()\n\tdeadline = time.Now().Add(pingTimeout)\n\tstartPing := time.Now()\n\tdidContact, err = m.sendPingAndWaitForAck(tcpAddr2, pingOut, deadline)\n\tpingTime := time.Now().Sub(startPing)\n\tif err != nil {\n\t\tt.Fatalf(\"expected no error during ping on closed socket, got: %s\", err)\n\t}\n\tif didContact {\n\t\tt.Fatalf(\"expected failed ping\")\n\t}\n\tif pingTime > pingTimeMax {\n\t\tt.Fatalf(\"took too long to fail ping, %9.6f\", pingTime.Seconds())\n\t}\n}\n\nfunc TestTCPPushPull(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\tm.nodes = append(m.nodes, &nodeState{\n\t\tNode: Node{\n\t\t\tName: \"Test 0\",\n\t\t\tAddr: net.ParseIP(m.config.BindAddr),\n\t\t\tPort: uint16(m.config.BindPort),\n\t\t},\n\t\tIncarnation: 0,\n\t\tState:       StateSuspect,\n\t\tStateChange: time.Now().Add(-1 * time.Second),\n\t})\n\n\taddr := net.JoinHostPort(m.config.BindAddr, strconv.Itoa(m.config.BindPort))\n\tconn, err := net.Dial(\"tcp\", addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tdefer conn.Close()\n\n\tlocalNodes := make([]pushNodeState, 3)\n\tlocalNodes[0].Name = \"Test 0\"\n\tlocalNodes[0].Addr = net.ParseIP(m.config.BindAddr)\n\tlocalNodes[0].Port = uint16(m.config.BindPort)\n\tlocalNodes[0].Incarnation = 1\n\tlocalNodes[0].State = StateAlive\n\tlocalNodes[1].Name = \"Test 1\"\n\tlocalNodes[1].Addr = net.ParseIP(m.config.BindAddr)\n\tlocalNodes[1].Port = uint16(m.config.BindPort)\n\tlocalNodes[1].Incarnation = 1\n\tlocalNodes[1].State = StateAlive\n\tlocalNodes[2].Name = \"Test 2\"\n\tlocalNodes[2].Addr = net.ParseIP(m.config.BindAddr)\n\tlocalNodes[2].Port = uint16(m.config.BindPort)\n\tlocalNodes[2].Incarnation = 1\n\tlocalNodes[2].State = StateAlive\n\n\t// Send our node state\n\theader := pushPullHeader{Nodes: 3}\n\thd := codec.MsgpackHandle{\n\t\tBasicHandle: codec.BasicHandle{\n\t\t\tTimeNotBuiltin: !m.config.MsgpackUseNewTimeFormat,\n\t\t},\n\t}\n\tenc := codec.NewEncoder(conn, &hd)\n\n\t// Send the push/pull indicator\n\tconn.Write([]byte{byte(pushPullMsg)})\n\n\tif err := enc.Encode(&header); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tfor i := 0; i < header.Nodes; i++ {\n\t\tif err := enc.Encode(&localNodes[i]); err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\t}\n\n\t// Read the message type\n\tvar msgType messageType\n\tif err := binary.Read(conn, binary.BigEndian, &msgType); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tvar bufConn io.Reader = conn\n\tmsghd := codec.MsgpackHandle{\n\t\tBasicHandle: codec.BasicHandle{\n\t\t\tTimeNotBuiltin: !m.config.MsgpackUseNewTimeFormat,\n\t\t},\n\t}\n\tdec := codec.NewDecoder(bufConn, &msghd)\n\n\t// Check if we have a compressed message\n\tif msgType == compressMsg {\n\t\tvar c compress\n\t\tif err := dec.Decode(&c); err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\t\tdecomp, err := decompressBuffer(&c)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\n\t\t// Reset the message type\n\t\tmsgType = messageType(decomp[0])\n\n\t\t// Create a new bufConn\n\t\tbufConn = bytes.NewReader(decomp[1:])\n\n\t\t// Create a new decoder\n\t\tdec = codec.NewDecoder(bufConn, &hd)\n\t}\n\n\t// Quit if not push/pull\n\tif msgType != pushPullMsg {\n\t\tt.Fatalf(\"bad message type\")\n\t}\n\n\tif err := dec.Decode(&header); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Allocate space for the transfer\n\tremoteNodes := make([]pushNodeState, header.Nodes)\n\n\t// Try to decode all the states\n\tfor i := 0; i < header.Nodes; i++ {\n\t\tif err := dec.Decode(&remoteNodes[i]); err != nil {\n\t\t\tt.Fatalf(\"unexpected err %s\", err)\n\t\t}\n\t}\n\n\tif len(remoteNodes) != 1 {\n\t\tt.Fatalf(\"bad response\")\n\t}\n\n\tn := &remoteNodes[0]\n\tif n.Name != \"Test 0\" {\n\t\tt.Fatalf(\"bad name\")\n\t}\n\tif bytes.Compare(n.Addr, net.ParseIP(m.config.BindAddr)) != 0 {\n\t\tt.Fatal(\"bad addr\")\n\t}\n\tif n.Incarnation != 0 {\n\t\tt.Fatal(\"bad incarnation\")\n\t}\n\tif n.State != StateSuspect {\n\t\tt.Fatal(\"bad state\")\n\t}\n}\n\nfunc TestSendMsg_Piggyback(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\t// Add a message to be broadcast\n\ta := alive{\n\t\tIncarnation: 10,\n\t\tNode:        \"rand\",\n\t\tAddr:        []byte{127, 0, 0, 255},\n\t\tMeta:        nil,\n\t\tVsn: []uint8{\n\t\t\tProtocolVersionMin, ProtocolVersionMax, ProtocolVersionMin,\n\t\t\t1, 1, 1,\n\t\t},\n\t}\n\tm.encodeAndBroadcast(\"rand\", aliveMsg, &a)\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\tudpAddr := udp.LocalAddr().(*net.UDPAddr)\n\n\t// Encode a ping\n\tping := ping{\n\t\tSeqNo:      42,\n\t\tSourceAddr: udpAddr.IP,\n\t\tSourcePort: uint16(udpAddr.Port),\n\t\tSourceNode: \"test\",\n\t}\n\tbuf, err := encode(pingMsg, ping, m.config.MsgpackUseNewTimeFormat)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Send\n\taddr := &net.UDPAddr{IP: net.ParseIP(m.config.BindAddr), Port: m.config.BindPort}\n\t_, err = udp.WriteTo(buf.Bytes(), addr)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\t// Wait for response\n\tdoneCh := make(chan struct{}, 1)\n\tgo func() {\n\t\tselect {\n\t\tcase <-doneCh:\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tpanic(\"timeout\")\n\t\t}\n\t}()\n\n\tin := make([]byte, 1500)\n\tn, _, err := udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tmsgType := messageType(in[0])\n\tif msgType != compoundMsg {\n\t\tt.Fatalf(\"bad response %v\", in)\n\t}\n\n\t// get the parts\n\ttrunc, parts, err := decodeCompoundMessage(in[1:])\n\tif trunc != 0 {\n\t\tt.Fatalf(\"unexpected truncation\")\n\t}\n\tif len(parts) != 2 {\n\t\tt.Fatalf(\"unexpected parts %v\", parts)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tvar ack ackResp\n\tif err := decode(parts[0][1:], &ack); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tif ack.SeqNo != 42 {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n\n\tvar aliveout alive\n\tif err := decode(parts[1][1:], &aliveout); err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\n\tif aliveout.Node != \"rand\" || aliveout.Incarnation != 10 {\n\t\tt.Fatalf(\"bad mesg\")\n\t}\n\n\tdoneCh <- struct{}{}\n}\n\nfunc TestEncryptDecryptState(t *testing.T) {\n\tstate := []byte(\"this is our internal state...\")\n\tconfig := &Config{\n\t\tSecretKey:       []byte{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15},\n\t\tProtocolVersion: ProtocolVersionMax,\n\t}\n\tsink := registerInMemorySink(t)\n\n\tm, err := Create(config)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\tdefer m.Shutdown()\n\n\tcrypt, err := m.encryptLocalState(state, \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\t// Create reader, seek past the type byte\n\tbuf := bytes.NewReader(crypt)\n\tbuf.Seek(1, 0)\n\n\tplain, err := m.decryptRemoteState(buf, \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tverifySampleExists(t, \"consul.usage.test.memberlist.size.remote\", sink)\n\n\tif !reflect.DeepEqual(state, plain) {\n\t\tt.Fatalf(\"Decrypt failed: %v\", plain)\n\t}\n}\n\nfunc TestRawSendUdp_CRC(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\ta := Address{\n\t\tAddr: udp.LocalAddr().String(),\n\t\tName: \"test\",\n\t}\n\n\t// Pass a nil node with no nodes registered, should result in no checksum\n\tpayload := []byte{3, 3, 3, 3}\n\tm.rawSendMsgPacket(a, nil, payload)\n\n\tin := make([]byte, 1500)\n\tn, _, err := udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tif len(in) != 4 {\n\t\tt.Fatalf(\"bad: %v\", in)\n\t}\n\n\t// Pass a non-nil node with PMax >= 5, should result in a checksum\n\tm.rawSendMsgPacket(a, &Node{PMax: 5}, payload)\n\n\tin = make([]byte, 1500)\n\tn, _, err = udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tif len(in) != 9 {\n\t\tt.Fatalf(\"bad: %v\", in)\n\t}\n\n\t// Register a node with PMax >= 5 to be looked up, should result in a checksum\n\tm.nodeMap[\"127.0.0.1\"] = &nodeState{\n\t\tNode: Node{PMax: 5},\n\t}\n\tm.rawSendMsgPacket(a, nil, payload)\n\n\tin = make([]byte, 1500)\n\tn, _, err = udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tif len(in) != 9 {\n\t\tt.Fatalf(\"bad: %v\", in)\n\t}\n}\n\nfunc TestIngestPacket_CRC(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\ta := Address{\n\t\tAddr: udp.LocalAddr().String(),\n\t\tName: \"test\",\n\t}\n\n\t// Get a message with a checksum\n\tpayload := []byte{3, 3, 3, 3}\n\tm.rawSendMsgPacket(a, &Node{PMax: 5}, payload)\n\n\tin := make([]byte, 1500)\n\tn, _, err := udp.ReadFrom(in)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err %s\", err)\n\t}\n\tin = in[0:n]\n\n\tif len(in) != 9 {\n\t\tt.Fatalf(\"bad: %v\", in)\n\t}\n\n\t// Corrupt the checksum\n\tin[1] <<= 1\n\n\tlogs := &bytes.Buffer{}\n\tlogger := log.New(logs, \"\", 0)\n\tm.logger = logger\n\tm.ingestPacket(in, udp.LocalAddr(), time.Now())\n\n\tif !strings.Contains(logs.String(), \"invalid checksum\") {\n\t\tt.Fatalf(\"bad: %s\", logs.String())\n\t}\n}\n\nfunc TestIngestPacket_ExportedFunc_EmptyMessage(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.EnableCompression = false\n\t})\n\tdefer m.Shutdown()\n\n\tudp := listenUDP(t)\n\tdefer udp.Close()\n\n\temptyConn := &emptyReadNetConn{}\n\n\tlogs := &bytes.Buffer{}\n\tlogger := log.New(logs, \"\", 0)\n\tm.logger = logger\n\n\ttype ingestionAwareTransport interface {\n\t\tIngestPacket(conn net.Conn, addr net.Addr, now time.Time, shouldClose bool) error\n\t}\n\n\terr := m.transport.(ingestionAwareTransport).IngestPacket(emptyConn, udp.LocalAddr(), time.Now(), true)\n\trequire.Error(t, err)\n\trequire.Contains(t, err.Error(), \"packet too short\")\n}\n\ntype emptyReadNetConn struct {\n\tnet.Conn\n}\n\nfunc (c *emptyReadNetConn) Read(b []byte) (n int, err error) {\n\treturn 0, io.EOF\n}\n\nfunc (c *emptyReadNetConn) Close() error {\n\treturn nil\n}\n\nfunc TestGossip_MismatchedKeys(t *testing.T) {\n\t// Create two agents with different gossip keys\n\tc1 := testConfig(t)\n\tc1.SecretKey = []byte(\"4W6DGn2VQVqDEceOdmuRTQ==\")\n\n\tm1, err := Create(c1)\n\trequire.NoError(t, err)\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tc2 := testConfig(t)\n\tc2.BindPort = bindPort\n\tc2.SecretKey = []byte(\"XhX/w702/JKKK7/7OtM9Ww==\")\n\n\tm2, err := Create(c2)\n\trequire.NoError(t, err)\n\tdefer m2.Shutdown()\n\n\t// Make sure we get this error on the joining side\n\t_, err = m2.Join([]string{c1.Name + \"/\" + c1.BindAddr})\n\tif err == nil || !strings.Contains(err.Error(), \"No installed keys could decrypt the message\") {\n\t\tt.Fatalf(\"bad: %s\", err)\n\t}\n}\n\nfunc listenUDP(t *testing.T) *net.UDPConn {\n\tvar udp *net.UDPConn\n\tfor port := 60000; port < 61000; port++ {\n\t\tudpAddr := fmt.Sprintf(\"127.0.0.1:%d\", port)\n\t\tudpLn, err := net.ListenPacket(\"udp\", udpAddr)\n\t\tif err == nil {\n\t\t\tudp = udpLn.(*net.UDPConn)\n\t\t\tbreak\n\t\t}\n\t}\n\tif udp == nil {\n\t\tt.Fatalf(\"no udp listener\")\n\t}\n\treturn udp\n}\n\nfunc TestHandleCommand(t *testing.T) {\n\tvar buf bytes.Buffer\n\tm := Memberlist{\n\t\tlogger: log.New(&buf, \"\", 0),\n\t}\n\tm.handleCommand(nil, &net.TCPAddr{Port: 12345}, time.Now())\n\trequire.Contains(t, buf.String(), \"missing message type byte\")\n}\n"
        },
        {
          "name": "net_transport.go",
          "type": "blob",
          "size": 10.1103515625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n\tsockaddr \"github.com/hashicorp/go-sockaddr\"\n)\n\nconst (\n\t// udpPacketBufSize is used to buffer incoming packets during read\n\t// operations.\n\tudpPacketBufSize = 65536\n\n\t// udpRecvBufSize is a large buffer size that we attempt to set UDP\n\t// sockets to in order to handle a large volume of messages.\n\tudpRecvBufSize = 2 * 1024 * 1024\n)\n\n// NetTransportConfig is used to configure a net transport.\ntype NetTransportConfig struct {\n\t// BindAddrs is a list of addresses to bind to for both TCP and UDP\n\t// communications.\n\tBindAddrs []string\n\n\t// BindPort is the port to listen on, for each address above.\n\tBindPort int\n\n\t// Logger is a logger for operator messages.\n\tLogger *log.Logger\n\n\t// MetricLabels is a map of optional labels to apply to all metrics\n\t// emitted by this transport.\n\tMetricLabels []metrics.Label\n}\n\n// NetTransport is a Transport implementation that uses connectionless UDP for\n// packet operations, and ad-hoc TCP connections for stream operations.\ntype NetTransport struct {\n\tconfig       *NetTransportConfig\n\tpacketCh     chan *Packet\n\tstreamCh     chan net.Conn\n\tlogger       *log.Logger\n\twg           sync.WaitGroup\n\ttcpListeners []*net.TCPListener\n\tudpListeners []*net.UDPConn\n\tshutdown     int32\n\n\tmetricLabels []metrics.Label\n}\n\nvar _ NodeAwareTransport = (*NetTransport)(nil)\n\n// NewNetTransport returns a net transport with the given configuration. On\n// success all the network listeners will be created and listening.\nfunc NewNetTransport(config *NetTransportConfig) (*NetTransport, error) {\n\t// If we reject the empty list outright we can assume that there's at\n\t// least one listener of each type later during operation.\n\tif len(config.BindAddrs) == 0 {\n\t\treturn nil, fmt.Errorf(\"At least one bind address is required\")\n\t}\n\n\t// Build out the new transport.\n\tvar ok bool\n\tt := NetTransport{\n\t\tconfig:       config,\n\t\tpacketCh:     make(chan *Packet),\n\t\tstreamCh:     make(chan net.Conn),\n\t\tlogger:       config.Logger,\n\t\tmetricLabels: config.MetricLabels,\n\t}\n\n\t// Clean up listeners if there's an error.\n\tdefer func() {\n\t\tif !ok {\n\t\t\tt.Shutdown()\n\t\t}\n\t}()\n\n\t// Build all the TCP and UDP listeners.\n\tport := config.BindPort\n\tfor _, addr := range config.BindAddrs {\n\t\tip := net.ParseIP(addr)\n\n\t\ttcpAddr := &net.TCPAddr{IP: ip, Port: port}\n\t\ttcpLn, err := net.ListenTCP(\"tcp\", tcpAddr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Failed to start TCP listener on %q port %d: %v\", addr, port, err)\n\t\t}\n\t\tt.tcpListeners = append(t.tcpListeners, tcpLn)\n\n\t\t// If the config port given was zero, use the first TCP listener\n\t\t// to pick an available port and then apply that to everything\n\t\t// else.\n\t\tif port == 0 {\n\t\t\tport = tcpLn.Addr().(*net.TCPAddr).Port\n\t\t}\n\n\t\tudpAddr := &net.UDPAddr{IP: ip, Port: port}\n\t\tudpLn, err := net.ListenUDP(\"udp\", udpAddr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Failed to start UDP listener on %q port %d: %v\", addr, port, err)\n\t\t}\n\t\tif err := setUDPRecvBuf(udpLn); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Failed to resize UDP buffer: %v\", err)\n\t\t}\n\t\tt.udpListeners = append(t.udpListeners, udpLn)\n\t}\n\n\t// Fire them up now that we've been able to create them all.\n\tfor i := 0; i < len(config.BindAddrs); i++ {\n\t\tt.wg.Add(2)\n\t\tgo t.tcpListen(t.tcpListeners[i])\n\t\tgo t.udpListen(t.udpListeners[i])\n\t}\n\n\tok = true\n\treturn &t, nil\n}\n\n// GetAutoBindPort returns the bind port that was automatically given by the\n// kernel, if a bind port of 0 was given.\nfunc (t *NetTransport) GetAutoBindPort() int {\n\t// We made sure there's at least one TCP listener, and that one's\n\t// port was applied to all the others for the dynamic bind case.\n\treturn t.tcpListeners[0].Addr().(*net.TCPAddr).Port\n}\n\n// See Transport.\nfunc (t *NetTransport) FinalAdvertiseAddr(ip string, port int) (net.IP, int, error) {\n\tvar advertiseAddr net.IP\n\tvar advertisePort int\n\tif ip != \"\" {\n\t\t// If they've supplied an address, use that.\n\t\tadvertiseAddr = net.ParseIP(ip)\n\t\tif advertiseAddr == nil {\n\t\t\treturn nil, 0, fmt.Errorf(\"Failed to parse advertise address %q\", ip)\n\t\t}\n\n\t\t// Ensure IPv4 conversion if necessary.\n\t\tif ip4 := advertiseAddr.To4(); ip4 != nil {\n\t\t\tadvertiseAddr = ip4\n\t\t}\n\t\tadvertisePort = port\n\t} else {\n\t\tif t.config.BindAddrs[0] == \"0.0.0.0\" {\n\t\t\t// Otherwise, if we're not bound to a specific IP, let's\n\t\t\t// use a suitable private IP address.\n\t\t\tvar err error\n\t\t\tip, err = sockaddr.GetPrivateIP()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, fmt.Errorf(\"Failed to get interface addresses: %v\", err)\n\t\t\t}\n\t\t\tif ip == \"\" {\n\t\t\t\treturn nil, 0, fmt.Errorf(\"No private IP address found, and explicit IP not provided\")\n\t\t\t}\n\n\t\t\tadvertiseAddr = net.ParseIP(ip)\n\t\t\tif advertiseAddr == nil {\n\t\t\t\treturn nil, 0, fmt.Errorf(\"Failed to parse advertise address: %q\", ip)\n\t\t\t}\n\t\t} else {\n\t\t\t// Use the IP that we're bound to, based on the first\n\t\t\t// TCP listener, which we already ensure is there.\n\t\t\tadvertiseAddr = t.tcpListeners[0].Addr().(*net.TCPAddr).IP\n\t\t}\n\n\t\t// Use the port we are bound to.\n\t\tadvertisePort = t.GetAutoBindPort()\n\t}\n\n\treturn advertiseAddr, advertisePort, nil\n}\n\n// See Transport.\nfunc (t *NetTransport) WriteTo(b []byte, addr string) (time.Time, error) {\n\ta := Address{Addr: addr, Name: \"\"}\n\treturn t.WriteToAddress(b, a)\n}\n\n// See NodeAwareTransport.\nfunc (t *NetTransport) WriteToAddress(b []byte, a Address) (time.Time, error) {\n\taddr := a.Addr\n\n\tudpAddr, err := net.ResolveUDPAddr(\"udp\", addr)\n\tif err != nil {\n\t\treturn time.Time{}, err\n\t}\n\n\t// We made sure there's at least one UDP listener, so just use the\n\t// packet sending interface on the first one. Take the time after the\n\t// write call comes back, which will underestimate the time a little,\n\t// but help account for any delays before the write occurs.\n\t_, err = t.udpListeners[0].WriteTo(b, udpAddr)\n\treturn time.Now(), err\n}\n\n// See Transport.\nfunc (t *NetTransport) PacketCh() <-chan *Packet {\n\treturn t.packetCh\n}\n\n// See IngestionAwareTransport.\nfunc (t *NetTransport) IngestPacket(conn net.Conn, addr net.Addr, now time.Time, shouldClose bool) error {\n\tif shouldClose {\n\t\tdefer conn.Close()\n\t}\n\n\t// Copy everything from the stream into packet buffer.\n\tvar buf bytes.Buffer\n\tif _, err := io.Copy(&buf, conn); err != nil {\n\t\treturn fmt.Errorf(\"failed to read packet: %v\", err)\n\t}\n\n\t// Check the length - it needs to have at least one byte to be a proper\n\t// message. This is checked elsewhere for writes coming in directly from\n\t// the UDP socket.\n\tif n := buf.Len(); n < 1 {\n\t\treturn fmt.Errorf(\"packet too short (%d bytes) %s\", n, LogAddress(addr))\n\t}\n\n\t// Inject the packet.\n\tt.packetCh <- &Packet{\n\t\tBuf:       buf.Bytes(),\n\t\tFrom:      addr,\n\t\tTimestamp: now,\n\t}\n\treturn nil\n}\n\n// See Transport.\nfunc (t *NetTransport) DialTimeout(addr string, timeout time.Duration) (net.Conn, error) {\n\ta := Address{Addr: addr, Name: \"\"}\n\treturn t.DialAddressTimeout(a, timeout)\n}\n\n// See NodeAwareTransport.\nfunc (t *NetTransport) DialAddressTimeout(a Address, timeout time.Duration) (net.Conn, error) {\n\taddr := a.Addr\n\n\tdialer := net.Dialer{Timeout: timeout}\n\treturn dialer.Dial(\"tcp\", addr)\n}\n\n// See Transport.\nfunc (t *NetTransport) StreamCh() <-chan net.Conn {\n\treturn t.streamCh\n}\n\n// See IngestionAwareTransport.\nfunc (t *NetTransport) IngestStream(conn net.Conn) error {\n\tt.streamCh <- conn\n\treturn nil\n}\n\n// See Transport.\nfunc (t *NetTransport) Shutdown() error {\n\t// This will avoid log spam about errors when we shut down.\n\tatomic.StoreInt32(&t.shutdown, 1)\n\n\t// Rip through all the connections and shut them down.\n\tfor _, conn := range t.tcpListeners {\n\t\tconn.Close()\n\t}\n\tfor _, conn := range t.udpListeners {\n\t\tconn.Close()\n\t}\n\n\t// Block until all the listener threads have died.\n\tt.wg.Wait()\n\treturn nil\n}\n\n// tcpListen is a long running goroutine that accepts incoming TCP connections\n// and hands them off to the stream channel.\nfunc (t *NetTransport) tcpListen(tcpLn *net.TCPListener) {\n\tdefer t.wg.Done()\n\n\t// baseDelay is the initial delay after an AcceptTCP() error before attempting again\n\tconst baseDelay = 5 * time.Millisecond\n\n\t// maxDelay is the maximum delay after an AcceptTCP() error before attempting again.\n\t// In the case that tcpListen() is error-looping, it will delay the shutdown check.\n\t// Therefore, changes to maxDelay may have an effect on the latency of shutdown.\n\tconst maxDelay = 1 * time.Second\n\n\tvar loopDelay time.Duration\n\tfor {\n\t\tconn, err := tcpLn.AcceptTCP()\n\t\tif err != nil {\n\t\t\tif s := atomic.LoadInt32(&t.shutdown); s == 1 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tif loopDelay == 0 {\n\t\t\t\tloopDelay = baseDelay\n\t\t\t} else {\n\t\t\t\tloopDelay *= 2\n\t\t\t}\n\n\t\t\tif loopDelay > maxDelay {\n\t\t\t\tloopDelay = maxDelay\n\t\t\t}\n\n\t\t\tt.logger.Printf(\"[ERR] memberlist: Error accepting TCP connection: %v\", err)\n\t\t\ttime.Sleep(loopDelay)\n\t\t\tcontinue\n\t\t}\n\t\t// No error, reset loop delay\n\t\tloopDelay = 0\n\n\t\tt.streamCh <- conn\n\t}\n}\n\n// udpListen is a long running goroutine that accepts incoming UDP packets and\n// hands them off to the packet channel.\nfunc (t *NetTransport) udpListen(udpLn *net.UDPConn) {\n\tdefer t.wg.Done()\n\tfor {\n\t\t// Do a blocking read into a fresh buffer. Grab a time stamp as\n\t\t// close as possible to the I/O.\n\t\tbuf := make([]byte, udpPacketBufSize)\n\t\tn, addr, err := udpLn.ReadFrom(buf)\n\t\tts := time.Now()\n\t\tif err != nil {\n\t\t\tif s := atomic.LoadInt32(&t.shutdown); s == 1 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tt.logger.Printf(\"[ERR] memberlist: Error reading UDP packet: %v\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check the length - it needs to have at least one byte to be a\n\t\t// proper message.\n\t\tif n < 1 {\n\t\t\tt.logger.Printf(\"[ERR] memberlist: UDP packet too short (%d bytes) %s\",\n\t\t\t\tlen(buf), LogAddress(addr))\n\t\t\tcontinue\n\t\t}\n\n\t\t// Ingest the packet.\n\t\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"udp\", \"received\"}, float32(n), t.metricLabels)\n\t\tt.packetCh <- &Packet{\n\t\t\tBuf:       buf[:n],\n\t\t\tFrom:      addr,\n\t\t\tTimestamp: ts,\n\t\t}\n\t}\n}\n\n// setUDPRecvBuf is used to resize the UDP receive window. The function\n// attempts to set the read buffer to `udpRecvBuf` but backs off until\n// the read buffer can be set.\nfunc setUDPRecvBuf(c *net.UDPConn) error {\n\tsize := udpRecvBufSize\n\tvar err error\n\tfor size > 0 {\n\t\tif err = c.SetReadBuffer(size); err == nil {\n\t\t\treturn nil\n\t\t}\n\t\tsize = size / 2\n\t}\n\treturn err\n}\n"
        },
        {
          "name": "peeked_conn.go",
          "type": "blob",
          "size": 1.5576171875,
          "content": "// Copyright 2017 Google Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Originally from: https://github.com/google/tcpproxy/blob/master/tcpproxy.go\n// at f5c09fbedceb69e4b238dec52cdf9f2fe9a815e2\n\npackage memberlist\n\nimport \"net\"\n\n// peekedConn is an incoming connection that has had some bytes read from it\n// to determine how to route the connection. The Read method stitches\n// the peeked bytes and unread bytes back together.\ntype peekedConn struct {\n\t// Peeked are the bytes that have been read from Conn for the\n\t// purposes of route matching, but have not yet been consumed\n\t// by Read calls. It set to nil by Read when fully consumed.\n\tPeeked []byte\n\n\t// Conn is the underlying connection.\n\t// It can be type asserted against *net.TCPConn or other types\n\t// as needed. It should not be read from directly unless\n\t// Peeked is nil.\n\tnet.Conn\n}\n\nfunc (c *peekedConn) Read(p []byte) (n int, err error) {\n\tif len(c.Peeked) > 0 {\n\t\tn = copy(p, c.Peeked)\n\t\tc.Peeked = c.Peeked[n:]\n\t\tif len(c.Peeked) == 0 {\n\t\t\tc.Peeked = nil\n\t\t}\n\t\treturn n, nil\n\t}\n\treturn c.Conn.Read(p)\n}\n"
        },
        {
          "name": "ping_delegate.go",
          "type": "blob",
          "size": 0.6982421875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport \"time\"\n\n// PingDelegate is used to notify an observer how long it took for a ping message to\n// complete a round trip.  It can also be used for writing arbitrary byte slices\n// into ack messages. Note that in order to be meaningful for RTT estimates, this\n// delegate does not apply to indirect pings, nor fallback pings sent over TCP.\ntype PingDelegate interface {\n\t// AckPayload is invoked when an ack is being sent; the returned bytes will be appended to the ack\n\tAckPayload() []byte\n\t// NotifyPing is invoked when an ack for a ping is received\n\tNotifyPingComplete(other *Node, rtt time.Duration, payload []byte)\n}\n"
        },
        {
          "name": "queue.go",
          "type": "blob",
          "size": 11.185546875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"math\"\n\t\"sync\"\n\n\t\"github.com/google/btree\"\n)\n\n// TransmitLimitedQueue is used to queue messages to broadcast to\n// the cluster (via gossip) but limits the number of transmits per\n// message. It also prioritizes messages with lower transmit counts\n// (hence newer messages).\ntype TransmitLimitedQueue struct {\n\t// NumNodes returns the number of nodes in the cluster. This is\n\t// used to determine the retransmit count, which is calculated\n\t// based on the log of this.\n\tNumNodes func() int\n\n\t// RetransmitMult is the multiplier used to determine the maximum\n\t// number of retransmissions attempted.\n\tRetransmitMult int\n\n\tmu    sync.Mutex\n\ttq    *btree.BTree // stores *limitedBroadcast as btree.Item\n\ttm    map[string]*limitedBroadcast\n\tidGen int64\n}\n\ntype limitedBroadcast struct {\n\ttransmits int   // btree-key[0]: Number of transmissions attempted.\n\tmsgLen    int64 // btree-key[1]: copied from len(b.Message())\n\tid        int64 // btree-key[2]: unique incrementing id stamped at submission time\n\tb         Broadcast\n\n\tname string // set if Broadcast is a NamedBroadcast\n}\n\n// Less tests whether the current item is less than the given argument.\n//\n// This must provide a strict weak ordering.\n// If !a.Less(b) && !b.Less(a), we treat this to mean a == b (i.e. we can only\n// hold one of either a or b in the tree).\n//\n// default ordering is\n// - [transmits=0, ..., transmits=inf]\n// - [transmits=0:len=999, ..., transmits=0:len=2, ...]\n// - [transmits=0:len=999,id=999, ..., transmits=0:len=999:id=1, ...]\nfunc (b *limitedBroadcast) Less(than btree.Item) bool {\n\to := than.(*limitedBroadcast)\n\tif b.transmits < o.transmits {\n\t\treturn true\n\t} else if b.transmits > o.transmits {\n\t\treturn false\n\t}\n\tif b.msgLen > o.msgLen {\n\t\treturn true\n\t} else if b.msgLen < o.msgLen {\n\t\treturn false\n\t}\n\treturn b.id > o.id\n}\n\n// for testing; emits in transmit order if reverse=false\nfunc (q *TransmitLimitedQueue) orderedView(reverse bool) []*limitedBroadcast {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\tout := make([]*limitedBroadcast, 0, q.lenLocked())\n\tq.walkReadOnlyLocked(reverse, func(cur *limitedBroadcast) bool {\n\t\tout = append(out, cur)\n\t\treturn true\n\t})\n\n\treturn out\n}\n\n// walkReadOnlyLocked calls f for each item in the queue traversing it in\n// natural order (by Less) when reverse=false and the opposite when true. You\n// must hold the mutex.\n//\n// This method panics if you attempt to mutate the item during traversal.  The\n// underlying btree should also not be mutated during traversal.\nfunc (q *TransmitLimitedQueue) walkReadOnlyLocked(reverse bool, f func(*limitedBroadcast) bool) {\n\tif q.lenLocked() == 0 {\n\t\treturn\n\t}\n\n\titer := func(item btree.Item) bool {\n\t\tcur := item.(*limitedBroadcast)\n\n\t\tprevTransmits := cur.transmits\n\t\tprevMsgLen := cur.msgLen\n\t\tprevID := cur.id\n\n\t\tkeepGoing := f(cur)\n\n\t\tif prevTransmits != cur.transmits || prevMsgLen != cur.msgLen || prevID != cur.id {\n\t\t\tpanic(\"edited queue while walking read only\")\n\t\t}\n\n\t\treturn keepGoing\n\t}\n\n\tif reverse {\n\t\tq.tq.Descend(iter) // end with transmit 0\n\t} else {\n\t\tq.tq.Ascend(iter) // start with transmit 0\n\t}\n}\n\n// Broadcast is something that can be broadcasted via gossip to\n// the memberlist cluster.\ntype Broadcast interface {\n\t// Invalidates checks if enqueuing the current broadcast\n\t// invalidates a previous broadcast\n\tInvalidates(b Broadcast) bool\n\n\t// Returns a byte form of the message\n\tMessage() []byte\n\n\t// Finished is invoked when the message will no longer\n\t// be broadcast, either due to invalidation or to the\n\t// transmit limit being reached\n\tFinished()\n}\n\n// NamedBroadcast is an optional extension of the Broadcast interface that\n// gives each message a unique string name, and that is used to optimize\n//\n// You shoud ensure that Invalidates() checks the same uniqueness as the\n// example below:\n//\n//\tfunc (b *foo) Invalidates(other Broadcast) bool {\n//\t\tnb, ok := other.(NamedBroadcast)\n//\t\tif !ok {\n//\t\t\treturn false\n//\t\t}\n//\t\treturn b.Name() == nb.Name()\n//\t}\n//\n// Invalidates() isn't currently used for NamedBroadcasts, but that may change\n// in the future.\ntype NamedBroadcast interface {\n\tBroadcast\n\t// The unique identity of this broadcast message.\n\tName() string\n}\n\n// UniqueBroadcast is an optional interface that indicates that each message is\n// intrinsically unique and there is no need to scan the broadcast queue for\n// duplicates.\n//\n// You should ensure that Invalidates() always returns false if implementing\n// this interface. Invalidates() isn't currently used for UniqueBroadcasts, but\n// that may change in the future.\ntype UniqueBroadcast interface {\n\tBroadcast\n\t// UniqueBroadcast is just a marker method for this interface.\n\tUniqueBroadcast()\n}\n\n// QueueBroadcast is used to enqueue a broadcast\nfunc (q *TransmitLimitedQueue) QueueBroadcast(b Broadcast) {\n\tq.queueBroadcast(b, 0)\n}\n\n// lazyInit initializes internal data structures the first time they are\n// needed.  You must already hold the mutex.\nfunc (q *TransmitLimitedQueue) lazyInit() {\n\tif q.tq == nil {\n\t\tq.tq = btree.New(32)\n\t}\n\tif q.tm == nil {\n\t\tq.tm = make(map[string]*limitedBroadcast)\n\t}\n}\n\n// queueBroadcast is like QueueBroadcast but you can use a nonzero value for\n// the initial transmit tier assigned to the message. This is meant to be used\n// for unit testing.\nfunc (q *TransmitLimitedQueue) queueBroadcast(b Broadcast, initialTransmits int) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\tq.lazyInit()\n\n\tif q.idGen == math.MaxInt64 {\n\t\t// it's super duper unlikely to wrap around within the retransmit limit\n\t\tq.idGen = 1\n\t} else {\n\t\tq.idGen++\n\t}\n\tid := q.idGen\n\n\tlb := &limitedBroadcast{\n\t\ttransmits: initialTransmits,\n\t\tmsgLen:    int64(len(b.Message())),\n\t\tid:        id,\n\t\tb:         b,\n\t}\n\tunique := false\n\tif nb, ok := b.(NamedBroadcast); ok {\n\t\tlb.name = nb.Name()\n\t} else if _, ok := b.(UniqueBroadcast); ok {\n\t\tunique = true\n\t}\n\n\t// Check if this message invalidates another.\n\tif lb.name != \"\" {\n\t\tif old, ok := q.tm[lb.name]; ok {\n\t\t\told.b.Finished()\n\t\t\tq.deleteItem(old)\n\t\t}\n\t} else if !unique {\n\t\t// Slow path, hopefully nothing hot hits this.\n\t\tvar remove []*limitedBroadcast\n\t\tq.tq.Ascend(func(item btree.Item) bool {\n\t\t\tcur := item.(*limitedBroadcast)\n\n\t\t\t// Special Broadcasts can only invalidate each other.\n\t\t\tswitch cur.b.(type) {\n\t\t\tcase NamedBroadcast:\n\t\t\t\t// noop\n\t\t\tcase UniqueBroadcast:\n\t\t\t\t// noop\n\t\t\tdefault:\n\t\t\t\tif b.Invalidates(cur.b) {\n\t\t\t\t\tcur.b.Finished()\n\t\t\t\t\tremove = append(remove, cur)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\tfor _, cur := range remove {\n\t\t\tq.deleteItem(cur)\n\t\t}\n\t}\n\n\t// Append to the relevant queue.\n\tq.addItem(lb)\n}\n\n// deleteItem removes the given item from the overall datastructure. You\n// must already hold the mutex.\nfunc (q *TransmitLimitedQueue) deleteItem(cur *limitedBroadcast) {\n\t_ = q.tq.Delete(cur)\n\tif cur.name != \"\" {\n\t\tdelete(q.tm, cur.name)\n\t}\n\n\tif q.tq.Len() == 0 {\n\t\t// At idle there's no reason to let the id generator keep going\n\t\t// indefinitely.\n\t\tq.idGen = 0\n\t}\n}\n\n// addItem adds the given item into the overall datastructure. You must already\n// hold the mutex.\nfunc (q *TransmitLimitedQueue) addItem(cur *limitedBroadcast) {\n\t_ = q.tq.ReplaceOrInsert(cur)\n\tif cur.name != \"\" {\n\t\tq.tm[cur.name] = cur\n\t}\n}\n\n// getTransmitRange returns a pair of min/max values for transmit values\n// represented by the current queue contents. Both values represent actual\n// transmit values on the interval [0, len). You must already hold the mutex.\nfunc (q *TransmitLimitedQueue) getTransmitRange() (minTransmit, maxTransmit int) {\n\tif q.lenLocked() == 0 {\n\t\treturn 0, 0\n\t}\n\tminItem, maxItem := q.tq.Min(), q.tq.Max()\n\tif minItem == nil || maxItem == nil {\n\t\treturn 0, 0\n\t}\n\n\tmin := minItem.(*limitedBroadcast).transmits\n\tmax := maxItem.(*limitedBroadcast).transmits\n\n\treturn min, max\n}\n\n// GetBroadcasts is used to get a number of broadcasts, up to a byte limit\n// and applying a per-message overhead as provided.\nfunc (q *TransmitLimitedQueue) GetBroadcasts(overhead, limit int) [][]byte {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\t// Fast path the default case\n\tif q.lenLocked() == 0 {\n\t\treturn nil\n\t}\n\n\ttransmitLimit := retransmitLimit(q.RetransmitMult, q.NumNodes())\n\n\tvar (\n\t\tbytesUsed int\n\t\ttoSend    [][]byte\n\t\treinsert  []*limitedBroadcast\n\t)\n\n\t// Visit fresher items first, but only look at stuff that will fit.\n\t// We'll go tier by tier, grabbing the largest items first.\n\tminTr, maxTr := q.getTransmitRange()\n\tfor transmits := minTr; transmits <= maxTr; /*do not advance automatically*/ {\n\t\tfree := int64(limit - bytesUsed - overhead)\n\t\tif free <= 0 {\n\t\t\tbreak // bail out early\n\t\t}\n\n\t\t// Search for the least element on a given tier (by transmit count) as\n\t\t// defined in the limitedBroadcast.Less function that will fit into our\n\t\t// remaining space.\n\t\tgreaterOrEqual := &limitedBroadcast{\n\t\t\ttransmits: transmits,\n\t\t\tmsgLen:    free,\n\t\t\tid:        math.MaxInt64,\n\t\t}\n\t\tlessThan := &limitedBroadcast{\n\t\t\ttransmits: transmits + 1,\n\t\t\tmsgLen:    math.MaxInt64,\n\t\t\tid:        math.MaxInt64,\n\t\t}\n\t\tvar keep *limitedBroadcast\n\t\tq.tq.AscendRange(greaterOrEqual, lessThan, func(item btree.Item) bool {\n\t\t\tcur := item.(*limitedBroadcast)\n\t\t\t// Check if this is within our limits\n\t\t\tif int64(len(cur.b.Message())) > free {\n\t\t\t\t// If this happens it's a bug in the datastructure or\n\t\t\t\t// surrounding use doing something like having len(Message())\n\t\t\t\t// change over time. There's enough going on here that it's\n\t\t\t\t// probably sane to just skip it and move on for now.\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tkeep = cur\n\t\t\treturn false\n\t\t})\n\t\tif keep == nil {\n\t\t\t// No more items of an appropriate size in the tier.\n\t\t\ttransmits++\n\t\t\tcontinue\n\t\t}\n\n\t\tmsg := keep.b.Message()\n\n\t\t// Add to slice to send\n\t\tbytesUsed += overhead + len(msg)\n\t\ttoSend = append(toSend, msg)\n\n\t\t// Check if we should stop transmission\n\t\tq.deleteItem(keep)\n\t\tif keep.transmits+1 >= transmitLimit {\n\t\t\tkeep.b.Finished()\n\t\t} else {\n\t\t\t// We need to bump this item down to another transmit tier, but\n\t\t\t// because it would be in the same direction that we're walking the\n\t\t\t// tiers, we will have to delay the reinsertion until we are\n\t\t\t// finished our search. Otherwise we'll possibly re-add the message\n\t\t\t// when we ascend to the next tier.\n\t\t\tkeep.transmits++\n\t\t\treinsert = append(reinsert, keep)\n\t\t}\n\t}\n\n\tfor _, cur := range reinsert {\n\t\tq.addItem(cur)\n\t}\n\n\treturn toSend\n}\n\n// NumQueued returns the number of queued messages\nfunc (q *TransmitLimitedQueue) NumQueued() int {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\treturn q.lenLocked()\n}\n\n// lenLocked returns the length of the overall queue datastructure. You must\n// hold the mutex.\nfunc (q *TransmitLimitedQueue) lenLocked() int {\n\tif q.tq == nil {\n\t\treturn 0\n\t}\n\treturn q.tq.Len()\n}\n\n// Reset clears all the queued messages. Should only be used for tests.\nfunc (q *TransmitLimitedQueue) Reset() {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\tq.walkReadOnlyLocked(false, func(cur *limitedBroadcast) bool {\n\t\tcur.b.Finished()\n\t\treturn true\n\t})\n\n\tq.tq = nil\n\tq.tm = nil\n\tq.idGen = 0\n}\n\n// Prune will retain the maxRetain latest messages, and the rest\n// will be discarded. This can be used to prevent unbounded queue sizes\nfunc (q *TransmitLimitedQueue) Prune(maxRetain int) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\t// Do nothing if queue size is less than the limit\n\tfor q.tq.Len() > maxRetain {\n\t\titem := q.tq.Max()\n\t\tif item == nil {\n\t\t\tbreak\n\t\t}\n\t\tcur := item.(*limitedBroadcast)\n\t\tcur.b.Finished()\n\t\tq.deleteItem(cur)\n\t}\n}\n"
        },
        {
          "name": "queue_test.go",
          "type": "blob",
          "size": 6.8740234375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"testing\"\n\n\t\"github.com/google/btree\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestLimitedBroadcastLess(t *testing.T) {\n\tcases := []struct {\n\t\tName string\n\t\tA    *limitedBroadcast // lesser\n\t\tB    *limitedBroadcast\n\t}{\n\t\t{\n\t\t\t\"diff-transmits\",\n\t\t\t&limitedBroadcast{transmits: 0, msgLen: 10, id: 100},\n\t\t\t&limitedBroadcast{transmits: 1, msgLen: 10, id: 100},\n\t\t},\n\t\t{\n\t\t\t\"same-transmits--diff-len\",\n\t\t\t&limitedBroadcast{transmits: 0, msgLen: 12, id: 100},\n\t\t\t&limitedBroadcast{transmits: 0, msgLen: 10, id: 100},\n\t\t},\n\t\t{\n\t\t\t\"same-transmits--same-len--diff-id\",\n\t\t\t&limitedBroadcast{transmits: 0, msgLen: 12, id: 100},\n\t\t\t&limitedBroadcast{transmits: 0, msgLen: 12, id: 90},\n\t\t},\n\t}\n\n\tfor _, c := range cases {\n\t\tt.Run(c.Name, func(t *testing.T) {\n\t\t\ta, b := c.A, c.B\n\n\t\t\trequire.True(t, a.Less(b))\n\n\t\t\ttree := btree.New(32)\n\n\t\t\ttree.ReplaceOrInsert(b)\n\t\t\ttree.ReplaceOrInsert(a)\n\n\t\t\tmin := tree.Min().(*limitedBroadcast)\n\t\t\trequire.Equal(t, a.transmits, min.transmits)\n\t\t\trequire.Equal(t, a.msgLen, min.msgLen)\n\t\t\trequire.Equal(t, a.id, min.id)\n\n\t\t\tmax := tree.Max().(*limitedBroadcast)\n\t\t\trequire.Equal(t, b.transmits, max.transmits)\n\t\t\trequire.Equal(t, b.msgLen, max.msgLen)\n\t\t\trequire.Equal(t, b.id, max.id)\n\t\t})\n\t}\n}\n\nfunc TestTransmitLimited_Queue(t *testing.T) {\n\tq := &TransmitLimitedQueue{RetransmitMult: 1, NumNodes: func() int { return 1 }}\n\tq.QueueBroadcast(&memberlistBroadcast{\"test\", nil, nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"foo\", nil, nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"bar\", nil, nil})\n\n\tif q.NumQueued() != 3 {\n\t\tt.Fatalf(\"bad len\")\n\t}\n\tdump := q.orderedView(true)\n\tif dump[0].b.(*memberlistBroadcast).node != \"test\" {\n\t\tt.Fatalf(\"missing test\")\n\t}\n\tif dump[1].b.(*memberlistBroadcast).node != \"foo\" {\n\t\tt.Fatalf(\"missing foo\")\n\t}\n\tif dump[2].b.(*memberlistBroadcast).node != \"bar\" {\n\t\tt.Fatalf(\"missing bar\")\n\t}\n\n\t// Should invalidate previous message\n\tq.QueueBroadcast(&memberlistBroadcast{\"test\", nil, nil})\n\n\tif q.NumQueued() != 3 {\n\t\tt.Fatalf(\"bad len\")\n\t}\n\tdump = q.orderedView(true)\n\tif dump[0].b.(*memberlistBroadcast).node != \"foo\" {\n\t\tt.Fatalf(\"missing foo\")\n\t}\n\tif dump[1].b.(*memberlistBroadcast).node != \"bar\" {\n\t\tt.Fatalf(\"missing bar\")\n\t}\n\tif dump[2].b.(*memberlistBroadcast).node != \"test\" {\n\t\tt.Fatalf(\"missing test\")\n\t}\n}\n\nfunc TestTransmitLimited_GetBroadcasts(t *testing.T) {\n\tq := &TransmitLimitedQueue{RetransmitMult: 3, NumNodes: func() int { return 10 }}\n\n\t// 18 bytes per message\n\tq.QueueBroadcast(&memberlistBroadcast{\"test\", []byte(\"1. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"foo\", []byte(\"2. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"bar\", []byte(\"3. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"baz\", []byte(\"4. this is a test.\"), nil})\n\n\t// 2 byte overhead per message, should get all 4 messages\n\tall := q.GetBroadcasts(2, 80)\n\trequire.Equal(t, 4, len(all), \"missing messages: %v\", prettyPrintMessages(all))\n\n\t// 3 byte overhead, should only get 3 messages back\n\tpartial := q.GetBroadcasts(3, 80)\n\trequire.Equal(t, 3, len(partial), \"missing messages: %v\", prettyPrintMessages(partial))\n}\n\nfunc TestTransmitLimited_GetBroadcasts_Limit(t *testing.T) {\n\tq := &TransmitLimitedQueue{RetransmitMult: 1, NumNodes: func() int { return 10 }}\n\n\trequire.Equal(t, int64(0), q.idGen, \"the id generator seed starts at zero\")\n\trequire.Equal(t, 2, retransmitLimit(q.RetransmitMult, q.NumNodes()), \"sanity check transmit limits\")\n\n\t// 18 bytes per message\n\tq.QueueBroadcast(&memberlistBroadcast{\"test\", []byte(\"1. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"foo\", []byte(\"2. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"bar\", []byte(\"3. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"baz\", []byte(\"4. this is a test.\"), nil})\n\n\trequire.Equal(t, int64(4), q.idGen, \"we handed out 4 IDs\")\n\n\t// 3 byte overhead, should only get 3 messages back\n\tpartial1 := q.GetBroadcasts(3, 80)\n\trequire.Equal(t, 3, len(partial1), \"missing messages: %v\", prettyPrintMessages(partial1))\n\n\trequire.Equal(t, int64(4), q.idGen, \"id generator doesn't reset until empty\")\n\n\tpartial2 := q.GetBroadcasts(3, 80)\n\trequire.Equal(t, 3, len(partial2), \"missing messages: %v\", prettyPrintMessages(partial2))\n\n\trequire.Equal(t, int64(4), q.idGen, \"id generator doesn't reset until empty\")\n\n\t// Only two not expired\n\tpartial3 := q.GetBroadcasts(3, 80)\n\trequire.Equal(t, 2, len(partial3), \"missing messages: %v\", prettyPrintMessages(partial3))\n\n\trequire.Equal(t, int64(0), q.idGen, \"id generator resets on empty\")\n\n\t// Should get nothing\n\tpartial5 := q.GetBroadcasts(3, 80)\n\trequire.Equal(t, 0, len(partial5), \"missing messages: %v\", prettyPrintMessages(partial5))\n\n\trequire.Equal(t, int64(0), q.idGen, \"id generator resets on empty\")\n}\n\nfunc prettyPrintMessages(msgs [][]byte) []string {\n\tvar out []string\n\tfor _, msg := range msgs {\n\t\tout = append(out, \"'\"+string(msg)+\"'\")\n\t}\n\treturn out\n}\n\nfunc TestTransmitLimited_Prune(t *testing.T) {\n\tq := &TransmitLimitedQueue{RetransmitMult: 1, NumNodes: func() int { return 10 }}\n\n\tch1 := make(chan struct{}, 1)\n\tch2 := make(chan struct{}, 1)\n\n\t// 18 bytes per message\n\tq.QueueBroadcast(&memberlistBroadcast{\"test\", []byte(\"1. this is a test.\"), ch1})\n\tq.QueueBroadcast(&memberlistBroadcast{\"foo\", []byte(\"2. this is a test.\"), ch2})\n\tq.QueueBroadcast(&memberlistBroadcast{\"bar\", []byte(\"3. this is a test.\"), nil})\n\tq.QueueBroadcast(&memberlistBroadcast{\"baz\", []byte(\"4. this is a test.\"), nil})\n\n\t// Keep only 2\n\tq.Prune(2)\n\n\trequire.Equal(t, 2, q.NumQueued())\n\n\t// Should notify the first two\n\tselect {\n\tcase <-ch1:\n\tdefault:\n\t\tt.Fatalf(\"expected invalidation\")\n\t}\n\tselect {\n\tcase <-ch2:\n\tdefault:\n\t\tt.Fatalf(\"expected invalidation\")\n\t}\n\n\tdump := q.orderedView(true)\n\n\tif dump[0].b.(*memberlistBroadcast).node != \"bar\" {\n\t\tt.Fatalf(\"missing bar\")\n\t}\n\tif dump[1].b.(*memberlistBroadcast).node != \"baz\" {\n\t\tt.Fatalf(\"missing baz\")\n\t}\n}\n\nfunc TestTransmitLimited_ordering(t *testing.T) {\n\tq := &TransmitLimitedQueue{RetransmitMult: 1, NumNodes: func() int { return 10 }}\n\n\tinsert := func(name string, transmits int) {\n\t\tq.queueBroadcast(&memberlistBroadcast{name, []byte(name), make(chan struct{})}, transmits)\n\t}\n\n\tinsert(\"node0\", 0)\n\tinsert(\"node1\", 10)\n\tinsert(\"node2\", 3)\n\tinsert(\"node3\", 4)\n\tinsert(\"node4\", 7)\n\n\tdump := q.orderedView(true)\n\n\tif dump[0].transmits != 10 {\n\t\tt.Fatalf(\"bad val %v, %d\", dump[0].b.(*memberlistBroadcast).node, dump[0].transmits)\n\t}\n\tif dump[1].transmits != 7 {\n\t\tt.Fatalf(\"bad val %v, %d\", dump[7].b.(*memberlistBroadcast).node, dump[7].transmits)\n\t}\n\tif dump[2].transmits != 4 {\n\t\tt.Fatalf(\"bad val %v, %d\", dump[2].b.(*memberlistBroadcast).node, dump[2].transmits)\n\t}\n\tif dump[3].transmits != 3 {\n\t\tt.Fatalf(\"bad val %v, %d\", dump[3].b.(*memberlistBroadcast).node, dump[3].transmits)\n\t}\n\tif dump[4].transmits != 0 {\n\t\tt.Fatalf(\"bad val %v, %d\", dump[4].b.(*memberlistBroadcast).node, dump[4].transmits)\n\t}\n}\n"
        },
        {
          "name": "security.go",
          "type": "blob",
          "size": 5.3818359375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/rand\"\n\t\"fmt\"\n\t\"io\"\n)\n\n/*\nEncrypted messages are prefixed with an encryptionVersion byte\nthat is used for us to be able to properly encode/decode. We\ncurrently support the following versions:\n\n\t0 - AES-GCM 128, using PKCS7 padding\n\t1 - AES-GCM 128, no padding. Padding not needed, caused bloat.\n*/\ntype encryptionVersion uint8\n\nconst (\n\tminEncryptionVersion encryptionVersion = 0\n\tmaxEncryptionVersion encryptionVersion = 1\n)\n\nconst (\n\tversionSize    = 1\n\tnonceSize      = 12\n\ttagSize        = 16\n\tmaxPadOverhead = 16\n\tblockSize      = aes.BlockSize\n)\n\n// pkcs7encode is used to pad a byte buffer to a specific block size using\n// the PKCS7 algorithm. \"Ignores\" some bytes to compensate for IV\nfunc pkcs7encode(buf *bytes.Buffer, ignore, blockSize int) {\n\tn := buf.Len() - ignore\n\tmore := blockSize - (n % blockSize)\n\tfor i := 0; i < more; i++ {\n\t\tbuf.WriteByte(byte(more))\n\t}\n}\n\n// pkcs7decode is used to decode a buffer that has been padded\nfunc pkcs7decode(buf []byte, blockSize int) []byte {\n\tif len(buf) == 0 {\n\t\tpanic(\"Cannot decode a PKCS7 buffer of zero length\")\n\t}\n\tn := len(buf)\n\tlast := buf[n-1]\n\tn -= int(last)\n\treturn buf[:n]\n}\n\n// encryptOverhead returns the maximum possible overhead of encryption by version\nfunc encryptOverhead(vsn encryptionVersion) int {\n\tswitch vsn {\n\tcase 0:\n\t\treturn 45 // Version: 1, IV: 12, Padding: 16, Tag: 16\n\tcase 1:\n\t\treturn 29 // Version: 1, IV: 12, Tag: 16\n\tdefault:\n\t\tpanic(\"unsupported version\")\n\t}\n}\n\n// encryptedLength is used to compute the buffer size needed\n// for a message of given length\nfunc encryptedLength(vsn encryptionVersion, inp int) int {\n\t// If we are on version 1, there is no padding\n\tif vsn >= 1 {\n\t\treturn versionSize + nonceSize + inp + tagSize\n\t}\n\n\t// Determine the padding size\n\tpadding := blockSize - (inp % blockSize)\n\n\t// Sum the extra parts to get total size\n\treturn versionSize + nonceSize + inp + padding + tagSize\n}\n\n// encryptPayload is used to encrypt a message with a given key.\n// We make use of AES-128 in GCM mode. New byte buffer is the version,\n// nonce, ciphertext and tag\nfunc encryptPayload(vsn encryptionVersion, key []byte, msg []byte, data []byte, dst *bytes.Buffer) error {\n\t// Get the AES block cipher\n\taesBlock, err := aes.NewCipher(key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Get the GCM cipher mode\n\tgcm, err := cipher.NewGCM(aesBlock)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Grow the buffer to make room for everything\n\toffset := dst.Len()\n\tdst.Grow(encryptedLength(vsn, len(msg)))\n\n\t// Write the encryption version\n\tdst.WriteByte(byte(vsn))\n\n\t// Add a random nonce\n\t_, err = io.CopyN(dst, rand.Reader, nonceSize)\n\tif err != nil {\n\t\treturn err\n\t}\n\tafterNonce := dst.Len()\n\n\t// Ensure we are correctly padded (only version 0)\n\tif vsn == 0 {\n\t\tio.Copy(dst, bytes.NewReader(msg))\n\t\tpkcs7encode(dst, offset+versionSize+nonceSize, aes.BlockSize)\n\t}\n\n\t// Encrypt message using GCM\n\tslice := dst.Bytes()[offset:]\n\tnonce := slice[versionSize : versionSize+nonceSize]\n\n\t// Message source depends on the encryption version.\n\t// Version 0 uses padding, version 1 does not\n\tvar src []byte\n\tif vsn == 0 {\n\t\tsrc = slice[versionSize+nonceSize:]\n\t} else {\n\t\tsrc = msg\n\t}\n\tout := gcm.Seal(nil, nonce, src, data)\n\n\t// Truncate the plaintext, and write the cipher text\n\tdst.Truncate(afterNonce)\n\tdst.Write(out)\n\treturn nil\n}\n\n// decryptMessage performs the actual decryption of ciphertext. This is in its\n// own function to allow it to be called on all keys easily.\nfunc decryptMessage(key, msg []byte, data []byte) ([]byte, error) {\n\t// Get the AES block cipher\n\taesBlock, err := aes.NewCipher(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the GCM cipher mode\n\tgcm, err := cipher.NewGCM(aesBlock)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Decrypt the message\n\tnonce := msg[versionSize : versionSize+nonceSize]\n\tciphertext := msg[versionSize+nonceSize:]\n\tplain, err := gcm.Open(nil, nonce, ciphertext, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Success!\n\treturn plain, nil\n}\n\n// decryptPayload is used to decrypt a message with a given key,\n// and verify it's contents. Any padding will be removed, and a\n// slice to the plaintext is returned. Decryption is done IN PLACE!\nfunc decryptPayload(keys [][]byte, msg []byte, data []byte) ([]byte, error) {\n\t// Ensure we have at least one byte\n\tif len(msg) == 0 {\n\t\treturn nil, fmt.Errorf(\"Cannot decrypt empty payload\")\n\t}\n\n\t// Verify the version\n\tvsn := encryptionVersion(msg[0])\n\tif vsn > maxEncryptionVersion {\n\t\treturn nil, fmt.Errorf(\"Unsupported encryption version %d\", msg[0])\n\t}\n\n\t// Ensure the length is sane\n\tif len(msg) < encryptedLength(vsn, 0) {\n\t\treturn nil, fmt.Errorf(\"Payload is too small to decrypt: %d\", len(msg))\n\t}\n\n\tfor _, key := range keys {\n\t\tplain, err := decryptMessage(key, msg, data)\n\t\tif err == nil {\n\t\t\t// Remove the PKCS7 padding for vsn 0\n\t\t\tif vsn == 0 {\n\t\t\t\treturn pkcs7decode(plain, aes.BlockSize), nil\n\t\t\t} else {\n\t\t\t\treturn plain, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil, fmt.Errorf(\"No installed keys could decrypt the message\")\n}\n\nfunc appendBytes(first []byte, second []byte) []byte {\n\thasFirst := len(first) > 0\n\thasSecond := len(second) > 0\n\n\tswitch {\n\tcase hasFirst && hasSecond:\n\t\tout := make([]byte, 0, len(first)+len(second))\n\t\tout = append(out, first...)\n\t\tout = append(out, second...)\n\t\treturn out\n\tcase hasFirst:\n\t\treturn first\n\tcase hasSecond:\n\t\treturn second\n\tdefault:\n\t\treturn nil\n\t}\n}\n"
        },
        {
          "name": "security_test.go",
          "type": "blob",
          "size": 1.5419921875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestPKCS7(t *testing.T) {\n\tfor i := 0; i <= 255; i++ {\n\t\t// Make a buffer of size i\n\t\tbuf := []byte{}\n\t\tfor j := 0; j < i; j++ {\n\t\t\tbuf = append(buf, byte(i))\n\t\t}\n\n\t\t// Copy to bytes buffer\n\t\tinp := bytes.NewBuffer(nil)\n\t\tinp.Write(buf)\n\n\t\t// Pad this out\n\t\tpkcs7encode(inp, 0, 16)\n\n\t\t// Unpad\n\t\tdec := pkcs7decode(inp.Bytes(), 16)\n\n\t\t// Ensure equivilence\n\t\tif !reflect.DeepEqual(buf, dec) {\n\t\t\tt.Fatalf(\"mismatch: %v %v\", buf, dec)\n\t\t}\n\t}\n\n}\n\nfunc TestEncryptDecrypt_V0(t *testing.T) {\n\tencryptDecryptVersioned(0, t)\n}\n\nfunc TestEncryptDecrypt_V1(t *testing.T) {\n\tencryptDecryptVersioned(1, t)\n}\n\nfunc encryptDecryptVersioned(vsn encryptionVersion, t *testing.T) {\n\tk1 := []byte{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n\tplaintext := []byte(\"this is a plain text message\")\n\textra := []byte(\"random data\")\n\n\tvar buf bytes.Buffer\n\terr := encryptPayload(vsn, k1, plaintext, extra, &buf)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\texpLen := encryptedLength(vsn, len(plaintext))\n\tif buf.Len() != expLen {\n\t\tt.Fatalf(\"output length is unexpected %d %d %d\", len(plaintext), buf.Len(), expLen)\n\t}\n\n\tmsg, err := decryptPayload([][]byte{k1}, buf.Bytes(), extra)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tcmp := bytes.Compare(msg, plaintext)\n\tif cmp != 0 {\n\t\tt.Errorf(\"len %d %v\", len(msg), msg)\n\t\tt.Errorf(\"len %d %v\", len(plaintext), plaintext)\n\t\tt.Fatalf(\"encrypt/decrypt failed! %d '%s' '%s'\", cmp, msg, plaintext)\n\t}\n}\n"
        },
        {
          "name": "state.go",
          "type": "blob",
          "size": 38.0234375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n)\n\ntype NodeStateType int\n\nfunc (t NodeStateType) metricsString() string {\n\tswitch t {\n\tcase StateAlive:\n\t\treturn \"alive\"\n\tcase StateDead:\n\t\treturn \"dead\"\n\tcase StateSuspect:\n\t\treturn \"suspect\"\n\tcase StateLeft:\n\t\treturn \"left\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"unhandled-value-%d\", t)\n\t}\n}\n\nconst (\n\tStateAlive NodeStateType = iota\n\tStateSuspect\n\tStateDead\n\tStateLeft\n)\n\n// Node represents a node in the cluster.\ntype Node struct {\n\tName  string\n\tAddr  net.IP\n\tPort  uint16\n\tMeta  []byte        // Metadata from the delegate for this node.\n\tState NodeStateType // State of the node.\n\tPMin  uint8         // Minimum protocol version this understands\n\tPMax  uint8         // Maximum protocol version this understands\n\tPCur  uint8         // Current version node is speaking\n\tDMin  uint8         // Min protocol version for the delegate to understand\n\tDMax  uint8         // Max protocol version for the delegate to understand\n\tDCur  uint8         // Current version delegate is speaking\n}\n\n// Address returns the host:port form of a node's address, suitable for use\n// with a transport.\nfunc (n *Node) Address() string {\n\treturn joinHostPort(n.Addr.String(), n.Port)\n}\n\n// FullAddress returns the node name and host:port form of a node's address,\n// suitable for use with a transport.\nfunc (n *Node) FullAddress() Address {\n\treturn Address{\n\t\tAddr: joinHostPort(n.Addr.String(), n.Port),\n\t\tName: n.Name,\n\t}\n}\n\n// String returns the node name\nfunc (n *Node) String() string {\n\treturn n.Name\n}\n\n// NodeState is used to manage our state view of another node\ntype nodeState struct {\n\tNode\n\tIncarnation uint32        // Last known incarnation number\n\tState       NodeStateType // Current state\n\tStateChange time.Time     // Time last state change happened\n}\n\n// Address returns the host:port form of a node's address, suitable for use\n// with a transport.\nfunc (n *nodeState) Address() string {\n\treturn n.Node.Address()\n}\n\n// FullAddress returns the node name and host:port form of a node's address,\n// suitable for use with a transport.\nfunc (n *nodeState) FullAddress() Address {\n\treturn n.Node.FullAddress()\n}\n\nfunc (n *nodeState) DeadOrLeft() bool {\n\treturn n.State == StateDead || n.State == StateLeft\n}\n\n// ackHandler is used to register handlers for incoming acks and nacks.\ntype ackHandler struct {\n\tackFn  func([]byte, time.Time)\n\tnackFn func()\n\ttimer  *time.Timer\n}\n\n// NoPingResponseError is used to indicate a 'ping' packet was\n// successfully issued but no response was received\ntype NoPingResponseError struct {\n\tnode string\n}\n\nfunc (f NoPingResponseError) Error() string {\n\treturn fmt.Sprintf(\"No response from node %s\", f.node)\n}\n\n// Schedule is used to ensure the Tick is performed periodically. This\n// function is safe to call multiple times. If the memberlist is already\n// scheduled, then it won't do anything.\nfunc (m *Memberlist) schedule() {\n\tm.tickerLock.Lock()\n\tdefer m.tickerLock.Unlock()\n\n\t// If we already have tickers, then don't do anything, since we're\n\t// scheduled\n\tif len(m.tickers) > 0 {\n\t\treturn\n\t}\n\n\t// Create the stop tick channel, a blocking channel. We close this\n\t// when we should stop the tickers.\n\tstopCh := make(chan struct{})\n\n\t// Create a new probeTicker\n\tif m.config.ProbeInterval > 0 {\n\t\tt := time.NewTicker(m.config.ProbeInterval)\n\t\tgo m.triggerFunc(m.config.ProbeInterval, t.C, stopCh, m.probe)\n\t\tm.tickers = append(m.tickers, t)\n\t}\n\n\t// Create a push pull ticker if needed\n\tif m.config.PushPullInterval > 0 {\n\t\tgo m.pushPullTrigger(stopCh)\n\t}\n\n\t// Create a gossip ticker if needed\n\tif m.config.GossipInterval > 0 && m.config.GossipNodes > 0 {\n\t\tt := time.NewTicker(m.config.GossipInterval)\n\t\tgo m.triggerFunc(m.config.GossipInterval, t.C, stopCh, m.gossip)\n\t\tm.tickers = append(m.tickers, t)\n\t}\n\n\t// If we made any tickers, then record the stopTick channel for\n\t// later.\n\tif len(m.tickers) > 0 {\n\t\tm.stopTick = stopCh\n\t}\n}\n\n// triggerFunc is used to trigger a function call each time a\n// message is received until a stop tick arrives.\nfunc (m *Memberlist) triggerFunc(stagger time.Duration, C <-chan time.Time, stop <-chan struct{}, f func()) {\n\t// Use a random stagger to avoid syncronizing\n\trandStagger := time.Duration(uint64(rand.Int63()) % uint64(stagger))\n\tselect {\n\tcase <-time.After(randStagger):\n\tcase <-stop:\n\t\treturn\n\t}\n\tfor {\n\t\tselect {\n\t\tcase <-C:\n\t\t\tf()\n\t\tcase <-stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// pushPullTrigger is used to periodically trigger a push/pull until\n// a stop tick arrives. We don't use triggerFunc since the push/pull\n// timer is dynamically scaled based on cluster size to avoid network\n// saturation\nfunc (m *Memberlist) pushPullTrigger(stop <-chan struct{}) {\n\tinterval := m.config.PushPullInterval\n\n\t// Use a random stagger to avoid syncronizing\n\trandStagger := time.Duration(uint64(rand.Int63()) % uint64(interval))\n\tselect {\n\tcase <-time.After(randStagger):\n\tcase <-stop:\n\t\treturn\n\t}\n\n\t// Tick using a dynamic timer\n\tfor {\n\t\ttickTime := pushPullScale(interval, m.estNumNodes())\n\t\tselect {\n\t\tcase <-time.After(tickTime):\n\t\t\tm.pushPull()\n\t\tcase <-stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Deschedule is used to stop the background maintenance. This is safe\n// to call multiple times.\nfunc (m *Memberlist) deschedule() {\n\tm.tickerLock.Lock()\n\tdefer m.tickerLock.Unlock()\n\n\t// If we have no tickers, then we aren't scheduled.\n\tif len(m.tickers) == 0 {\n\t\treturn\n\t}\n\n\t// Close the stop channel so all the ticker listeners stop.\n\tclose(m.stopTick)\n\n\t// Explicitly stop all the tickers themselves so they don't take\n\t// up any more resources, and get rid of the list.\n\tfor _, t := range m.tickers {\n\t\tt.Stop()\n\t}\n\tm.tickers = nil\n}\n\n// Tick is used to perform a single round of failure detection and gossip\nfunc (m *Memberlist) probe() {\n\t// Track the number of indexes we've considered probing\n\tnumCheck := 0\nSTART:\n\tm.nodeLock.RLock()\n\n\t// Make sure we don't wrap around infinitely\n\tif numCheck >= len(m.nodes) {\n\t\tm.nodeLock.RUnlock()\n\t\treturn\n\t}\n\n\t// Handle the wrap around case\n\tif m.probeIndex >= len(m.nodes) {\n\t\tm.nodeLock.RUnlock()\n\t\tm.resetNodes()\n\t\tm.probeIndex = 0\n\t\tnumCheck++\n\t\tgoto START\n\t}\n\n\t// Determine if we should probe this node\n\tskip := false\n\tvar node nodeState\n\n\tnode = *m.nodes[m.probeIndex]\n\tif node.Name == m.config.Name {\n\t\tskip = true\n\t} else if node.DeadOrLeft() {\n\t\tskip = true\n\t}\n\n\t// Potentially skip\n\tm.nodeLock.RUnlock()\n\tm.probeIndex++\n\tif skip {\n\t\tnumCheck++\n\t\tgoto START\n\t}\n\n\t// Probe the specific node\n\tm.probeNode(&node)\n}\n\n// probeNodeByAddr just safely calls probeNode given only the address of the node (for tests)\nfunc (m *Memberlist) probeNodeByAddr(addr string) {\n\tm.nodeLock.RLock()\n\tn := m.nodeMap[addr]\n\tm.nodeLock.RUnlock()\n\n\tm.probeNode(n)\n}\n\n// failedRemote checks the error and decides if it indicates a failure on the\n// other end.\nfunc failedRemote(err error) bool {\n\tswitch t := err.(type) {\n\tcase *net.OpError:\n\t\tif strings.HasPrefix(t.Net, \"tcp\") {\n\t\t\tswitch t.Op {\n\t\t\tcase \"dial\", \"read\", \"write\":\n\t\t\t\treturn true\n\t\t\t}\n\t\t} else if strings.HasPrefix(t.Net, \"udp\") {\n\t\t\tswitch t.Op {\n\t\t\tcase \"write\":\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// probeNode handles a single round of failure checking on a node.\nfunc (m *Memberlist) probeNode(node *nodeState) {\n\tdefer metrics.MeasureSinceWithLabels([]string{\"memberlist\", \"probeNode\"}, time.Now(), m.metricLabels)\n\n\t// We use our health awareness to scale the overall probe interval, so we\n\t// slow down if we detect problems. The ticker that calls us can handle\n\t// us running over the base interval, and will skip missed ticks.\n\tprobeInterval := m.awareness.ScaleTimeout(m.config.ProbeInterval)\n\tif probeInterval > m.config.ProbeInterval {\n\t\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"degraded\", \"probe\"}, 1, m.metricLabels)\n\t}\n\n\t// Prepare a ping message and setup an ack handler.\n\tselfAddr, selfPort := m.getAdvertise()\n\tping := ping{\n\t\tSeqNo:      m.nextSeqNo(),\n\t\tNode:       node.Name,\n\t\tSourceAddr: selfAddr,\n\t\tSourcePort: selfPort,\n\t\tSourceNode: m.config.Name,\n\t}\n\tackCh := make(chan ackMessage, m.config.IndirectChecks+1)\n\tnackCh := make(chan struct{}, m.config.IndirectChecks+1)\n\tm.setProbeChannels(ping.SeqNo, ackCh, nackCh, probeInterval)\n\n\t// Mark the sent time here, which should be after any pre-processing but\n\t// before system calls to do the actual send. This probably over-reports\n\t// a bit, but it's the best we can do. We had originally put this right\n\t// after the I/O, but that would sometimes give negative RTT measurements\n\t// which was not desirable.\n\tsent := time.Now()\n\n\t// Send a ping to the node. If this node looks like it's suspect or dead,\n\t// also tack on a suspect message so that it has a chance to refute as\n\t// soon as possible.\n\tdeadline := sent.Add(probeInterval)\n\taddr := node.Address()\n\n\t// Arrange for our self-awareness to get updated.\n\tvar awarenessDelta int\n\tdefer func() {\n\t\tm.awareness.ApplyDelta(awarenessDelta)\n\t}()\n\tif node.State == StateAlive {\n\t\tif err := m.encodeAndSendMsg(node.FullAddress(), pingMsg, &ping); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send UDP ping: %s\", err)\n\t\t\tif failedRemote(err) {\n\t\t\t\tgoto HANDLE_REMOTE_FAILURE\n\t\t\t} else {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t} else {\n\t\tvar msgs [][]byte\n\t\tif buf, err := encode(pingMsg, &ping, m.config.MsgpackUseNewTimeFormat); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to encode UDP ping message: %s\", err)\n\t\t\treturn\n\t\t} else {\n\t\t\tmsgs = append(msgs, buf.Bytes())\n\t\t}\n\t\ts := suspect{Incarnation: node.Incarnation, Node: node.Name, From: m.config.Name}\n\t\tif buf, err := encode(suspectMsg, &s, m.config.MsgpackUseNewTimeFormat); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to encode suspect message: %s\", err)\n\t\t\treturn\n\t\t} else {\n\t\t\tmsgs = append(msgs, buf.Bytes())\n\t\t}\n\n\t\tcompound := makeCompoundMessage(msgs)\n\t\tif err := m.rawSendMsgPacket(node.FullAddress(), &node.Node, compound.Bytes()); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send UDP compound ping and suspect message to %s: %s\", addr, err)\n\t\t\tif failedRemote(err) {\n\t\t\t\tgoto HANDLE_REMOTE_FAILURE\n\t\t\t} else {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Arrange for our self-awareness to get updated. At this point we've\n\t// sent the ping, so any return statement means the probe succeeded\n\t// which will improve our health until we get to the failure scenarios\n\t// at the end of this function, which will alter this delta variable\n\t// accordingly.\n\tawarenessDelta = -1\n\n\t// Wait for response or round-trip-time.\n\tselect {\n\tcase v := <-ackCh:\n\t\tif v.Complete == true {\n\t\t\tif m.config.Ping != nil {\n\t\t\t\trtt := v.Timestamp.Sub(sent)\n\t\t\t\tm.config.Ping.NotifyPingComplete(&node.Node, rtt, v.Payload)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// As an edge case, if we get a timeout, we need to re-enqueue it\n\t\t// here to break out of the select below.\n\t\tif v.Complete == false {\n\t\t\tackCh <- v\n\t\t}\n\tcase <-time.After(m.config.ProbeTimeout):\n\t\t// Note that we don't scale this timeout based on awareness and\n\t\t// the health score. That's because we don't really expect waiting\n\t\t// longer to help get UDP through. Since health does extend the\n\t\t// probe interval it will give the TCP fallback more time, which\n\t\t// is more active in dealing with lost packets, and it gives more\n\t\t// time to wait for indirect acks/nacks.\n\t\tm.logger.Printf(\"[DEBUG] memberlist: Failed UDP ping: %s (timeout reached)\", node.Name)\n\t}\n\nHANDLE_REMOTE_FAILURE:\n\t// Get some random live nodes.\n\tm.nodeLock.RLock()\n\tkNodes := kRandomNodes(m.config.IndirectChecks, m.nodes, func(n *nodeState) bool {\n\t\treturn n.Name == m.config.Name ||\n\t\t\tn.Name == node.Name ||\n\t\t\tn.State != StateAlive\n\t})\n\tm.nodeLock.RUnlock()\n\n\t// Attempt an indirect ping.\n\texpectedNacks := 0\n\tselfAddr, selfPort = m.getAdvertise()\n\tind := indirectPingReq{\n\t\tSeqNo:      ping.SeqNo,\n\t\tTarget:     node.Addr,\n\t\tPort:       node.Port,\n\t\tNode:       node.Name,\n\t\tSourceAddr: selfAddr,\n\t\tSourcePort: selfPort,\n\t\tSourceNode: m.config.Name,\n\t}\n\tfor _, peer := range kNodes {\n\t\t// We only expect nack to be sent from peers who understand\n\t\t// version 4 of the protocol.\n\t\tif ind.Nack = peer.PMax >= 4; ind.Nack {\n\t\t\texpectedNacks++\n\t\t}\n\n\t\tif err := m.encodeAndSendMsg(peer.FullAddress(), indirectPingMsg, &ind); err != nil {\n\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send indirect UDP ping: %s\", err)\n\t\t}\n\t}\n\n\t// Also make an attempt to contact the node directly over TCP. This\n\t// helps prevent confused clients who get isolated from UDP traffic\n\t// but can still speak TCP (which also means they can possibly report\n\t// misinformation to other nodes via anti-entropy), avoiding flapping in\n\t// the cluster.\n\t//\n\t// This is a little unusual because we will attempt a TCP ping to any\n\t// member who understands version 3 of the protocol, regardless of\n\t// which protocol version we are speaking. That's why we've included a\n\t// config option to turn this off if desired.\n\tfallbackCh := make(chan bool, 1)\n\n\tdisableTcpPings := m.config.DisableTcpPings ||\n\t\t(m.config.DisableTcpPingsForNode != nil && m.config.DisableTcpPingsForNode(node.Name))\n\tif (!disableTcpPings) && (node.PMax >= 3) {\n\t\tgo func() {\n\t\t\tdefer close(fallbackCh)\n\t\t\tdidContact, err := m.sendPingAndWaitForAck(node.FullAddress(), ping, deadline)\n\t\t\tif err != nil {\n\t\t\t\tvar to string\n\t\t\t\tif ne, ok := err.(net.Error); ok && ne.Timeout() {\n\t\t\t\t\tto = fmt.Sprintf(\"timeout %s: \", probeInterval)\n\t\t\t\t}\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed fallback TCP ping: %s%s\", to, err)\n\t\t\t} else {\n\t\t\t\tfallbackCh <- didContact\n\t\t\t}\n\t\t}()\n\t} else {\n\t\tclose(fallbackCh)\n\t}\n\n\t// Wait for the acks or timeout. Note that we don't check the fallback\n\t// channel here because we want to issue a warning below if that's the\n\t// *only* way we hear back from the peer, so we have to let this time\n\t// out first to allow the normal UDP-based acks to come in.\n\tselect {\n\tcase v := <-ackCh:\n\t\tif v.Complete == true {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Finally, poll the fallback channel. The timeouts are set such that\n\t// the channel will have something or be closed without having to wait\n\t// any additional time here.\n\tfor didContact := range fallbackCh {\n\t\tif didContact {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Was able to connect to %s over TCP but UDP probes failed, network may be misconfigured\", node.Name)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Update our self-awareness based on the results of this failed probe.\n\t// If we don't have peers who will send nacks then we penalize for any\n\t// failed probe as a simple health metric. If we do have peers to nack\n\t// verify, then we can use that as a more sophisticated measure of self-\n\t// health because we assume them to be working, and they can help us\n\t// decide if the probed node was really dead or if it was something wrong\n\t// with ourselves.\n\tawarenessDelta = 0\n\tif expectedNacks > 0 {\n\t\tif nackCount := len(nackCh); nackCount < expectedNacks {\n\t\t\tawarenessDelta += (expectedNacks - nackCount)\n\t\t}\n\t} else {\n\t\tawarenessDelta += 1\n\t}\n\n\t// No acks received from target, suspect it as failed.\n\tm.logger.Printf(\"[INFO] memberlist: Suspect %s has failed, no acks received\", node.Name)\n\ts := suspect{Incarnation: node.Incarnation, Node: node.Name, From: m.config.Name}\n\tm.suspectNode(&s)\n}\n\n// Ping initiates a ping to the node with the specified name.\nfunc (m *Memberlist) Ping(node string, addr net.Addr) (time.Duration, error) {\n\t// Prepare a ping message and setup an ack handler.\n\tselfAddr, selfPort := m.getAdvertise()\n\tping := ping{\n\t\tSeqNo:      m.nextSeqNo(),\n\t\tNode:       node,\n\t\tSourceAddr: selfAddr,\n\t\tSourcePort: selfPort,\n\t\tSourceNode: m.config.Name,\n\t}\n\tackCh := make(chan ackMessage, m.config.IndirectChecks+1)\n\tm.setProbeChannels(ping.SeqNo, ackCh, nil, m.config.ProbeInterval)\n\n\ta := Address{Addr: addr.String(), Name: node}\n\n\t// Send a ping to the node.\n\tif err := m.encodeAndSendMsg(a, pingMsg, &ping); err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Mark the sent time here, which should be after any pre-processing and\n\t// system calls to do the actual send. This probably under-reports a bit,\n\t// but it's the best we can do.\n\tsent := time.Now()\n\n\t// Wait for response or timeout.\n\tselect {\n\tcase v := <-ackCh:\n\t\tif v.Complete == true {\n\t\t\treturn v.Timestamp.Sub(sent), nil\n\t\t}\n\tcase <-time.After(m.config.ProbeTimeout):\n\t\t// Timeout, return an error below.\n\t}\n\n\tm.logger.Printf(\"[DEBUG] memberlist: Failed UDP ping: %v (timeout reached)\", node)\n\treturn 0, NoPingResponseError{ping.Node}\n}\n\n// resetNodes is used when the tick wraps around. It will reap the\n// dead nodes and shuffle the node list.\nfunc (m *Memberlist) resetNodes() {\n\tm.nodeLock.Lock()\n\tdefer m.nodeLock.Unlock()\n\n\t// Move dead nodes, but respect gossip to the dead interval\n\tdeadIdx := moveDeadNodes(m.nodes, m.config.GossipToTheDeadTime)\n\n\t// Deregister the dead nodes\n\tfor i := deadIdx; i < len(m.nodes); i++ {\n\t\tdelete(m.nodeMap, m.nodes[i].Name)\n\t\tm.nodes[i] = nil\n\t}\n\n\t// Trim the nodes to exclude the dead nodes\n\tm.nodes = m.nodes[0:deadIdx]\n\n\t// Update numNodes after we've trimmed the dead nodes\n\tatomic.StoreUint32(&m.numNodes, uint32(deadIdx))\n\n\t// Shuffle live nodes\n\tshuffleNodes(m.nodes)\n}\n\n// gossip is invoked every GossipInterval period to broadcast our gossip\n// messages to a few random nodes.\nfunc (m *Memberlist) gossip() {\n\tdefer metrics.MeasureSinceWithLabels([]string{\"memberlist\", \"gossip\"}, time.Now(), m.metricLabels)\n\n\t// Get some random live, suspect, or recently dead nodes\n\tm.nodeLock.RLock()\n\tkNodes := kRandomNodes(m.config.GossipNodes, m.nodes, func(n *nodeState) bool {\n\t\tif n.Name == m.config.Name {\n\t\t\treturn true\n\t\t}\n\n\t\tswitch n.State {\n\t\tcase StateAlive, StateSuspect:\n\t\t\treturn false\n\n\t\tcase StateDead:\n\t\t\treturn time.Since(n.StateChange) > m.config.GossipToTheDeadTime\n\n\t\tdefault:\n\t\t\treturn true\n\t\t}\n\t})\n\tm.nodeLock.RUnlock()\n\n\t// Compute the bytes available\n\tbytesAvail := m.config.UDPBufferSize - compoundHeaderOverhead - labelOverhead(m.config.Label)\n\tif m.config.EncryptionEnabled() {\n\t\tbytesAvail -= encryptOverhead(m.encryptionVersion())\n\t}\n\n\tfor _, node := range kNodes {\n\t\t// Get any pending broadcasts\n\t\tmsgs := m.getBroadcasts(compoundOverhead, bytesAvail)\n\t\tif len(msgs) == 0 {\n\t\t\treturn\n\t\t}\n\n\t\taddr := node.Address()\n\t\tif len(msgs) == 1 {\n\t\t\t// Send single message as is\n\t\t\tif err := m.rawSendMsgPacket(node.FullAddress(), &node, msgs[0]); err != nil {\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send gossip to %s: %s\", addr, err)\n\t\t\t}\n\t\t} else {\n\t\t\t// Otherwise create and send one or more compound messages\n\t\t\tcompounds := makeCompoundMessages(msgs)\n\t\t\tfor _, compound := range compounds {\n\t\t\t\tif err := m.rawSendMsgPacket(node.FullAddress(), &node, compound.Bytes()); err != nil {\n\t\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Failed to send gossip to %s: %s\", addr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// pushPull is invoked periodically to randomly perform a complete state\n// exchange. Used to ensure a high level of convergence, but is also\n// reasonably expensive as the entire state of this node is exchanged\n// with the other node.\nfunc (m *Memberlist) pushPull() {\n\t// Get a random live node\n\tm.nodeLock.RLock()\n\tnodes := kRandomNodes(1, m.nodes, func(n *nodeState) bool {\n\t\treturn n.Name == m.config.Name ||\n\t\t\tn.State != StateAlive\n\t})\n\tm.nodeLock.RUnlock()\n\n\t// If no nodes, bail\n\tif len(nodes) == 0 {\n\t\treturn\n\t}\n\tnode := nodes[0]\n\n\t// Attempt a push pull\n\tif err := m.pushPullNode(node.FullAddress(), false); err != nil {\n\t\tm.logger.Printf(\"[ERR] memberlist: Push/Pull with %s failed: %s\", node.Name, err)\n\t}\n}\n\n// pushPullNode does a complete state exchange with a specific node.\nfunc (m *Memberlist) pushPullNode(a Address, join bool) error {\n\tdefer metrics.MeasureSinceWithLabels([]string{\"memberlist\", \"pushPullNode\"}, time.Now(), m.metricLabels)\n\n\t// Attempt to send and receive with the node\n\tremote, userState, err := m.sendAndReceiveState(a, join)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := m.mergeRemoteState(join, remote, userState); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// verifyProtocol verifies that all the remote nodes can speak with our\n// nodes and vice versa on both the core protocol as well as the\n// delegate protocol level.\n//\n// The verification works by finding the maximum minimum and\n// minimum maximum understood protocol and delegate versions. In other words,\n// it finds the common denominator of protocol and delegate version ranges\n// for the entire cluster.\n//\n// After this, it goes through the entire cluster (local and remote) and\n// verifies that everyone's speaking protocol versions satisfy this range.\n// If this passes, it means that every node can understand each other.\nfunc (m *Memberlist) verifyProtocol(remote []pushNodeState) error {\n\tm.nodeLock.RLock()\n\tdefer m.nodeLock.RUnlock()\n\n\t// Maximum minimum understood and minimum maximum understood for both\n\t// the protocol and delegate versions. We use this to verify everyone\n\t// can be understood.\n\tvar maxpmin, minpmax uint8\n\tvar maxdmin, mindmax uint8\n\tminpmax = math.MaxUint8\n\tmindmax = math.MaxUint8\n\n\tfor _, rn := range remote {\n\t\t// If the node isn't alive, then skip it\n\t\tif rn.State != StateAlive {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Skip nodes that don't have versions set, it just means\n\t\t// their version is zero.\n\t\tif len(rn.Vsn) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif rn.Vsn[0] > maxpmin {\n\t\t\tmaxpmin = rn.Vsn[0]\n\t\t}\n\n\t\tif rn.Vsn[1] < minpmax {\n\t\t\tminpmax = rn.Vsn[1]\n\t\t}\n\n\t\tif rn.Vsn[3] > maxdmin {\n\t\t\tmaxdmin = rn.Vsn[3]\n\t\t}\n\n\t\tif rn.Vsn[4] < mindmax {\n\t\t\tmindmax = rn.Vsn[4]\n\t\t}\n\t}\n\n\tfor _, n := range m.nodes {\n\t\t// Ignore non-alive nodes\n\t\tif n.State != StateAlive {\n\t\t\tcontinue\n\t\t}\n\n\t\tif n.PMin > maxpmin {\n\t\t\tmaxpmin = n.PMin\n\t\t}\n\n\t\tif n.PMax < minpmax {\n\t\t\tminpmax = n.PMax\n\t\t}\n\n\t\tif n.DMin > maxdmin {\n\t\t\tmaxdmin = n.DMin\n\t\t}\n\n\t\tif n.DMax < mindmax {\n\t\t\tmindmax = n.DMax\n\t\t}\n\t}\n\n\t// Now that we definitively know the minimum and maximum understood\n\t// version that satisfies the whole cluster, we verify that every\n\t// node in the cluster satisifies this.\n\tfor _, n := range remote {\n\t\tvar nPCur, nDCur uint8\n\t\tif len(n.Vsn) > 0 {\n\t\t\tnPCur = n.Vsn[2]\n\t\t\tnDCur = n.Vsn[5]\n\t\t}\n\n\t\tif nPCur < maxpmin || nPCur > minpmax {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"Node '%s' protocol version (%d) is incompatible: [%d, %d]\",\n\t\t\t\tn.Name, nPCur, maxpmin, minpmax)\n\t\t}\n\n\t\tif nDCur < maxdmin || nDCur > mindmax {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"Node '%s' delegate protocol version (%d) is incompatible: [%d, %d]\",\n\t\t\t\tn.Name, nDCur, maxdmin, mindmax)\n\t\t}\n\t}\n\n\tfor _, n := range m.nodes {\n\t\tnPCur := n.PCur\n\t\tnDCur := n.DCur\n\n\t\tif nPCur < maxpmin || nPCur > minpmax {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"Node '%s' protocol version (%d) is incompatible: [%d, %d]\",\n\t\t\t\tn.Name, nPCur, maxpmin, minpmax)\n\t\t}\n\n\t\tif nDCur < maxdmin || nDCur > mindmax {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"Node '%s' delegate protocol version (%d) is incompatible: [%d, %d]\",\n\t\t\t\tn.Name, nDCur, maxdmin, mindmax)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// nextSeqNo returns a usable sequence number in a thread safe way\nfunc (m *Memberlist) nextSeqNo() uint32 {\n\treturn atomic.AddUint32(&m.sequenceNum, 1)\n}\n\n// nextIncarnation returns the next incarnation number in a thread safe way\nfunc (m *Memberlist) nextIncarnation() uint32 {\n\treturn atomic.AddUint32(&m.incarnation, 1)\n}\n\n// skipIncarnation adds the positive offset to the incarnation number.\nfunc (m *Memberlist) skipIncarnation(offset uint32) uint32 {\n\treturn atomic.AddUint32(&m.incarnation, offset)\n}\n\n// estNumNodes is used to get the current estimate of the number of nodes\nfunc (m *Memberlist) estNumNodes() int {\n\treturn int(atomic.LoadUint32(&m.numNodes))\n}\n\ntype ackMessage struct {\n\tComplete  bool\n\tPayload   []byte\n\tTimestamp time.Time\n}\n\n// setProbeChannels is used to attach the ackCh to receive a message when an ack\n// with a given sequence number is received. The `complete` field of the message\n// will be false on timeout. Any nack messages will cause an empty struct to be\n// passed to the nackCh, which can be nil if not needed.\nfunc (m *Memberlist) setProbeChannels(seqNo uint32, ackCh chan ackMessage, nackCh chan struct{}, timeout time.Duration) {\n\t// Create handler functions for acks and nacks\n\tackFn := func(payload []byte, timestamp time.Time) {\n\t\tselect {\n\t\tcase ackCh <- ackMessage{true, payload, timestamp}:\n\t\tdefault:\n\t\t}\n\t}\n\tnackFn := func() {\n\t\tselect {\n\t\tcase nackCh <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n\n\t// Add the handlers\n\tah := &ackHandler{ackFn, nackFn, nil}\n\tm.ackLock.Lock()\n\tm.ackHandlers[seqNo] = ah\n\tm.ackLock.Unlock()\n\n\t// Setup a reaping routing\n\tah.timer = time.AfterFunc(timeout, func() {\n\t\tm.ackLock.Lock()\n\t\tdelete(m.ackHandlers, seqNo)\n\t\tm.ackLock.Unlock()\n\t\tselect {\n\t\tcase ackCh <- ackMessage{false, nil, time.Now()}:\n\t\tdefault:\n\t\t}\n\t})\n}\n\n// setAckHandler is used to attach a handler to be invoked when an ack with a\n// given sequence number is received. If a timeout is reached, the handler is\n// deleted. This is used for indirect pings so does not configure a function\n// for nacks.\nfunc (m *Memberlist) setAckHandler(seqNo uint32, ackFn func([]byte, time.Time), timeout time.Duration) {\n\t// Add the handler\n\tah := &ackHandler{ackFn, nil, nil}\n\tm.ackLock.Lock()\n\tm.ackHandlers[seqNo] = ah\n\tm.ackLock.Unlock()\n\n\t// Setup a reaping routing\n\tah.timer = time.AfterFunc(timeout, func() {\n\t\tm.ackLock.Lock()\n\t\tdelete(m.ackHandlers, seqNo)\n\t\tm.ackLock.Unlock()\n\t})\n}\n\n// Invokes an ack handler if any is associated, and reaps the handler immediately\nfunc (m *Memberlist) invokeAckHandler(ack ackResp, timestamp time.Time) {\n\tm.ackLock.Lock()\n\tah, ok := m.ackHandlers[ack.SeqNo]\n\tdelete(m.ackHandlers, ack.SeqNo)\n\tm.ackLock.Unlock()\n\tif !ok {\n\t\treturn\n\t}\n\tah.timer.Stop()\n\tah.ackFn(ack.Payload, timestamp)\n}\n\n// Invokes nack handler if any is associated.\nfunc (m *Memberlist) invokeNackHandler(nack nackResp) {\n\tm.ackLock.Lock()\n\tah, ok := m.ackHandlers[nack.SeqNo]\n\tm.ackLock.Unlock()\n\tif !ok || ah.nackFn == nil {\n\t\treturn\n\t}\n\tah.nackFn()\n}\n\n// refute gossips an alive message in response to incoming information that we\n// are suspect or dead. It will make sure the incarnation number beats the given\n// accusedInc value, or you can supply 0 to just get the next incarnation number.\n// This alters the node state that's passed in so this MUST be called while the\n// nodeLock is held.\nfunc (m *Memberlist) refute(me *nodeState, accusedInc uint32) {\n\t// Make sure the incarnation number beats the accusation.\n\tinc := m.nextIncarnation()\n\tif accusedInc >= inc {\n\t\tinc = m.skipIncarnation(accusedInc - inc + 1)\n\t}\n\tme.Incarnation = inc\n\n\t// Decrease our health because we are being asked to refute a problem.\n\tm.awareness.ApplyDelta(1)\n\n\t// Format and broadcast an alive message.\n\ta := alive{\n\t\tIncarnation: inc,\n\t\tNode:        me.Name,\n\t\tAddr:        me.Addr,\n\t\tPort:        me.Port,\n\t\tMeta:        me.Meta,\n\t\tVsn: []uint8{\n\t\t\tme.PMin, me.PMax, me.PCur,\n\t\t\tme.DMin, me.DMax, me.DCur,\n\t\t},\n\t}\n\tm.encodeAndBroadcast(me.Addr.String(), aliveMsg, a)\n}\n\n// aliveNode is invoked by the network layer when we get a message about a\n// live node.\nfunc (m *Memberlist) aliveNode(a *alive, notify chan struct{}, bootstrap bool) {\n\tm.nodeLock.Lock()\n\tdefer m.nodeLock.Unlock()\n\tstate, ok := m.nodeMap[a.Node]\n\n\t// It is possible that during a Leave(), there is already an aliveMsg\n\t// in-queue to be processed but blocked by the locks above. If we let\n\t// that aliveMsg process, it'll cause us to re-join the cluster. This\n\t// ensures that we don't.\n\tif m.hasLeft() && a.Node == m.config.Name {\n\t\treturn\n\t}\n\n\tif len(a.Vsn) >= 3 {\n\t\tpMin := a.Vsn[0]\n\t\tpMax := a.Vsn[1]\n\t\tpCur := a.Vsn[2]\n\t\tif pMin == 0 || pMax == 0 || pMin > pMax {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Ignoring an alive message for '%s' (%v:%d) because protocol version(s) are wrong: %d <= %d <= %d should be >0\", a.Node, net.IP(a.Addr), a.Port, pMin, pCur, pMax)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Invoke the Alive delegate if any. This can be used to filter out\n\t// alive messages based on custom logic. For example, using a cluster name.\n\t// Using a merge delegate is not enough, as it is possible for passive\n\t// cluster merging to still occur.\n\tif m.config.Alive != nil {\n\t\tif len(a.Vsn) < 6 {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: ignoring alive message for '%s' (%v:%d) because Vsn is not present\",\n\t\t\t\ta.Node, net.IP(a.Addr), a.Port)\n\t\t\treturn\n\t\t}\n\t\tnode := &Node{\n\t\t\tName: a.Node,\n\t\t\tAddr: a.Addr,\n\t\t\tPort: a.Port,\n\t\t\tMeta: a.Meta,\n\t\t\tPMin: a.Vsn[0],\n\t\t\tPMax: a.Vsn[1],\n\t\t\tPCur: a.Vsn[2],\n\t\t\tDMin: a.Vsn[3],\n\t\t\tDMax: a.Vsn[4],\n\t\t\tDCur: a.Vsn[5],\n\t\t}\n\t\tif err := m.config.Alive.NotifyAlive(node); err != nil {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: ignoring alive message for '%s': %s\",\n\t\t\t\ta.Node, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Check if we've never seen this node before, and if not, then\n\t// store this node in our node map.\n\tvar updatesNode bool\n\tif !ok {\n\t\terrCon := m.config.IPAllowed(a.Addr)\n\t\tif errCon != nil {\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Rejected node %s (%v): %s\", a.Node, net.IP(a.Addr), errCon)\n\t\t\treturn\n\t\t}\n\t\tstate = &nodeState{\n\t\t\tNode: Node{\n\t\t\t\tName: a.Node,\n\t\t\t\tAddr: a.Addr,\n\t\t\t\tPort: a.Port,\n\t\t\t\tMeta: a.Meta,\n\t\t\t},\n\t\t\tState: StateDead,\n\t\t}\n\t\tif len(a.Vsn) > 5 {\n\t\t\tstate.PMin = a.Vsn[0]\n\t\t\tstate.PMax = a.Vsn[1]\n\t\t\tstate.PCur = a.Vsn[2]\n\t\t\tstate.DMin = a.Vsn[3]\n\t\t\tstate.DMax = a.Vsn[4]\n\t\t\tstate.DCur = a.Vsn[5]\n\t\t}\n\n\t\t// Add to map\n\t\tm.nodeMap[a.Node] = state\n\n\t\t// Get a random offset. This is important to ensure\n\t\t// the failure detection bound is low on average. If all\n\t\t// nodes did an append, failure detection bound would be\n\t\t// very high.\n\t\tn := len(m.nodes)\n\t\toffset := randomOffset(n)\n\n\t\t// Add at the end and swap with the node at the offset\n\t\tm.nodes = append(m.nodes, state)\n\t\tm.nodes[offset], m.nodes[n] = m.nodes[n], m.nodes[offset]\n\n\t\t// Update numNodes after we've added a new node\n\t\tatomic.AddUint32(&m.numNodes, 1)\n\t} else {\n\t\t// Check if this address is different than the existing node unless the old node is dead.\n\t\tif !bytes.Equal([]byte(state.Addr), a.Addr) || state.Port != a.Port {\n\t\t\terrCon := m.config.IPAllowed(a.Addr)\n\t\t\tif errCon != nil {\n\t\t\t\tm.logger.Printf(\"[WARN] memberlist: Rejected IP update from %v to %v for node %s: %s\", a.Node, state.Addr, net.IP(a.Addr), errCon)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If DeadNodeReclaimTime is configured, check if enough time has elapsed since the node died.\n\t\t\tcanReclaim := (m.config.DeadNodeReclaimTime > 0 &&\n\t\t\t\ttime.Since(state.StateChange) > m.config.DeadNodeReclaimTime)\n\n\t\t\t// Allow the address to be updated if a dead node is being replaced.\n\t\t\tif state.State == StateLeft || (state.State == StateDead && canReclaim) {\n\t\t\t\tm.logger.Printf(\"[INFO] memberlist: Updating address for left or failed node %s from %v:%d to %v:%d\",\n\t\t\t\t\tstate.Name, state.Addr, state.Port, net.IP(a.Addr), a.Port)\n\t\t\t\tupdatesNode = true\n\t\t\t} else {\n\t\t\t\tm.logger.Printf(\"[ERR] memberlist: Conflicting address for %s. Mine: %v:%d Theirs: %v:%d Old state: %v\",\n\t\t\t\t\tstate.Name, state.Addr, state.Port, net.IP(a.Addr), a.Port, state.State)\n\n\t\t\t\t// Inform the conflict delegate if provided\n\t\t\t\tif m.config.Conflict != nil {\n\t\t\t\t\tother := Node{\n\t\t\t\t\t\tName: a.Node,\n\t\t\t\t\t\tAddr: a.Addr,\n\t\t\t\t\t\tPort: a.Port,\n\t\t\t\t\t\tMeta: a.Meta,\n\t\t\t\t\t}\n\t\t\t\t\tm.config.Conflict.NotifyConflict(&state.Node, &other)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Bail if the incarnation number is older, and this is not about us\n\tisLocalNode := state.Name == m.config.Name\n\tif a.Incarnation <= state.Incarnation && !isLocalNode && !updatesNode {\n\t\treturn\n\t}\n\n\t// Bail if strictly less and this is about us\n\tif a.Incarnation < state.Incarnation && isLocalNode {\n\t\treturn\n\t}\n\n\t// Clear out any suspicion timer that may be in effect.\n\tdelete(m.nodeTimers, a.Node)\n\n\t// Store the old state and meta data\n\toldState := state.State\n\toldMeta := state.Meta\n\n\t// If this is us we need to refute, otherwise re-broadcast\n\tif !bootstrap && isLocalNode {\n\t\t// Compute the version vector\n\t\tversions := []uint8{\n\t\t\tstate.PMin, state.PMax, state.PCur,\n\t\t\tstate.DMin, state.DMax, state.DCur,\n\t\t}\n\n\t\t// If the Incarnation is the same, we need special handling, since it\n\t\t// possible for the following situation to happen:\n\t\t// 1) Start with configuration C, join cluster\n\t\t// 2) Hard fail / Kill / Shutdown\n\t\t// 3) Restart with configuration C', join cluster\n\t\t//\n\t\t// In this case, other nodes and the local node see the same incarnation,\n\t\t// but the values may not be the same. For this reason, we always\n\t\t// need to do an equality check for this Incarnation. In most cases,\n\t\t// we just ignore, but we may need to refute.\n\t\t//\n\t\tif a.Incarnation == state.Incarnation &&\n\t\t\tbytes.Equal(a.Meta, state.Meta) &&\n\t\t\tbytes.Equal(a.Vsn, versions) {\n\t\t\treturn\n\t\t}\n\t\tm.refute(state, a.Incarnation)\n\t\tm.logger.Printf(\"[WARN] memberlist: Refuting an alive message for '%s' (%v:%d) meta:(%v VS %v), vsn:(%v VS %v)\", a.Node, net.IP(a.Addr), a.Port, a.Meta, state.Meta, a.Vsn, versions)\n\t} else {\n\t\tm.encodeBroadcastNotify(a.Node, aliveMsg, a, notify)\n\n\t\t// Update protocol versions if it arrived\n\t\tif len(a.Vsn) > 0 {\n\t\t\tstate.PMin = a.Vsn[0]\n\t\t\tstate.PMax = a.Vsn[1]\n\t\t\tstate.PCur = a.Vsn[2]\n\t\t\tstate.DMin = a.Vsn[3]\n\t\t\tstate.DMax = a.Vsn[4]\n\t\t\tstate.DCur = a.Vsn[5]\n\t\t}\n\n\t\t// Update the state and incarnation number\n\t\tstate.Incarnation = a.Incarnation\n\t\tstate.Meta = a.Meta\n\t\tstate.Addr = a.Addr\n\t\tstate.Port = a.Port\n\t\tif state.State != StateAlive {\n\t\t\tstate.State = StateAlive\n\t\t\tstate.StateChange = time.Now()\n\t\t}\n\t}\n\n\t// Update metrics\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"msg\", \"alive\"}, 1, m.metricLabels)\n\n\t// Notify the delegate of any relevant updates\n\tif m.config.Events != nil {\n\t\tif oldState == StateDead || oldState == StateLeft {\n\t\t\t// if Dead/Left -> Alive, notify of join\n\t\t\tm.config.Events.NotifyJoin(&state.Node)\n\n\t\t} else if !bytes.Equal(oldMeta, state.Meta) {\n\t\t\t// if Meta changed, trigger an update notification\n\t\t\tm.config.Events.NotifyUpdate(&state.Node)\n\t\t}\n\t}\n}\n\n// suspectNode is invoked by the network layer when we get a message\n// about a suspect node\nfunc (m *Memberlist) suspectNode(s *suspect) {\n\tm.nodeLock.Lock()\n\tdefer m.nodeLock.Unlock()\n\tstate, ok := m.nodeMap[s.Node]\n\n\t// If we've never heard about this node before, ignore it\n\tif !ok {\n\t\treturn\n\t}\n\n\t// Ignore old incarnation numbers\n\tif s.Incarnation < state.Incarnation {\n\t\treturn\n\t}\n\n\t// See if there's a suspicion timer we can confirm. If the info is new\n\t// to us we will go ahead and re-gossip it. This allows for multiple\n\t// independent confirmations to flow even when a node probes a node\n\t// that's already suspect.\n\tif timer, ok := m.nodeTimers[s.Node]; ok {\n\t\tif timer.Confirm(s.From) {\n\t\t\tm.encodeAndBroadcast(s.Node, suspectMsg, s)\n\t\t}\n\t\treturn\n\t}\n\n\t// Ignore non-alive nodes\n\tif state.State != StateAlive {\n\t\treturn\n\t}\n\n\t// If this is us we need to refute, otherwise re-broadcast\n\tif state.Name == m.config.Name {\n\t\tm.refute(state, s.Incarnation)\n\t\tm.logger.Printf(\"[WARN] memberlist: Refuting a suspect message (from: %s)\", s.From)\n\t\treturn // Do not mark ourself suspect\n\t} else {\n\t\tm.encodeAndBroadcast(s.Node, suspectMsg, s)\n\t}\n\n\t// Update metrics\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"msg\", \"suspect\"}, 1, m.metricLabels)\n\n\t// Update the state\n\tstate.Incarnation = s.Incarnation\n\tstate.State = StateSuspect\n\tchangeTime := time.Now()\n\tstate.StateChange = changeTime\n\n\t// Setup a suspicion timer. Given that we don't have any known phase\n\t// relationship with our peers, we set up k such that we hit the nominal\n\t// timeout two probe intervals short of what we expect given the suspicion\n\t// multiplier.\n\tk := m.config.SuspicionMult - 2\n\n\t// If there aren't enough nodes to give the expected confirmations, just\n\t// set k to 0 to say that we don't expect any. Note we subtract 2 from n\n\t// here to take out ourselves and the node being probed.\n\tn := m.estNumNodes()\n\tif n-2 < k {\n\t\tk = 0\n\t}\n\n\t// Compute the timeouts based on the size of the cluster.\n\tmin := suspicionTimeout(m.config.SuspicionMult, n, m.config.ProbeInterval)\n\tmax := time.Duration(m.config.SuspicionMaxTimeoutMult) * min\n\tfn := func(numConfirmations int) {\n\t\tvar d *dead\n\n\t\tm.nodeLock.Lock()\n\t\tstate, ok := m.nodeMap[s.Node]\n\t\ttimeout := ok && state.State == StateSuspect && state.StateChange == changeTime\n\t\tif timeout {\n\t\t\td = &dead{Incarnation: state.Incarnation, Node: state.Name, From: m.config.Name}\n\t\t}\n\t\tm.nodeLock.Unlock()\n\n\t\tif timeout {\n\t\t\tif k > 0 && numConfirmations < k {\n\t\t\t\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"degraded\", \"timeout\"}, 1, m.metricLabels)\n\t\t\t}\n\n\t\t\tm.logger.Printf(\"[INFO] memberlist: Marking %s as failed, suspect timeout reached (%d peer confirmations)\",\n\t\t\t\tstate.Name, numConfirmations)\n\n\t\t\tm.deadNode(d)\n\t\t}\n\t}\n\tm.nodeTimers[s.Node] = newSuspicion(s.From, k, min, max, fn)\n}\n\n// deadNode is invoked by the network layer when we get a message\n// about a dead node\nfunc (m *Memberlist) deadNode(d *dead) {\n\tm.nodeLock.Lock()\n\tdefer m.nodeLock.Unlock()\n\tstate, ok := m.nodeMap[d.Node]\n\n\t// If we've never heard about this node before, ignore it\n\tif !ok {\n\t\treturn\n\t}\n\n\t// Ignore old incarnation numbers\n\tif d.Incarnation < state.Incarnation {\n\t\treturn\n\t}\n\n\t// Clear out any suspicion timer that may be in effect.\n\tdelete(m.nodeTimers, d.Node)\n\n\t// Ignore if node is already dead\n\tif state.DeadOrLeft() {\n\t\treturn\n\t}\n\n\t// Check if this is us\n\tif state.Name == m.config.Name {\n\t\t// If we are not leaving we need to refute\n\t\tif !m.hasLeft() {\n\t\t\tm.refute(state, d.Incarnation)\n\t\t\tm.logger.Printf(\"[WARN] memberlist: Refuting a dead message (from: %s)\", d.From)\n\t\t\treturn // Do not mark ourself dead\n\t\t}\n\n\t\t// If we are leaving, we broadcast and wait\n\t\tm.encodeBroadcastNotify(d.Node, deadMsg, d, m.leaveBroadcast)\n\t} else {\n\t\tm.encodeAndBroadcast(d.Node, deadMsg, d)\n\t}\n\n\t// Update metrics\n\tmetrics.IncrCounterWithLabels([]string{\"memberlist\", \"msg\", \"dead\"}, 1, m.metricLabels)\n\n\t// Update the state\n\tstate.Incarnation = d.Incarnation\n\n\t// If the dead message was send by the node itself, mark it is left\n\t// instead of dead.\n\tif d.Node == d.From {\n\t\tstate.State = StateLeft\n\t} else {\n\t\tstate.State = StateDead\n\t}\n\tstate.StateChange = time.Now()\n\n\t// Notify of death\n\tif m.config.Events != nil {\n\t\tm.config.Events.NotifyLeave(&state.Node)\n\t}\n}\n\n// mergeState is invoked by the network layer when we get a Push/Pull\n// state transfer\nfunc (m *Memberlist) mergeState(remote []pushNodeState) {\n\tfor _, r := range remote {\n\t\tswitch r.State {\n\t\tcase StateAlive:\n\t\t\ta := alive{\n\t\t\t\tIncarnation: r.Incarnation,\n\t\t\t\tNode:        r.Name,\n\t\t\t\tAddr:        r.Addr,\n\t\t\t\tPort:        r.Port,\n\t\t\t\tMeta:        r.Meta,\n\t\t\t\tVsn:         r.Vsn,\n\t\t\t}\n\t\t\tm.aliveNode(&a, nil, false)\n\n\t\tcase StateLeft:\n\t\t\td := dead{Incarnation: r.Incarnation, Node: r.Name, From: r.Name}\n\t\t\tm.deadNode(&d)\n\t\tcase StateDead:\n\t\t\t// If the remote node believes a node is dead, we prefer to\n\t\t\t// suspect that node instead of declaring it dead instantly\n\t\t\tfallthrough\n\t\tcase StateSuspect:\n\t\t\ts := suspect{Incarnation: r.Incarnation, Node: r.Name, From: m.config.Name}\n\t\t\tm.suspectNode(&s)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "state_test.go",
          "type": "blob",
          "size": 62.5009765625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-metrics/compat\"\n\tiretry \"github.com/hashicorp/memberlist/internal/retry\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc HostMemberlist(host string, t *testing.T, f func(*Config)) *Memberlist {\n\tt.Helper()\n\n\tc := DefaultLANConfig()\n\tc.Name = host\n\tc.BindAddr = host\n\tc.BindPort = 0 // choose a free port\n\tc.Logger = log.New(os.Stderr, host, log.LstdFlags)\n\tif f != nil {\n\t\tf(c)\n\t}\n\n\tm, err := newMemberlist(c)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to get memberlist: %s\", err)\n\t}\n\treturn m\n}\n\nfunc TestMemberList_Probe(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = time.Millisecond\n\t\tc.ProbeInterval = 10 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{\n\t\tNode:        addr1.String(),\n\t\tAddr:        []byte(addr1),\n\t\tPort:        uint16(m1.config.BindPort),\n\t\tIncarnation: 1,\n\t\tVsn:         m1.config.BuildVsnArray(),\n\t}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{\n\t\tNode:        addr2.String(),\n\t\tAddr:        []byte(addr2),\n\t\tPort:        uint16(m2.config.BindPort),\n\t\tIncarnation: 1,\n\t\tVsn:         m2.config.BuildVsnArray(),\n\t}\n\tm1.aliveNode(&a2, nil, false)\n\n\t// should ping addr2\n\tm1.probe()\n\n\t// Should not be marked suspect\n\tn := m1.nodeMap[addr2.String()]\n\tif n.State != StateAlive {\n\t\tt.Fatalf(\"Expect node to be alive\")\n\t}\n\n\t// Should increment seqno\n\tif m1.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqno %v\", m2.sequenceNum)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Suspect(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = time.Millisecond\n\t\tc.ProbeInterval = 10 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m3.Shutdown()\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1, Vsn: m3.config.BuildVsnArray()}\n\tm1.aliveNode(&a3, nil, false)\n\ta4 := alive{Node: addr4.String(), Addr: ip4, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a4, nil, false)\n\n\tn := m1.nodeMap[addr4.String()]\n\tm1.probeNode(n)\n\n\t// Should be marked suspect.\n\tif n.State != StateSuspect {\n\t\tt.Fatalf(\"Expect node to be suspect\")\n\t}\n\ttime.Sleep(10 * time.Millisecond)\n\n\t// One of the peers should have attempted an indirect probe.\n\tif s2, s3 := atomic.LoadUint32(&m2.sequenceNum), atomic.LoadUint32(&m3.sequenceNum); s2 != 1 && s3 != 1 {\n\t\tt.Fatalf(\"bad seqnos, expected both to be 1: %v, %v\", s2, s3)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Suspect_Dogpile(t *testing.T) {\n\tcases := []struct {\n\t\tname          string\n\t\tnumPeers      int\n\t\tconfirmations int\n\t\texpected      time.Duration\n\t}{\n\t\t{\"n=2, k=3 (max timeout disabled)\", 1, 0, 500 * time.Millisecond},\n\t\t{\"n=3, k=3\", 2, 0, 500 * time.Millisecond},\n\t\t{\"n=4, k=3\", 3, 0, 500 * time.Millisecond},\n\t\t{\"n=5, k=3 (max timeout starts to take effect)\", 4, 0, 1000 * time.Millisecond},\n\t\t{\"n=6, k=3\", 5, 0, 1000 * time.Millisecond},\n\t\t{\"n=6, k=3 (confirmations start to lower timeout)\", 5, 1, 750 * time.Millisecond},\n\t\t{\"n=6, k=3\", 5, 2, 604 * time.Millisecond},\n\t\t{\"n=6, k=3 (timeout driven to nominal value)\", 5, 3, 500 * time.Millisecond},\n\t\t{\"n=6, k=3\", 5, 4, 500 * time.Millisecond},\n\t}\n\n\tfor i, c := range cases {\n\t\tt.Run(c.name, func(t *testing.T) {\n\t\t\t// Create the main memberlist under test.\n\t\t\taddr := getBindAddr()\n\n\t\t\tm := HostMemberlist(addr.String(), t, func(c *Config) {\n\t\t\t\tc.ProbeTimeout = time.Millisecond\n\t\t\t\tc.ProbeInterval = 100 * time.Millisecond\n\t\t\t\tc.SuspicionMult = 5\n\t\t\t\tc.SuspicionMaxTimeoutMult = 2\n\t\t\t})\n\t\t\tdefer m.Shutdown()\n\n\t\t\tbindPort := m.config.BindPort\n\n\t\t\ta := alive{Node: addr.String(), Addr: []byte(addr), Port: uint16(bindPort), Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\t\t\tm.aliveNode(&a, nil, true)\n\n\t\t\t// Make all but one peer be an real, alive instance.\n\t\t\tvar peers []*Memberlist\n\t\t\tfor j := 0; j < c.numPeers-1; j++ {\n\t\t\t\tpeerAddr := getBindAddr()\n\n\t\t\t\tpeer := HostMemberlist(peerAddr.String(), t, func(c *Config) {\n\t\t\t\t\tc.BindPort = bindPort\n\t\t\t\t})\n\t\t\t\tdefer peer.Shutdown()\n\n\t\t\t\tpeers = append(peers, peer)\n\n\t\t\t\ta = alive{Node: peerAddr.String(), Addr: []byte(peerAddr), Port: uint16(bindPort), Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\t\t\t\tm.aliveNode(&a, nil, false)\n\t\t\t}\n\n\t\t\t// Just use a bogus address for the last peer so it doesn't respond\n\t\t\t// to pings, but tell the memberlist it's alive.\n\t\t\tbadPeerAddr := getBindAddr()\n\t\t\ta = alive{Node: badPeerAddr.String(), Addr: []byte(badPeerAddr), Port: uint16(bindPort), Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\t\t\tm.aliveNode(&a, nil, false)\n\n\t\t\t// Force a probe, which should start us into the suspect state.\n\t\t\tm.probeNodeByAddr(badPeerAddr.String())\n\n\t\t\tif m.getNodeState(badPeerAddr.String()) != StateSuspect {\n\t\t\t\tt.Fatalf(\"case %d: expected node to be suspect\", i)\n\t\t\t}\n\n\t\t\t// Add the requested number of confirmations.\n\t\t\tfor j := 0; j < c.confirmations; j++ {\n\t\t\t\tfrom := fmt.Sprintf(\"peer%d\", j)\n\t\t\t\ts := suspect{Node: badPeerAddr.String(), Incarnation: 1, From: from}\n\t\t\t\tm.suspectNode(&s)\n\t\t\t}\n\n\t\t\t// Wait until right before the timeout and make sure the timer\n\t\t\t// hasn't fired.\n\t\t\tfudge := 25 * time.Millisecond\n\t\t\ttime.Sleep(c.expected - fudge)\n\n\t\t\tif m.getNodeState(badPeerAddr.String()) != StateSuspect {\n\t\t\t\tt.Fatalf(\"case %d: expected node to still be suspect\", i)\n\t\t\t}\n\n\t\t\t// Wait through the timeout and a little after to make sure the\n\t\t\t// timer fires.\n\t\t\ttime.Sleep(2 * fudge)\n\n\t\t\tif m.getNodeState(badPeerAddr.String()) != StateDead {\n\t\t\t\tt.Fatalf(\"case %d: expected node to be dead\", i)\n\t\t\t}\n\t\t})\n\t}\n}\n\n/*\nfunc TestMemberList_ProbeNode_FallbackTCP(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMax time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMax = c.ProbeInterval + 20*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m3.Shutdown()\n\n\tm4 := HostMemberlist(addr4.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m4.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a3, nil, false)\n\n\t// Make sure m4 is configured with the same protocol version as m1 so\n\t// the TCP fallback behavior is enabled.\n\ta4 := alive{\n\t\tNode:        addr4.String(),\n\t\tAddr:        ip4,\n\t\tPort:        uint16(bindPort),\n\t\tIncarnation: 1,\n\t\tVsn: []uint8{\n\t\t\tProtocolVersionMin,\n\t\t\tProtocolVersionMax,\n\t\t\tm1.config.ProtocolVersion,\n\t\t\tm1.config.DelegateProtocolMin,\n\t\t\tm1.config.DelegateProtocolMax,\n\t\t\tm1.config.DelegateProtocolVersion,\n\t\t},\n\t}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Isolate m4 from UDP traffic by re-opening its listener on the wrong\n\t// port. This should force the TCP fallback path to be used.\n\tvar err error\n\tif err = m4.udpListener.Close(); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tudpAddr := &net.UDPAddr{IP: ip4, Port: 9999}\n\tif m4.udpListener, err = net.ListenUDP(\"udp\", udpAddr); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Should be marked alive because of the TCP fallback ping.\n\tif n.State != stateAlive {\n\t\tt.Fatalf(\"expect node to be alive\")\n\t}\n\n\t// Make sure TCP activity completed in a timely manner.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\ttime.Sleep(probeTimeMax)\n\tif m2.sequenceNum != 1 && m3.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n\n\t// Now shutdown all inbound TCP traffic to make sure the TCP fallback\n\t// path properly fails when the node is really unreachable.\n\tif err = m4.tcpListener.Close(); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\ttcpAddr := &net.TCPAddr{IP: ip4, Port: 9999}\n\tif m4.tcpListener, err = net.ListenTCP(\"tcp\", tcpAddr); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\t// Probe again, this time there should be no contact.\n\tstartProbe = time.Now()\n\tm1.probeNode(n)\n\tprobeTime = time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\tif n.State != stateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Make sure TCP activity didn't cause us to wait too long before\n\t// timing out.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\ttime.Sleep(probeTimeMax)\n\tif m2.sequenceNum != 2 && m3.sequenceNum != 2 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_FallbackTCP_Disabled(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMax time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMax = c.ProbeInterval + 20*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m3.Shutdown()\n\n\tm4 := HostMemberlist(addr4.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m4.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a3, nil, false)\n\n\t// Make sure m4 is configured with the same protocol version as m1 so\n\t// the TCP fallback behavior is enabled.\n\ta4 := alive{\n\t\tNode:        addr4.String(),\n\t\tAddr:        ip4,\n\t\tPort:        uint16(bindPort),\n\t\tIncarnation: 1,\n\t\tVsn: []uint8{\n\t\t\tProtocolVersionMin,\n\t\t\tProtocolVersionMax,\n\t\t\tm1.config.ProtocolVersion,\n\t\t\tm1.config.DelegateProtocolMin,\n\t\t\tm1.config.DelegateProtocolMax,\n\t\t\tm1.config.DelegateProtocolVersion,\n\t\t},\n\t}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Isolate m4 from UDP traffic by re-opening its listener on the wrong\n\t// port. This should force the TCP fallback path to be used.\n\tvar err error\n\tif err = m4.udpListener.Close(); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tudpAddr := &net.UDPAddr{IP: ip4, Port: 9999}\n\tif m4.udpListener, err = net.ListenUDP(\"udp\", udpAddr); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\t// Disable the TCP pings using the config mechanism.\n\tm1.config.DisableTcpPings = true\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\tif n.State != stateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Make sure TCP activity didn't cause us to wait too long before\n\t// timing out.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\ttime.Sleep(probeTimeMax)\n\tif m2.sequenceNum != 1 && m3.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_FallbackTCP_OldProtocol(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMax time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMax = c.ProbeInterval + 20*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m3.Shutdown()\n\n\tm4 := HostMemberlist(addr4.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m4.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a3, nil, false)\n\n\t// Set up m4 so that it doesn't understand a version of the protocol\n\t// that supports TCP pings.\n\ta4 := alive{\n\t\tNode:        addr4.String(),\n\t\tAddr:        ip4,\n\t\tPort:        uint16(bindPort),\n\t\tIncarnation: 1,\n\t\tVsn: []uint8{\n\t\t\tProtocolVersionMin,\n\t\t\tProtocolVersion2Compatible,\n\t\t\tProtocolVersion2Compatible,\n\t\t\tm1.config.DelegateProtocolMin,\n\t\t\tm1.config.DelegateProtocolMax,\n\t\t\tm1.config.DelegateProtocolVersion,\n\t\t},\n\t}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Isolate m4 from UDP traffic by re-opening its listener on the wrong\n\t// port. This should force the TCP fallback path to be used.\n\tvar err error\n\tif err = m4.udpListener.Close(); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tudpAddr := &net.UDPAddr{IP: ip4, Port: 9999}\n\tif m4.udpListener, err = net.ListenUDP(\"udp\", udpAddr); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\tif n.State != stateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Make sure TCP activity didn't cause us to wait too long before\n\t// timing out.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\ttime.Sleep(probeTimeMax)\n\tif m2.sequenceNum != 1 && m3.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n}\n*/\n\nfunc TestMemberList_ProbeNode_Awareness_Degraded(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMin time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMin = 2*c.ProbeInterval - 50*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m3.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1, Vsn: m3.config.BuildVsnArray()}\n\tm1.aliveNode(&a3, nil, false)\n\n\tvsn4 := []uint8{\n\t\tProtocolVersionMin, ProtocolVersionMax, ProtocolVersionMin,\n\t\t1, 1, 1,\n\t}\n\t// Node 4 never gets started.\n\ta4 := alive{Node: addr4.String(), Addr: ip4, Port: uint16(bindPort), Incarnation: 1, Vsn: vsn4}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Start the health in a degraded state.\n\tm1.awareness.ApplyDelta(1)\n\tif score := m1.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\tif n.State != StateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Make sure we timed out approximately on time (note that we accounted\n\t// for the slowed-down failure detector in the probeTimeMin calculation.\n\tif probeTime < probeTimeMin {\n\t\tt.Fatalf(\"probed too quickly, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\tif m2.sequenceNum != 1 && m3.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n\n\t// We should have gotten all the nacks, so our score should remain the\n\t// same, since we didn't get a successful probe.\n\tif score := m1.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Wrong_VSN(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m3.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1, Vsn: m3.config.BuildVsnArray()}\n\tm1.aliveNode(&a3, nil, false)\n\n\tvsn4 := []uint8{\n\t\t0, 0, 0,\n\t\t0, 0, 0,\n\t}\n\t// Node 4 never gets started.\n\ta4 := alive{Node: addr4.String(), Addr: ip4, Port: uint16(bindPort), Incarnation: 1, Vsn: vsn4}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Start the health in a degraded state.\n\tm1.awareness.ApplyDelta(1)\n\tif score := m1.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\t// Have node m1 probe m4.\n\tn, ok := m1.nodeMap[addr4.String()]\n\tif ok || n != nil {\n\t\tt.Fatalf(\"expect node a4 to be not taken into account, because of its wrong version\")\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Awareness_Improved(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\n\t// Start the health in a degraded state.\n\tm1.awareness.ApplyDelta(1)\n\tif score := m1.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\t// Have node m1 probe m2.\n\tn := m1.nodeMap[addr2.String()]\n\tm1.probeNode(n)\n\n\t// Node should be reported alive.\n\tif n.State != StateAlive {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Our score should have improved since we did a good probe.\n\tif score := m1.GetHealthScore(); score != 0 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Awareness_MissedNack(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMax time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMax = c.ProbeInterval + 50*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\n\tvsn := m1.config.BuildVsnArray()\n\t// Node 3 and node 4 never get started.\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1, Vsn: vsn}\n\tm1.aliveNode(&a3, nil, false)\n\ta4 := alive{Node: addr4.String(), Addr: ip4, Port: uint16(bindPort), Incarnation: 1, Vsn: vsn}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Make sure health looks good.\n\tif score := m1.GetHealthScore(); score != 0 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\n\tm1.nodeLock.Lock()\n\tif n.State != StateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\tm1.nodeLock.Unlock()\n\n\t// Make sure we timed out approximately on time.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// We should have gotten dinged for the missed nack. Note that the code under\n\t// test is waiting for probeTimeMax and then doing some other work before it\n\t// updates the awareness, so we need to wait some extra time. Rather than just\n\t// add longer and longer sleeps, we'll retry a few times.\n\tiretry.Run(t, func(r *iretry.R) {\n\t\tif score := m1.GetHealthScore(); score != 1 {\n\t\t\tr.Fatalf(\"expected health score to decrement on missed nack. want %d, \"+\n\t\t\t\t\"got: %d\", 1, score)\n\t\t}\n\t})\n}\n\nfunc TestMemberList_ProbeNode_Awareness_OldProtocol(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\taddr4 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\tip4 := []byte(addr4)\n\n\tvar probeTimeMax time.Duration\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 10 * time.Millisecond\n\t\tc.ProbeInterval = 200 * time.Millisecond\n\t\tprobeTimeMax = c.ProbeInterval + 20*time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr3.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m3.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a3, nil, false)\n\n\t// Node 4 never gets started.\n\ta4 := alive{Node: addr4.String(), Addr: ip4, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a4, nil, false)\n\n\t// Make sure health looks good.\n\tif score := m1.GetHealthScore(); score != 0 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\t// Have node m1 probe m4.\n\tn := m1.nodeMap[addr4.String()]\n\tstartProbe := time.Now()\n\tm1.probeNode(n)\n\tprobeTime := time.Now().Sub(startProbe)\n\n\t// Node should be reported suspect.\n\tif n.State != StateSuspect {\n\t\tt.Fatalf(\"expect node to be suspect\")\n\t}\n\n\t// Make sure we timed out approximately on time.\n\tif probeTime > probeTimeMax {\n\t\tt.Fatalf(\"took to long to probe, %9.6f\", probeTime.Seconds())\n\t}\n\n\t// Confirm at least one of the peers attempted an indirect probe.\n\ttime.Sleep(probeTimeMax)\n\tif m2.sequenceNum != 1 && m3.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqnos %v, %v\", m2.sequenceNum, m3.sequenceNum)\n\t}\n\n\t// Since we are using the old protocol here, we should have gotten dinged\n\t// for a failed health check.\n\tif score := m1.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n}\n\nfunc TestMemberList_ProbeNode_Buddy(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = time.Millisecond\n\t\tc.ProbeInterval = 10 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\n\tm1.aliveNode(&a1, nil, true)\n\tm1.aliveNode(&a2, nil, false)\n\tm2.aliveNode(&a2, nil, true)\n\n\t// Force the state to suspect so we piggyback a suspect message with the ping.\n\t// We should see this get refuted later, and the ping will succeed.\n\tn := m1.nodeMap[addr2.String()]\n\tn.State = StateSuspect\n\tm1.probeNode(n)\n\n\t// Make sure a ping was sent.\n\tif m1.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqno %v\", m1.sequenceNum)\n\t}\n\n\t// Check a broadcast is queued.\n\tif num := m2.broadcasts.NumQueued(); num != 1 {\n\t\tt.Fatalf(\"expected only one queued message: %d\", num)\n\t}\n\n\t// Should be alive msg.\n\tif messageType(m2.broadcasts.orderedView(true)[0].b.Message()[0]) != aliveMsg {\n\t\tt.Fatalf(\"expected queued alive msg\")\n\t}\n}\n\nfunc TestMemberList_ProbeNode(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = time.Millisecond\n\t\tc.ProbeInterval = 10 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\n\tn := m1.nodeMap[addr2.String()]\n\tm1.probeNode(n)\n\n\t// Should be marked alive\n\tif n.State != StateAlive {\n\t\tt.Fatalf(\"Expect node to be alive\")\n\t}\n\n\t// Should increment seqno\n\tif m1.sequenceNum != 1 {\n\t\tt.Fatalf(\"bad seqno %v\", m1.sequenceNum)\n\t}\n}\n\nfunc TestMemberList_Ping(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.ProbeTimeout = 1 * time.Second\n\t\tc.ProbeInterval = 10 * time.Second\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1}\n\tm1.aliveNode(&a2, nil, false)\n\n\t// Do a legit ping.\n\tn := m1.nodeMap[addr2.String()]\n\taddr, err := net.ResolveUDPAddr(\"udp\", net.JoinHostPort(addr2.String(), strconv.Itoa(bindPort)))\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\trtt, err := m1.Ping(n.Name, addr)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tif !(rtt > 0) {\n\t\tt.Fatalf(\"bad: %v\", rtt)\n\t}\n\n\t// This ping has a bad node name so should timeout.\n\t_, err = m1.Ping(\"bad\", addr)\n\tif _, ok := err.(NoPingResponseError); !ok || err == nil {\n\t\tt.Fatalf(\"bad: %v\", err)\n\t}\n}\n\nfunc TestMemberList_ResetNodes(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.GossipToTheDeadTime = 100 * time.Millisecond\n\t})\n\tdefer m.Shutdown()\n\n\ta1 := alive{Node: \"test1\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a1, nil, false)\n\ta2 := alive{Node: \"test2\", Addr: []byte{127, 0, 0, 2}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: \"test3\", Addr: []byte{127, 0, 0, 3}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a3, nil, false)\n\td := dead{Node: \"test2\", Incarnation: 1}\n\tm.deadNode(&d)\n\n\tm.resetNodes()\n\tif len(m.nodes) != 3 {\n\t\tt.Fatalf(\"Bad length\")\n\t}\n\tif _, ok := m.nodeMap[\"test2\"]; !ok {\n\t\tt.Fatalf(\"test2 should not be unmapped\")\n\t}\n\n\ttime.Sleep(200 * time.Millisecond)\n\tm.resetNodes()\n\tif len(m.nodes) != 2 {\n\t\tt.Fatalf(\"Bad length\")\n\t}\n\tif _, ok := m.nodeMap[\"test2\"]; ok {\n\t\tt.Fatalf(\"test2 should be unmapped\")\n\t}\n}\n\nfunc TestMemberList_NextSeq(t *testing.T) {\n\tm := &Memberlist{}\n\tif m.nextSeqNo() != 1 {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n\tif m.nextSeqNo() != 2 {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n}\n\nfunc ackHandlerExists(t *testing.T, m *Memberlist, idx uint32) bool {\n\tt.Helper()\n\n\tm.ackLock.Lock()\n\t_, ok := m.ackHandlers[idx]\n\tm.ackLock.Unlock()\n\n\treturn ok\n}\n\nfunc TestMemberList_setProbeChannels(t *testing.T) {\n\tm := &Memberlist{ackHandlers: make(map[uint32]*ackHandler)}\n\n\tch := make(chan ackMessage, 1)\n\tm.setProbeChannels(0, ch, nil, 10*time.Millisecond)\n\n\trequire.True(t, ackHandlerExists(t, m, 0), \"missing handler\")\n\n\ttime.Sleep(20 * time.Millisecond)\n\n\trequire.False(t, ackHandlerExists(t, m, 0), \"non-reaped handler\")\n}\n\nfunc TestMemberList_setAckHandler(t *testing.T) {\n\tm := &Memberlist{ackHandlers: make(map[uint32]*ackHandler)}\n\n\tf := func([]byte, time.Time) {}\n\tm.setAckHandler(0, f, 10*time.Millisecond)\n\n\trequire.True(t, ackHandlerExists(t, m, 0), \"missing handler\")\n\n\ttime.Sleep(20 * time.Millisecond)\n\n\trequire.False(t, ackHandlerExists(t, m, 0), \"non-reaped handler\")\n}\n\nfunc TestMemberList_invokeAckHandler(t *testing.T) {\n\tm := &Memberlist{ackHandlers: make(map[uint32]*ackHandler)}\n\n\t// Does nothing\n\tm.invokeAckHandler(ackResp{}, time.Now())\n\n\tvar b bool\n\tf := func(payload []byte, timestamp time.Time) { b = true }\n\tm.setAckHandler(0, f, 10*time.Millisecond)\n\n\t// Should set b\n\tm.invokeAckHandler(ackResp{0, nil}, time.Now())\n\tif !b {\n\t\tt.Fatalf(\"b not set\")\n\t}\n\n\trequire.False(t, ackHandlerExists(t, m, 0), \"non-reaped handler\")\n}\n\nfunc TestMemberList_invokeAckHandler_Channel_Ack(t *testing.T) {\n\tm := &Memberlist{ackHandlers: make(map[uint32]*ackHandler)}\n\n\tack := ackResp{0, []byte{0, 0, 0}}\n\n\t// Does nothing\n\tm.invokeAckHandler(ack, time.Now())\n\n\tackCh := make(chan ackMessage, 1)\n\tnackCh := make(chan struct{}, 1)\n\tm.setProbeChannels(0, ackCh, nackCh, 10*time.Millisecond)\n\n\t// Should send message\n\tm.invokeAckHandler(ack, time.Now())\n\n\tselect {\n\tcase v := <-ackCh:\n\t\tif v.Complete != true {\n\t\t\tt.Fatalf(\"Bad value\")\n\t\t}\n\t\tif bytes.Compare(v.Payload, ack.Payload) != 0 {\n\t\t\tt.Fatalf(\"wrong payload. expected: %v; actual: %v\", ack.Payload, v.Payload)\n\t\t}\n\n\tcase <-nackCh:\n\t\tt.Fatalf(\"should not get a nack\")\n\n\tdefault:\n\t\tt.Fatalf(\"message not sent\")\n\t}\n\n\trequire.False(t, ackHandlerExists(t, m, 0), \"non-reaped handler\")\n}\n\nfunc TestMemberList_invokeAckHandler_Channel_Nack(t *testing.T) {\n\tm := &Memberlist{ackHandlers: make(map[uint32]*ackHandler)}\n\n\tnack := nackResp{0}\n\n\t// Does nothing.\n\tm.invokeNackHandler(nack)\n\n\tackCh := make(chan ackMessage, 1)\n\tnackCh := make(chan struct{}, 1)\n\tm.setProbeChannels(0, ackCh, nackCh, 10*time.Millisecond)\n\n\t// Should send message.\n\tm.invokeNackHandler(nack)\n\n\tselect {\n\tcase <-ackCh:\n\t\tt.Fatalf(\"should not get an ack\")\n\n\tcase <-nackCh:\n\t\t// Good.\n\n\tdefault:\n\t\tt.Fatalf(\"message not sent\")\n\t}\n\n\t// Getting a nack doesn't reap the handler so that we can still forward\n\t// an ack up to the reap time, if we get one.\n\trequire.True(t, ackHandlerExists(t, m, 0), \"handler should not be reaped\")\n\n\tack := ackResp{0, []byte{0, 0, 0}}\n\tm.invokeAckHandler(ack, time.Now())\n\n\tselect {\n\tcase v := <-ackCh:\n\t\tif v.Complete != true {\n\t\t\tt.Fatalf(\"Bad value\")\n\t\t}\n\t\tif bytes.Compare(v.Payload, ack.Payload) != 0 {\n\t\t\tt.Fatalf(\"wrong payload. expected: %v; actual: %v\", ack.Payload, v.Payload)\n\t\t}\n\n\tcase <-nackCh:\n\t\tt.Fatalf(\"should not get a nack\")\n\n\tdefault:\n\t\tt.Fatalf(\"message not sent\")\n\t}\n\n\trequire.False(t, ackHandlerExists(t, m, 0), \"non-reaped handler\")\n}\n\nfunc TestMemberList_AliveNode_NewNode(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tif len(m.nodes) != 1 {\n\t\tt.Fatalf(\"should add node\")\n\t}\n\n\tstate, ok := m.nodeMap[\"test\"]\n\tif !ok {\n\t\tt.Fatalf(\"should map node\")\n\t}\n\n\tif state.Incarnation != 1 {\n\t\tt.Fatalf(\"bad incarnation\")\n\t}\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"bad state\")\n\t}\n\tif time.Now().Sub(state.StateChange) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\n\t// Check for a join message\n\tselect {\n\tcase e := <-ch:\n\t\tif e.Node.Name != \"test\" {\n\t\t\tt.Fatalf(\"bad node name\")\n\t\t}\n\tdefault:\n\t\tt.Fatalf(\"no join message\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected queued message\")\n\t}\n}\n\nfunc TestMemberList_AliveNode_SuspectNode(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\tted := &toggledEventDelegate{\n\t\treal: &ChannelEventDelegate{ch},\n\t}\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = ted\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\t// Listen only after first join\n\tted.Toggle(true)\n\n\t// Make suspect\n\tstate := m.nodeMap[\"test\"]\n\tstate.State = StateSuspect\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\t// Old incarnation number, should not change\n\tm.aliveNode(&a, nil, false)\n\tif state.State != StateSuspect {\n\t\tt.Fatalf(\"update with old incarnation!\")\n\t}\n\n\t// Should reset to alive now\n\ta.Incarnation = 2\n\tm.aliveNode(&a, nil, false)\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"no update with new incarnation!\")\n\t}\n\n\tif time.Now().Sub(state.StateChange) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\n\t// Check for a no join message\n\tselect {\n\tcase <-ch:\n\t\tt.Fatalf(\"got bad join message\")\n\tdefault:\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected queued message\")\n\t}\n}\n\nfunc TestMemberList_AliveNode_Idempotent(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\tted := &toggledEventDelegate{\n\t\treal: &ChannelEventDelegate{ch},\n\t}\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = ted\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\t// Listen only after first join\n\tted.Toggle(true)\n\n\t// Make suspect\n\tstate := m.nodeMap[\"test\"]\n\tstateTime := state.StateChange\n\n\t// Should reset to alive now\n\ta.Incarnation = 2\n\tm.aliveNode(&a, nil, false)\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"non idempotent\")\n\t}\n\n\tif stateTime != state.StateChange {\n\t\tt.Fatalf(\"should not change state\")\n\t}\n\n\t// Check for a no join message\n\tselect {\n\tcase <-ch:\n\t\tt.Fatalf(\"got bad join message\")\n\tdefault:\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n}\n\ntype toggledEventDelegate struct {\n\tmu      sync.Mutex\n\treal    EventDelegate\n\tenabled bool\n}\n\nfunc (d *toggledEventDelegate) Toggle(enabled bool) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.enabled = enabled\n}\n\n// NotifyJoin is invoked when a node is detected to have joined.\n// The Node argument must not be modified.\nfunc (d *toggledEventDelegate) NotifyJoin(n *Node) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif d.enabled {\n\t\td.real.NotifyJoin(n)\n\t}\n}\n\n// NotifyLeave is invoked when a node is detected to have left.\n// The Node argument must not be modified.\nfunc (d *toggledEventDelegate) NotifyLeave(n *Node) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif d.enabled {\n\t\td.real.NotifyLeave(n)\n\t}\n}\n\n// NotifyUpdate is invoked when a node is detected to have\n// updated, usually involving the meta data. The Node argument\n// must not be modified.\nfunc (d *toggledEventDelegate) NotifyUpdate(n *Node) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif d.enabled {\n\t\td.real.NotifyUpdate(n)\n\t}\n}\n\n// Serf Bug: GH-58, Meta data does not update\nfunc TestMemberList_AliveNode_ChangeMeta(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\tted := &toggledEventDelegate{\n\t\treal: &ChannelEventDelegate{ch},\n\t}\n\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = ted\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{\n\t\tNode:        \"test\",\n\t\tAddr:        []byte{127, 0, 0, 1},\n\t\tMeta:        []byte(\"val1\"),\n\t\tIncarnation: 1,\n\t\tVsn:         m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\t// Listen only after first join\n\tted.Toggle(true)\n\n\t// Make suspect\n\tstate := m.nodeMap[\"test\"]\n\n\t// Should reset to alive now\n\ta.Incarnation = 2\n\ta.Meta = []byte(\"val2\")\n\tm.aliveNode(&a, nil, false)\n\n\t// Check updates\n\tif bytes.Compare(state.Meta, a.Meta) != 0 {\n\t\tt.Fatalf(\"meta did not update\")\n\t}\n\n\t// Check for a NotifyUpdate\n\tselect {\n\tcase e := <-ch:\n\t\tif e.Event != NodeUpdate {\n\t\t\tt.Fatalf(\"bad event: %v\", e)\n\t\t}\n\t\tif !reflect.DeepEqual(*e.Node, state.Node) {\n\t\t\tt.Fatalf(\"expected %v, got %v\", *e.Node, state.Node)\n\t\t}\n\t\tif bytes.Compare(e.Node.Meta, a.Meta) != 0 {\n\t\t\tt.Fatalf(\"meta did not update\")\n\t\t}\n\tdefault:\n\t\tt.Fatalf(\"missing event!\")\n\t}\n\n}\n\nfunc TestMemberList_AliveNode_Refute(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: m.config.Name, Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, true)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\t// Conflicting alive\n\ts := alive{\n\t\tNode:        m.config.Name,\n\t\tAddr:        []byte{127, 0, 0, 1},\n\t\tIncarnation: 2,\n\t\tMeta:        []byte(\"foo\"),\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&s, nil, false)\n\n\tstate := m.nodeMap[m.config.Name]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\tif state.Meta != nil {\n\t\tt.Fatalf(\"meta should still be nil\")\n\t}\n\n\t// Check a broad cast is queued\n\tif num := m.broadcasts.NumQueued(); num != 1 {\n\t\tt.Fatalf(\"expected only one queued message: %d\",\n\t\t\tnum)\n\t}\n\n\t// Should be alive mesg\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != aliveMsg {\n\t\tt.Fatalf(\"expected queued alive msg\")\n\t}\n}\n\nfunc TestMemberList_AliveNode_Conflict(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.DeadNodeReclaimTime = 10 * time.Millisecond\n\t})\n\tdefer m.Shutdown()\n\n\tnodeName := \"test\"\n\ta := alive{Node: nodeName, Addr: []byte{127, 0, 0, 1}, Port: 8000, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, true)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\t// Conflicting alive\n\ts := alive{\n\t\tNode:        nodeName,\n\t\tAddr:        []byte{127, 0, 0, 2},\n\t\tPort:        9000,\n\t\tIncarnation: 2,\n\t\tMeta:        []byte(\"foo\"),\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&s, nil, false)\n\n\tstate := m.nodeMap[nodeName]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\tif state.Meta != nil {\n\t\tt.Fatalf(\"meta should still be nil\")\n\t}\n\tif bytes.Equal(state.Addr, []byte{127, 0, 0, 2}) {\n\t\tt.Fatalf(\"address should not be updated\")\n\t}\n\tif state.Port == 9000 {\n\t\tt.Fatalf(\"port should not be updated\")\n\t}\n\n\t// Check a broad cast is queued\n\tif num := m.broadcasts.NumQueued(); num != 0 {\n\t\tt.Fatalf(\"expected 0 queued messages: %d\", num)\n\t}\n\n\t// Change the node to dead\n\td := dead{Node: nodeName, Incarnation: 2}\n\tm.deadNode(&d)\n\tm.broadcasts.Reset()\n\n\tstate = m.nodeMap[nodeName]\n\tif state.State != StateDead {\n\t\tt.Fatalf(\"should be dead\")\n\t}\n\n\ttime.Sleep(m.config.DeadNodeReclaimTime)\n\n\t// New alive node\n\ts2 := alive{\n\t\tNode:        nodeName,\n\t\tAddr:        []byte{127, 0, 0, 2},\n\t\tPort:        9000,\n\t\tIncarnation: 3,\n\t\tMeta:        []byte(\"foo\"),\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&s2, nil, false)\n\n\tstate = m.nodeMap[nodeName]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\tif !bytes.Equal(state.Meta, []byte(\"foo\")) {\n\t\tt.Fatalf(\"meta should be updated\")\n\t}\n\tif !bytes.Equal(state.Addr, []byte{127, 0, 0, 2}) {\n\t\tt.Fatalf(\"address should be updated\")\n\t}\n\tif state.Port != 9000 {\n\t\tt.Fatalf(\"port should be updated\")\n\t}\n}\n\nfunc TestMemberList_SuspectNode_NoNode(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ts := suspect{Node: \"test\", Incarnation: 1}\n\tm.suspectNode(&s)\n\tif len(m.nodes) != 0 {\n\t\tt.Fatalf(\"don't expect nodes\")\n\t}\n}\n\nfunc TestMemberList_SuspectNode(t *testing.T) {\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.ProbeInterval = time.Millisecond\n\t\tc.SuspicionMult = 1\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tm.changeNode(\"test\", func(state *nodeState) {\n\t\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\t})\n\n\ts := suspect{Node: \"test\", Incarnation: 1}\n\tm.suspectNode(&s)\n\n\tif m.getNodeState(\"test\") != StateSuspect {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\tchange := m.getNodeStateChange(\"test\")\n\tif time.Now().Sub(change) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Check its a suspect message\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != suspectMsg {\n\t\tt.Fatalf(\"expected queued suspect msg\")\n\t}\n\n\t// Wait for the timeout\n\ttime.Sleep(10 * time.Millisecond)\n\n\tif m.getNodeState(\"test\") != StateDead {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\tnewChange := m.getNodeStateChange(\"test\")\n\tif time.Now().Sub(newChange) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\tif !newChange.After(change) {\n\t\tt.Fatalf(\"should increment time\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Check its a suspect message\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != deadMsg {\n\t\tt.Fatalf(\"expected queued dead msg\")\n\t}\n}\n\nfunc TestMemberList_SuspectNode_DoubleSuspect(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tstate := m.nodeMap[\"test\"]\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\ts := suspect{Node: \"test\", Incarnation: 1}\n\tm.suspectNode(&s)\n\n\tif state.State != StateSuspect {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\tchange := state.StateChange\n\tif time.Now().Sub(change) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\n\t// clear the broadcast queue\n\tm.broadcasts.Reset()\n\n\t// Suspect again\n\tm.suspectNode(&s)\n\n\tif state.StateChange != change {\n\t\tt.Fatalf(\"unexpected state change\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 0 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n}\n\nfunc TestMemberList_SuspectNode_OldSuspect(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 10, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tstate := m.nodeMap[\"test\"]\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\ts := suspect{Node: \"test\", Incarnation: 1}\n\tm.suspectNode(&s)\n\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 0 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n}\n\nfunc TestMemberList_SuspectNode_Refute(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: m.config.Name, Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, true)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\t// Make sure health is in a good state\n\tif score := m.GetHealthScore(); score != 0 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\ts := suspect{Node: m.config.Name, Incarnation: 1}\n\tm.suspectNode(&s)\n\n\tstate := m.nodeMap[m.config.Name]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Should be alive mesg\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != aliveMsg {\n\t\tt.Fatalf(\"expected queued alive msg\")\n\t}\n\n\t// Health should have been dinged\n\tif score := m.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n}\n\nfunc TestMemberList_DeadNode_NoNode(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\td := dead{Node: \"test\", Incarnation: 1}\n\tm.deadNode(&d)\n\tif len(m.nodes) != 0 {\n\t\tt.Fatalf(\"don't expect nodes\")\n\t}\n}\n\nfunc TestMemberList_DeadNodeLeft(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t})\n\tdefer m.Shutdown()\n\n\tnodeName := \"node1\"\n\ts1 := alive{\n\t\tNode:        nodeName,\n\t\tAddr:        []byte{127, 0, 0, 1},\n\t\tPort:        8000,\n\t\tIncarnation: 1,\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&s1, nil, false)\n\n\t// Read the join event\n\t<-ch\n\n\td := dead{Node: nodeName, From: nodeName, Incarnation: 1}\n\tm.deadNode(&d)\n\n\t// Read the dead event\n\t<-ch\n\n\tstate := m.nodeMap[nodeName]\n\tif state.State != StateLeft {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Check its a dead message\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != deadMsg {\n\t\tt.Fatalf(\"expected queued dead msg\")\n\t}\n\n\t// Clear queue\n\t// m.broadcasts.Reset()\n\n\t// New alive node\n\ts2 := alive{\n\t\tNode:        nodeName,\n\t\tAddr:        []byte{127, 0, 0, 2},\n\t\tPort:        9000,\n\t\tIncarnation: 3,\n\t\tMeta:        []byte(\"foo\"),\n\t\tVsn:         m.config.BuildVsnArray(),\n\t}\n\tm.aliveNode(&s2, nil, false)\n\n\t// Read the join event\n\t<-ch\n\n\tstate = m.nodeMap[nodeName]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\tif !bytes.Equal(state.Meta, []byte(\"foo\")) {\n\t\tt.Fatalf(\"meta should be updated\")\n\t}\n\tif !bytes.Equal(state.Addr, []byte{127, 0, 0, 2}) {\n\t\tt.Fatalf(\"address should be updated\")\n\t}\n\tif state.Port != 9000 {\n\t\tt.Fatalf(\"port should be updated\")\n\t}\n}\n\nfunc TestMemberList_DeadNode(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\n\tm := GetMemberlist(t, func(c *Config) {\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t})\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\t// Read the join event\n\t<-ch\n\n\tstate := m.nodeMap[\"test\"]\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\td := dead{Node: \"test\", Incarnation: 1}\n\tm.deadNode(&d)\n\n\tif state.State != StateDead {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n\n\tchange := state.StateChange\n\tif time.Now().Sub(change) > time.Second {\n\t\tt.Fatalf(\"bad change delta\")\n\t}\n\n\tselect {\n\tcase leave := <-ch:\n\t\tif leave.Event != NodeLeave || leave.Node.Name != \"test\" {\n\t\t\tt.Fatalf(\"bad node name\")\n\t\t}\n\tdefault:\n\t\tt.Fatalf(\"no leave message\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Check its a dead message\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != deadMsg {\n\t\tt.Fatalf(\"expected queued dead msg\")\n\t}\n}\n\nfunc TestMemberList_DeadNode_Double(t *testing.T) {\n\tch := make(chan NodeEvent, 1)\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tstate := m.nodeMap[\"test\"]\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\td := dead{Node: \"test\", Incarnation: 1}\n\tm.deadNode(&d)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\t// Notify after the first dead\n\tm.config.Events = &ChannelEventDelegate{ch}\n\n\t// Should do nothing\n\td.Incarnation = 2\n\tm.deadNode(&d)\n\n\tselect {\n\tcase <-ch:\n\t\tt.Fatalf(\"should not get leave\")\n\tdefault:\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 0 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n}\n\nfunc TestMemberList_DeadNode_OldDead(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 10, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\tstate := m.nodeMap[\"test\"]\n\tstate.StateChange = state.StateChange.Add(-time.Hour)\n\n\td := dead{Node: \"test\", Incarnation: 1}\n\tm.deadNode(&d)\n\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n}\n\nfunc TestMemberList_DeadNode_AliveReplay(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: \"test\", Addr: []byte{127, 0, 0, 1}, Incarnation: 10, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, false)\n\n\td := dead{Node: \"test\", Incarnation: 10}\n\tm.deadNode(&d)\n\n\t// Replay alive at same incarnation\n\tm.aliveNode(&a, nil, false)\n\n\t// Should remain dead\n\tstate, ok := m.nodeMap[\"test\"]\n\tif ok && state.State != StateDead {\n\t\tt.Fatalf(\"Bad state\")\n\t}\n}\n\nfunc TestMemberList_DeadNode_Refute(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta := alive{Node: m.config.Name, Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a, nil, true)\n\n\t// Clear queue\n\tm.broadcasts.Reset()\n\n\t// Make sure health is in a good state\n\tif score := m.GetHealthScore(); score != 0 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n\n\td := dead{Node: m.config.Name, Incarnation: 1}\n\tm.deadNode(&d)\n\n\tstate := m.nodeMap[m.config.Name]\n\tif state.State != StateAlive {\n\t\tt.Fatalf(\"should still be alive\")\n\t}\n\n\t// Check a broad cast is queued\n\tif m.broadcasts.NumQueued() != 1 {\n\t\tt.Fatalf(\"expected only one queued message\")\n\t}\n\n\t// Should be alive mesg\n\tif messageType(m.broadcasts.orderedView(true)[0].b.Message()[0]) != aliveMsg {\n\t\tt.Fatalf(\"expected queued alive msg\")\n\t}\n\n\t// We should have been dinged\n\tif score := m.GetHealthScore(); score != 1 {\n\t\tt.Fatalf(\"bad: %d\", score)\n\t}\n}\n\nfunc TestMemberList_MergeState(t *testing.T) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\ta1 := alive{Node: \"test1\", Addr: []byte{127, 0, 0, 1}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a1, nil, false)\n\ta2 := alive{Node: \"test2\", Addr: []byte{127, 0, 0, 2}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: \"test3\", Addr: []byte{127, 0, 0, 3}, Incarnation: 1, Vsn: m.config.BuildVsnArray()}\n\tm.aliveNode(&a3, nil, false)\n\n\ts := suspect{Node: \"test1\", Incarnation: 1}\n\tm.suspectNode(&s)\n\n\tremote := []pushNodeState{\n\t\tpushNodeState{\n\t\t\tName:        \"test1\",\n\t\t\tAddr:        []byte{127, 0, 0, 1},\n\t\t\tIncarnation: 2,\n\t\t\tState:       StateAlive,\n\t\t},\n\t\tpushNodeState{\n\t\t\tName:        \"test2\",\n\t\t\tAddr:        []byte{127, 0, 0, 2},\n\t\t\tIncarnation: 1,\n\t\t\tState:       StateSuspect,\n\t\t},\n\t\tpushNodeState{\n\t\t\tName:        \"test3\",\n\t\t\tAddr:        []byte{127, 0, 0, 3},\n\t\t\tIncarnation: 1,\n\t\t\tState:       StateDead,\n\t\t},\n\t\tpushNodeState{\n\t\t\tName:        \"test4\",\n\t\t\tAddr:        []byte{127, 0, 0, 4},\n\t\t\tIncarnation: 2,\n\t\t\tState:       StateAlive,\n\t\t},\n\t}\n\n\t// Listen for changes\n\teventCh := make(chan NodeEvent, 1)\n\tm.config.Events = &ChannelEventDelegate{eventCh}\n\n\t// Merge remote state\n\tm.mergeState(remote)\n\n\t// Check the states\n\tstate := m.nodeMap[\"test1\"]\n\tif state.State != StateAlive || state.Incarnation != 2 {\n\t\tt.Fatalf(\"Bad state %v\", state)\n\t}\n\n\tstate = m.nodeMap[\"test2\"]\n\tif state.State != StateSuspect || state.Incarnation != 1 {\n\t\tt.Fatalf(\"Bad state %v\", state)\n\t}\n\n\tstate = m.nodeMap[\"test3\"]\n\tif state.State != StateSuspect {\n\t\tt.Fatalf(\"Bad state %v\", state)\n\t}\n\n\tstate = m.nodeMap[\"test4\"]\n\tif state.State != StateAlive || state.Incarnation != 2 {\n\t\tt.Fatalf(\"Bad state %v\", state)\n\t}\n\n\t// Check the channels\n\tselect {\n\tcase e := <-eventCh:\n\t\tif e.Event != NodeJoin || e.Node.Name != \"test4\" {\n\t\t\tt.Fatalf(\"bad node %v\", e)\n\t\t}\n\tdefault:\n\t\tt.Fatalf(\"Expect join\")\n\t}\n\n\tselect {\n\tcase e := <-eventCh:\n\t\tt.Fatalf(\"Unexpect event: %v\", e)\n\tdefault:\n\t}\n}\n\nfunc TestMemberlist_Gossip(t *testing.T) {\n\tch := make(chan NodeEvent, 3)\n\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\taddr3 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\tip3 := []byte(addr3)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\t// Set the gossip interval fast enough to get a reasonable test,\n\t\t// but slow enough to avoid \"sendto: operation not permitted\"\n\t\tc.GossipInterval = 10 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t\t// Set the gossip interval fast enough to get a reasonable test,\n\t\t// but slow enough to avoid \"sendto: operation not permitted\"\n\t\tc.GossipInterval = 10 * time.Millisecond\n\t})\n\tdefer m2.Shutdown()\n\n\tm3 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t})\n\tdefer m3.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\ta3 := alive{Node: addr3.String(), Addr: ip3, Port: uint16(bindPort), Incarnation: 1, Vsn: m3.config.BuildVsnArray()}\n\tm1.aliveNode(&a3, nil, false)\n\n\t// Gossip should send all this to m2. Retry a few times because it's UDP and\n\t// timing and stuff makes this flaky without.\n\tretry(t, 15, 250*time.Millisecond, func(failf func(string, ...interface{})) {\n\t\tm1.gossip()\n\n\t\ttime.Sleep(3 * time.Millisecond)\n\n\t\tif len(ch) < 3 {\n\t\t\tfailf(\"expected 3 messages from gossip but only got %d\", len(ch))\n\t\t}\n\t})\n}\n\nfunc retry(t *testing.T, n int, w time.Duration, fn func(func(string, ...interface{}))) {\n\tt.Helper()\n\tfor try := 1; try <= n; try++ {\n\t\tfailed := false\n\t\tfailFormat := \"\"\n\t\tfailArgs := []interface{}{}\n\t\tfailf := func(format string, args ...interface{}) {\n\t\t\tfailed = true\n\t\t\tfailFormat = format\n\t\t\tfailArgs = args\n\t\t}\n\n\t\tfn(failf)\n\n\t\tif !failed {\n\t\t\treturn\n\t\t}\n\t\tif try == n {\n\t\t\tt.Fatalf(failFormat, failArgs...)\n\t\t}\n\t\ttime.Sleep(w)\n\t}\n}\n\nfunc TestMemberlist_GossipToDead(t *testing.T) {\n\tch := make(chan NodeEvent, 2)\n\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.GossipInterval = time.Millisecond\n\t\tc.GossipToTheDeadTime = 100 * time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t})\n\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\n\t// Shouldn't send anything to m2 here, node has been dead for 2x the GossipToTheDeadTime\n\tm1.nodeMap[addr2.String()].State = StateDead\n\tm1.nodeMap[addr2.String()].StateChange = time.Now().Add(-200 * time.Millisecond)\n\tm1.gossip()\n\n\tselect {\n\tcase <-ch:\n\t\tt.Fatalf(\"shouldn't get gossip\")\n\tcase <-time.After(50 * time.Millisecond):\n\t}\n\n\t// Should gossip to m2 because its state has changed within GossipToTheDeadTime\n\tm1.nodeMap[addr2.String()].StateChange = time.Now().Add(-20 * time.Millisecond)\n\n\tretry(t, 5, 10*time.Millisecond, func(failf func(string, ...interface{})) {\n\t\tm1.gossip()\n\n\t\ttime.Sleep(3 * time.Millisecond)\n\n\t\tif len(ch) < 2 {\n\t\t\tfailf(\"expected 2 messages from gossip\")\n\t\t}\n\t})\n}\n\nfunc TestMemberlist_FailedRemote(t *testing.T) {\n\ttype test struct {\n\t\tname     string\n\t\terr      error\n\t\texpected bool\n\t}\n\ttests := []test{\n\t\t{\"nil error\", nil, false},\n\t\t{\"normal error\", fmt.Errorf(\"\"), false},\n\t\t{\"net.OpError for file\", &net.OpError{Net: \"file\"}, false},\n\t\t{\"net.OpError for udp\", &net.OpError{Net: \"udp\"}, false},\n\t\t{\"net.OpError for udp4\", &net.OpError{Net: \"udp4\"}, false},\n\t\t{\"net.OpError for udp6\", &net.OpError{Net: \"udp6\"}, false},\n\t\t{\"net.OpError for tcp\", &net.OpError{Net: \"tcp\"}, false},\n\t\t{\"net.OpError for tcp4\", &net.OpError{Net: \"tcp4\"}, false},\n\t\t{\"net.OpError for tcp6\", &net.OpError{Net: \"tcp6\"}, false},\n\t\t{\"net.OpError for tcp with dial\", &net.OpError{Net: \"tcp\", Op: \"dial\"}, true},\n\t\t{\"net.OpError for tcp with write\", &net.OpError{Net: \"tcp\", Op: \"write\"}, true},\n\t\t{\"net.OpError for tcp with read\", &net.OpError{Net: \"tcp\", Op: \"read\"}, true},\n\t\t{\"net.OpError for udp with write\", &net.OpError{Net: \"udp\", Op: \"write\"}, true},\n\t\t{\"net.OpError for udp with read\", &net.OpError{Net: \"udp\", Op: \"read\"}, false},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tactual := failedRemote(test.err)\n\t\t\tif actual != test.expected {\n\t\t\t\tt.Fatalf(\"expected %t, got %t\", test.expected, actual)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMemberlist_PushPull(t *testing.T) {\n\taddr1 := getBindAddr()\n\taddr2 := getBindAddr()\n\tip1 := []byte(addr1)\n\tip2 := []byte(addr2)\n\n\tsink := registerInMemorySink(t)\n\n\tch := make(chan NodeEvent, 3)\n\n\tm1 := HostMemberlist(addr1.String(), t, func(c *Config) {\n\t\tc.GossipInterval = 10 * time.Second\n\t\tc.PushPullInterval = time.Millisecond\n\t})\n\tdefer m1.Shutdown()\n\n\tbindPort := m1.config.BindPort\n\n\tm2 := HostMemberlist(addr2.String(), t, func(c *Config) {\n\t\tc.BindPort = bindPort\n\t\tc.GossipInterval = 10 * time.Second\n\t\tc.Events = &ChannelEventDelegate{ch}\n\t})\n\tdefer m2.Shutdown()\n\n\ta1 := alive{Node: addr1.String(), Addr: ip1, Port: uint16(bindPort), Incarnation: 1, Vsn: m1.config.BuildVsnArray()}\n\tm1.aliveNode(&a1, nil, true)\n\ta2 := alive{Node: addr2.String(), Addr: ip2, Port: uint16(bindPort), Incarnation: 1, Vsn: m2.config.BuildVsnArray()}\n\tm1.aliveNode(&a2, nil, false)\n\n\t// Gossip should send all this to m2. It's UDP though so retry a few times\n\tretry(t, 5, 10*time.Millisecond, func(failf func(string, ...interface{})) {\n\t\tm1.pushPull()\n\n\t\ttime.Sleep(3 * time.Millisecond)\n\n\t\tif len(ch) < 2 {\n\t\t\tfailf(\"expected 2 messages from pushPull\")\n\t\t}\n\n\t\tinstancesMetricName := \"consul.usage.test.memberlist.node.instances\"\n\t\tverifyGaugeExists(t, \"consul.usage.test.memberlist.size.local\", sink)\n\t\tverifyGaugeExists(t, fmt.Sprintf(\"%s;node_state=%s\", instancesMetricName, StateAlive.metricsString()), sink)\n\t\tverifyGaugeExists(t, fmt.Sprintf(\"%s;node_state=%s\", instancesMetricName, StateDead.metricsString()), sink)\n\t\tverifyGaugeExists(t, fmt.Sprintf(\"%s;node_state=%s\", instancesMetricName, StateLeft.metricsString()), sink)\n\t\tverifyGaugeExists(t, fmt.Sprintf(\"%s;node_state=%s\", instancesMetricName, StateSuspect.metricsString()), sink)\n\t})\n}\n\nfunc TestVerifyProtocol(t *testing.T) {\n\tcases := []struct {\n\t\tAnodes   [][3]uint8\n\t\tBnodes   [][3]uint8\n\t\texpected bool\n\t}{\n\t\t// Both running identical everything\n\t\t{\n\t\t\tAnodes: [][3]uint8{\n\t\t\t\t{0, 0, 0},\n\t\t\t},\n\t\t\tBnodes: [][3]uint8{\n\t\t\t\t{0, 0, 0},\n\t\t\t},\n\t\t\texpected: true,\n\t\t},\n\n\t\t// One can understand newer, but speaking same protocol\n\t\t{\n\t\t\tAnodes: [][3]uint8{\n\t\t\t\t{0, 0, 0},\n\t\t\t},\n\t\t\tBnodes: [][3]uint8{\n\t\t\t\t{0, 1, 0},\n\t\t\t},\n\t\t\texpected: true,\n\t\t},\n\n\t\t// One is speaking outside the range\n\t\t{\n\t\t\tAnodes: [][3]uint8{\n\t\t\t\t{0, 0, 0},\n\t\t\t},\n\t\t\tBnodes: [][3]uint8{\n\t\t\t\t{1, 1, 1},\n\t\t\t},\n\t\t\texpected: false,\n\t\t},\n\n\t\t// Transitively outside the range\n\t\t{\n\t\t\tAnodes: [][3]uint8{\n\t\t\t\t{0, 1, 0},\n\t\t\t\t{0, 2, 1},\n\t\t\t},\n\t\t\tBnodes: [][3]uint8{\n\t\t\t\t{1, 3, 1},\n\t\t\t},\n\t\t\texpected: false,\n\t\t},\n\n\t\t// Multi-node\n\t\t{\n\t\t\tAnodes: [][3]uint8{\n\t\t\t\t{0, 3, 2},\n\t\t\t\t{0, 2, 0},\n\t\t\t},\n\t\t\tBnodes: [][3]uint8{\n\t\t\t\t{0, 2, 1},\n\t\t\t\t{0, 5, 0},\n\t\t\t},\n\t\t\texpected: true,\n\t\t},\n\t}\n\n\tfor _, tc := range cases {\n\t\taCore := make([][6]uint8, len(tc.Anodes))\n\t\taApp := make([][6]uint8, len(tc.Anodes))\n\t\tfor i, n := range tc.Anodes {\n\t\t\taCore[i] = [6]uint8{n[0], n[1], n[2], 0, 0, 0}\n\t\t\taApp[i] = [6]uint8{0, 0, 0, n[0], n[1], n[2]}\n\t\t}\n\n\t\tbCore := make([][6]uint8, len(tc.Bnodes))\n\t\tbApp := make([][6]uint8, len(tc.Bnodes))\n\t\tfor i, n := range tc.Bnodes {\n\t\t\tbCore[i] = [6]uint8{n[0], n[1], n[2], 0, 0, 0}\n\t\t\tbApp[i] = [6]uint8{0, 0, 0, n[0], n[1], n[2]}\n\t\t}\n\n\t\t// Test core protocol verification\n\t\ttestVerifyProtocolSingle(t, aCore, bCore, tc.expected)\n\t\ttestVerifyProtocolSingle(t, bCore, aCore, tc.expected)\n\n\t\t//  Test app protocol verification\n\t\ttestVerifyProtocolSingle(t, aApp, bApp, tc.expected)\n\t\ttestVerifyProtocolSingle(t, bApp, aApp, tc.expected)\n\t}\n}\n\nfunc testVerifyProtocolSingle(t *testing.T, A [][6]uint8, B [][6]uint8, expect bool) {\n\tm := GetMemberlist(t, nil)\n\tdefer m.Shutdown()\n\n\tm.nodes = make([]*nodeState, len(A))\n\tfor i, n := range A {\n\t\tm.nodes[i] = &nodeState{\n\t\t\tNode: Node{\n\t\t\t\tPMin: n[0],\n\t\t\t\tPMax: n[1],\n\t\t\t\tPCur: n[2],\n\t\t\t\tDMin: n[3],\n\t\t\t\tDMax: n[4],\n\t\t\t\tDCur: n[5],\n\t\t\t},\n\t\t}\n\t}\n\n\tremote := make([]pushNodeState, len(B))\n\tfor i, n := range B {\n\t\tremote[i] = pushNodeState{\n\t\t\tName: fmt.Sprintf(\"node %d\", i),\n\t\t\tVsn:  []uint8{n[0], n[1], n[2], n[3], n[4], n[5]},\n\t\t}\n\t}\n\n\terr := m.verifyProtocol(remote)\n\tif (err == nil) != expect {\n\t\tt.Fatalf(\"bad:\\nA: %v\\nB: %v\\nErr: %s\", A, B, err)\n\t}\n}\n\nfunc registerInMemorySink(t *testing.T) *metrics.InmemSink {\n\tt.Helper()\n\t// Only have a single interval for the test\n\tsink := metrics.NewInmemSink(1*time.Minute, 1*time.Minute)\n\tcfg := metrics.DefaultConfig(\"consul.usage.test\")\n\tcfg.EnableHostname = false\n\tmetrics.NewGlobal(cfg, sink)\n\treturn sink\n}\n\nfunc getIntervalMetrics(t *testing.T, sink *metrics.InmemSink) *metrics.IntervalMetrics {\n\tt.Helper()\n\tintervals := sink.Data()\n\trequire.Len(t, intervals, 1)\n\tintv := intervals[0]\n\treturn intv\n}\n\nfunc verifyGaugeExists(t *testing.T, name string, sink *metrics.InmemSink) {\n\tt.Helper()\n\tinterval := getIntervalMetrics(t, sink)\n\tinterval.RLock()\n\tdefer interval.RUnlock()\n\tif _, ok := interval.Gauges[name]; !ok {\n\t\tt.Fatalf(\"%s gauge not emmited\", name)\n\t}\n}\n\nfunc verifySampleExists(t *testing.T, name string, sink *metrics.InmemSink) {\n\tt.Helper()\n\tinterval := getIntervalMetrics(t, sink)\n\tinterval.RLock()\n\tdefer interval.RUnlock()\n\n\tif _, ok := interval.Samples[name]; !ok {\n\t\tt.Fatalf(\"%s sample not emmited\", name)\n\t}\n}\n"
        },
        {
          "name": "suspicion.go",
          "type": "blob",
          "size": 4.265625,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"math\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// suspicion manages the suspect timer for a node and provides an interface\n// to accelerate the timeout as we get more independent confirmations that\n// a node is suspect.\ntype suspicion struct {\n\t// n is the number of independent confirmations we've seen. This must\n\t// be updated using atomic instructions to prevent contention with the\n\t// timer callback.\n\tn int32\n\n\t// k is the number of independent confirmations we'd like to see in\n\t// order to drive the timer to its minimum value.\n\tk int32\n\n\t// min is the minimum timer value.\n\tmin time.Duration\n\n\t// max is the maximum timer value.\n\tmax time.Duration\n\n\t// start captures the timestamp when we began the timer. This is used\n\t// so we can calculate durations to feed the timer during updates in\n\t// a way the achieves the overall time we'd like.\n\tstart time.Time\n\n\t// timer is the underlying timer that implements the timeout.\n\ttimer *time.Timer\n\n\t// f is the function to call when the timer expires. We hold on to this\n\t// because there are cases where we call it directly.\n\ttimeoutFn func()\n\n\t// confirmations is a map of \"from\" nodes that have confirmed a given\n\t// node is suspect. This prevents double counting.\n\tconfirmations map[string]struct{}\n}\n\n// newSuspicion returns a timer started with the max time, and that will drive\n// to the min time after seeing k or more confirmations. The from node will be\n// excluded from confirmations since we might get our own suspicion message\n// gossiped back to us. The minimum time will be used if no confirmations are\n// called for (k <= 0).\nfunc newSuspicion(from string, k int, min time.Duration, max time.Duration, fn func(int)) *suspicion {\n\ts := &suspicion{\n\t\tk:             int32(k),\n\t\tmin:           min,\n\t\tmax:           max,\n\t\tconfirmations: make(map[string]struct{}),\n\t}\n\n\t// Exclude the from node from any confirmations.\n\ts.confirmations[from] = struct{}{}\n\n\t// Pass the number of confirmations into the timeout function for\n\t// easy telemetry.\n\ts.timeoutFn = func() {\n\t\tfn(int(atomic.LoadInt32(&s.n)))\n\t}\n\n\t// If there aren't any confirmations to be made then take the min\n\t// time from the start.\n\ttimeout := max\n\tif k < 1 {\n\t\ttimeout = min\n\t}\n\ts.timer = time.AfterFunc(timeout, s.timeoutFn)\n\n\t// Capture the start time right after starting the timer above so\n\t// we should always err on the side of a little longer timeout if\n\t// there's any preemption that separates this and the step above.\n\ts.start = time.Now()\n\treturn s\n}\n\n// remainingSuspicionTime takes the state variables of the suspicion timer and\n// calculates the remaining time to wait before considering a node dead. The\n// return value can be negative, so be prepared to fire the timer immediately in\n// that case.\nfunc remainingSuspicionTime(n, k int32, elapsed time.Duration, min, max time.Duration) time.Duration {\n\tfrac := math.Log(float64(n)+1.0) / math.Log(float64(k)+1.0)\n\traw := max.Seconds() - frac*(max.Seconds()-min.Seconds())\n\ttimeout := time.Duration(math.Floor(1000.0*raw)) * time.Millisecond\n\tif timeout < min {\n\t\ttimeout = min\n\t}\n\n\t// We have to take into account the amount of time that has passed so\n\t// far, so we get the right overall timeout.\n\treturn timeout - elapsed\n}\n\n// Confirm registers that a possibly new peer has also determined the given\n// node is suspect. This returns true if this was new information, and false\n// if it was a duplicate confirmation, or if we've got enough confirmations to\n// hit the minimum.\nfunc (s *suspicion) Confirm(from string) bool {\n\t// If we've got enough confirmations then stop accepting them.\n\tif atomic.LoadInt32(&s.n) >= s.k {\n\t\treturn false\n\t}\n\n\t// Only allow one confirmation from each possible peer.\n\tif _, ok := s.confirmations[from]; ok {\n\t\treturn false\n\t}\n\ts.confirmations[from] = struct{}{}\n\n\t// Compute the new timeout given the current number of confirmations and\n\t// adjust the timer. If the timeout becomes negative *and* we can cleanly\n\t// stop the timer then we will call the timeout function directly from\n\t// here.\n\tn := atomic.AddInt32(&s.n, 1)\n\telapsed := time.Since(s.start)\n\tremaining := remainingSuspicionTime(n, s.k, elapsed, s.min, s.max)\n\tif s.timer.Stop() {\n\t\tif remaining > 0 {\n\t\t\ts.timer.Reset(remaining)\n\t\t} else {\n\t\t\tgo s.timeoutFn()\n\t\t}\n\t}\n\treturn true\n}\n"
        },
        {
          "name": "suspicion_test.go",
          "type": "blob",
          "size": 4.4599609375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestSuspicion_remainingSuspicionTime(t *testing.T) {\n\tcases := []struct {\n\t\tn        int32\n\t\tk        int32\n\t\telapsed  time.Duration\n\t\tmin      time.Duration\n\t\tmax      time.Duration\n\t\texpected time.Duration\n\t}{\n\t\t{0, 3, 0, 2 * time.Second, 30 * time.Second, 30 * time.Second},\n\t\t{1, 3, 2 * time.Second, 2 * time.Second, 30 * time.Second, 14 * time.Second},\n\t\t{2, 3, 3 * time.Second, 2 * time.Second, 30 * time.Second, 4810 * time.Millisecond},\n\t\t{3, 3, 4 * time.Second, 2 * time.Second, 30 * time.Second, -2 * time.Second},\n\t\t{4, 3, 5 * time.Second, 2 * time.Second, 30 * time.Second, -3 * time.Second},\n\t\t{5, 3, 10 * time.Second, 2 * time.Second, 30 * time.Second, -8 * time.Second},\n\t}\n\tfor i, c := range cases {\n\t\tremaining := remainingSuspicionTime(c.n, c.k, c.elapsed, c.min, c.max)\n\t\tif remaining != c.expected {\n\t\t\tt.Errorf(\"case %d: remaining %9.6f != expected %9.6f\", i, remaining.Seconds(), c.expected.Seconds())\n\t\t}\n\t}\n}\n\nfunc TestSuspicion_Timer(t *testing.T) {\n\tconst k = 3\n\tconst min = 500 * time.Millisecond\n\tconst max = 2 * time.Second\n\n\ttype pair struct {\n\t\tfrom    string\n\t\tnewInfo bool\n\t}\n\tcases := []struct {\n\t\tnumConfirmations int\n\t\tfrom             string\n\t\tconfirmations    []pair\n\t\texpected         time.Duration\n\t}{\n\t\t{\n\t\t\t0,\n\t\t\t\"me\",\n\t\t\t[]pair{},\n\t\t\tmax,\n\t\t},\n\t\t{\n\t\t\t1,\n\t\t\t\"me\",\n\t\t\t[]pair{\n\t\t\t\tpair{\"me\", false},\n\t\t\t\tpair{\"foo\", true},\n\t\t\t},\n\t\t\t1250 * time.Millisecond,\n\t\t},\n\t\t{\n\t\t\t1,\n\t\t\t\"me\",\n\t\t\t[]pair{\n\t\t\t\tpair{\"me\", false},\n\t\t\t\tpair{\"foo\", true},\n\t\t\t\tpair{\"foo\", false},\n\t\t\t\tpair{\"foo\", false},\n\t\t\t},\n\t\t\t1250 * time.Millisecond,\n\t\t},\n\t\t{\n\t\t\t2,\n\t\t\t\"me\",\n\t\t\t[]pair{\n\t\t\t\tpair{\"me\", false},\n\t\t\t\tpair{\"foo\", true},\n\t\t\t\tpair{\"bar\", true},\n\t\t\t},\n\t\t\t810 * time.Millisecond,\n\t\t},\n\t\t{\n\t\t\t3,\n\t\t\t\"me\",\n\t\t\t[]pair{\n\t\t\t\tpair{\"me\", false},\n\t\t\t\tpair{\"foo\", true},\n\t\t\t\tpair{\"bar\", true},\n\t\t\t\tpair{\"baz\", true},\n\t\t\t},\n\t\t\tmin,\n\t\t},\n\t\t{\n\t\t\t3,\n\t\t\t\"me\",\n\t\t\t[]pair{\n\t\t\t\tpair{\"me\", false},\n\t\t\t\tpair{\"foo\", true},\n\t\t\t\tpair{\"bar\", true},\n\t\t\t\tpair{\"baz\", true},\n\t\t\t\tpair{\"zoo\", false},\n\t\t\t},\n\t\t\tmin,\n\t\t},\n\t}\n\tfor i, c := range cases {\n\t\tch := make(chan time.Duration, 1)\n\t\tstart := time.Now()\n\t\tf := func(numConfirmations int) {\n\t\t\tif numConfirmations != c.numConfirmations {\n\t\t\t\tt.Errorf(\"case %d: bad %d != %d\", i, numConfirmations, c.numConfirmations)\n\t\t\t}\n\n\t\t\tch <- time.Now().Sub(start)\n\t\t}\n\n\t\t// Create the timer and add the requested confirmations. Wait\n\t\t// the fudge amount to help make sure we calculate the timeout\n\t\t// overall, and don't accumulate extra time.\n\t\ts := newSuspicion(c.from, k, min, max, f)\n\t\tfudge := 25 * time.Millisecond\n\t\tfor _, p := range c.confirmations {\n\t\t\ttime.Sleep(fudge)\n\t\t\tif s.Confirm(p.from) != p.newInfo {\n\t\t\t\tt.Fatalf(\"case %d: newInfo mismatch for %s\", i, p.from)\n\t\t\t}\n\t\t}\n\n\t\t// Wait until right before the timeout and make sure the\n\t\t// timer hasn't fired.\n\t\talready := time.Duration(len(c.confirmations)) * fudge\n\t\ttime.Sleep(c.expected - already - fudge)\n\t\tselect {\n\t\tcase d := <-ch:\n\t\t\tt.Fatalf(\"case %d: should not have fired (%9.6f)\", i, d.Seconds())\n\t\tdefault:\n\t\t}\n\n\t\t// Wait through the timeout and a little after and make sure it\n\t\t// fires.\n\t\ttime.Sleep(2 * fudge)\n\t\tselect {\n\t\tcase <-ch:\n\t\tdefault:\n\t\t\tt.Fatalf(\"case %d: should have fired\", i)\n\t\t}\n\n\t\t// Confirm after to make sure it handles a negative remaining\n\t\t// time correctly and doesn't fire again.\n\t\ts.Confirm(\"late\")\n\t\ttime.Sleep(c.expected + 2*fudge)\n\t\tselect {\n\t\tcase d := <-ch:\n\t\t\tt.Fatalf(\"case %d: should not have fired (%9.6f)\", i, d.Seconds())\n\t\tdefault:\n\t\t}\n\t}\n}\n\nfunc TestSuspicion_Timer_ZeroK(t *testing.T) {\n\tch := make(chan struct{}, 1)\n\tf := func(int) {\n\t\tch <- struct{}{}\n\t}\n\n\t// This should select the min time since there are no expected\n\t// confirmations to accelerate the timer.\n\ts := newSuspicion(\"me\", 0, 25*time.Millisecond, 30*time.Second, f)\n\tif s.Confirm(\"foo\") {\n\t\tt.Fatalf(\"should not provide new information\")\n\t}\n\n\tselect {\n\tcase <-ch:\n\tcase <-time.After(50 * time.Millisecond):\n\t\tt.Fatalf(\"should have fired\")\n\t}\n}\n\nfunc TestSuspicion_Timer_Immediate(t *testing.T) {\n\tch := make(chan struct{}, 1)\n\tf := func(int) {\n\t\tch <- struct{}{}\n\t}\n\n\t// This should underflow the timeout and fire immediately.\n\ts := newSuspicion(\"me\", 1, 100*time.Millisecond, 30*time.Second, f)\n\ttime.Sleep(200 * time.Millisecond)\n\ts.Confirm(\"foo\")\n\n\t// Wait a little while since the function gets called in a goroutine.\n\tselect {\n\tcase <-ch:\n\tcase <-time.After(25 * time.Millisecond):\n\t\tt.Fatalf(\"should have fired\")\n\t}\n}\n"
        },
        {
          "name": "tag.sh",
          "type": "blob",
          "size": 0.4560546875,
          "content": "#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: MPL-2.0\n\nset -e\n\n# The version must be supplied from the environment. Do not include the\n# leading \"v\".\nif [ -z $VERSION ]; then\n    echo \"Please specify a version.\"\n    exit 1\nfi\n\n# Generate the tag.\necho \"==> Tagging version $VERSION...\"\ngit commit --allow-empty -a --gpg-sign=348FFC4C -m \"Release v$VERSION\"\ngit tag -a -m \"Version $VERSION\" -s -u 348FFC4C \"v${VERSION}\" master\n\nexit 0\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "todo.md",
          "type": "blob",
          "size": 0.2060546875,
          "content": "# TODO\n* Dynamic RTT discovery\n    * Compute 99th percentile for ping/ack\n    * Better lower bound for ping/ack, faster failure detection\n* Dynamic MTU discovery\n    * Prevent lost updates, increases efficiency\n"
        },
        {
          "name": "transport.go",
          "type": "blob",
          "size": 5.4404296875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n)\n\n// Packet is used to provide some metadata about incoming packets from peers\n// over a packet connection, as well as the packet payload.\ntype Packet struct {\n\t// Buf has the raw contents of the packet.\n\tBuf []byte\n\n\t// From has the address of the peer. This is an actual net.Addr so we\n\t// can expose some concrete details about incoming packets.\n\tFrom net.Addr\n\n\t// Timestamp is the time when the packet was received. This should be\n\t// taken as close as possible to the actual receipt time to help make an\n\t// accurate RTT measurement during probes.\n\tTimestamp time.Time\n}\n\n// Transport is used to abstract over communicating with other peers. The packet\n// interface is assumed to be best-effort and the stream interface is assumed to\n// be reliable.\ntype Transport interface {\n\t// FinalAdvertiseAddr is given the user's configured values (which\n\t// might be empty) and returns the desired IP and port to advertise to\n\t// the rest of the cluster.\n\tFinalAdvertiseAddr(ip string, port int) (net.IP, int, error)\n\n\t// WriteTo is a packet-oriented interface that fires off the given\n\t// payload to the given address in a connectionless fashion. This should\n\t// return a time stamp that's as close as possible to when the packet\n\t// was transmitted to help make accurate RTT measurements during probes.\n\t//\n\t// This is similar to net.PacketConn, though we didn't want to expose\n\t// that full set of required methods to keep assumptions about the\n\t// underlying plumbing to a minimum. We also treat the address here as a\n\t// string, similar to Dial, so it's network neutral, so this usually is\n\t// in the form of \"host:port\".\n\tWriteTo(b []byte, addr string) (time.Time, error)\n\n\t// PacketCh returns a channel that can be read to receive incoming\n\t// packets from other peers. How this is set up for listening is left as\n\t// an exercise for the concrete transport implementations.\n\tPacketCh() <-chan *Packet\n\n\t// DialTimeout is used to create a connection that allows us to perform\n\t// two-way communication with a peer. This is generally more expensive\n\t// than packet connections so is used for more infrequent operations\n\t// such as anti-entropy or fallback probes if the packet-oriented probe\n\t// failed.\n\tDialTimeout(addr string, timeout time.Duration) (net.Conn, error)\n\n\t// StreamCh returns a channel that can be read to handle incoming stream\n\t// connections from other peers. How this is set up for listening is\n\t// left as an exercise for the concrete transport implementations.\n\tStreamCh() <-chan net.Conn\n\n\t// Shutdown is called when memberlist is shutting down; this gives the\n\t// transport a chance to clean up any listeners.\n\tShutdown() error\n}\n\ntype Address struct {\n\t// Addr is a network address as a string, similar to Dial. This usually is\n\t// in the form of \"host:port\". This is required.\n\tAddr string\n\n\t// Name is the name of the node being addressed. This is optional but\n\t// transports may require it.\n\tName string\n}\n\nfunc (a *Address) String() string {\n\tif a.Name != \"\" {\n\t\treturn fmt.Sprintf(\"%s (%s)\", a.Name, a.Addr)\n\t}\n\treturn a.Addr\n}\n\n// IngestionAwareTransport is not used.\n//\n// Deprecated: IngestionAwareTransport is not used and may be removed in a future\n// version. Define the interface locally instead of referencing this exported\n// interface.\ntype IngestionAwareTransport interface {\n\tIngestPacket(conn net.Conn, addr net.Addr, now time.Time, shouldClose bool) error\n\tIngestStream(conn net.Conn) error\n}\n\ntype NodeAwareTransport interface {\n\tTransport\n\tWriteToAddress(b []byte, addr Address) (time.Time, error)\n\tDialAddressTimeout(addr Address, timeout time.Duration) (net.Conn, error)\n}\n\ntype shimNodeAwareTransport struct {\n\tTransport\n}\n\nvar _ NodeAwareTransport = (*shimNodeAwareTransport)(nil)\n\nfunc (t *shimNodeAwareTransport) WriteToAddress(b []byte, addr Address) (time.Time, error) {\n\treturn t.WriteTo(b, addr.Addr)\n}\n\nfunc (t *shimNodeAwareTransport) DialAddressTimeout(addr Address, timeout time.Duration) (net.Conn, error) {\n\treturn t.DialTimeout(addr.Addr, timeout)\n}\n\ntype labelWrappedTransport struct {\n\tlabel string\n\tNodeAwareTransport\n}\n\nvar _ NodeAwareTransport = (*labelWrappedTransport)(nil)\n\nfunc (t *labelWrappedTransport) WriteToAddress(buf []byte, addr Address) (time.Time, error) {\n\tvar err error\n\tbuf, err = AddLabelHeaderToPacket(buf, t.label)\n\tif err != nil {\n\t\treturn time.Time{}, fmt.Errorf(\"failed to add label header to packet: %w\", err)\n\t}\n\treturn t.NodeAwareTransport.WriteToAddress(buf, addr)\n}\n\nfunc (t *labelWrappedTransport) WriteTo(buf []byte, addr string) (time.Time, error) {\n\tvar err error\n\tbuf, err = AddLabelHeaderToPacket(buf, t.label)\n\tif err != nil {\n\t\treturn time.Time{}, err\n\t}\n\treturn t.NodeAwareTransport.WriteTo(buf, addr)\n}\n\nfunc (t *labelWrappedTransport) DialAddressTimeout(addr Address, timeout time.Duration) (net.Conn, error) {\n\tconn, err := t.NodeAwareTransport.DialAddressTimeout(addr, timeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := AddLabelHeaderToStream(conn, t.label); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to add label header to stream: %w\", err)\n\t}\n\treturn conn, nil\n}\n\nfunc (t *labelWrappedTransport) DialTimeout(addr string, timeout time.Duration) (net.Conn, error) {\n\tconn, err := t.NodeAwareTransport.DialTimeout(addr, timeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := AddLabelHeaderToStream(conn, t.label); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to add label header to stream: %w\", err)\n\t}\n\treturn conn, nil\n}\n"
        },
        {
          "name": "transport_test.go",
          "type": "blob",
          "size": 5.2607421875,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"log\"\n\t\"net\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestTransport_Join(t *testing.T) {\n\tnet := &MockNetwork{}\n\n\tt1 := net.NewTransport(\"node1\")\n\n\tc1 := DefaultLANConfig()\n\tc1.Name = \"node1\"\n\tc1.Transport = t1\n\tm1, err := Create(c1)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tm1.setAlive()\n\tm1.schedule()\n\tdefer m1.Shutdown()\n\n\tc2 := DefaultLANConfig()\n\tc2.Name = \"node2\"\n\tc2.Transport = net.NewTransport(\"node2\")\n\tm2, err := Create(c2)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tm2.setAlive()\n\tm2.schedule()\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{c1.Name + \"/\" + t1.addr.String()})\n\tif num != 1 {\n\t\tt.Fatalf(\"bad: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tif len(m2.Members()) != 2 {\n\t\tt.Fatalf(\"bad: %v\", m2.Members())\n\t}\n\tif m2.estNumNodes() != 2 {\n\t\tt.Fatalf(\"bad: %v\", m2.Members())\n\t}\n\n}\n\nfunc TestTransport_Send(t *testing.T) {\n\tnet := &MockNetwork{}\n\n\tt1 := net.NewTransport(\"node1\")\n\td1 := &MockDelegate{}\n\n\tc1 := DefaultLANConfig()\n\tc1.Name = \"node1\"\n\tc1.Transport = t1\n\tc1.Delegate = d1\n\tm1, err := Create(c1)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tm1.setAlive()\n\tm1.schedule()\n\tdefer m1.Shutdown()\n\n\tc2 := DefaultLANConfig()\n\tc2.Name = \"node2\"\n\tc2.Transport = net.NewTransport(\"node2\")\n\tm2, err := Create(c2)\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tm2.setAlive()\n\tm2.schedule()\n\tdefer m2.Shutdown()\n\n\tnum, err := m2.Join([]string{c1.Name + \"/\" + t1.addr.String()})\n\tif num != 1 {\n\t\tt.Fatalf(\"bad: %d\", num)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tif err := m2.SendTo(t1.addr, []byte(\"SendTo\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\n\tvar n1 *Node\n\tfor _, n := range m2.Members() {\n\t\tif n.Name == c1.Name {\n\t\t\tn1 = n\n\t\t\tbreak\n\t\t}\n\t}\n\tif n1 == nil {\n\t\tt.Fatalf(\"bad\")\n\t}\n\n\tif err := m2.SendToUDP(n1, []byte(\"SendToUDP\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tif err := m2.SendToTCP(n1, []byte(\"SendToTCP\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tif err := m2.SendBestEffort(n1, []byte(\"SendBestEffort\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\tif err := m2.SendReliable(n1, []byte(\"SendReliable\")); err != nil {\n\t\tt.Fatalf(\"err: %v\", err)\n\t}\n\ttime.Sleep(100 * time.Millisecond)\n\n\texpected := []string{\"SendTo\", \"SendToUDP\", \"SendToTCP\", \"SendBestEffort\", \"SendReliable\"}\n\n\tmsgs1 := d1.getMessages()\n\n\treceived := make([]string, len(msgs1))\n\tfor i, bs := range msgs1 {\n\t\treceived[i] = string(bs)\n\t}\n\t// Some of these are UDP so often get re-ordered making the test flaky if we\n\t// assert send ordering. Sort both slices to be tolerant of re-ordering.\n\trequire.ElementsMatch(t, expected, received)\n}\n\ntype testCountingWriter struct {\n\tt        *testing.T\n\tnumCalls *int32\n}\n\nfunc (tw testCountingWriter) Write(p []byte) (n int, err error) {\n\tatomic.AddInt32(tw.numCalls, 1)\n\tif !strings.Contains(string(p), \"memberlist: Error accepting TCP connection\") {\n\t\ttw.t.Error(\"did not receive expected log message\")\n\t}\n\ttw.t.Log(\"countingWriter:\", string(p))\n\treturn len(p), nil\n}\n\n// TestTransport_TcpListenBackoff tests that AcceptTCP() errors in NetTransport#tcpListen()\n// do not result in a tight loop and spam the log. We verify this here by counting the number\n// of entries logged in a given time period.\nfunc TestTransport_TcpListenBackoff(t *testing.T) {\n\n\t// testTime is the amount of time we will allow NetTransport#tcpListen() to run\n\t// This needs to be long enough that to verify that maxDelay is in force,\n\t// but not so long as to be obnoxious when running the test suite.\n\tconst testTime = 4 * time.Second\n\n\tvar numCalls int32\n\tcountingWriter := testCountingWriter{t, &numCalls}\n\tcountingLogger := log.New(countingWriter, \"test\", log.LstdFlags)\n\ttransport := NetTransport{\n\t\tstreamCh: make(chan net.Conn),\n\t\tlogger:   countingLogger,\n\t}\n\ttransport.wg.Add(1)\n\n\t// create a listener that will cause AcceptTCP calls to fail\n\tlistener, _ := net.ListenTCP(\"tcp\", nil)\n\tlistener.Close()\n\tgo transport.tcpListen(listener)\n\n\t// sleep (+yield) for testTime seconds before asking the accept loop to shut down\n\ttime.Sleep(testTime)\n\tatomic.StoreInt32(&transport.shutdown, 1)\n\n\t// Verify that the wg was completed on exit (but without blocking this test)\n\t// maxDelay == 1s, so we will give the routine 1.25s to loop around and shut down.\n\tc := make(chan struct{})\n\tgo func() {\n\t\tdefer close(c)\n\t\ttransport.wg.Wait()\n\t}()\n\tselect {\n\tcase <-c:\n\tcase <-time.After(1250 * time.Millisecond):\n\t\tt.Error(\"timed out waiting for transport waitgroup to be done after flagging shutdown\")\n\t}\n\n\t// In testTime==4s, we expect to loop approximately 12 times (and log approximately 11 errors),\n\t// with the following delays (in ms):\n\t//   0+5+10+20+40+80+160+320+640+1000+1000+1000 == 4275 ms\n\t// Too few calls suggests that the minDelay is not in force; too many calls suggests that the\n\t// maxDelay is not in force or that the back-off isn't working at all.\n\t// We'll leave a little flex; the important thing here is the asymptotic behavior.\n\t// If the minDelay or maxDelay in NetTransport#tcpListen() are modified, this test may fail\n\t// and need to be adjusted.\n\trequire.True(t, numCalls > 8)\n\trequire.True(t, numCalls < 14)\n\n\t// no connections should have been accepted and sent to the channel\n\trequire.Equal(t, len(transport.streamCh), 0)\n}\n"
        },
        {
          "name": "util.go",
          "type": "blob",
          "size": 8.7392578125,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"bytes\"\n\t\"compress/lzw\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-msgpack/v2/codec\"\n\t\"github.com/sean-/seed\"\n)\n\n// pushPullScale is the minimum number of nodes\n// before we start scaling the push/pull timing. The scale\n// effect is the log2(Nodes) - log2(pushPullScale). This means\n// that the 33rd node will cause us to double the interval,\n// while the 65th will triple it.\nconst pushPullScaleThreshold = 32\n\nconst (\n\t// Constant litWidth 2-8\n\tlzwLitWidth = 8\n)\n\nfunc init() {\n\tseed.Init()\n}\n\n// Decode reverses the encode operation on a byte slice input\nfunc decode(buf []byte, out interface{}) error {\n\tr := bytes.NewReader(buf)\n\thd := codec.MsgpackHandle{}\n\tdec := codec.NewDecoder(r, &hd)\n\treturn dec.Decode(out)\n}\n\n// Encode writes an encoded object to a new bytes buffer\nfunc encode(msgType messageType, in interface{}, msgpackUseNewTimeFormat bool) (*bytes.Buffer, error) {\n\tbuf := bytes.NewBuffer(nil)\n\tbuf.WriteByte(uint8(msgType))\n\thd := codec.MsgpackHandle{\n\t\tBasicHandle: codec.BasicHandle{\n\t\t\tTimeNotBuiltin: !msgpackUseNewTimeFormat,\n\t\t},\n\t}\n\tenc := codec.NewEncoder(buf, &hd)\n\terr := enc.Encode(in)\n\treturn buf, err\n}\n\n// Returns a random offset between 0 and n\nfunc randomOffset(n int) int {\n\tif n == 0 {\n\t\treturn 0\n\t}\n\treturn int(rand.Uint32() % uint32(n))\n}\n\n// suspicionTimeout computes the timeout that should be used when\n// a node is suspected\nfunc suspicionTimeout(suspicionMult, n int, interval time.Duration) time.Duration {\n\tnodeScale := math.Max(1.0, math.Log10(math.Max(1.0, float64(n))))\n\t// multiply by 1000 to keep some precision because time.Duration is an int64 type\n\ttimeout := time.Duration(suspicionMult) * time.Duration(nodeScale*1000) * interval / 1000\n\treturn timeout\n}\n\n// retransmitLimit computes the limit of retransmissions\nfunc retransmitLimit(retransmitMult, n int) int {\n\tnodeScale := math.Ceil(math.Log10(float64(n + 1)))\n\tlimit := retransmitMult * int(nodeScale)\n\treturn limit\n}\n\n// shuffleNodes randomly shuffles the input nodes using the Fisher-Yates shuffle\nfunc shuffleNodes(nodes []*nodeState) {\n\tn := len(nodes)\n\trand.Shuffle(n, func(i, j int) {\n\t\tnodes[i], nodes[j] = nodes[j], nodes[i]\n\t})\n}\n\n// pushPushScale is used to scale the time interval at which push/pull\n// syncs take place. It is used to prevent network saturation as the\n// cluster size grows\nfunc pushPullScale(interval time.Duration, n int) time.Duration {\n\t// Don't scale until we cross the threshold\n\tif n <= pushPullScaleThreshold {\n\t\treturn interval\n\t}\n\n\tmultiplier := math.Ceil(math.Log2(float64(n))-math.Log2(pushPullScaleThreshold)) + 1.0\n\treturn time.Duration(multiplier) * interval\n}\n\n// moveDeadNodes moves dead and left nodes that that have not changed during the gossipToTheDeadTime interval\n// to the end of the slice and returns the index of the first moved node.\nfunc moveDeadNodes(nodes []*nodeState, gossipToTheDeadTime time.Duration) int {\n\tnumDead := 0\n\tn := len(nodes)\n\tfor i := 0; i < n-numDead; i++ {\n\t\tif !nodes[i].DeadOrLeft() {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Respect the gossip to the dead interval\n\t\tif time.Since(nodes[i].StateChange) <= gossipToTheDeadTime {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Move this node to the end\n\t\tnodes[i], nodes[n-numDead-1] = nodes[n-numDead-1], nodes[i]\n\t\tnumDead++\n\t\ti--\n\t}\n\treturn n - numDead\n}\n\n// kRandomNodes is used to select up to k random Nodes, excluding any nodes where\n// the exclude function returns true. It is possible that less than k nodes are\n// returned.\nfunc kRandomNodes(k int, nodes []*nodeState, exclude func(*nodeState) bool) []Node {\n\tn := len(nodes)\n\tkNodes := make([]Node, 0, k)\nOUTER:\n\t// Probe up to 3*n times, with large n this is not necessary\n\t// since k << n, but with small n we want search to be\n\t// exhaustive\n\tfor i := 0; i < 3*n && len(kNodes) < k; i++ {\n\t\t// Get random nodeState\n\t\tidx := randomOffset(n)\n\t\tstate := nodes[idx]\n\n\t\t// Give the filter a shot at it.\n\t\tif exclude != nil && exclude(state) {\n\t\t\tcontinue OUTER\n\t\t}\n\n\t\t// Check if we have this node already\n\t\tfor j := 0; j < len(kNodes); j++ {\n\t\t\tif state.Node.Name == kNodes[j].Name {\n\t\t\t\tcontinue OUTER\n\t\t\t}\n\t\t}\n\n\t\t// Append the node\n\t\tkNodes = append(kNodes, state.Node)\n\t}\n\treturn kNodes\n}\n\n// makeCompoundMessages takes a list of messages and packs\n// them into one or multiple messages based on the limitations\n// of compound messages (255 messages each).\nfunc makeCompoundMessages(msgs [][]byte) []*bytes.Buffer {\n\tconst maxMsgs = 255\n\tbufs := make([]*bytes.Buffer, 0, (len(msgs)+(maxMsgs-1))/maxMsgs)\n\n\tfor ; len(msgs) > maxMsgs; msgs = msgs[maxMsgs:] {\n\t\tbufs = append(bufs, makeCompoundMessage(msgs[:maxMsgs]))\n\t}\n\tif len(msgs) > 0 {\n\t\tbufs = append(bufs, makeCompoundMessage(msgs))\n\t}\n\n\treturn bufs\n}\n\n// makeCompoundMessage takes a list of messages and generates\n// a single compound message containing all of them\nfunc makeCompoundMessage(msgs [][]byte) *bytes.Buffer {\n\t// Create a local buffer\n\tbuf := bytes.NewBuffer(nil)\n\n\t// Write out the type\n\tbuf.WriteByte(uint8(compoundMsg))\n\n\t// Write out the number of message\n\tbuf.WriteByte(uint8(len(msgs)))\n\n\t// Add the message lengths\n\tfor _, m := range msgs {\n\t\tbinary.Write(buf, binary.BigEndian, uint16(len(m)))\n\t}\n\n\t// Append the messages\n\tfor _, m := range msgs {\n\t\tbuf.Write(m)\n\t}\n\n\treturn buf\n}\n\n// decodeCompoundMessage splits a compound message and returns\n// the slices of individual messages. Also returns the number\n// of truncated messages and any potential error\nfunc decodeCompoundMessage(buf []byte) (trunc int, parts [][]byte, err error) {\n\tif len(buf) < 1 {\n\t\terr = fmt.Errorf(\"missing compound length byte\")\n\t\treturn\n\t}\n\tnumParts := int(buf[0])\n\tbuf = buf[1:]\n\n\t// Check we have enough bytes\n\tif len(buf) < numParts*2 {\n\t\terr = fmt.Errorf(\"truncated len slice\")\n\t\treturn\n\t}\n\n\t// Decode the lengths\n\tlengths := make([]uint16, numParts)\n\tfor i := 0; i < numParts; i++ {\n\t\tlengths[i] = binary.BigEndian.Uint16(buf[i*2 : i*2+2])\n\t}\n\tbuf = buf[numParts*2:]\n\n\t// Split each message\n\tfor idx, msgLen := range lengths {\n\t\tif len(buf) < int(msgLen) {\n\t\t\ttrunc = numParts - idx\n\t\t\treturn\n\t\t}\n\n\t\t// Extract the slice, seek past on the buffer\n\t\tslice := buf[:msgLen]\n\t\tbuf = buf[msgLen:]\n\t\tparts = append(parts, slice)\n\t}\n\treturn\n}\n\n// compressPayload takes an opaque input buffer, compresses it\n// and wraps it in a compress{} message that is encoded.\nfunc compressPayload(inp []byte, msgpackUseNewTimeFormat bool) (*bytes.Buffer, error) {\n\tvar buf bytes.Buffer\n\tcompressor := lzw.NewWriter(&buf, lzw.LSB, lzwLitWidth)\n\n\t_, err := compressor.Write(inp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Ensure we flush everything out\n\tif err := compressor.Close(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create a compressed message\n\tc := compress{\n\t\tAlgo: lzwAlgo,\n\t\tBuf:  buf.Bytes(),\n\t}\n\treturn encode(compressMsg, &c, msgpackUseNewTimeFormat)\n}\n\n// decompressPayload is used to unpack an encoded compress{}\n// message and return its payload uncompressed\nfunc decompressPayload(msg []byte) ([]byte, error) {\n\t// Decode the message\n\tvar c compress\n\tif err := decode(msg, &c); err != nil {\n\t\treturn nil, err\n\t}\n\treturn decompressBuffer(&c)\n}\n\n// decompressBuffer is used to decompress the buffer of\n// a single compress message, handling multiple algorithms\nfunc decompressBuffer(c *compress) ([]byte, error) {\n\t// Verify the algorithm\n\tif c.Algo != lzwAlgo {\n\t\treturn nil, fmt.Errorf(\"Cannot decompress unknown algorithm %d\", c.Algo)\n\t}\n\n\t// Create a uncompressor\n\tuncomp := lzw.NewReader(bytes.NewReader(c.Buf), lzw.LSB, lzwLitWidth)\n\tdefer uncomp.Close()\n\n\t// Read all the data\n\tvar b bytes.Buffer\n\t_, err := io.Copy(&b, uncomp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Return the uncompressed bytes\n\treturn b.Bytes(), nil\n}\n\n// joinHostPort returns the host:port form of an address, for use with a\n// transport.\nfunc joinHostPort(host string, port uint16) string {\n\treturn net.JoinHostPort(host, strconv.Itoa(int(port)))\n}\n\n// hasPort is given a string of the form \"host\", \"host:port\", \"ipv6::address\",\n// or \"[ipv6::address]:port\", and returns true if the string includes a port.\nfunc hasPort(s string) bool {\n\t// IPv6 address in brackets.\n\tif strings.LastIndex(s, \"[\") == 0 {\n\t\treturn strings.LastIndex(s, \":\") > strings.LastIndex(s, \"]\")\n\t}\n\n\t// Otherwise the presence of a single colon determines if there's a port\n\t// since IPv6 addresses outside of brackets (count > 1) can't have a\n\t// port.\n\treturn strings.Count(s, \":\") == 1\n}\n\n// ensurePort makes sure the given string has a port number on it, otherwise it\n// appends the given port as a default.\nfunc ensurePort(s string, port int) string {\n\tif hasPort(s) {\n\t\treturn s\n\t}\n\n\t// If this is an IPv6 address, the join call will add another set of\n\t// brackets, so we have to trim before we add the default port.\n\ts = strings.Trim(s, \"[]\")\n\ts = net.JoinHostPort(s, strconv.Itoa(port))\n\treturn s\n}\n"
        },
        {
          "name": "util_test.go",
          "type": "blob",
          "size": 8.4833984375,
          "content": "// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage memberlist\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestUtil_PortFunctions(t *testing.T) {\n\ttests := []struct {\n\t\taddr       string\n\t\thasPort    bool\n\t\tensurePort string\n\t}{\n\t\t{\"1.2.3.4\", false, \"1.2.3.4:8301\"},\n\t\t{\"1.2.3.4:1234\", true, \"1.2.3.4:1234\"},\n\t\t{\"2600:1f14:e22:1501:f9a:2e0c:a167:67e8\", false, \"[2600:1f14:e22:1501:f9a:2e0c:a167:67e8]:8301\"},\n\t\t{\"[2600:1f14:e22:1501:f9a:2e0c:a167:67e8]\", false, \"[2600:1f14:e22:1501:f9a:2e0c:a167:67e8]:8301\"},\n\t\t{\"[2600:1f14:e22:1501:f9a:2e0c:a167:67e8]:1234\", true, \"[2600:1f14:e22:1501:f9a:2e0c:a167:67e8]:1234\"},\n\t\t{\"localhost\", false, \"localhost:8301\"},\n\t\t{\"localhost:1234\", true, \"localhost:1234\"},\n\t\t{\"hashicorp.com\", false, \"hashicorp.com:8301\"},\n\t\t{\"hashicorp.com:1234\", true, \"hashicorp.com:1234\"},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.addr, func(t *testing.T) {\n\t\t\tif got, want := hasPort(tt.addr), tt.hasPort; got != want {\n\t\t\t\tt.Fatalf(\"got %v want %v\", got, want)\n\t\t\t}\n\t\t\tif got, want := ensurePort(tt.addr, 8301), tt.ensurePort; got != want {\n\t\t\t\tt.Fatalf(\"got %v want %v\", got, want)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestEncodeDecode(t *testing.T) {\n\tmsg := &ping{SeqNo: 100}\n\tbuf, err := encode(pingMsg, msg, false)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\tvar out ping\n\tif err := decode(buf.Bytes()[1:], &out); err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\tif msg.SeqNo != out.SeqNo {\n\t\tt.Fatalf(\"bad sequence no\")\n\t}\n}\n\nfunc TestRandomOffset(t *testing.T) {\n\tvals := make(map[int]struct{})\n\tfor i := 0; i < 100; i++ {\n\t\toffset := randomOffset(2 << 30)\n\t\tif _, ok := vals[offset]; ok {\n\t\t\tt.Fatalf(\"got collision\")\n\t\t}\n\t\tvals[offset] = struct{}{}\n\t}\n}\n\nfunc TestRandomOffset_Zero(t *testing.T) {\n\toffset := randomOffset(0)\n\tif offset != 0 {\n\t\tt.Fatalf(\"bad offset\")\n\t}\n}\n\nfunc TestSuspicionTimeout(t *testing.T) {\n\ttimeouts := map[int]time.Duration{\n\t\t5:    1000 * time.Millisecond,\n\t\t10:   1000 * time.Millisecond,\n\t\t50:   1698 * time.Millisecond,\n\t\t100:  2000 * time.Millisecond,\n\t\t500:  2698 * time.Millisecond,\n\t\t1000: 3000 * time.Millisecond,\n\t}\n\tfor n, expected := range timeouts {\n\t\ttimeout := suspicionTimeout(3, n, time.Second) / 3\n\t\tif timeout != expected {\n\t\t\tt.Fatalf(\"bad: %v, %v\", expected, timeout)\n\t\t}\n\t}\n}\n\nfunc TestRetransmitLimit(t *testing.T) {\n\tlim := retransmitLimit(3, 0)\n\tif lim != 0 {\n\t\tt.Fatalf(\"bad val %v\", lim)\n\t}\n\tlim = retransmitLimit(3, 1)\n\tif lim != 3 {\n\t\tt.Fatalf(\"bad val %v\", lim)\n\t}\n\tlim = retransmitLimit(3, 99)\n\tif lim != 6 {\n\t\tt.Fatalf(\"bad val %v\", lim)\n\t}\n}\n\nfunc TestShuffleNodes(t *testing.T) {\n\torig := []*nodeState{\n\t\t&nodeState{\n\t\t\tState: StateDead,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateAlive,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateAlive,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateDead,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateAlive,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateAlive,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateDead,\n\t\t},\n\t\t&nodeState{\n\t\t\tState: StateAlive,\n\t\t},\n\t}\n\tnodes := make([]*nodeState, len(orig))\n\tcopy(nodes[:], orig[:])\n\n\tif !reflect.DeepEqual(nodes, orig) {\n\t\tt.Fatalf(\"should match\")\n\t}\n\n\tshuffleNodes(nodes)\n\n\tif reflect.DeepEqual(nodes, orig) {\n\t\tt.Fatalf(\"should not match\")\n\t}\n}\n\nfunc TestPushPullScale(t *testing.T) {\n\tsec := time.Second\n\tfor i := 0; i <= 32; i++ {\n\t\tif s := pushPullScale(sec, i); s != sec {\n\t\t\tt.Fatalf(\"Bad time scale: %v\", s)\n\t\t}\n\t}\n\tfor i := 33; i <= 64; i++ {\n\t\tif s := pushPullScale(sec, i); s != 2*sec {\n\t\t\tt.Fatalf(\"Bad time scale: %v\", s)\n\t\t}\n\t}\n\tfor i := 65; i <= 128; i++ {\n\t\tif s := pushPullScale(sec, i); s != 3*sec {\n\t\t\tt.Fatalf(\"Bad time scale: %v\", s)\n\t\t}\n\t}\n}\n\nfunc TestMoveDeadNodes(t *testing.T) {\n\tnodes := []*nodeState{\n\t\t&nodeState{\n\t\t\tState:       StateDead,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateAlive,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t// This dead node should not be moved, as its state changed\n\t\t// less than the specified GossipToTheDead time ago\n\t\t&nodeState{\n\t\t\tState:       StateDead,\n\t\t\tStateChange: time.Now().Add(-10 * time.Second),\n\t\t},\n\t\t// This left node should not be moved, as its state changed\n\t\t// less than the specified GossipToTheDead time ago\n\t\t&nodeState{\n\t\t\tState:       StateLeft,\n\t\t\tStateChange: time.Now().Add(-10 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateLeft,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateAlive,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateDead,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateAlive,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t\t&nodeState{\n\t\t\tState:       StateLeft,\n\t\t\tStateChange: time.Now().Add(-20 * time.Second),\n\t\t},\n\t}\n\n\tidx := moveDeadNodes(nodes, (15 * time.Second))\n\tif idx != 5 {\n\t\tt.Fatalf(\"bad index\")\n\t}\n\tfor i := 0; i < idx; i++ {\n\t\tswitch i {\n\t\tcase 2:\n\t\t\t// Recently dead node remains at index 2,\n\t\t\t// since nodes are swapped out to move to end.\n\t\t\tif nodes[i].State != StateDead {\n\t\t\t\tt.Fatalf(\"Bad state %d\", i)\n\t\t\t}\n\t\tcase 3:\n\t\t\t//Recently left node should remain at 3\n\t\t\tif nodes[i].State != StateLeft {\n\t\t\t\tt.Fatalf(\"Bad State %d\", i)\n\t\t\t}\n\t\tdefault:\n\t\t\tif nodes[i].State != StateAlive {\n\t\t\t\tt.Fatalf(\"Bad state %d\", i)\n\t\t\t}\n\t\t}\n\t}\n\tfor i := idx; i < len(nodes); i++ {\n\t\tif !nodes[i].DeadOrLeft() {\n\t\t\tt.Fatalf(\"Bad state %d\", i)\n\t\t}\n\t}\n}\n\nfunc TestKRandomNodes(t *testing.T) {\n\tnodes := []*nodeState{}\n\tfor i := 0; i < 90; i++ {\n\t\t// Half the nodes are in a bad state\n\t\tstate := StateAlive\n\t\tswitch i % 3 {\n\t\tcase 0:\n\t\t\tstate = StateAlive\n\t\tcase 1:\n\t\t\tstate = StateSuspect\n\t\tcase 2:\n\t\t\tstate = StateDead\n\t\t}\n\t\tnodes = append(nodes, &nodeState{\n\t\t\tNode: Node{\n\t\t\t\tName: fmt.Sprintf(\"test%d\", i),\n\t\t\t},\n\t\t\tState: state,\n\t\t})\n\t}\n\n\tfilterFunc := func(n *nodeState) bool {\n\t\tif n.Name == \"test0\" || n.State != StateAlive {\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\ts1 := kRandomNodes(3, nodes, filterFunc)\n\ts2 := kRandomNodes(3, nodes, filterFunc)\n\ts3 := kRandomNodes(3, nodes, filterFunc)\n\n\tif reflect.DeepEqual(s1, s2) {\n\t\tt.Fatalf(\"unexpected equal\")\n\t}\n\tif reflect.DeepEqual(s1, s3) {\n\t\tt.Fatalf(\"unexpected equal\")\n\t}\n\tif reflect.DeepEqual(s2, s3) {\n\t\tt.Fatalf(\"unexpected equal\")\n\t}\n\n\tfor _, s := range [][]Node{s1, s2, s3} {\n\t\tif len(s) != 3 {\n\t\t\tt.Fatalf(\"bad len\")\n\t\t}\n\t\tfor _, n := range s {\n\t\t\tif n.Name == \"test0\" {\n\t\t\t\tt.Fatalf(\"Bad name\")\n\t\t\t}\n\t\t\tif n.State != StateAlive {\n\t\t\t\tt.Fatalf(\"Bad state\")\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestMakeCompoundMessage(t *testing.T) {\n\tmsg := &ping{SeqNo: 100}\n\tbuf, err := encode(pingMsg, msg, false)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\tmsgs := [][]byte{buf.Bytes(), buf.Bytes(), buf.Bytes()}\n\tcompound := makeCompoundMessage(msgs)\n\n\tif compound.Len() != 3*buf.Len()+3*compoundOverhead+compoundHeaderOverhead {\n\t\tt.Fatalf(\"bad len\")\n\t}\n}\n\nfunc TestDecodeCompoundMessage(t *testing.T) {\n\tmsg := &ping{SeqNo: 100}\n\tbuf, err := encode(pingMsg, msg, false)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\tmsgs := [][]byte{buf.Bytes(), buf.Bytes(), buf.Bytes()}\n\tcompound := makeCompoundMessage(msgs)\n\n\ttrunc, parts, err := decodeCompoundMessage(compound.Bytes()[1:])\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\tif trunc != 0 {\n\t\tt.Fatalf(\"should not truncate\")\n\t}\n\tif len(parts) != 3 {\n\t\tt.Fatalf(\"bad parts\")\n\t}\n\tfor _, p := range parts {\n\t\tif len(p) != buf.Len() {\n\t\t\tt.Fatalf(\"bad part len\")\n\t\t}\n\t}\n}\n\nfunc TestDecodeCompoundMessage_NumberOfPartsOverflow(t *testing.T) {\n\tbuf := []byte{0x80}\n\t_, _, err := decodeCompoundMessage(buf)\n\trequire.Error(t, err)\n\trequire.Equal(t, err.Error(), \"truncated len slice\")\n}\n\nfunc TestDecodeCompoundMessage_Trunc(t *testing.T) {\n\tmsg := &ping{SeqNo: 100}\n\tbuf, err := encode(pingMsg, msg, false)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\tmsgs := [][]byte{buf.Bytes(), buf.Bytes(), buf.Bytes()}\n\tcompound := makeCompoundMessage(msgs)\n\n\ttrunc, parts, err := decodeCompoundMessage(compound.Bytes()[1:38])\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\tif trunc != 1 {\n\t\tt.Fatalf(\"truncate: %d\", trunc)\n\t}\n\tif len(parts) != 2 {\n\t\tt.Fatalf(\"bad parts\")\n\t}\n\tfor _, p := range parts {\n\t\tif len(p) != buf.Len() {\n\t\t\tt.Fatalf(\"bad part len\")\n\t\t}\n\t}\n}\n\nfunc TestCompressDecompressPayload(t *testing.T) {\n\tbuf, err := compressPayload([]byte(\"testing\"), false)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\tdecomp, err := decompressPayload(buf.Bytes()[1:])\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected err: %s\", err)\n\t}\n\n\tif !reflect.DeepEqual(decomp, []byte(\"testing\")) {\n\t\tt.Fatalf(\"bad payload: %v\", decomp)\n\t}\n}\n"
        }
      ]
    }
  ]
}